# LLM Benchmark Project

This project is designed to benchmark and analyze the performance of various Large Language Models (LLMs) on a set of tasks. It includes configurations for multiple models, datasets for testing, ground truth for evaluation, analysis notebooks, and a Streamlit application for interactive visualization of results.

## Project Structure

```
.
├── .devcontainer/        # Development container configuration
├── configs/              # Configuration files for different LLMs (e.g., claude-3.5-sonnet.json, gpt_4o.json)
├── datasets/             # Datasets used for benchmarking tasks
├── ground_truth/         # Ground truth files for evaluating LLM outputs (e.g., E3_gt.md, M6_gt.md)
├── notebooks/            # Jupyter notebooks for analysis and exploration
│   └── LLM_Benchmark_Analysis.ipynb # Primary notebook for analyzing benchmark results
├── prompts/              # Prompts used to query the LLMs
├── results/              # Raw results generated by the LLMs
├── src/                  # Source code for the benchmarking framework or utilities
├── streamlit_app/        # Streamlit application for visualizing benchmark results
│   ├── app.py            # Main Streamlit application file
│   └── requirements.txt  # Python dependencies for the Streamlit app
└── ...                   # Other project files
```

## Key Components

*   **LLM Configurations (`configs/`)**: JSON files defining parameters and settings for each LLM tested.
*   **Ground Truth (`ground_truth/`)**: Markdown files containing the expected or ideal outputs for benchmark tasks. These are used to evaluate the performance of the LLMs. Examples include [ground_truth/E3_gt.md](ground_truth/E3_gt.md) and [ground_truth/M6_gt.md](ground_truth/M6_gt.md).
*   **Benchmark Analysis (`notebooks/LLM_Benchmark_Analysis.ipynb`)**: A Jupyter notebook ([notebooks/LLM_Benchmark_Analysis.ipynb](notebooks/LLM_Benchmark_Analysis.ipynb)) that processes the results from the `results/` directory, compares them against `ground_truth/`, and generates various performance metrics and visualizations. It analyzes aspects like average scores, task success rates, and performance by difficulty.
*   **Streamlit Dashboard (`streamlit_app/`)**: An interactive web application built with Streamlit ([streamlit_app/app.py](streamlit_app/app.py)) to visualize and explore the benchmark results. It likely uses data processed by the analysis notebook.

## Getting Started

1.  **Setup**:
    *   Ensure Python is installed.
    *   Install necessary dependencies. For the Streamlit app, refer to [streamlit_app/requirements.txt](streamlit_app/requirements.txt). The main analysis notebook ([notebooks/LLM_Benchmark_Analysis.ipynb](notebooks/LLM_Benchmark_Analysis.ipynb)) uses libraries like `pandas`, `matplotlib`, `seaborn`, and `numpy`.
2.  **Running Benchmarks**:
    *   (Details would be added here if known - e.g., scripts in `src/` to run the benchmarks using `configs/` and `prompts/` against `datasets/`, storing outputs in `results/`)
3.  **Analyzing Results**:
    *   Open and run the [notebooks/LLM_Benchmark_Analysis.ipynb](notebooks/LLM_Benchmark_Analysis.ipynb) Jupyter notebook to see a detailed breakdown of LLM performance. This notebook loads data, cleans scores (handling malformed scores as seen in its output), calculates statistics, and generates plots.
4.  **Viewing Dashboard**:
    *   Navigate to the `streamlit_app/` directory.
    *   Run the Streamlit app:
        ```sh
        streamlit run app.py
        ```
    *   This will launch a web interface to interactively explore the benchmark results.

## How it Works

The project likely follows this workflow:
1.  LLMs are prompted (using prompts from `prompts/`) to perform tasks on data from `datasets/`.
2.  The configurations for these LLMs are stored in `configs/`.
3.  The outputs (responses) from the LLMs are saved in the `results/` directory.
4.  The [notebooks/LLM_Benchmark_Analysis.ipynb](notebooks/LLM_Benchmark_Analysis.ipynb) notebook reads these results, compares them against the `ground_truth/` files, and performs statistical analysis. It calculates metrics such as average scores, success/incomplete/failure rates, and cost-effectiveness (score per dollar, based on `PRICING_CONFIG` in the notebook).
5.  The [streamlit_app/app.py](streamlit_app/app.py) provides an interactive dashboard to visualize these analyzed results, often including charts for average scores, performance by difficulty, and other comparisons.

# Running the benchmark on your local device:

1. Open your terminal and navigate to your desired folder
2. Run following command in terminal: git clone 
