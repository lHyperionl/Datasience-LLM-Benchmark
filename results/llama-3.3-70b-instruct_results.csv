task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
def load_wine_quality_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load dataset: {str(e)}"")

# Analyze the dataset
def analyze_dataset(data):
    print(""Column Names and Data Types:"")
    print(data.info())
    
    print(""\nDescriptive Statistics for Numerical Columns:"")
    numerical_columns = data.select_dtypes(include=['int64', 'f...",7,"The generated code correctly loads the dataset, identifies data types, and calculates descriptive statistics for numerical columns. It also reports the total number of rows and columns.

However, there are a few areas for improvement:
1.  **Data Type Identification Detail:** The `data.info()` output provides data types but doesn't explicitly categorize them as ""Numerical,"" ""Categorical,"" etc., as requested by the prompt. While the code does select numerical columns using `select_dtypes`, it doesn't print a clear, human-readable list of ""Column: 'col' | Identified Type: [type]"" for all columns as demonstrated in the ground truth.
2.  **Categorical/Ordinal Identification:** The code only identifies `object` dtype columns as categorical. For the `winequality-red.csv` dataset, it's known that all columns are numerical, but 'quality' is often treated as ordinal. The ground truth explicitly points this out as an example of how a numerical column could be conceptually ordinal. The generated code's approach would miss numerical columns that might be considered categorical/ordinal based on their values (e.g., a ""flag"" column with 0/1, or the 'quality' column itself). It also incorrectly assumes `object` dtypes are the only categorical ones; Pandas can have `category` dtype.
3.  **Missing `numpy` Import:** The `import numpy as np` is present but not used in the code.
4.  **Error Handling:** The `load_wine_quality_dataset` function simply prints the error message but doesn't exit, meaning `analyze_dataset` might be called with `None` if the file isn't found, leading to further errors. The ground truth handles this more robustly by exiting.",llama-3.3-70b-instruct,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Display the first 10 rows
print(df.head(10))

# Show the shape of the dataset
print(df.shape)

# List all column names and their data types
print(df.dtypes)

# Display the top 10 games by Global_Sales
print(df.nlargest(10, 'Global_Sales')[['Name', 'Platform', 'Genre', 'Global_Sales']])

# Show the unique values in the Platform and Genre columns
print(""Unique Platforms:"", df['Platform'].unique())
print(...",7,"The generated code correctly performs most of the requested EDA tasks.

Strengths:
- Correctly loads the dataset and displays the first 10 rows.
- Correctly shows the shape of the dataset.
- Correctly lists column names and data types (though `df.info()` is often preferred for more detail).
- Uses `nlargest` effectively to display top games by Global_Sales.
- Correctly shows unique values for Platform and Genre.
- Correctly calculates summary statistics for sales columns.
- Correctly identifies missing values.

Areas for Improvement:
- **Presentation/Readability:** The ground truth code includes clear print statements and separators (`---`, `===`) for each section, making the output much more readable and understandable, especially for a user analyzing the results. The generated code simply prints the output without any descriptive text before each section, making it harder to distinguish between the different parts of the analysis.
- **Error Handling:** The ground truth code includes `try-except` for `FileNotFoundError` when loading the CSV, which is crucial for robust code. It also checks if columns exist before trying to access them (e.g., `Global_Sales`, `Platform`, `Genre`, sales columns) to prevent `KeyError`. The generated code lacks these checks, which could lead to crashes if the file is missing or a column name is slightly different.
- **Completeness of Output for Task 3:** While `df.dtypes` works for data types, the request asked for ""list all column names and their data types,"" and `df.info()` provides a more comprehensive overview including non-null counts and memory usage, which is typically more useful in EDA.
- **Completeness of Output for Task 7:** While `df.isnull().sum()` is correct, the ground truth code adds a small check to only print columns with missing values and a concluding statement if no missing values are found, which is a nice touch for clarity.",llama-3.3-70b-instruct,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
def load_dataset(file_name):
    try:
        return pd.read_csv(file_name)
    except Exception as e:
        print(f""Error loading dataset: {str(e)}"")

# Clean and preprocess the dataset
def clean_dataset(df):
    # Handle missing values in the Year column
    df['Year'] = df['Year'].fillna(df['Year'].median())

    # Remove games with missing Publisher information
    df = df.dropna(subset=['Publisher'])

    # Create a new ...",6,"The generated code addresses most of the requirements but has several critical omissions and inaccuracies that prevent it from being production-ready or fully solving the task as described.

**Correctness & Completeness Issues:**

1.  **Year Column Type Handling:** The ground truth code explicitly converts `Year` to numeric with `errors='coerce'` and then to `int` after filling NaNs. The generated code skips this crucial step. If `Year` contains non-numeric strings, `df['Year'].median()` will fail or return incorrect results, and operations like `2023 - df['Year']` will also likely fail. This is a significant robustness issue.
2.  **`Years_Since_Release` Calculation:** The ground truth ensures `Years_Since_Release` is non-negative, which is a good practice as some `Year` values might be future dates due to data entry errors or placeholders. The generated code does not handle this, which could lead to negative `Years_Since_Release` values.
3.  **`Global_Sales` Type Handling:** Similar to `Year`, the `Global_Sales` column might not be numeric. The ground truth explicitly converts it to numeric with `errors='coerce'` and handles NaNs introduced by this conversion. The generated code directly applies `df[df['Global_Sales'] >= 0.1]`, which will raise an error if `Global_Sales` contains non-numeric data or if there are NaNs that haven't been handled upstream. This is a major flaw.
4.  **No `inplace=True` or reassignment:** Inside `clean_dataset`, `df['Year'] = df['Year'].fillna(df['Year'].median())` correctly reassigns the column. However, for `df.dropna(subset=['Publisher'])`, `df.drop_duplicates(...)`, and `df = df[df['Global_Sales'] >= 0.1]`, the `df` is correctly reassigned. The lack of `inplace=True` could cause issues if the user expected an in-place modification, but since it's `df = ...`, it's functionally correct in this specific case.
5.  **Missing initial dataset shape and intermediate print statements:** The ground truth provides more detailed logging about the dataset's shape at various stages of cleaning, which is very helpful for debugging and understanding the impact of each cleaning step. The generated code only provides the final shape.
6.  **Error Handling:** The `load_dataset` function has basic error handling, but the `clean_dataset` function does not handle potential errors that could arise from non-numeric data in `Year` or `Global_Sales`, as discussed above. The ground truth includes a `try-except` block for the entire cleaning process, making it more robust.

**Code Quality:**

1.  **Modularity:** The generated code uses functions (`load_dataset`, `clean_dataset`, `main`), which is good for modularity and reusability, a strong point over the linear script in the ground truth for larger projects.
2.  **Clarity:** The code is generally easy to read and understand.
3.  **Docstrings/Comments:** Lacks docstrings for functions.

**Conclusion:**

While the generated code attempts to solve the problem and adopts a more modular approach, it lacks crucial data type handling and robustness checks present in the ground truth. These omissions make it prone to errors with real-world, messy data and would lead to script failure or incorrect results for common data cleanliness issues. The missing `Years_Since_Release` validation and `Global_Sales` type conversion are critical flaws.",llama-3.3-70b-instruct,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('video_games_sales.csv')

# Create a bar chart showing the top 10 gaming platforms by total global sales
top_platforms = data.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False).head(10)
plt.figure(figsize=(10, 6))
plt.bar(top_platforms.index, top_platforms.values)
plt.title('Top 10 Gaming Platforms by Total Global Sales')
plt.xlabel('Platform')
plt.ylabel('Total Global Sales')...",7,"The generated code correctly addresses most of the visualization requirements, creating all the specified plot types (bar, pie, histogram, scatter, line) and including proper titles and labels. The imports are correct.

However, there are several areas where the code could be improved to match the quality and robustness of the ground truth:

1.  **Missing Data Cleaning/Preprocessing**: The ground truth includes robust data cleaning for `Year_of_Release` and sales columns, handling potential non-numeric values and NaNs. The generated code skips this crucial step, which can lead to errors or incorrect plots if the data isn't perfectly clean (e.g., `Year_of_Release` might have float or string values, or missing sales data).
2.  **Robustness in Plotting**:
    *   **Pie Chart**: The ground truth handles the ""Other"" category for genres with very few games, which makes the pie chart much more readable and informative by consolidating small slices. The generated code directly plots all genres, which can result in a cluttered and unreadable pie chart if there are many genres with tiny proportions.
    *   **Histogram**: The ground truth considers limiting the `Global_Sales` range for the histogram (e.g., to the 99th percentile) to better visualize the distribution, as sales data is often heavily skewed. The generated code plots the full range, which might obscure the distribution for the majority of games if there are extreme outliers.
    *   **Line Plot (Yearly Releases)**: The ground truth filters out unrealistic years (e.g., pre-1980 or post-2020) to ensure a more meaningful trend. The generated code plots all years, which might include erroneous data points.
3.  **Error Handling**: The ground truth includes a `try-except` block for loading the CSV, which makes the script more robust to `FileNotFoundError`. The generated code lacks this.
4.  **Plot Customization**: While basic labels and titles are present, the ground truth includes more advanced customizations (e.g., `xticks(rotation)`, `grid`, `tight_layout` on all plots, specific colors, `alpha` for scatter plot, `s` for marker size, `marker='o'` for line plot) that enhance readability and professionalism. Some `plt.tight_layout()` calls are missing in the generated code.

Overall, the generated code provides a functional solution, but it lacks the robustness, data handling best practices, and advanced plotting considerations present in the expert-level ground truth.",llama-3.3-70b-instruct,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
diabetes_df = pd.read_csv('diabetes.csv')

# Display the first 10 rows
print(diabetes_df.head(10))

# Show the shape of the dataset
print(diabetes_df.shape)

# List all column names and their data types
print(diabetes_df.dtypes)

# Calculate the percentage of diabetic vs non-diabetic patients
outcome_counts = diabetes_df['Outcome'].value_counts()
print(""Diabetic percentage: {:.2f}%"".format((outcome_counts[1] / len(diabetes_df))...",7,"The generated code addresses most of the requirements but has some shortcomings in terms of robustness, detail, and presentation.

**Strengths:**
*   Correctly loads the CSV and displays the first 10 rows.
*   Correctly shows the shape of the dataset.
*   Correctly displays column data types (though `df.info()` is often preferred for more detail).
*   Correctly calculates and displays missing values.
*   Correctly shows basic summary statistics.
*   Identifies columns with zero values, which is a good initial step for this specific dataset.

**Areas for Improvement:**
*   **Missing Value Handling:** The code for `isnull().sum()` is correct but doesn't explicitly filter to show only columns with *actually* missing values (i.e., `missing_values[missing_values > 0]`). The output is just the sum for *all* columns.
*   **Zero Value Identification:** While `diabetes_df.eq(0).sum()` is a concise way to find zeros, it identifies *all* columns with zeros. The ground truth specifically focuses on a list of columns where zeros are *likely* to represent missing data, which is a more nuanced and domain-aware approach for this specific dataset (e.g., 'Pregnancies' can legitimately be zero, 'Glucose' usually cannot). It also doesn't provide percentages of these zero values, which is useful context.
*   **Output Formatting:** The output is very minimal and lacks clear headings or separators, making it less readable compared to the ground truth. The ground truth uses `print` statements to clearly demarcate each section of the analysis.
*   **Robustness:** There's no error handling for `FileNotFoundError` if the CSV file doesn't exist, which the ground truth includes.
*   **Detail for Column Types:** While `df.dtypes` works, `df.info()` from the ground truth provides additional valuable information like non-null counts and memory usage, which is generally more comprehensive for EDA.
*   **Percentage Calculation for Outcome:** The ground truth uses `value_counts(normalize=True)` which is a more direct and often preferred way to get percentages for categorical data, and it directly provides percentages for both outcomes. The generated code manually calculates them based on indexed counts, which is more prone to error if the order of outcome categories changes or if there are more than two outcomes. The ground truth also explicitly checks if the 'Outcome' column exists.",llama-3.3-70b-instruct,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Identify columns with zero values that represent missing data
zero_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

# Replace zero values in these columns with NaN
df[zero_cols] = df[zero_cols].replace(0, np.nan)

# Handle missing values by filling them with the median of each respective column
df[zero_cols] = df[zero_col...",7,"The generated code generally addresses most requirements correctly.

**Strong Points:**
- Correctly loads the dataset.
- Accurately identifies columns for zero replacement and performs the replacement with `np.nan`.
- Handles missing values by filling them with the median of respective columns.
- Correctly removes duplicate rows.
- Creates the `BMI_Category` feature using `pd.cut` with appropriate bins and labels.
- Applies `StandardScaler` to the specified numerical features.
- Displays the shape and head of the cleaned dataset.

**Areas for Improvement / Missing Aspects:**
1.  **Robustness in File Loading:** The generated code lacks error handling for `FileNotFoundError` when loading `diabetes.csv`. The ground truth includes a `try-except` block which is good practice.
2.  **Handling of `BMI_Category` Bins:** The `BMI_Category` bins are slightly off compared to the specific ranges provided in the prompt:
    *   Prompt: 'Normal' (18.5-24.9), 'Overweight' (25-29.9)
    *   Generated: `bins=[0, 18.5, 25, 30, np.inf]`
    This means the 'Normal' category goes up to <25 (instead of 24.9) and 'Overweight' goes up to <30 (instead of 29.9). While often a minor detail, it's a direct deviation from the prompt's specifications. The ground truth uses `[0, 18.5, 24.9, 29.9, np.inf]` which addresses this precisely.
3.  **Scope of Scaling:** The generated code explicitly defines `numerical_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']`. While these are indeed numerical and need scaling, the prompt's request was to ""Scale numerical features using StandardScaler,"" implying all numerical features *relevant for scaling*. Without knowledge of the 'Outcome' column's role (which is typically a target and not scaled), hardcoding only these specific columns might be incomplete if other numerical features existed that should also be scaled, or if 'Outcome' was mistakenly scaled. The ground truth dynamically selects all numerical columns and then explicitly removes 'Outcome', which is a more robust approach.
4.  **Implicit Assumptions and Warnings:** The generated code assumes all `zero_cols` exist and doesn't provide warnings if any are missing or if 'BMI' is absent before creating `BMI_Category`. The ground truth includes checks for column existence.
5.  **Output Clarity:** While the shape and head are displayed, the ground truth provides a more verbose output, explicitly showing the original shape and confirming redundant rows removed or not, which enhances user understanding.",llama-3.3-70b-instruct,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
diabetes_df = pd.read_csv('diabetes.csv')

# Create a correlation heatmap
plt.figure(figsize=(10,8))
sns.heatmap(diabetes_df.drop('Outcome', axis=1).corr(), annot=True, cmap='coolwarm', square=True)
plt.title('Correlation Heatmap')
plt.show()

# Generate histograms for all numerical features
diabetes_df.drop('Outcome', axis=1).hist(figsize=(12,10), bins=10)
plt.tig...",7,"The generated code correctly addresses most of the requirements and produces the specified visualizations. The use of `seaborn` and `matplotlib` is appropriate.

However, there are several areas for improvement:
- **Histograms:** The `hist()` method directly on the DataFrame is used, which creates separate plots for each feature but lacks individual titles and proper labels. The requirement was to ""Generate histograms for all numerical features to show their distributions"" which implies individual attention to styling.
- **Box Plots:** Only a subset of the numerical features (Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI) are included in the box plots, whereas the requirement was to compare ""each feature"". This is a significant omission.
- **Titles and Labels:** While main titles are present for some plots, many subplots (especially histograms and box plots) lack specific titles or detailed axis labels as suggested by ""include proper titles and labels for all charts"". For example, the x-axis for box plots could clarify what 0 and 1 mean for 'Outcome'.
- **Loading Dataset:** The ground truth includes error handling for `FileNotFoundError` and prints basic DataFrame info which is good practice for data science tasks. The generated code lacks this.
- **Code Organization and Readability:** The ground truth segments the code clearly with comments for each task, enhancing readability. While the generated code has implicit sections, explicit comments would improve it.
- **Pair Plot:** The pair plot title is missing.

Overall, it's a good attempt, but attention to detail regarding plot completeness (all features) and better labeling would significantly improve its quality.",llama-3.3-70b-instruct,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Display the first 10 rows
print(df.head(10))

# Show the shape of the dataset
print(f""Dataset shape: {df.shape}"")

# List all column names and their data types
print(df.dtypes)

# Display the unique regional indicators and count of countries per region
print(df['Regional indicator'].value_counts())

# Show the top 10 happiest countries based on Ladder score
print(df.nlargest(10, 'Ladder score...",7,"The generated code correctly addresses most of the requirements.

**Strong points:**
*   Correctly loads the dataset and displays the first 10 rows.
*   Correctly identifies the shape and data types.
*   Correctly displays unique regional indicators and counts.
*   Correctly identifies the top 10 happiest countries.
*   Correctly calculates summary statistics and identifies missing values.
*   Uses `nlargest` which is an efficient way to get top N rows.

**Areas for improvement:**
*   **Error Handling (Critical):** The generated code lacks any error handling for `FileNotFoundError` when attempting to load the CSV. The ground truth code includes a `try-except` block for this, which is crucial for robust applications. Without it, the script will crash if the file is not present.
*   **Clarity and Readability:** While the output is technically correct, it lacks the formatting and separators found in the ground truth, making the output less readable and harder to distinguish between different tasks. The ground truth uses descriptive print statements and separators to clearly delineate each section of output.
*   **Robustness against missing columns:** The generated code assumes all specified columns exist (e.g., 'Regional indicator', 'Ladder score', 'Logged GDP per capita', etc.). If a column were misspelled or missing, it would raise a `KeyError`. The ground truth code includes checks for column existence before accessing them, making it more robust.
*   **Task 3 (Column names and data types):** While `df.dtypes` is correct, `df.info()` as used in the ground truth provides a more comprehensive summary including non-null counts, which is often more useful for EDA than just types.
*   **Explicit Print Statements:** Some outputs (e.g., `df.head(10)`, `df.dtypes`) are printed directly without a preceding descriptive string (e.g., ""1. First 10 rows of the dataset:""), which makes the output less user-friendly.",llama-3.3-70b-instruct,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
import numpy as np

# Load the dataset
def load_dataset(file_name):
    try:
        dataset = pd.read_csv(file_name)
        return dataset
    except Exception as e:
        print(f""Error loading dataset: {e}"")

# Handle missing values
def handle_missing_values(dataset):
    numerical_features = dataset.select_dtypes(include=['int64', 'float64']).columns
    categorical_features = dataset.select_dtypes...",7,"The generated code addresses most of the requirements and demonstrates a good understanding of the task. The modular design with functions is a good practice.

However, there are a few areas for improvement:

1.  **Strictness in Feature Creation (Happiness_Category):** The `pd.cut` approach for `Happiness_Category` is generally good, but the bins `[0, 4, 6.5, np.inf]` might not perfectly align with the `>6.5`, `4-6.5`, `<4` requirement. Specifically, the 'Medium' category should include 4 and 6.5, which `pd.cut`'s default `right=True` handles, but the ground truth uses `np.select` for stricter range control, often preferred for exact boundary conditions as specified. `pd.cut` with `right=False` on the lower bound could also be used, but the initial example was quite precise.
2.  **`Well_being_Score` Calculation:** The generated code directly sums three columns and divides by 3. While this is mathematically equivalent to `mean(axis=1)`, using `.mean(axis=1)` is more idiomatic pandas and robust for potentially different numbers of columns if the feature definition were to change.
3.  **Outlier Removal Side Effect:** In `remove_outliers`, `dataset = dataset[...]` creates a *new* dataframe. If the original `dataset` from `main()` is still pointing to the *old* dataframe reference and the function returns the new one (which it does), this is fine. However, a common pitfall is if `dataset` were modified in place (which it isn't here). Nevertheless, for clarity, it's often better to explicitly reassign outside the function or ensure in-place modification if intended. A more significant issue is that when returning a new DataFrame, if the next function `standardize_features` then operates on the `dataset` returned by `remove_outliers`, it correctly receives the filtered data.
4.  **No `Dataset Not Found` Handling in `main`:** The `load_dataset` function has `try-except` but `main()` doesn't `exit()` or handle the `None` return if a `FileNotFoundError` occurs. This could lead to subsequent errors if the file isn't found. The ground truth exits immediately.
5.  **Standardization of `Regional Indicator`:** The `standardize_features` function selects all numerical columns (`int64`, `float64`) for scaling. After label encoding, `Regional indicator` will become an `int64` column, leading to its accidental standardization. This is incorrect as `Regional indicator` is a categorical feature that was converted to numerical for encoding, not for scaling. The ground truth correctly identifies features to scale *before* encoding or handles it by design (scaling all numerical features *before* encoding the categorical one, which is then handled separately). The generated code performs all feature engineering (including encoding) *before* standardizing all numerical features.

Overall, the code is well-structured and tackles most aspects. The issues are mainly related to subtle implementation details and potential logical flow issues regarding feature types after transformations.",llama-3.3-70b-instruct,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from folium import Map

# Load dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# World map visualization
fig = px.choropleth(df, locations=""Country name"", color=""Ladder score"", 
                    color_continuous_scale='Blues', 
                    locationmode=""country names"")
fig.update_layout(title=""World Happiness ...",6,"The generated code attempts to address all parts of the task and uses the requested libraries. However, it falls short in several areas:

**Correctness:**
*   **Radar Chart:** The radar chart implementation is incorrect. It tries to plot raw values directly on the radar chart, which doesn't make sense for features with vastly different scales (e.g., 'Ladder score' is typically 0-8, while 'Log GDP per capita' is also a different range). Radar charts require normalized or scaled data for meaningful comparison. The ground truth correctly uses `MinMaxScaler` for this.
*   **Column Names:** The generated code uses original column names like `Log GDP per capita` and `Ladder score` throughout. While these might work, the ground truth's approach of consistently renaming them to more user-friendly names (e.g., `Happiness Score`, `GDP per Capita`) improves readability and maintainability.
*   **Plotly display:** The Plotly figures (`fig`, `fig_bar`) are not displayed when the code is executed in an environment where `fig.show()` is expected to immediately render. They are called at the end, but the Matplotlib plots (`plt.show()`) would block their display. This is a common issue with mixing interactive Plotly and static Matplotlib renders.

**Code Quality:**
*   **Imports:** `plotly.graph_objects as go` and `from folium import Map` are imported but not used.
*   **Error Handling:** There's no error handling for `FileNotFoundError` when loading the CSV, which is a good practice.
*   **Consistent Plot Display:** Mixing `plt.show()` and `fig.show()` calls can lead to unpredictable behavior in integrated environments (e.g., Jupyter notebooks). For individual plots, just calling `plt.show()` after each Matplotlib plot and `fig.show()` after each Plotly plot is generally better than calling them all at the end. The generated code calls `plt.show()` after some Matplotlib plots but postpones Plotly ones until the very end.
*   **Plot Customization:** Titles and labels are present but are not as detailed, e.g., missing year (2021) in some titles which is useful context.
*   **Redundancy in Plotly titles:** `title=""World Happiness Scores by Country""` is used, but the task explicitly requested comprehensive visualizations. The ground truth uses `title=""World Happiness Score Map (2021)""` which is more specific.

**Completeness:**
*   Most visualization types are covered.
*   The radar chart is present but functionally incorrect due to lack of scaling.
*   Scatter plots miss the `hue` by `Regional indicator` which adds valuable insight, as included in the ground truth.

To improve, the submission needs to fix the radar chart normalization, enhance plot details, and refine the plot display mechanism for better user experience.",llama-3.3-70b-instruct,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
# Import necessary libraries
import pandas as pd

# Load dataset and parse 'date' column as datetime objects
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Display the first 10 rows
print(""First 10 Rows:"")
print(df.head(10))

# Show the shape of the dataset (rows and columns)
print(""\nDataset Shape:"")
print(df.shape)

# List all column names and their data types
print(""\nColumn Names and Data Types:"")
print(df.dtypes)

# Calculate and display the total sum of '...",7,"The generated code correctly addresses most of the tasks.

**Strong Points:**
- **Correct Imports:** Imports `pandas` as required.
- **Loading and Parsing:** Correctly loads the CSV and parses the 'date' column.
- **Basic Display:** `head()`, `shape`, `dtypes`, and `isnull().sum()` are used correctly to display initial dataset info and missing values.
- **Continental Summary:** The grouping and summing for 'total_cases' and 'total_deaths' by 'continent' is correct.
- **Summary Statistics:** `describe()` provides the requested statistics for 'new_cases' and 'new_deaths' efficiently.

**Areas for Improvement (Why not a higher score):**

1.  **Robustness in File Loading:** The ground truth includes `try-except` blocks for file loading, which is a good practice for robustness. The generated code assumes the file will always be found.
2.  **`df.info()` vs. `df.dtypes`:** While `df.dtypes` provides data types, `df.info()` (used in the ground truth) is generally preferred for exploratory data analysis as it also provides non-null counts and memory usage, giving a more comprehensive overview of the DataFrame's structure and data quality.
3.  **Top 5 Locations (Precision):**
    *   The generated code uses `latest_dates = df.loc[df.groupby('location')['date'].idxmax()]` which is a valid way to get the row with the maximum date for each location.
    *   However, `latest_dates.nlargest(5, 'total_cases')[['location', 'total_cases']]` only shows `location` and `total_cases`. The ground truth explicitly includes `date` in the output (`print(top_5_locations[['location', 'date', 'total_cases']])`), which is more informative for verifying that the latest date was indeed used.
    *   More importantly, the ground truth code includes crucial `.dropna(subset=['total_cases'])` after `pd.to_numeric` conversion for `total_cases` when determining the top locations. This makes the logic more robust. If `total_cases` contains non-numeric values or NaNs, `nlargest` might skip those, which could lead to unexpected results or errors in some Pandas versions if the column isn't purely numeric. The ground truth explicitly handles this type of data cleaning within the relevant step.

To achieve a higher score, the generated code should incorporate error handling for file loading, use `df.info()` for column details, and ensure the top 5 locations display includes the latest date and robustly handles potential non-numeric `'total_cases'` values.",llama-3.3-70b-instruct,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Calculate and report the average quality
average_quality = df['quality'].mean()
print(f'Average quality of red wines: {average_quality}')

# Group by quality and calculate average alcohol content
quality_alcohol_avg = df.groupby('quality')['alcohol'].mean().reset_index()

# Create a bar chart
plt.figure(figsize=(8, 6))
sns.barplot(x='quality', y='alco...",8,"The generated code correctly addresses all three main requirements of the task: calculating the average quality, grouping by quality to find average alcohol content, and creating a bar chart for visualization. The imports are correct, and the code is well-structured and concise.

Areas for improvement to achieve a higher score (parity with ground truth):
1.  **Error Handling**: The generated code lacks error handling for `FileNotFoundError` when reading the CSV file, which the ground truth code includes. This makes the code more robust.
2.  **Output Readability**: The ground truth provides more detailed print statements, including formatting the average quality to two decimal places and explicitly printing the grouped alcohol content.
3.  **Visualization Enhancements**: The ground truth's visualization is more polished:
    *   It sets a `seaborn` style (`sns.set_style(""whitegrid"")`).
    *   It uses a `palette` for the bar chart (`palette='viridis'`).
    *   It adds data labels on top of the bars, showing the exact average alcohol content for each quality.
    *   It has more descriptive and larger font sizes for the title and labels, and adjusts tick label sizes.
    *   It explicitly uses `plt.tight_layout()` for better plot margins.

While the generated code fully completes the core task, these enhancements would improve its production readiness and presentation quality.",llama-3.3-70b-instruct,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

def clean_and_preprocess_data(file_name):
    # Load the dataset and parse the 'date' column as datetime objects
    dataset = pd.read_csv(file_name, parse_dates=['date'])

    # Handle missing values in 'hosp_patients' and 'icu_patients' by filling them with 0
    dataset[['hosp_patients', 'icu_patients']] = dataset[['hosp_patients', 'icu_patients']].fillna(0)

    # Handle missing values in other numerical columns by filling them with the mean ...",8,"The generated code correctly addresses most of the task requirements.

**Strong Points:**
*   **Correctness:** Loads data, parses dates, handles missing values for specified columns with the correct strategies (0 for `hosp_patients`/`icu_patients`, mean for others), creates `mortality_rate` and `vaccination_rate_per_capita` handling division by zero, and filters data for 2021.
*   **Code Quality:** Uses functions (`clean_and_preprocess_data`, `main`) which is good for modularity. Proper imports are included.
*   **Completeness:** Addresses all requirements for data loading, missing value handling, new column creation, filtering, and displaying results.

**Areas for Improvement (Why not a 9 or 10):**
1.  **Robustness for `mean()` calculation:** The ground truth includes `pd.to_numeric(errors='coerce')` before calculating the mean for `positive_rate` and `reproduction_rate`. This is a more robust approach, as `fillna(mean())` might fail if the column contains non-numeric values that haven't been coerced, or it might produce incorrect means. The generated code assumes these columns are already numeric, which might not always be the case in real-world messy data.
2.  **`inplace=True` vs. reassignment:** The ground truth uses `inplace=True` for `fillna` on `hosp_patients` and `icu_patients`, which is a common practice. The generated code reassigns the columns which is also valid but slightly different in style. Neither is strictly better, but consistency can be good.
3.  **Display `head()` columns:** The task requested displaying the first 5 rows with the *new columns*. The generated code specifically prints `[['date', 'hosp_patients', 'icu_patients', 'mortality_rate', 'vaccination_rate_per_capita']]`. While this includes the new columns, `head()` with no arguments would show all columns, including potentially other important ones, which might be a more direct interpretation of ""with the new columns"" (i.e., showing context with other columns as well, not just a select few). The ground truth `print(df_2021.head())` is a better interpretation here.
4.  **Error Handling for File Not Found:** The ground truth includes a `try-except FileNotFoundError` block, which makes the code more robust. The generated code does not explicitly handle this.

Overall, the generated code is very good and largely fulfills the requirements. The points above are minor improvements for robustness and precision.",llama-3.3-70b-instruct,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset and parse 'date' column
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Time series line plot of 'new_cases' for 'India'
india_cases = df[df['location'] == 'India']
plt.figure(figsize=(10,6))
plt.plot(india_cases['date'], india_cases['new_cases'])
plt.title('New Cases in India Over Time')
plt.xlabel('Date')
plt.ylabel('New Cases')
plt.grid(True)
plt.show()

# Bar chart of s...",6,"The generated code correctly addresses most of the plotting tasks. However, it falls short in crucial aspects of data preparation for certain plots, leading to inaccurate visualizations, especially for the 'total_deaths' bar chart and the 'gdp_per_capita' vs 'total_vaccinations' scatter plot. It also lacks some of the desired aesthetic and data-handling robustness present in the ground truth.

Here's a breakdown:

**Correctness:**
- **Time Series Plot (India):** Correct.
- **Bar Chart (Total Deaths by Continent):** Incorrect data aggregation. Simply summing 'total_deaths' by continent will lead to an inflated number because 'total_deaths' is a cumulative metric recorded daily. To get an accurate sum by continent, one should take the latest 'total_deaths' value for each *country* and then sum those country-level latest values by continent. The generated code sums all daily entries for each continent.
- **Scatter Plot (GDP per Capita vs. Total Vaccinations):** Incorrect data preparation. While it attempts to get the latest entry per location using `idxmax()`, it doesn't handle missing values (`NaN`) for 'gdp_per_capita' or 'total_vaccinations', which can lead to empty plots or warnings if those columns contain NaNs in the latest entries. The ground truth explicitly drops NaNs for these columns. Additionally, it doesn't use the `hue` or `size` mapping for better visualization as implied by the task (showing relationship for _all_ locations).
- **Histogram (Reproduction Rate):** Correct.
- **Box Plot (Stringency Index by Continent):** Correct, though does not explicitly drop NaN values for `stringency_index` which the ground truth does for cleaner plots.

**Code Quality:**
- Imports are proper.
- Structure is clear and follows the task requirements.
- Uses `matplotlib` and `seaborn` appropriately.
- Labels and titles are present.
- Lacks error handling for file loading.

**Completeness:**
- **Missing Data Handling:** The ground truth includes `dropna()` calls for scatter plot and box/hist plots to handle missing values, which is good practice and can prevent issues or produce more meaningful plots. The generated code lacks this.
- **Enhanced Scatter Plot:** The ground truth uses `hue='continent'` and `size='population'` for the scatter plot, which enhances the visualization and aligns with the typical way such relationships are explored in data science (adding more context). The generated code just has basic scatter plot.
- **Aesthetic Touches:** The ground truth includes `sns.set_theme(style=""whitegrid"")` and more elaborate grid lines and tight layouts, contributing to more professional-looking plots. The generated code is more basic in styling.
- **Specific aggregation logic:** The most significant completeness gap is in the handling of cumulative metrics ('total_deaths', 'total_vaccinations'). The ground truth specifically tackles this by taking the *latest* data point per location/country before aggregating, which is essential for accurate insights on cumulative metrics. The generated code simply sums `total_deaths` without this critical pre-processing step.",llama-3.3-70b-instruct,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
# Import necessary libraries
import pandas as pd

# Load the dataset
def load_dataset(file_path):
    try:
        data = pd.read_csv(file_path)
        return data
    except Exception as e:
        print(f""Failed to load dataset: {str(e)}"")
        return None

# Main function
def main():
    # Load the dataset
    data = load_dataset('clash_royale_cards.csv')

    if data is not None:
        # List all column names and identify the data type of each column
        print(""Column Nam...",8,"The generated code correctly addresses almost all requirements of the task. It properly loads the CSV, identifies column data types, calculates descriptive statistics for the specified numerical columns (`elixirCost`, `Win Rate`, `Usage`), and reports the total number of rows.

**Strong Points:**
*   **Correctness:** All core functionalities are implemented correctly.
*   **Readability:** The code is well-structured using functions (`load_dataset`, `main`) which enhances readability and organization.
*   **Error Handling:** Includes basic error handling for file loading, which is good practice.
*   **Completeness:** Addresses most of the task requirements.

**Areas for Improvement (to reach a perfect 10):**
*   **Descriptive Statistics Output Format:** The ground truth code uses `.agg()` to produce a concise table of all desired statistics for all specified columns at once. The generated code iterates through each column and uses `.describe()`, which outputs a more verbose set of statistics (including count, 25%, 50%, 75% percentiles) and separates the output for each column. While `describe()` includes the requested statistics (mean, median, std, min, max), the single, consolidated table from `.agg()` is generally preferred for presenting these specific statistics for multiple columns as requested. The task specifically asked for ""mean, median, standard deviation, min, max"" and not `describe()`'s full output.
*   **Error Handling Detail:** The `load_dataset` function catches a general `Exception`. While it prevents the program from crashing, the ground truth code specifically checks for `FileNotFoundError` which provides more targeted feedback to the user.
*   **Column Listing:** The ground truth explicitly lists column names and then their dtypes, which matches the prompt's request to ""List all column names and identify the data type of each column"" more literally than just printing `df.dtypes`.",llama-3.3-70b-instruct,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

def load_and_filter_cards():
    # Load the clash_royale_cards.csv dataset
    cards = pd.read_csv('clash_royale_cards.csv')

    # Filter the dataset to include only cards with an elixirCost less than or equal to 3
    filtered_cards = cards[cards['elixirCost'] <= 3]

    # Select and display the Card name and Win Rate columns
    selected_columns = filtered_cards[['Card', 'Win Rate']]

    # Sort the resulting data by Win Rate in descending order
    sorted_data ...",9,"The generated code correctly implements all the requirements of the task: it loads the CSV, filters by `elixirCost <= 3`, selects the specified columns (`Card`, `Win Rate`), and sorts the results by `Win Rate` descending. The code is well-structured with a function for the main logic and a `main` function for execution.

Areas for minor improvement (not critical for correctness but good practice):
1.  **Error Handling**: The generated code lacks explicit error handling for `FileNotFoundError` or other exceptions during `pd.read_csv`. The ground truth code includes robust `try-except` blocks which is good practice for production-ready code.
2.  **Configuration**: The ground truth uses constants for column names and file paths, which makes the code more maintainable and less prone to typos, especially in larger projects. While not strictly required for this small task, it's a good practice.
3.  **Redundant Display**: The prompt asked to ""display"" the columns, and the code does this by returning and printing the DataFrame. This is perfectly acceptable.

Overall, it's a very good solution that fully addresses the prompt's requirements.",llama-3.3-70b-instruct,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
def load_dataset(file_name):
    try:
        return pd.read_csv(file_name)
    except FileNotFoundError:
        print(""The file was not found. Please check the file path."")
        return None
    except pd.errors.EmptyDataError:
        print(""The file is empty. Please check the file contents."")
        return None
    except pd.errors.ParserError:
        print(""An error occurred while parsing the file. Please ...",7,"The generated code correctly loads the CSV and generates a histogram for the `elixirCost` column with a title and labeled axes, fulfilling the core requirements. The use of functions (`load_dataset`, `generate_histogram`, `main`) indicates good code organization.

However, there are several areas for improvement as compared to the ground truth:
*   **Missing Seaborn**: The ground truth uses Seaborn for a more visually appealing histogram, which is missing in the generated code. While Matplotlib `hist` is acceptable, `sns.histplot` often provides better defaults and aesthetics.
*   **Histogram Bins**: The `bins=10` in `plt.hist` is a generic choice. Given that `elixirCost` is discrete and typically ranges from 1 to 10, the ground truth's `bins=range(1, 12)` provides more accurate and interpretable binning, ensuring each discrete elixir cost has its own bin. The generated code's standard binning might group distinct elixir costs.
*   **Specific Labeling**: The `generate_histogram` function uses `f'Distribution of {column}'` for the title and `Elixir Cost` as `xlabel`. While functionally correct, the ground truth's title `Distribution of Elixir Cost for Clash Royale Cards` is more descriptive. Similarly, the `ylabel` in the generated code is 'Frequency', while 'Number of Cards' (as in the ground truth) is more contextually appropriate for this specific dataset.
*   **Robustness**: The ground truth includes more robust error handling beyond `FileNotFoundError` (e.g., handling missing `elixirCost` column, other `Exception` during loading) and more specific `xticks` and `yticks` styling for clarity. While the generated code has some error handling (`EmptyDataError`, `ParserError`), it doesn't check for the existence of the `elixirCost` column, which is crucial if the data structure were to change.
*   **Readability/Polish**: The ground truth adds a grid and sets `xticks` and `yticks` font sizes, contributing to a more polished and readable plot.",llama-3.3-70b-instruct,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# Load the iris dataset
def load_iris_dataset(file_name):
    try:
        iris_data = pd.read_csv(file_name)
        return iris_data
    except FileNotFoundError:
        print(""File not found. Please ensure the CSV file is in the current directory."")
        return None
    except pd.errors.EmptyDataError:
        print(""No data in file. Please check the CSV file."")
        return None
    except pd.errors.ParserError:
        print(""Error parsing the file. Plea...",8,"The generated code correctly addresses most of the task requirements. It loads the dataset, lists column data types, calculates descriptive statistics for the specified numerical columns, and reports unique species counts.

**Strong points:**
*   Proper use of `pandas` for data loading and manipulation.
*   Includes error handling for `FileNotFoundError` during CSV loading.
*   Correctly uses `df.dtypes` for data types and `df[numerical_columns].describe()` for descriptive statistics, which directly provides mean, std, min, max, and quantiles (though it includes more than just the requested five).
*   Correctly uses `df['Species'].value_counts()` for unique species counts.

**Areas for improvement:**
*   **Column Names Listing:** The task explicitly asked to ""List all column names and identify the data type of each column."" The generated code only prints `iris_data.dtypes`, which includes the data types but not a separate list of column names (e.g., `iris_data.columns.tolist()`). While `dtypes` implicitly shows column names with their types, a separate direct listing was requested.
*   **Descriptive Statistics Detail:** The prompt specifically asked for ""mean, median, standard deviation, min, max"". While `describe()` provides these (and more, like quartiles and count), the ground truth specifically selects `mean`, `median`, `std`, `min`, `max` using `.agg()`. The generated code's use of `.describe()` is acceptable as it includes the requested statistics, but it's less precise to the exact list requested. However, in data science, `describe()` is a very common and efficient way to get these, so it's a minor point. The ground truth's approach is more tailored to the exact output requested.
*   **Print Statements:** The ground truth includes more verbose and structured print statements (e.g., ""--- Column Names and Data Types ---"", ""Dataset 'iris.csv' loaded successfully.""). The generated code is functional but less explicit in its output descriptions. This is a stylistic point, but good for clarity.
*   **Robustness (Minor):** The ground truth adds checks if `numerical_columns` or `Species` column actually exist before attempting operations, which makes it more robust to malformed datasets. The generated code assumes these columns will always be present, which is typical for standard datasets but less robust for general use. For this specific task, it's fine as the dataset structure is known.

Overall, the generated code is highly functional and correct for the given task, with minor rooms for improvement in adhering strictly to the exact output format requested for column names and descriptive statistics, and adding more explicit checks for column existence.",llama-3.3-70b-instruct,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

def main():
    # Load the iris dataset
    iris_dataset = pd.read_csv('iris.csv')

    # Filter the dataset to include only 'Iris-setosa' records
    filtered_dataset = iris_dataset[iris_dataset['Species'] == 'Iris-setosa']

    # Select 'SepalLengthCm' and 'SepalWidthCm' from the filtered dataset
    selected_columns = filtered_dataset[['SepalLengthCm', 'SepalWidthCm']]

    # Display the selected columns
    print(selected_columns)

if __name__ == ""__main__"":
  ...",8,"The generated code correctly performs all the required operations: loading the CSV, filtering by 'Species' to 'Iris-setosa', and selecting and displaying the specified columns (`SepalLengthCm` and `SepalWidthCm`). The imports are proper, and the code is well-structured within a `main` function, following best practices.

The only minor area for improvement, especially for robust applications, is the lack of error handling (e.g., `FileNotFoundError` or `KeyError` if columns are missing), which is present in the ground truth. While not explicitly asked for as a primary task requirement, good production code often includes it. However, given the prompt's simplicity and focus on the core data manipulation task, the generated code is highly effective.",llama-3.3-70b-instruct,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
iris = pd.read_csv('iris.csv')

# Create a scatter plot
plt.figure(figsize=(10, 8))
for species in iris['Species'].unique():
    species_iris = iris[iris['Species'] == species]
    plt.scatter(species_iris['SepalLengthCm'], species_iris['SepalWidthCm'], label=species)

# Add title and labels
plt.title('Relationship between Sepal Length and Sepal Width')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')...",9,"The generated code correctly addresses all requirements: it loads the `iris.csv` dataset, creates a scatter plot of `SepalLengthCm` vs `SepalWidthCm`, colors points by `Species`, and includes a title, axis labels, and a legend. Imports are correct. The code is concise and effective.

Areas for minor improvement (not critical enough to significantly reduce the score):
- The `try-except` block for file loading and column existence checks (as in the ground truth) would make the code more robust against common errors.
- The axis labels (`Sepal Length (cm)` and `Sepal Width (cm)`) in the generated code are slightly different from the column names (`SepalLengthCm` and `SepalWidthCm`) used in the ground truth for labels. While not incorrect, consistency with column names can sometimes be preferred unless specific units are being added. The ground truth used the exact column names as labels.
- The title in the generated code is a bit less descriptive than the ground truth's title.
- The scatter plot creation in the ground truth includes `alpha`, `edgecolors`, and `linewidth` for better visual presentation, which are absent in the generated code. While these are enhancements, not core requirements, they contribute to better plot quality.",llama-3.3-70b-instruct,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

# Load the heart.csv dataset
def load_data(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load file: {e}"")

# Load the heart.csv dataset
data = load_data('heart.csv')

# List all column names and identify the data type of each column
def print_column_names_and_types(data):
    print(""Column Names and Data Types:"")
    print(data.dtypes)

# Calculate and report basic descriptive sta...",7,"The generated code correctly addresses most of the task requirements. It successfully loads the data, identifies column data types, calculates descriptive statistics for the specified numerical columns, and reports value counts for 'HeartDisease'.

Areas for improvement to achieve a higher score:
1.  **Error Handling for File Not Found:** The generated code includes a `try-except` block for file loading, but it doesn't handle the `FileNotFoundError` specifically by printing a user-friendly message or exiting the script if the file is not found, which means subsequent operations would likely fail without a DataFrame. The ground truth provides more robust error handling.
2.  **Output Readability for Descriptive Statistics:** While the descriptive statistics are correctly calculated, the output format for each column is printed individually, making it less concise and harder to compare multiple columns at a glance. The ground truth's use of `df[numerical_cols_for_stats].describe()` and then selecting specific rows (mean, median, std, min, max) provides a cleaner, tabular output for all specified columns simultaneously.
3.  **Missing Column Handling (Descriptive Statistics):** The generated code assumes all `numerical_columns` exist. If one were missing (e.g., a typo in 'RestingBP'), it would raise a `KeyError`. The ground truth explicitly checks for the existence of these columns and provides a warning if any are missing, but continues with the rest.
4.  **Redundancy in `load_data` and Function Calls:** The `load_data` function is defined, but then its return value is assigned globally and passed to subsequent functions. This structure is fine, but the global assignment could be more explicitly handled within an `if __name__ == ""__main__""` block, similar to how the ground truth directly loads the data within its main script flow, making it self-contained.
5.  **Output for Column Names:** The ground truth separates column names and their data types, which is a minor aesthetic improvement but contributes to readability. The generated code prints `df.dtypes` which lists both.",llama-3.3-70b-instruct,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

def load_and_filter_dataset():
    # Load the heart.csv dataset
    heart_data = pd.read_csv('heart.csv')

    # Filter the dataset
    filtered_data = heart_data[(heart_data['Age'] > 50) & (heart_data['HeartDisease'] == 1)]

    # Select and display specific columns
    selected_columns = filtered_data[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]
    return selected_columns

# Call the function and display the result
filtered_dataset = load_and_filter_dataset()...",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, filters the DataFrame based on the specified conditions (`Age > 50` AND `HeartDisease == 1`), and then selects and displays the `Age`, `Sex`, `ChestPainType`, and `Cholesterol` columns. The imports are correct, and the code structure is clean and functional.

Minor improvements could include:
1. Adding a `try-except` block for `FileNotFoundError` when reading the CSV, which would make the code more robust against missing files.
2. The function `load_and_filter_dataset` directly returns the filtered dataset, which is good. The `print(selected_columns)` was moved outside the function, which adheres to the principle of separating concerns (data processing vs. display).
3. For a more formal function signature (similar to the ground truth), specifying the `file_path` as an argument with a default value would be a good practice for reusability.

Even without these minor improvements, the core task requirements are met accurately and efficiently.",llama-3.3-70b-instruct,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

def analyze_wine_quality():
    # Load the dataset
    wine_quality_df = pd.read_csv('winequality-red.csv')

    # Filter wines based on conditions
    filtered_wines = wine_quality_df[(wine_quality_df['alcohol'] > 12) & (wine_quality_df['pH'] < 3.3)]

    # Find the wine with the highest 'quality' score in the filtered subset
    if not filtered_wines.empty:
        best_wine = filtered_wines.loc[filtered_wines['quality'].idxmax()]
        print(""Wine with the hig...",9,"The generated code correctly solves the task by applying the compound filtering conditions, identifying the wine with the highest quality in the filtered subset, and displaying its features. The code quality is good, with proper imports and a clear function structure. It handles the edge case where no wines match the criteria.

Strong points:
- Correctly implements both filtering conditions with `&`.
- Uses `idxmax()` to efficiently find the highest quality wine.
- Handles the `filtered_wines.empty` case.
- Concisely displays the best wine's features.

Areas for minor improvement (not critical for functionality but for robustness/best practices):
- The Ground Truth explicitly uses `DATASET_PATH` and passes it as an argument, which makes the function more reusable if the path changes or if it's part of a larger system. The generated code hardcodes the path inside the function.
- The Ground Truth includes extensive error handling (e.g., `FileNotFoundError`, `KeyError`), which is good practice for real-world applications. The generated code does not include this.
- The Ground Truth adds print statements to inform the user about the steps (e.g., dataset loaded, number of filtered wines), which can improve user experience.

Overall, it's a very good solution that directly addresses the prompt's requirements.",llama-3.3-70b-instruct,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the heart.csv dataset
heart_data = pd.read_csv('heart.csv')

# Generate a histogram to visualize the distribution of the Age column
plt.figure(figsize=(8, 6))
plt.hist(heart_data['Age'], bins=10, edgecolor='black')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Generate a bar chart to show the distribution of the Sex column
sex_counts = heart_data['Sex'].value_counts()
plt.figure(figsi...",7,"The generated code correctly loads the dataset and generates both requested plots (histogram for Age, bar chart for Sex). Both plots have titles and labeled axes, fulfilling the core requirements.

However, there are several areas for improvement:
- **Plotting Library Choice for Bar Chart:** While `pandas.plot(kind='bar')` works, directly using `matplotlib.pyplot` or `seaborn` for the bar chart would be more explicit and consistent with general plotting practices. It also misses the opportunity to map numerical `Sex` values (0, 1) to descriptive labels (Female, Male) which is common for this dataset and enhances readability, as done in the ground truth.
- **Error Handling:** The generated code lacks any error handling for `FileNotFoundError` or `KeyError`, which is crucial for robust data science scripts.
- **Vizualization Enhancement:** The ground truth uses Seaborn for a more visually appealing histogram with KDE and better default aesthetics. While not strictly required, it's good practice. Also, the bar chart could benefit from remapping the 'Sex' column to 'Male'/'Female' for clarity, as done in the ground truth. The bar chart's x-axis labels are numbers (0, 1) rather than human-readable categories.
- **Code Style/Readability:** While acceptable, adding comments for distinct sections would improve readability, especially for more complex tasks.
- **Details:** The ground truth code includes `plt.tight_layout()` and `plt.grid()`, which are minor but contribute to better plot presentation. It also specifies `fontsize` for titles and labels, which is a nice touch.",llama-3.3-70b-instruct,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
data = pd.read_csv('spotify_2023.csv')

# List all column names and identify the data type of each
print(""Column Names and Data Types:"")
print(data.dtypes)

# Convert the `streams` column to a numeric data type, handling potential errors
data['streams'] = pd.to_numeric(data['streams'], errors='coerce')

# Report any NaNs in the `streams` column
print(""\nNumber of NaNs in 'streams' column:"", data['streams'].isnull().sum())

# Define numerical colu...",9,"The generated code correctly addresses almost all requirements. It loads the dataset, lists column types, correctly converts the 'streams' column to numeric with error handling, reports NaNs, calculates descriptive statistics for the specified numerical columns, and counts unique artists.

Strong points:
- Correct use of `pd.read_csv`, `df.dtypes`, `pd.to_numeric(errors='coerce')`, `df.isnull().sum()`, `df.describe()`, and `df.nunique()`.
- Concise and readable.

Areas for minor improvement:
- **Error Handling for File Loading:** The generated code lacks a `try-except` block for `FileNotFoundError` when loading the CSV, which is present in the ground truth. This is a crucial robustness improvement for real-world scenarios.
- **Specific Statistics for `describe()`:** The ground truth specifically asked for mean, median, std, min, and max, and the ground truth code used `.loc[['mean', '50%', 'std', 'min', 'max']]` to achieve this. The generated code uses `describe()` which reports more than just these (e.g., count, 25%, 75%). While `describe()` includes the requested stats, the output is not as precise to the request.
- **Reporting NaNs:** The generated code only reports the count of NaNs. The ground truth also showed the original and new dtypes and a sample of rows with NaNs, which provides more comprehensive insight.
- **Clarity of Output:** The ground truth includes more descriptive print statements for each step, making the output easier to follow for someone who might not be intimately familiar with the code's objective. For example, ""--- Column Names and Data Types ---"" vs. just ""Column Names and Data Types:"".

Overall, it's a very good attempt that fulfills most of the core requirements effectively. The missing file error handling is the most significant omission from a robustness perspective.",llama-3.3-70b-instruct,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd
import numpy as np

def load_and_filter_spotify_data():
    # Load the dataset
    spotify_data = pd.read_csv('spotify_2023.csv')

    # Filter the dataset to include only songs where released_year is 2023
    filtered_data = spotify_data[spotify_data['released_year'] == 2023]

    # Select the required columns and convert streams to numeric
    required_columns = filtered_data[['track_name', 'artist(s)_name', 'streams']].copy()
    required_columns['streams'] = pd....",9,"The generated code correctly addresses all requirements of the task: loading the CSV, filtering by `released_year` == 2023, selecting the specified columns, ensuring `streams` is numeric and coercing errors, and sorting by `streams` in descending order. The use of a function `load_and_filter_spotify_data` and an `if __name__ == ""__main__"":` block is good practice for modularity, although not strictly required by the prompt.

One minor point for improvement, which is why it's not a perfect 10, is the explicit `dropna(subset=['streams'])`. While this is often desired for sorting by streams, the prompt only asks to ""Sort the resulting data by the `streams` column in descending order,"" implying that NaNs resulting from coercion should be handled by `sort_values` default behavior (which places NaNs at the end). The ground truth code also does this `dropna`, but it explicitly comments on it. The generated code performs it without comment. Removing rows with `NaN` in streams means that some valid rows that couldn't convert their stream value (e.g., due to ""2M"" or similar strings) are completely removed, rather than just being placed at the end of the sorted list. This can be a subtle difference depending on interpretation. However, given data science best practices, removing bad data is often preferred.

The inclusion of `import numpy as np` is unnecessary as `numpy` functions are not directly used in the code. This is a minor code quality point.

Overall, the code is very clean, readable, and effectively solves the problem.",llama-3.3-70b-instruct,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Create a histogram for the bpm column
plt.figure(figsize=(10, 6))
plt.hist(df['bpm'], bins=20, edgecolor='black')
plt.title('Distribution of Song BPM')
plt.xlabel('Beats Per Minute (BPM)')
plt.ylabel('Frequency')
plt.show()

# Create a scatter plot for danceability and energy
plt.figure(figsize=(10, 6))
plt.scatter(df['danceability_%'], df['energy_%'])
plt.title('Relationship...",9,"The generated code correctly loads the CSV, creates both the histogram for `bpm` and the scatter plot for `danceability_%` vs `energy_%`. Both plots have appropriate titles and axis labels, and required imports are present. The code is clean and directly addresses all requirements.

One minor improvement point for the scatter plot could be to use a title like ""Relationship between Danceability and Energy (%)"" to explicitly mention the units on both sides of the relationship, as the x-axis and y-axis labels already include the percentage. This is a very minor aesthetic point and doesn't detract significantly from the correctness or completeness.

Another small thing is that `plt.show()` is called twice, which is fine, but typically you might call it once at the very end after all plots have been defined if you want them to appear sequentially after the script finishes. However, calling `plt.show()` after each `plt.figure()` ensures each plot appears in its own window or output block, which is also a valid approach. It doesn't negatively impact functionality for this specific task.",llama-3.3-70b-instruct,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# List all column names and their inferred data types
print(""Column Names and Data Types:"")
print(df.dtypes)

# Clean the `Video views`, `Likes`, and `Dislikes` columns
df['Video views'] = df['Video views'].str.replace(',', '').astype(float, errors='coerce')
df['Likes'] = df['Likes'].str.replace(',', '').astype(float, errors='coerce')
df['Dislikes'] = df['Dislikes'].str.replace(',', ''...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Loading Data:** Successfully loads the CSV.
- **Column Info:** Correctly prints column names and their data types using `df.dtypes`.
- **Numeric Cleaning:** Implements the cleaning for `Video views`, `Likes`, and `Dislikes` by removing commas, coercing to float, and filling NaNs with 0. The use of `.str.replace(',', '')` followed by `.astype(float, errors='coerce')` is an efficient way to achieve this.
- **Descriptive Statistics:** Correctly calculates and prints descriptive statistics using `describe()`.
- **Category Cleaning:** Fills missing `Category` values with 'Unknown'.
- **Unique Category Count:** Reports the count of unique values in the `Category` column.
- **Imports:** Includes necessary imports (`pandas`).

**Areas for Improvement (Why it's not a 10):**
- **Error Handling for File Loading:** The code assumes the `youtube_trendings.csv` file will always be present. A `try-except FileNotFoundError` block would make it more robust, as in the ground truth.
- **Robustness for Column Existence:** The code directly accesses columns like `df['Video views']`, `df['Likes']`, `df['Dislikes']`, and `df['Category']` without checking if they actually exist in the DataFrame. If a column were misspelled or missing from the dataset, it would raise a `KeyError`. The ground truth includes checks for column existence before processing.
- **Output Format for Descriptive Statistics:** While `describe()` is correct, the ground truth explicitly prints mean, median, std, min, and max individually, which was requested (""Calculate and report basic descriptive statistics (mean, median, std, min, max)""). `describe()` provides all these, but `df.info()` from the ground truth is more comprehensive for listing column names and types than `df.dtypes`.

Overall, the code is highly functional and correctly implements the core data manipulation and reporting tasks. The missing error handling and column existence checks are minor robustness issues that prevent a perfect score.",llama-3.3-70b-instruct,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

def load_and_process_data(file_name):
    # Load the dataset
    data = pd.read_csv(file_name)

    # Clean the 'Video views' column
    data['Video views'] = data['Video views'].str.replace(',', '').astype(float, errors='coerce').fillna(0)

    # Filter the dataset
    filtered_data = data[data['Video views'] > 50000000]

    # Select and display the required columns
    result = filtered_data[['Video', 'Video views']]

    # Sort the data in descending order
    ...",9,"The generated code correctly implements all the requirements of the task. It properly loads the CSV, cleans the `Video views` column (removing commas, converting to numeric, handling errors by coercing to NaN, and filling NaNs with 0), filters the data, selects the specified columns, and sorts by views in descending order.

**Strong points:**
- All steps are correctly implemented.
- The cleaning of 'Video views' is concise and effective using method chaining (`str.replace`, `astype(float, errors='coerce')`, `fillna(0)`). This is a good and idiomatic pandas approach.
- The code is encapsulated within a function, which promotes reusability.
- Clear variable names are used.
- Includes `if __name__ == ""__main__"":` block for example usage, which is good practice.

**Minor Improvement:**
- While the handling of `astype(float, errors='coerce')` is correct, the ground truth uses `pd.to_numeric` which is often preferred for more robust numeric conversion in pandas, especially when dealing with mixed types or potential non-string numeric representations. In this specific case, both achieve the same result because `astype(float, errors='coerce')` implicitly converts to string before attempting float conversion if the column isn't already numeric or string-like. However, `pd.to_numeric` is generally more explicit and flexible.
- The ground truth includes `try-except FileNotFoundError` which is good for production-level code, though not explicitly required by the prompt's `assume the CSV file is in the current directory` clause. The generated code does not include this, which is acceptable given the prompt, but it's a difference to note.

Overall, excellent solution.",llama-3.3-70b-instruct,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

def clean_and_plot_data():
    # Load the dataset
    youtube_trends = pd.read_csv('youtube_trendings.csv')

    # Clean the 'Likes' and 'Video views' columns
    youtube_trends['Likes'] = pd.to_numeric(youtube_trends['Likes'].str.replace(',', ''), errors='coerce')
    youtube_trends['Video views'] = pd.to_numeric(youtube_trends['Video views'].str.replace(',', ''), errors='coerce')

    # Fill NaN values with 0
    youtube_trends['Li...",9,"The generated code correctly loads the dataset, cleans the specified columns by removing commas, converting to numeric, and filling NaNs with 0. It also generates a scatter plot with the correct title and axis labels. The use of `pd.to_numeric` with `errors='coerce'` and `replace` with `fillna(0)` is exact for the cleaning requirement. The plotting details meet all specifications.

One minor point of improvement, though not explicitly required by the prompt, would be to add a `try-except` block for file loading, similar to the ground truth, to handle `FileNotFoundError` gracefully. Also, while the generated code works, the ground truth's `astype(str)` before `replace` for column cleaning is a slightly more robust approach to handle cases where a column might already be numeric or mixed before the cleaning, although for CSV data, it's often read as strings initially. However, the generated code's approach is perfectly valid and common when the column is primarily read as an object/string type containing the numbers and commas.",llama-3.3-70b-instruct,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the data
def load_and_clean_data(file_name):
    df = pd.read_csv(file_name)

    # Clean the columns
    df['Mobile Weight'] = df['Mobile Weight'].apply(lambda x: pd.to_numeric(x.replace('g', ''), errors='coerce'))
    df['RAM'] = df['RAM'].apply(lambda x: pd.to_numeric(x.replace('GB', ''), errors='coerce'))
    df['Battery Capacity'] = df['Battery Capacity'].apply(lambda x: pd.to_numeric(x.replace('mAh', ''), errors='coerce'))
    df['La...",7,"The generated code correctly addresses most of the task requirements but has a few minor issues and areas for improvement.

**Strong Points:**
- **Correctness in Cleaning:** The cleaning logic for all specified columns (`Mobile Weight`, `RAM`, `Battery Capacity`, `Launched Price (USA)`) correctly uses `replace` and `pd.to_numeric(errors='coerce')`, which is efficient and handles invalid values properly.
- **Proper Imports:** `pandas` and `numpy` are imported correctly.
- **Task Coverage:** It loads data, cleans the specified columns, reports data types, calculates descriptive statistics, and counts unique company names.
- **Good Use of Functions:** The code is modularized into meaningful functions (`load_and_clean_data`, `report_column_info`, `report_descriptive_statistics`, `report_unique_company_names`, `main`).

**Areas for Improvement:**
- **File Name Typo:** The `file_name` in `main()` is 'obiles_2025.csv' instead of 'mobiles_2025.csv'. This is a critical logical error that would cause a `FileNotFoundError` if the file doesn't exist under the wrong name.
- **Missing `df.dtypes` in `report_descriptive_statistics`:** The task specifically asked for `mean, median, std, min, max`. While `df.describe()` includes these, the generated code uses `describe()` which also outputs 'count' and '25%, 50%, 75% percentiles' which are not explicitly requested. The ground truth code correctly filtered `describe()` to only the requested statistics, or one could compute them individually.
- **Handling of `astype(str)`:** The generated code directly applies `replace` on the column series without first converting to `astype(str)`. While `apply(lambda x: str(x).replace(...))` would implicitly handle non-string types, directly using `Series.str.replace` after `astype(str)` is generally safer and more explicit for string operations on mixed types. The ground truth code demonstrates this better practice. The current approach might fail if a column contained non-string types (e.g., float NaNs) that don't have a `.replace` method or don't implicitly convert to string in `apply`.
- **Error Handling for File Loading:** The code lacks explicit `try-except` blocks for `pd.read_csv`. If the file is not found, it will raise an unhandled `FileNotFoundError`, which is less robust than the ground truth's approach.
- **Redundant `numpy` import:** `numpy` is imported but not explicitly used in the provided code. While generally harmless, it's good practice to only import what's necessary.",llama-3.3-70b-instruct,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the Launched Price (India) column
df['Launched Price (India)'] = df['Launched Price (India)'].apply(lambda x: x.replace('INR ', '').replace(',', ''))
df['Launched Price (India)'] = pd.to_numeric(df['Launched Price (India)'], errors='coerce')

# Clean the RAM column
df['RAM'] = df['RAM'].apply(lambda x: x.replace('GB', ''))
df['RAM'] = pd.to_numeric(df['RAM'], errors='coerce')

# Fil...",9,"The generated code is highly correct and complete, successfully addressing all requirements of the task. It loads the CSV, cleans both the 'Launched Price (India)' and 'RAM' columns correctly by removing specified strings and converting to numeric with error handling. It then accurately filters for 'Apple' phones under 100,000 INR, selects the requested columns, and sorts the final output. The use of `apply(lambda x: x.replace(...))` is a valid and concise way to perform the string cleaning. The `pd.to_numeric` with `errors='coerce'` handles NaNs correctly. The filtering and sorting logic is also perfectly implemented.

One minor area for improvement, which doesn't detract significantly from the score but is good practice, is to explicitly handle potential `FileNotFoundError` when loading the CSV, as demonstrated in the ground truth. While not strictly required by the prompt, robust code often includes this. Also, for the `Launched Price (India)` cleaning, if the column contains non-string types *before* the `.apply()` method, it could lead to errors. Converting to `str` first (e.g., `df['Launched Price (India)'].astype(str).apply(...)`) would make it more robust, though in typical CSV contexts, string-like data is common. However, for this specific task and the given data, the current approach is perfectly fine. The `np` import is unused, but harmless. Overall, an excellent solution.",llama-3.3-70b-instruct,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the RAM column
df['RAM'] = df['RAM'].str.replace('GB', '').astype(float, errors='coerce')

# Clean the Battery Capacity column
df['Battery Capacity'] = df['Battery Capacity'].str.replace('mAh', '').astype(float, errors='coerce')

# Create a scatter plot
plt.figure(figsize=(10, 6))
for company in df['Company Name'].unique():
    company_df = df[df['Company Name'] == comp...",7,"The generated code successfully loads the data, cleans the specified columns, and generates a scatter plot with proper labels and a title. It correctly handles the conversion errors to NaN.

However, there are a few areas for improvement, primarily regarding the plotting aspect:
1.  **Plotting method**: The generated code uses a loop with `plt.scatter` to plot each company separately. While functionally correct, this is less efficient and less idiomatic in libraries like Matplotlib/Seaborn for coloring by a categorical variable. `seaborn.scatterplot` with the `hue` parameter (as shown in the ground truth) is the more standard and often preferred approach for this type of visualization, as it handles coloring and legend generation automatically and more elegantly.
2.  **Handling of `astype(float, errors='coerce')`**: While `astype(float, errors='coerce')` works for direct conversion, it implicitly converts the series to `object` type first if mixed types are present, then tries to convert to float. Using `.str.replace()` followed by `pd.to_numeric(..., errors='coerce')` is generally more robust and explicit for string cleaning and conversion from a mixed-type column, especially if the original column isn't guaranteed to be entirely strings or numeric strings after cleaning. In this specific case, `pd.Series.str.replace` followed by `Series.astype(float, errors='coerce')` works because `str.replace` returns a Series of strings.
3.  **Missing `seaborn` import**: Although the generated code doesn't use seaborn, the prompt asked to ""Include proper imports (pandas, matplotlib.pyplot or seaborn)"". The ground truth demonstrated a better approach using seaborn for the `hue` functionality.

Overall, the code is functional and addresses most requirements, but the plotting method could be improved for conciseness and best practice.",llama-3.3-70b-instruct,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd

# Load the dataset and display the first 5 rows
df = pd.read_csv('titanic.csv')
print(""First 5 Rows of the Dataset:"")
print(df.head())

# Show the shape of the dataset (rows and columns)
print(""\nShape of the Dataset:"")
print(df.shape)

# List all column names and their data types
print(""\nColumn Names and Data Types:"")
print(df.dtypes)

# Calculate the survival rate (percentage of passengers who survived)
survival_rate = (df['Survived'].sum() / len(df)) * 100
prin...",9,"The generated code is very good and correctly addresses almost all requirements of the task.

**Strong Points:**
*   **Correctness:** All tasks are performed correctly: loading data, displaying head, showing shape, calculating survival rate, and counting missing values.
*   **Completeness:** It covers all the specified requirements.
*   **Code Quality:** The code is clean, readable, and uses appropriate pandas methods.

**Areas for Improvement (minor points):**
*   **Error Handling:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is robust programming practice. The generated code assumes the file exists, which is acceptable for a direct task but less robust for production.
*   **Data Type Display (`df.info()` vs `df.dtypes`):** While `df.dtypes` correctly lists column names and their data types, `df.info()` from the ground truth provides a more comprehensive summary, including non-null counts and memory usage, which is generally preferred for ""exploratory data analysis"" as it gives more immediate insights into missing values and memory footprint. The task specifically asked for ""column names and their data types,"" and both fulfill this, but `df.info()` is more informative.

These are minor points for an ""expert code evaluator"" perspective. For the given task, the generated code is highly effective and accurate.",llama-3.3-70b-instruct,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

def load_imdb_dataset(file_name):
    try:
        # Load the dataset
        imdb_df = pd.read_csv(file_name)

        # List all column names and their data types
        print(""Column Names and Data Types:"")
        print(imdb_df.dtypes)

        # Calculate and report basic descriptive statistics for numerical columns
        numerical_cols = ['Year', 'Rating', 'Runtime']
        print(""\nDescriptive Statistics for Numerical Columns:"")
        print(imdb_df[num...",6,"The generated code addresses most of the requirements but has significant issues in the descriptive statistics calculation for `Runtime` and lacks the specific descriptive statistics (mean, median, std, min, max) requested.

**Strengths:**
*   Correctly loads the CSV file.
*   Correctly identifies and prints column names and their data types (using `df.dtypes`).
*   Correctly reports the total number of movies.
*   Includes basic error handling for `FileNotFoundError`.

**Weaknesses and Areas for Improvement:**
1.  **Descriptive Statistics for `Runtime`:** The prompt explicitly mentions calculating statistics for ""Year, Rating, and Runtime"". The 'Runtime' column in the `imdb_top_250.csv` dataset is typically in a string format (e.g., '142 min'). The generated code attempts to call `.describe()` directly on `imdb_df[numerical_cols]` without any preprocessing. This will likely cause an error or produce meaningless output for 'Runtime' if it's an object/string column, or it might not even include it in describe's numerical output if it's not converted. The ground truth explicitly handles this conversion, which is crucial for this dataset.
2.  **Specific Descriptive Statistics:** The prompt asked for ""mean, median, standard deviation, min, max"". The `df.describe()` method outputs a much broader set of statistics (count, mean, std, min, 25%, 50%, 75%, max). While it *includes* the requested ones, it doesn't *report only* them, and specifically doesn't label '50%' as 'median' as requested for clarity. The ground truth uses `.loc` and `rename` to precisely select and rename these statistics.
3.  **Missing `sys` import and `exit()`:** The ground truth includes `sys.exit(1)` for critical errors like `FileNotFoundError`, which is good practice for a script that processes data to ensure it terminates gracefully and indicates failure. The generated code prints an error but continues execution (though subsequent lines would likely fail without the DataFrame anyway).
4.  **Redundant function for a single script run:** While wrapping the code in a function `load_imdb_dataset` is not wrong, for a simple script that's meant to run once to perform specific actions, it's not strictly necessary and sometimes adds an extra layer of indirection for this type of benchmark task. The prompt asked for ""Python code"" implying the core logic.
5.  **`pd.errors.EmptyDataError` and `pd.errors.ParserError`:** While good to have error handling, for this specific task and dataset, `FileNotFoundError` is the most common and critical. Adding more specific `pd.errors` is generally good for robust applications but might be slightly over-engineering for a simple benchmark script unless those errors are expected.",llama-3.3-70b-instruct,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

def load_and_filter_imdb_data():
    # Load the imdb_top_250.csv dataset
    imdb_data = pd.read_csv('imdb_top_250.csv')

    # Filter the dataset to include only movies with a Rating greater than or equal to 9.0
    filtered_imdb_data = imdb_data[imdb_data['Rating'] >= 9.0]

    # Select and display the Title and Rating columns
    selected_imdb_data = filtered_imdb_data[['Title', 'Rating']]

    # Sort the resulting data by Rating in descending order
    sorted_i...",9,"The generated code correctly implements all the requirements of the task: loading the CSV, filtering by rating, selecting specified columns, and sorting in descending order. The imports are proper, and the code structure using a function is good practice.

One minor area for improvement is the lack of error handling (e.g., `FileNotFoundError`, `KeyError`) which is present in the ground truth. While not explicitly asked for, robust code for data loading typically includes this. However, given the prompt's focus on the core data manipulation steps, its absence doesn't severely detract from its correctness or completeness regarding the primary task. The use of a `main` guard `if __name__ == ""__main__"":` is also a good practice.",llama-3.3-70b-instruct,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Histogram for Rating distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['Rating'], kde=True, color='skyblue')
plt.title('Distribution of IMDB Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# Scatter plot for Rating vs Runtime
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Runtime', y='Rating', data=df, color='darkgreen')
plt.tit...",6,"The generated code correctly performs the basic plotting tasks. It loads the CSV, creates a histogram for 'Rating', and a scatter plot for 'Rating' vs 'Runtime', with appropriate titles and labels. This addresses the core requirements.

However, there's a significant oversight in handling the 'Runtime' column. Real-world datasets often have 'Runtime' as a string (e.g., '120 min'), not a clean numeric type. The generated code directly uses `df['Runtime']` for the scatter plot, which would likely lead to an error or incorrect plot if the 'Runtime' column in the `imdb_top_250.csv` dataset is not already in a purely numeric format. The ground truth code includes robust preprocessing to extract numeric values from the 'Runtime' string, handle potential errors, and drop invalid data points, which is crucial for production-ready code.

Another minor point is the lack of error handling for `pd.read_csv` if the file is not found. While not explicitly asked in the prompt, it's good practice.

**Areas for improvement to achieve a higher score:**
*   Implement robust data preprocessing for the `Runtime` column to convert it to a numeric type, handling common string formats (e.g., '120 min') and potential non-numeric entries.
*   Consider adding basic error handling for file loading (e.g., `try-except FileNotFoundError`).",llama-3.3-70b-instruct,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

def load_and_analyze_penguins_data():
    # Load the dataset
    penguins_data = pd.read_csv('penguins.csv')

    # Report the number of missing values (NA) for each column
    print(""Missing Values Count:"")
    print(penguins_data.isnull().sum())

    # Drop rows with missing values in specific numerical columns
    numerical_columns = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
    penguins_data_numerical = penguins_data.dropna(sub...",7,"The generated code addresses most of the task requirements correctly. It successfully loads the data, reports missing values, correctly filters the numerical columns, and calculates unique counts.

However, there are specific areas for improvement:
- **Descriptive Statistics Detail:** While `describe()` provides all the requested statistics (mean, median, std, min, max), the prompt specifically asked to ""calculate and report basic descriptive statistics (mean, median, standard deviation, min, max)"". The Ground Truth uses `agg()` which explicitly lists these and directly shows them, whereas `describe()` shows more than just these (e.g., count, 25%, 50%, 75% percentiles). This is a minor deviation but worth noting for strict adherence to the prompt.
- **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and other potential issues when loading the CSV. The generated code lacks this, making it less resilient to missing files.
- **Reporting Specificity for Missing Values:** The prompt asked to ""Report the number of missing values (NA) for each column."" While `isnull().sum()` does this, the ground truth filters to show only columns with missing values and adds a message if none are found, which is a more complete and user-friendly report. The current output simply lists all columns, including those with 0 missing values.
- **Clarity of Output:** The output formatting could be improved for better readability (e.g., adding more descriptive print statements as seen in the ground truth).

Overall, the core logic is sound, but the code could be more robust and offer more precise reporting based on the prompt's specific phrasing.",llama-3.3-70b-instruct,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

def load_and_filter_penguins():
    # Load the penguins.csv dataset
    penguins = pd.read_csv('penguins.csv')

    # Filter the dataset to include only records where the `species` column is 'Adelie' AND the `island` column is 'Torgersen'
    adelie_torgersen_penguins = penguins[(penguins['species'] == 'Adelie') & (penguins['island'] == 'Torgersen')]

    # Select only the `culmen_length_mm` and `culmen_depth_mm` columns
    culmen_measurements = adelie_torgersen_p...",9,"The generated code correctly implements all specified requirements: loading the CSV, filtering by species and island, selecting the specified columns, dropping rows with missing values in those columns, and displaying the first 5 rows. The code is well-structured and uses appropriate pandas operations.

A minor improvement for robustness, as seen in the ground truth, would be to include error handling for `FileNotFoundError` when attempting to load the CSV. Additionally, while `dropna()` without a `subset` argument works here because the DataFrame `culmen_measurements` only contains the two columns of interest, explicitly providing `subset=['culmen_length_mm', 'culmen_depth_mm']` to `dropna()` is good practice for clarity and robustness, especially if the DataFrame unexpectedly gained other columns during prior operations. However, for this specific task, the current implementation is effectively correct. The use of a function is also a good practice for encapsulating the logic.",llama-3.3-70b-instruct,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the penguins dataset
penguins = pd.read_csv('penguins.csv')

# Ensure only non-missing values are used
penguins_filtered = penguins.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])

# Create a scatter plot with seaborn
plt.figure(figsize=(10, 8))
sns.scatterplot(data=penguins_filtered, x='culmen_length_mm', y='culmen_depth_mm', hue='species')

# Set title and labels
plt.title('Culmen Leng...",9,"The generated code is largely correct and complete. It successfully loads the data, filters missing values as required, and creates the scatter plot with the correct axes, coloring, title, and labels. The use of seaborn simplifies the plotting process, which is good.

Strong points:
- Correctly loads the CSV using pandas.
- Correctly filters out rows where `culmen_length_mm`, `culmen_depth_mm`, OR `species` are missing using `dropna(subset=...)`.
- Generates a scatter plot using `seaborn.scatterplot`.
- Correctly maps `culmen_length_mm` to x-axis, `culmen_depth_mm` to y-axis.
- Correctly colors points by `species` using the `hue` parameter.
- Includes a title and correct x and y axis labels.
- Includes proper imports (`pandas`, `matplotlib.pyplot`, `seaborn`).

Areas for minor improvement (not critical enough to significantly reduce score):
- The ground truth includes error handling for `FileNotFoundError` and other general exceptions, which is good practice for robust code but was not explicitly requested or penalized for absence.
- The ground truth encapsulates the logic within a function, which makes the code more reusable and testable. While not explicitly asked for, it typically indicates higher code quality for larger projects. For this specific task, a script-like approach is acceptable.
- The ground truth also has a `if __name__ == '__main__'` block, which is also good practice for scripts.",llama-3.3-70b-instruct,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd
import numpy as np

# Read the dataset
def load_and_clean_penguins_dataset(file_path):
    try:
        penguins = pd.read_csv(file_path)
        return penguins
    except FileNotFoundError:
        print(""The file does not exist"")
        return None
    except pd.errors.EmptyDataError:
        print(""The file is empty"")
        return None
    except pd.errors.ParserError:
        print(""Error parsing the file"")
        return None

# Fill NA values with the mean...",9,"The generated code correctly addresses all parts of the task: reading the dataset, filling NAs in the specified columns with their means, calculating and printing the required statistics (mean, median, std dev), and printing the dataset's dimensions. The code is well-structured using functions, which enhances readability and reusability. It also includes basic error handling for file loading, although the `FileNotFoundError` handling could exit more gracefully or ensure no further operations are attempted on a `None` DataFrame. The variable names are descriptive, and the use of f-strings for output is good practice. The output format is slightly less detailed than the ground truth in terms of reporting the NA imputation process, but it fulfills the main requirements. A minor improvement could be to format the statistical outputs to two decimal places for better readability, similar to the ground truth. Overall, a very good solution.",llama-3.3-70b-instruct,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load dataset: {e}"")

# Load and preprocess the dataset
def preprocess_data(data):
    data.dropna(subset=['flipper_length_mm'], inplace=True)
    return data

# Create a histogram of flipper lengths
def plot_flipper_length(data):
    plt.figure(fi...",8,"The generated code correctly addresses all the core requirements of the task: loading data, handling NAs in `flipper_length_mm`, and generating both a histogram and a bar chart with appropriate titles and labels.

**Strong Points:**
- **Modularity:** The code is well-structured into functions (`load_dataset`, `preprocess_data`, `plot_flipper_length`, `plot_penguin_counts`, `main`), which improves readability, reusability, and maintainability.
- **Correctness:** It correctly loads the CSV, drops NA values from the specified column, and generates the requested plots.
- **Labels and Titles:** All plots have correct titles and axis labels as requested.
- **Use of libraries:** Uses `pandas` for data manipulation and `matplotlib.pyplot` and `seaborn` for plotting effectively.

**Areas for Improvement (to reach a higher score):**
- **Error Handling (Loading Data):** While `load_dataset` has a `try-except` block, it only prints an error message and returns `None`. The `main` function checks for `None`, but for a real-world scenario, a more robust exit or error propagation mechanism might be desired (e.g., `sys.exit()` as in the ground truth). The ground truth also explicitly checks for `FileNotFoundError`, which is more specific and helpful.
- **Informative Printing:** The ground truth includes print statements about the success of loading, number of rows dropped, and final shape, which are very helpful for debugging and understanding the data transformation process. The generated code lacks such informative outputs.
- **Plot Enhancements:**
    - The ground truth includes `plt.figure(figsize=...)` for both plots, which is good practice for consistent and readable plots. The generated code adds it, which is good.
    - Setting a specific seaborn style (`sns.set_theme`) as in the ground truth can make plots look more professional and consistent.
    - Ground truth adds `kde=True` to the histogram, which is also included in the generated code's `sns.histplot` call. It also added `bins=20` which is a good practice.
    - Specific modifications like `fontsize` for titles and labels, `xticks`/`yticks` font sizes, `grid` lines, and `plt.tight_layout()` (present in ground truth but not generated) improve the visual appeal and readability significantly.
- **Redundancy in `plot_penguin_counts`:** The line `island_counts = data['island'].value_counts()` is not strictly necessary as `sns.countplot` handles the counting internally. While harmless, it's slightly redundant. Also, the `order` parameter in `sns.countplot` (used in ground truth) can be useful for sorting bars by count.

Overall, the generated code is a solid solution that correctly performs the required tasks in a clean and modular way. The suggestions above are mostly about adding robustness, more informative output, and better visual aesthetics to the plots, which are common best practices in data visualization and analysis.",llama-3.3-70b-instruct,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
# Import necessary libraries
import pandas as pd

def analyze_penguins():
    # Load the dataset
    try:
        penguins_df = pd.read_csv('penguins.csv')
    except FileNotFoundError:
        print(""The 'penguins.csv' file was not found. Please check the file path."")
        return

    # Filter the data to include only penguins from the 'Torgersen' island
    torgersen_penguins_df = penguins_df[penguins_df['island'] == 'Torgersen']

    # Count and print the number of penguins belon...",9,"The generated code correctly addresses all requirements of the task: loading the dataset, filtering for 'Torgersen' island, and counting species. It handles the `FileNotFoundError` robustly and checks for an empty filtered DataFrame, which is good practice. The code is well-structured within a function and includes a `if __name__ == ""__main__"":` block, demonstrating good Python conventions.

Areas for minor improvement (not critical enough to significantly lower the score):
- The ground truth includes checks for the existence of 'island' and 'species' columns, which makes the code more robust against malformed or unexpected CSV files. While the prompt assumes a well-formed CSV, this adds defensive programming.
- The ground truth explicitly prints success messages for loading and filtering, which can be helpful for debugging or user feedback in a larger application.
- The `try-except` block for `FileNotFoundError` in the generated code is good, but does not catch general `Exception` during `read_csv` as the ground truth does, which could handle other potential issues during loading (e.g., parsing errors).",llama-3.3-70b-instruct,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

# Create the DataFrame
df = pd.DataFrame(data...",10,"The generated code is excellent. It correctly defines the DataFrame, prints its shape, and lists the data types of each column as required by the task. The code is clean, well-structured, and directly addresses all parts of the prompt. No improvements are necessary for this specific task.",llama-3.3-70b-instruct,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
def load_data():
    """"""Load the titanic.csv dataset""""""
    try:
        data = pd.read_csv('titanic.csv')
        return data
    except FileNotFoundError:
        print(""The file titanic.csv was not found."")
        return None

# Create visualizations
def create_visualizations(data):
    """"""Create visualizations for the titanic.csv dataset""""""
    # Bar chart for survivors vs non-survivors
    plt.figure(figsize=...",7,"The generated code correctly creates all four requested visualizations with proper titles and axis labels. It handles missing age values by dropping them, which is one valid approach (though filling with median or mean is often preferred). The code structure using functions for loading data and creating visualizations is good.

Areas for improvement:
1.  **Handling of Missing Age Values:** While `dropna()` works, it discards entire rows, potentially losing other useful data. The ground truth's approach of filling with the median is generally more robust for age distribution unless the intent is strictly to analyze complete age records. For a benchmark ""expert"" solution, a more sophisticated imputation (like median) is expected.
2.  **Readability/Clarity of Plots:**
    *   For the 'Survived' plot, replacing 0 and 1 with 'Non-survivor' and 'Survivor' labels on the x-axis would significantly improve readability, as seen in the ground truth.
    *   The `plt.show()` calls after each plot immediately display and close the plot. If multiple figures are created, it's often better to show all at once or control the flow more precisely, though for basic separate plots, this is acceptable.
    *   No `plt.tight_layout()` or `plt.xticks(rotation=0)` calls, which can sometimes lead to overlapping labels, especially with more complex data.
    *   No grid lines were added, which can sometimes make plots easier to read.
3.  **Error Handling:** The `FileNotFoundError` handling is basic; it prints a message and returns None. The ground truth adds an `exit()` which stops further execution that would otherwise fail on a `None` data frame. While the `if data is not None` check prevents a crash, it doesn't clearly indicate to the user what happened.",llama-3.3-70b-instruct,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create a pandas DataFrame with student performance data
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]
}
df = pd.DataFrame(data)

# Calculate and ...",9,"The generated code correctly addresses all parts of the task. It properly creates the DataFrame, calculates and prints the average midterm and final scores, and correctly identifies the subject with the highest average final score. The code quality is good, with correct imports and clear variable names.

The only minor area for improvement, which is a matter of presentation rather than correctness, is that the print statements for the average scores could be slightly more descriptive, similar to the ground truth (e.g., ""Average Midterm Score for all students""). Also, the ground truth adds the actual highest average score value when printing the subject, which is a nice touch for completeness, though not strictly required by the prompt's wording (""Identify and print the 'Subject'""). Given that the prompt only asks to ""print the 'Subject'"", the generated code fully meets this requirement.",llama-3.3-70b-instruct,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
    'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
    'Volume': [1000000,...",9,"The generated code is highly correct and complete, successfully addressing all requirements of the task. It correctly creates the DataFrame, sets 'Date' as the index, calculates the 'Daily_Range', identifies the date with the highest 'Close' price, and prints the updated DataFrame as requested. The import statement is correct.

One minor suggestion for improvement, though not critical to correctness:
- The print statement for the highest close price could be slightly more aligned with the ground truth's formatting by explicitly calling `.strftime('%Y-%m-%d')` on `max_close_date` for a consistent date format, although pandas automatically handles the date representation when printing the datetime object. The current output is functional and correct. For example, `print(f""The date with the highest close price is: {max_close_date.strftime('%Y-%m-%d')}"")` would match the ground truth's output format more closely.

Overall, an excellent solution.",llama-3.3-70b-instruct,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create a DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', ...",9,"The generated code is highly correct, complete, and of good quality. It successfully creates the DataFrame, calculates the 'Total_Revenue' column, correctly identifies the best-selling product by quantity, and prints the first 5 rows of the updated DataFrame. The logic for finding the best-selling product using `groupby().sum().reset_index().loc[...].idxmax()` is slightly more verbose than simply using `groupby().sum().idxmax()` as in the ground truth, but it achieves the same correct result. The output format for printing the best-selling product is also slightly different (""The best selling product is"" vs ""The product sold the most in terms of Quantity is:"") but both are acceptable. A perfect score would require exact adherence to output strings and the most concise approach.",llama-3.3-70b-instruct,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}

# Create the DataFrame
df = pd.DataFrame(data)

# Calculate and print the average ""SessionDurationMinutes""
average_session_duration =...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It successfully defines the DataFrame, calculates the average session duration, and counts users by device type. The code is clean, utilizes appropriate pandas functions (`.mean()`, `.value_counts()`), and the formatting of the output for the average session duration is good.

The only minor points for improvement, preventing a perfect 10, are stylistic and related to output clarity, which the ground truth code handles slightly better:
1. The ground truth code includes a separator (`print(""-"" * 50)`) between the two output sections, which improves readability, especially for a benchmark.
2. The print statements in the ground truth code are explicitly numbered (e.g., ""1. Average Session Duration:"", ""2. Number of users per Device Type:""), further enhancing clarity for a task with multiple distinct requirements.

These are minor presentational differences and do not impact the functional correctness or efficiency of the code. Overall, an excellent solution.",llama-3.3-70b-instruct,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

# Find and print the maximum ""TemperatureCelsius""
max_te...",10,"The generated code is correct, complete, and of good quality. It precisely follows all instructions: accurately defines the DataFrame, correctly identifies the maximum ""TemperatureCelsius"", and correctly identifies the minimum ""HumidityPercent"". The output formatting also matches the ground truth's intent with appropriate units. No improvements are needed for this specific task.",llama-3.3-70b-instruct,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# Print the total number of ""Likes"" across all posts
total_likes = df[""Likes""].sum()
print(""Total number o...",9,"The generated code correctly defines the DataFrame, calculates the total likes, and identifies the post type with the highest average number of shares. This solution completely addresses all task requirements. The code is clean, well-structured, and uses appropriate pandas functions. The only minor point is the slightly less descriptive print statements compared to the ground truth (e.g., comparing ""Total number of Likes:"" vs ""Total number of Likes across all posts:""), but this doesn't impact correctness or functionality.",llama-3.3-70b-instruct,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""F...",10,"The generated code is excellent. It correctly defines the DataFrame with the specified data types, accurately calculates the average age, and correctly counts the patients by gender. The code is well-structured, uses proper pandas functions, and produces the expected output. It fully addresses all requirements of the task. The comments are also clear and concise, explaining each step.",llama-3.3-70b-instruct,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}

# Create the DataFrame
df = pd.DataFrame(data)

# 1. Print the ""Title"" of the movie wit...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately identifies the movie with the highest rating using the efficient `idxmax()` method, and precisely calculates the average year. The code is clean, concise, and directly addresses all requirements of the task. The use of f-strings for printing is also good practice. This solution is robust, efficient, and well-structured, demonstrating a strong understanding of pandas functionalities for the given problem.",llama-3.3-70b-instruct,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019,, 2023, 2020],
    ""PriceUSD"": [25000,, 18000, 30000,, 24000],
    ""Mileage"": [30000,, 45000, 15000,, 35000]
}

# Create the DataFrame
df = pd.DataFrame(data)

# Print the total number of cars in the dataset
print(""...",3,"The generated code fails to correctly define the DataFrame due to syntax errors within the data dictionary. Specifically, there are extra commas (two consecutive commas) in the `Year`, `PriceUSD`, and `Mileage` lists, which makes the Python list definitions invalid. This prevents the DataFrame from being created successfully at all, leading to a `SyntaxError`. Therefore, the subsequent lines attempting to calculate and print information will also fail. The code does demonstrate the correct logic for calculating `len(df)` and `df[""PriceUSD""].mean()`, and for `round()` for formatting, but these cannot execute due to the initial error. The DataFrame definition is the most critical part of this specific task, and its failure renders the entire script non-functional.",llama-3.3-70b-instruct,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}
df = pd.DataFrame(data)

# Print the name of the Pokemon with the highest ""HP""
ma...",10,"The generated code is correct, complete, and adheres to good code quality practices.

**Correctness:**
* It accurately defines the DataFrame as specified in the task.
* It correctly identifies and prints the name of the Pokemon with the highest ""HP"" using `idxmax()`.
* It correctly identifies and prints the name of the Pokemon with the lowest ""Defense"" using `idxmin()`.

**Code Quality:**
* Proper use of `pandas` for DataFrame creation and manipulation.
* Clear variable names (`max_hp_pokemon`, `min_defense_pokemon`).
* Concise and efficient use of `idxmax()` and `idxmin()` combined with `.loc[]` for data retrieval.
* Uses f-strings for clear output.

**Completeness:**
* All requirements of the task (DataFrame creation, printing highest HP Pokemon, printing lowest Defense Pokemon) are fully addressed.

The generated code is functionally identical to the ground truth in terms of output and correctness, and slightly more concise in its approach to retrieving the names, which is a minor stylistic difference rather than a functional one. Both approaches (using `df.loc[idx, 'Name']` vs. `df.loc[idx]['Name']`) are valid and common in pandas.",llama-3.3-70b-instruct,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd
import numpy as np
import re

# Load the Titanic dataset
def load_dataset(file_name):
    try:
        dataset = pd.read_csv(file_name)
        return dataset
    except FileNotFoundError:
        print(""The file does not exist"")
        return None
    except pd.errors.EmptyDataError:
        print(""The file is empty"")
        return None
    except pd.errors.ParserError:
        print(""Error parsing the file"")
        return None

# Clean the dataset
def clean_dat...",7,"The generated code correctly addresses most of the requirements but has a critical flaw in the title extraction, which causes an error for many names.

**Strengths:**
*   **Correctness:**
    *   Loads the dataset correctly with error handling.
    *   Identifies missing values.
    *   Fills missing 'Age' values with the median.
    *   Fills missing 'Embarked' values with the most frequent port.
    *   Drops the 'Cabin' column as required.
    *   Displays the shape and first 5 rows of the cleaned dataset.
*   **Code Quality:**
    *   Uses functions (`load_dataset`, `clean_dataset`, `main`) for better modularity, which is good practice.
    *   Includes necessary imports (`pandas`, `re`).
    *   Uses `inplace=True` efficiently for modifications.
    *   Good `.iloc[0]` for mode, though `.mode()[0]` is also common.

**Areas for Improvement (leading to a score of 7 instead of higher):**
*   **Title Extraction Flaw:** The regular expression `re.search('Mrs.|Mr.|Miss.|Master.|Dr.|Rev.', x).group()` is problematic.
    *   It only checks for a limited set of common titles. The prompt asked for ""Mr., Mrs., Miss., Master., etc."", implying a more general extraction, like ` ([A-Za-z]+)\.`.
    *   Crucially, `re.search` returns `None` if no match is found, and then `.group()` on `None` raises an `AttributeError`. This means the code will crash for any name that doesn't contain one of the explicitly listed titles (e.g., 'Capt.'). The ground truth handles this by checking `if title_search:`.
    *   Even for the matched titles, it doesn't always perform as expected. For example, 'Mr. John' would capture 'Mr.', but 'John Smith' would fail. It should ideally capture the pattern ` Title.`. The ground truth's regex ` ([A-Za-z]+)\.` is more robust in this regard.
*   **Completeness (Minor omissions):**
    *   The `print(""Columns with missing values:"")` line after `missing_values = dataset.isnull().sum()` is not explicitly requested by the prompt, though it doesn't harm. The prompt asked to ""identify columns with missing values"" which `isnull().sum()` does, but not necessarily print it to the user.
    *   The `numpy` import is present but not used.

To achieve a higher score, the title extraction logic needs to be robustified to handle all names without errors and be more general in its title capture.",llama-3.3-70b-instruct,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the book inventory data
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}

# Create...",10,"The generated code is correct, efficient, and complete. It accurately defines the DataFrame, correctly identifies the title of the book with the highest price using `idxmax()` and `loc`, and precisely calculates the total stock using `sum()`. The code is well-structured, includes necessary imports, and directly addresses all requirements of the task. The print statements are clear and informative.",llama-3.3-70b-instruct,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
water_potability = pd.read_csv('water_potability.csv')

# Display the first 10 rows
print(water_potability.head(10))

# Show the shape of the dataset
print(water_potability.shape)

# List all column names and their data types
print(water_potability.dtypes)

# Calculate the percentage of potable vs non-potable water samples
potability_counts = water_potability['Potability'].value_counts()
potability_percentages = (potability_cou...",7,"The generated code correctly addresses most of the requirements.

**Strong points:**
- Correctly loads the dataset.
- Displays the first 10 rows and the shape of the dataset.
- Accurately calculates the percentage of potable vs non-potable samples.
- Correctly identifies and displays missing values.
- Shows basic summary statistics for numerical columns.

**Areas for improvement:**
- **Robustness (Missing file handling):** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions when loading the dataset, which is crucial for robust production-ready code. The generated code lacks this.
- **Clarity of output:** While it displays the results, the generated code doesn't include descriptive print statements or separators like the ground truth does (e.g., ""--- 1. First 10 rows of the dataset ---""). This makes the output less readable and harder to understand what each block of output pertains to.
- **Data types display:** The generated code uses `print(water_potability.dtypes)`, which is correct for listing data types. However, the ground truth uses `df.info()`, which provides a more comprehensive summary, including non-null counts along with data types, fulfilling ""List all column names and their data types"" more thoroughly.
- **Specific requirements:** The request asked for ""only the Python code without explanations"" in the final output, which the generated code provides. However, the ground truth included comments addressing each step, which is good practice for readability and maintainability but perhaps not strictly adhering to ""only the Python code without explanations"" for the final delivery itself, though it is used internally for comparison. The generated code is more concise in this regard (no comments). For this evaluation, I'm focusing on the *functional* parity and completeness based on the prompt's tasks.",llama-3.3-70b-instruct,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd
import numpy as np

# Load dataset
def load_data(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(""Error loading data: "", str(e))

# Identify columns with missing values
def identify_missing_values(data):
    missing_values = data.isnull().sum()
    return missing_values

# Fill missing values
def fill_missing_values(data):
    data[['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conducti...",6,"The generated code attempts to address all requirements but falls short in several aspects, particularly regarding the handling of missing values and the before/after summary. While the code is structured into functions, the overall execution and reporting of the cleaning process are not as explicit or accurate as the ground truth.

**Areas for Improvement:**

1.  **Missing Value Strategy Implementation:** The ground truth specifically requested filling numerical columns with their mean. The generated code attempts to apply `fillna` to a list of columns. However, `data[['ph', ...]].mean()` calculates the mean for each of these columns. Applying `data.fillna(..., inplace=True)` or `data = data.fillna(...)` would normally fill NaN values in each column with its respective mean. The generated code's current approach `data[['ph', ...]] = data[['ph', ...]].fillna(...)` essentially reassigns the subset of columns with the NaN-filled versions. This works for numerical columns if they were passed correctly, but it also includes 'Potability' which is a categorical/binary column. The ground truth correctly identifies and fills only numerical columns that have missing values. The generated code assumes specific columns have missing values and includes 'Potability' to be filled with the mode, which wasn't part of the instructions (only mean for numerical was specified). More importantly, the `Hardness`, `Solids`, `Chloramines`, `Sulfate`, etc. columns usually do NOT have missing values in this dataset; only `ph`, `Sulfate`, and `Trihalomethanes` typically contain NaNs. The generalized approach of `df.select_dtypes(include=np.number).columns[df.isnull().any()]` in the ground truth is far more robust.

2.  **Before/After Summary Accuracy:**
    *   The `create_summary` function's `original_data` parameter takes `data.copy()` which means it's already the dataset *after* missing value imputation and duplicate removal, and potentially range checking, if `create_summary` is called at the end as in `main()`. This leads to an inaccurate ""Before"" state in the summary. The ""Before"" statistics should represent the *initial* loaded dataset, before any cleaning operations. The ground truth explicitly prints ""Before Cleaning"" statistics and ""After Cleaning"" statistics at appropriate points in the cleaning pipeline.
    *   The generated code calculates `original_missing.sum()` and `cleaned_missing.sum()`, but it doesn't show *which* columns had missing values before or after, only the total count. The ground truth specifically shows the counts per column, which is more informative.

3.  **Data Range Verification:** The `check_ranges` function introduces custom upper bounds for many columns (e.g., Hardness <= 1000, Solids <= 100000). While this can be a valid data cleaning step, the prompt specified ""reasonable ranges for water quality parameters"" and the ground truth's specific examples (pH 0-14, others >= 0) are based on common domain knowledge without specific upper limits unless specified by external standards. Introducing arbitrary upper limits without specific reasons from the problem statement might be overcleaning or deviating from a standard ""reasonable range"" check. Additionally, `Potability` being checked for `0` or `1` is good, but applying it as `data = data[(data['Potability'] == 0) | (data['Potability'] == 1)]` after filling its NaNs with mode (which would be 0 or 1) might remove rows if the mode wasn't 0 or 1 from the start.

4.  **Clarity of Output:** The generated code's output is less descriptive. It only prints ""Missing Values Before:"" without clear separation of before/after cleaning steps as the ground truth does. The ground truth provides clear statements like ""Removed X duplicate rows"" and ""Removed Y rows due to out-of-reasonable-range values.""

5.  **Function Call Order:** The `main` function performs the cleaning steps but the summary is only generated at the very end using the final `data` for both 'before' and 'after' in its `create_summary` which is logically flawed. The ground truth captures ""before"" metrics right after loading and ""after"" metrics at the very end, which is the correct approach for comparison.

Overall, the generated code demonstrates an understanding of the individual cleaning steps but lacks the precise execution, robust handling of data types for missing values, and accurate before/after reporting seen in the ground truth.",llama-3.3-70b-instruct,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('water_potability.csv')

# 1. Bar chart for potability
plt.figure(figsize=(8, 6))
df['Potability'].value_counts().plot(kind='bar')
plt.title('Distribution of Potable vs Non-Potable Water Samples')
plt.xlabel('Potability')
plt.ylabel('Count')
plt.show()

# 2. Histograms for pH, Hardness, and Chloramines
fig, axs = plt.subplots(3, figsize=(8, 12))
sns.histplot(df['pH'], ax=axs[0])
axs[...",7,"The generated code successfully addresses most of the requirements and produces the four distinct visualizations. However, there are several areas for improvement, especially concerning the quality and detail of the plots, missing value handling, and robustness.

**Strong Points:**
*   Successfully generates all four required plot types (bar, histograms, boxplot, heatmap).
*   Uses `pandas`, `matplotlib.pyplot`, and `seaborn` correctly.
*   Plots have titles and axis labels.
*   The use of `plt.tight_layout()` helps in preventing plot overlaps.

**Areas for Improvement / Missing Aspects:**
*   **Bar Chart (Potability Distribution):**
    *   The generated code uses `df['Potability'].value_counts().plot(kind='bar')`. While it works, it's less standard for a distribution plot than `sns.countplot`.
    *   Labels for 0 and 1 on the x-axis are missing, making it less clear than the ground truth's `xticks` labels.
    *   Missing `palette` for visual appeal and better differentiation.
    *   Missing grid for easier readability.
*   **Histograms (pH, Hardness, Chloramines):**
    *   **Missing Value Handling:** The generated code directly plots `df['pH']` etc. without explicitly handling `NaN` values. While `sns.histplot` often ignores NaNs by default, explicitly using `.dropna()` (as in the ground truth) or specifying `na_action='hist'` can sometimes be clearer or necessary depending on the `seaborn` version or specific plot. More critically, the problem statement explicitly mentions ""handle missing values in plots.""
    *   **Subplot Layout:** The generated code uses `subplots(3, figsize=(8, 12))`, creating 3 rows and 1 column. The ground truth uses `subplots(1, 3, figsize=(18, 6))`, creating 1 row and 3 columns, which is generally better for comparing distributions side-by-side on a screen. The chosen `figsize` also isn't optimal for 3 rows; the plots appear too tall and thin.
    *   Missing `kde=True` for density estimation, which is usually helpful for understanding distributions.
    *   Missing specific colors and grid lines for better aesthetics and readability.
    *   The `xlabel` and `ylabel` for the histograms are generic (`pH` and `Count`) rather than more descriptive units (e.g., `pH Value` or `Frequency`) as in the ground truth.
*   **Boxplot (pH vs. Potability):**
    *   Similar to the bar chart, the x-axis labels (`0` and `1`) for Potability are not replaced with more descriptive labels (`Not Potable`, `Potable`), reducing clarity.
    *   Missing `palette` for better visual appeal.
    *   Missing grid lines.
*   **Correlation Heatmap:**
    *   The `numerical_features` list is manually exhaustive. While correct here, the ground truth uses `df.corr()` which automatically includes all numerical columns, making it more robust to changes in the dataset.
    *   Missing `linewidths` for separating the cells, which can improve readability.
    *   Missing rotation for x-axis labels, leading to potential overlap if feature names were longer. While it works for this dataset, it's good practice to include it.
*   **General:**
    *   No `sns.set_style(""whitegrid"")` was used, which contributes to overall plot aesthetic consistency.
    *   No error handling for `FileNotFoundError` when loading the CSV, which is a good practice for robustness, although not explicitly requested for every line of code.

To get a higher score (8 or above), the code would need to incorporate clearer labels for categorical axes, handle missing values more explicitly (especially on histograms where `dropna()` is often preferred if the original column contains NaNs), provide more descriptive labels for axes/titles, and apply aesthetic improvements like consistent styles or palettes.",llama-3.3-70b-instruct,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import numpy as np

# Load dataset
data = pd.read_csv('winequality-red.csv')

# Create new features
data['citric_per_sulphates'] = data['citric acid'] / data['sulphates']
data['alcohol_per_densi...",6,"The generated code has some correct aspects but also significant flaws and omissions compared to the ground truth and the task requirements.

**Strengths:**
*   **Feature Engineering:** Creates 3 new features (`citric_per_sulphates`, `alcohol_per_density`, `poly_alcohol_density`) and attempts polynomial features, fulfilling the minimum requirement for number of engineered features.
*   **Feature Selection Methods:** Correctly uses `RFE` with `RandomForestRegressor` and `SelectKBest` with `f_regression`.
*   **Imports:** Includes necessary libraries.

**Weaknesses and Areas for Improvement:**

1.  **Correctness of Polynomial Features:** The implementation of polynomial features is problematic.
    *   `poly.fit_transform(data.drop('quality', axis=1))` creates *all* polynomial features, including original and interaction terms.
    *   `poly_df.iloc[:, -10:]` takes *last 10* features, which might not be meaningful or consistently new. It's better to explicitly add specific interaction or higher-order terms if desired, or carefully manage the output of `PolynomialFeatures`. The ground truth explicitly creates *meaningful* specific features, rather than a generic batch of polynomial features.
    *   It doesn't handle potential `NaN`/`inf` values introduced by division in engineered features, which is crucial for robust code. The ground truth handles this explicitly with `EPSILON` and `fillna`.

2.  **RFE Importances/Rankings:** `rfe.ranking_` does *not* provide importance scores; it provides the ranking order. A lower rank is better. The task asks for importance scores *or* rankings. While rankings are technically given, the spirit of ""importance scores"" for RFE (like the Ground Truth extracting `feature_importances_` from the refitted estimator) is missed. `feature_importances_` from the underlying estimator is more informative.

3.  **SKB Importances/Rankings:** `skb.scores_` gives the scores for *all* features, not just the selected ones, and these aren't sorted. The prompt asks for ""importance scores or rankings"" *for the final selected features*. The ground truth correctly extracts scores only for selected features and sorts them, which is more useful for display.

4.  **Comparison and Contrast:** The comparison is very superficial (`print` statements of lists), whereas the prompt explicitly asks to ""Compare and contrast the feature sets selected by each method"" and the ground truth provides a detailed textual analysis of *why* differences might exist, considering the nature of the algorithms. This is a significant omission.

5.  **Final Selected Features:** The ""final selected features"" are just `np.intersect1d(rfe_features, skb_features)`. This is a valid way to find common features, but it doesn't align with the prompt's implied goal of presenting *both* sets of selected features, perhaps with a discussion or identifying a ""best"" set based on criteria. Also, the `final_importances` calculation is incorrect and doesn't map to the `rfe_importances` (rankings). It tries to index `rfe_importances` based on `X_train.columns` positions, which is flawed given `rfe_importances` is an array of rankings for the original features in `X_train`.

6.  **Unnecessary `train_test_split`:** While not strictly incorrect, applying `train_test_split` before feature selection for the *purpose of feature selection* is generally not how it's done. Feature selection is typically done on the entire dataset (or the training set if you split before selection, but then the test set would need to be processed identically). The ground truth processes the entire dataset for feature selection, which is simpler and more direct for this task.

7.  **Output Clarity:** The ground truth provides nicely formatted DataFrames for the selected features and their scores/rankings. The generated code prints raw arrays/lists, which is less readable and professional.

To improve, the code needs to:
*   Refine polynomial feature generation or ensure specifically meaningful interactions are added.
*   Handle `NaN`/`inf` values from feature engineering.
*   Correctly extract and display feature importance scores/rankings for *selected features only*, sorted, instead of just the raw output of `selector.ranking_` or `selector.scores_`.
*   Provide a meaningful textual comparison explaining the differences between the methods as requested.
*   Present the final selected features more clearly and perhaps discuss *why* one method's output might be preferred or how to combine them.",llama-3.3-70b-instruct,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import make_scorer
from sklearn.pipeline import Pip...",6,"The generated code attempts to address all requirements of the task but falls short on several critical aspects, primarily around robust data preprocessing, consistent evaluation methodology, and proper implementation of pipelines/cross-validation that align with best practices.

**Strengths:**
*   **Attempts to address all requirements:** The code includes sections for loading data, preprocessing, defining multiple ensemble models, creating voting and stacking ensembles, performing hyperparameter tuning, plotting feature importances, and generating residual plots.
*   **Correct model initialization:** The individual regressors are correctly initialized.
*   **Basic evaluation metrics:** MAE, MSE, RMSE, and R are calculated.

**Weaknesses and Areas for Improvement:**

1.  **Preprocessing Issues:**
    *   **Dropping NaNs:** The `df.dropna()` approach for handling missing values is too aggressive and can lead to significant data loss, especially if missing values are concentrated in a few columns. The ground truth uses `SimpleImputer` which is a more robust and common practice in ML.
    *   **`User_Score` handling:** The ground truth code specifically converts 'User_Score' to numeric and handles 'tbd'. The generated code completely ignores this.
    *   **Categorical Encoding:** The generated code uses `LabelEncoder` for `Genre` and `Publisher` and `pd.get_dummies` for `Platform`. However, it includes 'Name' and 'Publisher' in `categorical_cols` for LabelEncoding, but then subsequently drops 'Name' as an independent variable without proper reasoning. `LabelEncoder` is typically not suitable for features with no ordinal relationship as it can imply incorrect ordinality for downstream models. `OneHotEncoder` (as used in ground truth) is almost always preferred for nominal categorical variables.
    *   **Preprocessor Scope:** The preprocessing is done globally on the dataframe *before* splitting and cross-validation, which can lead to data leakage (e.g., using information from the test set during scaling or encoding). The ground truth correctly integrates preprocessing into `Pipeline` and `ColumnTransformer` to ensure it's applied correctly within each fold of cross-validation.

2.  **Cross-Validation and Pipeline Implementation:**
    *   **Inconsistent Cross-Validation:** The cross-validation for individual models is manually implemented outside of a `Pipeline` structure. This means the preprocessing steps are not consistently applied within each fold, especially problematic for `LabelEncoder` and `get_dummies` which were already fitted on the entire dataset. The ground truth uses `Pipeline` to encapsulate preprocessing and modeling, ensuring proper CV.
    *   **KFold application:** The individual model evaluation loop uses `X_train` for `kf.split`, then `model.fit(X_train_fold, y_train_fold)` and `model.predict(X_test_fold)`. This means CV is applied only on the *training set* and then a final evaluation is done on a separate `X_test`. This setup is not standard for comparing models with cross-validation on the *entire dataset* (or full data split into train/test then CV on train). The ground truth correctly uses `kf.split(X)` for all model evaluations to ensure consistent 5-fold CV across the entire dataset.

3.  **Hyperparameter Tuning:**
    *   **Scope of Tuning:** Only Random Forest is tuned. Task requires tuning for ""the best individual model."" The generated code doesn't dynamically identify the best model (by initial performance) for tuning as the ground truth does.
    *   **Lacks Pipeline Integration:** The `GridSearchCV` for RF is also done without integrating preprocessing into a `Pipeline`, which is crucial for robust tuning.

4.  **Feature Importance:**
    *   **Hardcoded Feature Names:** The `plt.barh(X_train.columns, feature_importances)` assumes `X_train.columns` directly corresponds to the features used by the model after preprocessing (especially one-hot encoded features). This is incorrect because `LabelEncoder` and `pd.get_dummies` change the column names / introduce new ones. The ground truth correctly identifies and aligns feature names after one-hot encoding.
    *   **Limited Scope:** Only Random Forest feature importance is plotted. The task asked for ""feature importance plots for ensemble models,"" implying potentially more than one or a general approach.

5.  **Evaluation Summary:**
    *   The evaluation metrics are printed for each model separately, but there's no consolidated summary table like the ground truth produces, which makes comparison harder.

6.  **Minor Issues:**
    *   `Name` column is dropped without explicit mention or reasoning in the preprocessing, and `Rank` is not handled at all, whereas the ground truth explicitly drops both.
    *   The `preprocess_df` function operates globally on the DataFrame, which is okay but not the best practice when pipelines are used within sklearn.
    *   `make_scorer` is imported but not used.

Overall, while the generated code covers the topics, the implementation details, especially regarding robust preprocessing with pipelines and consistent cross-validation, are significantly flawed, leading to potentially incorrect and unreliable results.",llama-3.3-70b-instruct,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_square...",4,"The generated code attempts to address several aspects of the prompt but falls significantly short in terms of correctness, completeness, and robustness compared to the ground truth.

**Major issues leading to a low score:**

1.  **Data Loading and Preprocessing:**
    *   The `Year` column likely refers to `Year_of_Release` in the actual dataset. The generated code uses `df['Year']` which would lead to a `KeyError` or operate on unintended data if a different `Year` column exists. The ground truth correctly identifies and cleans `Year_of_Release`.
    *   Handling of missing values and data type conversion (e.g., `Year_of_Release` to int) is either missing or inadequate in the generated code, which is crucial for time series and regression.

2.  **ARIMA Modeling:**
    *   The generated code uses `statsmodels.tsa.arima.model.ARIMA` directly without proper determination of `p, d, q` orders, which is a key aspect of ARIMA. It hardcodes `(5,1,0)`, which is an arbitrary choice. The ground truth uses `pmdarima.auto_arima` to automatically find the best order, which is far more robust and typically expected for ""advanced statistical modeling"".
    *   Forecasting and confidence intervals for ARIMA are entirely missing in the generated code.

3.  **Seasonal Decomposition:**
    *   The `seasonal_decompose` is called but the results are not used or plotted. For yearly data, `period` needs to be specified carefully, or else decomposition might fail or be meaningless. The ground truth provides a more robust check and usage.

4.  **Multivariate Regression:**
    *   The formula building for interaction terms is missing. The prompt specifically asks for ""interaction terms."" The generated code only uses `ColumnTransformer` for one-hot encoding, but doesn't create products of variables for interactions.
    *   Confidence and prediction intervals for this model are completely missing.
    *   It uses `train_test_split` which is inappropriate for time series data. The ground truth correctly uses `TimeSeriesSplit`.

5.  **Polynomial Regression with Regularization:**
    *   Similar to multivariate regression, it uses `train_test_split` instead of `TimeSeriesSplit`, which is incorrect for time-dependent data.
    *   Confidence and prediction intervals for these models are missing.

6.  **Bayesian Regression (PyMC3):**
    *   The PyMC3 model is extremely simplistic, only considering `Year` as a predictor, which is a major simplification compared to the full set of predictors (Genre, Platform, Publisher) that would be expected for a ""Bayesian regression"" solution on this dataset. It doesn't handle the categorical features or interactions that were mentioned for other regression models.
    *   There is no definition or usage of the `X_train['Year']` in the `mu` calculation for Bayesian regression, suggesting `X_train` from earlier `train_test_split` might be used, but it only contains categorical columns.
    *   Crucially, there is no code to extract or use prediction intervals or summary statistics from the `trace` object, which is a primary reason to use Bayesian methods (uncertainty quantification).

7.  **Change Point Detection:**
    *   It uses `Dynp` with an arbitrary `pen=10`. The ground truth uses `Pelt` and a more principled way to select `pen` (based on data size, 2\*log(n)), which is generally better practice.
    *   The change points are detected but not plotted or discussed in context with years.

8.  **Survival Analysis:**
    *   The definition of `T` (time) and `E` (event) is incorrect and simplistic. `T = df['Year'] - df['Release_Year']` assumes `Release_Year` exists, which isn't typically a column in `video_games_sales.csv` (`Year_of_Release` is). Also, `E = df['Global_Sales'] > 0` is not a meaningful event for survival analysis. Survival event is usually a 'death' or 'failure' event. The ground truth defines longevity based on *observed* duration in the dataset.
    *   It only implements Kaplan-Meier and lacks the CoxPHFitter and handling of covariates as requested in the task.
    *   Confidence intervals for survival analysis features are missing.

9.  **Confidence and Prediction Intervals:**
    *   A generic `ci` function is provided, which is misnamed (it calculates confidence intervals for the mean of residuals, not prediction intervals for new observations, nor confidence intervals for model coefficients).
    *   This function is defined but *never called* or applied to any of the models as required by the prompt's `Step 9`. This is a critical omission.

10. **Time Series Cross-Validation:**
    *   The `TimeSeriesSplit` is correctly imported and used at the end, but it is applied *only* to `yearly_sales` (a time series variable) and then tries to fit regression models that were previously defined on the original `df`. This leads to a mismatch (`X_train` is `yearly_sales['Year']`, but `regression` expects much more features for `X`). This section essentially does not perform proper cross-validation for the previously built multivariate/polynomial models.
    *   The ARIMA cross-validation logic is very basic and lacks the robust fitting/forecasting steps that the ground truth implements.

**Overall:** The generated code demonstrates a superficial understanding of the requirements. Many advanced aspects (proper ARIMA, specific CI/PI requirements for *all* models, robust survival analysis setup, correct time-series CV for different model types, interaction terms) are either absent, incorrectly implemented, or severely simplified, leading to a non-functional or misleading solution for an ""expert"" level task.",llama-3.3-70b-instruct,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
import umap
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.decomposition import PCA
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.cluster.hierarchy import fcluster
from s...",3,"The generated code attempts to address many aspects of the prompt but fails significantly in execution, primarily due to numerous functional and conceptual errors.

**Correctness (1/10):**
*   **Preprocessing:** The feature engineering ('Regional_Sales_Distribution', 'Publisher_Portfolio_Size') is fundamentally incorrect. 'Regional_Sales_Distribution' sums sales, instead of calculating proportions. 'Publisher_Portfolio_Size' counts total games by publisher, not unique games (which is more aligned with ""portfolio size""), and fails to merge this back correctly due to the use of `transform('count')` leading to incorrect values. Missing crucial handling of `NaN` values and data type conversions for 'Year_of_Release'.
*   **Data Scaling:** A critical omission is the lack of `StandardScaler` application to numerical features before clustering. Clustering algorithms rely on distance metrics, and unscaled data will lead to skewed results. Categorical features are also not handled (no OneHotEncoding).
*   **Clustering Functionality:** The clustering functions (KMeans, DBSCAN, GMM, Hierarchical) are defined but never actually used with the *processed* data (i.e., scaled numerical + one-hot encoded categorical data), meaning the `data` passed to them is the raw DataFrame, which will raise errors or produce garbage results. DBSCAN has fixed parameters (0.5, 10) which are unlikely to work for arbitrary datasets, and no attempt is made to determine them.
*   **Optimal Cluster Determination:** The `silhouette_analysis`, `elbow_method`, and `gap_statistic` functions are defined, but they are not used to *determine* optimal clusters. The `main` function directly passes a fixed `n_clusters=5` to `consensus_clustering`. The `gap_statistic` is not implemented at all.
*   **Consensus Clustering:** The implementation of `consensus_clustering` is completely incorrect. It just takes the mode of the labels from different algorithms for each data point, which is not how consensus clustering typically works (e.g., co-association matrix, ensemble methods). It also passes the raw `data` to clustering algorithms within this function, perpetuating the scaling issue.
*   **Anomaly Detection:** The `anomaly_detection` function is a placeholder that incorrectly identifies anomalies as clusters with less than 10 points, which is not a standard anomaly detection technique (e.g., Isolation Forest, One-Class SVM).
*   **Association Rule Mining:** `association_rule_mining` is passed the entire raw `data` DataFrame, which is not suitable for itemset mining without proper boolean transformation of relevant categorical features. It will likely fail or produce meaningless results.
*   **Recommendation System:** This function is a placeholder (`pass`).
*   **External Validation:** The `external_validation` function assumes `true_labels` are available, which is not the case for an unsupervised clustering task described in the prompt. It's only relevant if you have ground truth labels.

**Code Quality (4/10):**
*   **Imports:** Many necessary imports (`ColumnTransformer`, `Pipeline`, `NearestNeighbors`, `KneeLocator`, `f_oneway`, `chi2_contingency`, `IsolationForest`, `warnings`) are missing. Conversely, some irrelevant imports (`PCA`, `accuracy_score`, `f1_score`, `train_test_split`) are present.
*   **Structure:** The use of many small functions is good, but the flow in `main()` doesn't connect them logically and doesn't handle preprocessing or feature selection correctly before passing data to clustering.
*   **Readability:** Variable names are generally okay, but the comments are minimal and do not explain the logic.
*   **No Error Handling:** No `try-except` blocks for file loading or failed clustering attempts.

**Completeness (4/10):**
*   **Feature Engineering:** Attempts to implement, but the implementation is flawed as mentioned.
*   **Multiple Clustering Algorithms:** Implemented the functions, but they are not correctly utilized with processed data. DBSCAN lacks proper parameter tuning.
*   **Dimensionality Reduction:** Functions exist, but `main()` doesn't call them, and no visualizations are produced.
*   **Optimal Cluster Determination:** Functions are placeholders or not used effectively to *determine* the optimal k. Gap statistic is missing.
*   **Consensus Clustering:** Attempted, but the implementation is conceptually incorrect.
*   **Detailed Cluster Profiles:** `cluster_profiles` returns `describe()` output, which is a start, but `statistical_significance_testing` is a placeholder. No actual ""profiling"" (e.g., top N categories, mean numerical values per cluster) as described in the requirements.
*   **Anomaly Detection:** Rudimentary and incorrect implementation.
*   **Association Rule Mining:** Present but likely to fail due to incorrect data preparation for `apriori`.
*   **Recommendation System:** Not implemented.
*   **Validation:** Internal validation attempted, but external validation is misapplied. Missing comprehensive output and print statements to display results.

The most critical failures are the incorrect preprocessing, the complete lack of data scaling/encoding before clustering, the flawed implementation of consensus clustering, and the non-functional or conceptually wrong implementations of anomaly detection and the recommendation system. The code would likely not run without errors or would produce completely meaningless results.",llama-3.3-70b-instruct,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from optu...",4,"The generated code attempts to address many of the requirements but falls significantly short in implementation details, correctness, and completeness compared to the advanced nature of the task.

**Major Missing/Incorrect Implementations:**

1.  **Comprehensive Feature Engineering:** The code only performs `StandardScaler`. It completely misses handling zero values as NaNs, polynomial features, and custom interaction terms as specified in the ground truth and required by ""comprehensive feature engineering"".
2.  **Advanced Ensemble Methods:**
    *   **Voting Classifier:** Uses only two basic estimators (`LogisticRegression`, `DecisionTreeClassifier`) and lacks the diversity and advanced models (XGBoost, LightGBM, CatBoost, RandomForest, SVC, KNN) used in the ground truth.
    *   **Bagging:** Similar to voting, uses only two basic estimators.
    *   **Boosting:** While the models are imported and fitted, there's no proper evaluation or comparison between them, nor are they used effectively within a larger ensemble strategy.
    *   **Stacking Ensemble:** The `create_stack_model` function defines `VotingClassifier` as the ""stack"", which is fundamentally incorrect. Stacking involves base learners whose predictions become features for a meta-learner, not just another voting ensemble. The `VotingClassifier` itself does not have a `meta_learner` argument or the mechanics of a true stacking.
3.  **Bayesian Optimization:**
    *   It uses Optuna, which is good.
    *   The `objective` function for Optuna directly passes `XGBClassifier` without `use_label_encoder` or `eval_metric`, which are crucial.
    *   It performs `cross_val_score` on `X_train` and `y_train`, but the task specifically asked for nested CV or similar. The ground truth used a train-validation split for speed specifically during optimization for the single model, and then incorporated the best parameters into the main nested CV loop.
4.  **Nested Cross-Validation:** The provided `nested_cv` function is a regular k-fold cross-validation, not a nested one. It uses `kf.split(X_train)` which is incorrect as nested CV has an outer loop for evaluation and an inner loop for model selection/tuning. The task explicitly asked for ""nested cross-validation for unbiased model evaluation"".
5.  **Custom Ensemble Methods with Dynamic Weight Assignment:** The `CustomEnsemble` class is a simple hard voting/majority class ensemble with predefined, static weights. It does not implement ""dynamic weight assignment"" based on performance, as the ground truth demonstrates using AUCs.
6.  **CatBoost for handling categorical features:** The generated code imports CatBoost but doesn't demonstrate its unique capability of handling categorical features, which would require identifying and passing `cat_features` to the model. The dataset generally allows for this feature to be showcased.
7.  **Comprehensive Model Interpretation using SHAP values:**
    *   It only uses `shap.Explainer(LogisticRegression())`  it should use the best performing complex model from the ensemble (e.g., XGBoost, LGBM, or the final ensemble of stacking).
    *   It only shows `shap.plots.beeswarm(shap_values)`. It lacks `summary_plot` (bar/beeswarm for global), `dependence_plot` (for feature interaction), and `force_plot` (for local interpretation) which are standard SHAP functionalities and part of the ground truth.
8.  **Model Calibration and Reliability Analysis:**
    *   It imports `calibration_curve` and `brier_score_loss` and defines plotting and a brier score function.
    *   However, it only demonstrates this for a simple `LogisticRegression` on `X_test`, not for the best-performing ensemble or models emerging from the nested CV.
    *   It doesn't use `CalibratedClassifierCV` (Platt scaling, Isotonic regression) which is crucial for actual calibration.
9.  **Ensemble Diversity Analysis and Model Combination Strategies:**
    *   The `ensemble_diversity` function calculates ""diversity"" based on disagreement, which is a start, but the ground truth properly utilizes correlation matrix of OOF predictions which is a more standard and informative metric for ensemble diversity.
    *   The `ModelCombiner` is a very basic average and not an ""advanced"" strategy. The ground truth demonstrates a meta-learner on OOF predictions, which is a more sophisticated approach.
    *   It lacks the output of OOF predictions from nested CV which are necessary for proper diversity analysis and meta-learning evaluation.
10. **Structure and Readability:** The code is very sequential without clear functions or segments for each task, making it harder to follow and debug compared to the ground truth which is well-structured with dedicated functions and configuration sections.

**Minor Issues:**

*   Lack of proper context for `kf` in nested CV. It's defined globally rather than within a proper nested CV loop.
*   Missing `matplotlib.pyplot` and `seaborn` imports, but attempts to use `plt.show()`.
*   Lack of warnings suppression, though this is minor.
*   The use of `skopt` imports (`BayesSearchCV`, `Real`, `Categorical`, `Integer`) suggests an intent for `skopt`, but the actual Bayesian optimization uses `optuna`. This is confusing and indicates a potential copy-paste or incomplete thought process.
*   Hardcoding `n_estimators=10` for boosting algorithms, which is very low for typical performance.

In summary, the generated code touches upon many required components but fails to implement them correctly, completely, or in an advanced manner. It barely scratches the surface for most complex requirements and often provides simplified or incorrect interpretations of the task description compared to the comprehensive implementation in the ground truth.",llama-3.3-70b-instruct,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
# Import necessary libraries
import pandas as pd
import numpy as np
import pymc3 as pm
import pymc4 as pm4
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from scipy import stats
from survival import SurvivalAnalysis
from causalml.inference import CausalModel
from causalml.propensity import fit_propensity
from causalml.metrics import uplift_at_k
from statsmodels.formula.api import mixedlm
import matplotlib.pyplot as plt
from sklearn...",2,"The generated code attempts to address the task requirements but makes significant fundamental errors in scope, data handling, and library usage, leading to a largely incorrect and non-functional solution for the specified 'diabetes.csv' dataset.

**Primary Reasons for Failure:**

1.  **Incorrect Dataset Assumption/Handling**: The generated code assumes the 'diabetes.csv' file from the `scikit-learn` `datasets.load_diabetes()` which has features named 'BMI', 'BP', 'S1', 'S2', etc., and a target 'progression'. This is **not** the 'diabetes.csv' dataset implied by the prompt (which typically refers to the Pima Indians Diabetes Database, with columns like 'Pregnancies', 'Glucose', 'BloodPressure', 'Outcome'). This fundamental misunderstanding makes almost all subsequent modeling attempts incorrect for the actual dataset.
    *   `diabetes_df['age'] = diabetes_df['AGE']` and dropping 'AGE' implies a dataset that is not the Pima dataset.
    *   The use of `BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6'` for scaling and modeling absolutely confirms this.
    *   The `Outcome` column (binary diabetes status) from the Pima dataset is not used as the target for classification or regression where it should be. Instead, 'progression' and 'class' (which don't exist in the Pima dataset) are used.

2.  **Missing Value Handling (Incorrect):** The code simply uses `diabetes_df.dropna()`. This is insufficient and incorrect for the Pima 'diabetes.csv' which is known to have 0s representing missing values in columns like Glucose, BloodPressure, BMI, etc. The ground truth explicitly handles this.

3.  **PyMC Usage and Model Specification Issues**:
    *   **PyMC3 Deprecation**: The code attempts to import `pymc4` but then exclusively uses `pymc3`. PyMC3 is effectively deprecated in favor of PyMC (PyMC4 is the new name). The ground truth correctly uses `pymc3` but also suppresses deprecation warnings and uses `arviz` which is the standard for PyMC3/PyMC result handling.
    *   **Incorrect `pm_Normal`**: In `mixed_effects_model`, there's a typo `pm_Normal` instead of `pm.Normal`.
    *   **Overly Simplistic Models**: The Bayesian models (hierarchical, mixed-effects, logistic regression) are extremely simplistic (e.g., `beta0 + beta1 * data['BMI']`) and don't reflect the complexity of a diabetes risk model with multiple predictors, nor do they reflect hierarchical or mixed-effects concepts accurately with respect to the general `diabetes.csv` dataset. They basically model 'progression' as a function of 'BMI' only, which falls far short of ""advanced statistical modeling"" and addressing multiple variables.
    *   **Lack of Proper Grouping for Hierarchical/Mixed-Effects**: The `mixed_effects_model` tries to apply `u[data['SEX']]` but the original 'diabetes.csv' does not have a 'SEX' column, and it's unclear how `u` is defined to correspond to `SEX` in a general way. The ground truth simulates a group variable for this purpose.
    *   **Informative Priors**: The prompt specifically asks for ""informative priors based on medical knowledge"". The generated code uses generic `pm.Normal('beta', mu=0, sigma=10)` and `pm.HalfNormal('sigma', sigma=10)` which are very weakly informative (or non-informative) and do not demonstrate medical knowledge.

4.  **Survival Analysis (Simulated, but lacks complexity):**
    *   The `survival` library is not standard and typically `lifelines` or `statsmodels` are used for survival analysis.
    *   The simulation of `time` and `event` (`np.random.uniform`, `np.random.choice`) is very arbitrary and doesn't reflect any underlying plausible process related to diabetes onset risk or the actual `Outcome` variable. The ground truth has a much more structured simulation.

5.  **Gaussian Processes (Not Bayesian using PyMC):** The prompt explicitly asks to use Gaussian processes for *non-parametric modeling of relationships between variables*. The generated code uses `sklearn.gaussian_process.GaussianProcessRegressor`, which is a frequentist implementation, not a Bayesian one using PyMC3/PyMC, which was implied by the context of PyMC3 being used for other Bayesian tasks.

6.  **Causal Inference (Incomplete and Misapplied):**
    *   **Library Choice**: `causalml` is a valid library, but the implementation is very high-level and doesn't show the underlying mechanics (e.g., for propensity score matching, it just calls `fit_propensity` and `uplift_at_k` without showing the matching process or subsequent ATE calculation).
    *   **Assumptions**: Similar to other sections, the causal inference definitions (treatment, outcome, confounders) are based on the incorrect dataset features (BMI, BP, S1... vs. Outcome, Glucose, etc.).
    *   **Instrumental Variables**: The `causalml.inference.InstrumentalModel` is used, but its parameters are `['BMI']` as the endogenous variable and `['BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6']` as instruments, which is unlikely to meet the strong assumptions required for valid instrumental variables (i.e., instruments affect outcome *only* through the endogenous variable). It also does not explicitly show the two-stage least squares process as the ground truth does.

7.  **Model Comparison (Incomplete):** The `pm.waic` and `pm.loo` functions are called, but without comparing *multiple* models. The ground truth explicitly builds a second, simpler model for comparison.

8.  **Model Validation (Basic):** `pm.plot_posterior` and `pm.sample_posterior_predictive` are called, but there's no actual interpretation or comparison to the observed data for the PPC, just storing the results.

**Summary:**
The generated code reads the 'diabetes.csv' but then proceeds to assume an entirely different dataset structure, leading to a chain of errors and misinterpretations for almost every modeling step. While it imports some relevant libraries and nominally calls functions related to the requested tasks, the actual implementations are either incorrect for the given data, too simplistic, or use non-Bayesian methods where Bayesian ones were implied by the context (like GP). The lack of robust missing value handling and general code clarity (e.g., `pm_Normal`) further reduces its quality.",llama-3.3-70b-instruct,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture
from sklearn.cluster import SpectralClustering
from sklearn.cluster import OPTICS
from sklearn.metrics import silhouette_score
from sklearn.metrics import calinski_harabasz_score
from sklearn.metrics import davies_...",2,"The generated code attempts to address many aspects of the prompt but fails significantly in execution, demonstrating a superficial understanding of several key requirements.

**Correctness (0/10):**
*   **Preprocessing:** `data = data.dropna()` is incorrect for this dataset, where '0' represents missing values in several columns (e.g., Glucose, BloodPressure). The ground truth correctly identifies and imputes these. The generated code also scales numerical features before advanced feature engineering, which should ideally happen after.
*   **Feature Engineering:** The `Metabolic_Syndrome` indicator is overly simplistic and relies on unnormalized/scaled values (100, 130, 30) while the data is already scaled. This is a critical logical flaw. The ground truth creates more nuanced indicators based on medical guidelines.
*   **Clustering:**
    *   Many clustering algorithms (KMeans, DBSCAN, GMM, Spectral, OPTICS) are initialized but never actually fitted or used to assign labels. This makes a large portion of the clustering section non-functional.
    *   Hierarchical clustering (`hclust = linkage(data, method='ward')`) is performed on the entire DataFrame, which includes the target variable and potentially unscaled features, leading to incorrect results. It also doesn't extract clusters.
*   **Dimensionality Reduction:**
    *   UMAP is called but `umap` is not imported (should be `umap.UMAP`).
    *   The Autoencoder architecture is defined but not compiled, trained, or used to generate embeddings. The `Sequential` model is not imported from `tensorflow.keras.models`.
*   **Optimal Parameter Determination:** Only KMeans is considered for optimal K, and the method used (silhouette) is reasonable, but it's only one of the requested methods (elbow, gap statistic, statistical validation are missing). The optimal K is found but never applied to KMeans for the actual clustering results.
*   **Consensus Clustering:** The implementation `consensus = np.array(consensus)` simply stacks the KMeans labels (which are not recomputed with optimal K) and doesn't perform any co-occurrence matrix construction or hierarchical clustering on similarity, which is the core of consensus clustering. `consensus[0]` is then arbitrarily chosen for phenotyping.
*   **Patient Phenotyping:** The `grouped = phenotype.groupby('cluster')` and `stats = grouped.describe()` is a basic descriptive analysis, but it completely misses the statistical significance testing (ANOVA, Kruskal-Wallis, Chi-squared) and detailed interpretation required. It also doesn't visualize profiles.
*   **Anomaly Detection:** `IsolationForest` is correctly applied but other methods like OneClassSVM and LOF are missing. Visualization of anomalies is also missing.
*   **Time-Series Clustering:** The check `adfuller(data['Glucose'])[1] < 0.05` is an attempt to check for stationarity, not presence of temporal patterns. Moreover, `data['Glucose'].values.reshape(-1, 1)` is not a time series but a simple 1D array. The dataset is explicitly cross-sectional, and the prompt asks for a conceptual outline if no temporal data exists, which the generated code tries to force.
*   **Association Rule Mining:** `TransactionEncoder` expects a list of transactions (list of lists of items), not a DataFrame slice of numerical values. This section would fail directly. The discretization of continuous features into categorical bins (e.g., for Glucose into 'High Glucose', 'Low Glucose') is crucial for ARM and is completely absent.
*   **Risk Stratification:** This section is a simple RandomForest classifier on `Metabolic_Syndrome` (which was incorrectly defined earlier). It completely deviates from the requirement of building a risk stratification *based on clustering results* with *clinical interpretation*.

**Code Quality (3/10):**
*   **Imports:** Many imports are present, but several necessary ones are missing (`Keras`, `UMAP`, `matplotlib.pyplot`, `seaborn`, `scipy.stats` functions for significance testing, `sklearn.neighbors` for LOF). Some unused imports are also present (`ttest_ind`, `norm`).
*   **Structure:** Code blocks are present for sections, but the logical flow is broken due to incorrect or missing implementations within those sections.
*   **Readability:** Variable names are okay, but the lack of comments for complex sections makes it hard to follow the intended (but often flawed) logic.
*   **Reproducibility:** `random_state` is missing for many sklearn models (KMeans, GMM, SpectralClustering, IsolationForest, RandomForestClassifier) which hinders reproducibility.

**Completeness (1/10):**
*   **Multiple Clustering Algorithms:** Algorithms are initialized but not applied correctly. OPTICS is included but not properly used. Hierarchical clustering is only linked, not clustered.
*   **Advanced Dimensionality Reduction:** t-SNE and PCA are mentioned but not fully utilized. UMAP is broken by missing import. Autoencoder is defined but not used. Visualization of these embeddings is mostly missing.
*   **Consensus Clustering:** Grossly misunderstood and incorrectly implemented.
*   **Patient Phenotyping:** Misses statistical significance testing, visual profiles, and detailed interpretation.
*   **Anomaly Detection:** Only one of three specified methods is correctly applied, and visualization is missing.
*   **Time-Series Clustering:** Implemented incorrectly and applied to inappropriate data, failing to provide the requested conceptual outline.
*   **Association Rule Mining:** Completely flawed implementation due to data format and missing discretization steps.
*   **Risk Stratification:** Completely abandons clustering results for a simple classification task, missing the clinical interpretation aspect.

Overall, the generated code seems to have used keywords from the prompt to include libraries and function calls, but often without understanding their proper usage or how they fit into the overall analytical workflow described by the task. This leads to a non-functional and largely incorrect solution.",llama-3.3-70b-instruct,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.ensemble import VotingRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor
from lightgbm...",3,"The generated code attempts to address many aspects of the prompt but fails significantly in correctness, completeness, and code quality.

Critically Missing/Incorrect:
1.  **Data Loading and Preprocessing:**
    *   The `world-happiness-report-2021.csv` dataset's target variable is 'Ladder score', not 'Happiness Score'. This immediately causes `KeyError` and makes the core task incorrect.
    *   The dropping of missing values (`df.dropna()`) is a very simplistic preprocessing step compared to the comprehensive, `ColumnTransformer`-based preprocessing of the ground truth, which includes imputation and scaling by feature type.
    *   Feature engineering is minimal and incorrect (e.g., trying to log `Country` column after `get_dummies` is illogical and applies `get_dummies` before handling numerical features, which is also incorrect). The ground truth intelligently drops columns like `Country name` and uses `Regional indicator` appropriately.
    *   `MinMaxScaler` is imported but never used. `StandardScaler` is imported but not used.
    *   No proper `Pipeline` for preprocessing before model training, which leads to data leakage and inconsistent transformations if not handled carefully (though some models are put into pipelines, the preprocessing isn't robustly defined at the beginning with `ColumnTransformer`).

2.  **Ensemble Methods:**
    *   **Voting Regressor:** Hard and soft voting are for *classification*. For regression, it's weighted/unweighted averaging. The prompt explicitly asked for ""hard and soft voting"" but implied regression context by ""Voting Regressor"". The generated code implements basic `VotingRegressor` without explicitly demonstrating weighted averaging (soft voting equivalent for regression).
    *   **Bagging:** The `base_estimator` argument for `BaggingRegressor` is deprecated, and it should be `estimator`.
    *   **Boosting:** While libraries are imported, the usage is direct model fitting on `X_train`, `y_train` without robust preprocessing pipelines or proper evaluation metrics (e.g., `eval_metric` for XGBoost).
    *   **Custom Ensemble:** The custom ensemble's `fit` method uses `X_test` to calculate weights based on `mean_squared_error(y_test, y_pred)`. This is a severe form of data leakage, as `y_test` (the unseen test set) is used directly in training the custom ensemble's weights. The ground truth correctly uses out-of-fold (OOF) predictions to calculate weights, preventing this leakage. The `predict` method is also flawed as it simply averages the weighted predictions (`np.mean(y_pred, axis=0)`) rather than summing them, and `y_pred` accumulates weighted predictions, not the raw predictions.

3.  **Hyperparameter Tuning:**
    *   The `objective` function for Optuna evaluates the model on `X_test` and `y_test`. This is a *major* data leakage issue and invalidates the hyperparameter search, as the test set is used for tuning. The ground truth correctly uses cross-validation on the training data within the objective function.

4.  **Nested Cross-Validation:**
    *   The implementation uses a standard `KFold` split on `X_train` and then fits and predicts on `X_val_fold`. This is **not nested cross-validation**. Nested CV involves an outer loop for model evaluation and an inner loop for hyperparameter tuning. The generated code only shows a simple cross-validation step on what it calls `X_train`. It lacks the outer loop that would split the data *before* any tuning (even if it were correct) and then evaluate the *tuned* model in each outer fold.

5.  **Advanced Techniques (Blending, Multi-level Stacking):**
    *   The implementation of blending and multi-level stacking is simplistic and doesn't clearly separate training sets for base learners and meta-learners in the blending process to prevent leakage, nor does it demonstrate the mechanics properly. The `StackingRegressor` itself handles some of this, but the conceptual understanding seems weak.

6.  **Model Interpretation (SHAP, LIME):**
    *   SHAP: `shap.Explainer` without specifying the tree type (e.g., `shap.TreeExplainer` for XGBoost) might work but is less robust. More importantly, `shap_values` for `shap.Explainer` might return values in a different format than expected by `shap.force_plot` for `matplotlib=True`. The `force_plot` for tree models typically expects `shap_values[row_idx]` and `feature_names[row_idx]` for individual instances. The prompt requested global and local interpretation, and only a single instance of `force_plot` is attempted, which might not render correctly.
    *   LIME: `LimeTabularExplainer` is initialized with `X_train` (the training data) but then `explain_instance` is called with `X_test.iloc[0]`, which is okay. The ground truth shows more robust plotting of LIME. The `predict_fn` is correctly defined for LIME, but the overall interpretation section is less complete.

7.  **Model Calibration and Uncertainty Quantification:**
    *   `CalibratedClassifierCV` is for classification, not regression. The task is regression. While it can be adapted, it's a misapplication in its current form and the results are not meaningful for regression calibration. The ground truth correctly points out its classification primary use and shows other methods like residual plots and quantile regression which are appropriate for regression.
    *   No robust implementation of uncertainty quantification beyond a generic and incorrect calibration attempt.

8.  **Ensemble Diversity Analysis and Model Combination Optimization:**
    *   **Diversity:** The `calculate_diversity` function is conceptually flawed. `mean_squared_error(y_test, y_pred_i) + mean_squared_error(y_test, y_pred_j)` does not represent model diversity. Diversity usually refers to the disagreement or correlation between model predictions, or the variance of errors. The ground truth's approach using correlation of OOF predictions is appropriate.
    *   **Optimization:** `optimize_model_combination` simply tries all combinations of models and averages their predictions. This is an exhaustive search, not an optimization of *weights*, which is the standard approach for model combination optimization (e.g., using `scipy.optimize.minimize` as in the ground truth). It finds the *best subset of models equal-averaged*, not optimal weights for *all* models.
    *   Both these parts also suffer from data leakage by using `X_test` and `y_test` in their calculations.

Overall, the code demonstrates a superficial understanding of many advanced ML concepts. It attempts to address each point from the prompt but consistently misapplies techniques or introduces severe data leakage. The preprocessing is inadequate, and the core task setup (target variable name) is incorrect.",llama-3.3-70b-instruct,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import pykalman
from statsmodels.tsa.arima.model import ARIMA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from scipy import stats
from pymc3.stats import autocorrelation
import arviz as az
import theano.tensor as tt
import matplotlib.p...",3,"The generated code attempts to address several requirements but fails significantly in both correctness and completeness for a task of this complexity.

**Correctness Issues:**
*   **Data Loading and Preprocessing:** The code assumes pre-cleaned columns 'Happiness.Score', 'Standard.Error', 'Economy..GDP.per.Capita.' which are not present in the `world-happiness-report-2021.csv` as per the ground truth (e.g., `Ladder_score`, `Logged_GDP_per_capita`). This leads to immediate `KeyError` or `NaN` issues if those columns are not correctly handled or mapped. The `dropna()` call at the end of preprocessing throws away a lot of data, which is terrible missing value handling, especially for a task explicitly asking for *comprehensive* handling.
*   **Hierarchical Bayesian Model:**
    *   It tries to use `data['Region']` directly for `region_eff` indexing and `data['Region']` as an observed variable for the mean of `happiness`.
        *   `data['Region']` is a string column, not numerical indices needed for PyMC3 hierarchical models. The ground truth correctly maps regions to integer IDs. This model will not run.
        *   The model formulation `mu + region_eff[data['Region']]` suggests `region_eff` is indexed by `data['Region']`, which is wrong.
    *   The model assumes `world-happiness-report-2021.csv` has a 'Region' column, which it does not. It has `Regional_indicator`.
*   **Structural Equation Model (SEM):** The ""SEM"" implemented is just a simple multiple regression of happiness on three latent variables (`economy`, `social`, `psychological`) that are not observed or linked to the data in any way in the `sem` model. This is *not* SEM. SEM involves specifying relationships between observed and latent variables, including measurement models and structural paths. The ground truth *approximates* SEM by linking a series of regressions, which is still a rough approximation but much closer than this.
*   **Causal Inference (IV and PSM):**
    *   **IV:** The IV model is simply `happiness ~ gdp`. This is *not* an Instrumental Variable model. An IV model requires an instrument that affects the endogenous variable but not directly the outcome. The provided model lacks an instrument.
    *   **PSM:** The PSM section is incomplete. It calculates a scaled GDP array but does absolutely nothing with it. It doesn't calculate propensity scores, doesn't perform matching, and doesn't calculate ATE. It's a placeholder.
*   **Bayesian Regression (Wider/Informative Priors):** The example uses `data['Economy..GDP.per.Capita.']` directly, which has not been standardized. The ground truth standardizes predictors for better model convergence and interpretation with Bayesian models. The priors are defined, but without the scaling, their ""informativeness"" is less well-defined in relation to the magnitude of the raw data.
*   **Gaussian Processes:** The code uses `GPy`, which was not explicitly requested (PyMC3/PyMC4 preferred, and the ground truth used PyMC3's GP module). This is a deviation. While `GPy` is a valid GP library, the task specified PyMC3/PyMC4 as the primary Bayesian framework. The `GPy` model is initialized but not trained or used for prediction, nor is it within a PyMC3 context.
*   **Mediation Analysis:** The ""mediation"" model is just another multiple regression, similar to the ""SEM"" model, again involving latent variables (`social`, `psychological`) that are not defined by or linked to observed data. It does not perform mediation analysis (calculating direct, indirect, and total effects).
*   **Difference-in-Differences (DiD):**
    *   It simulates a `time` component, but `data['Economy..GDP.per.Capita.']` is used as the 'treatment' variable, which is a continuous variable, not a binary treatment assignment. DiD usually requires a binary treatment group and a pre/post period.
    *   The interaction term `data['Economy..GDP.per.Capita.'] * data['time']` is syntactically fine but conceptually flawed for a standard DiD setup. DiD examines the interaction of a binary treatment group with a binary time period.
    *   No simulation of *how* the treatment affects the outcome, leading to potentially meaningless results.
*   **Bayesian Model Comparison (WAIC, LOO-CV, Bayes Factors):**
    *   The `az.bfmi` function is used for ""Bayes factors,"" which is incorrect. BFMI (Bayesian Fraction of Missing Information) is a diagnostic for MCMC sampling, not a Bayes factor. Computing Bayes factors in PyMC3 is much more involved (e.g., using bridge sampling or path integration, which are beyond the simple `az.bfmi` call).
    *   The call `az.waic(traces[i], models[i])` is also problematic. `az.waic` and `az.loo` take `InferenceData` objects, not dictionaries of PyMC3 models. The `az.compare` function is the proper way to compare models.

**Completeness Issues:**
*   **Missing Value Handling:** The `dropna()` call is extremely simplistic and not ""comprehensive"" handling. The ground truth uses `SimpleImputer` and `ColumnTransformer`.
*   **Posterior Predictive Checks:** The code captures `az.from_pymc3(trace=traces[i])` into a list `ppc` but doesn't actually perform or plot a PPC (e.g., `az.plot_ppc`).
*   **Sensitivity Analysis:** Similar to PPC, `az.from_pymc3` is called into `sensitivity` but no actual sensitivity analysis (e.g., varying priors and comparing posteriors) is performed or presented.
*   **Credible Intervals:** While `az.hpd` is called, the output isn't printed or analyzed meaningfully beyond storage in a list. The `az.summary` function is typically used for this purpose to get a concise summary.
*   **Plots:** Plots are generated only for some trace variables, not for the complex relationships or model validations (PPC, GP fit, prior sensitivity) specifically requested.
*   **Priors:** While some priors are present, they are often vague (`mu=0, sigma=10`) or applied without proper scaling of predictors, which diminishes their informative power.
*   **PyMC3 instead of PyMC4:** The task allowed `PyMC3` or `PyMC4`. The generated code uses PyMC3, which is fine, but some of the imports (`theano.tensor`) suggest older PyMC3 versions or unnecessary explicit imports.

**Code Quality:**
*   **Imports:** Several unnecessary imports like `pykalman`, `ARIMA`, `LinearRegression`, `KFold`, `Isomap`, `r2_score`, `math` are present, indicating a copy-paste or lack of understanding of relevant libraries.
*   **Variable Naming:** Variable names are okay but could be more descriptive in places.
*   **Structure:** The code is structured section-wise as requested but the implementation within each section is severely lacking.
*   **Reproducibility:** `np.random.seed(0)` is set in some places but not consistently or at the very beginning.

In summary, the generated code only superficially addresses the requirements. The core statistical modeling and causal inference implementations are either incorrect, incomplete, or fundamentally misunderstanding the concepts requested. This code would not produce correct or interpretable results for the given task.",llama-3.3-70b-instruct,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.cluster import SpectralClustering
from hdbscan import HDBSCAN
from sklearn.manifold import TSNE
import umap
from sklearn.decomposition import Autoencoder
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
fr...",3,"The generated code attempts to address many parts of the prompt but fails significantly in correctness, completeness, and code quality.

**Correctness and Completeness Failures:**

1.  **Missing `matplotlib.pyplot` and `seaborn`:** The generated code doesn't include imports for plotting libraries, making any visualization aspects of the task either impossible or severely limited. The problem statement explicitly requires visualization through dimensionality reduction and other components.
2.  **`Autoencoder` Import and Usage:** The line `from sklearn.decomposition import Autoencoder` is incorrect. Scikit-learn does not have a built-in `Autoencoder` class. Autoencoders are typically implemented using deep learning frameworks like TensorFlow/Keras or PyTorch. The ground truth correctly uses `tensorflow.keras`. This is a critical functional error.
3.  **Feature Engineering (`Happiness Profile`, `Regional Characteristics`):** The feature engineering is extremely simplistic and likely nonsensical (`Life Ladder * Generosity`, `GDP per Capita * Healthy Life Expectancy`). It lacks the ""advanced"" aspect and thoughtful design present in the ground truth (e.g., interaction terms, ratios, dealing with corruption perception as an inverse). The ground truth also correctly handles feature engineering on the original features before scaling, then updates the scaled features for clustering.
4.  **Data Preprocessing:**
    *   `df.dropna()` is too aggressive. It removes all rows with *any* missing data, which can lead to significant data loss. The ground truth uses more robust median/mode imputation.
    *   The feature scaling is only applied to a small subset of features (`GDP per Capita`, `Social Support`, `Healthy Life Expectancy`, `Freedom to Make Life Choices`, `Generosity`). The ground truth correctly identifies and scales *all* numerical features that will be used for clustering, including the engineered ones and one-hot encoded regions. The generated code also re-uses string column names for indexing `df`, which is prone to errors if the dataset's column names don't exactly match `world-happiness-report-2021.csv`'s actual column names (e.g., ""Ladder score"" vs ""Life Ladder"", ""Logged GDP per capita"" vs ""GDP per Capita"").
5.  **Clustering Algorithm Input:** All clustering algorithms are applied to a fixed, small subset of features. This ignores the `Happiness Profile` and `Regional Characteristics` features that were supposedly engineered, and completely misses the one-hot encoded regional indicators in the ground truth which are crucial for ""country segmentation analysis"".
6.  **Optimal Clustering Parameters:**
    *   The `n_clusters` determination for K-Means simply finds the best silhouette score and sets `n_clusters` to `np.argmax(silhouette) + 2`. This only addresses K-Means and ignores optimal parameters for DBSCAN, HDBSCAN, GMM, etc., which often require different metrics (e.g., for density-based algorithms).
    *   The prompt specifically asks for ""optimal clustering parameters using silhouette analysis, gap statistic, elbow method, and stability analysis."" The generated code only implements silhouette for K-Means and completely misses the others. The ground truth highlights the conceptual complexity and partial implementation of these.
7.  **Consensus Clustering:** The proposed consensus clustering method (`max(set(cluster), key=cluster.count)`) is not a standard or robust consensus clustering approach. It simply takes the mode of the cluster labels for each country across algorithms, which doesn't build a co-association matrix or apply a robust meta-clustering step, as done in the ground truth. It also incorrectly assumes `consensus_labels` can be used to index `df` directly for `cluster_data = df[consensus_labels == cluster]`, as `consensus_labels` is a list, not a Series/array that can be used for boolean indexing on `df`.
8.  **Dimensionality Reduction:**
    *   The `Autoencoder` issue is repeated here.
    *   It mentions ""manifold learning"" but doesn't explicitly include a distinct manifold learning algorithm like Isomap, which the ground truth does.
    *   There's no visualization implemented for DR, which is a key part of ""visualization"" requirement.
9.  **Country Profiles:**
    *   `df[consensus_labels == cluster]` will fail because `consensus_labels` is a list, not a boolean array.
    *   The definition of ""country profiles"" is extremely basic and lacks the statistical significance testing (ANOVA) and effect size analysis (Eta-squared) explicitly requested and implemented by the ground truth.
10. **Anomaly Detection:** Only Isolation Forest is used. The prompt specifically asks for ""anomaly detection"" (plural), and the ground truth includes Isolation Forest, LOF, and One-Class SVM.
11. **Network Analysis:**
    *   Uses `networkx` but the `community.girvan_newman(G)` returns an iterator and doesn't actually assign communities or extract useful insights in the provided snippet.
    *   The similarity threshold `0.5` is arbitrary and not adaptive like the percentile approach in the ground truth. There's no calculation of network properties (degree centrality, betweenness centrality) or proper visualization.
12. **Association Rule Mining:**
    *   Applying `apriori` directly on continuous numerical features (`GDP per Capita`, etc.) without discretization (binning) is incorrect. Association rule mining requires categorical or binarized data. The ground truth correctly binarizes the features.
    *   The generated code also misses the `mlxtend.frequent_patterns` import for `apriori` and `association_rules`. (Edit: The import `from mlxtend.frequent_patterns import apriori, association_rules` is present at the top, so this specific point is incorrect). However, the use of `apriori` on continuous data without binning is still a major flaw.
13. **Recommendation System and Policy Insights:**
    *   The recommendation system is very basic, simply finding countries with similarity > 0.5. It lacks the nuance of recommending based on cluster membership *and then* similarity within the cluster, as implemented in the ground truth.
    *   The policy insights are extremely generic, just listing features where a country is below average. It completely misses the ""geopolitical validation"" aspect and the detailed comparative analysis between high/low happiness clusters.
    *   `df.columns` in `policy_insights` and `geopolitical_validation` and in the similarity computation for other sections likely includes the `Country` name and `Regional Indicator` which are not numerical features and should not be used in such comparisons.

**Code Quality Failures:**

1.  **Lack of Modularity/Structure:** The entire code is written as a single script, making it hard to read, maintain, and debug. The ground truth correctly uses a class `AdvancedHappinessAnalysis` to encapsulate the logic and state, which is much better practice for a complex task like this.
2.  **Repetitive Code:** The feature list `[['GDP per Capita', 'Social Support', 'Healthy Life Expectancy', 'Freedom to Make Life Choices', 'Generosity']]` is hardcoded and repeated excessively throughout the script, making it brittle and difficult to change. The ground truth defines a list of numerical features and uses it programmatically.
3.  **Hardcoded Parameters:** Cluster numbers (e.g., `n_clusters=5`) and thresholds (e.g., `eps=0.3`, `min_samples=10` for DBSCAN, `similarity > 0.5`) are hardcoded without any attempt at optimization or justification (other than `n_clusters` for K-Means, which is still limited). The ground truth tries to find optimal parameters dynamically.
4.  **No Error Handling/Robustness:** The code assumes ideal conditions (e.g., no missing values handled gracefully, specific column names exist). The ground truth includes `try-except` blocks and checks to ensure robustness.
5.  **Readability:** No comments explaining complex logic, very few print statements to indicate progress or results.

In summary, while the generated code tries to touch upon all the requirements, its implementation is largely superficial, functionally incorrect in several places (e.g., Autoencoder, ARM input), and lacks the depth, robustness, and best practices expected for ""advanced"" analysis and production-ready code.",llama-3.3-70b-instruct,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from statsmodels.tsa.statespace.sarimax import SARIMAX
from pmdarima import auto_arima
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
df = df[df['location'] == 'Germany']
df.set_index('date', inplace=True)

# Handle missing values
df['new_cases'] = df['new_cases'].ffill()
df['stringency_index'] = df['stringency_index'].ffill()
df['peop...",7,"The generated code correctly implements the core requirements of the task: data loading and filtering, SARIMA model training with exogenous variables, forecasting, and RMSE evaluation. It utilizes `auto_arima` for order selection and visualizes the results.

However, there are several areas for improvement:

1.  **Missing Value Handling:** The current missing value handling (`ffill()` only) is simplistic. If NaNs exist at the beginning of the series, `ffill` won't fill them. The ground truth demonstrates a more robust approach using `bfill()` as well, and even `interpolate()` as a fallback, followed by a final check and safeguard (`dropna`). This is critical for time series to prevent model errors.
2.  **Robustness and Error Handling:** The generated code lacks explicit error handling, such as checking if the file exists or if the filtered DataFrame for Germany is empty. The ground truth includes `try-except` for file loading and checks for data sufficiency (`len(data) < test_days`).
3.  **Clarity and Readability:** The ground truth code breaks down the task into clearly numbered sections, making it much easier to follow the logic and understand which part of the code addresses which requirement. The generated code is more monolithic.
4.  **Forecasting with Unseen Exogenous Variables:** The generated code performs `forecast()` directly using `test[['stringency_index', 'people_fully_vaccinated']]`. While this works, it implicitly assumes that these future exogenous variables are perfectly known during the forecasting horizon. In real-world scenarios, these would also need to be forecasted or assumed. The ground truth correctly uses `get_forecast` which is suitable for this.
5.  **Negative Forecasts:** Time series models can sometimes produce negative forecasts, especially for non-negative quantities like 'new_cases'. The ground truth explicitly handles this by setting negative forecasts to zero, which is good practice. The generated code does not.
6.  **Plotting Details:** The plot in the generated code is basic. The ground truth includes plotting the training data, adding a title with RMSE, axis labels, a grid, and `tight_layout()` for better presentation, making the plot more informative.
7.  **Warning Suppression:** While `suppress_warnings=True` is used in `auto_arima`, a general warning suppression (as in the ground truth) can make the output cleaner, especially from `statsmodels`.

The code is functional and largely correct for the prompt, but the areas mentioned above prevent it from being a truly robust and production-ready solution, which is important for ""expert code evaluation.""",llama-3.3-70b-instruct,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
from statsmodels.stats.multicomp import pairwise_tukeyhsd

# Load dataset
df = pd.read_csv('winequality-red.csv')

# Define quality groups
df['quality_group'] = pd.cut(df['quality'], bins=[0, 4, 6, 10], labels=['low', 'medium', 'high'])

# Perform statistical tests
features = df.columns.tolist()[:-2]  # ex...",4,"The generated code has several critical issues that make it largely incorrect and incomplete for the given task.

**Critical Errors/Incorrectness:**

1.  **Incorrect Normality Assumption Check:** The code checks normality for the *entire* feature column (`stats.shapiro(df[feature])`) instead of checking normality for each quality group (`low`, `medium`, `high`) separately. This is a fundamental misunderstanding of ANOVA assumptions. ANOVA and Kruskal-Wallis concern group differences, and the normality assumption applies *within* each group, not to the combined data.
2.  **Incorrect Kruskal-Wallis vs. ANOVA Logic:**
    *   If `shapiro_test.pvalue < 0.05` (meaning the *entire* feature is non-normal based on its faulty check), it uses Kruskal-Wallis.
    *   If `shapiro_test.pvalue >= 0.05` (meaning the *entire* feature is considered normal), it uses ANOVA.
    *   This logic is flawed due to the incorrect normality check.
    *   Furthermore, when using ANOVA, it tries to assign `kw_test = anova_table['F'][0]`, which is incorrect. ANOVA output is `anova_table['PR(>F)'][0]` for the p-value.
3.  **Missing Homogeneity of Variance Check (Levene's Test):** A critical assumption for ANOVA (after normality) is homogeneity of variances across groups. The generated code completely omits this check. Without it, even if normality were correctly checked, ANOVA could be inappropriately applied when Kruskal-Wallis might be more suitable.
4.  **Incorrect P-value Extraction and Significance Check:** The condition `if (not shapiro_test.pvalue < 0.05 and anova_table['PR(>F)'][0] < 0.05) or (shapiro_test.pvalue < 0.05 and kw_test.pvalue < 0.05):` is overly complicated and prone to error.
    *   When ANOVA is used, it references `anova_table['PR(>F)'][0]` for the p-value, which is correct for ANOVA.
    *   When Kruskal-Wallis is used, the code calculates `kw_test = stats.kruskal(...)` and then seems to try to use `kw_test.pvalue`. This part is conceptually correct for Kruskal-Wallis but the logic to get there is flawed.
5.  **Post-hoc Test Misapplication:**
    *   It uses `scikit_posthocs.posthoc_dunn` which requires `scikit_posthocs` to be installed. While `scikit_posthocs` is a valid library, the prompt did not list it as a required import, indicating it might not be a standard choice for this ecosystem. The ground truth used `stats.mannwhitneyu` with `multipletests` as a common alternative when `scikit_posthocs` isn't available or preferred.
    *   The `val_col` argument in `posthoc_dunn` is incorrect for the way `df[feature]` is passed as the first argument. It should be `posthoc_dunn(x=df[feature], g=df['quality_group'], ...)`.
    *   It prints the entire `dunn_test` DataFrame, which is less user-friendly for a summary than specific pairwise comparisons.
6.  **Missing Error Handling:** No handling for `FileNotFoundError` or other potential issues with data processing (e.g., empty groups for statistical tests).
7.  **Unclear and Inefficient Structure:** The core loop for statistical tests becomes very convoluted due to the incorrect logic for normality and test selection. The `kw_test` variable is overloaded to hold either Kruskal-Wallis results or ANOVA F-statistics, which is confusing.

**Completeness Issues:**

1.  **Missing Post-hoc P-value Summary:** The request asked to ""Summarize which features significantly differ between groups,"" and the ground truth provides a clear summary of which *pairwise* comparisons are significant. The generated code only prints the raw `Tukey` or `Dunn` tables, which don't provide a concise summary.
2.  **Lack of Intermediate Output/Logging:** The ground truth includes `print` statements to show which test is being applied, normality results, etc., which is crucial for understanding the analysis flow. The generated code lacks this.
3.  **No Clear Summary of Findings:** The final summary just lists significant features, but doesn't elaborate on *how* they differ (which quality groups are different), which is the essence of post-hoc analysis.

**Code Quality Issues:**

1.  **Variable Naming:** `kw_test` being used for both Kruskal-Wallis results and ANOVA F-statistic is poor practice.
2.  **Redundant Imports:** While `statsmodels.formula.api` and `statsmodels.stats.anova` *can* be used for ANOVA, `scipy.stats.f_oneway` is simpler and more direct for a basic one-way ANOVA as required here.
3.  **Magic Numbers:** `0.05` is hardcoded multiple times. A global `ALPHA` variable would be better.
4.  **Hardcoded Column Indexing:** `features = df.columns.tolist()[:-2]` is brittle. It assumes `quality` and `quality_group` are always the last two columns.

In summary, the generated code attempts to address all parts of the prompt but fails significantly in implementing the statistical logic correctly, particularly around assumption testing and the decision tree for choosing between ANOVA and Kruskal-Wallis.",llama-3.3-70b-instruct,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import roc_auc_score, f1_score
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
import shap
from datetime import datetime

# Load data
data = pd.read_csv(...",7,"The generated code addresses most of the requirements but has several issues that prevent it from achieving a higher score.

**Positive Points:**
*   **Data Loading and Parsing:** Correctly loads data and parses the 'date' column.
*   **Feature Engineering:** Implements most of the requested rolling averages, standard deviations, lag features, and 'days\_since\_first\_case' correctly.
*   **Missing Value Handling (Partial):** Fills NaNs for lag features but misses comprehensive handling for other potential NaNs.
*   **Model Training:** Uses XGBoostClassifier and GridSearchCV for hyperparameter tuning.
*   **Evaluation:** Calculates ROC-AUC and F1-score as requested.
*   **SHAP Explanation:** Implements SHAP for feature importance visualization.

**Areas for Improvement (Why it lost points):**

1.  **Incorrect Target Variable Definition:**
    *   The prompt specifies: ""A 'high\_daily\_death\_increase' is defined as a day where 'new\_deaths' for a 'location' is in the top 10% of its own historical 'new\_deaths' (**excluding days with zero new deaths**).""
    *   The generated code iterates through locations and correctly filters `new_deaths > 0` for percentile calculation. However, the final assignment `data.loc[(data['location'] == location) & (data['new_deaths'] > percentile), 'high_daily_death_increase'] = True` doesn't explicitly guarantee that `new_deaths > 0` *for the day being classified* alongside the percentile check. While the `> percentile` part implicitly handles this if the percentile is positive, it's less robust than the ground truth's explicit `(df['new_deaths'] > df['location_90th_percentile_deaths']) & (df['new_deaths'] > 0)`. More critically, iterating through unique locations and modifying the DataFrame in a loop like this is very inefficient for large datasets compared to vectorized pandas operations used in the ground truth.

2.  **Inadequate Handling of `people_fully_vaccinated` and `population`:**
    *   The prompt asked for a ratio: `'people_fully_vaccinated' / 'population'`. The generated code includes both `people_fully_vaccinated` and `population` as separate features, which is not what was requested. It also doesn't handle potential division by zero or inf/NaN for the ratio if it were to calculate it.

3.  **Missing Value Handling (General):**
    *   The code only explicitly handles NaNs for lag features. It does not address NaNs that might arise from rolling calculations (e.g., at the very start of a series where `min_periods` might not be met, though `min_periods=1` helps) or from original columns like `stringency_index`, `reproduction_rate`, `median_age`, etc., which often contain NaNs in real-world data. The prompt specifically asked for handling missing values from *feature engineering* and suggested filling with 0 or backfill. The ground truth uses `ffill().fillna(0)` which is a more robust strategy for time-series data.

4.  **Inefficient Feature Selection and Preprocessing Steps:**
    *   The scaling and encoding are done in separate steps *before* the train-test split, affecting the test set with training data statistics (data leakage). Also, the one-hot encoding creates new columns dynamically, and these aren't explicitly included in the `features` list initially.
    *   The prompt specifically requested the use of `ColumnTransformer` and `Pipeline` for preprocessing *within* the model pipeline, which is a best practice for preventing data leakage and ensuring consistent transformation. The generated code performs these steps manually and globally, which is less robust.

5.  **Incorrect Train-Test Split:**
    *   The prompt explicitly states: ""Evaluate using ROC-AUC and F1-score on a **time-based split (e.g., last 20% of data for testing)**."" The `train_test_split` with `shuffle=False` tries to provide a time-based split, but it relies on the data already being sorted chronologically. If the input data is not sorted by date *and* location, this split isn't truly time-based. The ground truth correctly sorts the data by date first to ensure a proper temporal split.

6.  **`GridSearchCV` `learning_rate` Range:**
    *   `'learning_rate': [0., 0.5, 1]` includes `0.`, which is generally not a useful learning rate as it means no learning will occur.

7.  **SHAP Feature Names and Integration:**
    *   SHAP requires feature names corresponding to the preprocessed data. The generated code passes `X_test` directly, which contains raw and manually scaled/encoded features. If the features were modified (e.g., scaled), `X_test` might not represent the actual input to the `best_model` correctly, or the feature names for SHAP might not be aligned with the final processed features (especially with one-hot encoding adding dimension). The ground truth carefully extracts feature names after preprocessing within the pipeline for correct SHAP interpretation.

8.  **Minor Code Quality:**
    *   Missing imports for `matplotlib.pyplot` and `warnings` if they were intended for visualization or warning suppression.
    *   Overuse of global variable `data` throughout. `df` is a more common convention for DataFrame.
    *   No print statements or comments about progress, making it harder to debug or understand.",llama-3.3-70b-instruct,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.regression.mixed_linear_model import MixedLM
import matplotlib.pyplot as plt

# Load data
def load_data(file_path):
    data = pd.read_csv(file_path)
    data['date'] = pd.to_datetime(data['date'])
    return data

# Clean and prepare data
def prepare_data(data):
    data['vaccination_percentage'] = data['people_fully_vaccinated'] / data['population']
    
    # Handle missing values
    data['reproduc...",3,"The generated code attempts to address the task but fails significantly in several key areas, particularly regarding the mixed-effects model implementation and missing value handling.

**Correctness (1/5):**
*   **Mixed-Effects Model:** The `MixedLM.from_formula` call uses `re_formula='~ location'`. This specifies random slopes for `location`, not random intercepts. More importantly, it doesn't correctly capture the nested random effects for 'location' within 'continent' as required by the prompt (`random intercepts for 'location' nested within 'continent'`). The ground truth uses `groups=df['continent_location_id'], re_formula=""1""` which correctly implements random intercepts for each unique (continent, location) combination. The generated code also imports `statsmodels.api as sm` but then directly uses `MixedLM` from `statsmodels.regression.mixed_linear_model`, which is not the `smf.mixedlm` suggested for formula-based models and can lead to confusion or errors for users expecting a formula interface.
*   **Missing Value Handling:** The imputation strategy (`fillna(x.median())` grouped by `location`) is too simplistic and not robust, especially for 'vaccination_percentage' which is derived from 'population' and 'people_fully_vaccinated'. The ground truth uses a more sophisticated approach: `ffill().bfill()` for 'population'/'people_fully_vaccinated' (as they are cumulative/slowly changing), followed by linear interpolation within groups for the main variables, and then a global median fallback. The generated code calculates `vaccination_percentage` *before* handling potential NaNs in `people_fully_vaccinated` or `population`, which can lead to NaNs in `vaccination_percentage` right away, and then only applies median imputation which can be less appropriate than interpolation for time-series data. It also lacks clipping `vaccination_percentage` at 100%.
*   **Interpretation of Results:** The `interpret_results` function only prints whether a parameter is significant or not, without indicating the direction or magnitude of the effect, which is a major part of interpretation. It also misinterprets `model_fit.reml_order_params` as ""Variance Components"" in the reporting function; `reml_order_params` lists the order of parameters, not the variance components. The actual variance components are `model_fit.cov_re` or `model_fit.random_effects_results.scale`.
*   **Model Assumptions Discussion:** While it mentions assumptions, the descriptions are generic and don't directly relate to how one would check them in the context of *this specific model* (e.g., using specific `statsmodels` outputs or standard plots), unlike the ground truth which provides specific advice.

**Code Quality (2/5):**
*   **Imports:** Uses `matplotlib.pyplot as plt` but doesn't use it. Imports both `sm` and `MixedLM` where only `MixedLM` is used directly in `fit_model`. The `smf.mixedlm` (formula API) is generally preferred for ease of use with formulas.
*   **Structure:** The use of functions (`load_data`, `prepare_data`, `fit_model`, `report_results`, `interpret_results`, `main`) is good for modularity.
*   **Missing Value Handling Details:** The `prepare_data` function calculates `vaccination_percentage` without first ensuring `population` and `people_fully_vaccinated` are clean and numeric, leading to potential `inf` or `NaN` values before imputation. It also has a `missing='raise'` in `fit_model`, meaning any remaining NaNs after the weak imputation would cause the model to fail, contrary to a robust solution.
*   **Lack of Error Handling:** No error handling for file not found or model convergence issues, which is crucial for data science scripts.

**Completeness (2/5):**
*   **Missing Value Handling:** Fails to implement robust missing value handling as requested (e.g., interpolation, median imputation *grouped by location* followed by fallback). The `fillna(x.median())` is a form of median imputation grouped by location, but it's not comprehensive enough for time-series data or derived variables.
*   **Fixed Effects Reporting:** Does not explicitly report standard errors and p-values alongside the coefficients in a clear, interpretable table. The output is fragmented.
*   **Random Effects Reporting:** Does not correctly report the variance components for the random effects. `reml_order_params` is meaningless here.
*   **Discussion of Model Assumptions:** Lacks depth in discussion and specific examples for checking.
*   **Interaction Term Interpretation:** Does not interpret the `stringency_index:vaccination_percentage` interaction term specifically, which is a key part of the model.

In summary, the generated code misses the core implementation of the nested random effects and provides a subpar approach to missing value handling and result interpretation.",llama-3.3-70b-instruct,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select the features for clustering
features = df[['elixirCost', 'Win Rate', 'Usage']]

# Standardize the features
scaler = StandardScaler()
standardized_features = scaler.fit_transform(features)

# Determi...",8,"The generated code correctly addresses most aspects of the task, demonstrating good understanding of K-Means clustering, standardization, PCA, and visualization.

**Strong Points:**
*   Correctly loads the dataset and selects the specified features.
*   Correctly standardizes the features using `StandardScaler`.
*   Includes both the elbow method (WCSS plot) and silhouette scores for determining K, which is an excellent addition beyond the minimum requirement.
*   Applies K-Means clustering with the chosen K (`silhouette_score` based selection).
*   Correctly performs PCA to reduce dimensionality to 2 components.
*   Generates a scatter plot of the PCA components, coloring points by cluster labels.
*   Properly imports all necessary libraries.

**Areas for Improvement (Why not a 9 or 10):**
*   **Feature Selection:** `features = df[['elixirCost', 'Win Rate', 'Usage']]` directly tries to select columns, which might fail if one of the columns is not present (though the prompt guarantees they exist). The ground truth's approach of explicitly defining `features` variable and then checking for existence is more robust.
*   **Robustness in K-Selection:** The line `kmeans = KMeans(n_clusters=k)` is missing `n_init=10` and `random_state`. While `n_init` defaults to `auto` in newer scikit-learn versions, setting it explicitly to `10` is a best practice and suppresses a warning. `random_state` ensures reproducibility of the K-Means results, which is crucial for benchmarking and consistent elbow/silhouette plots.
*   **K-Selection Logic:** Choosing `k = silhouette_scores.index(max(silhouette_scores)) + 2` is a good programmatic way to select K, but it implicitly assumes the highest silhouette score is always the ""best"" K, which isn't always the case (sometimes a slightly lower silhouette score with more interpretable clusters is preferred). Also, the plot for K selection (`plt.show()`) happens *before* this K is chosen, so the user can't manually override or see which K was picked on the plot. It might be better to display the plots, then choose K (either programmatically or comment on manual choice), and then proceed.
*   **Plot Labeling:** The PCA plot labels are just ""Principal Component 1"" and ""Principal Component 2"". Adding the explained variance percentage to these labels (as in the ground truth) makes the plot much more informative.
*   **Readability of PCA Plotting:** The loop `for i in range(k): plt.scatter(pca_features[labels == i, 0], pca_features[labels == i, 1], label=f'Cluster {i+1}')` is functionally correct but less concise than passing `c=labels` directly to `plt.scatter` and letting `matplotlib` handle the coloring and a colorbar (though a legend is good too). The `cmap` argument for `plt.scatter` can also improve the visual distinction of clusters.

Overall, it's a very good solution that competently solves the problem. The suggested improvements are mostly about enhancing robustness, reproducibility, and plot interpretability.",llama-3.3-70b-instruct,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select features and target
X = df[['elixirCost', 'Usage', 'rarity']]
y = df['Win Rate']

# Preprocess the rarity column using one...",7,"The generated code correctly addresses most of the task requirements: loading data, selecting features/target, splitting data, training a model, and evaluating it.

However, there are a few areas for improvement:

1.  **OneHotEncoding Column Names:** When `OneHotEncoder` is used, the resulting DataFrame from `pd.DataFrame(X_rarity_encoded)` loses the meaningful column names. This can make `X` harder to interpret and potentially lead to issues if the order of encoded columns isn't consistent (though less likely in this specific case). The `pd.get_dummies` method (used in ground truth) handles this more elegantly by default, creating columns like `rarity_Legendary`, `rarity_Epic`, etc.
2.  **Redundant Imports:** `LinearRegression` is imported but commented out and not used. `StandardScaler` is imported and used, but it wasn't requested by the prompt (which specifically asked for preprocessing on `rarity` and mentioned other common preprocessing steps for regression implicitly, but not specifically scaling features like `elixirCost` and `Usage`). While scaling is often good practice, it wasn't explicitly asked for and adds an unneeded step for this benchmark.
3.  **No `drop_first=True` for OneHotEncoder:** The `OneHotEncoder` by default includes all categories without `drop_first=True`, which can lead to multicollinearity, especially with `LinearRegression` (though `RandomForestRegressor` is less sensitive to it). `pd.get_dummies` with `drop_first=True` is generally better practice for categorical features in regression to avoid this.
4.  **No Error Handling/Missing Value Handling:** The generated code lacks explicit error handling for file not found or checks for missing columns, nor does it handle potential `NaN` values in the selected features/target. While the prompt didn't explicitly ask for this, robust code would include it (as seen in the ground truth).
5.  **Clarity in Output:** The ground truth includes more descriptive print statements (e.g., ""--- Model Evaluation Results ---"") which improve readability, though this is a minor aesthetic point.

Overall, the code runs and produces the required output for the given task, but could be more robust, cleaner, and better aligned with best practices for specific preprocessing steps as demonstrated in the ground truth.",llama-3.3-70b-instruct,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# Identify the unique categories in the 'rarity' column
rarity_categories = data['rarity'].unique()
print(""Unique categories in 'rarity' column: "", rarity_categories)

# State the null and alternative hypotheses
print(""\nNull Hypothesis: The mean 'Win Rate' is the same across all 'rarity' categories."")
print(""Alternative Hypothesis: The mean 'Win Rate' is not the same across al...",6,"The generated code correctly addresses most of the requirements: loading data, identifying unique categories, stating hypotheses, performing ANOVA, printing F-statistic and p-value, and interpreting the result.

However, there are critical issues that significantly impact its robustness and correctness, leading to a lower score:

1.  **Data Cleaning and Robustness (Major Flaw):** The generated code *completely lacks* error handling for the `Win Rate` column not being numeric, or for missing values (`NaN`) in either `Win Rate` or `rarity`. The ground truth code correctly uses `pd.to_numeric(errors='coerce')` and `dropna()`. Without this, if the 'Win Rate' column contains non-numeric strings or NaN values, the `f_oneway` function will likely raise an error or produce incorrect results. This is a common real-world data scenario that robust code must handle.

2.  **Handling Insufficient Data per Group (Major Flaw):** The `scipy.stats.f_oneway` function requires numerical input. While it can handle empty arrays (by ignoring them), it's crucial to ensure that each group provided to ANOVA has enough meaningful data points (ideally at least 2). The ground truth code explicitly checks that `len(group_win_rates) >= 2` before including a group. The generated code passes `data.loc[data['rarity'] == category, 'Win Rate']` directly, which could pass Series with all NaNs (if `Win Rate` wasn't cleaned) or Series with insufficient non-NaN values, potentially leading to errors or warnings from `f_oneway` that are not handled. This makes the code less robust for varying dataset quality.

3.  **Missing File Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice for any data loading operation. The generated code does not.

4.  **Minor Output Formatting:** The F-statistic and p-value are printed with full precision, whereas the ground truth formats them to a reasonable number of decimal places (`.4f`), which is generally better for readability.

To improve, the generated code needs to incorporate robust data cleaning (handling non-numeric and missing values) and ensure that groups passed to ANOVA have sufficient valid data, similar to the ground truth. Adding file error handling would also be beneficial.",llama-3.3-70b-instruct,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
df = pd.read_csv('iris.csv')

# Define features X and target y
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm...",8,"The generated code correctly addresses almost all requirements of the task. It loads the data, defines features and target, encodes the target, splits the data, trains the specified models, and evaluates them using the required metrics. It also correctly identifies the best model based on F1-score.

Strong points:
- Correct selection of features and target variables.
- Proper use of `LabelEncoder` for the target variable.
- Correct data splitting with `test_size=0.2` and `random_state=42`.
- Accurate implementation of all three specified classifiers with correct parameters.
- Correct calculation and printing of accuracy, precision (macro), recall (macro), and F1-score (macro).
- Correct logic for identifying the best model based on the highest F1-score.
- All necessary imports are included.

Areas for improvement (leading to a score of 8 instead of 9 or 10):
- **Missing `stratify` in `train_test_split`**: The ground truth uses `stratify=y_encoded`. While not explicitly requested in the prompt, stratifying the target variable during splitting is good practice, especially with potentially imbalanced datasets, to ensure that the class distribution in the train and test sets is similar to the original dataset. The Iris dataset is balanced, so its absence doesn't break functionality, but it's a best practice for classification tasks.
- **Logistic Regression `max_iter`**: The ground truth includes `LogisticRegression(max_iter=200)`. While the default `max_iter=100` might work for Iris, sometimes Logistic Regression might fail to converge without increasing `max_iter` when not otherwise specified. The prompt asked for ""default parameters"" for Logistic Regression, so not including `max_iter` technically adheres to that, but the ground truth's inclusion is a more robust approach.
- **Error Handling**: The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code assumes the file will always be present. While not explicitly asked for, it's a detail that adds to code quality.",llama-3.3-70b-instruct,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# Load dataset
df = pd.read_csv('iris.csv')

# Select features
selected_features = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Standardize features
scaler = StandardScaler()
standardized_features = scaler.fit_transform(selected_features)

# K-Means clustering
kmeans...",9,"The generated code correctly performs all the required steps: loading the data, selecting and standardizing features, applying K-Means (with correct parameters), performing PCA, and generating two scatter plots with appropriate coloring. The titles for the plots are also correct.

Areas for minor improvement (to achieve a perfect 10):
1.  **Label Encoding for Species:** The generated code manually creates `species_map` and `true_labels` to numerically encode the 'Species' column. While functional, using `sklearn.preprocessing.LabelEncoder` as in the ground truth is a more robust and standard way to handle categorical encoding in scikit-learn related tasks. This was explicitly mentioned in the task requirements: ""after encoding `Species` to numerical values for coloring"".
2.  **Plot Enhancements (Optional but good practice):** The ground truth code includes color bars for both plots and uses `cmap='viridis'` for consistency and better visual representation, and `alpha=0.8` for potential overlap visualization. While not strictly required by the prompt, these improve the visualization quality. The scatter plots could also optionally have a `grid(True)`.
3.  **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError`. While the prompt specifically stated to ""assume the CSV file is in the current directory"", adding robust file handling is generally a good practice for real-world scenarios. However, given the explicit assumption, this is a very minor point.

Overall, the code is very good and effectively solves the problem. The issues are minor points of polish or specific library usage preference rather than functional correctness.",llama-3.3-70b-instruct,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# Load the dataset
iris = pd.read_csv('iris.csv')

# Create two new features: PetalArea and SepalArea
iris['PetalArea'] = iris['PetalLengthCm'] * iris['PetalWidthCm']
iris['SepalArea'] = iris['SepalLengthCm'] * iris['SepalWidthCm']

# Define features X using the original four numerical features and the two newly ...",10,"The generated code perfectly matches the requirements. It correctly loads the dataset, creates the specified features, defines `X` and `y` as required, encodes the target, initializes the `GradientBoostingClassifier` with the right parameters, performs 5-fold cross-validation, and reports the mean accuracy. The code structure is clear and complete, with all necessary imports.",llama-3.3-70b-instruct,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load dataset
df = pd.read_csv('heart.csv')

# Define features and target
X = df[['Age', 'Sex', 'ChestPainType', 'Re...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong points:**
- Correctly loads the dataset.
- Correctly defines `X` and `y` based on the specified columns.
- Implements one-hot encoding for categorical features and StandardScaler for numerical features using `ColumnTransformer` and `Pipeline`, which is the correct and robust way to handle preprocessing.
- Correctly identifies categorical and numerical features for preprocessing.
- Splits data into training and test sets with the correct `test_size` and `random_state`.
- Trains a `RandomForestClassifier` with the specified `n_estimators` and `random_state`.
- Evaluates the model using all requested metrics: accuracy, precision (macro), recall (macro), F1-score (macro), and ROC-AUC, displaying them with appropriate formatting.
- The use of a `Pipeline` is excellent for preventing data leakage and streamlining the workflow.

**Areas for minor improvement:**
- The prompt specified that numerical features should be determined by ""all columns except `HeartDisease`"" and then listed the features. The generated code explicitly lists numerical features which is correct for this specific problem, but a more dynamic approach (like in the ground truth, `[col for col in features if col not in categorical_features]`) would be more robust if the feature list were to change or contain more complex types, though for the given problem, the explicit list is fully functional.
- The prompt mentioned including proper imports and assumed the CSV file in the current directory, which is handled implicitly by the `pd.read_csv`, but the ground truth adds more robust error handling (`try-except FileNotFoundError`), which is good practice for production code. This is a very minor point and not a direct requirement of the ""code functionality"" as such.

Overall, the generated code is highly successful in meeting the requirements.",llama-3.3-70b-instruct,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('heart.csv')

# Select numerical features for clustering
numerical_features = df[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']]

# Standardize features
scaler = StandardScaler()
standardized_features = scaler.fit_transform(numer...",9,"The generated code correctly performs all the required steps: loading data, selecting and standardizing features, determining K using both WCSS and Silhouette scores, applying K-Means with the specified parameters, performing PCA, and visualizing the clusters.

One minor area for improvement is in the determination of `best_k`. The generated code chooses the `best_k` based *solely* on the highest silhouette score: `best_k = silhouette_values.index(max(silhouette_values)) + 2`. While this is a valid approach, the task explicitly asked to ""Use the elbow method (plotting Within-Cluster Sum of Squares - WCSS) and silhouette scores to justify your choice of K."" The ground truth code plots both and then makes a *chosen* K value (e.g., `chosen_k = 3`) which implies a more subjective decision based on the visual inspection of both plots, not just an algorithmic selection based on silhouette scores. For data understanding tasks, particularly related to clustering, visual inspection and human interpretation of the elbow and silhouette plots often play a crucial role in deciding K.

Additionally, the legend labels in the final PCA plot are `f'Cluster {i+1}'` instead of `f'Cluster {i}'`, which might be slightly misleading if one correlates `i` directly to the cluster index. But this is a very minor stylistic point.

Overall, the code is well-structured, uses correct libraries, and fulfills most requirements.",llama-3.3-70b-instruct,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
import numpy as np

# Load data
df = pd.read_csv('winequality-red.csv')

# Create binary target variable
df['good'] = np.where(df['quality'] >= 7, 1, 0)

# Define features and target
X = df.drop(['quality', 'good'], axis=1)
y = df['good']

...",8,"The generated code generally addresses all the requirements of the task.

**Strong Points:**
- Correctly loads the data.
- Correctly creates the binary target variable using `np.where`.
- Properly separates features (X) and target (y).
- Implements `StandardScaler` for feature scaling.
- Correctly splits data into training and testing sets.
- Trains a logistic regression model.
- Evaluates the model using accuracy, F1-score, and classification report.
- Identifies and reports feature importance using model coefficients.

**Areas for Improvement to achieve a higher score:**
- **Missing Value Handling:** While the prompt asks for handling missing values ""if any"", the generated code merely prints their count and comments out the actual imputation line. The Ground Truth provides a more robust, although simple (median), imputation strategy that would execute if missing values were detected. While `winequality-red.csv` typically has no missing values, a complete solution should include a working handler.
- **Scaling and Train-Test Split Order:** The code scales the *entire* dataset (`X_scaled = scaler.fit_transform(X)`) before splitting into train and test sets (`train_test_split(X_scaled, y, ...)`). This is a common mistake and leads to ""data leakage"" because the test data influences the scaling parameters (mean and standard deviation) learned by the scaler. The scaler should **only** be `fit` on the training data (`X_train`) and then `transform` both the training and test sets. The Ground Truth correctly implements this by fitting on `X_train` and then transforming both `X_train` and `X_test`.
- **Reproducibility for `LogisticRegression`:** The `LogisticRegression` model is initialized without a `random_state`. While `solver='liblinear'` is often deterministic, it's good practice to set `random_state` for algorithms that might have stochastic elements, ensuring full reproducibility.
- **Clarity of Feature Importance Output:** The output for feature importance is just a list of feature-value pairs. While correct, the Ground Truth formats it into a pandas DataFrame, which is more readable and includes absolute coefficients, making the ranking clearer and more directly comparable to the coefficients. The interpretation provided in the Ground Truth is also helpful for a user.
- The generated code doesn't include the suggested `print` statements for tracking progress, which are useful for debugging and understanding flow.",llama-3.3-70b-instruct,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

# Load the dataset
df = pd.read_csv('heart.csv')

# Box plot for Oldpeak distribution by HeartDisease
plt.figure(figsize=(8,6))
sns.boxplot(x='HeartDisease', y='Oldpeak', data=df)
plt.title('Distribution of Oldpeak by HeartDisease')
plt.show()

# Logistic regression with Oldpeak as independent variable
X = df[['Oldpeak']]
y = df['HeartDisease...",7,"The generated code correctly addresses most of the task requirements but has some areas for improvement regarding completeness and robustness.

**Strengths:**
*   Correctly loads the dataset.
*   Successfully generates the box plot as requested.
*   Performs logistic regression with `Oldpeak` and extracts the coefficient and odds ratio.
*   Correctly discretizes `Oldpeak` into three quantile bins using `pd.qcut`.
*   Calculates the proportion of `HeartDisease=1` within each bin correctly.

**Areas for Improvement (leading to a score of 7):**
1.  **Interpretation of Odds Ratio:** The ground truth provides a more detailed interpretation of the odds ratio, explaining what `exp(coefficient)` means for a one-unit increase in `Oldpeak` and how it translates to a percentage increase/decrease in odds. The generated code only prints the odds ratio without interpretation. This was explicitly asked for in requirement 3.
2.  **Visualization of Proportions:** The task asked to ""Calculate and display the proportion..."". While the code calculates it, the last plot (`sns.countplot`) shows counts, not the *proportion* of `HeartDisease=1` within each bin. The ground truth correctly plots the proportions. Creating a plot of proportions (e.g., using a bar plot of the `proportions` DataFrame) would be more aligned with the request.
3.  **Error Handling and Robustness:** The ground truth includes error handling for `FileNotFoundError` and a `try-except` block for `pd.qcut` which can fail if `Oldpeak` has too many identical values, suggesting an alternative binning method. The generated code lacks these robustness improvements.
4.  **Plot Labels and Titles:** While the box plot has a title, it's missing x and y labels, and customized xticks which were present in the ground truth for better readability. The count plot also lacks detailed labels.

To achieve a higher score, the generated code should:
*   Add the detailed interpretation of the logistic regression coefficient/odds ratio.
*   Change the final plot to accurately display the *proportions* of `HeartDisease=1` in each bin, rather than counts.
*   Consider adding basic error handling for file loading.
*   Improve plot aesthetics with more descriptive labels and titles.",llama-3.3-70b-instruct,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

# Load data
data = pd.read_csv('spotify_2023.csv')

# Convert streams to numeric and drop rows with NaN
data['streams'] = pd.to_numeric(data['streams'], errors='coerce')
data = data.dropna(subset=['...",7,"The generated code correctly addresses most of the requirements of the task, including loading data, converting streams, creating the target variable, selecting features, training a Logistic Regression model, and evaluating it using F1-score (macro), ROC AUC, and confusion matrix.

However, there is a significant issue with the one-hot encoding step that prevents the model from being built correctly. When `pd.concat` is used to combine `selected_features` and `encoded_df`, the indices do not align, leading to `NaN` values for the `encoded_df` part. This is because `selected_features` retains the original DataFrame's index, while `encoded_df` gets a new 0-based index. Consequently, `X` (combined_features) contains many NaN values, which `LogisticRegression` cannot handle without imputation or explicit removal, leading to an error or incorrect model training. The ground truth avoided this by performing `get_dummies` on the subset `X` directly after it was created.

Here are specific points for improvement:
*   **One-Hot Encoding and Concatenation Issue**: The main flaw is how `encoded_df` is created and concatenated. `OneHotEncoder` by default returns a numpy array, and when converted to DataFrame, it gets a new 0-based index. When concatenated with `selected_features` which retains the original `data` DataFrame's index, `NaN` values are introduced where indices don't match. This can cause `LogisticRegression` to fail or produce incorrect results. The correct approach is to either reindex `encoded_df` to match `selected_features` or, as demonstrated in the ground truth, use `pd.get_dummies` directly on the DataFrame from which features are being selected, which handles index alignment automatically.
*   **Missing `random_state` for Logistic Regression**: While `max_iter` is set, `random_state` is missing for `LogisticRegression`. While not strictly required for Logistic Regression (it's often more relevant for models with randomness in their training, like SGDClassifier or tree-based models), including it (as in the ground truth) promotes reproducibility in the rare cases where solver algorithms might involve randomness (though for default 'lbfgs' it typically doesn't matter much).
*   **Output Clarity**: The output prints are minimal, just the scores and the confusion matrix. Adding descriptive labels for the confusion matrix (like in the ground truth) would improve clarity for the user.
*   **Error Handling**: The ground truth includes `try-except` for file loading, which is good practice. The generated code lacks this.",llama-3.3-70b-instruct,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load data
data = pd.read_csv('spotify_2023.csv')

# Select audio features and drop rows with any NaNs
selected_features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%', 
                     'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm']
data = data[selected_features].dropna()

# Stan...",7,"The generated code correctly addresses most of the requirements but has a few areas for improvement and minor inaccuracies.

**Strengths:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Data Loading:** Loads the `spotify_2023.csv` file.
*   **Feature Selection and NaN Handling:** Correctly selects the specified audio features and drops rows with NaNs using `dropna()`.
*   **Standardization:** Applies `StandardScaler` correctly.
*   **K-Means Clustering:** Applies K-Means with the specified parameters (K=4, random_state=42, n_init='auto').
*   **PCA:** Performs PCA to 2 components with `random_state=42`.
*   **Visualization:** Creates a scatter plot of the two principal components colored by cluster labels.
*   **Mean Calculation:** Attempts to calculate and display mean values for original features per cluster.

**Areas for Improvement / Minor Inaccuracies:**

1.  **Original Data for Mean Calculation:** The most significant issue is in calculating the cluster means. When `data = data[selected_features].dropna()` is executed, `data` is overwritten. Then `standardized_data = scaler.fit_transform(data)` is performed, and `cluster_labels` are generated based on `standardized_data`. However, when calculating `cluster_data = data.iloc[cluster_labels == i]`, the `data` DataFrame, while containing the original, unstandardized values, has *lost its original index* after the `dropna()` operation if `dropna()` removed rows that were not contiguous. The `cluster_labels` are `numpy` array indices from `standardized_data`. This mismatch could lead to incorrect mean calculations if the indexing doesn't perfectly align after `dropna()`. The ground truth code correctly handles this by creating a `df_original_for_analysis` *before* standardization and then uses the `cluster_labels` to group this original, unstandardized data.
2.  **Robustness of File Loading:** The generated code doesn't include a `try-except` block for `FileNotFoundError` as seen in the ground truth, making it less robust if the file is not present.
3.  **Visualization Details:**
    *   The plot title is a bit generic (`K-Means Clustering on Spotify Audio Features`) compared to the ground truth's more descriptive title (`K-Means Clusters (K=4) on PCA-Reduced Spotify Audio Features`).
    *   It lacks specific plot enhancements like `cmap`, `s`, `alpha`, and a color bar, which improves readability and presentation of the clusters.
4.  **Output Clarity for Cluster Means:** The output for cluster means is less structured. It prints ""Cluster X Mean Values:"" followed by `cluster_data.mean()`, which is a Series. While functional, it doesn't present a nicely formatted table like `groupby().mean()` does.
5.  **Handling of `n_init='auto'`:** While `n_init='auto'` is used, the generated code doesn't mention the deprecation warning that might arise if run with older scikit-learn versions, which `random_state=42` resolves for consistency. The ground truth implicitly handles this by specifying `n_init='auto'` which became the default in `sklearn` version 1.4. In older versions, a warning might be raised or the default `n_init=10` would be used if `'auto'` was not available. However, for a generic solution, `n_init='auto'` is the correct parameter as requested.

Overall, the code follows the logic well but misses some crucial detail in data handling for the final analysis step and could improve plot presentation and error handling.",llama-3.3-70b-instruct,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('spotify_2023.csv')

# Convert `streams` to numeric and coerce errors to NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# Filter for songs with `streams` greater than 10,000,000 and where `released_year` is between 2010 and 2023, inclusive
df = df[(df['streams'] > 10000000) & (df['released_year'] >= 2010) & (df['released_year'] <= 2023)].dropna(subset=['streams'])

# Group the filtered...",9,"The generated code successfully completes almost all aspects of the task.

**Strong Points:**
- **Correctness:** It correctly loads the data, converts 'streams' to numeric, filters by streams > 10M and released_year between 2010 and 2023. It correctly performs the `dropna` on `streams` after filtering (though the order of operations in the filter chain could be slightly optimized for clarity).
- **Functionality:** It correctly groups by `released_year` and calculates the average `danceability_%`, `energy_%`, and `valence_%`.
- **Plotting:** It generates three separate line plots with correct titles and axis labels.

**Minor Areas for Improvement (preventing a perfect 10):**
- **Order of Operations in Filtering:** The generated code applies the `dropna(subset=['streams'])` *after* the `streams` and `released_year` filtering. While it effectively achieves the desired outcome (dropping NaNs from `streams` for the *filtered* dataset), the ground truth's approach of dropping NaNs post-conversion but *before* the numerical `streams` filter is slightly more logical and potentially more robust if the intent was to ensure *all* `streams` comparisons are on valid numbers. In this specific case, it doesn't break the logic because the comparison `df['streams'] > 10000000` on a NaN will result in `False`, so rows with NaN streams would be implicitly excluded anyway. However, explicitly dropping NaNs first for clarity and robustness is often preferred.
- **Error Handling (Optional but good practice):** The ground truth code includes basic error handling for `FileNotFoundError` when loading the CSV, which is a good practice for robust scripts. The generated code lacks this.
- **Plotting Details:** While the plots are correct, the x-axis tick labels are not rotated, which can lead to overlapping labels for the specific `released_year` range, especially if there were many years. The ground truth also uses a more compact subplot layout (1 row, 3 columns) which might be better for comparison of features side-by-side, though the 3-row layout is also acceptable. The ground truth also adds markers and grids to the plots, enhancing readability. These are minor stylistic choices.

Overall, the code is very good, directly solving the core requirements of the task. The points for improvement are mostly about robustness, clarity, and minor plotting enhancements rather than functional deficiencies.",llama-3.3-70b-instruct,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder

# Load dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean Likes, Dislikes, and Video views columns
df['Likes'] = df['Likes'].str.replace(',', '').astype(float)
df['Dislikes'] = df['Dislikes'].str.replace(',', '').astype(float)
df['Video views'] = df['V...",7,"The generated code correctly addresses most of the task requirements but has a critical error in handling the `published` column and a less robust approach for one-hot encoding compared to the ground truth.

**Strong Points:**
*   Correctly loads the dataset.
*   Correctly identifies and cleans `Likes`, `Dislikes`, and `Video views` by removing commas, converting to float, and filling NaNs with 0.
*   Correctly fills missing `Category` values with 'Unknown'.
*   Correctly performs train-test split with the specified `random_state`.
*   Correctly trains a Linear Regression model.
*   Correctly evaluates and reports R2 and MAE.
*   Includes necessary imports.

**Areas for Improvement (leading to score of 7):**

1.  **Handling `published` column:** The prompt explicitly states to treat the `published` column (year) as a numeric feature. The generated code `df['published'] = pd.to_numeric(df['published'])` attempts to convert the entire `published` string column to numeric directly. This will fail because `published` contains full dates and times, not just years or numbers. The ground truth correctly converts `published` to datetime objects first and then extracts the year, which is the correct approach for this task. This is a critical functional error.
2.  **One-Hot Encoding robustness**: While `OneHotEncoder` is used, `encoder.get_feature_names_out()` with `df[['Category']]` means the column names will be `Category_value1`, `Category_value2`, etc. This is fine. However, `pd.get_dummies` (used in ground truth) is often simpler for direct DataFrame integration, especially for a single column, as it directly creates new columns and concatenates them. The ground truth also explicitly adds `dtype=int` to `pd.get_dummies` which ensures integer representation for the OHE columns, which is good practice. The generated code's `category_df` has float dtype by default from `OneHotEncoder`.
3.  **Error Handling**: The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. While not explicitly asked, it improves robustness.
4.  **Clarity of Feature Selection**: When constructing `X`, the line `X = df[['Dislikes', 'Video views', 'published'] + list(category_df.columns)]` implicitly assumes `published` is a numeric column, which as noted earlier, is not correctly processed. Also, it's slightly less explicit than creating a list of numeric features and then concatenating with the OHE features.

The critical issue with the `published` column processing prevents the code from running successfully or producing meaningful results as intended by the task description regarding the year feature.",llama-3.3-70b-instruct,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
youtube_trends = pd.read_csv('youtube_trendings.csv')

# Clean the 'Video views' column
youtube_trends['Video views'] = youtube_trends['Video views'].str.replace(',', '').astype(float)
youtube_trends['Video views'] = youtube_trends['Video views'].fillna(0).astype(int)

# Ensure the 'published' column is numeric
youtube_trends['published'] = pd.to_numeric(youtube_trends['published'])

# Group the data by year and ca...",7,"The generated code successfully tackles most of the requirements but has a few critical issues.

**Strengths:**
*   **Correct Libraries:** Uses `pandas` and `matplotlib.pyplot` as required.
*   **Data Loading:** Correctly loads the CSV file.
*   **Video Views Cleaning:** Successfully removes commas, converts to float, fills NaNs with 0, and then converts to int. This part is well done.
*   **Grouping and Aggregation:** Correctly groups by year and calculates the mean of 'Video views' and the count of unique 'title' (assuming 'title' is the `Video` title column, which is a common naming convention for such datasets).
*   **Plotting:** Creates two subplots and plots the correct trends on each.
*   **Labels and Titles:** Adds appropriate titles and axis labels to both plots.
*   **Layout:** Uses `fig.tight_layout()` for better presentation.

**Areas for Improvement/Issues:**

*   **`published` Column Handling:** The most significant error is the handling of the `published` column. The prompt states the `published` column (year) should be treated as numeric. The dataset typically has a `publishedAt` column which is a full timestamp string (e.g., '2023-01-15T12:30:00Z'), and the `published` refers to the year from that. The generated code directly converts `youtube_trends['published'] = pd.to_numeric(youtube_trends['published'])`. If the original `published` column is indeed just the year (e.g., `2018`), this might work. However, if it's `publishedAt`, this will cause errors unless `published` is a pre-extracted year column. The ground truth correctly assumes `publishedAt` and extracts the year. This is a critical point that could lead to a crash or incorrect data based on the dataset's actual `published` column content. Assuming `publishedAt` exists and `published` does not, this step is incomplete.
*   **Video Column Name:** The prompt specifies ""total number of unique videos (count of unique `Video` titles)"". The generated code uses `'title': 'nunique'`. While `title` is a very common column name for video titles, the prompt specifically mentioned `Video` as the column, which should be strictly adhered to. If the dataset column is `Video`, the code will fail or produce incorrect results.
*   **`Video views` type after fillna:** The prompt asked for `Video views` to be converted to numeric and fill NaNs. The generated code converts to `float` then `int`. While `int` works for views, `numeric` implies it could be `float` too. The result is fine, but it's a minor detail.
*   **Explicit column names:** The prompt uses `Video` and `Video views` (with space and capitalization). The generated code uses `title` and `Video views`. Consistency with the prompt (i.e., `Video` for titles) would be ideal.

To improve, the code needs to explicitly convert a `publishedAt` column to a year column using `pd.to_datetime` and `.dt.year`, and ensure it's using the correct column name for video titles (e.g., `Video` instead of `title` if `Video` is the specified column in the dataset schema).",llama-3.3-70b-instruct,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
import nltk
from nltk.corpus import stopwords

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Create a new feature 'title_word_count' representing the number of words in each title
df['title_word_count'] = df['Video'].apply(lambda x: len(str(x).split()))

# Clean the 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '').astype(float)
df['Video views'] = df['Video views'].fillna(0)

...",7,"The generated code largely addresses the requirements but has several areas for improvement, particularly regarding robustness, error handling, and efficiency.

**Strengths:**
*   **Correctness in core logic:** It correctly implements the `title_word_count`, cleans `Video views`, calculates correlation, and identifies top common words.
*   **Proper imports:** All necessary libraries are imported.
*   **Clear variable names:** Variable names are descriptive.

**Areas for Improvement:**
*   **Missing Error Handling (File Not Found):** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is crucial for robustness. The generated code lacks this.
*   **NLTK Download Handling:** The `nltk.download('stopwords')` call should ideally be wrapped in a `try-except` block with `nltk.data.find()` as in the ground truth, to prevent repeated downloads and handle cases where it might fail.
*   **""Video views"" Cleaning:** The conversion `astype(float)` without `errors='coerce'` can lead to errors if the `str.replace(',', '')` doesn't result in a perfectly numeric string (e.g., if there are non-numeric characters remaining). `pd.to_numeric(..., errors='coerce')` is more robust.
*   **Date Filtering:** The `df['published'].apply(lambda x: int(x.split('-')[0]))` for filtering years is less robust and potentially less efficient than converting the 'published' column to datetime objects using `pd.to_datetime` and then filtering by `dt.year`, as done in the ground truth. This approach handles various date formats more gracefully and is standard practice.
*   **Word Processing Robustness:** In the title processing, `cleaned_title.split()` might create empty strings if there are multiple spaces after punctuation removal. The ground truth adds `.isalpha()` to filter out such empty strings or numbers that might remain (though numbers should be removed by `[^\w\s]`). Adding `and word.isalpha()` to the list comprehension for words is a good addition.
*   **Output Formatting:** The output for correlation and most common words is functional, but the ground truth's formatting is slightly more informative (e.g., specifying the correlation type, precise correlation value formatting, and count alongside the word).

**Completeness:**
The code addresses all the requirements, but some implementations are less robust or efficient than the ground truth. The primary missing elements are error handling for file loading and the NLTK download, and the less robust handling of `Video views` cleaning and date parsing.",llama-3.3-70b-instruct,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score
import re

# Load data
data = pd.read_csv('mobiles_2025.csv')

# Clean 'Launched Price (USA)' and create binary target 'is_expensive_usa'
data['Launched Price (USA)'] = data['Launched Price (USA)'].apply(lambda x: re.sub('USD |,', '', x) if pd.no...",6,"The generated code attempts to address all requirements of the task. However, there are several issues that reduce its correctness and robustness, particularly around data cleaning, NaN handling, and feature selection.

**Key Issues:**

1.  **`is_expensive_usa` target creation:**
    *   The ground truth correctly handles the `is_expensive_usa` target creation by setting it to 1 if `price > 1000`, 0 if `price <= 1000` (and `price` is not NaN), and `np.nan` if the original price was unparseable. It then `dropna` based on `is_expensive_usa`, ensuring that only rows with a clearly defined target remain.
    *   The generated code uses `np.where(data['Launched Price (USA)'] > 1000, 1, 0)`. This directly sets `0` for any `NaN` values in `Launched Price (USA)` (since `NaN > 1000` is `False`). This is incorrect as `NaN` prices should lead to `NaN` target values, which are then dropped. This means `is_expensive_usa` effectively never contains `NaN`s, making `data.dropna(subset=['is_expensive_usa'], inplace=True)` ineffective for truly unparseable price entries.
    *   The prompt specifically asks to ""Drop rows where `is_expensive_usa` is NaN"". The generated code's logic makes this step moot for the intended purpose.

2.  **Camera Feature Cleaning:**
    *   The prompt explicitly asks for ""extract the first numerical MP value (default 0 if none)"".
    *   The generated code uses `re.search(r'\d+', str(x)).group()`, which extracts *any* digit sequence, not specifically ""MP"" values. For example, ""12 MP"" would be correctly parsed, but ""12.5 MP"" would become ""12"", and ""100 mm"" would become ""100"" which is incorrect for camera MP. The ground truth correctly uses `r'(\d+(\.\d+)?)\s*MP'` to extract floating-point MP values, which is much more precise. After extraction the generated code also converts to numeric using `pd.to_numeric(..., errors='coerce')`, which is good, but the initial regex is flawed.

3.  **One-Hot Encoding and Feature Selection:**
    *   The generated code uses `OneHotEncoder` separately, creating `encoded_df` as a new DataFrame.
    *   Then, it defines `X` with the numerical features and *then* tries to `pd.concat([X, encoded_df], axis=1)`. The issue here is that `encoded_df` will have a default integer index, and if `data` had any rows dropped prior to this (e.g., due to target cleaning or other `dropna` operations), the indices of `X` and `encoded_df` will not align, leading to misaligned or all-NaN merged columns.
    *   The ground truth uses `pd.get_dummies` directly on `df_cleaned`, which correctly aligns the one-hot encoded columns with the original DataFrame's index. This is a much safer and standard approach.
    *   The generated code drops `Company Name` and `Processor` *before* defining `X` and `y`. This is problematic because `X` is initially defined using column names, and then later `encoded_df` is concatenated. The issue is that `encoded_df` is created from `data[['Company Name', 'Processor']]` *before* these columns are dropped from `data`. But if `data` has had rows dropped (e.g., due to NaNs), `encoded_df` will no longer align by index with the `data` dataframe if `data.drop` modifies `data` *before* the concatenation, or if `encoded_df` implicitly drops rows through slicing. The safest way is to use `pd.get_dummies` directly on the main DataFrame then select columns for X.

4.  **NaN Handling in X and y:**
    *   The code does `X.dropna(inplace=True)` and then `y = y[X.index]`. This is logically correct for dropping NaNs *after* feature selection, but it's crucial that `X` and `y` always have aligned indices. The preceding issues with `encoded_df` and `pd.concat` could lead to subtle index mismatches that result in incorrect data remaining or being dropped. The ground truth's method of concatenating `X` and `y` and then dropping `NaN`s from the combined DataFrame ensures perfect alignment.

5.  **Robustness and Error Handling:**
    *   The generated code lacks any `try-except` block for loading the CSV, which is good practice.
    *   It doesn't print any progress or debug information, making it harder to follow.

**Areas for Improvement (to reach 8+):**

*   **Implement correct `is_expensive_usa` logic:** Ensure `NaN` values in the cleaned price convert to `NaN` in the target, and then drop rows based on *that* `NaN`.
*   **Refine Camera Feature Cleaning:** Use a regex that specifically targets ""MP"" values as a floating-point number, not just any digit.
*   **Improve One-Hot Encoding and Feature Assembly:** Use `pd.get_dummies` on the primary DataFrame (`data` in this case) instead of `OneHotEncoder` followed by manual concatenation. This simplifies feature management and avoids index alignment issues. Select all features for X *after* this step.
*   **Robust NaN Handling:** Use a method similar to the ground truth where `X` and `y` are concatenated for a final `dropna` to ensure consistent row removal across features and target.
*   **Add Comments and Print Statements:** Enhance code readability and debugging.
*   **Add CSV loading error handling.**",llama-3.3-70b-instruct,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
def load_dataset():
    try:
        return pd.read_csv('mobiles_2025.csv')
    except Exception as e:
        print(f""Failed to load dataset: {str(e)}"")
        return None

# Clean price columns
def clean_price(price):
    try:
        price = str(price).replace('PKR', '').replace('INR', '').replace('CNY', '').replace('USD', '').replace('AED', '').replace(',', '')
        return float(price)
    except:
        return np.nan
...",6,"The generated code correctly loads the dataset, cleans price columns, calculates average global price, and identifies the top 5 models. However, there are significant areas for improvement in terms of efficiency, robustness, and adherence to best practices, particularly in the `calculate_average_global_price` function.

**Strengths:**
- **Modular design:** The use of functions (`load_dataset`, `clean_price`, `calculate_average_global_price`, `get_top_models`, `main`) improves readability and organization.
- **Error handling for file loading:** Basic `try-except` for `pd.read_csv` is present.
- **Correct identification of top N models:** The sorting and `head(5)` logic for top models is correct.

**Areas for Improvement:**
- **Redundant `clean_price` calls:** The `clean_price` function converts to `str` then removes symbols and converts to `float`. The ground truth code handles `pd.isna` directly at the beginning, which is more robust and prevents errors if the input is already a non-string NaN.
- **Inefficient price cleaning application:** Applying `clean_price` to each column individually within `calculate_average_global_price` is less efficient than using a loop or applying it across a subset of columns, as demonstrated in the ground truth.
- **Highly inefficient `Average Global Price` calculation:**
    - The generated code first calculates `Average Global Price` using `np.mean` on a list, which *does not* inherently skip NaNs for elements in a Python list (it would produce NaN if any are NaN).
    - Then, it reassigns `Average Global Price` with another `np.mean` using a list comprehension (`[x for x in ... if not np.isnan(x)]`). This is correct in terms of skipping NaNs, but `df.apply` with a lambda creating a list and then calling `np.mean` on it is significantly less efficient than using `df[price_columns].mean(axis=1, skipna=True)`. The latter is vectorized and much faster for larger datasets.
    - The ground truth's `df[price_columns].mean(axis=1, skipna=True)` is the canonical and most efficient way to achieve this.
- **No handling for missing price columns:** The generated code assumes all specific price columns exist. If one were missing (e.g., 'Launched Price (Pakistan)'), it would raise a `KeyError`. The ground truth handles this with a `if col in df.columns:` check, making it more robust.
- **General `except`:** Using a general `except Exception as e` or `except` without specifying the exception type is generally discouraged as it can hide other programming errors. `FileNotFoundError` is more specific for file loading.
- **Clarity in `main` function:** The `main` function could benefit from clearer variable names (e.g., `cleaned_df` instead of reassigning `df`).",llama-3.3-70b-instruct,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
def load_dataset(file_name):
    try:
        return pd.read_csv(file_name)
    except Exception as e:
        print(f""Failed to load dataset: {e}"")
        return None

# Clean the dataset
def clean_dataset(df):
    # Clean RAM, Battery Capacity, and Launched Price (USA) columns
    df['RAM'] = df['RAM'].apply(lambda x: pd.to_numeric(x.replace('GB', ''), errors='coerce'))
    df['Battery Capacity'] = df['Battery C...",6,"The generated code addresses most of the requirements but has several issues that reduce its correctness and robustness.

**Correctness:**
- **RAM Cleaning:** The `RAM` cleaning `apply(lambda x: pd.to_numeric(x.replace('GB', ''), errors='coerce'))` works if `x` is already a string. However, if 'RAM' column contains non-string types (e.g., numbers, NaN), `x.replace` will raise an AttributeError. The ground truth explicitly converts to string first.
- **Battery Capacity Cleaning:** Same issue as RAM.
- **Launched Price Cleaning:** Same issue as RAM, `x.replace` can fail.
- **NaN Handling for Cleaning:** The generated code applies `dropna` after cleaning. If a conversion error happens (due to `errors='coerce'`), the `NaN` will correctly be there. However, the cleaning functions (`replace`) should ideally handle non-string types gracefully by converting to string first.
- **`Launched Year` Type:** The task implicitly requires `Launched Year` to be numeric for grouping and sorting. The generated code does not explicitly convert `Launched Year` to numeric or handle potential errors, which might lead to issues if the column is not already numeric or contains non-numeric entries (e.g., strings). The ground truth handles this by converting to `int` and dropping NaNs.
- **`Company Name` Consistency:** The generated code assumes 'Apple' is always exactly capitalized. The ground truth also uses 'Apple', but for a robust solution, one might consider standardizing cases (e.g., `.str.lower()`). This is a minor point, but worth noting for production-ready code.
- **Plotting Library:** The task mentions `matplotlib.pyplot or seaborn`. The generated code only imports `matplotlib.pyplot` and then uses `axs[0].boxplot()`, which is Matplotlib's native boxplot function. While this is acceptable given the prompt, `seaborn.boxplot()` offers slightly more aesthetic and functional versatility, as demonstrated in the ground truth.

**Code Quality:**
- **Modularity:** The code is well-structured into functions (`load_dataset`, `clean_dataset`, `filter_and_group_apple_data`, `generate_box_plots`, `main`), which is good for organization and reusability.
- **Error Handling (Loading):** The `load_dataset` function includes a `try-except` block for `FileNotFoundError`, which is good.
- **Docstrings/Comments:** Lacks detailed comments or docstrings explaining the purpose of functions or complex logic.
- **Imports:** Imports are minimal and correct for the functions used.
- **Plot Styling:** The plots are basic matplotlib plots, lacking any specific styling applied (e.g., `plt.style.use`) unlike the ground truth which sets a `seaborn-darkgrid` style.

**Completeness:**
- **`Launched Year` Type Conversion/Handling:** Does not explicitly ensure `Launched Year` is numeric and handles errors/NaNA properly, which is crucial for grouping and analysis.
- **Titles and Labels:** Titles and y-axis labels are present as required.
- **Subplots:** Correctly uses subplots for each year.
- **Error Messages/Information:** The ground truth provides more informative print statements about data loading, cleaning, and filtering steps, which is helpful for debugging and understanding the code's flow. The generated code is more concise but provides less feedback.
- **Robustness in Cleaning:** The direct `x.replace` without `astype(str)` makes cleaning less robust to mixed data types in the raw CSV.

Overall, the generated code provides a functional approach but sacrifices robustness and some quality aspects for conciseness. The issues with handling non-string types during cleaning and the lack of explicit `Launched Year` type handling are the most significant deductions.",llama-3.3-70b-instruct,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, Stackin...",5,"The generated code attempts to address many aspects of the prompt but falls short in several key areas.

**Correctness & Completeness Issues:**

1.  **Preprocessing & Feature Engineering:**
    *   **Advanced Feature Engineering:** The prompt explicitly asks for ""polynomial features, interaction terms, text mining from names."" While ""FarePerFamily"" and ""FamilySize"" are created, and `NamePrefix` attempts text mining, there's no explicit polynomial features or interaction terms generated. The `Family` and `FamilySize` are redundant, and `NamePrefix` is applied before `ColumnTransformer` is defined, which might not be used.
    *   **Missing Values (Multiple Imputation):** The prompt specifically requests ""multiple imputation strategies."" The generated code only uses `SimpleImputer` (median), which is a single imputation, not multiple. The ground truth leverages `IterativeImputer` for this.
    *   **Custom Transformer:** The prompt implies the need for structured preprocessing. The ground truth's `FeatureEngineerTransformer` is a good example of how to encapsulate custom FE, which is missing in the generated code.

2.  **Hyperparameter Tuning:**
    *   **Pipeline Integration:** The generated code performs `preprocessor.fit_transform(X_train)` before passing it to `grid_search.fit()`. This means that the `GridSearchCV` is tuning the *base model* on already transformed data, not tuning the *entire pipeline* (preprocessor + model). This is a critical flaw because preprocessing steps (like scaling) often benefit from being part of the cross-validation loop to prevent data leakage and properly estimate performance. The ground truth correctly integrates the preprocessor into a `Pipeline` passed to `RandomizedSearchCV`.
    *   **Param Naming:** Because of the lack of pipeline integration in tuning, the parameter grids are directly `n_estimators`, `max_depth`, etc., instead of being prefixed with `classifier__` as needed when the model is part of a pipeline.
    *   **RandomizedSearchCV for Efficiency:** The prompt mentioned `RandomizedSearchCV` as an option for efficiency, while the generated code exclusively uses `GridSearchCV` on large parameter spaces, which can be computationally expensive.

3.  **Ensemble Methods:**
    *   **StackingClassifier Input:** Similar to hyperparameter tuning, the `stacking_ensemble.fit` and `voting_ensemble.fit` lines again call `preprocessor.fit_transform(X_train_fold)`. Ensembles typically work by combining *trained* base models. When passed `best_estimator_` from GridSearchCV, these are already pipelines including preprocessing in the ground truth. Here, it seems the base models are just the raw classifiers, and preprocessing is applied externally again before fitting the ensemble, which isn't the standard or most robust way. The `best_estimator_` from the tuning *should* be a full pipeline if tuning was done correctly. This leads to likely incorrect training of the ensembles.

4.  **Feature Selection:**
    *   **Multiple Techniques & Comparison:** The prompt asks for ""multiple techniques and compare their impact."" The generated code only implements `SelectKBest` with `mutual_info_classif`. It doesn't show multiple techniques (e.g., RFE, SelectFromModel) nor does it explicitly compare their impact on model performance (e.g., by training models on the selected features and evaluating them).
    *   **Feature Names:** After preprocessing, it's hard to interpret which features `SelectKBest` selected without the feature names (which are handled by the ground truth).
    *   **Impact:** The `support` variable is calculated but not used to train models on the selected features to ""compare their impact.""

5.  **Extensive Model Evaluation:**
    *   **Cross-Validation:** The cross-validation loop in the generated code only evaluates the `stacking_ensemble` and `voting_ensemble` but not the individual base models' performance after tuning.
    *   **Learning Curves:** The prompt explicitly asks for learning curves. The code generates them but doesn't plot them or display the results as requested.
    *   **Validation Curves:** Same as learning curves, the code calculates but doesn't plot.
    *   **Performance Metrics:** While `accuracy_score` is used, the prompt asks for ""comprehensive performance metrics"" (e.g., precision, recall, F1, ROC AUC). Only `roc_auc_score` is used once for SHAP analysis but not explicitly for overall model evaluation throughout the CV or final comparison.
    *   **Statistical Significance:** The generated code uses `ttest_ind` which is for independent samples, but for comparing models on the *same data splits* (which is what k-fold CV results in), a *paired* t-test (`ttest_rel`) is more appropriate for statistical significance, as shown in the ground truth. Also, it only compares the two ensembles and doesn't compare the best ensemble against base models.

6.  **Model Interpretability:**
    *   **SHAP Integration:** The `TreeExplainer` is correctly used, but `shap_values` are calculated (for XGBoost), and `summary_plot` is called without `plt.show()`, meaning the plot wouldn't display.
    *   **LIME Integration:** The LIME explainer is created and an explanation is generated, but `predict_proba` is directly called on `grid_search_rf.best_estimator_`. For LIME, the `predict_proba` function *must* accept data transformed by the *same preprocessing pipeline* as the model was trained on. The current approach assumes the `best_estimator_` (a raw RF classifier) can directly handle the raw `X_test.values`, which is incorrect if `preprocessor` was applied during training. This needs a custom `predict_fn` that handles preprocessing, similar to the ground truth. Furthermore, `feature_names` for LIME should correspond to the *processed* feature names if the model expects processed data, or LIME needs to be able to transform itself.

7.  **Code Quality:**
    *   **Redundant Feature Engineering:** The raw feature engineering (`data['Sex'] = ...`, `data['Title'] = ...`) modifies the `data` DataFrame in place *before* the `ColumnTransformer` is defined. This means that features like `Title`, `Family`, `FarePerFamily`, `FamilySize`, `NamePrefix` are *not* handled by the `ColumnTransformer`'s pipelines (imputation, scaling, one-hot encoding, etc.). They are effectively ""passthrough"" through `ColumnTransformer` (which implicitly handles unspecified columns). This makes the preprocessing inconsistent and potentially problematic. The `FeatureEngineerTransformer` in the ground truth handles this more cleanly within the pipeline structure.
    *   **`fit_transform` in loop:** `preprocessor.fit_transform` is called repeatedly inside the CV loop for both training and testing, which is inefficient and can lead to data leakage if not careful. `fit_transform` should only be called once on the training data, then `transform` on test data. When `preprocessor` is *inside* the pipeline for `GridSearchCV` or `cross_val_score`, this is handled automatically.

Overall, the generated code demonstrates an understanding of the components requested but struggles significantly with their proper integration into a cohesive and robust machine learning pipeline, particularly around preprocessing with `ColumnTransformer` and embedding models within `Pipeline` and `GridSearchCV`/`RandomizedSearchCV`. The lack of plotting for curves and incomplete feature selection and evaluation are also significant weaknesses.",llama-3.3-70b-instruct,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Define features X using the Year and Runtime columns
X = df[['Year', 'Runtime']]

# Define the target y using the Rating column
y = df['Rating']

# Split the data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_te...",7,"The generated code correctly implements all the core requirements of the task: it imports necessary libraries, loads the data, defines features and target, splits the data, trains a Linear Regression model, makes predictions, and evaluates the model using R-squared and MAE. The `random_state` for `train_test_split` is also correctly applied.

**Areas for Improvement to achieve a higher score:**

1.  **Robustness in Data Loading/Preprocessing:** The ground truth code includes error handling for `FileNotFoundError` when loading the CSV and also handles potential issues with the 'Runtime' column (e.g., ' min' suffix) and ensures that 'Year' and 'Rating' are numeric. It also includes `dropna` for robustness. These steps are crucial for real-world scenarios and make the code more resilient to variations in the input data format. The generated code assumes the CSV is perfectly formatted and present.
2.  **Output Clarity:** While the generated code prints the R2 and MAE, the ground truth includes more descriptive print statements at various steps (e.g., confirming data loaded, data split, model trained), which improves the user's understanding of the code's execution flow.
3.  **Handling Non-Numeric Runtime:** The problem statement implies that 'Runtime' might need cleaning (e.g., '120 min'). The generated code will likely fail if 'Runtime' contains ' min' or other non-numeric characters, as `pd.to_numeric` is not explicitly called after text cleaning. The ground truth explicitly handles this by removing ' min' and then coercing to numeric.

While the generated code passes the core functional requirements, its lack of data preprocessing robustness and slightly less detailed output prevents a perfect score.",llama-3.3-70b-instruct,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load the dataset
imdb_data = pd.read_csv('imdb_top_250.csv')

# Select features for clustering
features = imdb_data[['Rating', 'Runtime']]

# Standardize selected features
scaler = StandardScaler()
standardized_features = scaler.fit_transform(features)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
cluster_...",6,"The generated code correctly loads the data, selects features, standardizes them, and applies K-Means clustering with the specified parameters. It also generates a scatter plot and calculates mean ratings and runtimes per cluster, addressing most requirements.

However, there are two significant issues that prevent a higher score:
1.  **Lack of Robust Data Cleaning for 'Runtime'**: The prompt implied the need for data cleaning or at least acknowledged potential issues with the `Runtime` column (which is very common in such datasets, usually being strings like '142 min'). The ground truth explicitly handles this by converting 'Runtime' from string to numeric and dropping NaNs. The generated code assumes 'Runtime' is already numeric and clean, which is often not the case for `imdb_top_250.csv`, leading to potential errors or incorrect clustering if runtime values are not parsed correctly. This is a crucial step for practical data science tasks.
2.  **Missing `n_init='auto'` in K-Means**: While the prompt specifies `n_init='auto'`, the generated code uses the default value for `n_init`, which for `sklearn` versions prior to 1.4 was `10` and in 1.4+ is `auto`. Although in newer versions it defaults to `auto`, explicitly setting it adheres more closely to the prompt's requirements and ensures compatibility across different `sklearn` versions.

Minor points for improvement:
*   The ground truth added error handling for `FileNotFoundError` and missing columns, which is good practice but not explicitly requested.
*   The ground truth assigns the cluster labels back to the DataFrame (`df['cluster_label'] = ...`), which makes subsequent grouping operations (like calculating means) more straightforward and readable (`df.groupby('cluster_label')[['Rating', 'Runtime']].mean()`). The generated code iterates through cluster numbers and filters the original DataFrame, which works but is less idiomatic pandas.
*   The plot could benefit from a `cmap` and `s` argument for better visual differentiation and point size.
*   The title and axis labels are a bit generic compared to the ground truth's specific titles.",llama-3.3-70b-instruct,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import nltk

# Download NLTK stopwords if not already downloaded
nltk.download('punkt')
nltk.download('stopwords')

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create a new feature called title_length
df['title_length'] = df['Title'].str.len()

# Calculate and print the Pearson correlation coefficient between title_length and Rating
...",8,"The generated code correctly addresses most of the requirements.

Strong points:
- Correctly loads the dataset.
- Accurately calculates `title_length`.
- Calculates and prints the Pearson correlation coefficient.
- Successfully preprocesses titles by converting to lowercase, removing punctuation, and tokenizing.
- Correctly removes stop words.
- Identifies and prints the 10 most common words with their frequencies.
- Imports are mostly correct (though `string` is used in a less robust way than `re` for punctuation, it fulfills the prompt's mention of `string`).
- NLTK downloads are handled.

Areas for improvement (preventing a perfect score):
- **Punctuation Removal:** The use of `string.punctuation` for punctuation removal is less robust than a regex-based approach. `string.punctuation` only covers ASCII punctuation. For more comprehensive punctuation removal (as implied by ""characters that are not alphanumeric or whitespace""), a regex `re.sub(r'[^a-zA-Z0-9\s]', '', text)` would be better. The ground truth uses `re` for this reason.
- **Handling NaN/Non-numeric 'Rating':** The correlation calculation `df['title_length'].corr(df['Rating'])` assumes the 'Rating' column is clean and numeric. If 'Rating' contains non-numeric values or NaNs, this could raise an error or produce an unexpected result. The ground truth explicitly uses `pd.to_numeric` with `errors='coerce'` and `dropna` to handle this robustly. While the task description implies a clean dataset, a robust solution would account for this.
- **Minor Clarity:** The prompt asks for `df['processed_title'] = df['processed_title'].apply(word_tokenize)` which was correctly implemented, but then the next processing steps continually reassign to `df['processed_title']`. While functional, creating new columns for intermediate steps (`tokenized_title`, `filtered_title`) as in the ground truth can sometimes improve readability and debuggability. However, for this specific task, overwriting is acceptable.",llama-3.3-70b-instruct,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Load dataset
df = pd.read_csv('penguins.csv')

# Define features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm...",7,"The generated code correctly performs most of the requested tasks. It loads the data, defines features and target, performs median imputation for numerical features, and mode imputation for categorical features. It also correctly one-hot encodes categorical features and scales numerical features. The RandomForestClassifier is trained and evaluated using 5-fold cross-validation, and the mean accuracy is printed.

However, there are a few issues that prevent it from achieving a higher score:
1.  **Incorrect `island` NA handling after mode imputation**: The instruction stated: ""(Note: if a row still has NA in `island` after mode imputation, e.g. if all were NA, it can be dropped, or a placeholder like 'Unknown' can be used if mode is NA itself. For this task, if mode imputation results in NA for island, drop those rows)."" The generated code applies mode imputation first, then attempts to drop rows where `island` is NA with `X = X[X['island'].notna()]`. While this line generally might remove NAs, it does not specifically address the edge case where the *mode itself* might be NA (which is generally rare for string columns but could happen if the entire column is NA). More importantly, the current order of operations applies mode imputation *first* to `island` within the feature loop, and *then* the separate line `X = X[X['island'].notna()]` is executed. If `island` had NAs *after* mode imputation, it would imply the mode itself was NA (highly unlikely for a string column with values) or an issue with the mode imputation. The ground truth code specifically handles the case where *all* `island` values are NA, correctly dropping all rows if that happens. The generated code's logic `X = X[X['island'].notna()]` would not correctly handle this specific case of an entirely NA 'island' column, as `mode().iloc[0]` would likely fail or return a non-sensical value if the column is all NaN. Furthermore, the `y` variable is not aligned after dropping rows based on `island` in `X`, which will cause a mismatch in the `cross_val_score` function, leading to an error.

2.  **Imputation applied before Pipeline**: The numerical and categorical imputations are performed *before* they are passed to the `ColumnTransformer` and `Pipeline`. While the numerical features are scaled *within* the pipeline, the imputation is done outside. This is a missed opportunity to use `SimpleImputer` within the `Pipeline` setup, which is generally considered best practice for cleaner, more robust, and production-ready machine learning pipelines as it ensures the imputation strategy is learned only from the training data during cross-validation. The prompt specifically asks for ""advanced preprocessing"", and using `SimpleImputer` within `Pipelines` for imputation would align better with that.

3.  **Missing handling of NA in target `y`**: The code assumes `y` has no missing values. While `penguins.csv` does not have missing `species` values, robust code should handle this, as in the ground truth, which drops rows with missing target values.

4.  **Minor print format**: The print statement `print(""Mean accuracy: %.2f"" % scores.mean())` is slightly less modern than f-strings (`print(f""Mean accuracy: {scores.mean():.4f}"")`), and it prints to 2 decimal places instead of the 4 requested in the ground truth example.

The use of `ColumnTransformer` and `Pipeline` is a good practice for structuring the preprocessing steps, even if the imputation was not fully integrated into it. The issue with `y` not being aligned after dropping rows in `X` is a critical bug for correct execution.",llama-3.3-70b-instruct,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Select the features
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']

# Drop any rows where any of these four selected features have missing values (NA)
df_features = df[features].dropna()

# Extract species for comparison...",6,"The generated code correctly performs most of the required steps: loading data, selecting features, standardizing, K-Means, and PCA. It also generates two scatter plots. However, there is a critical flaw in how the `species` column is handled for the second plot after NA removal.

**Areas for Improvement:**
1.  **NA handling for `species`:** The most significant issue is in the line `species = df['species'].dropna().iloc[df_features.index.values]`. This attempts to re-align the species column but is incorrect for two main reasons:
    *   `df['species'].dropna()` drops NAs only from the species column, not necessarily aligning with the rows where selected features had NAs.
    *   `iloc[df_features.index.values]` uses the original index of `df_features` (which is a slice of `df` with NAs dropped from `features`). If `df` had NAs in other columns but not in `features` or `species`, or if `species` itself had NAs that `df_features` didn't account for, this alignment will be wrong.
    *   The correct way (as in the ground truth) is to include `species` in the dataframe *before* dropping NAs, then drop NAs based on the `selected_features`. This ensures `species` rows correspond exactly to the processed feature rows.
2.  **Missing `sns` import and usage:** The prompt implicitly asks for visually appealing plots (common in data science tasks), and `seaborn` is often preferred for this. The ground truth uses `seaborn` and makes the plots more aesthetically pleasing, as well as including grid lines and better legend placement which are standard practice. The generated code falls back to `matplotlib.pyplot` which is functional but less refined.
3.  **Missing `plt.show()` after each plot:** The generated code only has one `plt.show()` at the very end. This will display both plots simultaneously but often it's clearer practice to show them sequentially or use subplots. The ground truth separates them with `plt.show()` after each.
4.  **Plot titles:** While titles are present, they are less descriptive than the ground truth's. ""K-Means Clustering on Penguins Dataset"" is okay, but ""PCA of Penguin Features: K-Means Clusters (K=3)"" is more informative about what the plot actually represents.
5.  **Lack of `plt.xlabel` and `plt.ylabel`**: The generated plots lack axis labels, which is important for clarity.",llama-3.3-70b-instruct,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Load the dataset
penguins = pd.read_csv('penguins.csv')

# Impute missing values
penguins['culmen_length_mm'] = penguins['culmen_length_mm'].fillna(penguins['culmen_length_mm'].mean())
penguins['culmen_depth_mm'] = penguins['culmen_depth_mm'].fillna(penguins['...",7,"The generated code addresses most of the task requirements but has some notable areas for improvement, especially concerning the visualization and a minor deviation in model training setup.

**Strong Points:**
*   **Correct Data Loading and Imputation:** The code correctly loads the data and performs the specified imputations (mean for numerical, mode for 'sex', dropping rows for 'island' NA). The order of operations for `island` NA handling (doing it *after* other imputations) is slightly different from the ground truth but still effective in principle for the NA handling part.
*   **Correct Feature Engineering:** One-hot encoding for `island` and `sex` is done correctly, and `species` is label encoded as the target.
*   **Feature Importance Extraction:** The feature importances are correctly extracted and printed along with their feature names.
*   **Basic Visualization Present:** Box plots are generated per species, which is a significant part of the visual requirement.

**Areas for Improvement / Deviations from Ground Truth / Minor Issues:**

*   **Missing Boxplot Grouping:** The request was for ""box plots to visualize the distribution of `flipper_length_mm` (using imputed data) grouped by the imputed `sex` column *for each species*."" The ground truth achieves this with a single `seaborn.boxplot` call using `x='species'`, `y='flipper_length_mm'`, and `hue='sex'`. The generated code creates separate subplots for each species, which is acceptable, but the box plots *within* each species plot do not correctly group by sex using `seaborn`'s `hue` functionality, which would be more concise and standard. Instead, it seems to manually filter for 'MALE' and 'FEMALE' and plot two separate boxes. This makes the plotting redundant and less elegant than using `seaborn`'s grouping capabilities. Also, it only considers 'MALE' and 'FEMALE' sex categories, potentially not handling other unique categories correctly (though 'MALE' and 'FEMALE' are the primary ones in this dataset).
*   **Unnecessary `train_test_split`:** The task did not ask for model evaluation or prediction on a test set, only for training a GradientBoostingClassifier to extract feature importances. Splitting the data into train/test sets is an extra step that isn't required for this specific task and means the feature importances are based only on a subset of the data (80% in this case), potentially leading to slightly different, though still valid, importances than if trained on the full dataset. The ground truth trains on the full dataset, which aligns better with the broad feature importance analysis without an explicit need for model generalization evaluation.
*   **Plotting Library Usage:** The generated code uses `matplotlib.pyplot` for plotting directly and implements the box plots more manually. The ground truth leverages `seaborn`, which is generally preferred for statistical plots due to its higher-level interface and aesthetic improvements, making the plotting code cleaner and more standard for data science visualization.
*   **Missing `seaborn` Import:** While plots are generated, `seaborn` is not imported, indicating the manual plotting approach. Adding `import seaborn as sns` and utilizing it for the box plots would significantly improve code quality for visualization.
*   **Hardcoded Sex Categories for Plotting:** In the boxplot loop, the code explicitly checks for 'MALE' and 'FEMALE'. This is less robust than letting a plotting library (like seaborn) handle categorical variables automatically.
*   **Readability of Feature Importance Output:** While functional, the ground truth's use of a Pandas DataFrame for displaying feature importances is generally more readable and professional than a simple `for` loop printout for a larger number of features.",llama-3.3-70b-instruct,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
# Import necessary libraries
import pandas as pd
from sklearn.impute import KNNImputer

# Load the dataset
def load_dataset():
    try:
        return pd.read_csv('penguins.csv')
    except Exception as e:
        print(f""Error loading dataset: {e}"")
        return None

# Impute numerical columns using K-Nearest Neighbors imputation
def impute_numerical_columns(df):
    numerical_columns = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
    imputer = KNNImp...",9,"The generated code successfully completes all aspects of the task, meeting the requirements.

**Strengths:**
*   **Encapsulation:** The use of functions (`load_dataset`, `impute_numerical_columns`, `impute_categorical_columns`, `main`) promotes good code organization and reusability.
*   **Correct Imputation:** It correctly applies KNN imputation to the specified numerical columns and mode imputation to the 'sex' column.
*   **Verification:** It prints the remaining NA values, although less verbosely than the ground truth, it still provides the required verification.
*   **Error Handling:** Includes a basic `try-except` block for file loading.

**Areas for Minor Improvement (to reach a perfect 10):**
*   **Explicit Error Message for File Not Found:** The error message `Error loading dataset: [e]` is generic. A more specific message like the ground truth's `Error: 'penguins.csv' not found...` would be more helpful to the user if the error is indeed `FileNotFoundError`.
*   **Clarity in Output:** While it prints remaining NAs, the ground truth's initial NA printout and the specific column verification at the end provide more transparency and confirmation for the user. However, this is more about user experience than code correctness.
*   **Robustness for Mode Imputation:** The current `df[column].mode().iloc[0]` could fail if, for some reason, the mode calculation returns an empty Series (e.g., if the column is entirely NA or empty). While unlikely for common datasets, the ground truth's check (`if not df[categorical_col_for_mode].mode().empty:`) adds a layer of robustness.

Overall, the code is well-written, correct, and fulfills the prompt's requirements effectively.",llama-3.3-70b-instruct,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
penguins = pd.read_csv('penguins.csv')

# Filter the dataset to include only 'Adelie' and 'Gentoo' species
adelie_gentoo = penguins[penguins['species'].isin(['Adelie', 'Gentoo'])]

# Handle NA values in the `culmen_length_mm` column
adelie_gentoo = adelie_gentoo.dropna(subset=['culmen_length_mm'])

# Split the data into two separate series for 'Adelie' and 'Gentoo' species
adelie_culmen_length = adelie_gentoo[adelie_gentoo...",8,"The generated code correctly implements all the core requirements of the task. It successfully loads the data, filters for the specified species, handles NA values, performs the t-test using `scipy.stats.ttest_ind`, and interprets the p-value against a significance level of 0.05.

**Strong Points:**
*   **Correctness:** All logical steps (loading, filtering, NA handling, t-test, interpretation) are correctly implemented.
*   **Conciseness:** The code is quite concise and directly addresses the problem.
*   **Imports:** Necessary libraries (`pandas`, `scipy.stats`) are correctly imported.

**Areas for Improvement (Why not a 10):**
*   **Robustness:** The code assumes the `penguins.csv` file always exists. A `try-except` block for `FileNotFoundError` would make it more robust, similar to the ground truth.
*   **Readability/Comments:** While the code is relatively straightforward, adding comments for each step (e.g., ""Load the dataset,"" ""Filter data"") would improve readability for more complex tasks.
*   **Function Encapsulation:** Encapsulating the logic within a function (as done in the ground truth) would make the code reusable and more organized, especially for larger projects. This would also allow for clearer default parameters like `alpha`.
*   **Detailed Output:** The printing of results is somewhat minimalistic. The ground truth provides more informative print statements, including data shapes, counts, and mean values, which are helpful for debugging and understanding the data at each stage of processing. For instance, showing how many rows were dropped due to NAs is good practice.
*   **Explicit `equal_var`:** While `ttest_ind` defaults to `True` for `equal_var`, explicitly setting it (e.g., `equal_var=True`) enhances clarity about the chosen t-test variant (Student's t-test vs. Welch's t-test).

Overall, it's a very good attempt that fulfills the core requirements. The improvements suggested are primarily about best practices for robustness, readability, and organization, which are crucial for production-level code or larger data science projects.",llama-3.3-70b-instruct,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
# Import necessary libraries
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report

# Load the dataset
def load_dataset():
    return pd.read_csv('penguins.csv')

# D...",6,"The generated code attempts to solve the problem and has a good structure with functions. However, there's a critical error in its preprocessing pipeline setup that prevents it from correctly handling the data as specified in the task and how `ColumnTransformer` is designed to be used with `Pipeline`.

Here's a breakdown:

**Positive aspects:**
*   **Modularity:** The use of functions (`load_dataset`, `define_preprocessing`, `preprocess_and_split`, `train_model`, `evaluate_model`, `main`) improves code organization and readability.
*   **Correct imports:** All necessary libraries are imported.
*   **Correct feature/target definition:** Identifies 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex' as features and 'species' as target.
*   **Correct split parameters:** Uses `test_size=0.25`, `random_state=42`, and `stratify=y_encoded`.
*   **Correct model parameters:** `n_estimators=100`, `random_state=42`.
*   **Correct evaluation metrics:** Uses `accuracy_score` and `classification_report`.

**Areas for Improvement / Critical Errors:**

1.  **Incorrect Preprocessing Application**:
    *   The `preprocess_and_split` function calls `preprocessor.fit_transform(X)`. This applies the `ColumnTransformer` directly to the `X` (original DataFrame). This is fine for transformation, but then the `RandomForestClassifier` (which expects processed numerical features) is trained *outside* of this preprocessing pipeline.
    *   The `ColumnTransformer` (preprocessor) is *not* part of a `Pipeline` that includes the `RandomForestClassifier`. This means the `X_train` and `X_test` passed to `train_model` and `evaluate_model` are NumPy arrays resulting from the `preprocessor.fit_transform(X)` and `preprocessor.transform(X_test)` (though `transform` is not explicitly called for `X_test` here).
    *   **The crucial issue:** The `preprocessor` is fit on `X` (the *entire* dataset) within `preprocess_and_split`, *before* the `train_test_split` happens. This is a data leakage issue. The imputer and one-hot encoder learn from the test set data when `fit_transform` is called on the whole `X` prior to splitting. Preprocessing steps (especially `fit`) should only learn from the training data. The `ColumnTransformer` should be part of the `sklearn.pipeline.Pipeline` *along with* the classifier, and this complete pipeline should be fit on `X_train`, `y_train`. The ground truth code correctly demonstrates this by creating `model_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', RandomForestClassifier(...))])`.
    *   The generated code also incorrectly identifies numerical and categorical features using `select_dtypes` *inside* `define_preprocessing` without ensuring that the columns defined in the task (`culmen_length_mm`, etc.) are explicitly considered, especially when `sex` is specified as a categorical feature needing mode imputation, which `select_dtypes` might incorrectly classify if it contains numerical values after some transformation or if it's already encoded. The task explicitly lists the features and their types.

2.  **Missing `transform` for `X_test`:** In `preprocess_and_split`, `X_prep = preprocessor.fit_transform(X)` is used. While `fit_transform` on the whole X, followed by `train_test_split` will give you `X_train_prep` and `X_test_prep`, it's not the correct way to handle the test set. The `preprocessor` should be fit only on `X_train`, and then `transform` (not `fit_transform`) should be called on `X_test` to avoid data leakage. The `Pipeline` approach (as in the ground truth) elegantly handles this by fitting the entire pipeline on `X_train, y_train` and then calling `predict` on `X_test`.

3.  **`LabelEncoder` placement:** While `y_encoded = le.fit_transform(y)` is correct for label encoding the target, doing it before `train_test_split` on the *entire* `y` is okay for target encoding when `stratify` is used. However, it's generally cleaner and more consistent to fit the `LabelEncoder` on `y_train` and then transform both `y_train` and `y_test`. The current approach works for `LabelEncoder` specifically because it's a target transformation and not a feature transformation that would leak information. Still, the `Pipeline` approach would integrate this better if `LabelEncoder` were part of a ColumnTransformer for target (it's not common for targets).

The main issue is the separation of preprocessing from the model training process, leading to data leakage in the way the preprocessor is fit, and not correctly passing processed `X_test` to the evaluation.",llama-3.3-70b-instruct,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Calculate the Pearson correlatio...",10,"The generated code correctly defines the DataFrame, calculates the specified Pearson correlation coefficients, prints them, and accurately identifies the feature with the stronger absolute correlation with ""PerformanceScore"". The code is clean, well-structured, and directly addresses all requirements of the task. The use of `.corr()` automatically defaults to Pearson correlation, so explicitly stating `method='pearson'` as in the ground truth is not strictly necessary but good practice. The output format for printing the correlations is slightly different (no f-string formatting to limit decimal places), but this does not impact correctness or completeness.",llama-3.3-70b-instruct,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
from lifelines import KaplanMeierFitter
from lifelines import CoxPHFitter
from lifelines import plotting
from scipy.stats import norm
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pymc3 as pm
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

# Load the data
titanic_df = pd.read_csv('titanic...",3,"The generated code attempts to address several requirements but fails significantly in correctness, completeness, and statistical rigor.

**Primary reasons for failure:**

*   **Incorrect Definition of Time-to-Event and Event Indicator:** The prompt explicitly states ""Kaplan-Meier survival analysis treating the voyage as time-to-event data (with survival as event indicator)"". The ground truth correctly interprets this as a fixed time T=1 (all individuals arrive at the end of the voyage together) and E=death (1-Survived). The generated code uses `titanic_df['Age']` as `T` and `titanic_df['Survived']` as `E` (event observed). This is fundamentally incorrect for ""voyage as time-to-event"". If Age were the time, then the event would be `death at age` and the event indicator would be death, not survival. Using 'Survived' (1 for survived, 0 for died) as `event_observed` in `lifelines` is also an inversion, as `lifelines` expects 1 for event observed (e.g., death) and 0 for censored. This renders the Kaplan-Meier and Cox models statistically meaningless in the context of the prompt's definition of time-to-event.
*   **Missing Data Preprocessing:** The generated code loads the data but does not perform any imputation for missing 'Age' or 'Embarked' values, which are present in `titanic.csv`. This will lead to errors or silently drop rows, affecting all subsequent analyses.
*   **Flawed Kaplan-Meier Plot:** `plotting.plot_lifetimes(kmf)` is not the standard way to plot KM curves from `lifelines`. `kmf.plot_survival_function()` or `kmf.plot()` should be used. The chosen method is for plotting individual lifetimes, not a summary KM curve.
*   **Incorrect Cox PH Model Application:** Similar to KM, the `duration_col='Age'` and `event_col='Survived'` (binary `Survived` as the event, expecting 1 for event, 0 for censored) are incorrect given how `lifelines` handles these parameters and the prompt's implied time-to-event. The `df` passed to `cph.fit` also contains the `Survived` column which is then used as `event_col`, which is redundant and potentially problematic.
*   **Misinterpretation of ""Advanced Statistical Tests"" and ""Multivariate Analysis"":** The implementation uses `statsmodels.OLS` (Ordinary Least Squares) for `X` and `y`, where `y` is `Survived`. This is regression, not suitable for binary outcomes like Titanic survival, and doesn't represent multivariate survival analysis. Interaction effects testing and model diagnostics were also requested and are missing or poorly implemented.
*   **Propensity Score Matching is Incomplete and Incorrect:**
    *   It only calculates propensity scores and assigns them to a new column. It does not perform the actual matching (e.g., nearest neighbors, caliper matching).
    *   It does not analyze the treatment effect after matching or check for covariate balance, which are critical steps in PSM.
    *   The variables `Pclass_1`, `Pclass_2`, `Pclass_3` are used as if they are distinct Pclass dummy variables from one-hot encoding, but the input `titanic_df['Pclass']` is converted to 'class 1', 'class 2', etc., and then `pd.get_dummies(X)` is called, but it's not clear how the `Pcalss` columns are handled. `Pclass_3` is not standard for one-hot encoding if `drop_first=True` is used.
*   **Bootstrap Resampling:** The coefficients from the bootstrapped Cox model are appended to `bootstrap_results`, but then `np.percentile` is called on this array without correctly structuring it, leading to a potentially non-informative output. It also reports hazard ratios, not coefficients, as requested.
*   **Bayesian Analysis:**
    *   The Bayesian model uses `titanic_df['Pclass_1']`, `titanic_df['Sex_male']`, etc., as if they are already in the dataframe, but `pd.get_dummies(X)` was applied to `X` (which is a subset of `titanic_df`), not `titanic_df` itself. This will likely cause errors.
    *   The `pm.math.exp(mu) / (1 + pm.math.exp(mu))` is an attempt at a sigmoid, but `pm.invlogit` (or `pm.math.sigmoid`) is the correct function. The likelihood is also for a standard logistic regression, not survival analysis.
    *   The `pm.sample` call lacks important parameters like `cores`, `random_seed`, and `return_inferencedata=True` for a standard reproducible PyMC3 workflow.
    *   No analysis of the trace or posterior distribution is performed beyond calling `traceplot`.
*   **Model Validation and Sensitivity Analysis:** The section for this is extremely brief and only includes `cross_val_score` on an OLS model (which is inappropriate). It completely misses model diagnostics for survival models (like proportional hazards assumption checks, which `lifelines` supports), sensitivity analysis for imputation, or Bayesian prior sensitivity, all explicitly requested.
*   **Imports and Conversions:** While some correct imports are present, `titanic_df['Survived'] = titanic_df['Survived'].apply(lambda x: 1 if x == 'yes' else 0)` suggests `titanic.csv` has 'yes'/'no' for 'Survived', which is unusual and not standard for the common Kaggle Titanic dataset. Also, `Pclass` conversion to 'class X' and then to `get_dummies` is an unnecessary intermediate step which can lead to inconsistencies.

In summary, the code demonstrates a superficial understanding of the requested techniques. The core definitions of survival analysis (Time, Event) are incorrect, essential preprocessing steps are missing, complex methods like PSM are left incomplete, and Bayesian modeling is poorly integrated.",llama-3.3-70b-instruct,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# Create a DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
    'Final_Score': [90, 88, 82, ...",7,"The generated code correctly performs most of the required tasks.

**Strong Points:**
- Correctly creates the DataFrame.
- Correctly calculates and prints the Pearson correlation coefficient.
- Generates a scatter plot with correct axes and coloring by `Subject`.
- Uses `seaborn` for plotting, which is good practice for aesthetics.
- Correctly adds the `Overall_Grade` column using `pd.cut`, which is a clean and efficient way to assign grades based on bins.
- Prints the grade counts.

**Areas for Improvement / Missing Aspects:**
1.  **Plot Labels/Legend:** While the scatter plot is colored by subject, it's missing the explicit `plt.legend()` call to display the legend. Seaborn's `scatterplot` usually adds a legend automatically, but it's good practice to ensure it's there and potentially customize its placement or title, which wasn't explicitly done. The ground truth code explicitly places the legend outside (`bbox_to_anchor`).
2.  **Grade Binning Accuracy:** The definition for `pd.cut` is `bins=[0, 70, 80, 90, 100], labels=['D', 'C', 'B', 'A'], right=False`.
    *   `right=False` means the rightmost edge of the bin is excluded.
    *   This translates to:
        *   `[0, 70)` -> D (Correct for D < 70)
        *   `[70, 80)` -> C (Correct for 70 <= C < 80)
        *   `[80, 90)` -> B (Correct for 80 <= B < 90)
        *   `[90, 100)` -> A (This means 90 <= A < 100, which is `A` if `Final_Score >= 90` but **incorrectly excludes 100 if a score of 100 was possible and assigned 'A'**. The ground truth uses `>= 90` for 'A', which is more robust for edge cases like 100. For scores > 100, the `pd.cut` would also fail, though not relevant to this specific dataset.)
    While it works for the given dataset, the `right=False` combined with the 100 upper bound could be problematic if scores included exactly 100 or higher. The ground truth's `np.select` is more robust for the specific logical conditions.
3.  **Output Formatting:** The correlation coefficient output could be more informative (`Pearson correlation coefficient between 'Attendance_Rate' and 'Final_Score'`) and formatted (e.g., `:.4f`).
4.  **Completeness of Grading:** The `pd.cut` approach for grades might not perfectly align with the asked conditions for 'A' (A if Final_Score >= 90) if a score of exactly 100 was present, as `[90, 100)` means up to but not including 100. However, for the given data, it yields the same results. The ground truth's use of `np.select` with explicit `>=` and `<` conditions is a better match for the precise requirements.
5.  **Small Details:** The generated code doesn't explicitly `print` statements indicating the completion of each step (like ""Creating DataFrame"", ""Scatter plot displayed successfully""), but this was not a hard requirement, more for user experience.

Overall, it's a good attempt that addresses most requirements correctly, but could be refined in terms of plot details and the robustness of the grading logic.",llama-3.3-70b-instruct,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']), 
        'Ticker': ['TECHCO']*10, 
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0], 
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0], 
        'Low': [149.5, 150.5, 1...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong points:**
*   **Correct DataFrame creation and indexing:** The DataFrame is correctly initialized and the 'Date' column is set as the index.
*   **Accurate calculations:** Daily returns, 3-day SMA, and 5-day SMA are calculated correctly according to the specified formulas and windows.
*   **Effective plotting:** The plot correctly displays the 'Close' price, 'SMA_3Day', and 'SMA_5Day' on the same chart with a title and legend.
*   **Correct bullish crossover identification:** The logic for identifying bullish crossovers (SMA_3Day crossing above SMA_5Day) is sound and correctly implemented using `shift()` and boolean indexing.

**Areas for minor improvement:**
*   **Plot Y-axis label:** The Y-axis label specifies 'Price ($)' which is good, but the prompt only asked for labeled axes. Both are acceptable.
*   **Plot `plt.show()` placement:** While `plt.show()` is present, it's placed before the print statement for crossover dates. It's generally good practice to show the plot as the very last step if no further visual processing is involved, but this is a minor stylistic point and doesn't affect correctness.
*   **Formatting of printed dates:** The ground truth formats the dates for cleaner output (`.strftime('%Y-%m-%d')`). While the generated code prints the `DatetimeIndex` directly (which is understandable), formatting it could improve readability for the user. This is a very minor detail.

Overall, an excellent solution.",llama-3.3-70b-instruct,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001',...",9,"The generated code correctly addresses all parts of the task. It successfully creates the DataFrame, calculates total sales revenue per product, generates and displays the horizontal bar chart, calculates the average order value, and identifies repeat customers.

**Strengths:**
*   **Correctness:** All calculations (revenue, average order value, repeat customers) are correct.
*   **Plot Generation:** The horizontal bar chart is correctly generated with appropriate labels and title.
*   **Conciseness:** The code is quite concise while remaining readable.
*   **Imports:** Necessary imports are included.

**Areas for Minor Improvement:**
*   **Plot Aesthetics:** While the plot is functional, the ground truth's use of `plt.figure(figsize=...)`, `plt.grid()`, and `plt.tight_layout()` improves readability and presentation. The generated code could benefit from these minor aesthetic enhancements.
*   **Chart Method:** The ground truth uses `product_revenue.plot(kind='barh', colour='skyblue')` which is a more pandas-idiomatic way to plot directly from a Series, whereas the generated code uses `plt.barh`, which is also correct but requires treating `product_revenue` as a DataFrame (after `reset_index()`) to access columns. Both are valid, but the pandas plotting method is often preferred for simplicity.
*   **Output Formatting:** The output for ""Customers with Multiple Purchases"" could be more user-friendly, similar to the ground truth's formatted print statement, especially if there were many customers.
*   **Intermediate Prints:** The ground truth includes `print` statements for intermediate results and separators, which are helpful for debugging and understanding the code's flow, especially in a benchmark scenario. While not strictly required by the prompt, they enhance clarity.

Overall, it's a very good solution that fulfills all the requirements of the task. The minor points are mostly about presentation and minor code style preferences rather than functional correctness.",llama-3.3-70b-instruct,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd
import numpy as np

# Create the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Prepare the data for the classification model
# Create a new numerical feature ""Dev...",10,"The generated code is excellent and perfectly addresses all requirements of the task.

1.  **Correctness**: The code correctly defines the initial DataFrame. It accurately creates `DeviceType_Numeric` by mapping ""Desktop"" to 0 and ""Mobile"" to 1. It correctly calculates `PagesPerMinute` and handles division by zero by replacing both `np.inf`, `-np.inf`, and `np.nan` (which could result from 0/0) with 0. The final feature selection is also correct, and the head of the resulting DataFrame is printed as requested.

2.  **Code Quality**: The code is clean, concise, and well-structured. Imports are correct (`pandas` and `numpy`). Variable names are descriptive. The chain of `.replace([np.inf, -np.inf], np.nan).fillna(0)` for handling division by zero is a modern and efficient pandas idiom.

3.  **Completeness**: All four requirements of the task are fully met:
    *   DataFrame definition: Yes.
    *   `DeviceType_Numeric` creation: Yes.
    *   `PagesPerMinute` creation with zero division handling: Yes.
    *   Feature selection: Yes.
    *   Printing head of feature DataFrame: Yes.

The generated code is functionally identical to the ground truth in achieving the task's objectives, with slightly more concise handling of the `PagesPerMinute` cleaning step, which is a good practice.",llama-3.3-70b-instruct,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26...",9,"The generated code correctly performs all the required steps: it defines the DataFrame as specified, calculates the mean and standard deviation of ""TemperatureCelsius"", identifies outliers based on the 1.5 standard deviation rule, and prints the requested details for each outlier. The code is clean and well-structured, making it easy to understand.

One minor improvement, though not critical for correctness as `Timestamp` is initially a string, would be to convert the 'Timestamp' column to datetime objects using `pd.to_datetime()`. This is good practice for timestamp columns and makes the data type explicit, although the current string format works fine for printing in this specific case. However, since the task only asked to print the timestamp and not to perform any time-based operations, this is a minor point. The prompt also didn't explicitly ask for f-string formatting of the temperature value, but it's often good practice to format floats for readability.",llama-3.3-70b-instruct,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [1...",10,"The generated code correctly defines the DataFrame, calculates the ""EngagementScore"" as specified, and accurately identifies and prints the ""PostType"" with the highest average ""EngagementScore"" for each ""Platform"". The code is well-structured and uses appropriate pandas operations for grouping and aggregation. The output format also matches the requirements. This is a complete and correct solution to the task.

**Strong points:**
*   **Correctness:** All calculations and logical steps are accurate, leading to the correct results.
*   **Clarity:** The code is easy to read and understand, with logical variable names.
*   **Efficiency:** Uses pandas' built-in `groupby`, `mean`, and `idxmax` effectively, which are generally efficient for these types of operations.
*   **Completeness:** Addresses all three parts of the task: DataFrame creation, EngagementScore calculation, and identifying/printing the highest average for each platform.",llama-3.3-70b-instruct,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

# Create DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold""...",10,"The generated code correctly defines the DataFrame, calculates the ""LengthOfStay"" column, computes the average length of stay and patient count per diagnosis, prints this summary, and correctly identifies the diagnosis with the longest average length of stay. All requirements of the task are met, and the code is clean and efficient.

Strong points:
- Correctly uses `pd.to_datetime` for date columns.
- Accurately calculates `LengthOfStay` using datetime difference and `.dt.days`.
- Efficiently calculates average length of stay and patient count per diagnosis using `groupby().agg(['mean', 'count'])`.
- Clearly renames columns for readability after aggregation.
- Correctly identifies the diagnosis with the longest average length of stay using `idxmax()`.
- The output format is clear and directly addresses the printing requirements.",llama-3.3-70b-instruct,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

# Define the data
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [22...",9,"The generated code correctly defines the DataFrame, calculates the ""WeightedRating"" precisely as specified, adds it as a new column, and accurately identifies and prints the title and weighted rating of the movie with the highest weighted rating. All requirements of the task are met.

The code is clean, well-structured, and uses pandas functionalities efficiently (e.g., vectorized operations for WeightedRating calculation, `idxmax()` for finding the maximum). It follows best practices for creating DataFrames and performing operations.

A minor improvement could be to include a check for `total_votes == 0` to prevent potential `ZeroDivisionError`, though for this specific dataset, `total_votes` will never be zero. The ground truth code includes such a check, which makes it slightly more robust for general cases. However, for the given problem statement and data, the generated code is perfectly functional and correct.",llama-3.3-70b-instruct,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

# Define the data
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 1000...",9,"The generated code correctly defines the DataFrame, calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"", and prints the resulting grouped DataFrame. It achieves all the task requirements efficiently. The use of `df.groupby(""Brand"")[[""PriceUSD"", ""Mileage""]].mean().reset_index()` is a concise and standard way to perform the aggregation and ensure 'Brand' remains a regular column in the output. The code is well-structured and uses appropriate pandas functions. It fully completes the task. A minor stylistic point is that the column names in the output DataFrame are 'PriceUSD' and 'Mileage', which are the original column names. The ground truth renames them to 'Average_PriceUSD' and 'Average_Mileage' for clarity. While not strictly required by the prompt, doing so can improve readability for the end-user. However, the current output is certainly correct and understandable.",llama-3.3-70b-instruct,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, ...",9,"The generated code is highly correct and complete, successfully addressing all requirements of the task. It correctly defines the DataFrame, adds the ""TotalStats"" column, identifies the Pokemon with the highest total stats, and calculates the average speed for each ""Type1"".

**Strong Points:**
- **Correctness:** All calculations and data manipulations are accurate.
- **Completeness:** All three sub-tasks (add TotalStats, find max TotalStats Pokemon, calculate average Speed by Type1) are implemented.
- **Readability:** The code is clear and easy to understand with comments explaining each step.
- **Efficiency (for highest TotalStats):** Using `idxmax()` is an efficient way to find the index of the maximum value.

**Minor Area for Improvement:**
- **Average Speed Calculation:** While the loop-based approach for calculating average speed per `Type1` works correctly, `df.groupby('Type1')['Speed'].mean()` is generally more idiomatic, concise, and often more performant for this type of operation in pandas. The current loop iterates through unique types and filters the DataFrame in each iteration, which can be less efficient for very large DataFrames compared to `groupby()`. However, for this dataset size, the performance difference is negligible.
- **Output Formatting:** The ground truth includes more descriptive print statements for clarity, which the generated code could also incorporate to improve user experience. For example, printing the entire DataFrame after adding the `TotalStats` column.

Overall, a very good solution that fulfills the task requirements effectively.",llama-3.3-70b-instruct,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from scipy.spatial.distance import pdist
from scipy.cluster.hierarch...",3,"The generated code attempts to address the task but falls short in several critical areas, leading to significant correctness and completeness issues.

**Primary Reasons for Failure & Critical Missing/Incorrect Parts:**

1.  **Incorrect Data Handling & Feature Engineering:**
    *   `titanic_df = titanic_df.dropna()`: This is a major issue. Dropping all rows with *any* NaN (Parch, Cabin, Age, Embarked, Fare) will lead to losing a substantial portion of the dataset (e.g., Age has many NaNs, Cabin has most). The ground truth correctly imputes values. This drastically alters the dataset and subsequent analysis.
    *   **Feature Engineering Simplistic/Incorrect:**
        *   `Name_Vector` and `Ticket_Vector` via `TfidfVectorizer`: This implementation is fundamentally flawed. `vectorizer.fit_transform([x])` is called for *each individual name/ticket string*. This creates a TF-IDF vectorizer that has only seen *one document* at a time, resulting in meaningless 'mean' vectors that don't capture any text-based relationships across the dataset. The ground truth extracts meaningful features like `Title`, `NameLength`, `TicketPrefix` which are much more robust.
        *   `Ticket_Suffix`: The regex `(.*)\s*\d{1,6}$` attempts to extract a prefix but is prone to errors and doesn't handle all cases well compared to the `get_ticket_prefix` function in the ground truth.
        *   Missing crucial advanced features from the ground truth like `FarePerPerson`, `TicketGroupSize` (done via a simpler but less robust method), `CabinDeck`, `HasCabin`, `Age_Class`, `Fare_Class`, `FarePerPerson_Class`.
        *   `get_dummies` directly applied to `Embarked`, `Title`, `Ticket_Suffix` without proper handling of rare categories or imputation before one-hot encoding, leading to many sparse columns.

2.  **Preprocessing & Scaling:**
    *   `StandardScaler` applied only to `Age` and `Fare`. The ground truth uses `ColumnTransformer` and `Pipeline` for separate scaling of numerical features and one-hot encoding of categorical features, ensuring all relevant features are properly preprocessed. The generated code lacks a comprehensive preprocessing pipeline.

3.  **Clustering & Dimensionality Reduction Input:**
    *   `titanic_df.drop(['Survived'], axis=1)` is used as input for all clustering and dimensionality reduction. This dataframe still contains many categorical columns that have not been properly one-hot encoded (or handled at all if `get_dummies` wasn't applied comprehensively), leading to errors or poor results if the columns are directly numeric representations of categories. The ground truth correctly builds `X_processed` and `X_cluster` using a `ColumnTransformer` with `OneHotEncoder`.

4.  **Optimal Number of Clusters:**
    *   The `gap_scores` list is initialized but *never populated*.
    *   The ""elbow method"" plot is not generated, only the inertia values are stored.
    *   There's no clear analysis or selection of an optimal `k` based on the metrics, it just jumps to a fixed `k=5` for clustering.

5.  **DBSCAN Implementation:**
    *   `DBSCAN(eps=0.5, min_samples=10)`: These are arbitrary parameters. DBSCAN is highly sensitive to parameters, and without a method to find them (like the k-distance plot), it's unlikely to perform well. The note in the ground truth addresses this explicitly.

6.  **Cluster Stability:**
    *   The stability analysis only checks K-Means against *itself* (different random states), which is a good first step, but the ground truth also implies consensus clustering beyond just ARI, and a broader stability analysis across algorithms is not present. The ""stability_scores"" print statement lacks context.

7.  **Analysis and Visualizations:**
    *   `Feature importance analysis` is a very basic bar chart using `np.abs(titanic_df[feature].values).sum()`. This is not a meaningful measure of feature importance for clustering. The ground truth performs a profiling of clusters by showing mean/mode of original features and heatmaps of scaled feature means, which is much more insightful.
    *   Survival analysis is present but the profiling of *cluster characteristics* (mean/median/mode of features for each cluster) is largely missing, making it hard to interpret *why* clusters differ in survival.
    *   Plots are generated but the lack of proper feature engineering and data preprocessing makes their meaningfulness questionable.

8.  **Completeness/Task Requirements:**
    *   **Advanced Feature Engineering:** Largely incomplete and incorrect as detailed above.
    *   **Multiple Clustering Algorithms:** Implemented, but potentially on flawed data.
    *   **Optimal Cluster Determination:** Incomplete (missing Gap Statistic, no explicit determination).
    *   **Dimensionality Reduction:** Implemented, but on potentially flawed data.
    *   **Cluster Characteristics & Survival:** Lack of detailed cluster profiling.
    *   **Cluster Stability & Consensus:** Partially implemented for K-Means stability, but no consensus clustering.
    *   **Comprehensive Visualizations:** Lacking key profiling visualizations (e.g., feature distributions per cluster), and feature importance is poor.
    *   **External Validation:** Only survival rates are shown, but no deeper validation (e.g., comparing to prior expectations, or more formal statistical tests on survival differences).

In summary, the code makes fundamental mistakes in data handling (especially `dropna()`), implements feature engineering poorly, and uses incorrect inputs for clustering and visualization, leading to results that are unlikely to be meaningful or correct. It lacks the robustness and depth seen in the ground truth for a comprehensive analysis.",llama-3.3-70b-instruct,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dyst...",9,"The generated code correctly performs all requested operations.

**Strong Points:**
*   **Correct DataFrame Creation:** The DataFrame is correctly initialized as per the task description.
*   **Correct Aggregation:** It accurately calculates the count of books and average price per author using `groupby()` and `agg()`.
*   **Clear Renaming:** Renaming the aggregated columns (`BookID` to `Count of Books` and `Price` to `Average Price`) improves readability.
*   **Correct Identification of Max Stock Author:** It correctly identifies the author with the most books in stock by summing `Stock` and then using `sort_values` and `head(1)`.
*   **Output:** The output for the author information and the author with the most books in stock is clear and correct.

**Areas for Improvement (Minor):**
*   **Formatting of Author Statistics:** The output for ""Author Information"" prints the entire DataFrame, which is functional but less custom-formatted than the ground truth. The ground truth iterates and uses f-strings for more controlled and potentially more aesthetically pleasing output (e.g., currency formatting for price). While not strictly a problem, it indicates a slight difference in presentation style.
*   **Clarity in Max Stock Identification:** While `sort_values(""Stock"", ascending=False).head(1)` is correct, using `idxmax()` on the summed series (as in the ground truth) is often considered a more direct and sometimes slightly more efficient way to find the index of the maximum value for this specific task.

Overall, the code is highly functional and correctly solves the problem. The minor points are more about stylistic choices or slightly more idiomatic pandas operations rather than correctness issues.",llama-3.3-70b-instruct,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn...",6,"The generated code attempts to address many aspects of the prompt, but it falls short in several areas regarding advanced implementation and completeness.

**Areas for Improvement (to reach a higher score):**

1.  **Advanced Preprocessing:**
    *   **Multiple Imputation:** The code uses `SimpleImputer` (median), which is a basic strategy. The prompt specifically asked for ""multiple imputation strategies"" which implies more advanced methods like MICE (`IterativeImputer`) used in the ground truth.
    *   **Outlier Detection/Treatment:** No explicit outlier detection or treatment mechanism (like Winsorization or IQR-based capping) is implemented, which was a clear requirement.
    *   **Advanced Feature Engineering:** ""Polynomial and interaction features"" were requested, but `PolynomialFeatures` is missing. This is a significant omission.

2.  **Model Optimization:**
    *   **Hyperparameter Tuning:** While `GridSearchCV` is used, the `param_grids` provided are too narrow for ""advanced"" optimization. For example, `max_depth` for tree models includes `None`, which can lead to overfitting without sufficient regularization. Also, `SVM` is missing `kernel` in its `param_grid`.
    *   **Algorithm-Specific Parameters:** The parameters for `MLPClassifier` and `XGBClassifier` are also quite basic. No learning rate or regularization terms are explored for XGBoost or Gradient Boosting, which are crucial.

3.  **Ensemble Methods:**
    *   **Stacking Implementation:** The stacking implementation is incorrect. The `stacking_model` pipeline is defined but the `XGBClassifier` within it is directly fitted to `X_train`, which is not how stacking works. Stacking requires base models to predict on out-of-fold data (`X_train` is usually split into sub-folds, and base models train on one part and predict on another) to generate meta-features for the final estimator. The prompt asked for `StackingClassifier` from `sklearn.ensemble`.
    *   **Blending:** Blending was explicitly asked for but is completely missing.

4.  **Advanced Evaluation:**
    *   **Stratified K-Fold:** While `StratifiedKFold` is defined, it's used in a loop manually for base model evaluation, which is less robust than using it directly within `GridSearchCV`'s `cv` parameter or a `cross_val_score` function for consistent evaluation across models. The learning curve and validation curve functions correctly use it.
    *   **Evaluation Metrics:** The code primarily uses `accuracy_score`. The prompt requested a comprehensive list including `roc_auc_score`, `precision_score`, `recall_score`, `f1_score`, and `brier_score_loss`. These are largely missing from the evaluation printouts and dashboard.
    *   **Learning/Validation Curves:** These are plotted, which is good, but the input data for `learning_curve` and `validation_curve` should usually be the *processed* training data (`X_train_processed_df` in GT) and the `model` should be the optimized one, not the `Pipeline` directly from `models` dict (which might not be the optimized one).

5.  **Model Interpretability:**
    *   **SHAP:** SHAP is mentioned and `shap.summary_plot` is called, but the `explainer` is typically instantiated with a fitted model, and `shap_values` for `TreeExplainer` often return a list for multiclass classification which needs to be handled for binary cases (`shap_values[1]`). Also, no `dependence_plot` or other SHAP visualizations were included. The prompt specifically asked for SHAP *or* LIME, so SHAP is an acceptable choice if implemented well.

6.  **Statistical Significance Testing:**
    *   The `ttest_rel` is applied between raw `y_pred1` and `y_pred2`. Statistical significance testing between models should ideally be performed on a series of performance metrics (e.g., ROC AUC, F1-score) obtained from multiple cross-validation folds, not directly on the predictions from a single test set. This part is incorrect.

7.  **Model Comparison Dashboard:**
    *   Only a simple bar plot of accuracy is presented. The prompt asked for a ""comprehensive model comparison dashboard with performance metrics, feature importance, and prediction confidence intervals."" Confidence intervals are completely missing. Feature importance from SHAP is not integrated into a dashboard.

8.  **Model Calibration:**
    *   Calibration curves are plotted, which is good. However, Brier score comparison before/after calibration is missing. `CalibratedClassifierCV` is used, but the `sigmoid` method is only one option (isotonic is often preferred if enough data is available).

Overall, the code demonstrates basic understanding of some concepts but lacks the ""advanced"" and ""comprehensive"" implementations requested in the prompt, particularly in preprocessing, ensemble methods (blending, correct stacking), detailed evaluation metrics, statistical testing, and the complexity of the dashboard.",llama-3.3-70b-instruct,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
# Import necessary libraries
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import ols
import pymc3 as pm
from sklearn.impute import KNNImputer
from sklearn.impute import IterativeImputer
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import acc...",4,"The generated code attempts to address many requirements but falls short in correctness, advanced implementation details, and overall completeness.

**Primary Reasons for Failure & Critical Missing/Incorrect Aspects:**

1.  **Missing Data Imputation - MICE Incorrect:** The `impyute.imputation.cs.mice` function is used, but this library is not standard and not included in the ground truth's expected imports (pandas, statsmodels, pymc3, scipy, sklearn, matplotlib, seaborn, numpy). `IterativeImputer` from `sklearn` is the standard way to perform MICE-like imputation, as used in the ground truth. The generated code's MICE implementation would likely fail due to a missing dependency.
2.  **Multivariate Models - Incomplete/Incorrect:**
    *   **GAM:** The generated code uses `LinearGAM` from `pygam` and applies it directly to the target variable `Potability`, which is binary (classification). A `LogisticGAM` should be used for binary classification, as correctly implemented in the ground truth. The `accuracy` calculation after `LinearGAM` is also questionable for a regression model predicting a binary outcome.
    *   **Bayesian Logistic Regression:** The PyMC3 model setup is incorrect for logistic regression. It uses `pm.Bernoulli('y_obs', logit_p=mu, observed=y)` which implies `mu` is the log-odds (logit_p). However, the `y` variable is the entire `Potability` column from the original `water_quality` dataframe, not split into train/test, and it doesn't align with the scaling or data preparation. It also tries to use `X` directly within `pm.math.dot(X, beta)` without converting it to a PyMC3 shared variable or using `pm.Data`, which would lead to errors if `X` isn't a numpy array at that point. The `pymc3` implementation is also missing proper data preparation (`pm.Data`), which is crucial for making predictions on new data. Furthermore, the evaluation is just `pm.summary(trace)`, not actual prediction or performance metrics.
3.  **Hypothesis Testing - Substantially Incorrect/Misleading:**
    *   **MANOVA:** The generated code uses `ols` from `statsmodels.formula.api` and simply calls `model.summary()`. This is **Ordinary Least Squares (OLS) regression**, not Multivariate ANOVA (MANOVA). MANOVA is implemented using `statsmodels.multivariate.manova.MANOVA`, which is explicitly imported and demonstrated in the ground truth. This is a critical error.
    *   **Permutation Tests:** The implementation uses `scipy.stats.ttest_ind(df['Potability'], df['pH'], random_state=42)`. This is a standard independent samples t-test, not a permutation test. A permutation test involves shuffling data and recalculating the test statistic many times to build a null distribution, as shown in the ground truth, which is completely missing.
    *   **Bootstrap Confidence Intervals:** The `scipy.stats.bootstrap` function for CI calculation is deprecated or not directly providing the expected CI without a `data` parameter. The ground truth correctly implements a manual bootstrap loop for transparency and control.
4.  **Dimensionality Reduction - Factor Analysis:** The output of `fa.noise_variance_` for Factor Analysis is not `Explained Variance Ratio`. It's the unique variance (or error variance) for each original variable not explained by the common factors. This is a misleading statement for evaluation.
5.  **Robust Statistical Methods - Incorrect Application:**
    *   **HuberRegressor:** This is a robust *regression* model, not directly applicable as a robust *classification* method when `Potability` is the target. The ground truth demonstrated `statsmodels.robust.robust_linear_model.RLM` by modeling a *continuous* variable to showcase the robust regression concept, which is a more appropriate demonstration.
    *   **Robust PCA:** The generated code imports `PCA` again and claims its output is `Robust PCA Explained Variance Ratio` without specifying it's robust. Standard `sklearn.decomposition.PCA` is not inherently robust to outliers. There are robust PCA methods (e.g., based on median or L1 norms), but `sklearn.decomposition.PCA` is not one of them by default. This is a misrepresentation.
6.  **Model Diagnostics - Severely Lacking/Incorrect:**
    *   **OLS for Diagnostics:** The code uses `statsmodels.regression.linear_model.OLS` for `Potability` (binary classification). OLS is for linear regression with continuous outcomes, not logistic regression diagnostics. The ground truth correctly uses `statsmodels.genmod.generalized_linear_model.GLM` with a `Binomial` family, which is appropriate for logistic regression diagnostics.
    *   **VIF for entire DataFrame:** `variance_inflation_factor(X)` receives the entire DataFrame `X`, but `variance_inflation_factor` expects an array-like of the design matrix and an index of the column to calculate VIF *for*. Passing the whole DataFrame like this will lead to an error or incorrect calculation.
    *   **Goodness-of-Fit - Chi-square on target:** `chisquare(f_obs=df['Potability'].values)` performs a chi-square test for observed frequencies, not a goodness-of-fit test for a logistic regression model. This is incorrect.
    *   **Leverage Plots:** The `influence_plot(df, df['Potability'])` call is incorrect. `influence_plot` requires a `statsmodels` model result object, not a DataFrame and a column.
7.  **Completeness:** Many specific requirements like Q-Q plots, leverage plots and detailed residual analysis for GLM (as in GT) are either missing or incorrectly implemented. The concept of ""diagnostic dashboard"" in the prompt leads to `sns.pairplot`, which is more exploratory data analysis than model diagnostics.

The code attempts to touch upon all requirements but fails significantly in the depth and correctness of implementation for advanced statistical concepts. Many functions would either throw errors or produce meaningless results. The package imports are also not as comprehensive as the ground truth.",llama-3.3-70b-instruct,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, GaussianMixture
from sklearn.decomposition import PCA, FactorAnalysis
import umap
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.mixture import GaussianMixture
from sklearn.metrics import adjusted_rand_score
from scipy.cluster.hiera...",6,"The generated code attempts to address many aspects of the prompt but falls short in several areas, particularly concerning the ""advanced"" nature of the request, robustness, and completeness.

**Correctness:**
- **Feature Engineering:** The ""feature engineering"" section is highly misleading and incorrect. It merely renames existing numerical features into categorical 'level' features based on arbitrary thresholds and creates a 'potability_risk' from the 'Potability' target, which is then dropped. It absolutely *does not* engineer advanced features like water quality indices, chemical balance ratios, or contamination risk scores as explicitly requested. This is a critical failure in meeting the core task.
- **Preprocessing:** It drops the engineered categorical features before scaling, which makes sense, but the imputation of missing values is completely absent, even though the ground truth and real-world data often require it.
- **Optimal K Determination:** The code attempts this, but it's missing the Gap Statistic and the Elbow method is applied visually, which is standard, but the overall presentation and interpretation of scores could be better managed as in the ground truth.
- **Clustering Algorithms:** It applies KMeans, Agglomerative, DBSCAN, and GMM. However, it *misses* Spectral Clustering. Also, DBSCAN is applied with arbitrary `eps` and `min_samples` without the necessary k-distance graph analysis for proper parameter selection, making its application unlikely to yield meaningful results without manual tuning.
- **Dimensionality Reduction:** It includes PCA, UMAP, and t-SNE but *misses* Factor Analysis. Additionally, it only performs 2D reduction, not 3D, which was requested.
- **Cluster Stability Analysis:** This is almost entirely missing. There's no bootstrap resampling or consensus clustering implemented. The prompt explicitly asked for this.
- **Cluster Profiling:** The radar chart part is correct in its intent but `px.line_polar` is used incorrectly for a radar chart with `theta=cluster_profiles.index` (which would be cluster numbers) and `r=cluster_profiles['ph']`. A proper radar chart should have features as theta and values for each cluster as r, allowing comparison across clusters. The `cluster_profiles = water_quality_data.groupby('cluster').mean()` is a good start, but the visualization is flawed.
- **External Validation:** No explicit part is mentioned or implemented.
- **Anomaly Detection:** It correctly implements Isolation Forest, but misses the ""distance from cluster centroid"" method.

**Code Quality:**
- **Imports:** Mostly correct, but `TSNE` was missing from the `sklearn.manifold` import line. `GaussianMixture` was imported twice. `adjusted_rand_score` was imported but not used. `cophenet` and `pdist` were imported but not used.
- **Readability:** Variables are somewhat inconsistently named (e.g., `water_quality_data` then `water_quality_data_scaled`).
- **Structure:** The code is structured sequentially, but the output comments are less descriptive than the ground truth.
- **Error Handling:** Completely absent. The ground truth includes a basic `try-except` for file loading.
- **Warnings:** No warnings suppression.

**Completeness:**
- **Advanced Feature Engineering:** Failed significantly.
- **Multiple Clustering Algorithms:** Missing Spectral Clustering, poor DBSCAN parameterization.
- **Determining Optimal Clusters:** Missing Gap Statistic.
- **Dimensionality Reduction:** Missing Factor Analysis, only 2D not 3D.
- **Cluster Stability Analysis:** Almost entirely missing.
- **Cluster Profiling:** Incomplete and flawed radar chart.
- **Comprehensive Visualization Dashboard:** Lacks interactivity beyond basic `px.scatter`, no 3D visualizations from DR.
- **External Validation:** Not addressed.
- **Anomaly Detection:** One method implemented, one missed.
- **Temporal Patterns:** Acknowledges static data, similar to ground truth.

Overall, the generated code provides a basic framework for clustering but lacks the ""advanced"" and ""comprehensive"" details requested in the prompt, especially in feature engineering, stability analysis, and the full range of DR and validation techniques. The critical misinterpretation of feature engineering is a major drawback.",llama-3.3-70b-instruct,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Determine which features have the strongest positive and negative correlations with wine 'quality'
correlations = df.corrwith(df['quality']).abs().sort_values()

# List the top 3 positively correlated and top 3 negatively correlate...",7,"The generated code largely addresses the requirements but has a few critical flaws, especially in identifying the top positive and negative correlations, and some minor issues in plotting.

**Correctness:**
*   **Correlation Calculation (Flawed):** The `df.corrwith(df['quality']).abs().sort_values()` for `correlations` is not directly used to find the top/bottom 3. More critically, the `top_pos` and `top_neg` calculations are incorrect.
    *   `top_pos = df.corrwith(df['quality']).sort_values(ascending=False).head(4)[1:]` is problematic. `head(4)[1:]` is an attempt to remove 'quality' itself, but it implicitly assumes 'quality' will always be the first item. If 'quality' is not the highest correlated (e.g., if there were perfect positive correlations with other features, which is unlikely but not guaranteed), this might miss a true top positive correlation. A better approach is to explicitly drop 'quality' before taking the top N.
    *   `top_neg = df.corrwith(df['quality']).sort_values(ascending=True).head(3)` directly takes the lowest 3. This is correct if the lowest 3 are indeed negative correlations. However, `correlation.drop('quality').nsmallest(3)` is a more robust way to get the *most negative* (smallest values) after removing self-correlation. The current `head(3)` might include features that are slightly positive but small, if the range of negative correlations is narrow. The task specifically asks for *negatively correlated* features.
*   **Scatter Plots and Regression Lines:** The scatter plots and linear regression lines are correctly generated using `sklearn.linear_model.LinearRegression`.

**Code Quality:**
*   **Imports:** Proper imports are included.
*   **Structure:** The code is logically structured, following the steps from the prompt.
*   **Clarity:** Variable names are clear.

**Completeness:**
*   **1) Strongest positive/negative correlations:** Partially correct. The method for finding top 3 is not robust for positive and slightly off for negative.
*   **2) List top 3 positive/negative:** The output for this is incorrect due to the calculation issues mentioned above.
*   **3) Correlation matrix heatmap:** Correctly generated.
*   **4) Scatter plots with regression lines:** Correctly generated.

**Areas for Improvement:**
1.  **Robust Correlation Identification:**
    *   Calculate `quality_correlations = df.corr()[TARGET_FEATURE].drop(TARGET_FEATURE)`.
    *   Then, `top_3_positive = quality_correlations.nlargest(3)` and `top_3_negative = quality_correlations.nsmallest(3)`. This is more robust and explicitly targets the strongest positive and negative correlations.
2.  **Explicit Labels for Regression:** Add labels (e.g., R-squared) to the regression lines in the scatter plots, similar to the ground truth, to provide more analytical insight.
3.  **Visualization Aesthetics:** While functional, adding labels, titles, and legends to plots, and potentially customizing figure sizes (e.g., using `plt.figure(figsize=...)` for each plot rather than `subplots` for all) would improve readability and presentation.
4.  **Error Handling:** The ground truth includes basic error handling for file loading, which is good practice for production-ready code. While not explicitly asked for, it's a good touch.",llama-3.3-70b-instruct,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load dataset and clean missing values
data = pd.read_csv('video_games_sales.csv')
data.fillna(0, inplace=True)

# T-test for comparison of Nintendo games with other publishers
nintendo_data = data[data['Publisher'] == 'Nintendo']
other_publishers_data = data[data['Publisher']!= 'Nintendo']
t_stat_nintendo, p_value_nintendo = stats.ttest_ind(nintendo_data['Global_Sales'], other_publishers_data['Global_Sales'])
print(""T-te...",6,"The generated code attempts to address all parts of the task, which is a good start. However, it has significant issues in data cleaning and interpretation that lead to potentially incorrect statistical results and incomplete output.

**Strengths:**
*   Correctly identifies and attempts to apply all four required statistical tests (t-test, chi-square, Mann-Whitney U).
*   Correctly uses `scipy.stats` for the tests.
*   Attempts to extract the top 5 genres and platforms for the chi-square test.
*   Includes a preliminary interpretation of results based on the p-values and a significance level.

**Weaknesses and Areas for Improvement:**

1.  **Data Cleaning (Critical Failure):**
    *   The line `data.fillna(0, inplace=True)` is **critically flawed** for this dataset. Filling missing numerical sales data with 0 drastically alters the distribution and mean, making subsequent statistical tests invalid. Filling categorical columns like 'Publisher', 'Genre', 'Platform' or 'Year_of_Release' (before conversion) with 0 also makes no sense and will cause errors or incorrect grouping.
    *   It does **not** handle 'Year_of_Release' conversion to `int` or `float` before filtering, which will likely cause issues if `Year_of_Release` contains `NaN`s or non-numeric strings (as is common). The ground truth specifically addresses this.
    *   It does **not** handle missing values in 'Global_Sales' appropriately (i.e., dropping rows, not filling with 0).
    *   No handling for 'tbd' in 'User_Score' which, while not used in the hypothesis tests, was mentioned in the prompt as a good practice for comprehensive cleaning.

2.  **Interpretation of Results (Incomplete):**
    *   The interpretation just states `Reject H0: True/False` but doesn't explain what H0 or H1 are for each test, nor does it provide a meaningful, human-readable conclusion for each test (e.g., ""Nintendo sales are significantly higher"" instead of just ""True""). The prompt explicitly asks to ""interpret results"".
    *   It only displays the p-value, not the t-statistic, chi-square static, or U-statistic, which are useful for understanding the magnitude and direction of the effect.

3.  **T-test Alternatives and Assumptions:**
    *   The `ttest_ind` function has an `equal_var` parameter (defaults to `True` for Student's t-test). For sales data, variances are often unequal, making Welch's t-test (`equal_var=False`) more robust. The ground truth used this.
    *   For the ""Nintendo vs Other Publishers"" test, the prompt asked to test if Nintendo games have ""significantly higher"" sales. This implies a **one-tailed test** (alternative='greater'). The generated code uses the default two-tailed test, which is incorrect for this specific hypothesis.

4.  **Edge Cases/Robustness:**
    *   The code does not include any checks for empty dataframes or groups before performing statistical tests (e.g., `if len(group) < 2:`), which can lead to errors if a publisher or genre has very few entries, or if data cleaning leads to empty sets. This is a crucial robustness check.
    *   No `try-except` block for `pd.read_csv`, which is good practice.

5.  **Clarity and Structure:**
    *   While the code is somewhat structured by test, it lacks the detailed print statements and clear separation of sections that make the ground truth code much more readable and user-friendly for complex tasks involving multiple tests.

The critical flaw in data cleaning (especially `fillna(0)`) makes the generated statistical results highly suspect and invalidates the primary goal of the task from a data science perspective. The lack of detailed interpretation also falls short of the ""interpret results"" requirement.",llama-3.3-70b-instruct,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt

# Load dataset and handle missing values
df = pd.read_csv('video_games_sales.csv')
df = df.fillna(df.mean())

# Create binary target variable
df['High_Sales'] = np....",6,"The generated code addresses most of the requirements but has several critical issues and areas for improvement, primarily concerning missing value handling, feature selection, and robustness.

**Strengths:**
*   Correctly loads the dataset and creates the 'High_Sales' target variable.
*   Correctly uses `LabelEncoder` for the specified categorical features.
*   Correctly creates the `Total_Regional_Sales` and `Years_Since_Release` features.
*   Successfully splits data into train/test sets.
*   Correctly trains a `RandomForestClassifier` with 100 estimators.
*   Calculates and prints the required evaluation metrics (accuracy, precision, recall, F1, confusion matrix).
*   Correctly calculates and attempts to display feature importance.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling (Critical Flaw):**
    *   The `df.fillna(df.mean())` approach is problematic, especially for categorical columns which can't be filled with a mean, and for `Year` which is crucial for `Years_Since_Release`.
    *   The prompt specifies ""Handle missing values."" The ground truth drops NaNs for critical columns like `Global_Sales`, `Year`, `Publisher`, and regional sales, which is a more sensible approach for this task, as `mean()` imputation for `Year` would introduce fractional years and for categorical columns would fail or lead to errors if applied without selecting numerical columns first. `Global_Sales` cannot be imputed with mean if it's used for target variable creation.
    *   This is a major correctness issue as it can lead to incorrect data and model behavior.

2.  **Feature Selection for `X`:**
    *   The generated code explicitly selects a subset of features for `X`: `[['Platform', 'Genre', 'Publisher', 'Total_Regional_Sales', 'Years_Since_Release']]`.
    *   It **omits** the original `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales` columns, which are strong predictors and were explicitly part of the features in the ground truth and generally in such ML tasks. While `Total_Regional_Sales` is created from them, dropping the individual regional sales values removes valuable granular information. This omission reduces the potential performance of the model significantly.

3.  **Feature Importance Visualization:**
    *   While it attempts to display feature importance using `matplotlib.pyplot.bar`, the `plt.show()` call is missing in a benchmark environment, which would prevent the plot from displaying.
    *   The prompt specifically asks to ""Display feature importance"" which usually implies printing to console or a simple display, not necessarily a plot, but a plot is a good addition. However, the ground truth's approach of printing a sorted Series is simpler and guaranteed to display in a non-GUI environment.

4.  **Robustness (File Not Found):**
    *   The ground truth includes a `try-except FileNotFoundError` block, which makes the code more robust against the CSV file not being present. The generated code does not include this.

5.  **Unnecessary Import (`matplotlib.pyplot`):**
    *   While used, `matplotlib` is not explicitly required by the prompt's output format (which implies textual output) and adds an extra dependency. If a plot is intended, it should be fully implemented and handled.

Overall, the core structure is there, but the fundamental data preprocessing steps for missing values and the selection of features for the model training are significant deviations from best practices and the complete fulfillment of the task.",llama-3.3-70b-instruct,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression

# Load dataset and handle missing values
def load_dataset(file_name):
    data = pd.read_csv(file_name)
    data.fillna(data.mean(), inplace=True)
    return d...",4,"The generated code attempts to address the requirements but falls short in several critical areas, primarily due to incorrect or incomplete implementations of the requested features and data handling.

**Correctness & Completeness Issues:**

1.  **Missing Values Handling:**
    *   The `load_dataset` function uses `data.fillna(data.mean(), inplace=True)`. This is problematic for categorical columns like 'Publisher', 'Genre', and 'Name', which would become NaN after `mean()` fails on them, or would not be handled at all. The ground truth correctly uses `SimpleImputer` with strategies appropriate for numerical (median/mean) and categorical (most_frequent) data types.
    *   `Year` is treated purely numerically, but it's often more appropriate to impute it carefully (e.g., median) and then convert to int, which the ground truth does.

2.  **Feature Engineering:**
    *   **'Sales_Ratio_NA_EU':** The generated code `data['Sales_Ratio_NA_EU'] = data['NA_Sales'] / data['EU_Sales']` does not handle potential division by zero (or very small numbers) which the ground truth correctly mitigates with `+ 1e-6`. This could lead to infinite or NaN values.
    *   **'Genre_Market_Share':** The definition `data.groupby('Genre')['Global_Sales'].transform('sum') / data['Global_Sales'].sum()` is incorrect. `data['Global_Sales'].sum()` calculates the sum of *all* global sales in the dataset, not the sum of global sales *per genre*. The ground truth correctly calculates the sum of global sales per genre and then divides by the total sum of *all* global sales to get the market share. The generated code calculates `(sum of sales per genre) / (total sales)` for *every row* which is not market share. It should be `(sum of sales FOR THAT GENRE) / (total sales ACROSS ALL GENRES)`. The `transform` function would correctly apply the genre-specific sum to each row belonging to that genre, but the denominator is wrong.
    *   **Interaction Features (`Year` x `Genre`):** The generated `data['Year_Genre_Interaction'] = data['Year'] * data.groupby('Genre')['Year'].transform('mean')` is a very strange and incorrect interpretation of ""interaction features between Year and Genre"". It's multiplying the year of a game by the *average year* of games in that genre, which is not a standard interaction term. Interaction terms typically involve element-wise (or column-wise) multiplication of the features themselves, often after one-hot encoding categorical features. The ground truth correctly uses one-hot encoding for 'Genre' and multiplies 'Year' by each genre dummy column.

3.  **PCA Analysis:**
    *   `pca_analysis(data)`: It performs PCA only on `data.select_dtypes(include=[np.number])`. This might include the original sales columns *and* the newly engineered sales columns, which is redundant and could lead to less meaningful principal components. Also, `n_components=0.95` is passed, which automatically selects components to explain 95% variance, but the task asked to ""explain variance"", implying showing the individual variance ratio or cumulative plot, which the ground truth does by not limiting `n_components` initially and then plotting.
    *   The returned `pca_data` is not integrated back into `data` in the `main` function, limiting its subsequent use.

4.  **Polynomial Features:**
    *   The `polynomial_features` function works correctly for creating the features, but the resulting `poly_df` DataFrame is created, printed in `main` but then *not concatenated back* into the main `data` DataFrame, making it unused for subsequent steps like outlier handling or potential further analysis. This indicates incompleteness in integrating engineered features.

5.  **Outlier Handling:**
    *   The `handle_outliers` function is deeply flawed. It passes `data.select_dtypes(include=[np.number])` to it, and the function then calls `data.quantile(0.25)`, `data.quantile(0.75)`, etc. If `data` is a DataFrame with multiple numerical columns, `data.quantile()` will return a Series where the index matches the columns, and `IQR` will also be a Series. The line `data = data[~((data < lower_bound).any(axis=1) | (data > upper_bound).any(axis=1))]` attempts to filter rows. This approach *removes* rows with outliers, whereas the task requested to ""Identify and handle outliers"", and the ground truth demonstrated *capping* outliers, which is generally preferred to avoid losing data, especially when dealing with skewed distributions common in sales data. Removing rows can significantly reduce the dataset size.
    *   The ground truth specifies `outlier_cols` for which to apply the IQR method. The generated code applies it to all numerical columns.

**Code Quality Issues:**

*   **Modularity vs. Integration:** While using functions for each step is good for modularity, several functions (like `pca_analysis`, `polynomial_features`) return new DataFrames without integrating them back into the main `data` DataFrame processed in `main`. This breaks the flow of continuous data transformation.
*   **Prints and Explanations:** The original task asked for ""only the Python code without explanations"" for the output, but the ground truth produced print statements. The generated code has minimal useful print statements (only `print(data.head())` at the end and the explained variance, which is good but not enough for the full lifecycle).
*   **Error Handling:** The `load_dataset` function doesn't include specific `try-except` for `FileNotFoundError` as the ground truth did, making it less robust.

Overall, the generated code demonstrates an understanding of the components required, but the implementation details are often incorrect, incomplete, or misinterpret the specific requirements of advanced data analysis steps.",llama-3.3-70b-instruct,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load dataset
data = pd.read_csv('diabetes.csv')

# Replace zero values with NaN and perform median imputation
data = data.replace(0, np.nan)
data = data.fillna(data.median())

# T-test for glucose levels
t_stat, p_value_t = stats.ttest_ind(data.loc[data['Outcome'] == 1, 'Glucose'], data.loc[data['Outcome'] == 0, 'Glucose'])
print(f'T-test p-value: {p_value_t}')

# Mann-Whitney U test for BMI
u_stat, p_value_u = stats.man...",6,"The generated code addresses most of the requirements but has significant flaws in data preparation and lacks detailed interpretation.

**Strengths:**
*   It correctly identifies and uses the specified statistical tests (t-test, Mann-Whitney U, chi-square, Pearson correlation, ANOVA).
*   It attempts to perform data cleaning (zero values to NaN and median imputation).
*   It provides a basic interpretation of results based on the p-value.

**Weaknesses and Areas for Improvement:**

1.  **Incorrect Data Cleaning:** The global `data = data.replace(0, np.nan)` followed by `data = data.fillna(data.median())` is incorrect and harmful for this specific dataset.
    *   `Pregnancies` can legitimately be 0 and should not be replaced with NaN.
    *   `Outcome` (the target variable) can also be 0 and should definitely not be imputed, as it would alter the labels.
    *   Other columns like `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, and `BMI` indeed can't be 0, but the global replacement treats all 0s universally. The ground truth correctly identifies specific columns for imputation.
    *   Median imputation is done globally, which means columns that weren't expected to have NaNs (like `Pregnancies` if a 0 was incorrectly turned to NaN and then imputed) would also get altered, potentially leading to misleading data.

2.  **Lack of Robustness:** There's no error handling for file not found (e.g., `try-except` block for `pd.read_csv`).

3.  **Missing Interpretation Details:** While it prints ""Significant difference"" or ""No significant difference,"" it lacks the detailed interpretation required by the prompt, such as comparing means/medians, the direction of the correlation, or the implications of the chi-square and ANOVA results. The prompt specifically asks for ""interpret results with significance level =0.05"".

4.  **Age Group Definitions:** The age group bins are slightly off compared to the prompt's explicit definition (`young: <30`, `middle: 30-50`, `older: >50`). The generated code uses `[0, 30, 50, 100]` which creates `(0, 30]`, `(30, 50]`, `(50, 100]`. The ground truth uses `[0, 29, 50, np.inf]` with `right=True` to achieve `<30`, `30-50`, and `>50`. This is a subtle but important difference in categorization.

5.  **BMI Category Definitions:** Similar to age groups, the BMI categories are slightly off. The prompt specifies: `Underweight: <18.5, Normal: 18.5-24.9, Overweight: 25-29.9, Obese: >=30`. The generated code uses `[0, 18.5, 25, 30, 100]`, which with `pd.cut` defaults to `right=True`, means `(0, 18.5]`, `(18.5, 25]`, `(25, 30]`, `(30, 100]`. The ground truth uses `right=False` and the specified boundaries correctly.

6.  **Readability/Clarity:** The output is just p-values and a generic interpretation. It doesn't print the statistics (t-stat, U-stat, chi2-stat, F-stat) of the tests, nor the medians/means of the groups being compared, which are crucial for understanding the nature of any observed differences.

To get a higher score, the generated code needs to correct the data cleaning logic, provide more detailed output and interpretation as requested, and ensure category binning aligns precisely with the problem statement.",llama-3.3-70b-instruct,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
f...",6,"The generated code addresses most of the requirements but has several issues that reduce its correctness, completeness, and code quality.

**Correctness & Completeness:**

*   **Handling Missing Values (Zeros):** The ground truth correctly identifies specific columns where 0s represent missing values and replaces them with NaN before imputation. The generated code replaces 0s with the median in *all* columns (except 'Outcome'), which is incorrect for columns like 'Pregnancies', 'Age', etc., where 0 is a valid value. This significantly impacts preprocessing and model performance.
*   **Feature Engineering - `Age_group` and `Insulin_level`:**
    *   `Age_group`: The bins chosen in the generated code (`[0, 25, 35, 45, 60]`) are less granular and might not capture relevant age patterns as effectively as the ground truth's definition. The labels used (`Young`, `Adult`, `Middle_Aged`, `Old`) are also quite broad.
    *   `Insulin_level`: The generated code uses `df['Insulin'].quantile(0.33)` and `df['Insulin'].quantile(0.67)` which is correct for tertiles on the surface, but the `include_lowest=True` might lead to issues if 0 values are still present or if the quantile calculation behaves unexpectedly with the median-imputed data (though it's conceptually correct). The main issue here is the incorrect 0-value handling prior to this.
*   **Categorical Encoding:** The generated code uses `LabelEncoder` for `Age_group` and `Insulin_level`. This implicitly assigns ordinal relationships (e.g., 'low' < 'normal' < 'high' for insulin, or 'Young' < 'Adult' for age group), which is generally inappropriate for nominal categorical variables like these. `OneHotEncoder` (as used in the ground truth via `ColumnTransformer`) is the correct approach to avoid misleading the models.
*   **Preprocessing Pipeline:** The generated code processes numerical and categorical features separately and then only scales numerical features. `StandardScaler` is applied to `X_train` and `X_test` *after* `LabelEncoder` has already modified `Age_group` and `Insulin_level` into numerical values, which then get unnecessarily scaled. This isn't a robust preprocessing strategy compared to `ColumnTransformer` from `sklearn.compose`, which efficiently handles different types of features with specific transformations. The ground truth correctly uses `ColumnTransformer` to define a proper preprocessing pipeline, which is crucial for good machine learning practice.
*   **Feature Importance Display:** The generated code displays feature importance but `X.columns` would only show the original columns pre-encoding. For tree-based models, feature importance should be displayed using the *encoded* feature names (including one-hot encoded categories), which the ground truth correctly handles by creating `all_feature_names`.
*   **Confusion Matrix and ROC Curve Plotting:** The generated code creates separate plots for each model, leading to many individual figures. The ground truth code efficiently groups them (e.g., all confusion matrices in a single subplot grid, all ROC curves on one plot), which is much better for comparison and visualization.
*   **Reproducibility:** The ground truth includes `np.random.seed(42)` and `random_state` in all models/splits for reproducibility, which is good practice. The generated code only sets `random_state` for `RandomForestClassifier` and `GradientBoostingClassifier`, not `LogisticRegression` or `SVC`, nor the `train_test_split`.
*   **Evaluation Metrics:** The generated code calculates the metrics for each model but doesn't store them in a structured way (like the `results` dictionary in the GT), making it harder to compare. Also, the cross-validation scoring is only shown for default `accuracy_score`, not for all metrics like precision, recall, F1, and AUC as requested.

**Code Quality:**

*   **Imports:** The generated code has correct imports for the functionalities it attempts.
*   **Structure:** The code is structured somewhat sequentially, but less cleanly separated into logical blocks compared to the ground truth's use of comments and clear sectioning.
*   **Efficiency:** The iterative plotting of confusion matrices and ROC curves is less efficient and user-friendly than using subplots or combining ROC curves on a single graph.
*   **Clarity:** The preprocessing steps are less clear and prone to errors due to the manual `LabelEncoder` application instead of a robust `ColumnTransformer`.

Overall, while it runs and produces some output, the core preprocessing and encoding steps deviate significantly from best practices and would lead to less accurate and reliable models, especially in the handling of missing values and categorical features. Many visualization aspects are also suboptimal.",llama-3.3-70b-instruct,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, RFE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from imblearn.over_sampling ...",5,"The generated code attempts to address many aspects of the prompt but falls short in several key areas, leading to incorrect or incomplete execution of the requirements.

**Correctness:**
*   **Missing Value Handling:** The generated code uses `df.dropna(inplace=True)` after replacing 0s with NaNs. This is problematic because many medical datasets have 0s that represent missing values (e.g., Glucose, BloodPressure, BMI), and simply dropping rows with these values would remove a significant portion of the dataset and lead to data loss. The ground truth correctly identifies specific columns where 0s are problematic and applies median imputation. The generated code's approach is incorrect for this dataset.
*   **Engineered Feature 'Pedigree_Age_product':** The prompt asked for `Pedigree_Age_product`. The generated code uses `df['PedigreeFunction'] * df['Age']` which is likely a typo for `DiabetesPedigreeFunction`. The ground truth uses `DiabetesPedigreeFunction`.
*   **Polynomial Features Application:** The generated code applies `PolynomialFeatures` without dropping the original columns, leading to redundancy. Also, it concatenates `poly_df` where `poly_df` includes an intercept term which may not be desirable and duplicates the `1` feature. The ground truth handles this by dropping the original columns and then concatenating, ensuring proper feature transformation.
*   **PCA Explanation:** The prompt explicitly asks to ""explain variance ratios."" While the code plots `variance_ratios`, it doesn't print or numerically explain them as the ground truth does (e.g., cumulative explained variance, number of components).
*   **Feature Selection (SelectKBest, RFE):** The generated code applies `SelectKBest` and `RFE`, but the output `top_features_kbest` and `top_features_rfe` are just numpy arrays of the selected feature *values*, not the *names* of the selected features. This makes it impossible to know which features were actually selected. The ground truth correctly retrieves and prints the names of the selected features.
*   **Correlation Filtering:** The correlation filtering is applied to the original `df` *before* scaling and before defining `X` and `y` clearly, which is not ideal. It should be applied to the processed feature set (`X_processed` or `X_scaled_standard_df` in the ground truth).
*   **Overall Flow:** The execution order for operations like scaling, correlation filtering, PCA, and feature selection is a bit jumbled, making it harder to follow the data transformation pipeline. For instance, PCA and KBest/RFE are applied to `df_standard`, but then correlation filtering is applied to `df`, and SMOTE is applied to `df_filtered`, creating inconsistencies in the feature sets used. The ground truth maintains a clear flow (`X` -> scaling -> correlation filtering -> PCA/feature selection on a consistent `X_processed`/`X_filtered_corr`).

**Code Quality:**
*   **Imports:** Mostly good, but `LabelEncoder` is imported but not used.
*   **Readability:** The variable naming is generally clear.
*   **Comments/Prints:** Lacks the informative print statements at each major step that the ground truth provides, making it harder to track execution and intermediate results.
*   **Warnings:** Doesn't suppress warnings, which can be noisy (e.g., from `LogisticRegression` solver).

**Completeness:**
*   **All Engineered Features:** The engineered features are attempted, but `PedigreeFunction` instead of `DiabetesPedigreeFunction` is a minor error. The `Risk_Factor_Count` thresholds (`Insulin > 200`) are arbitrary and not based on common medical guidelines like the ground truth's use of Glucose, BP, BMI, etc.
*   **Polynomial Features:** Applied, but with issues as noted above.
*   **Scaling:** Both `StandardScaler` and `MinMaxScaler` are applied, but only `StandardScaler` is explicitly used for subsequent steps, which is acceptable.
*   **PCA:** Applied, but the explanation of variance ratios is only visual, not numerical as requested.
*   **SelectKBest & RFE:** Used, but the output of selected features is not properly captured/displayed.
*   **Correlation Filtering:** Implemented, but applied to an inconsistent data frame (`df` instead of scaled `X`).
*   **SMOTE:** Applied correctly.
*   **Feature Importance:** Applied correctly and visualized.

In summary, the generated code has significant functional issues, especially regarding missing value imputation and maintaining a consistent data pipeline for transformations, which would lead to incorrect results or poor model performance. While it attempts to implement most requested features, the correctness and rigorousness are lacking.",llama-3.3-70b-instruct,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Handle missing values
df = df.dropna()

# Test 1: t-test for Western European countries
western_europe = df[df['Regional indicator'] == 'Western Europe']
other_regions = df[df['Regional indicator'] != 'Western Europe']
t_stat, p_val_1 = stats.ttest_ind(western_europe['Ladder score'], other_regions['Ladder score'])
prin...",4,"The generated code attempts to address all parts of the task but fails significantly in several key areas, leading to incorrect or incomplete analysis for most tests.

**Major Issues:**

*   **Missing Value Handling:** The generated code uses `df.dropna()`. While this handles NaNs, the ground truth code specifically drops NaNs only from ""critical columns"". `dropna()` without specifying `subset` drops rows if *any* column has a NaN, which might be overly aggressive and remove more data than necessary for the specific analyses required. However, the problem statement says ""handle any missing values"" and doesn't specify how, so `dropna()` is a valid, albeit potentially suboptimal, interpretation. **Correction:** This point is probably minor for this problem.
*   **Missing Interpretation Details (alpha, direction of difference):** The generated code only prints ""Reject/Accept null hypothesis"" and whether there's a ""significant difference."" It **completely lacks interpretation of the direction of the difference or correlation** (e.g., ""significantly HIGHER happiness scores,"" ""significantly POSITIVE correlation""), which is a critical part of statistical analysis and was explicitly asked for in the prompt (""interpret results with significance level =0.05""). The ground truth code clearly shows how this interpretation should be done for each test, based on the means, medians, or correlation coefficients.
*   **Test 3 (ANOVA): Incorrect usage:** The generated code calls `stats.f_oneway(df['Ladder score'], df['Regional indicator'])`. This is fundamentally incorrect. `f_oneway` expects multiple arrays, where each array represents a group to be compared. `df['Regional indicator']` is a series of strings, not numerical data suitable for ANOVA comparison against 'Ladder score'. The ground truth correctly groups 'Ladder score' by each unique 'Regional indicator' and passes these groups as separate arguments to `f_oneway`. This test is fundamentally flawed.
*   **Test 5 (Chi-square): Incorrect category creation and contingency table:**
    *   The creation of happiness categories is not robust (`np.concatenate` and `np.repeat` from slicing based on arbitrary thresholds is messy and prone to errors). A more robust way (as in the ground truth) is to use `pd.qcut` or define thresholds based on quantiles and then apply a function (as in the ground truth).
    *   More critically, `cont_table = pd.crosstab(happiness_categories, regions)` will likely fail or produce an incorrect table because `happiness_categories` and `regions` are concatenated arrays from potentially non-aligned dataframes *after* arbitrary slicing, making their direct alignment questionable. The `pd.crosstab` approach in the ground truth (`pd.crosstab(df['Happiness Category'], df['Regional indicator'])`) is correct because `Happiness Category` is a new column *within the same DataFrame*, ensuring correct alignment.
*   **Inconsistent Parameter Usage (e.g., `equal_var` in t-test):** The ground truth uses `equal_var=False` for t-tests, which is generally more robust and recommended when assumptions about equal variance cannot be guaranteed. The generated code defaults to `equal_var=True`, which is less conservative and might be inappropriate. While not explicitly stated in the prompt, it's a good practice often followed in real-world data analysis and the ground truth demonstrated it.
*   **Redundant Import:** `matplotlib.pyplot as plt` is imported but not used.

**Minor Issues:**

*   **Lack of Readability and Debug Output:** The generated code directly prints p-values without context or relevant statistics (like means, medians, U-statistics, F-statistics, correlation coefficients). The ground truth provides detailed print statements for each test, including the relevant statistics and clear ""Result"" and ""Interpretation"" sections, making the output much more understandable and useful.
*   **Error Handling:** The ground truth includes basic file not found error handling, which is good practice. The generated code does not.
*   **Code Structure:** Merging all interpretation at the end after all tests are run makes it harder to follow the results of each test individually. The ground truth's structure of printing results immediately after each test is clearer.",llama-3.3-70b-instruct,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.compose import ColumnTrans...",5,"The generated code attempts to address all parts of the task but falls short on several critical aspects, leading to functional errors and suboptimal implementation.

**Correctness:**
*   **Preprocessing:** The `preprocess_data` function uses `df.dropna()` without specifying a subset, which is too broad and removes all rows with any missing value, even in irrelevant columns. More critically, it uses `LabelEncoder` on 'Regional indicator' but then doesn't include 'Regional indicator' in the `X` features, making the encoding step useless and misleading. The `X` features are hardcoded with simplified names like 'GDP per capita' and 'Freedom' which do not match the actual column names in the dataset ('Logged GDP per capita', 'Freedom to make life choices'). This will cause a KeyError.
*   **Model Training/Evaluation:** The `evaluate_models` function fits the models on `X_test`, `y_test` and then evaluates predictions on the same `X_test`, `y_test`. This is incorrect. Models should be fitted on `X_train`, `y_train` and then evaluated on `X_test`, `y_test` to assess generalization performance.
*   **Hyperparameter Tuning:** The `hyperparameter_tuning` function receives a `model` object directly, but then attempts to use `param_grid[model.__class__.__name__]`. This is problematic because `model.__class__.__name__` could be 'LinearRegression', 'Ridge', etc., and there are no `param_grid` entries for those, only for 'Random Forest Regressor' and 'Gradient Boosting Regressor'. This design prevents tuning of all models or leads to errors for untuned models. Furthermore, `GridSearchCV` should be initialized with `estimator=model`, not just `model`.
*   **Feature Importance:** The `create_plots` function's assumed `model` (which is `tuned_model`) might not always have `feature_importances_` (e.g., if the best model was Linear Regression, Ridge, or Lasso). The code doesn't explicitly check for this for all models, though it does for `tuned_model`.
*   **Column Mismatch:** The `new_data` in `make_predictions` uses `GDP per capita` etc., which again do not match the expected column names used in `X` after preprocessing. This will lead to errors during prediction.

**Code Quality:**
*   **Modularity:** The use of functions (`load_data`, `preprocess_data`, etc.) is good for modularity, but the overall execution flow in `main` could be cleaner and more robust.
*   **Error Handling:** No error handling for `FileNotFoundError` when loading the CSV.
*   **Random State:** Missing `random_state` for `train_test_split` and for initializing models (Ridge, Lasso, RandomForestRegressor, GradientBoostingRegressor) which affects reproducibility.
*   **Clarity:** The feature columns are hardcoded with simplified names that don't match the dataset, leading to potential confusion and errors.
*   **Imports:** `SimpleImputer` and `ColumnTransformer` are imported but not used, indicating a lack of clear strategy for preprocessing. `StandardScaler` (which is usually essential for penalized regression models) is not imported or used.

**Completeness:**
*   **Missing Value Handling:** The `dropna()` is too simplistic and doesn't explicitly mention dropping rows only if key columns are missing.
*   **Categorical Encoding:** The prompt explicitly asks for encoding categorical variables if used. While the chosen features are numerical, the generated code attempts to encode 'Regional indicator' but then doesn't use it as a feature, making the encoding step redundant. The ground truth correctly identifies that no encoding is required for the *chosen* features.
*   **Residual Plots:** The residual plot generated is a scatter plot of `Actual vs Predicted`, which is useful, but the classic residual plot (Residuals vs Predicted) is also mentioned in the prompt and attempted, but without proper labeling/interpretation based on the plot. No specific residual plot (like residuals vs. feature) is created.
*   **Feature Importance:** The feature importance plot is generic (uses indices instead of feature names), making it less informative without mapping to original feature names.
*   **Cross-validation:** Implemented, but the output description is minimal.
*   **Analysis of Predictions:** The prompt asks for *analysis* of new predictions, which is absent in the generated code.

The reliance on simplified column names for features and the incorrect fitting/evaluation in `evaluate_models` are critical flaws that prevent the code from running successfully or providing meaningful results as intended by the task.",llama-3.3-70b-instruct,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

# Load...",3,"The generated code attempts to address the task but fails significantly in correctness and completeness, making it largely unsuitable for the specified advanced feature engineering and dimensionality reduction.

**Major issues for the low score:**

1.  **Incorrect Data Handling & Preprocessing:**
    *   The `preprocess_data` function uses 'GDP per capita' instead of 'Logged GDP per capita', which is the correct column name in the dataset. This leads to `KeyError` or incorrect calculations.
    *   Missing values (`nan`) are not explicitly handled in the preprocessing step, which is crucial for subsequent calculations and can lead to errors.
    *   The `Regional_Happiness_Rank` and `GDP_vs_Regional_Average` calculations in the generated code use `method='min'` and a lambda function for `transform` which will produce incorrect results or not match the ground truth's logic for calculating mean differences.
    *   The PolynomialFeatures, scaling, PCA, t-SNE, and other functions repeatedly select only a very small, fixed subset of features (['Ladder score', 'GDP per capita', 'Social support', 'Healthy life expectancy']) instead of the dynamically growing set of features after engineering. This severely limits the scope and purpose of feature engineering.

2.  **Incorrect Use of Scikit-learn Functions (Classification vs. Regression):**
    *   The task explicitly describes a regression problem (e.g., 'Ladder score' is a continuous variable). However, the generated code frequently uses classification estimators and scoring functions:
        *   `mutual_info_classif` instead of `mutual_info_regression`
        *   `f_classif` instead of `f_regression`
        *   `RandomForestClassifier` instead of `RandomForestRegressor`
    *   This is a fundamental error that makes the feature selection and importance evaluation inappropriate and potentially misleading for the given task.

3.  **Incomplete Feature Engineering:**
    *   Several required engineered features are completely missing:
        *   'Regional_Happiness_Rank' is implemented, but 'GDP_vs_Regional_Average' is present but conceptually flawed due to `transform(lambda x: x - x.mean())` which does not create a new column, rather computes a value per group. The ground truth correctly calculates the regional mean first and then subtracts it.
        *   Interaction features are defined in `create_interaction_features` but are poorly implemented (e.g., `data[['Ladder score', ...]].mul(data[['Ladder score', ...]].T)` creates a giant DataFrame and doesn't explicitly create 'GDP_x_SocialSupport' etc.). Polynomial features implicitly create some interactions, but the specific ones requested (GDP_x_SocialSupport, GDP_x_HealthyLifeExpectancy) are not explicitly created.

4.  **Ineffective Feature Selection & Dimensionality Reduction:**
    *   PCA and t-SNE are applied to a tiny, fixed subset of original features rather than the progressively engineered and scaled feature set, making them largely pointless for evaluating the full impact of feature engineering.
    *   Feature selection methods (SelectKBest, RFE) are also applied to this tiny subset, and again, use classification metrics. The `y` variable for feature selection is `data['Regional indicator']`, which is a categorical label, completely changing the problem from prediction of 'Ladder score' to classification of 'Regional indicator'.

5.  **Lack of Visualization:**
    *   The prompt specifically asks for visualization of PCA, t-SNE, and feature importance. The generated code includes a single heatmap for correlation, but lacks the crucial plots for dimensionality reduction and feature importance.

6.  **Code Structure and Readability:**
    *   Breaking down into many small functions is good, but the repeated slicing of `data[['Ladder score', 'GDP per capita', 'Social support', 'Healthy life expectancy']]` across multiple functions is inefficient and hides the actual feature set that should be growing.
    *   The interaction features function creates a very large and confusing DataFrame.
    *   No proper handling of `Logged GDP per capita` vs `GDP per capita` which leads to errors.
    *   No warnings suppression as in the ground truth.

In essence, the generated code demonstrates familiarity with some library functions but misapplies them to the given problem, particularly by treating it as a classification task and failing to properly manage the dynamic set of engineered features.",llama-3.3-70b-instruct,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Load and filter data
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])
usa_data = df[df['location'] == 'USA'].copy()

# Set index and resample data
usa_data.set_index('date', inplace=True)
weekly_data = u...",7,"The generated code correctly performs most of the required steps: loading/filtering data, resampling, decomposition, ADF test, and plotting ACF/PACF. However, there are a few areas for improvement and minor errors that prevent a higher score.

**Strong Points:**
*   Correctly loads, parses date, and filters data for 'USA'.
*   Correctly sets index and resamples 'new_cases' to weekly sum.
*   Performs `seasonal_decompose` using an additive model.
*   Conducts the ADF test and correctly checks the p-value for stationarity.
*   Applies first-order differencing if the series is non-stationary and re-tests.
*   Plots ACF and PACF for the appropriate series (original if stationary, differenced if not).
*   Uses `matplotlib.pyplot` for plotting and `statsmodels` for time series functions.

**Areas for Improvement / Errors:**
1.  **Decomposition Plot:** The decomposition plotting uses `plt.subplot(41x)` and manually plots each component. While this is valid, `decomposition.plot()` (which `statsmodels` provides) is a more concise and often preferred way to plot the decomposition results, as shown in the ground truth. It also adds a title.
2.  **`seasonal_decompose` `period` parameter:** The `seasonal_decompose` call `seasonal_decompose(weekly_data, model='additive')` is missing the `period` parameter. For weekly data, a period of 52 (weeks in a year) is typical for annual seasonality. Without it, `statsmodels` might default to a less appropriate period (e.g., 1). This is a significant omission for time series decomposition.
3.  **Typo in PACF plot function:** In the `if result[1] > 0.05` block, the PACF plotting function is `plot_pcaf` instead of `plot_pacf`. This would cause a `NameError`.
4.  **No `plt.gca()` for `plot_acf`/`plot_pacf`:** While `ax=plt.gca()` works, it's generally cleaner to pass the specific `axes` object when using `plt.subplots`. For instance, `plot_acf(weekly_data, ax=axes[0])` if `axes` was defined as `fig, axes = plt.subplots(2, 1, figsize=(10, 6))`. The current usage can sometimes lead to plots overlapping if not careful.
5.  **Output Messages/Clarity:** The output is quite minimal. The ground truth provides more informative print statements about the steps being performed and the conclusions of the ADF test. While not strictly required by the prompt ""Provide only the Python code without explanations,"" the prompt also asks for proper imports/structure, and informative outputs are part of good code practice for analysis scripts.
6.  **Missing `warnings` import/handling (minor):** The ground truth includes warning suppression for `matplotlib` and `statsmodels` which can be useful as some plots might throw warnings. Not a critical bug, but good practice.
7.  **No explicit handling for insufficient data:** The code assumes sufficient data points exist for decomposition, ADF, ACF/PACF. The ground truth includes checks for `empty` DataFrames/series and minimum data points for these operations, which makes the code more robust.",llama-3.3-70b-instruct,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
import numpy as np

# Load the dataset
wine_data = pd.read_csv('winequality-red.csv')

# Categorize wines based on 'pH' values
wine_data['pH_group'] = pd.cut(wine_data['pH'], bins=[-np.inf, 3.2, 3.4, np.inf], 
                              labels=['Low', 'Medium', 'High'], right=False)

# Calculate average 'quality' for each pH group
average.qualities = wine_data.groupby('pH_group')['quality'].mean()

# Perform...",6,"The generated code addresses most of the requirements but has significant issues in implementation and completeness, particularly regarding confidence intervals and the visualization.

**Strengths:**
*   **Correct pH Categorization:** The `pd.cut` function is used correctly to categorize pH values into three groups.
*   **Correct Mean Calculation:** The code correctly calculates the average quality for each pH group.
*   **Correct ANOVA Test:** The `scipy.stats.f_oneway` function is used correctly to perform the ANOVA test.
*   **Basic Plotting:** A basic plot is generated with correct average quality values on the y-axis and pH groups on the x-axis.

**Weaknesses and Areas for Improvement:**
*   **Confidence Intervals (Critical Issue):**
    *   The prompt explicitly asked for *error bars representing 95% confidence intervals*. The generated code uses `yerr=stats.sem(...)`, which represents the standard error of the mean, not the confidence interval. Calculating confidence intervals requires using the t-distribution or z-distribution, the standard error, and the degrees of freedom, which is not implemented. This is a major omission for the visualization requirement.
    *   The confidence interval calculation should ideally also be reported in the printed output as requested implicitly by the ground truth's detailed breakdown and the clear requirement for error bars.
*   **Visualization Issues:**
    *   The `plt.errorbar` function is used, which is less intuitive for comparing group means than a bar plot with error bars (like `plt.bar` with `yerr`) which was the implicitly expected visualization type given the context of comparing group means.
    *   The `yerr=stats.sem(wine_data.groupby('pH_group')['quality'])` calculation for `yerr` will not work as intended. `stats.sem` needs an array-like input, and applying it directly to the grouped series like this will likely result in an error or incorrect calculation if it somehow runs. The SEM should be calculated per group. The ground truth correctly calculates SEM per group and then uses it to derive CI.
    *   The x-axis labels for the `errorbar` plot are `wine_data['pH_group'].unique()`, which is correct for ordering, but the plot itself would be clearer as a bar chart as in the ground truth.
*   **Reported Output:** The code only prints `average_qualities`, F-statistic, and p-value. It misses printing the confidence intervals, which were a key part of the ground truth's detailed output.
*   **Variable Naming Issue:** `average.qualities` is an invalid Python variable name. It should be `average_qualities`. This would lead to a `SyntaxError`. (Self-correction needed here: I see this was fixed in the later print statement, but it highlights a potential bug before execution).
*   **Code Structure/Robustness:** The ground truth includes robust features like handling `FileNotFoundError`, defining constants for better maintainability (e.g., `PH_BINS`, `PH_LABELS`, `DATASET_PATH`), and ensuring categorical order for consistency. The generated code lacks these.
*   **Clarity of Labels:** pH group labels are 'Low', 'Medium', 'High', which are less descriptive than the ground truth's labels ('Low pH (< 3.2)', 'Medium pH (3.2 to 3.4)', 'High pH (> 3.4)').",llama-3.3-70b-instruct,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder

# Load the data...",6,"The generated code correctly addresses most of the task requirements but has several issues that prevent it from being a perfect solution.

**Issues:**

1.  **Incorrect Feature Selection for Imputation and Scaling:** The code attempts to impute and scale `X` *before* adding the 'continent' column as a feature. The problem description clearly states: ""Select features: 'new_cases', 'hosp_patients', 'icu_patients', 'reproduction_rate', 'stringency_index', 'people_fully_vaccinated', and 'median_age'. The target variable is 'new_deaths'."" and then later: ""Encode the 'continent' column using one-hot encoding and add it to the features."" This implies 'continent' is also a feature. The current code initially defines `X` without 'continent', which means 'continent' is not processed through the initial imputer and scaler for numerical features. The `df['continent'] = df['continent'].astype(str)` line after the initial `X` definition is redundant as `continent` is not part of `X` at that point.

2.  **Order of Preprocessing Operations (Data Leakage):** The scaling (and imputation) is performed on the *entire dataset* (`X_imputed`, `y_imputed`) *before* the train-test split. This leads to data leakage, where information from the test set influences the scaling (and imputation) parameters calculated from the training set, giving an overly optimistic performance estimate. Scaling should be done *after* the split, fitting the scaler *only* on the training data and then transforming both train and test sets. The ground truth correctly performs scaling after the split.

3.  **Missing Value Handling for `y` (Target):** The task specified ""Handle missing values in selected features *and* target by imputing with the median."" The generated code imputes `y` using `SimpleImputer(strategy='median')`. While technically addressing the requirement for `y`, it usually makes more sense to handle missing values in target variables by dropping rows or more sophisticated methods, as imputation here could artificially influence the target distribution. However, given the explicit instruction, the *method* used is technically correct for the prompt, but its application on `y_imputed` is problematic because `X_imputed` and `y_imputed` are processed separately, potentially leading to a mismatch of original and imputed rows if the number of missing values differs between `X` and `y`.

4.  **Handling of `continent` feature for Imputation:** The ground truth correctly identifies `continent` as a categorical feature that needs one-hot encoding, not median imputation. The generated code's initial `X` selection excludes `continent`, so it doesn't get imputed. However, the subsequent one-hot encoding for `continent` occurs after the numerical features are imputed, and the combined `X_imputed` then undergoes scaling only on the *original* numerical features (before `continent` columns are added as part of `X_imputed`). This is a complex flaw in the preprocessing pipeline setup.

5.  **Scaling Categorical Features:** The line `X_imputed[numerical_features] = scaler.fit_transform(X_imputed[numerical_features])` is problematic. `numerical_features` is defined as `X.columns.tolist()`, which refers to the columns *before* one-hot encoding. After `pd.concat`, `X_imputed` now contains the one-hot encoded 'continent' columns. When `numerical_features` is used to index `X_imputed`, it only selects the original numerical columns (excluding the new one-hot encoded columns). This means the one-hot encoded columns are *not* scaled, which is correct as they do not need scaling. However, the overall structure of applying preprocessing steps in the correct order could be cleaner and more robust using `ColumnTransformer` or by clearly separating numerical and categorical feature processing.

**Strengths:**

*   **Correct Model Training and Evaluation:** Both Linear Regression and Random Forest models are correctly trained and evaluated using MAE, MSE, and R-squared.
*   **Proper Imports:** All necessary libraries are imported.
*   **File Loading:** Correctly loads the CSV and parses dates.
*   **Metric Printing:** Evaluation metrics are printed clearly.
*   **Random State:** Random state is used for reproducibility.

To improve, the code should redesign the preprocessing pipeline to ensure correct sequence of operations (impute, encode, then scale after split) and proper application of these steps to the relevant feature types, ideally using `ColumnTransformer` for better organization and preventing data leakage.",llama-3.3-70b-instruct,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Handle missing values
df['reproduction_rate'].fillna(df['reproduction_rate'].median(), inplace=True)
df['stringency_index'].fillna(df['stringency_index'].median(), inplace=True)
df['median_age'].fillna(df['median_age'].median(), inplace=True)
df['total_deaths'].fillna(df['total_deaths'].median(), inplace=T...",6,"The generated code attempts to address all parts of the task but has several inaccuracies and logical errors, particularly in the t-test section and data handling.

**Correctness Issues:**

1.  **Missing Value Imputation:** The generated code directly imputes `fillna(df[col].median(), inplace=True)` without checking if the column exists or if `df[col]` itself might be entirely null, which could cause issues or incorrect imputation if a column is missing or completely empty. While the task implies the columns exist, robust code should handle this. The original code's median imputation logic also doesn't correctly handle the scenario where the median might be NaN if all values are NaN.
2.  **ANOVA Implementation (statsmodels):** While `statsmodels` is a valid way to perform ANOVA, `stats.f_oneway` from `scipy` is more direct for comparing groups' means. The generated code uses `C(continent)` to treat 'continent' as a categorical variable, which is correct for `statsmodels`. However, it doesn't explicitly filter out non-target continents before fitting the model, though `C(continent)` will handle the levels. The `iloc[0, -1]` to get the p-value is correct for `statsmodels` ANOVA output but less readable than accessing by column name if available.
3.  **Pearson Correlation:** The correlation is calculated on columns where NaNs might still exist if the imputation step didn't cover all possible scenarios or if the data itself has NaNs for 'new_cases' or 'stringency_index' *after* the initial imputation (e.g., if there were non-numeric values). Although the prompt specified imputation, checking for `dropna()` right before the test as in the ground truth is safer.
4.  **T-test (High Death Rate Country Calculation):** This is the most significant error.
    *   `latest_entries = df.loc[df.groupby('country')['date'].idxmax()]`: This correctly identifies the latest entry per country.
    *   `death_rate_ratio = latest_entries['total_deaths'] / latest_entries['population']`: This calculates the ratio for the latest entries.
    *   `high_death_rate_threshold = death_rate_ratio.quantile(0.75)`: This correctly gets the threshold from the latest entries.
    *   `df['high_death_rate_country'] = (df.groupby('country')['total_deaths'].transform(lambda x: x.max()) / df.groupby('country')['population'].transform(lambda x: x.max())) > high_death_rate_threshold`: This is problematic.
        *   It re-calculates `total_deaths / population` using the *maximum* `total_deaths` and `population` for each country across *all dates*, not specifically the *latest* entry. This is a subtle but critical divergence from the requirement (""use latest entry per country"").
        *   It then applies this calculation to the *entire* `df`, creating a `high_death_rate_country` flag for *every row* of the original `df`, instead of per country based on its latest data. The task implies the 'high_death_rate_country' flag should be assigned at the country level for the t-test based on 'median_age' (which is also country-specific, or at least a stable country attribute).
        *   The t-test then uses `df.loc[df['high_death_rate_country'], 'median_age']`. Because the `high_death_rate_country` flag is duplicated across all rows for a given country (based on the `transform` operation), the t-test groups `median_age` values from *all dates* for countries that qualify, which is not what was intended. The 'median_age' is generally a country-level attribute, and the comparison should be between distinct countries classified as high death rate vs. others, using their respective 'median_age'. The ground truth correctly extracts unique countries with their latest data for the t-test.
    *   `t_stat, p_value = stats.ttest_ind(high_death_rate_median_age, low_death_rate_median_age)`: This performs an independent t-test, which is appropriate for comparing two independent groups, but the groups themselves (`high_death_rate_median_age` and `low_death_rate_median_age`) are incorrectly formulated due to the issue above.

**Code Quality & Completeness:**

*   **Imports:** Proper imports are included.
*   **Structure:** The code is logically structured according to the sub-tasks.
*   **Hypothesis Statements:** Null hypotheses are correctly stated.
*   **P-value Reporting & Conclusion:** P-value is printed, and conclusions are drawn based on `=0.05`.
*   **Clarity on Imputation:** The ground truth shows better practice by listing `impute_cols` and iterating, making it more flexible. The generated code lists them individually which is less scalable.
*   **Missing `np` import:** `numpy` is implicitly needed if `np.inf` or `np.nan` were to be handled (though not directly used in the final version of the code, it's generally good practice if numerical operations could lead to these).

To improve, the code needs a crucial re-evaluation of the t-test logic, specifically how the `high_death_rate_country` category is assigned and how the `median_age` groups are formed for the comparison. The groups for the t-test should be derived from the `latest_entries` dataframe, not applied back to the entire `df` as a row-wise flag. Additionally, it would be beneficial to add more robust error handling, similar to the ground truth (e.g., file not found, insufficient data for a test).",llama-3.3-70b-instruct,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select numerical columns
numerical_cols = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
numerical_df = df[numerical_cols]

# Calculate pairwise correlation matrix
correlation_matrix = numerical_df.corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True...",9,"The generated code correctly loads the dataset, selects the specified numerical columns, calculates the pairwise correlation matrix, and visualizes it using a heatmap with proper imports. It successfully addresses all core requirements of the task.

The heatmap visualization is good, using `annot=True` and `cmap='coolwarm'` which are good choices. The use of `square=True` is also a nice touch for a correlation matrix.

Areas for minor improvement (not critical for functionality):
- **Error Handling**: The generated code lacks explicit error handling (e.g., for `FileNotFoundError` if the CSV doesn't exist, or `KeyError` if columns are mistyped). While the ground truth's extensive error handling is very robust, some basic `try-except` blocks would improve its robustness for real-world use.
- **Modularity**: The ground truth encapsulates the logic in a function, which is good practice for reusability and testability. The generated code is a script, which is acceptable for a direct task but less extensible.
- **Clarity of Output**: The ground truth includes print statements for debugging/confirmation (e.g., `Dataset loaded successfully`, `First 5 rows of selected data`), which is helpful for understanding the execution flow in a benchmark scenario. The generated code lacks these outputs.
- **Figure Size**: The `figsize` chosen in the ground truth (`10, 8`) is slightly larger which might make the annotations more readable for a larger number of variables in some cases, although `8, 6` is perfectly acceptable for this specific case.

Overall, it's a very good solution that directly answers the prompt effectively.",llama-3.3-70b-instruct,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

def load_and_calculate_card_averages():
    # Load the dataset
    cards_data = pd.read_csv('clash_royale_cards.csv')

    # Group the data by the 'rarity' column and calculate averages
    averages = cards_data.groupby('rarity')[['elixirCost', 'Win Rate', 'Usage']].mean().reset_index()

    # Display the calculated averages
    print(averages)

if __name__ == ""__main__"":
    load_and_calculate_card_averages()
```",9,"The generated code correctly loads the dataset, groups it by rarity, and calculates the average `elixirCost`, `Win Rate`, and `Usage` for each rarity. It also properly displays the results. The use of a function `load_and_calculate_card_averages()` is good practice for encapsulating the logic, and the `if __name__ == ""__main__"":` block ensures the function is called correctly when the script is executed.

The only minor difference from the ground truth is the usage of `.reset_index()` which converts the `rarity` index back into a column. While not strictly necessary given the prompt's request to ""display these calculated average values for each rarity"" (which is achieved by printing the `groupby().mean()` result directly as done in the ground truth), it changes the output format from a DataFrame with 'rarity' as the index to one where 'rarity' is a regular column. Both fulfill the display requirement, but the ground truth's output is arguably slightly more idiomatic for grouped summaries where the grouping key remains naturally as the index. This is a very minor point and does not detract significantly from correctness or functionality.",llama-3.3-70b-instruct,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Create a new binary target column
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)

# Select relevant features
features = df[['elixirCost', 'Usage', 'rarity']]
target = df['is_high_win_rate']

# Apply one-hot encoding to the categorical 'rarity' column
encoder = OneHotEncoder(sparse_output=Fa...",9,"The generated code correctly performs all requested steps: loading the data, creating the target column, selecting features, applying one-hot encoding, and splitting the data. The use of `OneHotEncoder` for rarity is appropriate and correctly implemented.

One minor point that could be improved for robustness:
- The `features_encoded` dataframe concatenation relies on implicit positional alignment if the original `features` dataframe is not re-indexed. While this often works in simple cases where `fit_transform` returns outputs aligned with the input, explicitly resetting the index or merging by index would be more robust. In this specific case, `pd.get_dummies` (as used in the ground truth) is often a simpler and more robust approach when encoding directly within a DataFrame. However, the `OneHotEncoder` approach is functionally correct here.
- The omission of `try-except` block for file loading as seen in the ground truth is a minor quality point.

Overall, it's a very good solution that fully addresses the task requirements.",llama-3.3-70b-instruct,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv('iris.csv')

# Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = df['Species']

# Encode the categorical target variable
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Split t...",9,"The generated code correctly implements all the requirements of the task: data loading, feature/target definition, target encoding, data splitting with the specified parameters, KNN model training, and accuracy reporting. The imports are correct and the code structure is logical and easy to follow.

The only minor difference from the ground truth is the omission of `stratify=y_encoded` in the `train_test_split` function. While not explicitly stated as a requirement in the prompt, it's a good practice for classification tasks, especially with potentially imbalanced datasets, to ensure that the class distribution is maintained across training and testing sets. However, for the standard Iris dataset, its impact is often minimal. The ground truth also includes error handling for `FileNotFoundError`, which is a robust addition, but not strictly required by the prompt's scope. Overall, it's a very good and correct solution.",llama-3.3-70b-instruct,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
iris_data = pd.read_csv('iris.csv')

# Filter the data to get two groups: Species == 'Iris-versicolor' and Species == 'Iris-virginica'
versicolor_group = iris_data[iris_data['Species'] == 'Iris-versicolor']
virginica_group = iris_data[iris_data['Species'] == 'Iris-virginica']

# Define the null and alternative hypotheses
null_hypothesis = ""The mean PetalLengthCm is the same for Iris-versicolor and Iris-virginica.""
alternat...",9,"The generated code is almost perfect. It correctly addresses all requirements: loading data, filtering, stating hypotheses, performing the t-test, printing results, and interpreting the p-value against the given alpha. The code is well-structured and uses appropriate libraries.

Minor improvements could be:
1.  **Error Handling for File Not Found:** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code lacks this.
2.  **Clarity of Hypothesis Statement:** While correct, the ground truth's hypothesis statements are slightly more formal (""_versicolor = _virginica"") and clearly separate the two. The generated code is also clear, but could benefit from a similar structure to implicitly show the equality/inequality.
3.  **Edge Case for Empty Groups:** The ground truth checks if `versicolor_group` or `virginica_group` are empty after filtering. This is a good defensive programming measure to prevent errors if the species names in the CSV are unexpected or missing.

Despite these minor points, the core logic and output are fully correct and meet the prompt's requirements effectively.",llama-3.3-70b-instruct,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the iris dataset
iris = pd.read_csv('iris.csv')

# Define the numerical features
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

# Create a figure with subplots
fig, axs = plt.subplots(2, 2, figsize=(15, 10))

# Loop over the features and create box plots
for i, feature in enumerate(features):
    row = i // 2
    col = i % 2
    sns.boxplot(x='Species', y=feature, data=iris...",9,"The generated code correctly loads the dataset and generates box plots for each numerical feature, grouped by species, with appropriate titles. It addresses all core requirements of the task.

Strong points:
- Correctly uses pandas to load the CSV.
- Correctly identifies and uses `seaborn.boxplot` for plotting.
- Successfully creates box plots for each of the four specified numerical features.
- Correctly groups the box plots by the `Species` column.
- Sets appropriate titles for each subplot, indicating the feature.
- Uses `plt.subplots` to arrange the plots in a single figure, which is an efficient way to display multiple related plots.
- Uses `plt.tight_layout()` for better plot presentation.

Areas for minor improvement (preventing a perfect 10):
- The `try-except` block for file loading, present in the ground truth, makes the code more robust against `FileNotFoundError`. While not explicitly required by the prompt, it's good practice for production-ready code.
- The ground truth generates each plot in a separate figure (`plt.figure()` inside the loop), whereas the generated code puts them all in one figure using subplots. Both are valid interpretations of ""create a box plot showing its distribution"" but the ground truth's approach might be slightly clearer if the intent was truly four *separate* plots rather than a single figure with four subplots. However, the subplot approach is often preferred for presentation.
- The title `feature` (e.g., `SepalLengthCm`) is sufficient, but the ground truth's title `Distribution of {feature} by {GROUPING_COLUMN}` is slightly more descriptive. This is a minor aesthetic point.",llama-3.3-70b-instruct,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('heart.csv')

# Select the required features and target variable
selected_data = data[['Age', ...",9,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, selects the specified features and target, correctly applies one-hot encoding to the categorical features (`Sex` and `ChestPainType`), splits the data into training and testing sets with the correct ratio and `random_state`, trains a Logistic Regression model, and reports the accuracy score.

The code demonstrates good understanding of `ColumnTransformer` and `Pipeline` for preprocessing. However, there are a few minor deviations/unnecessary additions compared to the ground truth and the task requirements:
1.  **Unnecessary Imputation and Scaling for Numerical Features:** The task did not specify any imputation or scaling for numerical features (`Age`, `RestingBP`, `Cholesterol`, `MaxHR`). The generated code includes `SimpleImputer` and `StandardScaler` for numerical features. While this is often good practice in real-world scenarios, it wasn't explicitly requested by the prompt. The ground truth correctly omits this as per the prompt.
2.  **Unnecessary Imputation for Categorical Features:** Similarly, `SimpleImputer` for categorical features (`fill_value='missing'`) was not specified. The prompt simply asked for one-hot encoding.
3.  **Separate `fit_transform` and `transform`:** The generated code applies `preprocessor.fit_transform(X_train)` and `preprocessor.transform(X_test)` separately, then trains the model on these transformed arrays. While functionally correct, the ground truth's approach of using a `Pipeline` that encapsulates both the `preprocessor` and the `LogisticRegression` model is generally considered better practice in scikit-learn. It ensures that the preprocessing steps are always applied correctly and consistently when using `model.fit()` and `model.predict()`, reducing potential errors and making the workflow clearer.

Overall, the core task requirements are met, and the code is well-structured and uses appropriate scikit-learn tools. The minor deviations relate to adding extra processing steps not explicitly asked for, and a slightly less integrated pipeline approach compared to the ideal solution.",llama-3.3-70b-instruct,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('heart.csv')

# Create two groups based on the 'Sex' column
male_group = df[df['Sex'] == 'M']['Cholesterol']
female_group = df[df['Sex'] == 'F']['Cholesterol']

# Null and alternative hypotheses
null_hypothesis = ""The mean Cholesterol levels are the same for male and female patients.""
alternative_hypothesis = ""The mean Cholesterol levels are different for male and female patients.""

# Perform an independen...",9,"The generated code correctly performs the independent two-sample t-test as requested. It loads the data, creates the two groups based on 'Sex', performs the t-test on 'Cholesterol', and correctly interprets the results based on the given alpha. The null and alternative hypotheses are also clearly stated in the interpretation section.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly separates male and female cholesterol levels.
*   Performs the correct t-test (`stats.ttest_ind`).
*   Prints the t-statistic and p-value.
*   Correctly interprets the p-value against the alpha level and states whether to reject or fail to reject the null hypothesis.
*   Clearly states the null and alternative hypotheses within the output.

**Areas for minor improvement:**
*   **Robustness:** The ground truth code includes error handling for `FileNotFoundError` and checks for the existence of required columns, as well as checks for sufficient data in groups. The generated code assumes the `heart.csv` file exists and contains the necessary columns and data, which is fine for a direct task but less robust for production.
*   **`equal_var` parameter in `ttest_ind`:** By default, `ttest_ind` assumes equal variances (`equal_var=True`). While often acceptable for educational examples, in real-world scenarios, Welch's t-test (`equal_var=False`) is generally preferred as it doesn't assume equal variances and is more robust. The ground truth explicitly uses `equal_var=False`. This is a subtle point but important for statistical rigor.
*   **NaN handling:** The ground truth explicitly uses `.dropna()` when creating the groups. While `ttest_ind` generally handles NaNs by default for series, explicit dropping can sometimes be clearer or handle specific cases. In this particular scenario, it's unlikely to make a difference if there are no NaNs in the 'Cholesterol' column, but it's a good practice.
*   **Output formatting:** The ground truth uses f-strings with `.4f` for consistent decimal places, which makes the output slightly cleaner. This is a very minor point.

Overall, the generated code successfully fulfills the core requirements of the task. The minor improvements are related to best practices and robustness rather than core functional correctness.",llama-3.3-70b-instruct,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Function to calculate IQR and identify outliers
def identify_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers, lower_bound, upper_bound

# Identify outliers
outliers_...",8,"The generated code correctly performs all the requested tasks: identifying outliers using IQR, reporting their count, winsorizing the data, and comparing statistics before and after treatment. The use of functions (`identify_outliers` and `winsorize`) improves modularity and readability.

Areas for improvement to reach a perfect score:
1.  **Error Handling for File Loading:** The ground truth includes `try-except FileNotFoundError` for robust loading, which is missing in the generated code. This is important for real-world scenarios.
2.  **Generalization:** While the functions are good, the main logic for processing columns (`volatile acidity` and `residual sugar`) is still hardcoded repeatitively (e.g., calling `identify_outliers` twice, `winsorize` twice, and printing statistics four times). A loop over a list of columns (similar to the ground truth's `columns_to_treat`) would make the code more scalable and maintainable if more columns needed similar treatment.
3.  **Clarity of Output:** The ground truth provides more verbose output, detailing the Q1, Q3, IQR, and bounds, which can be very helpful for understanding the outlier identification process. The generated code only reports the number of outliers.
4.  **Implicit Variable Passing:** In the `winsorize` function, `df` is modified in place, but then returned and reassigned (`df_winsorized = winsorize(df_winsorized, ...)`), which is redundant. Modifying `df` directly within the function is sufficient if the intent is to modify the passed DataFrame. However, creating a copy (`df_winsorized = df.copy()`) before calling the function is good practice to preserve the original DataFrame.",llama-3.3-70b-instruct,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('heart.csv')

# Create a crosstab for the grouped bar chart
ct = pd.crosstab(df['ChestPainType'], df['HeartDisease'])

# Create a grouped bar chart
ct.plot(kind='bar', figsize=(10, 6))

# Set title and labels
plt.title('Count of Heart Disease by Chest Pain Type')
plt.xlabel('Chest Pain Type')
plt.ylabel('Count')
plt.legend(title='Heart Disease')

# Show the plot
plt.show()
```",7,"The generated code correctly addresses the core task of loading the data and creating a grouped bar chart with appropriate labels, title, and legend. It uses `pd.crosstab` effectively, which is a concise way to prepare the data for this type of plot.

However, there are a few areas for improvement:
1.  **Readability/Clarity of Legend**: The legend displays '0' and '1' for HeartDisease. The ground truth maps these to 'No Heart Disease' and 'Heart Disease' respectively, which significantly improves the clarity and interpretability of the chart for the end-user.
2.  **Robustness**: The generated code lacks error handling for `FileNotFoundError` or other potential issues when loading the CSV, which is present in the ground truth.
3.  **Encapsulation**: The code is not encapsulated within a function, making it less reusable. The ground truth uses a function `create_heart_disease_chest_pain_grouped_bar_chart` which is good practice.
4.  **Plotting Library choice**: While `matplotlib` is used (via `pandas.plot`), `seaborn` is often preferred for statistical plots due to its aesthetic appeal and built-in functionalities for grouped plots (like `sns.countplot`), which can sometimes be more direct for this type of task. The ground truth uses `seaborn.countplot`, which is a more idiomatic choice for this specific task.
5.  **Output Aesthetics**: The default colors and general aesthetic of the `pandas.DataFrame.plot` are sufficient but could be enhanced with `seaborn` styling (as seen in the ground truth with `sns.set_style` and `palette`).

Overall, it's a good attempt that creates a functional plot, but a few refinements would make it more robust, user-friendly, and aligned with best practices for data visualization.",llama-3.3-70b-instruct,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

def load_and_analyze_data():
    # Load the dataset
    spotify_data = pd.read_csv('spotify_2023.csv')

    # Clean the in_shazam_charts column
    spotify_data['in_shazam_charts'] = pd.to_numeric(spotify_data['in_shazam_charts'].str.replace(',', ''), errors='coerce')
    print(""Count of NaNs in 'in_shazam_charts' column:"", spotify_data['in_shazam_charts'].isnull().sum())

    # Group the data by key and mode
    grouped_data = spotify_data.groupby(['key', 'mode'])...",9,"The generated code correctly loads the dataset, cleans the `in_shazam_charts` column by removing commas and coercing to numeric, reports the NaN count, and then groups the data by `key` and `mode` to calculate the average `danceability_%` and `valence_%`. All requirements are met accurately.

**Strong points:**
- Correct implementation of all data processing steps (cleaning, grouping, aggregation).
- Proper use of `pd.to_numeric` with `errors='coerce'`.
- Correct calculation and reporting of NaN counts.
- Clear and concise code.

**Minor areas for improvement (not affecting score significantly):**
- The ground truth includes a `try-except` block for `FileNotFoundError` which is good practice for robust code, though not strictly required by the prompt's core task.
- The ground truth explicitly converts `in_shazam_charts` to `str` before `str.replace(',', '')`, which is safer, though `pd.to_numeric` would likely handle the mixed types anyway.
- The ground truth separates the print statements for better readability, but the generated code's print is also adequate.",llama-3.3-70b-instruct,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Load the dataset
data = pd.read_csv('spotify_2023.csv')

# Ensure the 'streams' column is numeric
data['streams'] = pd.to_numeric(data['streams'], errors='coerce')

# Drop rows with NaN in 'streams' or predictor columns
predictor_columns = ['in_spotify_playlists', 'energy_%']
data = data.dropna(subset=['st...",9,"The generated code correctly addresses almost all aspects of the task. It loads the dataset, correctly handles the `streams` column to be numeric and drops NaNs, selects the specified features and target, splits the data, trains a Linear Regression model, and evaluates it using R-squared and MSE. The code quality is good, using appropriate imports and a clear structure.

One minor point for improvement, compared to the ground truth, is the lack of error handling for the `FileNotFoundError` when loading the CSV. While the prompt stated to ""assume the CSV file is in the current directory,"" including a `try-except` block for `FileNotFoundError` is generally good practice for robust code in real-world scenarios. Also, the MSE reporting truncates to 4 decimal places where the ground truth specified 2, which is a minor discrepancy but still within reasonable reporting. The ground truth also included print statements for debugging/tracking progress, which weren't strictly required but are often helpful, though the prompt asked for ""only the Python code without explanations"" which might imply minimal print statements. Overall, it's a very strong submission.",llama-3.3-70b-instruct,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

def feature_engineering():
    # Load the dataset
    spotify_data = pd.read_csv('spotify_2023.csv')

    # Combine released_year, released_month, and released_day into a single datetime column
    spotify_data['release_date'] = pd.to_datetime(spotify_data[['released_year', 'released_month', 'released_day']], errors='coerce')

    # Create a new feature named days_since_release
    fixed_date = pd.to_datetime('2023-12-31')
    spotify_data['days_since_release'] = (...",7,"The generated code correctly performs the main tasks: loading the dataset, attempting to combine date components, calculating `days_since_release`, and displaying the requested columns.

However, there is a critical subtle issue in how `release_date` is created: `pd.to_datetime(spotify_data[['released_year', 'released_month', 'released_day']], errors='coerce')` is not the standard or reliable way to construct a date from separate year, month, day columns in pandas. While it *might* work in some specific pandas versions or data configurations, the robust and recommended approach is to either construct a string first (as in the ground truth) or use `pd.to_datetime({'year': df['year'], 'month': df['month'], 'day': df['day']})`. This method is less robust and could lead to incorrect dates or more `NaT` values than necessary if the columns are not correctly interpreted by `to_datetime`.

Additionally, the handling of `NaT` values for `days_since_release` by filling them with `-1` and then converting to `int` is a deviation from the requirement that the feature should be an integer, which typically implies a positive number of days or NaN for invalid dates. The ground truth uses `pandas.Int64` which allows for `NaN` values in an integer column, which is a better fulfillment of ""integer"" without forcing arbitrary values like -1.

Strong points include using `errors='coerce'` for date conversion and calculating `days_since_release`. The code is also clean and well-structured with a wrapper function.

Areas for improvement:
- Use a more robust and standard method for combining year, month, and day into a datetime object (e.g., concatenate to string first or use the dictionary approach in `pd.to_datetime`).
- Reconsider the handling of `NaN` values in `days_since_release`. If the requirement is ""integer"", the `pandas.Int64` (nullable integer) type is preferable as it maintains the concept of missing data rather than substituting an arbitrary value like -1.",llama-3.3-70b-instruct,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

# Load the youtube_trendings.csv dataset
def load_and_clean_data(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load the dataset: {str(e)}"")
        return None

# Clean the data
def clean_data(data):
    # Remove commas and convert to numeric, replacing errors with NaN
    data['Video views'] = pd.to_numeric(data['Video views'].str.replace(',', ''), errors='coerce')
    data['Like...",8,"The generated code successfully completes the task by loading the dataset, cleaning the specified columns, filling missing values, and performing the required aggregation. It demonstrates good code organization with functions for loading, cleaning, and calculating sums.

However, there are a couple of areas for improvement that prevent a higher score:
1.  **Robustness to missing columns**: The `clean_data` function directly attempts to access `data['Video views']`, `data['Likes']`, and `data['Category']`. If any of these columns are missing from the `youtube_trendings.csv` file, it will raise a `KeyError` and crash. The ground truth handles this more robustly by checking for column existence before processing.
2.  **`astype(str)` for replacement**: While `pd.to_numeric` can handle various input types, it's safer and more explicit to convert the columns to string type *before* applying `.str.replace(',', '')`, especially if there's a chance the column might already be numeric or mixed type. The ground truth uses `astype(str)`. In this specific case, `pd.to_numeric` would likely coerce to string internally before regex, but being explicit is better practice.

Overall, it's a solid solution, but the robustness in handling potentially missing columns is a significant differentiator.",llama-3.3-70b-instruct,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the 'Likes' column
df['Likes'] = df['Likes'].str.replace(',', '')
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

# Fill missing values in the 'Category' column
df['Category'] = df['Category'].fillna('Unknown')

# Create two groups of cleaned 'Likes' values
music_likes = df.loc[df['Category'] == 'Music', 'Likes']
sports_likes ...",8,"The generated code correctly addresses most of the requirements of the task.

**Strong points:**
- Correctly loads the dataset.
- Successfully cleans the `Likes` column by removing commas, converting to numeric, and handling NaNs by filling with 0.
- Correctly fills missing values in the `Category` column with 'Unknown'.
- Creates the two groups (`music_likes` and `sports_likes`) based on the cleaned `Likes` values and categories.
- Performs an independent two-sample t-test using `scipy.stats.ttest_ind`.
- Prints the t-statistic and p-value.
- Interprets the p-value correctly based on the given significance level of alpha = 0.05.

**Areas for improvement to achieve a higher score:**
- **Clarity of `Likes` cleaning:** While functional, the ground truth code creates a `Likes_cleaned` column, which is good practice to retain the original column if needed and make it explicit that a cleaned version is being used. The generated code overwrites the 'Likes' column directly which is acceptable for this problem but less robust for general data processing.
- **Handling `FileNotFoundError`:** The generated code does not include error handling for `FileNotFoundError` when loading the CSV, which can cause the script to crash if the file is not present. The ground truth code includes a `try-except` block for this.
- **Hypotheses Statement:** The generated code only defines the hypotheses as strings. The ground truth code `print`s them explicitly in a more formal way. This is a minor point, but the prompt asked to ""Clearly state the null and alternative hypotheses"", which the ground truth does better by printing them out in a structured manner.
- **`equal_var` parameter in `ttest_ind`:** The ground truth code specifies `equal_var=False` for Welch's t-test which is a more robust choice when variances are not known to be equal. The generated code implicitly uses `equal_var=True` (Student's t-test) by defaulting. While not explicitly stated in the prompt, it's a best practice for t-tests where variance homogeneity isn't guaranteed.
- **Sample Size Check:** The ground truth includes a check for minimum sample size before performing the t-test (i.e., `len(music_likes) < 2`), which is important to prevent errors or meaningless results if one of the groups is too small. The generated code lacks this.",llama-3.3-70b-instruct,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the 'Likes' and 'Dislikes' columns
df['Likes'] = df['Likes'].astype(str).str.replace(',', '')
df['Dislikes'] = df['Dislikes'].astype(str).str.replace(',', '')

df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Dislikes'] = pd.to_numeric(df['Dislikes'], errors='coerce')

df['Likes'] = df['Likes'].fillna(0)
df['Dislikes'] = df['Dislikes'].fillna(0)

# Create new colum...",9,"The generated code successfully accomplishes all the core requirements of the task. It loads the dataset, correctly cleans the 'Likes' and 'Dislikes' columns by removing commas, converting to numeric, and filling NaNs with 0. It then correctly calculates `TotalEngagement` and `LikeDislikeRatio` according to the specified formulas. Finally, it displays the required columns.

The strong points are its conciseness and accuracy in performing the data manipulation tasks.

The main area for improvement is error handling for file loading. The ground truth code includes robust `try-except` blocks to handle `FileNotFoundError` and general exceptions during file loading, which makes the code more robust and user-friendly if the dataset is missing. The generated code assumes the file will always be present, which is a minor shortfall in terms of production-readiness or general robustness but doesn't strictly fail the core data engineering task. Adding an explicit cast to `float` for `Likes` and `Dislikes` after `fillna(0)` (as in the ground truth) could be considered for strict type adherence, but `pd.to_numeric` and subsequent arithmetic operations generally handle the types correctly here.",llama-3.3-70b-instruct,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd

def load_and_clean_data(file_path):
    # Load the dataset
    data = pd.read_csv(file_path)

    # Clean the 'Launched Price (USA)' column
    data['Launched Price (USA)'] = pd.to_numeric(
        data['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''),
        errors='coerce'
    )

    # Clean the 'Screen Size' column
    data['Screen Size'] = pd.to_numeric(
        data['Screen Size'].str.replace(' inches', ''),
        errors='coerce'
    )

...",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans both 'Launched Price (USA)' and 'Screen Size' columns, handles conversion errors by setting to NaN, groups by 'Company Name' and 'Launched Year', and calculates the correct averages. The use of functions (`load_and_clean_data`, `aggregate_data`, `main`) improves code organization and readability.

A minor improvement could be adding error handling for `pd.read_csv` (e.g., `FileNotFoundError`) as done in the ground truth, though the prompt implies the file will be present. Also, the `astype(str)` explicit conversion before `str.replace` in the ground truth is a good defensive programming practice, as `pd.to_numeric` on a column that might already contain numbers could lead to issues before string operations if not handled carefully, though in many cases pandas will coerce correctly. In this specific scenario, the generated code's approach for cleaning strings then converting to numeric is largely sufficient and idiomatic for pandas. The column names after cleaning also follow the task's implicit naming convention (though not strictly specified, the ground truth uses different names which is fine, the generated code modifies in place which is also fine). The output aggregated column names are slightly different (e.g., `Average_Price` vs `Average_Launched_Price_USD`), but this is a minor naming difference and doesn't affect correctness.",llama-3.3-70b-instruct,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

# Load the dataset
def load_dataset(file_name):
    try:
        return pd.read_csv(file_name)
    except FileNotFoundError:
        print(""The file does not exist"")
        return None
    except pd.errors.EmptyDataError:
        print(""The file is empty"")
        return None
    except pd.errors.ParserError:
        print(""Error parsing the file"")
        return None

# Define a function to clean camera columns
def clean_camera(camera_spec):
    match =...",7,"The generated code correctly implements most of the requirements. It loads the dataset, defines a function to clean camera specifications, creates the new 'Total Camera MP' feature, and displays the requested columns for the first 10 rows.

However, there is a crucial issue in the `clean_camera` function: it does not handle non-string inputs (like `NaN` values, which are common in real-world data) gracefully. If `camera_spec` is `NaN`, `re.search` will raise a `TypeError` because `re.search` expects a string or bytes-like object, not a float (`np.nan`). The ground truth specifically addresses this by handling `pd.isna(camera_spec)` and converting to string, making it more robust.

Additionally, the `load_dataset` function, while well-intentioned with error handling, is not strictly necessary as per the task description which only required loading the CSV and assumed it's in the current directory. The task also didn't explicitly ask for ` EmptyDataError` or `ParserError` handling, though including `FileNotFoundError` is good practice. The ground truth's simpler `try-except FileNotFoundError` is sufficient for the task.

The ground truth also includes more descriptive print statements indicating progress, which is a minor but good practice for larger scripts. The `.to_string()` method used in the ground truth for displaying the DataFrame provides slightly better console formatting, but the generated code's output is still acceptable.

To improve, the generated code should:
1.  Make the `clean_camera` function more robust to handle `NaN` or non-string inputs by converting `camera_spec` to string or checking for `pd.isna()` before applying regex, similar to the ground truth.
2.  Simplify the `load_dataset` function to align more closely with the specific `FileNotFoundError` handling demonstrated in the ground truth, or remove it as it was not explicitly required to be a separate function.",llama-3.3-70b-instruct,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the data
df['RAM'] = df['RAM'].apply(lambda x: pd.to_numeric(x.replace('GB', '', errors='coerce')))
df['Battery Capacity'] = df['Battery Capacity'].apply(lambda x: pd.to_numeric(x.replace('mAh', '', errors='coerce')))
df['Screen Size'] = df['S...",7,"The generated code largely addresses the requirements of the task and produces a working solution.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly identifies and defines features (X) and target (y).
*   Correctly splits the data using `train_test_split` with the specified `random_state`.
*   Correctly trains a `LinearRegression` model.
*   Correctly evaluates the model using `r2_score` and `mean_absolute_error`.
*   Includes all necessary imports.

**Areas for improvement:**
*   **Cleaning `errors='coerce'` in string methods:** The use of `errors='coerce'` within `.apply()` with string methods like `.replace()` is incorrect. `pd.to_numeric` has an `errors` argument, but `str.replace` does not. This error handling might not work as intended for the string replacement part. It should be applied only to `pd.to_numeric`. For example, `x.replace('GB', '')` should happen FIRST, then `pd.to_numeric(..., errors='coerce')`. The current implementation could lead to unexpected behavior if a value is not a string. The Ground Truth first converts the column to `str` type, then performs string replacements, then `pd.to_numeric` with `errors='coerce'`.
*   **Robustness of cleaning:** The generated code assumes that `df['RAM']` (and other columns) are always string-like and can directly apply `.replace()`. If there are already numeric or NaN values, this could raise errors or `AttributeError: 'float' object has no attribute 'replace'`. The Ground Truth handles this by converting the column to string type first for robustness.
*   **Type Conversion for Cleaning:** The lambda functions `lambda x: pd.to_numeric(x.replace('GB', '', errors='coerce'))` are trying to apply `errors='coerce'` to the `replace` method, which is incorrect. `errors='coerce'` is an argument for `pd.to_numeric`, not `str.replace`. The code should first perform the string replacement and then apply `pd.to_numeric` with `errors='coerce'`. This is a subtle but important bug in the cleaning logic.
*   **Output Formatting:** The output for R-squared and MAE is functional but lacks the formatting and descriptive text (e.g., ""Model Evaluation Results:"") present in the ground truth, which often improves readability for users.",llama-3.3-70b-instruct,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
from scipy.stats import chi2_contingency, ttest_ind, f_oneway
import numpy as np

# Load the dataset
titanic = pd.read_csv('titanic.csv')

# Conduct a chi-square test to determine if there's a significant association between passenger class (Pclass) and survival
observed = pd.crosstab(titanic['Pclass'], titanic['Survived'])
chi2, p_val, dof, expected = chi2_contingency(observed)
print(f""Chi-square test - Pclass vs Survived:"")
print(f""P-value: {p_val}, Test statistic...",7,"The generated code correctly performs the three requested statistical tests (chi-square, t-test, ANOVA) and calculates their respective p-values and test statistics. It also correctly attempts to calculate and display effect sizes for each test.

However, there are several areas for improvement:

1.  **Interpretation of Results:** The ""conclusion"" for each test is a simple boolean (`p_val < 0.05`). The ground truth code provides more detailed, human-readable interpretations of rejecting/failing to reject the null hypothesis and what that means in the context of the variables. This is a crucial part of statistical analysis.
2.  **Effect Size Calculation Accuracy/Robustness:**
    *   **Cramer's V:** The formula used for Cramer's V is incorrect. `min(len(titanic['Pclass'].unique()), len(titanic['Survived'].unique()))-1` should be `min(observed.shape) - 1` or `min(rows-1, cols-1)` as in the ground truth. It also misses the `if (n * min_dim) > 0 else 0` guard for division by zero.
    *   **Cohen's d:** The formula used for Cohen's d is for pooled standard deviation assuming equal variances. The ground truth uses a more robust calculation for pooled standard deviation, and critically, it performs Levene's test to determine whether to assume equal variances for the t-test itself, which impacts the t-statistic and p-value. The generated code implicitly assumes equal variances for the t-test (by not specifying `equal_var=False`) and then uses a pooled standard deviation formula that might not be appropriate if variances are truly unequal.
    *   **Eta-squared:** The formula `f_stat / (f_stat + (len(first_class_fares) - 1 + len(second_class_fares) - 1 + len(third_class_fares) - 1))` is incorrect for Eta-squared. Eta-squared is typically SS_between / SS_total. The ground truth code correctly implements this calculation.
3.  **Missing Value Handling (Pre-computation):** While `dropna()` is used for `Age` in the t-test, the ANOVA and Chi-square tests don't explicitly handle potential missing values for their respective columns (Fare, Pclass, Survived). While unlikely for these specific columns in the Titanic dataset, robust code should consider this. The ground truth handles this via `dropna(subset=[...])`.
4.  **Assumptions Check:** The ground truth includes Levene's test for the t-test and ANOVA to explicitly check the assumption of homogeneity of variances, which is good practice and informs the interpretation (and in the case of t-test, the `equal_var` parameter). The generated code skips this.
5.  **Code Structure and Readability:** The ground truth code is more verbose with clear headings, hypothesis statements, and explanations of each step, which is excellent for a ""statistical analysis"" task. The generated code is more concise but lacks these informative elements.
6.  **Error Handling:** The ground truth includes `try-except` for file loading. The generated code assumes the file exists.

Overall, it's a good start that performs the core computations, but the effect size calculations need fixing, and the statistical rigor (assumptions, detailed interpretation) could be improved.",llama-3.3-70b-instruct,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Group the data by the 'Year' column and calculate the average 'Rating' and 'Runtime'
df_grouped = df.groupby('Year')[['Rating', 'Runtime']].mean().reset_index()

# Create a line plot showing the trend of average 'Rating' over the 'Year'
plt.figure(figsize=(10, 6))
plt.plot(df_grouped['Year'], df_grouped['Rating'], marker='o')
plt.title('Trend of Average Rating Over the Years'...",8,"The generated code correctly performs all the requested tasks: loading the CSV, grouping by 'Year', calculating average 'Rating' and 'Runtime' (though the 'Runtime' average is calculated but not used in the plot, which is acceptable as per the prompt), and creating a line plot of average 'Rating' over 'Year' with title and labels. The imports are correct, and the structure is clean and direct.

Areas for improvement:
1.  **Robustness (Data Cleaning/Type Conversion):** The ground truth includes essential data cleaning steps for 'Year' and 'Runtime' columns (e.g., handling non-numeric characters, converting to int, cleaning ' min' from runtime), and also for 'Rating'. The generated code assumes the columns are already in a clean, numeric format or can be directly converted, which might lead to errors if the dataset has unexpected formatting (e.g., 'Year' as ""(2000)"", 'Runtime' as ""120 min""). This is a common issue with real-world datasets.
2.  **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError`, `KeyError`, and general exceptions, making the code more robust against common issues like a missing file or incorrect column names. The generated code lacks any error handling.
3.  **Function Encapsulation:** The ground truth encapsulates the logic within a function (`analyze_imdb_data`) and uses a `if __name__ == ""__main__"":` block, which is good practice for modularity, reusability, and testability. The generated code is a script-like top-level execution.
4.  **Plotting Details:** While the generated plot meets the basic requirements, the ground truth's plotting code includes `figsize`, `marker`, `linestyle`, more descriptive title, `fontsize` for labels, `xticks` rotation, and `tight_layout`, which enhance the visual appeal and readability of the plot. The ground truth also explicitly sorts by year before plotting, ensuring the trend is always chronologically correct, though `groupby` often preserves order for simple cases.

Overall, the generated code is a functional and concise solution to the problem statement itself, but lacks the robustness, error handling, and best practices often expected in production-ready data science code.",llama-3.3-70b-instruct,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
imdb_top_250 = pd.read_csv('imdb_top_250.csv')

# Create two groups of movies based on the Year column
group_before_1990 = imdb_top_250[imdb_top_250['Year'] < 1990]
group_after_1990 = imdb_top_250[imdb_top_250['Year'] >= 1990]

# Perform an independent two-sample t-test
t_stat, p_val = stats.ttest_ind(group_before_1990['Rating'], group_after_1990['Rating'])

# Clearly state the null and alternative hypotheses for this test...",8,"The generated code correctly performs the data loading, group creation, t-test execution, and interpretation. It correctly states the hypotheses and prints the required values (t-statistic and p-value).

Strong points:
- Correctly uses pandas to load the CSV.
- Correctly creates the two groups based on the `Year` column.
- Correctly applies `scipy.stats.ttest_ind` to the 'Rating' column of the groups.
- Accurately states the null and alternative hypotheses.
- Appropriately interprets the p-value against the alpha level and states whether to reject the null hypothesis.

Areas for improvement to achieve a higher score (9 or 10):
- **Robustness in data loading/type conversion:** The ground truth code includes `try-except` for `FileNotFoundError` and `pd.to_numeric` with error handling. This makes the code more robust against missing files or non-numeric data in 'Year' or 'Rating' columns. The generated code assumes the file exists and that 'Year' and 'Rating' are already in suitable numeric formats, which might not always be the case.
- **Handling of missing values/corrupted data:** The ground truth explicitly handles `NaN` values created by `pd.to_numeric` for robustness.
- **Explicitly stating `equal_var` in `ttest_ind`:** While `True` is the default, explicitly setting `equal_var=True` or `equal_var=False` (for Welch's t-test) makes the assumption clearer and improves code readability for statistical tests. The ground truth code makes this explicit.
- **Formatting of printed output:** The ground truth code formats the t-statistic and p-value to a specific number of decimal places, which makes the output cleaner and easier to read.",llama-3.3-70b-instruct,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
imdb_top_250 = pd.read_csv('imdb_top_250.csv')

# Create a new column named Decade by categorizing the Year column
imdb_top_250['Decade'] = (imdb_top_250['Year'] // 10) * 10

# Convert the Decade column to string and append 's'
imdb_top_250['Decade'] = imdb_top_250['Decade'].astype(str) + 's'

# Display the Title, Year, and the new Decade column for the first 10 movies in the dataset
print(imdb_top_250[['Title', 'Year', 'Decade']].head(10))
```",9,"The generated code correctly addresses all the requirements of the task. It properly loads the CSV, creates the `Decade` column using the correct logic (integer division by 10, multiplying by 10, and appending 's'), and displays the requested columns for the first 10 rows. The imports are correct.

Strong points:
- Direct and concise implementation of the logic.
- Correct calculation for the 'Decade' column.
- Correct selection and display of the required columns.

Areas for minor improvement (not critical for this simple task, but good practice):
- The Ground Truth code includes error handling (e.g., `FileNotFoundError`, `KeyError`), which is good practice for robust applications, though not explicitly required by the prompt's simplicity.
- The Ground Truth code encapsulates logic in functions, which aids reusability and modularity. For larger projects, this structure is preferred.
- The output format requested `to_string(index=False)` which cleans up the output, but `head(10)` is also perfectly acceptable and common.

Overall, for the given task, the generated code is highly effective and correct.",llama-3.3-70b-instruct,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Error loading dataset: {e}"")

# Impute missing values in numerical columns
def impute_numerical_columns(data, columns):
    for column in columns:
        mean_value = data[column].mean(skipna=True)
        data[column] = data[column].fillna(mean_value)
    return data

# Impute missing values in categorical c...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, imputes numerical columns with their respective means, imputes the 'sex' column with its mode, and then correctly groups the data and calculates the average `body_mass_g`. The code is well-structured using functions, which improves readability and modularity. Error handling for file loading is also present.

A minor point for improvement, though not critical for correctness, is that the `impute_numerical_columns` and `impute_categorical_columns` functions assume `data[column] = data[column].fillna(mean_value)` or `mode_value` will affect the original DataFrame when passed by reference. While this works in Python for DataFrames, explicitly returning and reassigning `data` (as done in `main()`) sometimes signals intent more clearly, or using `inplace=True` within the function would be another common pattern. However, as written, it still functions correctly. The `skipna=True` in `mean()` is redundant as it's the default behavior but doesn't cause any harm.",llama-3.3-70b-instruct,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
penguins = pd.read_csv('penguins.csv')

# Filter the data to include only 'Adelie' and 'Gentoo' species
penguins_filtered = penguins[penguins['species'].isin(['Adelie', 'Gentoo'])]

# Remove any rows with missing 'flipper_length_mm' or 'species' values
penguins_complete = penguins_filtered.dropna(subset=['flipper_length_mm', 'species'])

# Perform independent two-sample t-test
adelie_lengths = penguins_complete[penguins_co...",8,"The generated code correctly performs all steps of the task: loading data, filtering, handling NAs, performing a t-test, and interpreting the result. It correctly imports necessary libraries.

Strengths:
- Correctly loads and filters the dataset.
- Correctly removes NA values from the specified columns (`flipper_length_mm`, `species`).
- Successfully performs the independent two-sample t-test using `scipy.stats.ttest_ind`.
- Correctly formulates and implicitly states the null and alternative hypotheses.
- Prints the t-statistic and p-value.
- Provides a correct interpretation of the p-value based on the specified alpha level.

Areas for Improvement:
- **Hypotheses Clarity:** While the hypotheses are implicitly there, the ground truth code explicitly states them in a print statement, which was a specific requirement (""Clearly state the null and alternative hypotheses for this test""). The generated code has them as comments, which is less ""clear"" for a user reading the output. This is a minor point but important for fully meeting the prompt's explicit requirement.
- **Error Handling:** The ground truth includes basic error handling for file not found, which is good practice. The generated code lacks this. While not explicitly asked in the prompt, robust code often includes such checks.
- **`equal_var` parameter in `ttest_ind`:** The ground truth explicitly discusses the `equal_var` parameter for `ttest_ind` and uses `equal_var=False` (Welch's t-test), which is often a more robust choice when variances are unknown or unequal. The generated code uses the default, which is `equal_var=True` (Student's t-test). While the prompt didn't strictly specify which version of the t-test, explaining or choosing a more robust default like Welch's (as the ground truth does) would be beneficial. However, for a basic t-test, the default is acceptable.
- **Output Formatting:** The ground truth provides more descriptive print statements for each step, which enhances readability and understanding of the code's execution flow. For example, showing how many rows are filtered or how many NAs are dropped helps in debugging and understanding the data transformation.

Overall, it's a very good solution that addresses almost all requirements accurately. The main missing piece is the explicit print out of the hypotheses.",llama-3.3-70b-instruct,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('penguins.csv')

# Select features and target
X = data[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]
y = data['species']

# Drop rows with missing values
data_clean = data.dropna(subset=['species', 'culmen...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly selects the specified feature columns and target column.
*   Correctly drops rows with missing values in the specified columns.
*   Correctly encodes the `species` target variable using `LabelEncoder`.
*   Correctly splits the data into training (80%) and testing (20%) sets with `random_state=42`.
*   Correctly trains a `LogisticRegression` model with `max_iter=1000`.
*   Correctly makes predictions and reports the `accuracy_score`.
*   Includes all necessary imports.

**Areas for improvement (prevented a perfect score):**
*   **Minor inefficiency/redundancy in data cleaning:** The `X` and `y` are initially selected from the original `data` DataFrame. Then, `data_clean` is created by dropping NAs, and `X` and `y` are re-assigned from `data_clean`. While this works, it's slightly less efficient. A more direct approach would be to clean the original DataFrame first based on the relevant columns, and then assign `X` and `y` from the cleaned DataFrame directly.
*   **No error handling for file not found:** The ground truth includes a `try-except` block for `FileNotFoundError`. While not explicitly requested as a core task requirement, it's good practice for robust code.
*   **Lack of explicit messaging:** The ground truth code provides print statements at various steps (e.g., ""Dataset loaded successfully"", ""Data split..."", ""Model trained successfully"", etc.). While not strictly necessary for functionality, these messages enhance readability and user feedback, which can be valuable in a pipeline. The generated code only prints the final accuracy.",llama-3.3-70b-instruct,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
def load_dataset(file_name):
    try:
        return pd.read_csv(file_name)
    except FileNotFoundError:
        print(""The file does not exist"")
        return None
    except pd.errors.EmptyDataError:
        print(""The file is empty"")
        return None
    except pd.errors.ParserError:
        print(""Error parsing the file"")
        return None

# Load the dataset
df = load_dataset('peng...",9,"The generated code correctly addresses all requirements of the task: it loads the dataset, handles NAs in the specified columns, calculates the average body mass per species, and generates a bar plot with a title and labeled axes.

**Strong points:**
*   Correctly uses `pd.read_csv` to load the dataset.
*   Correctly uses `dropna(subset=['body_mass_g', 'species'])` to handle NAs.
*   Correctly uses `groupby('species')['body_mass_g'].mean().reset_index()` for the aggregation.
*   Correctly generates a bar plot using `seaborn.barplot`.
*   Adds a title and labels to the plot.
*   Includes basic error handling for file loading.

**Areas for minor improvement (preventing a perfect 10):**
*   The `load_dataset` function's error handling prints a message and then returns `None`. While this allows the subsequent `if df is not None:` check, for a benchmark where the CSV is assumed to be in the current directory, it might be more direct to simply `exit()` or raise an error if the file isn't found, similar to the ground truth. This prevents further processing on an empty DataFrame, which could lead to subsequent errors not related to the initial file loading.
*   The plot's axis labels and title manually hardcode the column names (`'Species'`, `'Average Body Mass (g)'`). The ground truth uses `replace(""_"", "" "").title()` on the column names, which is a more robust and dynamic way to generate user-friendly labels directly from the column names, especially if the column names were different. This isn't a critical flaw for this specific task but is a good practice for more flexible code.
*   The ground truth also includes `plt.xticks(rotation=45, ha='right')` and `plt.grid(axis='y', linestyle='--', alpha=0.7)`, which are good practices for plot readability, especially with categorical data. While not explicitly asked for, these add to the overall quality of the visualization.",llama-3.3-70b-instruct,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
def load_dataset(file_path):
    try:
        data = pd.read_csv(file_path)
        return data
    except Exception as e:
        print(f""Failed to load dataset: {str(e)}"")

# Perform feature engineering and correlation analysis
def analyze_penguins(file_path):
    # Load the dataset
    penguins = load_dataset(file_path)

    # Drop rows where 'culmen_length_mm', 'culmen_depth_mm', 'flipper_...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, handles NA values in the specified columns, creates the `culmen_area` feature, calculates the Pearson correlation matrix for the specified columns, and visualizes it as an annotated heatmap with the correct title.

**Strong points:**
- **Correctness:** All steps are implemented accurately according to the instructions.
- **Modularity:** The code is well-structured into functions (`load_dataset`, `analyze_penguins`, `main`), which improves readability and reusability.
- **Error Handling (basic):** The `load_dataset` function includes a basic `try-except` block for `FileNotFoundError` (although it catches a generic `Exception`, which is okay for this context but could be more specific).
- **Clear Visualization:** The heatmap is correctly generated with annotations and a title.

**Minor areas for improvement (not critical for this task, but good practice):**
- **Explicit `method='pearson'` for `.corr()`:** While Pearson is the default for `pandas.corr()`, explicitly stating `method='pearson'` can improve clarity, especially when the prompt specifies it.
- **Handling empty DataFrame after dropping NAs:** The ground truth includes a check for an empty DataFrame after dropping NAs, which is good practice to prevent subsequent errors if too many rows are dropped. This wasn't explicitly asked for but is a robust addition.
- **Plotting details:** The ground truth includes `fmt="".2f""`, `linewidths=.5`, and `plt.tight_layout()` for better visual presentation. `square=True` is also good but not explicitly asked for. `plt.xticks` and `plt.yticks` rotations are sometimes helpful for dense plots, but not strictly necessary here.

Overall, the generated code is excellent and fully satisfies the prompt's requirements.",llama-3.3-70b-instruct,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

def load_and_preprocess_penguins_data(path_to_csv='penguins.csv'):
    """"""
    Load penguins dataset and preprocess for classification task.
    
    Parameters:
    path_to_csv (str): Path to penguins.csv dataset
    
    Returns:
    data (pd.DataFrame): Processed features
    target (pd.Series): Target variable 'species'
    """"""
    # Load the dataset
    penguins_data = pd.read_csv(path_to_csv)

    # Select featu...",9,"The generated code is highly correct and addresses almost all requirements of the task.

**Strong points:**
- **Correctness:** The code correctly loads the data, selects features, imputes numerical NaNs with the mean, handles the 'sex' column exactly as specified (imputes 'Unknown' and maps), and applies one-hot encoding to 'island'.
- **Structure:** Encapsulating the preprocessing logic within a function `load_and_preprocess_penguins_data` is excellent for modularity and reusability.
- **Clarity:** The code is readable and well-commented where necessary implicitly (variable names are descriptive).
- **Efficiency:** The use of `fillna(numerical_features.mean())` for numerical imputation is concise and efficient.
- **Output:** It correctly prints the shape and head of the processed features.

**Areas for minor improvement:**
- **SettingWithCopyWarning:** The line `features[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']] = numerical_features.fillna(numerical_features.mean())` can raise a `SettingWithCopyWarning`. While often benign, it's good practice to avoid by explicitly working on a copy or using `.loc`. A safer approach would be:
    ```python
    features.loc[:, ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']] = features[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']].fillna(features[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']].mean())
    ```
    Or even better:
    ```python
    for col in numerical_features:
        features[col].fillna(features[col].mean(), inplace=True)
    ```
    The same applies to `features['sex'] = features['sex'].fillna('Unknown')` and `features['sex'] = features['sex'].map(sex_map)`. It's generally safer to work on a copy of the slice or assign directly to avoid potential `SettingWithCopyWarning`. However, for this specific case, it works because `features` is already a slice.
- **Handling `KeyError` for target:** The ground truth includes `exit()` for `FileNotFoundError`, which is robust. The generated code also handles `FileNotFoundError` implicitly if `pd.read_csv` throws it, but doesn't explicitly catch or handle it as robustly as the ground truth.
- **Target variable output:** The task asked to print the shape and first 5 rows of the ""resulting feature matrix"", which the generated code does correctly. The ground truth also optionally showed `y.value_counts()`, but that wasn't a strict requirement of the prompt.

Overall, the generated code is very good and provides a complete and correct solution to the problem, with only very minor stylistic points keeping it from a perfect 10.",llama-3.3-70b-instruct,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
# Import necessary library
import pandas as pd

# Create DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107], 
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 
        'Quantity': [1, 2, 1, 1, 3, 1, 1], 
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150], 
        'CustomerID': ['CUST001', 'CUST002',...",9,"The generated code correctly addresses all parts of the task. It successfully creates the DataFrame, calculates total spending per customer, identifies and prints the top 3 customers, and calculates total quantity sold per category. The code quality is good, with proper imports and a logical flow.

One minor point for improvement, not critical but good practice, is that the `Total_Spending` column calculation creates a new column in the DataFrame, which is then summed. The ground truth labels this `LineTotal`, which more accurately reflects that it's the spending per line item, before being summed up for the customer. This is a subtle semantic difference and doesn't impact correctness.

Also, the output format for the top 3 customers and total quantity per category in the generated code is a DataFrame, which is perfectly acceptable and informative. The ground truth opted for a slightly more formatted print for the top 3 customers, but this is a presentation choice, not a correctness issue. The generated code covers all requirements accurately.",llama-3.3-70b-instruct,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import numpy as np

# Load data
data = pd.read_csv('titan...",6,"The generated code addresses most of the task requirements but has several shortcomings in terms of correctness, completeness, and code quality compared to the ground truth.

**Areas for Improvement:**

*   **Data Preprocessing and Pipelines:**
    *   **Handling Missing Values:** The generated code only handles missing `Age` with the mean (ground truth uses median) and `Embarked` with 'S' (ground truth uses mode). It completely misses handling `Cabin` (ground truth drops it due to too many missing values) and `Name`, `Ticket`, `PassengerId` (ground truth drops these as irrelevant/non-predictive).
    *   **Categorical Encoding:** The generated code uses `LabelEncoder` for 'Sex' and 'Embarked'. While it works, it assigns arbitrary numerical order, which can mislead the model as Logistic Regression assumes an ordinal relationship. `OneHotEncoder` (as used in the ground truth) is the correct approach for nominal categorical variables like 'Sex', 'Pclass', and 'Embarked', as it avoids this false ordinality. The generated code also `LabelEncoder` 'Embarked' after filling NA with 'S', which is okay, but `Pclass` is left as a raw number without proper consideration of its categorical nature (ground truth treats it as categorical and one-hot encodes it).
    *   **Feature Engineering:** The generated code ignores the instruction to combine `SibSp` and `Parch` into `FamilySize`.
    *   **Pipelines:** The generated code lacks a `ColumnTransformer` and a `Pipeline` structure for preprocessing and modeling, which is a best practice for managing data transformations and preventing data leakage, as demonstrated in the ground truth. The preprocessing is done separately before splitting, which means scaling parameters are learned on the full dataset, then applied to training and test sets. This is a subtle data leakage issue. The ground truth's use of `Pipeline` correctly applies transformations independently to train and test sets.

*   **Feature Coefficients Interpretation:**
    *   The generated code prints `model.coef_`, which is an array of coefficients without corresponding feature names. This makes it impossible to interpret the impact of specific features, especially after `LabelEncoder` and `StandardScaler` (feature names are lost). The ground truth correctly retrieves `encoded_feature_names` from the preprocessor to map coefficients back to their original or one-hot encoded feature names.
    *   No interpretation text is provided for the coefficients.

*   **Visualization:**
    *   The confusion matrix is printed as a raw array. The ground truth uses `ConfusionMatrixDisplay` for a much more readable and informative visual representation.
    *   The ROC curve plotting is mostly correct, but the display and title are less refined than the ground truth's `RocCurveDisplay`.

*   **Completeness/Minor Details:**
    *   The `max_iter=1000` for Logistic Regression is added, which is fine, but not explicitly requested.
    *   The `random_state` is used for `train_test_split` but not for `LogisticRegression` itself (ground truth sets `random_state=42` for consistency).
    *   Cross-validation is performed, but the output only shows the scores for accuracy and not for other requested metrics (precision, recall, f1, ROC-AUC) as per the instruction and the ground truth's comprehensive `cross_validate` approach.
    *   The interpretation of cross-validation results is minimal and not as detailed as required.",llama-3.3-70b-instruct,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

# Create a pandas DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']), 
        'Ticker': ['TECHCO']*8, 
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0], 
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5], 
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0], 
        'Close': [152.0, 151.0, 153.5...",10,"The generated code is excellent. It correctly addresses all aspects of the task, including creating the DataFrame, setting the 'Date' as the index, accurately calculating the 3-day Simple Moving Average (SMA) of the 'Close' price with proper NaN handling for initial days, and correctly calculating the percentage change in 'Volume' from the previous day, also with correct NaN handling for the first day. The code is well-structured, uses appropriate pandas functions (`rolling().mean()`, `pct_change()`), and includes the necessary import. The output format is also correct as it prints the final DataFrame concisely.",llama-3.3-70b-instruct,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
# Import necessary library
import pandas as pd

# Create DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75...",9,"The generated code correctly creates the DataFrame, calculates the 'Score_Improvement' column, computes the average score improvement per subject, and identifies the students meeting the specified criteria.

**Strong points:**
*   **Correctness:** All three main tasks are fully addressed and executed correctly.
*   **Clarity:** The code is well-structured and easy to understand.
*   **Efficiency:** Uses pandas vectorized operations for column creation and filtering, which is efficient.
*   **Completeness:** Addresses all requirements of the prompt.
*   **Output Format:** The output for the average score improvement is clear, and the identification of top students is also handled well, including the case where no students meet the criteria.

**Areas for minor improvement (leading to a score of 9 instead of 10):**
*   **Average Score Improvement Output:** While correct, the ground truth's use of `groupby().mean()` directly prints the result in a more consolidated and typically preferred pandas Series format, which is more idiomatic than iterating through unique subjects and printing individually. Both approaches are functionally correct, but the `groupby` output is often more convenient for further analysis or display.
*   **StudentID Output Format:** The generated code uses `top_students['StudentID'].tolist()` to print the student IDs, which outputs a Python list like `['S001', 'S006']`. The ground truth iterates and prints each ID on a new line, which can be slightly clearer depending on the desired presentation format. However, both are acceptable.

Overall, an excellent solution that provides correct and robust code for the task.",llama-3.3-70b-instruct,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Filter the DataFrame
filtered_df...",9,"The generated code correctly defines the DataFrame, filters it according to the specified conditions, and accurately calculates the average number of projects completed for the filtered group. The code is well-structured and uses appropriate pandas operations. It computes the correct result.

One minor area for improvement is that the ground truth code includes an `if not filtered_df.empty:` check, which is a good practice for robustness in real-world scenarios to handle cases where no data matches the filter criteria. While not strictly required by the prompt's output format (which expects a specific calculation if data exists), it enhances the robustness of the solution. The prompt asks to ""calculate and print the average number of 'ProjectsCompleted'"" which implicitly assumes data will be there. The generated code prints `nan` if `filtered_df` is empty, which is a valid pandas behavior but less user-friendly than an explicit message. However, given the context of a benchmark, the primary objective is meeting the specified calculations, which it does perfectly for non-empty results. The output format for printing the final result is slightly different (e.g., ""Average Projects Completed: X.XX"" vs. ""The average number of 'ProjectsCompleted' for Tech department employees with PerformanceScore > 3 is: X.XX""), but both convey the correct information.",llama-3.3-70b-instruct,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate conversion rates
desktop_conversion_rate = df[df[""DeviceType""] == ""Desktop""][""Converted""].me...",9,"The generated code correctly defines the DataFrame and accurately calculates the conversion rates for both Desktop and Mobile users. It uses `pandas` effectively for filtering and calculating the mean of boolean values, which directly gives the conversion proportion. The printing format is also clear and correctly rounded.

The only minor area for improvement is handling edge cases where a `DeviceType` might not exist in the DataFrame (though not an issue with the given data). The ground truth includes `if not desktop_users_df.empty:` checks, which are good practice for robustness in real-world scenarios, although for this specific dataset and task, they are not strictly necessary as both device types are present. Without these checks, if `Desktop` or `Mobile` users were absent, `.mean()` on an empty Series would return `NaN` or an error depending on the pandas version, which might require handling. However, given the explicit task and provided data, this is a very minor point of robustness rather than correctness for the given task.",llama-3.3-70b-instruct,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

# Define the data
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, , 22.8, 21.9, ],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

# Create the DataFrame
df = pd.DataFrame(data)

# Calculate and print the average AirQualityI...",0,"The generated code has a critical syntax error in the `TemperatureCelsius` list definition (`[22.5, 22.7, , 22.8, 21.9, ]`). This prevents the code from running at all, making it fundamentally incorrect and non-functional. Even if this error were fixed, the original data provided in the prompt for `TemperatureCelsius` was `[22.5, 22.7, 22.6, 22.8, 21.9, 22.0]`, which the generated code does not match. Therefore, it fails to correctly define the DataFrame as specified in the task. Due to the unrecoverable syntax error, no further evaluation of correctness or completeness is possible.",llama-3.3-70b-instruct,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

# Create a new feature ""EngagementScore""
df[""EngagementScore""] ...",9,"The generated code correctly defines the DataFrame, calculates the EngagementScore, filters for Instagram posts, and accurately calculates their average EngagementScore. The code is logically structured, uses standard pandas operations effectively, and fulfills all task requirements.

**Strong Points:**
*   **Correctness:** The code correctly implements all specified computations and filtering.
*   **Clarity:** Variable names (`instagram_posts`, `average_engagement_score`) are clear and descriptive.
*   **Conciseness:** The solution is efficient and to the point.
*   **Proper Imports:** `pandas` is imported correctly.

**Areas for Minor Improvement (to reach a perfect 10):**
*   **Output Formatting:** The ground truth code formats the average engagement score to two decimal places (`.2f`), which is good practice for numerical outputs like averages. The generated code prints the raw float. While functionally correct, improved formatting enhances readability.
*   **Edge Case Handling (Minor):** The ground truth includes a check `if not instagram_posts_df.empty:` to handle the scenario where no Instagram posts are found. While not strictly required by the prompt's examples (which guarantee Instagram posts), it's a robust addition for real-world scenarios. For this specific task, it's not a major omission.",llama-3.3-70b-instruct,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male""...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the ""LengthOfStay"" column accurately, filters for ""Flu"" diagnosis patients, and precisely calculates and prints their average ""LengthOfStay"". The code is clean, well-structured, and directly addresses all parts of the prompt's requirements. It demonstrates a strong understanding of pandas data manipulation.",llama-3.3-70b-instruct,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

# Define the data
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}

# Create the DataFrame
df = pd.Dat...",9,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates their average rating, fulfilling all task requirements. The code is clean, well-structured, and directly addresses the prompt.

One minor improvement, as exemplified in the ground truth code, would be to consider an edge case where no 'Sci-Fi' movies might be present in the DataFrame. While not strictly necessary for *this specific dataset* (as it does contain Sci-Fi movies), robust code often includes checks for such scenarios (e.g., `if sci_fi_movies.empty:`). However, for the given task and data, the current implementation is perfectly functional and correct. The output formatting is also consistent with common practice.",llama-3.3-70b-instruct,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

# Filter the DataFrame to include only Toyota ...",10,"The generated code is entirely correct and effectively addresses all aspects of the task. It properly defines the DataFrame, accurately filters for ""Toyota"" brand cars, and correctly calculates their average mileage, printing the result as requested. The code is clean, well-structured, and uses standard pandas operations efficiently. No improvements are needed.",llama-3.3-70b-instruct,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

# Create DataFrame
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83,...",9,"The generated code correctly defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates and prints their average Attack stat, addressing all requirements of the task. The code is clean, concise, and uses appropriate pandas functionalities. It does not include the explicit check for `fire_pokemon_df.empty` or formatted printing as in the ground truth, which while not strictly necessary for this specific dataset (as Fire-type Pokemon are present), is a good practice for robustness in a more general solution. However, given the defined task and data, the solution is highly effective.",llama-3.3-70b-instruct,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# Load the dataset
df = pd.read_csv('titanic.csv')

# Create a figure with multiple subplots
fig, axs = plt.subplots(2, 2, figsize=(15, 10))

# Survival rates by class
sns.countplot(data=df, x='Pclass', hue='Survived', ax=axs[0, 0])
axs[0, 0].set_title('Survival Rates by Class')
axs[0, 0].set_xlabel('Class')
axs[0, 0].set_ylabel('Count')

# Survival r...",5,"The generated code attempts to address all parts of the task but falls short on several key aspects, primarily in the *sophistication* and *correctness* of the visualizations as requested by the prompt (""advanced data visualization and storytelling"").

**Correctness & Sophistication (Low Score Justification):**
- **1. Multi-panel figure showing survival rates by different demographic groups:** The generated code uses `countplot` to show the *counts* of survivors and non-survivors. The prompt explicitly asks for ""survival rates."" This is a significant conceptual error as a count plot does not directly show rates/proportions. The ground truth correctly calculates and plots these as bar plots of mean survival.
- **2. Heatmap showing survival rates across combinations of class and gender:** This is done correctly, but it's duplicated (once in the first `plt.subplots` block and again as a standalone plot), which is redundant.
- **3. Violin plots comparing age distributions of survivors vs non-survivors by gender:** The generated code uses `hue='Sex'` instead of `hue='Survived'`, which was the primary comparison requested. The ground truth correctly uses `hue='Survived_Cat'` and `split=True` for a more direct comparison of survivors vs non-survivors *within* each gender. The generated code's plot makes it harder to directly compare age distributions between survivors and non-survivors.
- **4. Interactive scatter plot (Age, Fare, Survival, Pclass colors):** The generated code uses `facet_row='Survived'` which creates two separate subplots, which wasn't explicitly asked for but is an acceptable alternative to `symbol='Survived_Str'` if done well. However, it misses the requirement to use ""different colors for passenger classes."" The `color='Pclass'` argument in `plotly.express` does this, but the output is effectively two separate plots rather than one plot with survival shown as a symbol. The ground truth's use of `symbol` is more aligned with ""different colors for passenger classes"" and ""survival"" as a third variable *within* the same plot.
- **5. Comprehensive dashboard-style figure:**
    - The first three plots again use `countplot` for survival rates instead of bar plots of mean survival rates, repeating the conceptual error.
    - Some plots are duplicates or less informative than they could be (e.g., repeating the violin plot from before, a simple age vs. fare scatter without survival context).
    - It lacks the ""storytelling"" aspect and comprehensive insights/annotations that the ground truth provides. The dashboard is a simple collection of plots rather than a guided visual narrative.

**Code Quality:**
- **Redundancy:** Several plots are generated twice (e.g., the heatmap, AgeGroup count plot).
- **Missing Data Preprocessing:** The code doesn't handle missing 'Age' values before creating 'AgeGroup', which could lead to NaNs in the 'AgeGroup' column. The ground truth correctly imputes missing age values.
- **Limited Customization/Refinement:** Matplotlib/Seaborn plots lack finer details like `ylim` for consistent comparison, more descriptive labels, or specific color palettes (`palette='viridis'` in ground truth).
- **Hardcoding Age Bins:** While `pd.cut` is good, the bins chosen ([0, 18, 65, 100]) are very coarse. The ground truth uses more nuanced groups.

**Completeness:**
- While it attempts all 5 points, the quality of execution for each point is variable, as detailed above. The ""storytelling"" and ""proper annotations and insights"" aspects of requirement 5 are largely missing.",llama-3.3-70b-instruct,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price""...",9,"The generated code correctly defines the DataFrame, filters for the 'Dystopian' genre, and accurately calculates the average stock for that genre. It fulfills all the requirements of the task. The code is clean, well-structured, and directly addresses the problem.

The only minor area for improvement, which is not strictly required by the prompt but present in the ground truth for robustness, would be to add a check for the `dystopian_books` DataFrame being empty before attempting to calculate the mean. This would prevent potential errors or `NaN` outputs if, for some reason, no ""Dystopian"" books were present in the initial data. However, given the provided test data, this is not an issue, and the code works perfectly as is. The ground truth also includes optional print statements which were not strictly part of the task's requirements but are good for debugging/verification.",llama-3.3-70b-instruct,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
from scipy import stats
import numpy as np

# Load dataset
data = pd.read_csv('water_potability.csv')

# 1. Conduct t-tests
for feature in data.columns:
    if feature != 'Potability':
        potable_mean = data[data['Potability'] == 1][feature].mean()
        non_potable_mean = data[data['Potability'] == 0][feature].mean()
        t_stat, p_val = stats.ttest_ind(data[data['Potability'] == 1][feature], data[data['Potability'] == 0][feature])
        print(f"" FEATUR...",6,"The generated code attempts to address all requirements of the task but suffers from several issues related to correctness, robustness, and statistical methodology.

**Correctness & Methodology Issues:**

1.  **Missing Value Handling:** The most critical omission is the complete lack of missing value handling. The ground truth explicitly shows robust imputation with the mean. The generated code will likely fail or produce incorrect results if there are `NaN` values in the dataset (which `water_potability.csv` *does* have). This is a fundamental flaw for statistical analysis.
2.  **T-test `equal_var` parameter:** The `ttest_ind` call in the generated code uses the default `equal_var=True`. The ground truth correctly uses `equal_var=False` (Welch's t-test), which is generally more robust and recommended when population variances cannot be assumed equal, especially without prior variance testing.
3.  **Cohen's d Calculation:** The formula used for Cohen's d is incorrect. It uses a very simplified pooled standard deviation (`np.sqrt(((data[data['Potability'] == 1][feature].std()**2 + data[data['Potability'] == 0][feature].std()**2)/2))`), which is only valid if the sample sizes are equal. The ground truth uses the correct formula for pooled standard deviation when sample sizes are unequal, which is almost always the case in real-world data science problems.
4.  **Shapiro-Wilk for Large Datasets:** The generated code directly applies Shapiro-Wilk to the *entire* dataset. While the current dataset size might just be acceptable for Shapiro-Wilk (around 3000 rows according to common knowledge of `water_potability.csv`), the ground truth explicitly mentions and handles the limitation of Shapiro-Wilk for larger sample sizes (e.g., >5000), which is good practice and robustness.
5.  **Chi-square Test:** The chi-square goodness-of-fit test is applied correctly for the specified scenario (comparing to a 50/50 expected distribution).
6.  **Looping through Columns:** The t-tests and normality tests iterate through `data.columns` without explicitly filtering for numerical features. This could lead to errors if non-numerical columns (other than 'Potability') exist in the dataset. While `water_potability.csv` mainly contains numerical data, robust code should account for this.

**Code Quality & Completeness Issues:**

1.  **Readability:** The output formatting is less structured and detailed compared to the ground truth. The ground truth offers more contextual print statements for clarity.
2.  **Lack of Constants:** The significance level (`0.05`) is hardcoded multiple times instead of being defined as a constant (`ALPHA`) as in the ground truth, making it harder to modify globally.
3.  **Error Handling:** There's no `try-except` block for file loading, which is a minor but good practice.
4.  **Missing `numpy` Import:** The generated code implicitly uses `np` for `np.sqrt` without `import numpy as np` at the top (though it is present in the provided snippet, it's missing in the problem description's evaluation part). *Self-correction: I see `import numpy as np` is indeed provided in the generated code itself.*

**Strengths:**

*   Attempts to perform all requested tests (t-tests, chi-square, Shapiro-Wilk).
*   Calculates Cohen's d, even if the formula is incorrect.
*   Interprets p-values against the significance level.

The main shortcomings revolve around statistical robustness (missing value handling, correct Cohen's d, appropriate `equal_var` for t-test) and general code robustness (handling column types, large dataset considerations).",llama-3.3-70b-instruct,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
from sklearn.pipeline import Pi...",6,"The generated code addresses most of the requirements but has several significant issues that prevent it from being a high-quality solution.

**Positive Aspects:**
*   **Most Requirements Met:** The code attempts to perform data preprocessing, train multiple models, evaluate with comprehensive metrics, visualize ROC curves, perform k-fold cross-validation, compare models, and display feature importance.
*   **Correct Metric Calculation:** The metrics are calculated correctly for the individual model evaluations.
*   **Clear Model Definition:** Models are clearly defined in a dictionary.
*   **Basic Visualization:** ROC curves are plotted, and a confusion matrix is printed (though not visualized properly).

**Areas for Improvement / Issues:**

1.  **Data Preprocessing: Data Leakage:**
    *   The `SimpleImputer` and `StandardScaler` are fit on the *entire dataset* (`df` before train-test split, and `X` before train-test split for scaling). This constitutes **data leakage**. The imputation and scaling should be performed *after* the train-test split, fitting only on the training data and transforming both train and test sets.
    *   The ground truth code correctly implements imputation and scaling *after* the train-test split, fitting only on `X_train` and then transforming both `X_train` and `X_test`. It also shows a robust pipeline approach for cross-validation for this specific reason.

2.  **Cross-Validation Implementation:**
    *   The k-fold cross-validation is performed manually using `KFold` and looping through train/test indices. While functional, it's not the standard `cross_val_score` or `cross_validate` from `sklearn.model_selection`.
    *   Crucially, the manual CV `kf.split(X_train)` is splitting the *already scaled training data* for the folds. This means the preprocessing steps (imputation and scaling) are not being applied *per fold correctly*. For robust cross-validation, preprocessing should be part of a `Pipeline` that is passed to `cross_val_score` or done within each fold, which is much harder to implement manually without leakage. The ground truth code correctly demonstrates the use of `Pipeline` within `cross_val_score` for this reason.
    *   Only accuracy is reported for CV. The requirement asks for k-fold CV to assess *model stability* and ideally report multiple metrics (mean and std dev).

3.  **Visualization of Confusion Matrix:**
    *   The code only `print`s the `confusion_matrix` directly, which is hard to read and not a proper visualization as requested (""Create confusion matrices""). The ground truth code uses `sns.heatmap` for proper visualization.

4.  **ROC Curve Visualization:**
    *   Each model's ROC curve is plotted on a *separate* `plt.show()`, leading to multiple plots. The request implies a consolidated view for comparison (or at least making it easy to compare). The ground truth code plots all ROC curves on a single subplot for direct comparison.
    *   It uses `roc_curve` which is fine, but `RocCurveDisplay.from_estimator` is often more convenient and standard.

5.  **Feature Importance for SVM:**
    *   The code incorrectly assumes SVM will have `feature_importances_` if it's the best model. SVMs (especially non-linear ones) don't have a `feature_importances_` attribute or directly interpretable coefficients like linear models. The ground truth code handles this by explicitly stating that SVM doesn't provide direct feature importance.

6.  **Comparison and Best Model Selection:**
    *   The best model is selected based on `ROC-AUC` from the *test set performance*. While `ROC-AUC` is a good metric, selecting the best model based *solely* on a single metric from a single test set run can be prone to variance. Best practice, especially if CV is performed, is to select the model based on CV performance (e.g., mean F1-Score from CV, as done in the ground truth).

7.  **Minor Issues:**
    *   Hardcoded column names for imputation and scaling. While it works for this dataset, it's less flexible.
    *   The `df.replace('Unknown', np.nan, inplace=True)` for missing values is unnecessary as `water_potability.csv` uses `np.nan` directly.
    *   No proper suppression of warnings or structured output with clear print statements for different stages as in the ground truth.
    *   Missing `ConfusionMatrixDisplay` import if planned to use it.
    *   No error handling for file not found.

The most critical issues are the data leakage in preprocessing and the incorrect approach to cross-validation which also leads to data leakage within the CV process itself if not handled by a Pipeline.",llama-3.3-70b-instruct,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import PolynomialFeatures
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score

# Load the dataset
df = ...",5,"The generated code attempts to address most of the requirements but falls short significantly in several critical areas, leading to incorrect or incomplete results.

**Critical Issues (Why it failed to score higher):**

1.  **Missing Value Handling:** The ground truth code explicitly handles missing values by imputing them with the mean. The generated code completely misses this crucial step, which can lead to errors in subsequent calculations (e.g., `df['ph'] / df['Chloramines']` can produce `NaN` if `ph` or `Chloramines` have NaNs, and `df.corr()` will drop rows with NaNs by default or produce NaNs for correlations). This is a fundamental data preprocessing step often required in data science tasks.

2.  **Feature Engineering - Meaningfulness & Scope:**
    *   **Ratios:** Only two ratios are created (`pH_over_chloramines`, `hardness_over_solids`). The task explicitly asks for ""new meaningful features such as ratios between chemical parameters (e.g., pH/Chloramines, Hardness/Solids)"". The ground truth creates several more comprehensive and logical ratios.
    *   **Interaction Terms:** The generated code does not create any explicit interaction terms (e.g., `ph * Chloramines`). It implicitly creates some within `PolynomialFeatures`, but it doesn't separate them as distinct requirements.
    *   **Polynomial Features:** The implementation of `PolynomialFeatures` is problematic for two reasons:
        *   It re-uses original column names for polynomial terms (`ph`, `Hardness`, etc.), leading to direct column name collisions and potential data overwrite/confusion (`df = pd.concat([df, df_poly], axis=1)` could create duplicate column names or misalign data if not handled carefully, though in this case `df_poly` will just add the _transformed_ columns, which are then named confusingly). `df_poly = pd.DataFrame(df_poly, columns=[...])` manually assigns names, which is fragile and error-prone. The ground truth uses `poly.get_feature_names_out(poly_features_cols)` which is robust.
        *   It applies `PolynomialFeatures` only to a *subset* of features (ph, Hardness, Solids, Chloramines) and then manually labels *some* of the combinations. It doesn't fully capture ""polynomial features"" in a general sense applied to multiple relevant original features or ensure non-duplication of original columns as the ground truth does. The ground truth applies it more comprehensively and correctly assigns names.
        *   The ground truth code `for col in poly_temp_df.columns: if col not in X_fe.columns:` explicitly avoids adding already existing columns when `PolynomialFeatures` adds them back (e.g., `x1`, `x2` from `degree=2, include_bias=False`). The generated code does not account for this.

3.  **Feature Selection - Scaling:** The generated code performs `SelectKBest` and `RFE` directly on `X` (the raw features). Feature scaling (e.g., `StandardScaler`) is crucial before applying distance-based or regularization-based methods like `SelectKBest` (especially with `f_classif` which is sensitive to variance) and RFE (if the estimator is sensitive to scale, like many linear models, though less critical for tree-based models, it's still good practice and required for some estimators). The ground truth correctly applies `StandardScaler` to `X_fe` before selection.

4.  **Visualizations:**
    *   **Correlation Heatmap:** The generated code has `annot=True`, which for a matrix of this size (original + engineered), will make the heatmap unreadable due to overlapping annotations. The ground truth correctly sets `annot=False` for large heatmaps.
    *   **Pairwise Scatter Plots:** The generated code's loop for pairwise scatter plots is inefficient and flawed (`for feature1 in top_features: for feature2 in top_features: if feature1!= feature2:`). It generates (N^2 - N) separate plots, which is excessive and hard to interpret. The standard and correct way to do this is using `seaborn.pairplot`, correctly applied in the ground truth, which provides a concise grid of all pairwise relationships and distributions.
    *   **Feature Importance Plot:** Only shows importance for X.columns (original features), not the engineered features, which is essential given the task emphasis on engineered features. The ground truth correctly trains on the full `X_scaled_df` to capture engineered features' importances.

5.  **Comparison of Feature Sets:** While it prints overlaps, it's less comprehensive than the ground truth, which explicitly shows overlap, unique features, and the union of selected features from different methods, providing a more thorough analysis.

**Minor Issues:**

*   `train_test_split` and `PCA` are imported but never used, which is unnecessary.
*   The manual column naming for `df_poly` is bad practice and prone to errors.
*   The `corr_with_potability = ... [1:6]` line for top 5 is correct, but not explicitly printed which ones they are. The ground truth prints the names.

In summary, while the generated code tries to follow the prompt's steps, the execution is often superficial, incomplete, or technically flawed, especially concerning data handling, feature engineering depth, and visualization quality.",llama-3.3-70b-instruct,LLM_judge_config
