task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1) List all column names and identify data types
column_info = {}
for col in df.columns:
    dtype = pd.api.types.infer_dtype(df[col])
    if 'int' in dtype or 'float' in dtype:
        col_type = 'Numerical'
    elif 'categorical' in dtype or 'string' in dtype:
        col_type = 'Categorical'
    else:
        col_type = 'Other'
    column_info[col] = col_type

print(""Column Names and Data Types:"")
for col, dtyp...",7,"The generated code correctly loads the dataset, identifies data types, calculates descriptive statistics for numerical columns, and reports dataset dimensions. However, there are a few areas for improvement.

**Correctness:**
- The identification of data types using `pd.api.types.infer_dtype` is okay, but the comparison `if 'int' in dtype or 'float' in dtype:` is a bit crude and might misclassify. The ground truth's use of `pd.api.types.is_numeric_dtype` is more robust.
- The `desc_stats` indexing for descriptive statistics is problematic: `df[numerical_cols].describe().loc[['mean', '50%', 'std', 'min', '25%', '50%', '75%', 'max']]` tries to select '50%' twice, which is redundant for median. The ground truth simply prints `describe()`, which inherently includes '50%' (median) as part of its output. The re-indexing `desc_stats.index = [...]` is also redundant and leads to `50th percentile` appearing twice because it maps '50%' to both 'median' and '50th percentile' in the list. It's better to just use the default `describe()` output for clarity unless specific columns are needed.
- The categorical/ordinal column identification is based on a heuristic (`if (column_info[col] == 'Categorical') or (unique_values / total_values < 0.05):`). While an attempt to identify categorical columns beyond strict data types, it makes assumptions and might not be accurate for all datasets. For the `winequality-red.csv` dataset, it incorrectly identifies most columns as potential categorical/ordinal due to the ratio condition, even though they are continuous numerical features. The ground truth correctly notes that in this specific dataset, all columns are numerical by pandas dtypes, but `quality` is often treated as ordinal conceptually. This conceptual understanding is difficult for code to infer without explicit context.

**Code Quality:**
- Proper imports are present.
- The structure is logical, addressing each requirement sequentially.
- No `FileNotFoundError` handling is included, which is a robustness issue compared to the ground truth.

**Completeness:**
- All requirements are addressed, but with varying degrees of accuracy and robustness as noted above. The descriptive statistics output is slightly off by manually re-indexing and the categorical identification is heuristic-based and less precise for this dataset than the ground truth's more accurate, albeit less ""smart-inferring,"" approach.",gpt-4.1-nano,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 1) Display the first 10 rows
print(""First 10 rows:"")
print(df.head(10))

# 2) Show the shape of the dataset
print(""\nDataset shape (rows, columns):"")
print(df.shape)

# 3) List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# 4) Top 10 games by Global_Sales
print(""\nTop 10 games by Global_Sales:"")
print(df[['Rank', 'Name', 'Global_Sales']].sort_values(by='Global_S...",7,"The generated code correctly addresses most of the requirements and produces the expected output for a standard dataset. It properly loads the data, displays the head, shape, dtypes, top sales, unique platform/genre, sales statistics, and missing values.

However, there are several areas where it falls short compared to the ground truth, primarily in robustness and user-friendliness:
- **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is crucial for robust code. The generated code lacks this.
- **Output Clarity:** While the generated code includes print statements, the ground truth uses more descriptive headers (`--- 1. Loading Dataset and Displaying First 10 Rows ---`) and separators (`=80`) to make the output clearer and easier to navigate, especially for complex EDA tasks.
- **Specificity in `describe()`:** The ground truth explicitly filters for `existing_sales_columns` before calling `describe()`, which makes it more robust against datasets where not all assumed sales columns might be present. The generated code assumes all `sales_cols` exist without checking.
- **Handling of Missing Values Output:** The ground truth specifically shows *only* columns with missing values and a concluding remark if none are found. The generated code just prints `df.isnull().sum()`, which might show all columns with zeros, making it less direct for identifying *issues*.
- **Redundancy/Efficiency:** The ground truth also provides a more detailed explanation of `df.shape` output (`The dataset has {df.shape[0]} rows and {df.shape[1]} columns.`).

To achieve a higher score, the generated code should incorporate better error handling (especially for file loading), enhance output readability with clear section headers, and add checks for column existence before performing operations like `describe()` or sorting.",gpt-4.1-nano,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"import pandas as pd

# 1) Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 2) Handle missing values in the Year column
median_year = df['Year'].median()
df['Year'] = df['Year'].fillna(median_year)

# 3) Remove games with missing Publisher information
df = df.dropna(subset=['Publisher'])

# 4) Create 'Years_Since_Release' column
current_year = 2023
df['Years_Since_Release'] = current_year - df['Year']

# 5) Remove duplicate games based on Name and Platform
df = df.drop_duplicates(sub...",7,"The generated code addresses most of the requirements correctly. It successfully loads the dataset, handles missing 'Year' values with the median, removes games with missing 'Publisher', creates the 'Years_Since_Release' column, removes duplicates, and filters out low-selling games. It also displays the shape and head of the cleaned dataset.

However, there are a few areas for improvement:
1.  **Robustness in 'Year' and 'Global_Sales' conversion**: The ground truth code explicitly converts 'Year' and 'Global_Sales' to numeric types using `pd.to_numeric` with `errors='coerce'`, which makes the code more robust against non-numeric entries in these columns. This is a common issue in real-world data and the generated code assumes these columns are already numeric or can be implicitly coerced without errors.
2.  **Handling of 'Years_Since_Release' negativity**: The ground truth code includes a step to ensure 'Years_Since_Release' is non-negative after calculation, which is a good practice to prevent illogical negative values if `CURRENT_YEAR` is less than `Year` (e.g., future release dates or data entry errors).
3.  **Informative print statements and error handling**: The ground truth code provides more detailed print statements showing the shape at various stages of cleaning, which is helpful for debugging and understanding the data transformation process. It also includes `try-except` blocks for `FileNotFoundError` and general exceptions, making the script more robust in a production environment.
4.  **Immutability vs. In-place operations**: While both approaches work, the generated code frequently reassigns `df` (e.g., `df = df.dropna(...)`) rather than using `inplace=True`. This is often considered a better practice for clarity and chaining operations, but for individual steps, `inplace=True` can be slightly more memory efficient if the dataframe is very large. In this context, it's a minor point, but worth noting the difference in style.

Overall, it's a good attempt that covers the core logic, but lacks some of the robustness and best practices seen in the ground truth.",gpt-4.1-nano,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 1) Bar chart of top 10 gaming platforms by total global sales
platform_sales = df.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False).head(10)
plt.figure(figsize=(10,6))
platform_sales.plot(kind='bar')
plt.title('Top 10 Gaming Platforms by Total Global Sales')
plt.xlabel('Platform')
plt.ylabel('Total Global Sales (Millions)')
plt.xticks(rotation=45)
plt.tight_la...",7,"The generated code successfully creates all the requested visualizations. The imports are proper, and the structure is clear, with each plot generated sequentially.

However, there are a few areas for improvement:
1.  **Robustness in Data Loading/Cleaning:** The ground truth includes a `try-except` block for file loading and more comprehensive data cleaning (coercing sales columns to numeric, handling `Year_of_Release` NaNs and type conversion) before plotting. The generated code assumes the CSV will always load perfectly and does not handle potential `NaN` values or incorrect data types as thoroughly, which could lead to errors or unexpected plot behavior with messy data. For example, `df['Global_Sales'].dropna()` is used for the histogram, but other plots could benefit from pre-emptive cleaning.
2.  **Completeness of Requirements:**
    *   The task explicitly stated ""Include proper titles and labels for all charts."" While titles and basic x/y labels are present, the ground truth adds more finesse like units (e.g., ""(Millions)"") to the labels, and features like `grid` and `xticks(rotation=45, ha='right')` for better readability, which are missing in the generated code's bar chart.
    *   The ground truth also includes slightly more robust and visually appealing handling of the pie chart (e.g., handling 'Other' category for small genres), histogram (limiting range to percentile), and line plot (filtering year range for clarity), which are minor but contribute to better visualization practices.
3.  **Explicit Plot Numbering:** While not a strict requirement, the generated code numbers its plots 1-5, whereas the prompt implies 6 plots. This is a minor misalignment but could indicate a miscount of the required plots. Specifically, the line plot was numbered 5, despite being the 6th type of plot requested in the prompt.

Overall, the core task is met, but attention to robustness, minor data cleaning, and enhanced visualization details would improve the score.",gpt-4.1-nano,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('diabetes.csv')

# 1) Display the first 10 rows
print(""First 10 rows of the dataset:"")
print(df.head(10))

# 2) Show the shape of the dataset
print(""\nDataset shape (rows, columns):"")
print(df.shape)

# 3) List all column names and data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# 4) Calculate the percentage of diabetic vs non-diabetic patients
print(""\nOutcome distribution (percentage):"")
outcome_counts = df['Outcome'...",7,"The generated code correctly addresses most of the requirements.

**Strong Points:**
- Correctly loads the dataset.
- Prints the first 10 rows.
- Shows the dataset shape.
- Lists column names and data types (though `df.dtypes` is less informative than `df.info()` for seeing non-null counts).
- Calculates the percentage of diabetic vs non-diabetic patients accurately.
- Displays missing value counts.
- Shows summary statistics correctly.
- Attempts to identify zero values that might represent missing data.

**Areas for Improvement:**
- **Zero Values for Missing Data (Crucial):** The ground truth explicitly defines a list of `COLUMNS_TO_CHECK_FOR_ZERO_AS_MISSING`. These are typically 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', and 'BMI'. Pregnancies and Outcome can legitimately be zero. The generated code iterates through all columns, which can lead to misinterpretation (e.g., 'Pregnancies' being 0 is normal). This is a significant point of difference for EDA in this specific dataset.
- **Error Handling:** The ground truth includes `try-except` blocks for file loading, which is good practice. The generated code lacks this.
- **Clarity of Output:** While the generated code prints the information, the ground truth uses more extensive print statements (e.g., `--- 1. First 10 rows of the dataset ---`) and separators (`=`*50) to make the output clearer and easier to navigate for an analyst.
- **`df.info()` vs `df.dtypes`:** For displaying column names and data types, `df.info()` is generally preferred as it also provides non-null counts, which is valuable for identifying potential missing data at a glance.
- **Readability of percentages:** The ground truth rounds percentages to 2 decimal places which makes them more readable.
- **No Missing Values Found Message (Minor):** For missing values, if there are none, the ground truth explicitly states ""No missing values found."", which is clearer than just showing a list of columns with all zeros.
- **Configuration (Minor but good practice):** The ground truth uses `DATASET_PATH` and `TARGET_COLUMN` variables, making the code more configurable and maintainable.",gpt-4.1-nano,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"import pandas as pd
from sklearn.preprocessing import StandardScaler

# 1) Load the dataset
df = pd.read_csv('diabetes.csv')

# 2) Identify columns where zero indicates missing data
cols_with_zero_as_missing = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

# 3) Replace zero values with NaN
df[cols_with_zero_as_missing] = df[cols_with_zero_as_missing].replace(0, pd.NA)

# 4) Handle missing values by filling with median
for col in cols_with_zero_as_missing:
    df[col] = df[col]....",6,"The generated code correctly addresses most of the requirements but has several areas for improvement.

**Areas for Improvement:**

1.  **Robustness in File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError`. The generated code assumes the file will always be present, which is less robust in a real-world scenario.
2.  **Handling `pd.NA` vs. `np.nan`:** The generated code uses `pd.NA` for replacing zeros. While `pd.NA` is more type-aware, `np.nan` is more commonly used and generally works well with numerical data in pandas. Using `np.nan` would align better with typical data cleaning practices especially where `fillna` is used later.
3.  **Specific Column Handling (Missing Zeros):** The generated code applies `replace(0, pd.NA)` and `fillna(df[col].median())` to only the `cols_with_zero_as_missing` list. While this was the explicit request, the `fillna` step should ideally be applied to *all* columns that might have *any* `NaN` values, not just those where zeros were replaced. The specific instruction `Fill them with the median of each respective column` applies to the replaced zeros. The ground truth focuses the median fill step on the specifically identified columns, which is correct given the context.
4.  **Clarity in Numerical Feature Selection for Scaling:** The generated code selects all `int64` and `float64` columns for scaling. This is generally good, but it implicitly scales the `Outcome` column (assuming it's numerical, which it typically is in this dataset, representing 0 or 1). The task details imply that `Outcome` is a target variable and usually isn't scaled. The ground truth explicitly excludes `Outcome` from scaling, which is a better practice.
5.  **BMI Categorization Boundary:** In the `categorize_bmi` function, the 'Normal' category is defined as `18.5 <= bmi < 25`. The prompt specifically requests `18.5-24.9`. While `24.9` and `25` are numerically close, `pd.cut` with `right=True` handles this boundary more precisely and is generally preferred for categorization tasks as it's more idiomatic pandas.
6.  **Descriptive Output:** The ground truth provides more informative print statements, including the original shape, what actions were taken (e.g., duplicates removed, columns scaled), and clear headings, which enhances usability and understanding of the cleaning process. The generated code also includes a basic shape and head display, but it could be more descriptive.",gpt-4.1-nano,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Set plot styles
sns.set(style='whitegrid')
plt.figure(figsize=(10, 8))

# 1) Correlation heatmap
plt.figure(figsize=(10, 8))
corr = df.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap of Numerical Variables')
plt.show()

# 2) Histograms for all numerical features
numeric_features = df.columns.tolist()
plt.figure(figsize=(...",7,"The generated code correctly addresses most of the task requirements and produces the specified visualizations. The structure is logical, and proper imports are included.

However, there are a few areas for improvement:
1.  **Histogram Subplots**: The generated code uses `numeric_features = df.columns.tolist()`, which includes the 'Outcome' column in the histograms. The task implies numerical *features*, and the ground truth code correctly excludes 'Outcome' from the feature-specific plots (histograms, box plots).
2.  **Box Plot Generation**: The generated code creates a new figure for each box plot, leading to many individual plots instead of a combined subplot figure, which is less efficient and harder to view. The ground truth uses subplots, which is preferable for comparing multiple features.
3.  **Redundant `plt.figure`**: There's an extra `plt.figure(figsize=(10, 8))` before the correlation heatmap that is immediately overwritten by another `plt.figure(figsize=(10, 8))`.
4.  **Clarity in Labels/Titles**: While titles and labels are present, some could be more descriptive (e.g., in box plots, explicitly stating '0: Non-Diabetic, 1: Diabetic' for the Outcome axis, similar to the ground truth).

Addressing these points would make the code more robust, efficient, and aligned with standard data visualization best practices.",gpt-4.1-nano,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# 1) Display the first 10 rows
print(""First 10 rows of the dataset:"")
print(df.head(10))

# 2) Show the shape of the dataset
rows, cols = df.shape
print(f""\nDataset shape: {rows} rows, {cols} columns"")

# 3) List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# 4) Unique regional indicators and count of countries per region
print(""\nUnique region...",8,"The generated code successfully addresses all aspects of the task, demonstrating good understanding of basic EDA operations with pandas. It correctly loads the data, displays the first 10 rows, shows the dataset shape, lists column names and types, performs regional analysis, identifies happy countries, calculates summary statistics, and finds missing values.

Strengths:
- Correctly implements all requested tasks.
- Uses appropriate pandas functions for each operation (e.g., `head()`, `shape`, `dtypes`, `value_counts()`, `sort_values()`, `describe()`, `isnull().sum()`).
- Code is clean and readable with appropriate comments.

Areas for improvement (minor):
- The `df.dtypes` output is correct, but `df.info()` as used in the ground truth often provides a more comprehensive summary, including non-null counts, which is very useful in EDA.
- While it correctly catches some columns by name, adding error handling for missing columns (e.g., for `Regional indicator` or `Ladder score`) would make the code more robust, similar to the ground truth.
- The ground truth includes file not found error handling, which is good practice for robust scripts that read external files.
- The output separation (e.g., using `=""*70`) in the ground truth enhances readability significantly when running the full script. The generated code's use of `\n` is good, but consistent separators improve the user experience for larger outputs.",gpt-4.1-nano,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Load dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Handle missing values
# Numerical columns: fill with mean
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())

# Categorical columns: fill with mode
categorical_cols = df.select_dtypes(include=['object']).columns
df[categorical_cols]...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being production-ready or fully correct for the given task.

**Correctness Issues:**
1.  **Missing Value Handling for Categorical Columns:** `df[categorical_cols].mode().iloc[0]` is problematic. If a categorical column has *its own* missing values, `df[categorical_cols].mode()` returns a DataFrame where each column is a Series of modes. `iloc[0]` then selects the first row of this DataFrame, which means it will pick the mode for each column. While this *can* work, the more robust way is to iterate column by column as shown in the ground truth, which implicitly handles cases where `mode()` might return multiple modes or be empty for a specific column. A simpler `fillna(df[col].mode()[0])` per column is more explicit and safer.
2.  **`Well_being_Score` Calculation:** The line `df['Well_being_Score'] = df[['Social support', 'Healthy life expectancy', 'Freedom'].mean(axis=1)]` is syntactically incorrect. It should be `df[['Social support', 'Healthy life expectancy', 'Freedom to make life choices']].mean(axis=1)`. It misses `to make life choices` for 'Freedom' and the syntax for `.mean()` is applied to `'Freedom'` only, not the list of columns. This will result in a `SyntaxError`.
3.  **Standardization of Numerical Features:** The list `numeric_features = ['Ladder score', 'Log GDP per capita', 'Healthy life expectancy', 'Social support', 'Freedom', 'Generosity', 'Corruption']` is hardcoded and incomplete. It misses 'Well_being_Score' (which is numerical and derived), 'Country name' (should not be standardized but is not explicitly excluded), and other potential numerical columns in the dataset not explicitly listed. The ground truth's `select_dtypes(include=np.number).columns` approach is more dynamic and robust. It also attempts to standardize 'Corruption' but the dataset name seems to be 'Perceptions of corruption'. This will cause a KeyError.
4.  **`Regional indicator` Column Name:** The code attempts to handle `Region` or `Regional indicator` but the dataset's specific column name is `Regional indicator`. The conditional check is unnecessary and just `df['Regional indicator']` would suffice given the context of the problem.
5.  **`GDP_Category` Column Name Mismatch:** The code assumes 'Log GDP per capita' whereas the actual column is 'Logged GDP per capita'. This will lead to a `KeyError`.

**Completeness:**
1.  **Error Handling for File Loading:** The ground truth includes `try-except` for `FileNotFoundError`, which is good practice. The generated code lacks this.
2.  **Display Output:** The print statement `print(f""Dataset shape: {df.shape}"")` is good, but the overall presentation (e.g., a header for ""Cleaned Dataset Information"" as in the ground truth) is less polished.

**Code Quality:**
1.  **Hardcoding vs. Dynamic Selection:** Hardcoding lists of numerical columns or specific column names (like `numeric_features`) is less robust than using `select_dtypes` for numerical columns, especially if the dataset structure changes slightly or if new numerical features are added.
2.  **Clarity of Operations:** While the comments are present, the code structure for missing values could be clearer by iterating through columns for categorical data, similar to how it handles numerical data in the ground truth, or at least ensuring the `mode()` application is correct for all columns at once.
3.  **Redundant Logic:** The `if region_col: le = LabelEncoder()...` block is overly cautious given the specific column name stated in the problem description.

To improve, the generated code needs to fix the syntax error in `Well_being_Score` and ensure the column names match the input dataset, handle the standardization more dynamically, and make the categorical missing value handling more explicit.",gpt-4.1-nano,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go

# Load dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Display first few rows to understand structure
# print(df.head())

# 1) Map visualization showing happiness scores by country
# Using Plotly's choropleth map
fig_map = px.choropleth(
    df,
    locations='Country name',
    locationmode='country names',
    color='Ladder s...",7,"The generated code successfully creates most of the requested visualizations. The correctness is generally good, and the code quality is decent with proper imports and reasonable structure.

However, there are several areas for improvement:

1.  **Column Renaming:** The ground truth renames columns for clarity (`Ladder score` to `Happiness Score`, `Logged GDP per capita` to `GDP per Capita`, etc.). The generated code proceeds without this, leading to slightly less readable variable names in plots and making future operations on the dataframe less intuitive. While not explicitly asked for, it's a good practice for data cleaning.
2.  **Robustness (File Loading):** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust. The generated code lacks this.
3.  **Scatter Plots:** The task explicitly asked for *two* scatter plots: ""GDP per capita vs Ladder score"" and ""Social support vs Ladder score"". The generated code creates them as two separate figures, which is less ideal than combining them into a single figure with subplots as done in the ground truth, making comparison easier and reducing visual clutter.
4.  **Radar Chart Details:**
    *   **Normalization:** The most significant issue is the radar chart. The ground truth correctly identifies the need to `MinMaxScaler` the features for the radar chart. Without normalization, comparing values like `Freedom` (typically 0-1) with `Healthy life expectancy` (e.g., 60-80) on the same radar chart is misleading, as the features with larger absolute scales will dominate the visual. The generated code uses raw values.
    *   **Categories:** The categories used for the radar chart in the generated code (`'Healthy life expectancy', 'Freedom', 'Corruption', 'Generosity', 'GDP per capita'`) are missing `Social Support` and `Perceptions of Corruption` (which is named 'Corruption' in the generated code, but the dataset column is 'Perceptions of corruption'). The ground truth uses a more comprehensive set of factors.
    *   **Mean for Top 5:** The line `top5_means = top5[categories].mean()` is calculated but not used, implying a potential misunderstanding of how to plot individual country profiles on a radar chart. The requirement was to show the profile of the top 5 countries, not their average profile. The code does iterate through rows, so it plots individual countries, but the `mean` line is unnecessary.
5.  **Plotting Consistency/Aesthetics:** While not a strict correctness issue, the ground truth sets `sns.set_style` and `plt.rcParams` for consistent and generally better aesthetics across all Matplotlib and Seaborn plots. The generated code defaults to basic settings.
6.  **`Corruption` vs `Perceptions of corruption`:** The generated code tries to use `Corruption` as a column name for the radar chart, which isn't directly in the original dataset. It should use `Perceptions of corruption`. This will likely cause a KeyError unless the dataset implicitly has a 'Corruption' column (which it doesn't).

Overall, the code performs most tasks but misses crucial details like feature normalization for the radar chart and combines scatter plots. The lack of robustness for file loading is also a minor drawback.",gpt-4.1-nano,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"import pandas as pd

# 1) Load the dataset and parse 'date' as datetime
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2) Display the first 10 rows
print(""First 10 rows:"")
print(df.head(10))

# 3) Show the shape of the dataset
rows, cols = df.shape
print(f""\nDataset shape: {rows} rows, {cols} columns"")

# 4) List all column names and their data types
print(""\nColumn names and data types:"")
for col in df.columns:
    print(f""{col}: {df[col].dtype}"")

# 5) Calculate total ...",7,"The generated code addresses most of the requirements and demonstrates a good understanding of foundational Pandas operations for EDA.

**Strengths:**
*   Correctly loads the CSV and parses dates.
*   Correctly displays the first 10 rows and dataset shape.
*   Correctly calculates continental sums for cases and deaths.
*   Correctly counts missing values for each column.
*   Calculates and prints individual summary statistics (mean, median, min, max, std) for 'new_cases' and 'new_deaths', which is a valid way to fulfill the request.

**Areas for Improvement:**

1.  **Robustness (Error Handling):** The code lacks `try-except` blocks for file loading, which is crucial for robust applications. The ground truth includes this, preventing script failure if the file is not found.
2.  **Column Data Types (`df.info()` vs. manual loop):** For listing column names and data types, `df.info()` (as used in the ground truth) is generally preferred over a manual loop. `df.info()` provides more comprehensive information, including non-null counts and memory usage, making it a more complete and idiomatic way to inspect data types in Pandas.
3.  **Top 5 Locations (Logic Error):** The approach for finding the top 5 locations with the highest `total_cases` based on the *latest date for each location* is incorrect.
    *   **Generated Code's Logic:** `latest_date = df['date'].max()` then `latest_data = df[df['date'] == latest_date]`. This finds `total_cases` only for the absolutely latest date across the *entire dataset*, not the latest date *for each individual location*. For example, if Location A's latest entry is Jan 10 and Location B's latest entry is Jan 5, the generated code would only consider data from Jan 10 (and potentially exclude Location B's latest data if Jan 10 is the global max).
    *   **Correct Logic (Ground Truth):** Group by location, sort by date within each group, and then select the latest date per location (e.g., using `drop_duplicates(subset=['location'], keep='last')` after sorting by date). This correctly identifies the highest `total_cases` for each location observed on its *own* latest reported date. This is a significant logical flaw for this specific task requirement.
4.  **Summary Statistics Presentation:** While the individual prints for mean, median, etc., for 'new\_cases' and 'new\_deaths' are functional, using `df[['new_cases', 'new_deaths']].describe()` or `df[['new_cases', 'new_deaths']].agg(['mean', 'median', 'min', 'max', 'std'])` would provide a more concise and readable output, aligning better with standard EDA practices. The ground truth uses `agg` which is efficient. Also, the ground truth correctly converts columns to numeric and handles potential NaNs which is important for statistics.
5.  **Handling Non-Numeric Data:** The generated code directly calls `.mean()`, `.median()`, etc., on 'total\_cases', 'new\_cases', and 'new\_deaths' without explicit type conversion (`pd.to_numeric`) or handling potential non-numeric values. While it might work if the data is perfectly clean, more robust code would explicitly convert these columns to numeric (e.g., `errors='coerce'`) before performing calculations, as the ground truth demonstrates. Similarly, the handling of NaNs in `continent` for grouping is not explicitly done (though `groupby().sum()` might implicitly ignore them, it's good practice to be explicit if clarity is needed).

**Conclusion:** The code is functional for many parts but has a critical logical error in the ""top 5 locations"" requirement and lacks robustness and some idiomatic Pandas practices for a production-ready EDA script.",gpt-4.1-nano,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1) Calculate and report the average 'quality'
avg_quality = df['quality'].mean()
print(f""Average quality of red wines: {avg_quality:.2f}"")

# 2) Group by 'quality' and calculate average 'alcohol'
grouped = df.groupby('quality')['alcohol'].mean().reset_index()

# 3) Create a bar chart
plt.figure(figsize=(8,6))
plt.bar(grouped['quality'], grouped['alcohol'], color='skyblue')
plt.xlabe...",8,"The generated code correctly addresses all three requirements of the task: calculating the mean quality, grouping by quality to get average alcohol, and creating a visualization. The imports are proper, and the structure is clear.

Areas for improvement to achieve a higher score:
1.  **Robustness**: The generated code does not include error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth and makes the code more robust.
2.  **Visualization Enhancement**: While functional, the bar chart lacks the aesthetic improvements (e.g., using `seaborn` for nicer default styles and palettes) and data labels on the bars that are present in the ground truth, which enhance readability and professionalism (though `matplotlib.pyplot` is acceptable for a basic bar chart). The ground truth also explicitly adds specific `fontsize` to titles and labels, making the plot more presentable.
3.  **Output Clarity**: The print statements in the ground truth are more descriptive (e.g., ""1. Average 'quality' of red wines: ..."", ""2. Average 'alcohol' content for each 'quality' group:""). The generated code's print output is more succinct.

Overall, it's a solid attempt that fulfills the core requirements.",gpt-4.1-nano,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"import pandas as pd

# Load the dataset and parse 'date' column as datetime
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Handle missing values in specific columns
df['hosp_patients'].fillna(0, inplace=True)
df['icu_patients'].fillna(0, inplace=True)

# Fill missing values in other numerical columns with their mean
numerical_columns = ['positive_rate', 'reproduction_rate']
for col in numerical_columns:
    df[col].fillna(df[col].mean(), inplace=True)

# Create 'mortalit...",8,"The generated code correctly addresses most of the requirements. It successfully loads the data, handles missing values as specified, creates the 'mortality_rate' and 'vaccination_rate_per_capita' columns, and filters the data for 2021. The handling of division by zero for the rate calculations is also correct.

Areas for improvement:
- The task explicitly asked to ""`Include proper imports`"". The generated code is missing `import numpy as np`, which is used in `float('inf')` and `float('-inf')` in the ground truth, though `float('inf')` technically works without `numpy` in Python. However, given data science conventions, `np.inf` is usually preferred and is more explicit.
- The task asked for specific columns (`'positive_rate', 'reproduction_rate'`) to be filled with their mean. The generated code uses a loop over `numerical_columns` which is correct, but the ground truth explicitly lists these columns and ensures they are numeric before calculating the mean (`pd.to_numeric(df['positive_rate'], errors='coerce')`). While pandas often infers correctly, explicit type conversion for numerical columns before mean calculation is a good practice to prevent errors if the column contains non-numeric strings that prevent mean calculation (though `fillna` would handle `NaN`s if `pd.read_csv` correctly parsed them to `NaN` already). For this specific task, it might not be strictly necessary if the data is clean, but it adds robustness.
- The display statements are slightly different from the ground truth. The ground truth provides more descriptive print statements.
- The ground truth uses `.copy()` after filtering (`df[df['date'].dt.year == 2021].copy()`) to prevent `SettingWithCopyWarning`, which is a good practice when creating a subset of a DataFrame that might be modified later. The generated code omits this.",gpt-4.1-nano,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset and parse 'date' column
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 1) Time series line plot of 'new_cases' for 'India'
india_data = df[df['location'] == 'India'].sort_values('date')
plt.figure(figsize=(12,6))
sns.lineplot(data=india_data, x='date', y='new_cases')
plt.title(""New COVID-19 Cases in India Over Time"")
plt.xlabel(""Date"")
plt.ylabel(""New Cases"")
plt.tight_layout()
plt....",7,"The generated code addresses most of the visualization requirements, including time series, bar, scatter, histogram, and box plots. It correctly loads the data and parses the date column. The plots generally have proper titles and labels.

However, there are several areas for improvement:

1.  **Error Handling:** The generated code lacks error handling for file not found or other data loading issues, which is present in the ground truth.
2.  **Specific Requirements Misinterpretation (Bar Chart):** For the bar chart of 'total_deaths' by 'continent', the instruction implied using the *latest* 'total_deaths' for each *country* (location) summed by continent, not just a direct `groupby().sum()` across all entries. The generated code's approach of `df.groupby('continent')['total_deaths'].sum()` would sum up all entries, leading to inflated and incorrect 'total_deaths' if a country has multiple entries. The ground truth correctly handles this by first getting the latest 'total_deaths' for each location (`drop_duplicates(subset='location', keep='last')`) before grouping by continent. This is a significant logical flaw for that specific plot.
3.  **Scatter Plot Enhancements:** The scatter plot for GDP vs. Vaccinations could benefit from a `log` scale for both axes, as these values often span several orders of magnitude, making linear scales less informative. Also, the ground truth uses `population` for sizing points, which is a nice additional detail.
4.  **Plot Styling:** While functional, the generated code doesn't explicitly set a professional style like `sns.set_theme(style=""whitegrid"")` as the ground truth does, which improves visual appeal.
5.  **Output Clarity:** The ground truth includes print statements to indicate which plot is being generated, which is helpful for debugging and understanding the flow.
6.  **Bar Chart `xticks` Rotation:** The bar chart for continents could benefit from `plt.xticks(rotation=45, ha='right')` for better readability of continent names, especially if there are many.",gpt-4.1-nano,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 1) List all column names and their data types
print(""Column Names and Data Types:"")
for column in df.columns:
    print(f""{column}: {df[column].dtype}"")

# 2) Calculate descriptive statistics for numerical columns
numerical_columns = ['elixirCost', 'Win Rate', 'Usage']
print(""\nDescriptive Statistics:"")
for col in numerical_columns:
    if col in df.columns:
        print(f""\n{col}:"")
        print(f""  Mean: {d...",8,"The generated code successfully completes the task by loading the dataset, listing column names and data types, calculating descriptive statistics for the specified columns, and reporting the total number of cards.

**Strong points:**
*   **Correctness:** All required operations are performed correctly.
*   **Clarity:** The code is well-structured and easy to understand.
*   **Completeness:** All parts of the prompt are addressed.
*   **Error Handling (Implicit):** The check `if col in df.columns:` for numerical statistics is good practice, preventing `KeyError`.

**Areas for improvement:**
*   **Robustness:** The code lacks explicit error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. This could lead to a crash if the file is missing.
*   **Output Formatting:** While functional, the output format for column names and descriptive statistics is less organized than the ground truth. For instance, `df.dtypes` provides a clean, single-line output for data types which is often preferred over a loop for each type. Similarly, using `.agg()` provides a more concise and tabular view of statistics.
*   **Efficiency:** Looping through each numerical column and printing individual statistics is less efficient and produces a less readable output than using `df[numerical_columns].describe()` or `df[numerical_columns].agg(...)` for descriptive statistics.",gpt-4.1-nano,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Filter cards with elixirCost less than or equal to 3
filtered_df = df[df['elixirCost'] <= 3]

# Select 'Card' and 'Win Rate' columns
selected_df = filtered_df[['Card', 'Win Rate']]

# Sort by 'Win Rate' in descending order
sorted_df = selected_df.sort_values(by='Win Rate', ascending=False)

# Display the result
print(sorted_df)
```",9,"The generated code correctly addresses all the requirements of the task: it loads the CSV, filters by `elixirCost` <= 3, selects the specified columns (`Card`, `Win Rate`), and sorts the results by `Win Rate` in descending order. The imports are correct, and the code is concise and readable.

The only minor point of improvement compared to the ground truth (which is very robust) is the lack of error handling for `FileNotFoundError` or missing columns. While not explicitly asked for in the prompt's simplicity, it's a good practice for real-world scenarios. However, for a direct task completion, it functions perfectly.",gpt-4.1-nano,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Plot histogram of 'elixirCost'
plt.figure(figsize=(8, 6))
sns.histplot(df['elixirCost'], bins=10, kde=False)
plt.title('Distribution of Elixir Cost')
plt.xlabel('Elixir Cost')
plt.ylabel('Frequency')
plt.show()",7,"The generated code correctly loads the CSV and generates a histogram for the `elixirCost` column with a title and labeled axes. This fulfills the primary requirements of the task. The code is concise and uses appropriate libraries (pandas, matplotlib, seaborn).

However, it lacks the robust error handling present in the ground truth for `FileNotFoundError` or missing columns, which is a good practice for real-world scenarios. The `bins=10` chosen is a default and not as specifically tailored as `bins=range(1, 12)` in the ground truth, which better represents the discrete nature of elixir costs (typically 1-10) and ensures each cost gets its own bin. Also, the ground truth includes `xticks` setting for clarity. While not explicitly asked for, these additions make the ground truth more complete and robust.",gpt-4.1-nano,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('iris.csv')

# 1) List all column names and their data types
columns = df.columns.tolist()
dtypes = df.dtypes
print(""Column Names and Data Types:"")
for col in columns:
    print(f""{col}: {dtypes[col]}"")

# 2) Descriptive statistics for numerical columns
numeric_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
stats = df[numeric_cols].describe().loc[['mean', '50%', 'std', 'min', 'max']]
stats.rename(index={'50%': 'm...",7,"The generated code correctly performs all the requested tasks: loading the data, identifying column names and data types, calculating descriptive statistics for the specified columns, and reporting unique species counts.

**Strong Points:**
- Correctly uses `pandas` for data loading and manipulation.
- Accurately identifies column names and data types.
- Correctly calculates and presents descriptive statistics (mean, median, std, min, max) for the specified numerical columns using `describe()` and `rename`.
- Successfully reports the number of unique species and their counts.
- The code is concise and readable.

**Areas for Improvement (Why not a 10):**
- **Robustness (Missing file handling):** Unlike the ground truth, the generated code directly attempts to load `iris.csv` without any `try-except` block. If `iris.csv` is not present, the script will crash with a `FileNotFoundError`. The ground truth includes robust error handling, which is crucial for real-world applications.
- **Output Clarity/Formatting:** While the output contains the correct information, the ground truth uses more explicit headings and a consistent print structure (`--- Section Title ---`) to make the output clearer and more segmented, especially for a benchmark where specific outputs are expected. For instance, `print(""\nDescriptive Statistics:"")` is a good start, but more explicit separators could enhance readability.
- **Explicit Check for Column Existence:** The generated code assumes all `numeric_cols` and the `Species` column exist. If a column name were misspelled or missing in the CSV, it would raise a KeyError. The ground truth includes checks (`if col in df.columns`) before operating on columns, making it more robust against malformed or unexpected data.
- **No explicit print for column names vs. data types:** The generated code prints column names and data types in a loop (`for col in columns: print(f""{col}: {dtypes[col]}"")`). While correct, the ground truth's approach (`print(""Column Names:"", df.columns.tolist())` and `print(df.dtypes)`) provides a more direct and often more efficient way to display these, especially for `dtypes`.

In summary, the generated code fulfills the core requirements but lacks the robustness and detailed output formatting that a more production-ready or benchmark-perfect solution would include.",gpt-4.1-nano,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('iris.csv')

# Filter for 'Iris-setosa'
setosa_df = df[df['Species'] == 'Iris-setosa']

# Select and display 'SepalLengthCm' and 'SepalWidthCm' columns
print(setosa_df[['SepalLengthCm', 'SepalWidthCm']])
```",9,"The generated code correctly addresses all the core requirements of the task: it loads the CSV, filters for 'Iris-setosa' species, and selects and displays the specified `SepalLengthCm` and `SepalWidthCm` columns. The imports are proper, and the code structure is clean and concise. The use of boolean indexing and direct column selection is idiomatic pandas.

The only minor area for improvement, especially for a production-level or robust script, would be the addition of error handling (e.g., `try-except` blocks for `FileNotFoundError` or `KeyError` if columns are missing), which is present in the ground truth but not strictly specified as a requirement in the prompt's evaluation criteria. However, for a data science benchmark focusing on the core logic, it performs excellently.",gpt-4.1-nano,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('iris.csv')

# Create the scatter plot
plt.figure(figsize=(8,6))
species_unique = df['Species'].unique()

for species in species_unique:
    subset = df[df['Species'] == species]
    plt.scatter(subset['SepalLengthCm'], subset['SepalWidthCm'], label=species)

plt.title('Sepal Length vs Sepal Width in Iris Dataset')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.legend()
plt.show...",9,"The generated code correctly performs all the required tasks: it loads the `iris.csv` dataset, creates a scatter plot of `SepalLengthCm` vs `SepalWidthCm`, colors the points by `Species`, and includes a title, x-axis label, y-axis label, and a legend. It also includes the necessary imports and has a good structure.

The only minor area for improvement is the axis labels. While they are present and descriptive, the ground truth uses the exact column names (`SepalLengthCm`, `SepalWidthCm`), which is sometimes preferred for direct correspondence, even though ""Sepal Length (cm)"" is also perfectly acceptable and arguably more user-friendly. Another very minor point is the lack of error handling (like the ground truth's `try-except` block for `FileNotFoundError`), but the prompt did not explicitly ask for robust error handling, focusing more on the core plotting task. Overall, it's a very good solution.",gpt-4.1-nano,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# 1) List all column names and their data types
print(""Column Names and Data Types:"")
for col in df.columns:
    print(f""{col}: {df[col].dtype}"")

# 2) Descriptive statistics for specified numerical columns
numeric_columns = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']
print(""\nDescriptive Statistics:"")
for col in numeric_columns:
    print(f""\n{col}:"")
    print(f""  Mean: {df[col].mean()}"")
    print(f""  Median: {df[col].med...",7,"The generated code correctly performs all the requested tasks: loading the CSV, printing column names and data types, calculating descriptive statistics for the specified numerical columns, and reporting value counts for 'HeartDisease'.

However, there are areas for improvement regarding robustness and output formatting compared to the ground truth.

**Areas for improvement:**
1.  **Error Handling:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV. The ground truth includes a `try-except` block for this, which is crucial for robust code.
2.  **Descriptive Statistics Output:** While correct, the manual printing of each statistic (`mean`, `median`, `std`, etc.) for each column is less efficient and harder to read than using `df[numeric_columns].describe()` as seen in the ground truth. The `describe()` method provides a well-formatted table of all these statistics at once.
3.  **Output Clarity/Formatting:** The output formatting is functional but could be improved. The ground truth uses headers and separators (`---` and `*` lines) to clearly demarcate different sections of the output, making it more readable and professional. For instance, the descriptive statistics output is very verbose due to printing each statistic individually.
4.  **Column Existence Checks:** The ground truth includes checks to ensure that the specified numerical columns actually exist in the DataFrame before attempting to calculate statistics. This makes the code more robust against variations in the dataset. While unlikely for a fixed task, it's good practice.

Overall, it's a good attempt that solves the core problem, but it could be more robust and produce cleaner output.",gpt-4.1-nano,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# Filter for patients with Age > 50 and HeartDisease == 1
filtered_df = df[(df['Age'] > 50) & (df['HeartDisease'] == 1)]

# Select and display specific columns
result = filtered_df[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]
print(result)
```",9,"The generated code correctly implements all specified requirements: it loads the CSV, filters the data for `Age` > 50 and `HeartDisease` == 1, and then selects and displays the requested columns. The code is concise and directly addresses the problem.

One minor area for improvement, which is present in the ground truth but not strictly required by the prompt, would be to wrap the logic in a function. This improves reusability and testability. Additionally, adding error handling (e.g., `FileNotFoundError`) would make the code more robust. However, given the explicit instructions to provide *only* the Python code without explanations and the problem statement's focus on the data manipulation logic, the provided solution is excellent. The ground truth includes a lot of extra scaffolding (docstrings, error handling, `if __name__ == ""__main__"":`) that wasn't explicitly asked for, making the generated code a very good direct answer to the prompt.",gpt-4.1-nano,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Filter wines with alcohol > 12 and pH < 3.3
filtered_df = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]

# Identify the wine with the highest quality in the filtered subset
top_wine = filtered_df.loc[filtered_df['quality'].idxmax()]

# Display all features of this wine
print(top_wine)
```",8,"The generated code correctly implements all specified requirements: it filters the DataFrame based on the compound condition for 'alcohol' and 'pH', identifies the wine with the highest 'quality' within the filtered subset, and displays all its features. The use of Pandas operations (`read_csv`, boolean indexing, `idxmax`, `loc`) is efficient and idiomatic.

Areas for improvement:
1.  **Error Handling**: The ground truth includes robust error handling for `FileNotFoundError`, `KeyError`, and general exceptions, which is crucial for real-world applications. The generated code lacks this.
2.  **Modularity/Function Encapsulation**: Wrapping the logic within a function (as in the ground truth) improves reusability, testability, and overall code organization, especially for more complex tasks.
3.  **Comments and Docstrings**: While not strictly required by the prompt's output format, good code practices include comments to explain complex logic and docstrings for functions, which are present in the ground truth.
4.  **Edge Case Handling**: The ground truth explicitly checks if `filtered_wines` is empty before trying to find the max quality. While `idxmax()` would raise an error on an empty Series, an explicit check provides clearer user feedback.

Despite these suggestions for improvement, the core logic for solving the problem is perfectly implemented.",gpt-4.1-nano,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('heart.csv')

# Plot histogram for 'Age'
plt.figure(figsize=(8, 6))
sns.histplot(df['Age'], bins=20, kde=False)
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Plot bar chart for 'Sex'
plt.figure(figsize=(6, 4))
sns.countplot(x='Sex', data=df)
plt.title('Distribution of Sex')
plt.xlabel('Sex')
plt.ylabel('Count')
plt.show()
```",7,"The generated code correctly loads the dataset and produces both the histogram for 'Age' and the bar chart for 'Sex'. It uses appropriate libraries (pandas, matplotlib, seaborn) and includes titles and labels as requested.

Areas for improvement:
1.  **Robustness (Error Handling):** The ground truth includes `try-except` blocks for `FileNotFoundError` and `KeyError`, which makes the code more robust. The generated code lacks this.
2.  **Clarity for 'Sex' column:** The ground truth code maps the numerical 'Sex' column (likely 0 and 1) to 'Female' and 'Male' for better readability in the bar chart. The generated code plots 0 and 1 directly, which is less user-friendly without context.
3.  **Plot Customization:** While meeting the basic requirements, the ground truth adds more detailed customization like `fontsize`, `grid`, `tight_layout`, and specific `color` palettes, which improve the aesthetics and readability of the plots. The generated code is more basic in its plotting.
4.  **`kde=True` for Histogram:** The ground truth includes `kde=True` for the histogram, which provides a Kernel Density Estimate overlay, often helpful for understanding distribution shape. The generated code sets `kde=False`. While not strictly required, it's a common practice for histograms to include KDE.",gpt-4.1-nano,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 1) List all column names and their data types
print(""Column Names and Data Types:"")
for col in df.columns:
    print(f""{col}: {df[col].dtype}"")

# 2) Convert 'streams' column to numeric, coercing errors to NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# Report any NaNs in 'streams'
num_nan_streams = df['streams'].isna().sum()
print(f""\nNumber of NaNs in 'streams' after conversion: {num_...",7,"The generated code successfully addresses most of the task requirements, demonstrating correct implementation for loading data, converting 'streams', calculating descriptive statistics, and counting unique artists.

However, there are a few areas for improvement:
1.  **Error Handling for File Loading:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV. The ground truth includes a `try-except` block, which is crucial for robust code.
2.  **Explicit Data Type Reporting (for all columns):** While it iterates and prints the data type for each column, it doesn't provide the same comprehensive output as `df.info()` which also shows non-null counts, memory usage, etc., which is often preferred for initial data understanding. The ground truth uses `df.info()` which is more concise and informative.
3.  **Reporting NaNs:** The ground truth code provides more context around the `streams` conversion, including the original and new dtypes, and (optionally) sample rows where NaNs were introduced. The generated code only reports the count of NaNs.
4.  **Clarity of Output Headings:** The output headings for each section (e.g., ""Column Names and Data Types:"", ""Descriptive Statistics:"") are present, but the ground truth adds more comprehensive and descriptive headings like ""--- Column Names and Data Types ---"", which makes the output clearer and easier to follow.
5.  **Robustness for Missing Columns:** The ground truth includes checks like `if existing_numerical_cols:` or `if 'artist(s)_name' in df.columns:` to prevent `KeyError` if a specified column is not found in the dataset. The generated code assumes all columns will exist, which could lead to errors with different datasets.

Overall, it's a good attempt, but the robustness and detail in reporting could be improved to match the quality of the ground truth.",gpt-4.1-nano,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Filter for songs released in 2023
df_2023 = df[df['released_year'] == 2023]

# Convert 'streams' to numeric, coercing errors to NaN
df_2023['streams'] = pd.to_numeric(df_2023['streams'], errors='coerce')

# Select relevant columns
result = df_2023[['track_name', 'artist(s)_name', 'streams']]

# Sort by 'streams' in descending order
result_sorted = result.sort_values(by='streams', ascending=False)

# Display the resul...",8,"The generated code correctly addresses almost all requirements: it loads the CSV, filters by `released_year` == 2023, selects the specified columns, converts `streams` to numeric with error coercion, and sorts by `streams` in descending order. The code is concise and readable.

However, there are a couple of minor points for improvement that keep it from a perfect score:
1.  **SettingWithCopyWarning Potential:** Modifying `df_2023['streams']` after filtering (`df_2023 = df[df['released_year'] == 2023]`) can sometimes trigger a `SettingWithCopyWarning`. While it might work in this specific case, it's generally safer to use `.loc` to ensure you're working on a copy as intended or on the original DataFrame to avoid chained assignments, like `df.loc[df['released_year'] == 2023, 'streams'] = pd.to_numeric(...)`.
2.  **Handling NaN in Streams:** The ground truth includes a `dropna(subset=['streams'])` before sorting. While not explicitly stated as a requirement, sorting with NaNs by default puts them at the end. For ""top streams"" lists, often NaNs are intended to be excluded. The generated code implicitly handles this by placing NaNs at the end of the sorted list, which might be acceptable depending on interpretation, but the ground truth's explicit drop is often preferred for clarity.

Overall, it's a very good solution that successfully solves the core task.",gpt-4.1-nano,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 1) Histogram of 'bpm'
plt.figure(figsize=(8, 6))
plt.hist(df['bpm'], bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of BPM')
plt.xlabel('BPM')
plt.ylabel('Frequency')
plt.show()

# 2) Scatter plot of 'danceability_%' vs 'energy_%'
plt.figure(figsize=(8, 6))
plt.scatter(df['danceability_%'], df['energy_%'], alpha=0.7, color='orange')
plt.title('Danceabili...",9,"The generated code successfully loads the dataset, generates a histogram for `bpm`, and a scatter plot for `danceability_%` vs `energy_%`. Both plots have appropriate titles and axis labels. The necessary imports are included.

The code is concise and directly addresses all requirements of the task. The use of `plt.show()` after each plot is good practice for displaying separate plots.

Areas for minor improvement:
- Error handling for `FileNotFoundError` or missing columns could be added for robustness, though not explicitly asked for in the prompt.
- The `bins` parameter for the histogram could be `auto` (as in Ground Truth) or carefully chosen, 30 is a reasonable default but `auto` can sometimes be better.
- While the titles and labels are correct, they could be slightly more descriptive (e.g., ""Distribution of BPM in Spotify 2023"" vs. ""Distribution of BPM""). This is a minor stylistic point.

Overall, it's a very good solution that fully satisfies the core requirements.",gpt-4.1-nano,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# 1) List all column names and their inferred data types
print(""Column Names and Data Types:"")
print(df.dtypes)

# 2) Clean and convert specified columns
columns_to_clean = ['Video views', 'Likes', 'Dislikes']
for col in columns_to_clean:
    # Remove commas
    df[col] = df[col].astype(str).str.replace(',', '', regex=False)
    # Convert to numeric, coercing errors to NaN
    df[col] = pd.to_numeric(df[...",7,"The generated code addresses most of the requirements but falls short in a few areas that prevent it from being a perfect solution.

**Strong points:**
- Correctly loads the CSV and handles the specified columns for conversion, filling NaNs, and cleaning.
- Uses `astype(str).str.replace` and `pd.to_numeric(errors='coerce')` for robust cleaning of numeric columns.
- Correctly fills NaNs in the 'Category' column.
- Calculates descriptive statistics for the cleaned numeric columns.

**Areas for improvement:**
1.  **Reporting `df.info()` for column names and types:** The task explicitly asked to ""List all column names and their *inferred data types*"". While `print(df.dtypes)` shows data types, `df.info()` is typically preferred as it also shows non-null counts, which is valuable information. The generated code only provides `df.dtypes`.
2.  **Reporting unique category count vs. value_counts:** The task asked to ""Report the count of unique values in the `Category` column."" The generated code prints `value_counts()`, which lists each unique category and its count, not just the total count of unique categories (`df['Category'].nunique()`). This is a nuance but the instruction was specific.
3.  **Descriptive Statistics Output:** While the `describe()` method with `loc` and `rename` correctly gets the mean, median, std, min, and max, the output format is a DataFrame. The ground truth prints each statistic individually with more descriptive labels, which might be considered more ""reported"" as per the prompt. It's a minor formatting difference, but it's less direct.
4.  **Error Handling for File Not Found:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes and is good practice. This is a point of robustness.
5.  **Clarity of Output:** The ground truth includes more print statements to denote what sections of the output correspond to which step, making the output more readable and self-explanatory. The generated code is more concise but less verbose in its reporting.",gpt-4.1-nano,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the 'Video views' column
df['Video views'] = df['Video views'].astype(str).str.replace(',', '', regex=False)
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce').fillna(0)

# Filter videos with views greater than 50,000,000
filtered_df = df[df['Video views'] > 50000000]

# Select 'Video' and cleaned 'Video views' columns
result = filtered_df[['Video', 'Video views']]

# Sort by '...",9,"The generated code correctly implements all specified requirements: it loads the CSV, cleans the 'Video views' column by removing commas, converting to numeric, and handling NaNs by filling with 0. It then filters the data based on the view count, selects the required columns, and sorts the result in descending order. The code is concise and efficient.

The only minor area for improvement, preventing a perfect 10, is the lack of a `try-except` block for file loading, which is a good practice for robustness, as present in the ground truth. However, since the prompt specifies ""assume the CSV file is in the current directory"" and focuses on the data manipulation logic, the absence of error handling for file not found doesn't critically detract from the solution's correctness for the core task. The `fillna(0)` is correctly chained after `pd.to_numeric(errors='coerce')`, which is a clean way to handle the conversion and NaN filling.",gpt-4.1-nano,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"import pandas as pd
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Likes' column
df['Likes'] = df['Likes'].astype(str).str.replace(',', '', regex=False)
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Likes'].fillna(0, inplace=True)

# Clean 'Video views' column
df['Video views'] = df['Video views'].astype(str).str.replace(',', '', regex=False)
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce')
df['Video views'...",9,"The generated code correctly performs all the required steps: loading the data, cleaning the specified columns by removing commas, converting to numeric, and filling NaNs with 0, and then creating a scatter plot with the specified title and axis labels. The imports are correct.

One minor area for improvement is that the cleaning of the two columns (`Likes` and `Video views`) could have been abstracted into a loop or function, similar to the ground truth, to reduce redundancy, especially if there were many columns to clean. Also, it doesn't include error handling for `FileNotFoundError` when loading the CSV, which the ground truth does, making the ground truth more robust. However, the prompt only asked for ""proper imports"" and ""assume the CSV file is in the current directory"", so robust error handling wasn't explicitly a core requirement for a perfect score within the task's scope. The `inplace=True` usage is a valid way to fill NaNs.",gpt-4.1-nano,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean 'Mobile Weight' column
df['Mobile Weight'] = pd.to_numeric(df['Mobile Weight'].str.replace('g', '', regex=False).str.strip(), errors='coerce')

# Clean 'RAM' column
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', '', regex=False).str.strip(), errors='coerce')

# Clean 'Battery Capacity' column
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', '', regex=False).str.strip(), er...",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans all specified columns ('Mobile Weight', 'RAM', 'Battery Capacity', 'Launched Price (USA)') by removing the specified strings and converting them to numeric types, handling errors by coercing to NaN. It then correctly lists the data types after cleaning, calculates and reports the required descriptive statistics (mean, median, std, min, max) for the cleaned numeric columns, and accurately counts the unique values in the 'Company Name' column. The code is well-structured and uses appropriate pandas functions. It uses `pd.to_numeric` with `errors='coerce'` as required. The descriptive statistics output correctly includes 'median' by renaming '50%'. The only minor point is the explicit loop for printing dtypes which could be `print(df.dtypes)` for conciseness, but the current implementation is not incorrect. It doesn't include the error handling for `FileNotFoundError` as the ground truth does, but the prompt implicitly assumes the file exists in the directory. Overall, it's a very good solution.",gpt-4.1-nano,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean 'Launched Price (India)' column
df['Launched Price (India)'] = df['Launched Price (India)'].astype(str).str.replace('INR ', '', regex=False).str.replace(',', '', regex=False)
df['Launched Price (India)'] = pd.to_numeric(df['Launched Price (India)'], errors='coerce')

# Clean 'RAM' column
df['RAM'] = df['RAM'].astype(str).str.replace('GB', '', regex=False)
df['RAM'] = pd.to_numeric(df['RAM'], errors='coerce')

#...",9,"The generated code correctly performs all the required operations: loading the CSV, cleaning both 'Launched Price (India)' and 'RAM' columns (including error handling for numeric conversion), filtering the data based on company and price, selecting the specified columns, and sorting the final result. The code is concise and demonstrates a good understanding of pandas operations.

Areas of strength:
- Correct use of `pd.read_csv`.
- Efficient chaining of `.str.replace()` calls for cleaning price and ramen columns.
- Proper use of `pd.to_numeric(errors='coerce')` for robust type conversion.
- Accurate filtering condition for `Company Name` and `Launched Price (India)`.
- Correct column selection using a list of column names.
- Correct sorting using `sort_values` (ascending is default).

Areas for minor improvement (not critical for correctness in this specific task):
- The ground truth includes error handling for `FileNotFoundError` during CSV loading, which is good practice for robustness. The generated code does not include this, which is a minor reduction in code quality for a production-ready script but perfectly acceptable for a direct task solution.
- The ground truth also has print statements for debugging and clarity, which were not explicitly requested but are helpful in a real-world scenario.
- Using `.copy()` after filtering is a good practice to prevent `SettingWithCopyWarning`, which the generated code implicitly avoids by reassigning to `result_df` but isn't explicitly there after the filter.

Overall, it's a very solid solution that completely fulfills the task requirements.",gpt-4.1-nano,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the RAM column
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', '', case=False).str.strip(), errors='coerce')

# Clean the Battery Capacity column
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', '', case=False).str.strip(), errors='coerce')

# Plotting
plt.figure(figsize=(10,6))
sns.scatterplot(
    data=df,
    x='RAM',...",9,"The generated code successfully addresses almost all requirements of the task.

**Strong Points:**
- **Correct CSV Loading:** Loads `mobiles_2025.csv` correctly.
- **Effective Cleaning:** Properly cleans both the `RAM` and `Battery Capacity` columns by removing units ('GB', 'mAh') and converting them to numeric, correctly handling conversion errors with `errors='coerce'`. It also handles `strip()` for whitespace.
- **Correct Plot Type and Variables:** Uses a `sns.scatterplot` as required, plotting `Battery Capacity` on the y-axis and `RAM` on the x-axis.
- **Correct Coloring:** Successfully colors the points by `Company Name` using the `hue` parameter.
- **Proper Labels and Title:** Includes a relevant title and correct labels for both the x-axis and y-axis.
- **Good Code Quality:** Imports are correct, and the code is structured logically. It also includes `plt.tight_layout()` for better plot presentation.

**Areas for Minor Improvement (Why not a 10):**
- **In-place Column Modification:** The generated code overwrites the original `RAM` and `Battery Capacity` columns directly (`df['RAM'] = ...`, `df['Battery Capacity'] = ...`). While this works, it can sometimes be safer or more conventional in data cleaning pipelines to create new cleaned columns (e.g., `RAM_cleaned`) and keep the original raw data, especially if the raw data might be needed later or debugging. The ground truth took this approach. However, for this specific task, overwriting is functionally acceptable.
- **Missing FileNotFoundError Handling:** The generated code does not include error handling for `FileNotFoundError` when loading the CSV, which the ground truth did. This is a minor robustness issue.
- **Readability/Robustness of Cleaning:** While `str.replace('GB', '', case=False)` is used and works, explicitly converting to string first (`.astype(str)`) before applying `.str` methods can sometimes prevent errors if a column unexpectedly contains non-string types (e.g., `NaN` or numeric types that weren't detected as objects by pandas). The ground truth was slightly more robust here.

Overall, it's an excellent solution that fulfills the core requirements effectively.",gpt-4.1-nano,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('titanic.csv')

# Display the first 5 rows
print(""First 5 rows of the dataset:"")
print(df.head())

# Show the shape of the dataset
print(""\nShape of the dataset (rows, columns):"")
print(df.shape)

# List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# Calculate the survival rate
survival_rate = df['Survived'].mean() * 100
print(f""\nSurvival rate: {survival_rate:.2f}%"")

# Count of missing v...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong Points:**
- **Correctness:** All tasks are performed correctly.
    - Loading the dataset (`pd.read_csv`).
    - Displaying the first 5 rows (`df.head()`).
    - Showing the shape (`df.shape`).
    - Listing column data types (`df.dtypes` is a valid alternative to `df.info()` for just types).
    - Calculating survival rate accurately using `df['Survived'].mean() * 100`, which is a concise and correct way to get the percentage of 1s in a binary column.
    - Counting missing values (`df.isnull().sum()`).
- **Code Quality:** The code is clean, readable, and uses proper Pandas conventions. Imports are correct.
- **Completeness:** Nearly all requirements are met.

**Areas for Minor Improvement (Why not a 10):**
- **Robustness (File Not Found):** The generated code lacks error handling for `FileNotFoundError` when attempting to load `titanic.csv`. While the prompt specifically asked for ""only the Python code without explanations"" and assumed the file is in the current directory, robust production-ready code would include this. The ground truth demonstrated this, which is a good practice.
- **Output Verbosity for Data Types:** While `df.dtypes` correctly lists column names and their data types, `df.info()` from the ground truth also provides non-null counts and memory usage, which are often very useful in EDA, effectively addressing ""data types"" in a more comprehensive way. However, `df.dtypes` *does* fulfill the literal request for ""column names and their data types.""

Overall, excellent solution. The main missing piece is robust error handling, and `df.info()` would be marginally more informative for data types.",gpt-4.1-nano,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 1) List all column names and their data types
print(""Column names and data types:"")
print(df.dtypes)

# 2) Calculate and report descriptive statistics for numerical columns
numerical_cols = ['Year', 'Rating', 'Runtime']
desc_stats = df[numerical_cols].describe().loc[['mean', '50%', 'std', 'min', 'max']]
desc_stats.rename(index={'50%': 'median'}, inplace=True)
print(""\nDescriptive statistics:"")
print(desc_stats)

# 3)...",6,"The generated code correctly performs the basic tasks of loading the CSV, displaying column dtypes, calculating descriptive statistics, and reporting the total number of movies. However, it lacks robustness and completeness in several critical areas compared to the ground truth.

**Strengths:**
*   Correctly loads the CSV.
*   Correctly uses `df.dtypes` to list column names and types.
*   Correctly calculates descriptive statistics for the specified columns using `.describe()` and `.loc`.
*   Correctly renames '50%' to 'median'.
*   Correctly reports the total number of movies using `len(df)`.

**Weaknesses and Areas for Improvement:**
*   **Error Handling (Missing File):** The generated code does not include any error handling for `FileNotFoundError` if 'imdb_top_250.csv' is not present. The ground truth includes a `try-except` block and graceful exit. This is a significant omission for robust code.
*   **'Runtime' Column Preprocessing:** The most glaring omission is the lack of preprocessing for the 'Runtime' column. In real-world datasets, 'Runtime' is almost always a string (e.g., ""142 min"") and needs to be converted to numeric before statistical calculations. The generated code directly tries to calculate stats on it, which would likely fail or produce incorrect results if 'Runtime' is an object/string type. The ground truth meticulously handles this conversion.
*   **Data Type Handling for Statistics:** While `df.dtypes` is used, the code assumes `Year`, `Rating`, and `Runtime` will always be numeric. It doesn't explicitly check if they *are* numeric before performing statistics, which can lead to errors if the `Runtime` column (or even `Year` or `Rating` due to data quality issues) isn't numeric. The ground truth includes `pd.api.types.is_numeric_dtype` checks.
*   **Clarity of Output:** The ground truth includes more descriptive print statements (e.g., ""--- Column Names and Data Types ---"" vs. just ""Column names and data types:"").
*   **Assumptions vs. Robustness:** The generated code makes stronger assumptions about the data's format (`Runtime` always numeric) and availability (file always exists). The ground truth is designed to be much more robust to variations in the data and environment.",gpt-4.1-nano,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Filter movies with Rating >= 9.0
filtered_df = df[df['Rating'] >= 9.0]

# Select Title and Rating columns
selected_df = filtered_df[['Title', 'Rating']]

# Sort by Rating in descending order
sorted_df = selected_df.sort_values(by='Rating', ascending=False)

print(sorted_df)",9,"The generated code correctly addresses all the requirements of the task: it loads the CSV, filters by rating, selects the specified columns, and sorts in descending order. The imports are proper, and the code structure is clear and readable.

The only minor area for improvement that would bring it closer to the robustness of the ground truth is the lack of error handling (e.g., `FileNotFoundError`, `KeyError`). While the prompt didn't explicitly ask for error handling, it's a best practice in real-world code and was included in the ground truth for comparison. However, given the prompt's focus on the core data manipulation steps, its absence doesn't detract significantly from its correctness for the stated task.",gpt-4.1-nano,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 1) Histogram of the Rating column
plt.figure(figsize=(8, 6))
sns.histplot(df['Rating'], bins=20, kde=True)
plt.title('Distribution of Movie Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# 2) Scatter plot of Rating vs Runtime
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Runtime', y='Rating', data=df)
plt.title('Movie Rating vs Runtim...",7,"The generated code correctly performs the requested plotting tasks: it loads the CSV, creates a histogram for 'Rating', and a scatter plot for 'Rating' vs 'Runtime'. It also includes appropriate titles and axis labels. The code quality is good with proper imports and a clear structure.

However, a significant area for improvement is the lack of robust error handling and data preprocessing. The 'Runtime' column often comes in a string format (e.g., ""120 min"") and needs to be converted to a numeric type before plotting. The generated code assumes 'Runtime' is directly numerical, which would likely cause a `TypeError` or incorrect plot if the column contained non-numeric strings. Additionally, it doesn't handle `FileNotFoundError` for the CSV, which the ground truth gracefully addresses. Adding these data cleaning and error handling steps would make the code much more robust and production-ready.",gpt-4.1-nano,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"import pandas as pd

# Load dataset
df = pd.read_csv('penguins.csv')

# 1) Report missing values per column
missing_values = df.isna().sum()
print(""Missing values per column:\n"", missing_values)

# 2) Filter out rows with missing values in the specified numerical columns
numerical_columns = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_filtered = df.dropna(subset=numerical_columns)

# Calculate and report descriptive statistics
stats = df_filtered[numerical_colum...",8,"The generated code correctly performs all the required tasks: loading the data, reporting missing values, filtering and calculating descriptive statistics for specified numerical columns, and counting unique values in `species` and `island`.

**Strong points:**
- Correctly uses `pandas.read_csv` to load the dataset.
- Accurately reports missing values using `df.isna().sum()`.
- Correctly filters rows based on `NaN` values in the specified numerical columns using `df.dropna(subset=...)`.
- Successfully calculates mean, median (by renaming '50%'), standard deviation, min, and max for the filtered numerical columns.
- Correctly counts unique values for 'species' and 'island' using `nunique()`.
- Provides concise and readable code.

**Areas for improvement:**
- The output for missing values is a raw Series, which might be less user-friendly than the ground truth's approach of filtering to show only columns with missing values and adding descriptive text.
- The descriptive statistics output uses `describe()` which is good, but `agg()` as used in the ground truth often provides more direct control over the specific statistics and their names and can be slightly more efficient if only a few specific statistics are needed. The renaming for median is a minor workaround.
- The ground truth includes error handling (e.g., `try-except` for `FileNotFoundError`) and checks for column existence which makes it more robust for real-world scenarios. While not explicitly asked for in the prompt, robust error handling is good practice.
- The ground truth also explicitly reports the number of rows before and after filtering, which provides useful context about the impact of the `dropna` operation.
- The output text could be more descriptive and formatted similarly to the ground truth for better readability (e.g., clear headings for each section).",gpt-4.1-nano,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter for species 'Adelie' and island 'Torgersen'
filtered_df = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')]

# Select relevant columns
selected_df = filtered_df[['culmen_length_mm', 'culmen_depth_mm']]

# Drop rows with missing values in the selected columns
clean_df = selected_df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm'])

# Display the first 5 rows
print(clean_df.head())",9,"The generated code correctly implements all the requirements of the task: it loads the dataset, filters by species and island, selects the specified columns, drops rows with missing values in those columns, and displays the first 5 rows. The code is well-structured and uses appropriate pandas functions. It uses proper imports.

The only minor area for improvement, which is not strictly a code quality issue against the prompt's requirements but a good practice, is the lack of error handling for file loading (e.g., `FileNotFoundError`), which the ground truth includes. However, the prompt stated ""Assume the CSV file is in the current directory,"" making robust error handling less critical for this specific evaluation, but it's a difference worth noting. Also, the `print(clean_df.head())` implicitly defaults to 5 rows, matching the prompt, but `print(clean_df.head(5))` would be slightly more explicit. However, this is a very minor point. Overall, the code is excellent and fully addresses the prompt.",gpt-4.1-nano,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter out rows with missing values in relevant columns
df_clean = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])

# Create scatter plot
plt.figure(figsize=(8,6))
sns.scatterplot(
    data=df_clean,
    x='culmen_length_mm',
    y='culmen_depth_mm',
    hue='species',
    palette='Set2'
)

plt.title('Penguins: Culmen Length vs Culmen Depth by Spe...",9,"The generated code successfully completes almost all aspects of the task. It correctly loads the CSV, filters out rows with NAs in the specified columns, creates a scatter plot with `culmen_length_mm` on the x-axis and `culmen_depth_mm` on the y-axis, colors the points by `species`, and includes a title and axis labels. The use of `sns.scatterplot` and `df.dropna(subset=...)` demonstrates good practice.

The minor point for improvement is that the ground truth code includes error handling for `FileNotFoundError` and a check if `df_filtered` is empty after dropping NAs. While not strictly required by the prompt's explicit criteria, these are good practices for robust code. However, given the prompt asked for ""only the Python code without explanations"" and the specific requirements, the generated code satisfies them very well. The `palette='Set2'` is a nice touch for aesthetics which wasn't explicitly asked for but doesn't detract.",gpt-4.1-nano,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Fill NA values in numerical columns with their respective means
for column in ['culmen_length_mm', 'culmen_depth_mm']:
    mean_value = df[column].mean()
    df[column].fillna(mean_value, inplace=True)

# Calculate and print mean, median, std for culmen_length_mm
culmen_length_mean = df['culmen_length_mm'].mean()
culmen_length_median = df['culmen_length_mm'].median()
culmen_length_std = df['culmen_length_mm'].s...",8,"The generated code correctly addresses all requirements of the task. It loads the dataset, fills NA values in the specified columns with their respective means, calculates and prints the mean, median, and standard deviation for the cleaned columns, and finally prints the total number of rows and columns.

Strong points:
- Correctly uses pandas for data manipulation.
- Implements the NA imputation correctly using `fillna` and `mean()`.
- Calculates the required statistics accurately.
- Correctly reports dataset dimensions using `df.shape`.

Areas for improvement (minor):
- Error handling for `FileNotFoundError` is missing, which is present in the ground truth. While not explicitly asked for, it's good practice for robust code.
- The printing of statistics could be more generalized using a loop, similar to how NA imputation is handled, instead of repeating code for each column. This improves readability and maintainability, especially if more columns were involved. However, for just two columns, the current approach is acceptable.",gpt-4.1-nano,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Drop rows where 'flipper_length_mm' is NA
df = df.dropna(subset=['flipper_length_mm'])

# 3) Create a histogram of 'flipper_length_mm'
plt.figure(figsize=(8, 6))
sns.histplot(df['flipper_length_mm'], bins=20, kde=False)
plt.title('Distribution of Flipper Length (mm)')
plt.xlabel('Flipper Length (mm)')
plt.ylabel('Frequency')
plt.show()

# 4) Create a b...",8,"The generated code correctly addresses all parts of the task. It successfully loads the data, drops NA values for the specified column, and generates both the histogram for flipper length and the bar chart for island counts with appropriate titles and labels.

**Strong points:**
*   Correctly loads the CSV and uses `read_csv`.
*   Correctly drops NA values using `dropna(subset=[...])`.
*   Uses `sns.histplot` and `sns.countplot` effectively for the visualizations.
*   Applies correct titles and axis labels as requested.
*   Follows the import conventions.

**Areas for improvement to achieve a higher score:**
*   **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. While not explicitly asked for, robust code often includes such error handling, especially when assuming a file is in the current directory.
*   **Detailed Output/Information:** The ground truth provides print statements about the loading status, initial shape, and NA handling details (number of NAs found, rows dropped). This adds clarity and a better user experience, which is valuable in data science scripts.
*   **Plot Customization (Minor):** While the generated code meets the labeling requirements, the ground truth adds more visual enhancements like `kde=True` (for the histogram's distribution shape), `bins=20` (specified in GT, but generated uses it), setting `fontsize` for titles and labels, and adding grid lines. These are minor but contribute to more professional-looking plots often desired in data visualization tasks.
*   **Redundancy in `plt.figure`:** It's good practice to create a figure before each plot, but the generated code explicitly uses `plt.figure(figsize=(8, 6))` for both. While correct, `seaborn` functions often create a figure if one doesn't exist. Setting `figsize` directly as an argument to seaborn plot functions or globally with `sns.set` can be more streamlined in some cases, although the current approach is perfectly fine.",gpt-4.1-nano,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter for penguins from 'Torgersen' island
torgersen_penguins = df[df['island'] == 'Torgersen']

# Count and print the number of penguins for each species in the subset
species_counts = torgersen_penguins['species'].value_counts()
print(species_counts)
```",8,"The generated code correctly solves the core task: it loads the data, filters for 'Torgersen' island, and counts species. It is concise and directly addresses the prompt's requirements.

However, it lacks robustness compared to the ground truth.
- **Error Handling:** It doesn't include `try-except` blocks for `FileNotFoundError` when loading the CSV, which is crucial for making the code more robust in real-world scenarios. It also doesn't handle potential `KeyError` if columns like 'island' or 'species' are missing from the dataset.
- **Edge Cases:** It doesn't explicitly check if `torgersen_penguins` is empty after filtering (i.e., no 'Torgersen' penguins found), which could lead to an empty `species_counts` Series being printed without context, or potential issues if further operations were performed.
- **Clarity/Completeness of Output:** While it prints the `species_counts`, the ground truth adds more descriptive print statements to clarify each step and the final output. The ground truth also explicitly checks for column existence before filtering/counting, which adds to its completeness.

Overall, it's a functional solution but could benefit from more robust error handling and clearer output.",gpt-4.1-nano,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

# Print the shape of the DataFrame
print(df.shape)

# Li...",10,"The generated code is entirely correct and complete. It accurately defines the DataFrame, prints its shape, and lists the data types of each column as requested in the task. The code is clean, well-structured, and directly addresses all requirements without any unnecessary additions or omissions. It correctly imports `pandas` and uses the `pd.DataFrame`, `df.shape`, and `df.dtypes` attributes effectively.",gpt-4.1-nano,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('titanic.csv')

# 1) Bar chart for survivor vs non-survivors
survivor_counts = df['Survived'].value_counts()
plt.figure(figsize=(6,4))
survivor_counts.plot(kind='bar', color=['red', 'green'])
plt.title('Survivor vs Non-Survivors')
plt.xlabel('Survival Status (0 = Did Not Survive, 1 = Survived)')
plt.ylabel('Count')
plt.xticks(ticks=[0,1], labels=['Did Not Survive', 'Survived'], rotation=0)
plt.tight_layout()...",7,"The generated code correctly creates four distinct plots with proper titles and axis labels. It successfully handles the different data types and visualization types requested. However, there are a few areas for improvement:

1.  **Handling Missing Ages:** The ground truth code correctly handles missing 'Age' values by filling them with the median, which is a robust strategy for potentially non-normal distributions and avoids losing data. The generated code uses `dropna()`, which simply discards rows with missing ages. While technically valid for a histogram, it's less ideal as it reduces the sample size and might not represent the full dataset as accurately as imputation. The prompt explicitly asks to ""handle missing values appropriately,"" and imputation is generally considered more appropriate than simply dropping data for this type of visualization.
2.  **Readability/Clarity on X-axis Labels:**
    *   For the 'Survived' plot, the ground truth code uses `map` to convert `0` and `1` to 'Non-survivor' and 'Survivor' directly on the `value_counts` index, making the plot immediately readable without needing an explanation in the x-axis label. The generated code uses hardcoded `xticks` and provides the mapping in the `xlabel`, which is functional but less elegant.
    *   Similarly, for 'Pclass', the ground truth uses `sort_index()` and labels directly '1', '2', '3' using the default axis labels, which is clean. The generated code also uses hardcoded `xticks`.
3.  **Robustness (File Loading):** The ground truth includes error handling for `FileNotFoundError`, which is good practice for scripts that might be run in different environments. The generated code assumes the file will always be present. While not explicitly asked for in the prompt, it significantly improves code robustness.
4.  **Minor Code Style/Completeness (Imports):** The ground truth includes `numpy`, though not directly used in the plotting functions, it's often a common import when dealing with pandas and numerical operations. The generated code is missing it.

Overall, the generated code met most primary requirements for visualization and plot generation, but the approach to handling missing data (dropping vs. imputation) and the clarity of axis labels could be improved to match the robustness and user-friendliness of the ground truth.",gpt-4.1-nano,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]
}

df = pd.DataFrame(data)

# 1) Calculate and print the average 'Midterm_Scor...",9,"The generated code correctly creates the DataFrame, calculates and prints the average midterm and final scores, and correctly identifies the subject with the highest average final score. The code structure is clear and follows the instructions.

Minor points for improvement (not critical enough to reduce the score significantly):
- The `print` statements could include slightly more descriptive text, similar to the ground truth, for better readability of the output (e.g., ""Average Midterm Score for all students:"" instead of just ""Average Midterm Score:"").
- Including string formatting for the numerical output (e.g., `:.2f`) would make the output more consistent and professional for scores.

Overall, the code is very good and fully addresses the prompt's requirements.",gpt-4.1-nano,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame with the provided data
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
    'Ticker': ['TECHCO'] * 6,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
    'Volume': [1000000, 1200000, 1100000, 1300...",10,"The generated code correctly creates the DataFrame as specified, sets 'Date' as the index, calculates the 'Daily_Range' column accurately, identifies the date with the highest 'Close' price using `idxmax()`, and then prints the updated DataFrame as requested. The use of `['TECHCO'] * 6` for the 'Ticker' column is a neat and efficient way to initialize the list, demonstrating good coding practice. The output format for the highest close date also matches the ground truth's intent (`.date()` vs `.strftime('%Y-%m-%d')` both produce the same date string). Overall, the code is correct, complete, and well-structured.",gpt-4.1-nano,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame with the provided data
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics'] * 7,
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST001']
}

df = pd.DataFrame(data)

# 1) Calculate total r...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It correctly creates the DataFrame, calculates the 'Total_Revenue' column, identifies the best-selling product by quantity, and prints the first 5 rows of the DataFrame. The code quality is also good, with proper imports and a clear structure.

The only minor point for improvement, which does not affect correctness or functionality, is that `print(df.head())` by default prints the first 5 rows. Explicitly calling `df.head(5)` as in the ground truth is slightly more explicit, though not strictly necessary here. Additionally, the ground truth includes a descriptive print statement for the head of the DataFrame, which is a nice touch for readability. These are minor stylistic differences and do not detract significantly from the overall quality of the generated code.",gpt-4.1-nano,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 1) Calculate and print the average SessionDurationMinutes
average_session_duration = df[""SessionDurati...",9,"The generated code correctly performs all requested operations: it defines the DataFrame accurately, calculates the average session duration, and counts users by device type. The code is clean, concise, and uses appropriate pandas methods for each task.

The only minor area for improvement is in the output formatting for the average session duration. The ground truth code formats the output to two decimal places and provides a more descriptive label, which can improve readability and clarity in real-world applications. The generated code's print statements are functional but could be slightly enhanced for better user experience. However, this does not affect the correctness or functionality of the code.",gpt-4.1-nano,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

# Find and print the maximum temperature recorded
max_temperature = df[""Tempera...",9,"The generated code correctly defines the DataFrame as specified and accurately finds and prints both the maximum ""TemperatureCelsius"" and minimum ""HumidityPercent"". The code is clean, concise, and uses appropriate pandas methods.

The only minor point of improvement is the print statement format. While `print(""Maximum Temperature (C):"", max_temperature)` is perfectly functional, the ground truth uses an f-string `print(f""Maximum TemperatureCelsius recorded: {max_temperature}C"")`, which is generally considered more modern and flexible for string formatting in Python. This is a very minor aesthetic and readability point and does not affect correctness or functionality.",gpt-4.1-nano,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}

df = pd.DataFrame(data)

# 1) Print the total number of Likes across all posts
total_likes = df['Likes'].sum()
print(""Total Likes:"", total_likes)

# 2) F...",10,"The generated code is correct, complete, and well-structured. It successfully defines the DataFrame, calculates the total number of likes, and accurately identifies the post type with the highest average number of shares. The variable names are clear, and the comments effectively describe each step, matching the requirements of the prompt perfectly. Output format slightly differs from ground truth but the core logic and results are identical.",gpt-4.1-nano,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",10,"The generated code is entirely correct and complete. It successfully defines the DataFrame, calculates the average age, and counts patients by gender, adhering to all specified requirements. The code is clean, well-structured, and directly addresses the problem statement. The use of f-strings for printing the average age and the direct printing of value_counts for gender counts are both standard and effective.",gpt-4.1-nano,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}

df = pd.DataFrame(data)

# 1) Print the Title of the movie with the highest Rating
highest_rating_movie_title = df....",9,"The generated code correctly defines the DataFrame and implements both required tasks. It accurately identifies the title of the highest-rated movie using `idxmax()` which is an efficient and clean Pandas method. It also correctly calculates and prints the average year. The code is concise and demonstrates good understanding of basic Pandas operations.

Areas for minor improvement:
1. **Output Clarity:** The output for the printed values could be more descriptive (e.g., ""The movie with the highest rating is: [Title]""). The ground truth code includes this, which improves user understanding.
2. **Robustness (minor):** While `idxmax()` is good, if there were multiple movies with the exact same highest rating, `idxmax()` would only return the index of the *first* occurrence. The ground truth handles this scenario more explicitly by filtering, though for this specific task, returning one title is sufficient.
3. **Commenting:** While not strictly necessary for such a short and clear code, adding a few comments explaining the logic for each task (similar to the ground truth) would enhance readability for others.

Overall, it's a very good solution that fulfills all task requirements correctly and efficiently.",gpt-4.1-nano,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

# Print the total number of cars
print(""Total ...",10,"The generated code is excellent. It correctly defines the pandas DataFrame as specified, accurately calculates and prints the total number of cars using `len(df)`, and correctly calculates and prints the average ""PriceUSD"" using `df[""PriceUSD""].mean()`. All requirements of the task are met with clear and concise Python code. The code quality is high, with proper imports and a logical structure.",gpt-4.1-nano,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}
df = pd.DataFrame(data)

# Find the Pokmon with the highest HP
max_hp_pokemon = ...",9,"The generated code correctly performs all requested operations: it accurately defines the DataFrame, identifies the Pokemon with the highest HP, and identifies the Pokemon with the lowest Defense. The use of `idxmax()` and `idxmin()` followed by direct indexing (`df.loc[...]['Name']`) is concise and efficient. The code is well-structured and uses appropriate pandas functionalities.

Minor points for improvement (not critical enough to reduce the score significantly):
- The print statements could be more descriptive, similar to the ground truth (e.g., ""The Pokemon with the highest HP is: [name]""). This improves user readability.
- Variable names like `max_hp_pokemon` and `min_defense_pokemon` are good, but explicitly stating that it's the ""name"" found (e.g., `highest_hp_pokemon_name`) could add a tiny bit more clarity, though current names are understandable in context.

Overall, it's a very solid and correct solution.",gpt-4.1-nano,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('titanic.csv')

# Identify columns with missing values
missing_cols = df.columns[df.isnull().any()]
print(""Columns with missing values:"", missing_cols.tolist())

# Fill missing Age values with median age
df['Age'].fillna(df['Age'].median(), inplace=True)

# Fill missing Embarked values with the most frequent port
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# Drop the Cabin column due to excessive missing values
df.drop('...",7,"The generated code correctly addresses most of the requirements. It loads the data, handles missing 'Age' and 'Embarked' values appropriately, and drops the 'Cabin' column. The 'Title' extraction is also largely correct, although it captures slightly different titles than the ground truth (e.g., 'Mr', 'Mrs' vs 'Mr.', 'Mrs.').

Areas for improvement:
1.  **Title Extraction Standardization:** The ground truth standardizes common titles (e.g., 'Mlle' to 'Miss', 'Ms' to 'Miss', 'Mme' to 'Mrs') and groups rare titles. While not explicitly asked for, this is a very common and beneficial step in this kind of feature engineering for machine learning. The generated code's title extraction is more literal and doesn't include this standardization, leading to more unique title categories.
2.  **Robustness of Title Extraction:** The regex `r',\s*([^\.]+)\.'` will extract ""Mr"" from ""Mr. John Doe"" but return ""None"" if there's no comma, like ""Master. Smith"" (though this is less common in real datasets). The ground truth's ` ([A-Za-z]+)\.` is more robust in this specific context as it looks for the pattern directly after a space and before a period.
3.  **Missing Value Identification Output:** The generated code prints `missing_cols.tolist()`, which isn't explicitly requested as a display item in the task (the task asks to ""identify columns with missing values"" but doesn't specify printing them, only displaying the ""cleaned dataset's shape and the first 5 rows""). While identifying is good, printing it was not part of the final display requirements.
4.  **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError`. While not explicitly required by the prompt, it's good practice for robust code.

Overall, it's a good attempt, but the title extraction could be more refined and standardized to match typical data cleaning practices for this dataset.",gpt-4.1-nano,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data and create the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}...",9,"The generated code is highly correct and effectively addresses all given requirements. It accurately defines the DataFrame, correctly identifies the title of the book with the highest price using `idxmax()` and `loc`, and precisely calculates the total stock using `sum()`. The code structure is clear and concise, with appropriate imports.

The only minor area for improvement, preventing a perfect 10, is the output clarity. While the ground truth explicitly labels the printed outputs with descriptive strings (e.g., ""The title of the book with the highest price is:""), the generated code simply prints the raw values. For a real-world scenario or a more user-friendly output, including descriptive text would be beneficial. However, in terms of pure functionality and correctness for the task, it performs perfectly.",gpt-4.1-nano,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Display the first 10 rows
print(""First 10 rows of the dataset:"")
print(df.head(10))
print(""\n"")

# Show the shape of the dataset
rows, cols = df.shape
print(f""Dataset shape: {rows} rows, {cols} columns\n"")

# List all column names and their data types
print(""Column names and data types:"")
print(df.dtypes)
print(""\n"")

# Calculate the percentage of potable vs non-potable water samples
potability_counts = df['Potab...",8,"The generated code correctly performs all the requested tasks. It loads the dataset, displays the first 10 rows, shows the shape, lists column names and data types, calculates potability percentages, displays missing values, and shows summary statistics.

**Strong points:**
*   All required imports are present (`pandas`).
*   Each task is addressed, and the output is generally clear.
*   The `df.head(10)`, `df.shape`, `df.dtypes`, `df['Potability'].value_counts() / len(df) * 100`, `df.isnull().sum()`, and `df.describe()` methods are used appropriately for the tasks.

**Areas for improvement (to achieve a higher score):**
*   **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and other exceptions during file loading, which is crucial for real-world applications. The generated code lacks this.
*   **Clarity of Output:** While the output is functional, the ground truth uses more descriptive print statements and separators (e.g., `--- 1. First 10 rows of the dataset ---`, `=""*50""`) which makes the output much easier to read and understand, especially when multiple sections are printed to the console.
*   **`df.info()` vs `df.dtypes`**: While `df.dtypes` fulfills the literal request to list data types, `df.info()` (used in the ground truth) provides much more comprehensive information, including non-null counts and memory usage, which is often preferred in EDA.
*   **Handling of `Potability` column absence:** The ground truth checks if the 'Potability' column exists before trying to calculate percentages, making it more robust. The generated code assumes its presence.
*   **Displaying only columns with missing values:** The ground truth filters `df.isnull().sum()` to only show columns that *have* missing values, which can be cleaner for datasets with very few missing values. It also explicitly states ""No missing values found"" if none exist.",gpt-4.1-nano,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('water_potability.csv')

# 1) Identify columns with missing values
missing_values = df.isnull().sum()
columns_with_missing = missing_values[missing_values > 0].index.tolist()

# 2) Fill missing values with mean for numerical columns
for col in columns_with_missing:
    if pd.api.types.is_numeric_dtype(df[col]):
        df[col].fillna(df[col].mean(), inplace=True)

# 3) Remove duplicate rows if any
initial_shape = df.shape
df.drop_duplicate...",7,"The generated code addresses most requirements but has a few shortcomings:

**Positives:**
*   Correctly identifies and fills missing numerical values with the mean.
*   Correctly identifies and removes duplicate rows.
*   Attempts to verify data ranges using a dictionary and loop, which is a good approach.
*   Provides a summary of before and after cleaning `df.shape`.
*   Displays the first 5 rows of the cleaned dataset.
*   Uses proper imports.

**Areas for Improvement:**
*   **Error Handling (File Not Found):** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is crucial for robust code. The generated code lacks this.
*   **Summary Detail:** The ""summary showing before and after statistics"" is less comprehensive than the ground truth.
    *   The ""Before Cleaning"" output correctly shows all missing values, but the summary output `initial_missing` still shows columns that *initially* had missing values, not just those > 0.
    *   It doesn't explicitly state the number of rows removed due to range checks, which is a key part of the cleaning process and impact. The `rows_removed` variable only accounts for duplicates. The range check is performed *after* duplicates, so the change in `df.shape` after range checking should be explicitly reported.
    *   The ""Duplicate rows removed"" is printed twice in the summary, both ""Before Cleaning"" and ""After Cleaning"", which is redundant and misleading.
*   **Data Range Check Logic:** While the intention is good, the range checking `df = df[df[param] >= min_val]` lines are performed sequentially and reassign `df` in each iteration. This is functionally correct but less efficient than applying all range filters in one go (e.g., `df = df[(df['ph'] >= 0) & (df['ph'] <= 14) & (df['Hardness'] >= 0) & ...]`) or accumulating conditions. More importantly, it doesn't report how many rows were removed *by this specific step*.
*   **Modularity/Readability:** While not strictly required by the prompt, the ground truth breaks down the steps with comments and prints messages at each stage of cleaning (e.g., ""Filling missing values:"", ""Removed X duplicate rows"", ""Removed X rows due to out-of-reasonable-range""), making the cleaning process clearer and easier to follow, especially for debugging or understanding the impact of each step. The generated code is more monolithic.
*   **`initial_duplicate_count = 0`:** This line is incorrect; `initial_duplicate_count` should represent the actual number of duplicates *before* removal if it's to be compared. However, the calculation `rows_removed = initial_shape[0] - df.shape[0]` correctly captures the duplicates removed, so this variable itself is redundant.",gpt-4.1-nano,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('water_potability.csv')

# 1) Bar chart for potable vs non-potable water samples
plt.figure(figsize=(6,4))
sns.countplot(x='Potability', data=df)
plt.title('Distribution of Potable vs Non-Potable Water Samples')
plt.xlabel('Potability (0 = Non-Potable, 1 = Potable)')
plt.ylabel('Count')
plt.xticks([0,1], ['Non-Potable', 'Potable'])
plt.tight_layout()
plt.show()

# 2) Histograms for pH, ...",8,"The generated code successfully addresses all the core requirements of the task. It creates four distinct visualizations (bar chart, histograms, boxplot, heatmap) with proper titles and labels. The use of `matplotlib.pyplot` and `seaborn` is correct, and the `tight_layout()` function is consistently used for better plot presentation.

**Strong points:**
*   **Correct Visualizations:** All four requested plots are generated correctly.
*   **Proper Labeling:** Titles and axis labels are present and relevant for each plot.
*   **Imports:** All necessary libraries are imported.
*   **Handling Missing Values (Implicitly):** Both `sns.histplot` and `sns.boxplot` inherently handle NaNs by ignoring them in calculations, which is acceptable for this task. `df.corr()` also handles NaNs appropriately for heatmap generation.

**Areas for improvement to achieve a higher score:**
*   **Error Handling for File:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.
*   **Explicit Missing Value Handling for Histograms:** While `sns.histplot` does ignore NaNs, explicitly using `.dropna()` as in the ground truth for `df['ph'].dropna()` makes the intent clearer and can sometimes be beneficial for consistency if other operations required explicit handling.
*   **Consistency in Titles/Labels:** Some titles and labels could be a bit more descriptive (e.g., ""Distribution of Potable vs Non-Potable Water Samples"" vs. ""Distribution of Water Potability"").
*   **Plot Styling/Customization:** The ground truth used `sns.set_style(""whitegrid"")` and custom `palette` or `color` arguments for plots, making them more visually appealing and consistent. The generated code uses default seaborn styles and colors. Adding a consistent style could improve the overall presentation.
*   **Gridlines:** Adding gridlines (`plt.grid(axis='y', linestyle='--', alpha=0.7)`) as seen in the ground truth can often improve readability for bar charts and boxplots.
*   **Subplot Organization for Histograms:** The ground truth explicitly uses `fig, axes = plt.subplots(1, 3, ...)` which provides `axes` objects for more precise control over individual subplots, including setting titles and labels via `axes[i].set_title()`. The generated code uses `plt.subplot(1, 3, i+1)` and then `plt.title()`, which works but the `axes` object approach is often preferred for more complex subplot layouts.",gpt-4.1-nano,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.preprocessing import PolynomialFeatures

# Load dataset
df = pd.read_csv('winequality-red.csv')

# 1) Create new meaningful features
# a) Alcohol to residual sugar ratio
df['alcohol_sugar_ratio'] = df['alcohol'] / (df['residual sugar'] + 1e-5)
# b) Total phenols an...",7,"The generated code correctly addresses most aspects of the task, demonstrating good understanding of feature engineering and selection.

**Strong Points:**
*   **Feature Engineering:** Successfully creates more than three new features, including a polynomial feature, ratio, and interaction, which are meaningful.
*   **Multiple Feature Selection:** Correctly implements both RFE with RandomForest and SelectKBest with f\_regression.
*   **Comparison:** Identifies and prints common features between the two methods.
*   **Imports:** Includes all necessary imports.

**Areas for Improvement (Why it lost points):**
*   **Feature Engineering Quality:** The generated code includes `df['phenols_flavonoids'] = df['total phenols'] * df['flavonoids']`, but the `winequality-red.csv` dataset does not contain 'total phenols' or 'flavonoids' features. This will cause a `KeyError` at runtime, indicating a lack of thorough testing or dataset familiarity. This is a critical error as it prevents the code from running successfully.
*   **Display of Importance/Rankings:** While it displays selected features, for RFE, it only lists the features and doesn't explicitly display their importance scores or rankings derived from the underlying model, which the prompt specifically asked for (""importance scores or rankings""). For SelectKBest, it prints the score, but it's not well formatted and the top 7 features are extracted again instead of directly using `kbest_features` and then retrieving their scores.
*   **Clarity in Comparison:** The comparison section is quite basic. The ground truth provides a more in-depth comparison, explaining *why* differences might occur (e.g., filter vs. wrapper methods, univariate vs. multivariate analysis), which adds significant value to the response. The generated code mostly just lists the common features.
*   **Train/Test Split:** While not strictly harmful, `train_test_split` was used for the feature selection, which isn't necessary for the task of *identifying* important features and can introduce slight variability if not handled carefully (though `random_state` helps). The prompt focused on feature engineering and selection, not a full ML pipeline. The ground truth correctly notes this is good practice for a full pipeline but not strictly needed here.
*   **Error Handling/Robustness:** The code assumes the CSV file is present and directly accesses columns without defensive checks (like the `try-except` for file loading in the ground truth or handling potential `NaN`/`inf` values post-engineering, though the latter is less critical if the calculations are safe). The `KeyError` for non-existent columns is a major example of this.
*   **Redundancy in SelectKBest:** There's a slight redundancy in the SelectKBest part: `kbest_features` already contains the names of the top K features, but then `kbest_top_features` is created by re-sorting the `kbest_scores_dict`. It's better to directly use `kbest_indices` or `kbest.get_support()` to retrieve the features and their corresponding scores.

**Overall:** The code is a good attempt, but the `KeyError` due to non-existent columns and the lack of proper importance display for RFE and less detailed comparison prevent a higher score. Fixing the column name issue is crucial for executability.",gpt-4.1-nano,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_validate
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor, StackingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import L...",7,"The generated code provides a decent attempt at solving the problem, implementing most of the required components. However, there are several areas where it deviates significantly from the ground truth or the requirements.

**Strengths:**
- Implements multiple ensemble methods (Random Forest, Gradient Boosting, XGBoost, AdaBoost).
- Performs cross-validation for individual models and the voting ensemble.
- Implements voting and stacking ensembles.
- Conducts hyperparameter tuning using GridSearchCV.
- Creates feature importance plots.
- Evaluates models using MAE, MSE, RMSE, and R score.
- Implements residual plots.
- Handles missing values.

**Weaknesses and Areas for Improvement:**

1.  **Preprocessing (Critical Deviation):**
    *   The generated code uses `LabelEncoder` for categorical variables. The ground truth, and typically best practice for regression, uses `OneHotEncoder`. `LabelEncoder` imposes an arbitrary ordinal relationship which is not suitable for tree-based models unless specifically designed for it, and it can mislead the model.
    *   The generated code first imputes numerical and categorical columns globally, then applies `StandardScaler`. This sequence can lead to data leakage if not careful. The ground truth correctly uses `ColumnTransformer` and `Pipeline` to encapsulate preprocessing within the cross-validation folds, preventing data leakage and ensuring correct scaling/encoding based only on training data.
    *   The initial missing value handling is done on the entire DataFrame before splitting, which is a minor form of data leakage. While `SimpleImputer` is used, it applies `fit_transform` globally. The ground truth correctly integrates `SimpleImputer` within pipelines to ensure it only ""learns"" from the training folds.

2.  **Cross-Validation Implementation:**
    *   The generated code correctly uses `cross_val_score` for initial individual model evaluation but doesn't consistently apply a proper cross-validation loop for subsequent evaluations (e.g., voting ensemble, stacking ensemble, tuned model evaluation). The ground truth uses a `KFold` object explicitly and performs the train/test split within each fold, then evaluates, which is a more robust way to ensure all steps (preprocessing, model fitting, prediction) are done strictly within the CV folds.
    *   For the Voting and Stacking ensembles, the generated code fits them on the `X_train`, `y_train` from the initial `train_test_split`, and then evaluates on `X_test`, `y_test`. This is a single train-test split evaluation, not a cross-validation of the ensemble.

3.  **Hyperparameter Tuning:**
    *   The prompt asks for tuning the *best* individual model. The generated code hardcodes tuning for *RandomForest* regardless of which model is best. The ground truth correctly identifies the best model and then performs tuning on it.

4.  **Feature Importance Plots:**
    *   While correctly implemented for individual models, the `feature_names` passed to `plot_feature_importances` are the original `X.columns`. After `LabelEncoder` and potential scaling, these original names might not accurately reflect the processed features for which importances are calculated, especially with `LabelEncoder` where one column becomes one feature. With OneHotEncoding (as in GT), new columns/features are created, and `get_feature_names_out` correctly provides them. This makes the generated feature importance plots potentially misleading.
    *   The ground truth also correctly notes that feature importances are not directly available for Voting Ensemble or Stacking Ensemble as a whole.

5.  **Code Structure and Readability:**
    *   The ground truth uses `Pipeline` and `ColumnTransformer` effectively to create a robust and reproducible machine learning workflow. The generated code's more manual approach to preprocessing makes it less modular and more prone to errors like data leakage.
    *   The `print` statements in the ground truth clearly indicate which task is being addressed, which improves readability and navigability.

6.  **Completeness:**
    *   Requirement 1: ""Load and preprocess the dataset, handling missing values and encoding categorical variables"". Encoding is done, but not optimally (`LabelEncoder` vs `OneHotEncoder`).
    *   Requirement 5: ""Implement a voting ensemble that combines all four models"". Implemented, but not cross-validated.
    *   Requirement 6: ""Perform hyperparameter tuning using GridSearchCV for the best individual model"". Performed, but for a hardcoded model, not the identified best.
    *   Requirement 9: ""Implement stacking ensemble with a meta-learner (Linear Regression)"". Implemented, but not cross-validated.

To achieve a higher score, the generated code needs to adopt a more robust preprocessing strategy using `Pipeline` and `ColumnTransformer` with `OneHotEncoder`, consistently apply cross-validation to *all* models (including ensembles and tuned models), and correctly identify the best model for hyperparameter tuning.",gpt-4.1-nano,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller

from sklearn.linear_model import Ridge, Lasso, LinearRegression
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.model_selection import TimeSeriesSplit, cross_val_score

import pymc3 as pm
import theano
import the...",4,"The generated code attempts to address many requirements but falls short significantly in correctness, completeness, and robustness for several key areas.

**Correctness & Completeness Issues:**

1.  **Data Loading & Aggregation:** The generated code assumes 'Year' is directly available in `df` and aggregates sales by it. The ground truth correctly identifies `Year_of_Release` and handles potential NaNs, then sets a proper datetime index for time series analysis. The generated code also references `df['Discontinuation_Date']` and `df['Release_Date']` for survival analysis, which are not present in the given `video_games_sales.csv` dataset, nor are `Game` or `Profit` columns. This shows a misunderstanding of the actual dataset provided in the task description.
2.  **ARIMA Modeling:**
    *   It uses `statsmodels.tsa.arima.model.ARIMA` which requires explicit `p,d,q` orders. The ground truth uses `pmdarima.auto_arima` for automatic order selection, which is standard practice and more robust.
    *   The `period=1` for `seasonal_decompose` on yearly data is meaningless for typical seasonality and essentially only decomposes into trend and residual, which the ground truth also notes, but it still makes an attempt at a period that might reveal longer cycles. The overall approach for ARIMA is rudimentary compared to the ground truth.
    *   Confidence/prediction intervals for ARIMA are not explicitly extracted or presented, unlike the ground truth.
3.  **Multivariate Regression:**
    *   The feature `Year` from `df_reg[['Year']]` is likely incorrect as the column is `Year_of_Release`.
    *   The interaction term logic `PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)` applied to `X` (which includes *all* dummy variables) will create a huge number of interaction terms, leading to potential multicollinearity issues and rendering the model uninterpretable and unstable, especially without reducing the number of categories as done in the ground truth. The ground truth carefully selects top categories and only adds interaction terms for them, making it more practical.
    *   No confidence or prediction intervals are shown/calculated for the multivariate model from sklearn, while the ground truth correctly uses `statsmodels.api` which provides these directly.
4.  **Polynomial Regression with Regularization:**
    *   Calls `cross_val_score` with `cv=5` which performs standard k-fold cross-validation, not time series cross-validation. The problem explicitly asks for *time series* cross-validation for *all* models. The ground truth correctly uses `TimeSeriesSplit` for this.
    *   The `X_poly` used here for Ridge/Lasso includes all features (Year, Genre, Platform, Publisher), not just polynomial features of `Year_of_Release` as a base for polynomial regression as done in the ground truth for simplicity and common practice. This confuses polynomial regression with general multivariate regression + interactions.
    *   Confidence/prediction intervals are completely missing for these models.
5.  **Bayesian Regression:**
    *   The model assumes `df_reg['Year']` directly, which is incorrect.
    *   It only includes `Year` as a predictor, completely neglecting categorical features and their inclusion as varying effects (e.g., `beta_genre[X_bayesian[:, 1].astype(int)]`) as demonstrated in the ground truth. This is a very simplified model and doesn't fully represent Bayesian regression with PyMC3 on the given dataset.
    *   It omits the crucial data scaling that helps PyMC3 sampling.
    *   No clear prediction intervals/credible intervals are extracted post-sampling for comparison with other models.
6.  **Change Point Detection:**
    *   It assumes `sales_ts.index[cp]` which errors because `cp` is an *index* in `sales_ts.values`, not the original DataFrame index. The plotting logic for change points is incorrect.
    *   The choice of penalty `pen=10` is arbitrary; the ground truth provides a more principled approach (e.g., `2 * np.log(n_samples)` for BIC-like behavior).
7.  **Survival Analysis:**
    *   This is the most critical failure point: it invents `Release_Date`, `Discontinuation_Date`, and `Profit` columns, which do not exist in the `video_games_sales.csv` dataset. The ground truth correctly defines 'game longevity' based on available `Year_of_Release` data to estimate a 'survival' period relevant to the data, even if simplified. This invention of columns makes the survival analysis section completely non-functional and based on false assumptions.
    *   It only implements Kaplan-Meier and lacks the Cox Proportional Hazards model which was explicitly requested, and which the ground truth provides.
8.  **Confidence/Prediction Intervals:** The code attempts to calculate PIs for the OLS model using `statsmodels.sandbox.regression.predstd.wls_prediction_std` but only for a *single* data point (mean of features), not systematically for predictions across the dataset or for all models as requested. Intervals for other models are absent or incorrectly implemented.
9.  **Time Series Cross-Validation:**
    *   The `cross_val_score` for Ridge/Lasso with `cv=tscv` is correct for those specific models.
    *   However, it does not implement time series cross-validation for ARIMA, which is crucial given ARIMA is a time series model. The ground truth implements a proper rolling forecast cross-validation for ARIMA.
    *   It only does TS-CV for polynomial regression with regularization, not for the multivariate regression model, whereas the ground truth attempts it for both.

**Code Quality Issues:**

*   **Imports:** Several imports are missing (e.g., `pmdarima`, `arviz`, `statsmodels.formula.api`, `warnings`, `lifelines.CoxPHFitter`). It also imports `theano` and `theano.tensor` which are internal dependencies of pymc3 and not usually explicitly imported by the user.
*   **Structure:** Code comments are broad headings, not specific details, and some sections are very sparse.
*   **Error Handling:** Lacks robust error handling, especially for file not found, or model convergence issues, which the ground truth includes using `try-except` blocks.
*   **Reproducibility:** No random seed setting, unlike the ground truth.
*   **Plotting:** Plots are `plt.show()` only, not `plt.savefig()`, meaning they are not saved unless explicitly told to, which is poor for programmatic execution.

Overall, the generated code demonstrates a general understanding of the statistical models but fails to correctly apply them to the given dataset or fully meet the specific requirements (e.g., interaction terms, Bayesian model scope, survival analysis data definitions, comprehensive CI/PIs, proper time series cross-validation for all models) for an ""expert code evaluator"" benchmark.",gpt-4.1-nano,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest
from mlxtend.frequent_patte...",6,"The generated code attempts to address most of the requirements but has significant issues in correctness, completeness, and robustness compared to the ground truth.

**Strengths:**
- Attempts to implement multiple clustering algorithms (K-Means, DBSCAN, Hierarchical, GMM).
- Includes dimensionality reduction (t-SNE, UMAP) for visualization.
- Tries to determine optimal clusters using Silhouette and Elbow methods, and a basic Gap Statistic.
- Includes anomaly detection (Isolation Forest) and association rule mining.
- Provides a basic recommendation system based on clustering.
- Uses some internal validation metrics (Silhouette, Davies-Bouldin).

**Areas for Improvement and Specific Criticisms:**

1.  **Preprocessing:**
    *   **Incomplete/Incorrect Handling of Missing Values:** `df.fillna(0, inplace=True)` for numerical sales columns is acceptable for some cases, but for `Year_of_Release`, `Name`, `Genre`, `Platform`, `Publisher`, `Rating` (which are categorical or identifiers) it's problematic. For instance, filling `Year_of_Release` with 0 is incorrect, and dropping NaNs on crucial categorical columns (as in GT) is better.
    *   **Categorical Encoding:** Uses `LabelEncoder` for 'Genre', 'Publisher', 'Platform', 'Rating'. This assigns arbitrary numerical relationships (e.g., Genre 'Action' = 0, 'Sports' = 1), which is inappropriate for clustering algorithms that assume continuous numerical data or proper one-hot encoding for nominal categories. The Ground Truth correctly uses `OneHotEncoder`.
    *   **Feature Creation `Regional_Sales_Distribution`:** Divides by `Total_Sales` which is `df[region_cols].sum(axis=1)`. This `Total_Sales` isn't necessarily `Global_Sales`. In cases where `Global_Sales` might be 0, the Ground Truth correctly handles division by zero using `df.apply` or conditional checks.
    *   **`Market_Position` Normalization:** Normalizing `Genre_Rank` to `[0,1]` is unnecessary and potentially misleading, as `rank` already gives a clear position.
    *   **Feature Selection for `X`:** The generated code only includes `region_prop_cols`, `Market_Position`, `Publisher_Portfolio_Size`. It completely omits other potentially useful numerical features like `Global_Sales`, `Year_of_Release`. It also doesn't incorporate the encoded categorical features (like genre, platform) into `X`, effectively losing a lot of information for clustering. The Ground Truth uses `ColumnTransformer` and includes both numerical and one-hot encoded categorical features.
    *   **`Rating` Column:** The Ground Truth correctly assumes the `Rating` column is likely missing / not consistently present in `video_games_sales.csv` and wisely omits it from processing if not explicitly stated. The generated code includes it in `categorical_cols` without checking for its existence or handling its potential absence robustly.

2.  **Optimal Cluster Determination:**
    *   **Lack of Robustness for Gap Statistic:** The provided `gap_statistic` implementation is a basic attempt but deviates significantly from a full, robust gap statistic calculation (e.g., it only returns `gaps` and doesn't clearly indicate optimal `k` from `log(W_k)` and `s_k` as per the original Tibshirani paper). Furthermore, the usage `np.argmax(gaps) + 2` is a heuristic and might not be accurate. The Ground Truth uses `KneeLocator` for elbow and relies on silhouette for guidance.
    *   **No Gap Statistic Plotting:** The generated code computes `gaps` but doesn't visualize them.

3.  **Clustering Algorithms:**
    *   **DBSCAN Parameter Tuning:** `DBSCAN(eps=0.5, min_samples=5)` is hardcoded. DBSCAN is highly sensitive to `eps` and `min_samples`, and these values are unlikely to be optimal or even reasonable without data-driven selection (like a k-distance plot or statistical methods). The Ground Truth attempts a heuristic for `eps` using nearest neighbors and plots the k-distance graph, which is a better approach.
    *   **Consensus Clustering:**
        *   The `consensus_matrix` function has a major bug: `consensus /= n_methods` is applied `n_methods` times (once per `labels` in `cluster_labels_list`), leading to an incorrect consensus matrix. It should be applied once after all label comparisons have been summed up.
        *   It includes `labels_dbscan` in consensus clustering, but DBSCAN can produce noise points (-1) and a variable number of clusters, which complicates direct inclusion in a fixed-k consensus. The GT handles this by treating noise as a separate cluster or excluding them.
        *   The use of `scipy.spatial.distance.squareform` and `linkage` directly on `diss` (1 - consensus) is correct for converting to a linkage matrix, but the final `fcluster` with `t=0.5` and `criterion='distance'` (which needs `maxclust` often) without determining `optimal_k_gap` for it, it leads to issues. The GT re-applies `AgglomerativeClustering` with the `chosen_optimal_k` on the derived distance matrix, which is more straightforward.

4.  **Cluster Profiling and Statistical Significance:**
    *   **`ttest_ind` misuse:** `ttest_ind(groups[0], groups[1], equal_var=False)` is used to determine ""significant features"". This only compares *two* groups. For multiple clusters, ANOVA (for numerical features) and Chi-squared tests (for categorical features), as used in the Ground Truth, are appropriate.
    *   **Profiling categorical features:** The generated code doesn't profile categorical features (e.g., top genres within a cluster), which is crucial for understanding segments.

5.  **Association Rule Mining:**
    *   **Discretization:** The discretization of continuous variables (`Global_Sales`, `features`) into categories is a good idea for ARM, but the implementation is basic and doesn't consider how many bins or the distribution carefully.
    *   **`filter(regex='^(.'join(attributes)+')')`:** This regex will likely fail or cause unexpected behavior. It should be `filter(regex='^(Genre|Publisher|Platform|Rating|Global_Sales_Category)_')` to correctly select OHE columns.
    *   **Missing important features:** The generated code only includes an arbitrary `attributes` list for ARM, ignoring the newly engineered features like `Market_Position` or `Publisher_Portfolio_Size`.

6.  **Recommendations:**
    *   The `target_game_idx = 0` and subsequent lookup is very fragile. It doesn't allow a user to specify a game by name, which is more practical. The GT implements a function that takes a game name.

7.  **Completeness & Robustness:**
    *   **Missing Gap Statistic Implementation:** The Gap Statistic plotting and full interpretation are largely missing.
    *   **Error Handling:** The Ground Truth includes `try-except FileNotFoundError` and robust handling for DBSCAN failures. The generated code lacks this.
    *   **Imports:** Several necessary imports like `warnings`, `scipy.stats.f_oneway`, `scipy.stats.chi2_contingency`, `kneed.KneeLocator`, `sklearn.compose.ColumnTransformer`, `sklearn.pipeline.Pipeline`, `sklearn.neighbors.NearestNeighbors` (for DBSCAN heuristic) are missing.
    *   **Visualization:** While t-SNE/UMAP plots are mentioned, the actual code only shows K-Means, missing the visualization for other algorithms. Legends for plots are also not clearly defined.

Essentially, while the generated code tries to touch upon all points, the implementation quality, especially in preprocessing, optimal k determination, statistical testing, and consensus clustering, significantly lags behind the ground truth. Many critical steps are either implemented incorrectly, partially, or are missing robust error handling.",gpt-4.1-nano,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV, cross_validate
from sklearn.preprocessing import StandardScaler, PowerTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

from sklearn.ensemble import VotingClassifier, BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import Decision...",6,"The generated code attempts to address most of the requirements but has significant issues in correctness and completeness, particularly in the implementation of nested CV, Bayesian optimization, and handling of custom ensembles.

**Correctness Issues:**
1.  **Nested Cross-Validation Implementation:** The `nested_cv_evaluation` function is fundamentally flawed.
    *   It performs a `GridSearchCV` *inside* the outer loop for each fold. This means the hyperparameters are re-optimized for *each outer fold*, which is not the standard nested CV approach for evaluating a *fixed* model's performance. Nested CV is typically structured to perform hyperparameter tuning in the *inner* loop on the *training data of the outer fold*, and then evaluate the *best model from the inner loop* on the *test data of the outer fold*. The current implementation effectively re-tunes and evaluates, making it more akin to repeated-CV with hyperparameter search than a proper nested CV for unbiased evaluation.
    *   The `get_param_grid` function is problematic as it requires an *instance* of a model, while `GridSearchCV` expects a class or a pipeline. The way it's used in `nested_cv_evaluation` (passing `model`) will cause issues because `model` is already an instantiated estimator, not a class.
    *   When `objective_xgb` calls `nested_cv_evaluation`, it does not pass the actual data splits to the inner CV, which is crucial for hyperparameter tuning. It essentially trains and evaluates on the entire dataset `X`, `y` supplied, making the `nested_cv_evaluation` redundant for Optuna's purpose and incorrect for its own stated goal.
2.  **Bayesian Optimization (Optuna):**
    *   The `objective_xgb` function directly uses `nested_cv_evaluation(model, X, y)`. This means that Optuna is trying to optimize hyperparameters by running a (flawed) nested CV on the *entire dataset* for *each trial*. This is extremely inefficient and incorrect. Bayesian optimization in nested CV should be part of the *inner loop* of the nested CV, or performed on a separate validation set. The current setup means the Optuna tuning itself is evaluated with nested CV, not that nested CV is used to evaluate the *results* of the tuning process.
    *   `LightGBM` and `CatBoost` are listed in `models` for nested CV evaluation but are then initialized using `**study.best_params`, implying they share the same tuned parameters as XGBoost, which is incorrect. They should have their own Optuna tuning process or use default/predefined parameters.
3.  **Model Calibration:** The line `calibrated_clf = CalibratedClassifierCV(stacking_clf.fitted_estimators[0], method='isotonic', cv='prefit')` tries to apply `cv='prefit'` which assumes the base estimator is already fitted. However, `stacking_clf.fitted_estimators` is a list of fitted base estimators *within the stacking ensemble*, not the StackingClassifier itself or a single base model that has been trained explicitly for calibration context. It's unclear which model `stacking_clf.fitted_estimators[0]` refers to (it would be `lr`, not the full stack). The calibration should be performed on the final stacking model's predictions. The provided solution also makes assumptions about the model's fitted state without explicitly re-fitting for the calibration step.
4.  **SHAP:** The SHAP explanation uses `stacking_clf.fitted_estimators[0]`, which is the first base estimator (Logistic Regression) of the StackingClassifier, not the final stacked model or any powerful tree-based model for which `TreeExplainer` would be appropriate.
5.  **Data Preprocessing:** The use of `PowerTransformer` without explicit reasoning is arbitrary and may not always be beneficial.
6.  **Custom Ensemble:** The `DynamicWeightedEnsemble` `fit` method assigns weights based on performance on the *training data* it's fitting on (`X`, `y`), not a separate validation set, leading to overfitting of weights.

**Completeness Issues:**
1.  **Feature Engineering:** Provides only basic imputation and scaling/transformation, not ""comprehensive feature engineering"" as requested (e.g., polynomial features, interaction terms, domain-specific features are missing).
2.  **Boosting Algorithms:** While XGBoost, LightGBM, and CatBoost are included, AdaBoost and general `GradientBoostingClassifier` are not explicitly instantiated and evaluated as separate entities in the main `models` dictionary for nested CV.
3.  **Model Interpretation (SHAP):** While a `summary_plot` is called, other SHAP plots like dependence plots or force plots are not explicitly shown or generated.
4.  **Calibration:** The `plt.show()` calls for calibration visualization are commented out, meaning the plots won't be displayed.
5.  **Dynamic Weight Assignment:** The `DynamicWeightedEnsemble` has a placeholder logic for weights based on training AUC rather than actual validation performance, making it less robust.
6.  **Ensemble Diversity Analysis:** While a correlation matrix is generated, the prompt asked for ""ensemble diversity analysis and model combination strategies."" The correlation matrix is a good start for diversity, but a more in-depth analysis (e.g., beyond just correlation, discussing why diversity matters) and further advanced combination strategies (beyond the basic StackingClassifier and a simple custom ensemble) are implied but not fully delivered. The custom ensemble implementation for diversity analysis is weak.

**Code Quality:**
1.  **Imports:** Correct.
2.  **Structure:** Attempts a logical flow, but the errors in nested CV and Optuna break the intended design.
3.  **Random State:** Consistent use of `random_state`.
4.  **Clarity:** The intentions are clear, but the implementation details fall short.
5.  **`use_label_encoder=False`:** Correctly included for XGBoost.

Overall, the code provides a good rough sketch but needs significant refinement, especially in the core concepts of nested CV and Bayesian optimization, to correctly implement the requested advanced techniques.",gpt-4.1-nano,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"import pandas as pd
import numpy as np
import pymc3 as pm
import arviz as az
from sklearn.preprocessing import StandardScaler
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split
from lifelines import CoxPHFitter, KaplanMeierFitter
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import NearestNeighbors

# Load dataset
df = pd.read_csv('diabetes.csv')

# 1) Preprocessing with missing va...",6,"The generated code attempts to address all requirements of the prompt, but several sections are either incomplete, incorrect, or make oversimplified assumptions that limit their practical utility and correctness.

**Strengths:**
*   Correctly uses PyMC3 for Bayesian modeling.
*   Includes MCMC sampling, which is a core requirement.
*   Attempts to implement hierarchical models, survival analysis, Gaussian processes, and causal inference.
*   Includes model comparison (WAIC, LOO) and posterior predictive checks.
*   Basic data preprocessing (imputation, scaling) is present.

**Weaknesses and Areas for Improvement:**

1.  **Data Preprocessing and Simulated Missing Values:** The ground truth correctly identifies domain-specific missing values (0s in physiological measurements). The generated code assumes `df.isnull().sum()` will find them, which it won't for 0s. It then uses `KNNImputer` but doesn't explicitly replace those 0s with NaNs first, making the imputation less effective or incorrect for this specific dataset. The ""Encode categorical variables if any"" block is a generic placeholder and the dataset `diabetes.csv` doesn't have a 'gender' column.

2.  **Hierarchical Model (`patient_id` assumption):** The generated code's hierarchical model relies on a `patient_id` column which does not exist in the `diabetes.csv` dataset. The placeholder `if 'patient_id' in df_imputed.columns:` and subsequent `group_idx = np.zeros(len(df_imputed), dtype=int)` if not found, effectively reduces the hierarchical model to a non-hierarchical one if 'patient_id' is absent, which it is. The ground truth correctly creates a 'group' variable (e.g., `Age_Group`) from existing data, which is a much more robust approach for this dataset.

3.  **Mixed-Effects Model:** ""Already implemented as hierarchical above"" is a weak justification. While hierarchical models can be mixed-effects, the prompt asked for separate clarity on mixed-effects models. The ground truth explicitly structures the hierarchical model as having both fixed and random (group-level) effects, clarifying its mixed-effects nature.

4.  **Bayesian Logistic Regression with Informative Priors:** The generated code applies informative priors but uses arbitrary feature names (`X['age']`, `X['bmi']`) that might not exist in the scaled `X` DataFrame without explicit selection/renaming, or assumes that `age` and `bmi` are columns in `X`, which they are likely to be, but the ground truth example of `beta_age * df_scaled['Age']` is safer as it refers to the actual column names in the processed DataFrame. The intercept prior of `mu=-2` for `p` is also quite specific and not broadly justified, whereas the ground truth uses `mu=0`, `sigma=1` for general intercepts, and more thoughtful informative priors for specific coefficients.

5.  **Survival Analysis:** The simulation of `time_to_event` and `event_occurred` is overly simplistic compared to the ground truth's more detailed simulation which considers a ""risk score"" and censoring logic tailored to the 'Outcome' variable. The use of `KaplanMeierFitter` is mentioned in imports but not used. The `survival_df` creation isn't fully using the `Outcome` column, which is available and should drive the event status. The formula `age + bmi` in the CoxPH model lacks other relevant diabetes predictors.

6.  **Gaussian Processes:** The GP model is set up to model `y.values` (which is `Outcome`, binary) using a `Marginal` GP with a `Normal` likelihood. This is incorrect. Gaussian Processes are typically used for continuous targets (regression) or classification with specific GP classifiers (e.g., `gp.Latent` with a `Bernoulli` likelihood for binary outcomes). Modeling a binary outcome with a `Normal` likelihood in a GP is inappropriate. The ground truth correctly models `Glucose` (continuous) using GP.

7.  **Causal Inference (PSM):** The PSM implementation is simplified. It calculates propensity scores and finds matched indices, but then states ""For simplicity, skip further analysis here"". The core of PSM is to compare outcomes in the matched groups to estimate the Average Treatment Effect, which is missing. The ground truth successfully calculates and prints the ATT. The IV section is explicitly a placeholder/commented out, which is not an implementation.

8.  **Context and Variable Naming:** The generated code uses `diabetes` as the target variable name, while the actual column in `diabetes.csv` is `Outcome`. This requires manual adjustment by the user or leads to errors. The ground truth correctly uses `Outcome`.

Overall, while the generated code attempts to hit all points, the quality and correctness of implementation for several complex requirements (hierarchical models, survival analysis simulation, Gaussian processes for binary data, and comprehensive causal inference interpretation) fall short of an expert-level solution.",gpt-4.1-nano,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, KBinsDiscretizer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.cluster import OPTICS
from sklearn.metrics import silhouette_score
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import NearestNe...",6,"The generated code attempts to address many of the requirements but falls short in several areas regarding correctness, completeness, and robustness.

**Strengths:**
*   **Comprehensive module inclusion:** Many necessary libraries are imported.
*   **Attempt at all sections:** The code
    has placeholders and attempts for almost all requested sections (preprocessing, multiple clustering, dim reduction, etc.).
*   **Good structure:** The code is well-structured with comments indicating the different task sections.
*   **Visualization:** Basic visualizations (t-SNE scatter, elbow plot) are included.

**Weaknesses and Areas for Improvement:**

1.  **Correctness & Robustness (Major Issue):**
    *   **Data Loading and Feature Engineering (Critically Flawed):** The generated code makes assumptions about column names (`blood_pressure`, `glucose`, `triglycerides`, `HDL`, `BMI`, `waist_circumference`) that are *not* present in the `diabetes.csv` dataset, which contains `Pregnancies`, `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`, `DiabetesPedigreeFunction`, `Age`, `Outcome`. This leads to `KeyError` or columns being filled with zeros. The feature engineering for metabolic syndrome is effectively non-functional for the provided dataset.
    *   **Autoencoder Implementation:** The autoencoder is a custom `sklearn` like class, but it requires `tensorflow` to be present. More importantly, it's not actually used for 2D visualization like t-SNE and UMAP, and `encoding_dim=10` is too high for *direct* visualization. The task specifically asked for autoencoder-based embeddings *for visualization* (implying 2D or 3D).
    *   **Optimal K for Clustering:** The gap statistic calculation lacks proper integration with the `gap_statistic` library (it just calculates `n_clusters` but doesn't use it). The code relies heavily on `best_k` derived from KMeans silhouette for other algorithms (GMM, Hierarchical, Spectral, OPTICS), which is not an optimal approach as each algorithm may have its own optimal `k`/parameters based on its underlying assumptions.
    *   **DBSCAN/OPTICS Parameter Tuning:** These algorithms are highly sensitive to `eps` and `min_samples`. The code uses fixed values (0.5, 5) without any attempt at systematic parameter searching or estimation (like k-distance plot for DBSCAN).
    *   **Consensus Clustering:** The `consensus_labels` function relies on `scipy.stats.mode` which finds the most frequent label. While a form of consensus, it's a very simplistic approach and not what is typically implied by ""consensus clustering"" in the context of identifying stable subgroups (which usually involves co-occurrence matrices and hierarchical clustering on them). The ground truth provides a strong example of this.
    *   **Anomaly Detection:** `sklearn.ensemble.Autoencoder` is not a valid class in scikit-learn. The task requested Isolation Forest, OneClassSVM, and LOF, but the generated code only uses Isolation Forest and attempts (unsuccessfully) to import `Autoencoder`.
    *   **Time-Series Clustering:** The conceptual part is good, but the code attempts to *implement* it by grouping and averaging, which is a very simplistic and often inadequate approach for true time-series clustering (which would involve DTW or sequence alignment). The `PatientID` column is also assumed but not present in `diabetes.csv`.
    *   **Association Rule Mining:** The discretization using `KBinsDiscretizer` and then direct `.astype(bool)` on the entire dataframe for Apriori can lead to very sparse data and might not be the most effective way to represent patient characteristics for ARM. The ground truth's approach of binning specific relevant features into named categories and then one-hot encoding them is much more interpretable and robust.
    *   **Risk Stratification:** The definition of `PatientID` for the risk system is `df.index`, which is not an actual patient ID and could lead to issues if the index is non-unique or reset.

2.  **Completeness:**
    *   **Optimal Parameter Determination:** While silhouette and elbow are attempted for K-Means, other algorithms like DBSCAN, GMM, Hierarchical, Spectral, and OPTICS lack proper parameter search strategies (e.g., BIC/AIC for GMM, reachability plots for Optics, density analysis for DBSCAN).
    *   **Patient Phenotype:** It attempts to compare means and modes, but statistical significance testing is missing (e.g., ANOVA/Kruskal-Wallis).
    *   **Anomaly Detection:** Only Isolation Forest is correctly implemented; OneClassSVM and LOF are missing.
    *   **Clinical Interpretation:** This is a major requirement for risk stratification, and it's completely missing in the generated code.

3.  **Code Quality:**
    *   **Imports:** Several unused imports exist (`GridSearchCV`, `KModes`, `networkx`, `datetime`). An incorrect import (`Autoencoder` in `sklearn.ensemble`) is also present.
    *   **Hardcoded Values:** Many parameters (e.g., `eps`, `min_samples` for DBSCAN/OPTICS) are hardcoded without justification or search.
    *   **Clarity:** Lack of comments for critical assumptions or complex logic.
    *   **Error Handling:** No robust error handling (e.g., for file not found).

To greatly improve the score, the generated code needs to correctly handle the dataset's actual column names, implement robust parameter tuning for all clustering algorithms, correctly implement all requested anomaly detection methods, and provide a meaningful consensus clustering output and clinical interpretation.",gpt-4.1-nano,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.ensemble import VotingRegressor, BaggingRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import Decision...",5,"The generated code attempts to address all requirements but falls short in several critical areas, often providing superficial or incorrect implementations for advanced concepts.

**Key Issues:**

1.  **Data Preprocessing and Feature Engineering:**
    *   **Incorrect Target Column:** The code assumes `'Life ladder'` as the target. The ground truth (and common knowledge for this dataset) uses `'Ladder score'`. This is a fundamental error.
    *   **Incomplete Preprocessing Pipeline:** The preprocessing is done as discrete steps (`fillna`, `PolynomialFeatures`, `OneHotEncoder`, `StandardScaler`). This is not integrated into a `Pipeline` or `ColumnTransformer` *before* the `train_test_split`, which leads to data leakage (scaling and encoding fit on the entire dataset) and makes it impossible to correctly use with `cross_val_score` or any `Pipeline`-based model evaluation/training. The original implementation correctly uses a `ColumnTransformer` within a `Pipeline`.
    *   **`X_final` creation:** `np.hstack([X_poly, X_cat.toarray()])` directly combines arrays without proper feature naming, which would make interpreting SHAP/LIME difficult later.

2.  **Ensemble Methods:**
    *   **Voting Regressor (Hard/Soft):** For regression, 'hard voting' is simple averaging, and 'soft voting' is weighted averaging. The generated code attempts to create a custom `SoftVotingRegressor` which just does simple averaging, effectively duplicating `VotingRegressor(voting='hard')` for regressors. `VotingRegressor` already supports weights. The implementation of `SoftVotingRegressor` is also incorrect as it operates on `X, y` directly, not scaled/preprocessed data.
    *   **AdaBoost Implementation:** The code first creates `ada` as a `GradientBoostingRegressor`, then redefines `ada_boost` as an `AdaBoostRegressor`. This is confusing and potentially misleading.
    *   **Lack of Diversity in Base Estimators:** While some are listed, the full scope of `base_estimators` in the ground truth is more comprehensive (e.g., SVR, KNN, RandomForest, ExtraTrees, etc.)

3.  **Bayesian Optimization:**
    *   The implementation uses Optuna correctly for a single model (XGBoost). However, it does not demonstrate how this would integrate into nested cross-validation, which is crucial for robust hyperparameter tuning. The ground truth shows how to integrate it as an objective within a nested CV setup, or at least acknowledges this limitation.

4.  **Nested Cross-Validation:**
    *   **Incorrect Implementation:** The `evaluate_model` function performs simple K-Fold cross-validation, not *nested* cross-validation. True nested CV involves an outer loop for evaluation and an inner loop for hyperparameter tuning. The code just takes pre-tuned models and evaluates them with a single CV run. This misses the core concept of nested CV for unbiased model selection and evaluation.
    *   **Data Leakage with `X_final`:** Because preprocessing isn't part of a pipeline, `X_final` is preprocessed on the full dataset, leading to data leakage during cross-validation, making `cross_val_score` results optimistic.

5.  **Custom Ensemble with Dynamic Weight Assignment:**
    *   **Incorrect Weight Calculation:** The calculation `weights = {k: (1 / errors[k]) for k in errors}` and subsequent normalization is a valid *approach*, but the OOF predictions should be used, not just the single `cross_val_score` result (which is averaged over folds and not an OOF prediction vector). The ground truth correctly generates OOF predictions for each model and then uses those to calculate weights.
    *   **`weighted_ensemble_predict` Function:** This function is defined but never called or used to evaluate the custom ensemble. It also implicitly assumes `X_test` will have the same shape for all models, which is incorrect if models expect different preprocessing stages or data features.

6.  **Advanced Techniques (Blending and Multi-level Stacking):**
    *   **Blending:** The blending example is very rudimentary and only blends on `X_test` *after* training, rather than using a separate validation set and training base models on a different training split for proper unbiased blending. It also just re-weights predictions and not actually trains the top models on `X_train` and then combines their predictions for the meta-learner on `X_val` as blending implies.
    *   **Multi-level Stacking:** No actual multi-level stacking is demonstrated or implemented. It briefly mentions it but provides no code for it, only for the `StackingRegressor` which is a single-level stack.

7.  **Model Interpretation (SHAP/LIME):**
    *   **`explainer = shap.Explainer(xgb_best)`:** For tree models, `shap.TreeExplainer` is usually preferred. `shap.Explainer` is more generic.
    *   **LIME `training_data` and `feature_names`:** The `X_train` used for `training_data` and the generic `f{i}` for feature names are problematic. The `X_train` here is already scaled, which LIME usually expects raw data for its perturbations to make sense. Also, the generic feature names make the interpretation unreadable for humans. The ground truth correctly uses `X.values` (raw data) and original feature names.
    *   **`exp.show_in_notebook(show_table=True)`:** This won't display results in a standard Python script execution; `exp.as_pyplot_figure()` or similar is needed. The ground truth accounts for this.

8.  **Model Calibration and Uncertainty Quantification:**
    *   **Calibration:** The generated code only provides a simple residual check, which is a start but doesn't implement a true calibration technique like Platts scaling or isotonic regression for regression. The ground truth correctly uses quantile regression (implicitly showing prediction intervals), which is a better fit for regression ""calibration"" and UQ.
    *   **Uncertainty Quantification:** The simple 1.96 * residual_std is an overly simplistic and often inaccurate way to estimate prediction intervals, assuming normality and homoscedasticity of errors.

9.  **Ensemble Diversity Analysis and Model Combination Optimization:**
    *   **Diversity Analysis:** The correlation matrix is a good start.
    *   **Optimization of Model Combinations:** This section is completely omitted, only mentioning ""can be performed via metaheuristic algorithms"" but providing no code. The ground truth correctly implements a weight optimization using `scipy.optimize.minimize`.

**Overall:**

The code attempts to cover all requirements but often implements them incorrectly, superficially, or misses crucial details that make them ineffective or flat-out wrong for the given task. The fundamental error in preprocessing (data leakage, lack of pipeline integration) alone makes the reported performance metrics unreliable and the subsequent advanced techniques problematic. The lack of adherence to proper pipeline usage for data preprocessing is a major functional flaw.",gpt-4.1-nano,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import arviz as az
import pystan
from sklearn.preprocessing import StandardScaler
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import pairwise_distances
from causalinference.inference.median_matching import PropensityScoreMatching
import statsmodels.api as sm
import statsmodels.formula.api as smf

# Load dataset...",5,"The generated code attempts to address all requirements but falls short in critical areas, leading to significant correctness and completeness issues.

**Correctness and Completeness Flaws:**

*   **Missing Value Handling:** The `KNNImputer` handles numerical missing values, but the categorical imputation (`fillna(mode)`) is applied *before* `pd.get_dummies`, meaning any new categories introduced by `get_dummies` after imputation would not be handled. More importantly, it doesn't handle the target variable's missing values if any, and the `ColumnTransformer` approach from the ground truth is far more robust and standard for mixed-type data preprocessing.
*   **Predictor Columns:** `predictor_cols` are hardcoded with original column names ('Economy (GDP per Capita)', etc.). The code does not perform the critical step of renaming columns (e.g., to `Economy_GDP_per_Capita`) which leads to issues in subsequent steps that rely on these renamed columns (e.g., `smf.ols` formulas, `X[predictor_cols]`). This is a foundational error that breaks many subsequent parts.
*   **Hierarchical Model:**
    *   The `regional_effect` is added unconditionally even if `regions` is None, which will lead to an error.
    *   `X` is incorrectly indexed; it expects the processed, standardized `predictor_cols`, but `pd.get_dummies` affects column names.
    *   The conditional logic for `regional_effect` is flawed. If `regions` is None, `regional_effect` becomes `0`, effectively turning the model into a non-hierarchical one, not robustly handling the non-existence of a region column. The ground truth correctly maps regional IDs and ensures the model is hierarchical when needed.
*   **SEM Approximation:** The generated code only performs two separate OLS regressions using `statsmodels.formula.api`, which is *not* a Structural Equation Model. True SEM involves simultaneous estimation of multiple equations and latent variables, often requiring specialized libraries (e.g., `lavaan` in R, `statsmodels.formula.api` with custom paths, or more complex PyMC3 modeling). The ground truth's approach of multiple PyMC3 models to approximate causal pathways is more aligned with the spirit of Bayesian SEM than disconnected OLS.
*   **Causal Inference (IV & PSM):**
    *   **IV:** Uses `statsmodels.sandbox.regression.gmm.IV2SLS`, which is fine, but the *simulation* of the instrument is very crude (`np.random.normal`) and doesn't demonstrate understanding of a valid instrument's properties (correlation with endogenous variable, uncorrelation with outcome, exclusion restriction). The ground truth's simulation, while still a simulation, shows more thought about how an instrument would relate to the endogenous variable and outcome.
    *   **PSM:** It uses `causalinference.inference.median_matching.PropensityScoreMatching`, which is a niche library and not standard `causalinference` from `pycausal`. The PSM implementation is incomplete:
        *   It calculates propensity scores.
        *   It calls `psm.match` but doesn't actually estimate the ATE from the *matched* samples; it incorrectly tries to derive ATT from `df.loc[matched_indices]['Life_Ladder'].mean() - df[~df.index.isin(matched_indices)]['Life_Ladder'].mean()`, which is wrong. The comparison should be between treatment and control *within the matched set*. The ground truth correctly performs matching and calculates ATE using the matched `treated_df` and `matched_control_df`.
*   **Bayesian Regression with Informative Priors:** The `shape` parameter for `betas` prior (`shape=3`) is hardcoded, which might not match the number of truly selected predictor columns. The `X[predictor_cols[:3]]` again hardcodes specific columns rather than adapting to the actual `predictor_cols`.
*   **Gaussian Processes:** The `X[predictor_cols]` passed to `gp.Marginal` is a DataFrame with multiple columns, but the Square Exponential kernel `ExpQuad(1, ls=length_scale)` is defined for a single dimension (1). This will lead to an error or incorrect behavior. GP usually works best with 1D or 2D inputs for simpler models, or requires higher-dimensional kernels. The ground truth correctly uses only GDP for the GP demonstration, which is more typical.
*   **Mediation Analysis:** The generated code uses Baron & Kenny's approach with two separate OLS models (`smf.ols`), which is outdated and less robust than bootstrapping or Bayesian methods for mediation. The ""indirect effect"" calculation is a simple product, not a full mediation analysis. The ground truth provides a more robust Bayesian approach by drawing samples from the posteriors of the relevant parameters.
*   **Difference-in-Differences:** The simulation of `post_policy` and `treatment_group` is applied to the *entire* cross-sectional dataset randomly, not by creating a synthetic panel by duplicating and assigning treatment/control groups to countries across time periods. This makes the DiD interpretation questionable. The ground truth correctly duplicates data for a pre/post structure.
*   **Model Comparison:** The model comparison (`trace_full`, `trace_red`) attempts `az.loo` and `az.waic` but arbitrarily reduces `predictor_cols` to just two for the ""reduced"" model (`X[predictor_cols[:2]]`), which is not a meaningful comparison setup. The ground truth compares the hierarchical model with a standard linear model, which is a more appropriate comparison.
*   **Bayes Factor:** The ""approximation"" of Bayes Factor via WAIC/LOO differences (`np.exp(waic_red.waic - waic_full.waic)`) is technically incorrect. While WAIC/LOO estimate predictive accuracy, they do not directly provide Bayes Factors. Bayes Factors require marginal likelihoods, which are computationally more intensive. The ground truth correctly states that Bayes Factors are not directly computed and relies on WAIC/LOO for comparison.
*   **Pystan Import:** `pystan` is imported but never used.

**Code Quality Issues:**

*   **Readability/Structure:** The code is a monolithic block without clear separation between each task or function.
*   **Column Naming Inconsistency:** The original column names from the CSV are used inconsistently with renamed versions (`Economy (GDP per Capita)` vs `Economy_GDP_per_Capita`, `Life Ladder` vs `Life_Ladder`). This is a common source of errors. The ground truth proactively renames all columns for consistency.
*   **Error Handling:** No robust error handling (e.g., `try-except` for file loading).
*   **Imports:** Several imports are present but unused (`pystan`, `train_test_split`).
*   **Comments:** Sparse comments, and some are misleading (e.g., ""Using statsmodels for a simplified SEM"").

In summary, the generated code attempts to use the right libraries but misapplies many core concepts for the statistical and causal inference methods requested. It lacks the robustness, correctness, and attention to detail that the ground truth demonstrates, particularly in data preprocessing, model specification, and causal inference techniques.",gpt-4.1-nano,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np

# Data preprocessing and feature engineering
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA

# Clustering algorithms
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.neighbors import NearestNeighbors
from sklearn.manifold import SpectralEmbedding

import hdbscan
import umap
import seaborn as sns
import matplotlib.pyplot as plt...",2,"The generated code attempts to address many parts of the prompt but fails significantly in execution, leading to a non-functional and incomplete solution.

**Major Issues:**

1.  **Missing Imports and Incorrect Usage:**
    *   `Keras` is imported as `keras` and `Model`, `Input`, `Dense` are imported from `keras.models` and `keras.layers` respectively, but the actual code uses `keras.models` and `keras.layers` which requires `tensorflow.keras`. The `keras` import itself might need to come from `tensorflow.keras.models`. The Ground Truth correctly uses `tensorflow.keras`.
    *   `sklearn.cluster.DBSCAN` is used as `sklearn.cluster.DBSCAN` instead of just `DBSCAN` given the import `from sklearn.cluster import ... DBSCAN`.
    *   `pyod` is imported but not correctly used; it's missing specific imports and expects a `pyod.models.auto_encoder.AutoEncoder` which is generally a separate library.
    *   `IsolationForest` is correctly imported, but the `pyod` part is problematic.
    *   `sklearn.neighbors.NearestNeighbors` is imported, but the code uses `NearestNeighbors` without error.
    *   `DescrStatsW` is imported from `statsmodels.stats.weightedt.common`, but not used.
    *   The `gap_statistic` library requires installation and is not a standard `scikit-learn` or `numpy` package. Its use is acceptable if it functions correctly, but it's a point of failure if not installed.
    *   The `df['Country or region']` column access is incorrect based on the dataset (it should be `df['Country name']` as per the ground truth). This will cause key errors.

2.  **Functionality and Completeness:**
    *   **No structure/Class:** The prompt implicitly asks for a well-structured solution (indicated by the `class AdvancedHappinessAnalysis` in ground truth). The generated code is a monolithic script without functions or a class, making it hard to read, maintain, and execute selectively.
    *   **Data Loading and Preprocessing:** It hardcodes a small subset of features (e.g., `features = ['Healthy life expectancy at birth', 'Perceptions of corruption', 'GDP per capita', 'Freedom to make life choices', 'Generosity']`) and even gets the column names wrong (e.g., `Healthy life expectancy at birth` vs `Healthy life expectancy` in the dataset). It completely ignores the ""advanced feature engineering including happiness profiles and regional characteristics"" aspect, only creating a simple `Composite_Happiness_Score` by averaging existing features, which is not what 'happiness profiles' mean in an advanced context. The ground truth shows more sophisticated feature engineering.
    *   **Optimal Parameter Determination (DBSCAN):** The DBSCAN parameter tuning attempts to use silhouette, but it's flawed. `sil_scores_dbscan.append(-1)` is a poor way to handle single-cluster or noise-only clusters. The `best_eps` logic is shaky.
    *   **Hierarchical Clustering:** It hardcodes `n_clusters=4` for hierarchical clustering, ignoring the optimal K determination.
    *   **Spectral Clustering:** The spectral embedding is performed, but then `KMeans` is applied to the embedding, which is a common approach, but the optimal cluster count isn't derived.
    *   **Consensus Clustering:** The `AgglomerativeClustering` for consensus clustering uses `distance_threshold=0.2`. The `linkage` function expects a condensed distance matrix, but it's given `1 - co_assoc` directly, which is an `(N, N)` square matrix. This is incorrect. The Ground Truth correctly uses `np.triu_indices` for `linkage`.
    *   **Profile Statistics and Significance Testing:** Uses `ttest_ind` which is good, but `DescrStatsW` is imported but not used. The output/storage of these tests is not clearly handled beyond a comment.
    *   **No Visualizations for DR techniques:** While DR is performed, there are no visualizations showing clustering on top of DR techniques, which is crucial for validation. The Ground Truth actively plots these.
    *   **Anomaly Detection with PyOD:** The `pyod` part is unlikely to work out of the box without specific `pyod` model imports and installation.
    *   **Effect Size:** Prompt asks for effect size analysis (e.g., Cohen's d or Eta-squared). The generated code calculates Cohen's d but doesn't store/print it or explain its interpretation meaningfully. The Ground Truth provides a better interpretation.
    *   **Network Analysis:** Missing a robust thresholding method for edge creation. The current approach will result in a fully connected graph or a very dense one if the threshold is too low, or no graph if too high, making it uninterpretable. The Ground Truth's 90th percentile approach is more pragmatic.
    *   **Association Rule Mining:** The binning uses labels `[0,1,2]` and creates boolean columns like `GDP per capita_bin_0`, which makes the rules less human-readable than the Ground Truth's `low_GDP`, `medium_GDP`, etc.
    *   **Recommendation System and Policy Insights:** These are very basic placeholders. The geopolitical validation is merely a comment, not code. The 'policy insights' are just qualitative statements.

3.  **Code Quality:**
    *   Lack of comments explaining complex logic.
    *   Variable names are sometimes vague.
    *   No proper use of functions/class, making the code harder to debug and reuse.
    *   No error handling.

In summary, the generated code demonstrates a general awareness of the required steps but fails to implement them correctly or robustly. Critical missing components, incorrect library usage, and a lack of proper structure make it far from a production-ready or even a correctly executing solution for the given advanced task.",gpt-4.1-nano,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"import pandas as pd
import numpy as np
from statsmodels.tsa.statespace.sarimax import SARIMAX
import pmdarima as pm
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# 1) Load data, parse 'date', filter for 'Germany', set 'date' as index
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
germany_df = df[df['location'] == 'Germany'].copy()
germany_df.set_index('date', inplace=True)
germany_df.sort_index(inplace=True)

# 2) Handle missing values (for...",9,"The generated code correctly implements all specified requirements for the time series forecasting task. It successfully handles data loading, filtering, time series indexing, and missing value imputation for target and exogenous variables. The use of `auto_arima` for hyperparameter selection and The `SARIMAX` model for training and forecasting, along with RMSE evaluation and plotting, demonstrates a complete understanding of the task.

**Strong points:**
- All requirements are met.
- Correct use of `pmdarima.auto_arima` and `statsmodels.tsa.statespace.sarimax.SARIMAX`.
- Proper handling of exogenous variables for both model fitting and forecasting.
- Correct splitting of data into train/test sets.
- Clear and concise plotting of results.

**Areas for minor improvement (to reach a perfect 10):**
- **Missing Value Handling Detail:** The `ffill` method was applied to individual columns. While this works, a more robust and complete missing value handling strategy (as seen in the ground truth) would involve `bfill` after `ffill`, and potentially interpolation, to ensure all possible NaNs (especially at the beginning of the series) are addressed. The ground truth also explicitly checks for remaining NaNs, which is good practice.
- **Negative Forecasts:** The generated code does not explicitly handle potential negative forecasts for 'new_cases', which are illogical in this context. The ground truth includes a line `forecast_values[forecast_values < 0] = 0` to address this, which is important for practical time series forecasting of quantities that cannot be negative.
- **Error Handling/Robustness:** The ground truth includes `try-except` for file loading and checks for insufficient data before splitting. While not strictly required by the prompt, these add robustness to the code.
- **Plotting Markers:** While `marker='o'` is used, a common practice for time series plots is to use lines. The ground truth uses lines, which is generally more appropriate for continuous time series visualization.",gpt-4.1-nano,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import shapiro, kruskal
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import scikit_posthocs as sp
import statsmodels.api as sm

# Load dataset
df = pd.read_csv('winequality-red.csv')

# Define quality categories
def categorize_quality(score):
    if score <= 4:
        return 'low'
    elif score <= 6:
        return 'medium'
    else:
        return 'high'

df['qu...",6,"The generated code attempts to address all requirements but has significant correctness and completeness issues that prevent it from being production-ready or fully reliable for a data science benchmark.

**Correctness Issues:**
1.  **Normality Check and Test Selection Logic:** The generated code's logic for choosing between ANOVA and Kruskal-Wallis is flawed. It checks for normality using `shapiro` for each group but then *always* performs `kruskal` regardless of whether the data is normal or not. The task explicitly asks for ANOVA if assumptions (normality and homogeneity) are met. This is a critical logical error. The `scipy.stats.levene` test for homogeneity of variance is completely missing, which is a key assumption for ANOVA.
2.  **Post-hoc Test for ANOVA:** When `normal` is True (even though it then proceeds to do Kruskal-Wallis anyway), it attempts to perform `pairwise_tukeyhsd`. However, `pairwise_tukeyhsd` requires a numpy array of `endog` values and a list of `groups` *corresponding to those `endog` values*. The generated code passes `df[feature]` and `df['quality_category']` which implicitly assumes the DataFrame is sorted by quality category, which is not guaranteed and wrong for calculating Tukey's honestly significant difference. This is a common misuse of `pairwise_tukeyhsd`.
3.  **Kruskal-Wallis Post-hoc:** It correctly uses `scikit_posthocs.posthoc_dunn` for non-parametric post-hoc tests after Kruskal-Wallis, which is good.
4.  **Incomplete Post-hoc Output:** The code only prints a generic message (""Significantly different across groups (Tukey HSD)"" or ""(Dunn's test)"") but doesn't output the detailed results of the post-hoc tests (which specific pairs are significant, p-values, etc.). This is a key part of post-hoc analysis.

**Completeness Issues:**
1.  **Homogeneity of Variance Check:** Completely missing `scipy.stats.levene`, which is crucial for deciding between ANOVA and Kruskal-Wallis.
2.  **Detailed Output of Post-hoc Results:** As noted above, the detailed results are not printed, making the post-hoc analysis incomplete for practical interpretation.
3.  **Error Handling:** The code has no error handling for file not found or insufficient data in groups for specific tests (e.g., `shapiro` on very small groups).
4.  **Summary:** The summary of significantly different features is a simple list, but it lacks the detail of *why* they are significant (p-values from the main test) and *how* they differ (details from post-hoc tests).

**Code Quality:**
1.  **Imports:** Correct use of necessary libraries, including `scikit_posthocs` which is appropriate for Dunn's test.
2.  **Readability:** Generally readable.
3.  **Variable Naming:** Clear.
4.  **Categorical Ordering:** While `categories = ['low', 'medium', 'high']` is defined, `df['quality_category']` is not explicitly converted to a `CategoricalDtype` with this order, which can sometimes lead to inconsistent plotting order depending on `seaborn`'s default behavior, though for `boxplot` with `order=categories` it would likely be fine.

Overall, the core statistical decision-making (ANOVA vs. Kruskal-Wallis based on assumptions) is incorrect, and the post-hoc usage (especially for Tukey's) is flawed. The output of the post-hoc tests is insufficient for a data science benchmark. These are fundamental issues for a task focusing on statistical analysis.",gpt-4.1-nano,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, f1_score
import shap
import datetime

# 1. Load data
data = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2. Determine the 90th percentile...",8,"The generated code largely addresses all requirements of the task and demonstrates a good understanding of the necessary steps for a data science classification pipeline.

**Strong Points:**
*   **Correct Target Variable Definition:** The code correctly defines 'high_daily_death_increase' based on a location's 90th percentile of `new_deaths` (excluding zeros), which is a complex but crucial part of the task. It correctly handles the merging of percentiles back to the DataFrame.
*   **Comprehensive Feature Engineering:** It successfully implements:
    *   Rolling 7-day means and standard deviations.
    *   Lag features for specified columns at 1, 3, and 7 days.
    *   `days_since_first_case` calculation, including handling the nuance of first ""case"" date.
*   **Appropriate Missing Value Handling:** It fills NaNs generated from rolling/lag features with 0, which is a reasonable approach for the start of time series.
*   **Correct Feature Selection and Preprocessing:** It identifies and handles categorical and numerical features correctly using `ColumnTransformer` for one-hot encoding and scaling.
*   **Robust Model Training and Evaluation:**
    *   Uses XGBoost as requested.
    *   Correctly implements `GridSearchCV` for hyperparameter tuning.
    *   Performs a time-based split for `X_train`/`X_test` based on date, which is critical for time-series data.
    *   Evaluates with ROC-AUC and F1-score.
*   **SHAP Implementation:** It correctly uses `shap.TreeExplainer` on the best model, transforms the test data for SHAP values, and attempts to map feature names back.

**Areas for Improvement (Why not a 9 or 10):**
1.  **`people_fully_vaccinated` usage:** The prompt specifically asked to use `people_fully_vaccinated / population` as a ratio. The generated code includes `people_fully_vaccinated` and `population` as separate features, but does not explicitly create the ratio. This is a minor but direct mismatch with the prompt.
2.  **Missing Values in Original Features:** While the code handles NaNs in engineered features, it doesn't explicitly address potential NaNs in original numerical features (like `stringency_index`, `gdp_per_capita`, `diabetes_prevalence`, etc.) that were listed in the prompt's `selected_features` section. The prompt's ground truth code explicitly calls this out. The generated code could benefit from a more explicit strategy for these non-engineered numerical NaNs beyond just `fillna(0)` which happens implicitly through the scaler if not handled beforehand.
3.  **SHAP Feature Naming:** The SHAP explanation part tries to get the feature names right, but `all_feature_names = list(ohe_feature_names) + numerical_features` concatenates a list of already preprocessed numerical features with original numerical features. This could lead to incorrect or duplicate feature names for displaying SHAP.
    *   Specifically, `numerical_features` variable in the `SHAP` section still refers to the original names, not the ones after scaling. When combined with OHE names, the order and names might not perfectly match the `X_test_transformed` matrix.
    *   The ground truth demonstrates a more robust way to get the final feature names from the preprocessor's `get_feature_names_out` method after transformation, which ensures correct mapping back to the SHAP values.
4.  **`days_since_first_case` `fillna(-1)`:** `df['days_since_first_case'] = df['days_since_first_case'].clip(lower=0).fillna(-1)` - filling with -1 for locations with no cases might need consideration. If these locations truly have no cases, then -1 might be an acceptable indicator, but `0` (as used in the ground truth for missing values) might be more consistent with the general missing value handling strategy and less likely to introduce strange patterns for models.
5.  **Imports:** `datetime` is imported but not used. Not a critical issue, but good to clean up.
6.  **Redundant `preprocessor.fit(X_train)` before SHAP:** The `best_model` already contains a fitted preprocessor within its pipeline. Calling `preprocessor.fit(X_train)` again standalone won't affect the model's internal preprocessor and is redundant. The `X_test_transformed` is correctly produced by `best_model.named_steps['preprocessor'].transform(X_test)`.

Overall, the code is very strong and provides a correct and mostly complete solution to a complex problem. The issues are mostly about fine-tuning for robustness and exact adherence to all minor prompt specifications.",gpt-4.1-nano,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf

# 1. Load data and parse 'date'
data = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Calculate 'vaccination_percentage'
data['vaccination_percentage'] = data['people_fully_vaccinated'] / data['population']

# 2. Handle missing values
# For 'reproduction_rate', 'stringency_index', and 'vaccination_percentage'
# First, perform interpolation within each location
...",6,"The generated code attempts to address all aspects of the task, but several critical issues prevent it from being a fully correct and robust solution.

**Correctness Issues:**

1.  **Missing Value Handling:**
    *   The `df['vaccination_percentage'] = data['people_fully_vaccinated'] / data['population']` line will produce NaNs if `population` is 0 or NaN, or if `people_fully_vaccinated` is NaN. It calculates this *before* any imputation on `people_fully_vaccinated` or `population`. The ground truth correctly imputes these components first.
    *   The second imputation step `data.groupby('location')[col].transform(lambda group: group.fillna(group.median()))` for 'reproduction_rate', 'stringency_index', and 'vaccination_percentage' is redundant and potentially problematic after the `interpolate` step. If `interpolate` with `limit_direction='both'` is used (which it wasn't here, only `limit_direction='forward'` by default for `apply`), then `fillna(median())` would be a fallback for entire groups or leading/trailing NaNs. Here, the `apply` and `transform` logic can behave differently. More importantly, using `median()` *per group* might not be the best strategy for time-series data unless median is truly appropriate. The ground truth's approach of global median AFTER linear interpolation fallback is more robust.
    *   There's no capping of `vaccination_percentage` at 100%, which can lead to values > 100 if the raw data has issues.

2.  **Mixed-Effects Model Specification (Critical Flaw):**
    *   The prompt clearly states: ""Random effects: random intercepts for 'location' **nested within** 'continent'.""
    *   The generated code's `smf.mixedlm(..., groups=data['continent'], re_formula=""1"", vc_formula={'location': '0 + C(cont_loc)'})` attempts to specify nesting but incorrectly.
        *   `groups=data['continent']` implies random effects *per continent*.
        *   `re_formula=""1""` (for `groups`) means random intercept for *each continent*.
        *   `vc_formula={'location': '0 + C(cont_loc)'}`: This is *not* the correct way to specify nested random intercepts in `statsmodels.mixedlm` for the task given. `statsmodels.mixedlm` handles nesting by providing a unique group identifier for each innermost group (e.g., `continent`\_`location`). The `groups` argument should be `data['cont_loc']` as in the ground truth.
        *   The generated code ends up fitting a model with random intercepts for `continent` (via `groups` and `re_formula`) AND tries to estimate variance components for `location` within `cont_loc` as a separate, more complex random effect structure (via `vc_formula`). This is not a simple random intercept nested model as requested. It's fundamentally different and likely not the intended analysis. The `mixedlm_fit.cov_re` will show variance for `continent` and `mixedlm_fit.vcomp` for the `vc_formula` part, which is confusing and doesn't directly answer the ""variance of random intercepts for location nested within continent"".
    *   The ground truth's approach of creating `df['continent_location_id']` and using `groups=df['continent_location_id']` with `re_formula=""1""` is the standard and correct way to model random intercepts nested within two factors for `smf.mixedlm`.

3.  **Model Fitting and Reporting:**
    *   The `reml=False` (Maximum Likelihood) is technically fine, but REML (Restricted Maximum Likelihood) is generally preferred for estimating variance components in mixed models as it provides unbiased estimates. The prompt didn't specify, so it's not a direct error, but worth noting.
    *   `mixedlm_fit.cov_re` will return the covariance matrix for the `groups` random effect (i.e., `continent`), not for the `continent_location_id` as intended by the task. `mixedlm_fit.vcomp` will be related to the `vc_formula` which was incorrectly set up. Thus, the reporting of variance components is incorrect for what the task asked.

**Code Quality/Completeness:**

*   **Imports:** Correct.
*   **Structure:** Follows the numbered steps reasonably well.
*   **Interpretation:** The generated code includes comments on interpretation, which is good, but doesn't actually implement the detailed interpretation presented in the ground truth, merely prints the summary table. The prompt asked to ""Interpret the significance and direction of these effects.""
*   **Robustness:** Lacks error handling for file not found, which is a good practice.
*   **Warnings:** Doesn't handle potential `statsmodels` warnings.
*   **Initial Data Sorting:** It fails to sort the data by `location` and `date` before interpolation, which is crucial for correct time-series interpolation.

**Overall:**

The main failing point is the incorrect specification of the nested random effects model, which leads to an incorrect model being fitted and incorrect variance components being reported for the task's requirements. The missing value imputation also has room for improvement regarding robustness.",gpt-4.1-nano,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score

# Load dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select features
features = ['elixirCost', 'Win Rate', 'Usage']
X = df[features]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Determine optimal K u...",9,"The generated code is very good and largely fulfills the requirements of the task.

**Strong Points:**
- **Correctness:** The code correctly loads data, selects and standardizes features, implements K-Means, performs PCA, and visualizes the results.
- **Feature Selection & Standardization:** Correctly identifies and scales `elixirCost`, `Win Rate`, and `Usage`.
- **K Determination:** Uses *both* the Elbow method (WCSS) and Silhouette scores, which is a robust approach to choosing K. It then programmatically selects `optimal_K` based on the highest silhouette score, which is excellent.
- **K-Means Application:** Applies KMeans with the determined `optimal_K`.
- **PCA Dimensionality Reduction:** Correctly reduces dimensions to 2 using PCA.
- **Visualization:** Creates a scatter plot of the two principal components, coloring points by cluster labels, which is exactly as requested.
- **Imports:** All necessary imports are present.

**Areas for Improvement (leading to a score of 9 instead of 10):**
1.  **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and missing columns. The generated code assumes the file exists and the columns are present, which can lead to crashes if the prerequisites aren't met.
2.  **`n_init` Parameter:** In `sklearn.cluster.KMeans` (starting from scikit-learn 1.4), `n_init` defaults to `'auto'` (which means `n_init=10` in many cases) but it's good practice to explicitly set it to `10` or `20` to ensure multiple centroid initializations and avoid warnings in older versions or for clarity. The generated code omits `n_init` in the KMeans instantiation, allowing it to default. While `'auto'` is often 10, explicitly setting it matches best practices for reproducibility and avoids potential future warnings/changes in defaults.
3.  **Plot Labels/Titles Detail:** While the plots are generally good, the ground truth's PCA plot labels provide the explained variance ratio for each component, which adds valuable context to the visualization. The generated code's PCA plot labels are more generic.
4. **General Output/Logging:** The ground truth includes more verbose print statements to show the steps (e.g., ""Dataset loaded successfully"", ""First 5 rows of standardized features""), which can be very helpful for debugging and understanding flow in a script meant for a wider audience. The generated code is more concise in its output.

Overall, the generated code is highly functional and demonstrates a strong understanding of the task requirements. The minor points mentioned are mostly about robustness, user-friendliness, and slightly more detailed plotting, which are good to have but don't detract significantly from the core functionality.",gpt-4.1-nano,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select features and target
X = df[['elixirCost', 'Usage', 'rarity']]
y = df['Win Rate']

# Preprocessing for 'rarity' using on...",9,"The generated code correctly addresses almost all aspects of the task, demonstrating a strong understanding of data preprocessing and model evaluation.

**Strong Points:**
*   **Correct Data Loading and Selection:** Successfully loads the CSV and selects the specified features (`elixirCost`, `Usage`, `rarity`) and target (`Win Rate`).
*   **Proper Preprocessing for `rarity`:** Utilizes `OneHotEncoder` within a `ColumnTransformer` and `Pipeline`, which is a robust and recommended approach for handling categorical features in `scikit-learn`. This is more advanced and often preferred over `pd.get_dummies` in a production setting.
*   **Correct Data Splitting:** Implements `train_test_split` with the correct `test_size` and `random_state`.
*   **Regression Model Training:** Trains a `LinearRegression` model, which is a valid choice as per the task description.
*   **Correct Evaluation Metrics:** Calculates and prints both R-squared and MSE as required.
*   **Excellent Use of `Pipeline` and `ColumnTransformer`:** This is a very good practice for organizing machine learning workflows and ensuring that preprocessing steps are consistently applied to new data.

**Areas for Minor Improvement (preventing a perfect 10):**
*   **Handling `KeyError` for 'Win Rate' and `FileNotFoundError`:** The ground truth includes `try-except` blocks for file loading and checks for missing columns. While the prompt implied the file and columns would exist, robust code should include these checks for real-world scenarios.
*   **No handling of `drop_first=True` for OneHotEncoder:** While not explicitly required by the prompt, `drop_first=True` in `OneHotEncoder` (or `pd.get_dummies`) is often used to prevent multicollinearity in linear models when one-hot encoding, though it can sometimes be omitted
    depending on the model's robustness to multicollinearity (e.g., Tree-based models are less sensitive). Given `LinearRegression` is used, it would be a good addition. The default `drop_first=False` is used here.
*   **No explicit handling of `NaN` values** in the chosen features. Although the provided dataset might not have them, a robust solution would include a strategy for missing data (e.g., imputation or dropping, similar to the ground truth).

Overall, the code is highly functional, well-structured, and demonstrates best practices in scikit-learn for MLOps. The minor points are about robustness and completeness to various data situations rather than fundamental correctness in solving the immediate problem.",gpt-4.1-nano,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy.stats import f_oneway

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Identify unique rarity categories
rarity_categories = df['rarity'].unique()

# Prepare data for ANOVA: list of Win Rates for each rarity group
grouped_data = [df.loc[df['rarity'] == rarity, 'Win Rate'] for rarity in rarity_categories]

# State hypotheses
# Null hypothesis (H0): All group means are equal (no difference in mean Win Rate across rarities)
# Alternative hyp...",7,"The generated code correctly performs the ANOVA test and interprets the results, addressing the core requirements. However, it lacks robustness and completeness compared to the ground truth.

Specific areas for improvement:
- **Error Handling:** The generated code does not include any error handling for file not found, missing columns, or non-numeric 'Win Rate' values. The ground truth includes `try-except` blocks for file loading and `pd.to_numeric` with `errors='coerce'`, followed by `dropna` for robustness.
- **Data Preprocessing Robustness:** The ground truth specifically handles cases where 'Win Rate' might not be numeric and ensures that groups have at least two observations for a valid ANOVA, preventing potential errors or misleading results if a rarity category has insufficient data. The generated code's `grouped_data` creation might result in empty arrays or arrays with single values if issues exist, which `f_oneway` might handle gracefully, but it's better to explicitly check and inform the user.
- **Hypotheses Statement:** While the hypotheses are correctly *stated* in comments, the formal print statements in the ground truth make the output more structured and user-friendly.
- **Output Formatting:** The ground truth prints F-statistic and p-value with specific formatting (`.4f`) and includes the alpha level, which enhances readability.
- **Completeness of Requirements:** The prompt explicitly asked to ""State the null and alternative hypotheses for the test"" (suggesting print statements) and to ""Print the F-statistic and the p-value"". The generated code states them as comments and prints the values, but the ground truth's detailed output matches the prompt's tone more closely.",gpt-4.1-nano,LLM_judge_config
H25,hard,Classification,iris.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# 1) Load the dataset
df = pd.read_csv('iris.csv')

# 2) Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = ...",9,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, defines features and target, encodes the target, splits the data, trains the specified models, evaluates them using the required metrics (accuracy, precision, recall, F1-score with macro average), and identifies the best model based on F1-score. Proper imports are included.

Areas for improvement:
1.  **Missing `stratify` in `train_test_split`**: The ground truth includes `stratify=y_encoded` in the `train_test_split` function. This is good practice for classification tasks, especially with potentially imbalanced datasets, to ensure that both training and testing sets have a representative distribution of classes. The generated code omits this.
2.  **`max_iter` for Logistic Regression**: The ground truth code includes `max_iter=200` for `LogisticRegression` to prevent potential `ConvergenceWarning`. While not strictly required by the prompt, it's a practical consideration for robust model training. The generated code uses default, which might (rarely, for iris) lead to convergence issues.
3.  **Error Handling for File Not Found**: The ground truth includes a `try-except` block for `FileNotFoundError`. The prompt implicitly assumes the file is present, but robust code would handle this. While not explicitly asked for, it's a good practice demonstrated by the ground truth.

Overall, the code is very good and functional. The minor omissions relate more to best practices and robustness rather than direct failure to meet explicit requirements.",gpt-4.1-nano,LLM_judge_config
H26,hard,Clustering,iris.csv,"import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('iris.csv')

# Select features
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
X = df[features]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='au...",8,"The generated code correctly addresses the majority of the requirements. It successfully loads the data, selects and standardizes features, applies K-Means and PCA, and generates two scatter plots as requested.

**Strong points:**
* Correctly loads data and selects features.
* Correctly applies `StandardScaler`.
* Correctly uses `KMeans` with specified parameters (`n_clusters=3`, `n_init='auto'`, `random_state=42`).
* Correctly applies `PCA` to 2 components.
* Generates both required scatter plots.
* Correctly uses `LabelEncoder` for true species visualization.
* Appropriate use of `matplotlib.pyplot` for plotting.

**Areas for Improvement:**
* **Plot Clarity (Species Plot):** The `colorbar` for the true species plot (`cmap='Set1'`) uses numeric labels (0, 1, 2) which are less intuitive than actual species names. The ground truth code demonstrates how to use `inverse_transform` to display the actual species names in the colorbar, which significantly improves readability and understanding for the user.
* **Plot Title Consistency:** The titles are good, but could be slightly more descriptive or consistent with the prompt's examples (e.g., ""PCA of Iris Dataset: Points Colored by K-Means Cluster Labels"" vs. ""K-Means Clusters on PCA Components""). This is a minor stylistic point.
* **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError`, which is good practice for robustness. The generated code does not include this, which is a minor omission but not a critical failure for a benchmark task assuming the file is present.

Overall, the generated code is highly functional and meets most of the requirements. The primary area for improvement is enhancing the readability of the true species plot through a more informative colorbar.",gpt-4.1-nano,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"import pandas as pd
from sklearn.model_selection import cross_val_score, KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# Load dataset
df = pd.read_csv('iris.csv')

# Feature engineering: create PetalArea and SepalArea
df['PetalArea'] = df['PetalLengthCm'] * df['PetalWidthCm']
df['SepalArea'] = df['SepalLengthCm'] * df['SepalWidthCm']

# Define features
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'PetalA...",9,"The generated code is highly correct and complete, addressing all requirements of the task.

**Strong points:**
- **Correct Feature Engineering:** Accurately calculates `PetalArea` and `SepalArea`.
- **Correct Feature Selection for X:** Uses the specified six features (`SepalLengthCm`, `SepalWidthCm`, `PetalLengthCm`, `PetalWidthCm`, `PetalArea`, `SepalArea`) for `X`.
- **Correct Target Encoding:** Properly uses `LabelEncoder` for the `Species` column.
- **Correct Model Initialization:** Initializes `GradientBoostingClassifier` with `n_estimators=100` and `random_state=42`.
- **Correct Cross-validation:** Performs 5-fold cross-validation using `cross_val_score` and reports the mean accuracy.
- **Proper Imports:** Includes all necessary imports.
- **Concise and Readable:** The code is well-structured and easy to understand.

**Minor Improvement:**
- **Error Handling (File Not Found):** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. While not explicitly mandated by the prompt, robust code often includes such handling. The generated code assumes the file will always be present, which could lead to a crash if 'iris.csv' is missing. This is a very minor point and doesn't detract from the core task completion.
- **Explicit `KFold` vs. Implicit `cv=5`:** The generated code explicitly defines `KFold` with `shuffle=True` and `random_state=42` before passing it to `cross_val_score`. The problem statement simply asked for ""5-fold cross-validation,"" which can be achieved by simply setting `cv=5` (default `KFold` behavior in `cross_val_score` is not shuffled unless `shuffle=True` is set in the `KFold` object passed - the prompt did not specify shuffling, but it's good practice for reproducibility if splitting). The ground truth implicitly defaults to shuffling if `cv` is an int. Given `random_state=42` is used for the model, ensuring reproducible splits is a reasonable extension of the prompt and not a flaw. However, for a simple ""5-fold"", `cv=5` is sufficient and potentially more direct according to the minimal prompt requirements. The explicit `KFold` is not incorrect, just slightly more verbose than necessary for the given prompt requirements.",gpt-4.1-nano,LLM_judge_config
H28,hard,Classification,heart.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load dataset
df = pd.read_csv('heart.csv')

# Define features and target
feature_cols = ['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 
              ...",9,"The generated code correctly implements all specified requirements, including loading data, defining features and target, one-hot encoding categorical features, scaling numerical features, splitting data with the correct ratio and random state, training a RandomForestClassifier with the specified parameters, and evaluating the model using accuracy, macro-averaged precision, recall, F1-score, and ROC-AUC.

The use of `ColumnTransformer` is appropriate for handling mixed data types. The code also correctly processes the training and testing data separately to avoid data leakage.

One minor improvement, although not strictly a requirement for the task or a major flaw, is that the ground truth code utilizes `Pipeline` which is generally considered best practice for encapsulating preprocessing and modeling steps, especially in more complex scenarios or for deployment. This makes the code cleaner, less prone to errors regarding data transformation order, and ensures that transformations learned on the training data are consistently applied to new data. The generated code performs these steps manually (`fit_transform` then `transform`).

Another minor point, the ground truth includes `stratify=y` in `train_test_split`, which is good practice for classification tasks to ensure balanced class distribution in train/test sets, especially with imbalanced datasets. While not explicitly asked for, it's a valuable addition for robust model evaluation. The generated code omits this.

Overall, it's a very good solution that fulfills the prompt's requirements effectively.",gpt-4.1-nano,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score

# Load dataset
df = pd.read_csv('heart.csv')

# Select specific numerical features
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']
X = df[features].copy()

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_trans...",9,"The generated code correctly implements all specified requirements: data loading, feature selection and standardization, determining K using both elbow and silhouette methods, applying K-Means, performing PCA, and visualizing the clusters.

One minor area for improvement is the selection of `optimal_k`. While choosing the K with the highest silhouette score is a valid heuristic, the ground truth code explicitly states that ""one would visually inspect these plots to make the most informed decision"" and then makes an *example* choice (`chosen_k = 3`). The generated code automates the choice solely based on the highest silhouette score, which might not always align with a nuanced visual interpretation of the elbow method. However, for a programmatic benchmark, this automated selection is acceptable and often desired.

Another minor point is the `try-except` block for `FileNotFoundError` in the ground truth, which is a good practice for robustness but was not strictly requested in the prompt's requirements. The generated code omits this error handling.

Overall, the code is very clean, well-structured, and directly addresses the prompt. The plotting and legend generation are also well-handled.",gpt-4.1-nano,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report

# Load dataset
df = pd.read_csv('winequality-red.csv')

# Create binary target: 'good' >=7, 'bad' <7
df['quality_binary'] = df['quality'].apply(lambda x: 1 if x >= 7 else 0)

# Drop original 'quality' column
X = df.drop(...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, creates the binary target, handles missing values (though it assumes `X` is still a DataFrame after `fit_transform`), scales features, splits data, trains a logistic regression model, evaluates it, and reports feature importance.

**Strong points:**
- Correctly implements all steps from the prompt.
- Uses appropriate `sklearn` modules for each step.
- Handles missing values with `median()` and scales features using `StandardScaler`.
- Performs stratified split for `train_test_split`, which is good for imbalanced classes.
- Clearly presents evaluation metrics and feature importance.
- Uses `abs()` for sorting feature importance, which is correct for magnitude.

**Minor areas for improvement (to achieve a 10):**
- The missing value handling `X = X.fillna(X.median())` occurs *before* scaling. If `X` were not a DataFrame at this point (e.g., if a previous step had converted it to a numpy array), this line would fail. However, in this specific code, the `X` remains a DataFrame right before this line, so it works, but it's a subtle point related to the sequence of operations. It's generally safer to handle `fillna` *before* converting to `numpy` array (which `fit_transform` does). The ground truth handles it correctly by explicitly checking for nulls and then scaling *after* handling.
- The `X_scaled` array is used directly for training, which is fine, but when reporting feature importance, the original `X.columns` are used. This works because the order remains the same, but for robust code, creating a `pd.DataFrame` from `X_scaled` with column names (as done in the ground truth for `X_train_scaled_df` and `X_test_scaled_df`) can sometimes make debugging and consistency checks easier, especially if intermediate steps were to alter column order. However, for Logistic Regression coefficients, this is not strictly necessary as long as the feature order is maintained.
- Does not include error handling for `FileNotFoundError` when loading the CSV, unlike the ground truth. While not explicitly asked in the prompt, it's good practice.
- The reporting of feature importance uses `.to_string(index=False)` which is good for readability, but the formatting could be slightly more explicit (e.g., mentioning whether the coefficient is positive or negative, or why `max_iter` was chosen, though the problem didn't ask for explanations, only code).",gpt-4.1-nano,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('heart.csv')

# 1) Create a box plot for Oldpeak by HeartDisease
plt.figure(figsize=(8,6))
sns.boxplot(x='HeartDisease', y='Oldpeak', data=df)
plt.xlabel('Heart Disease (0=No, 1=Yes)')
plt.ylabel('Oldpeak')
plt.title('Distribution of Oldpeak by Heart Disease St...",9,"The generated code correctly addresses all parts of the task.

**Strong points:**
- It loads the data, creates the specified box plot correctly.
- It performs logistic regression, extracts the coefficient, and correctly calculates the odds ratio with a good interpretation.
- It successfully discretizes `Oldpeak` using `pd.qcut` into three bins and calculates the proportion of `HeartDisease=1` within each bin.
- All necessary imports are present.

**Areas for minor improvement:**
- **Robustness for file loading:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice for robust code. The generated code assumes the file exists.
- **Interpretation detail:** The ground truth provides a slightly more detailed interpretation of the odds ratio, including whether it's an increase or decrease and the percentage change. While minor, it adds clarity.
- **Visualizing proportions:** The ground truth also includes an optional bar plot to visualize the proportions, which is helpful for understanding the results. The generated code only prints them.
- **Handling `qcut` errors:** The ground truth includes a `try-except` block for `pd.qcut` to handle potential `ValueError` if there are too many identical values, which is a good robustness measure.

Overall, the generated code is very good and largely fulfills the requirements. The minor points are about robustness and presentation that are present in the ground truth but not strictly required by the prompt, thus a score of 9 is appropriate.",gpt-4.1-nano,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

# 1) Load data
df = pd.read_csv('spotify_2023.csv')

# 2) Convert 'streams' to numeric and drop NaNs
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df...",6,"The generated code addresses most of the task requirements but has a significant issue with feature handling and preprocessing, leading to incorrect model training and evaluation.

**Strong points:**
*   Correctly loads data, converts `streams` to numeric, and handles NaNs.
*   Correctly creates the `is_popular` target variable using the 75th percentile.
*   Identifies the correct numerical features.
*   Correctly uses `train_test_split` with `random_state`.
*   Initializes `LogisticRegression` with `max_iter=1000` and `random_state`.
*   Correctly calculates and prints F1-score (macro), ROC AUC score, and confusion matrix.
*   Includes all necessary imports.

**Areas for improvement/Corrections:**
*   **Incorrect Feature Handling for `ColumnTransformer`**: The task explicitly states to use the specified numerical features (`danceability_%`, `energy_%`, etc.) AND one-hot encoded `key` and `mode`. The generated code initializes `numeric_features` with the same list as `features`, which is correct. However, it then attempts to apply `StandardScaler` to `numeric_features` and `OneHotEncoder` to `categorical_features` within a `ColumnTransformer`. The issue is that `numeric_features` are duplicated as they are passed through the `ColumnTransformer`. More critically, the `ColumnTransformer` is applied to `X_train` which *only* contains the `numeric_features`. It does *not* contain `key` and `mode` at the point of `ColumnTransformer` application because `X = df[features].copy()` only selected the initial numerical features.
    
    To fix this, `X` should initially contain *all* features (`features` + `categorical_features`) before `train_test_split`. Then, the `ColumnTransformer` correctly identifies and transforms the specified columns within `X_train` and `X_test`.
*   **Applying `ColumnTransformer`**: The generated code applies `preprocessor.fit_transform(X_train)`. This returns a NumPy array. When `model.fit(pipeline, y_train)` is called, the model is trained on this transformed NumPy array. However, `X_test_transformed` is also a NumPy array. While the model can handle NumPy arrays, the approach for `ColumnTransformer` with `LogisticRegression` is typically to put the `ColumnTransformer` *inside* a `Pipeline`. This ensures consistent preprocessing for both training and testing data and correctly links feature names (especially important if using a model that inspects feature names, though not strictly necessary for Logistic Regression after one-hot encoding).
*   **One-Hot Encoding `drop='first'`**: The ground truth and common practice for classification models often avoid `drop='first'` for one-hot encoding unless specifically required to prevent multicollinearity, as some models (like Logistic Regression with regularization) can handle it. The task did not specify `drop='first'`, so just `OneHotEncoder()` without `drop='first'` would be closer to a default expectation or the ground truth.
*   **Lack of `random_state` in `LogisticRegression`**: The task requirement was to use `LogisticRegression` with default parameters + `max_iter=1000`. The ground truth added `random_state=42` which is good practice for reproducibility, but not explicitly stated in the prompt for the model itself, only for `train_test_split`. The generated code correctly included `random_state=42` for the model, which is an improvement.

The main reason for the lower score is the fundamental error in how features are handled with the `ColumnTransformer`. If `key` and `mode` are not part of `X` when the `ColumnTransformer` is applied, they are never passed to the one-hot encoder, meaning the model is trained on an incorrect feature set, rendering the subsequent evaluation metrics meaningless for the full task.",gpt-4.1-nano,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('spotify_2023.csv')

# Select specified audio features
features = [
    'danceability_%', 'energy_%', 'valence_%', 
    'acousticness_%', 'instrumentalness_%', 
    'liveness_%', 'speechiness_%', 'bpm'
]
df_features = df[features]

# Drop rows with NaNs in selected features
df_clean = ...",8,"The generated code largely fulfills the requirements of the task.

**Strong Points:**
*   **Correct Feature Selection and NaN Handling:** Correctly identifies and selects the specified audio features, and handles NaNs by dropping rows with `df_features.dropna()`.
*   **Correct Standardization:** Uses `StandardScaler` on the cleaned data.
*   **Correct K-Means Application:** Applies K-Means with K=4, `random_state=42`, and `n_init='auto'`.
*   **Correct PCA Application:** Applies PCA to 2 components with `random_state=42`.
*   **Correct Plotting:** Creates a scatter plot of the two principal components, coloring points by their K-Means assigned cluster labels, and adds a title and labels.
*   **Mean Feature Calculation:** Correctly calculates and displays the mean values of the original (unstandardized) selected audio features for each cluster.

**Areas for Improvement (Why not a 9 or 10):**
*   **Missing Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code assumes the file exists.
*   **Slightly Less Robust Mean Calculation:** While functionally correct, the `for` loop iteration for calculating cluster means is a bit less idiomatic Pandas than directly using `groupby().mean()` as seen in the ground truth. The `groupby()` approach is generally more efficient and concise for this type of operation.
*   **Plotting Legend:** The `plt.legend(*scatter.legend_elements(), title='Clusters')` attempts to add a legend, but for a scatter plot where colors represent distinct categorical labels (clusters), a colorbar is often more effective and explicitly requested implicitly by ""coloring points by their K-Means assigned cluster labels"". The ground truth uses a colorbar, which is clearer for distinguishing between cluster labels based on color. The current legend will only show one entry with the scatter markers.
*   **Clarity on Original Data for Mean Calculation:** The line `df_results = df.loc[df_clean.index].copy()` correctly retrieves the original unstandardized data for the rows that weren't dropped by `dropna()`. However, the ground truth's approach of `df_original_for_analysis = df_features.copy()` *before* scaling and then adding the cluster labels to this copy is slightly more direct in its intent for preserving the original data linked to the analysis subset. The current method is functionally equivalent but might be slightly less intuitive at first glance.",gpt-4.1-nano,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load data
df = pd.read_csv('spotify_2023.csv')

# Step 2: Convert 'streams' to numeric, coerce errors to NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# Step 3: Filter data
filtered_df = df[
    (df['streams'] > 10_000_000) &
    (df['released_year'] >= 2010) &
    (df['released_year'] <= 2023)
].dropna(subset=['streams'])

# Step 4: Group by 'released_year'
grouped = filtered_df.groupby('released_year'...",9,"The generated code correctly addresses all aspects of the task, including loading data, converting streams, filtering based on streams and year, grouping by year, calculating averages, and generating all three required plots with appropriate titles and labels. The structure is clear and proper imports are used.

One minor area for improvement is the order of filtering. The generated code applies all filters (`streams` > 10M, `released_year` range) and *then* drops NaNs from `streams` (`.dropna(subset=['streams'])`). While this works for the current logic because `pd.to_numeric(..., errors='coerce')` assigns NaN to non-numeric `streams` *before* any filtering, a more robust approach (as shown in the ground truth) would be to drop `streams` NaNs *immediately after* conversion and *before* applying numerical filters (like `streams > 10M`), to ensure that the numerical comparison is only performed on valid numeric data. In this specific case, it doesn't cause an error, but it's a good practice for data processing. Also, adding `try-except` for file loading would make it more robust. The plot layout (3 rows, 1 column) is also a valid interpretation, though the ground truth's (1 row, 3 columns) might be slightly more compact horizontally. However, this is a style choice rather than a correctness issue.",gpt-4.1-nano,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder
import numpy as np

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Define a function to clean numeric columns
def clean_numeric(column):
    return pd.to_numeric(column.astype(str).str.replace(',', '', regex=False), errors='coerce').fillna(0)

...",9,"The generated code is largely correct and addresses most of the requirements.

**Strong Points:**
- Correctly loads the dataset.
- Implements a clean and reusable `clean_numeric` function for `Likes`, `Dislikes`, and `Video views`, handling commas, conversion to numeric, and NaN filling with 0. This is a good approach.
- Correctly handles the `published` column by extracting the year and filling NaNs with 0.
- Correctly fills missing `Category` values with 'Unknown' and applies one-hot encoding using `pd.get_dummies`.
- Defines `X` and `y` using the specified columns.
- Performs the train-test split with the correct `test_size` and `random_state`.
- Trains a `LinearRegression` model.
- Evaluates the model using R-squared and MAE as requested.
- Includes all necessary imports.

**Areas for Minor Improvement:**
- **Robustness in `published_year` parsing:** The `published_year` parsing logic is a bit complex and might not be entirely robust for all possible `published` column formats if they are not consistently a date string. The prompt implies treating the year as a numeric feature, which suggests that a direct conversion to `pd.to_datetime` and then `dt.year` is the most straightforward and robust way if 'published' generally implies dates. The current code attempts `format='%Y-%m-%d'` selectively, and then a `to_numeric` fallback. The ground truth's simpler `pd.to_datetime` and `dt.year` (with `errors='coerce'`) then `fillna(0)` is generally sufficient and clearer if `published` means a date string.
- **Handling of `published` column:** If the `published` column did *not* contain date information but was some other numeric year, the current parsing (`pd.to_datetime(...).dt.year`) might not work as intended for such cases. The prompt just asks to treat the `published` column (year) as a numeric feature, so extracting the year from a date seems the most logical interpretation given a typical 'published' column in a dataset like this. The ground truth handles it assuming it's a date and extracting the year. The generated code also tries a specific format, then falls back to `to_numeric` on the original column if datetime conversion fails. This is slightly more complex than necessary given the prompt's implied simple year extraction.
- **Explicit Type Casting for Cleaned Numeric Columns:** While `clean_numeric` returns floats (due to `fillna(0)`), the prompt doesn't explicitly state the final type for cleaned `Likes`, `Dislikes`, `Video views`. The ground truth explicitly converts them back to `int` after cleaning, which might be preferred for counts. This is a minor point.
- **Output formatting:** The prompt asks for `""{r2:.4f}""` and `""{mae:.2f}""` formatting for the print statements. The generated code prints without specific formatting. This is a very minor aesthetic point.

Overall, the code is very good and functional. The minor points are mostly about slightly increasing robustness and adhering to subtle formatting details.",gpt-4.1-nano,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Video views' column
df['Video views'] = df['Video views'].astype(str).str.replace(',', '', regex=False)
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce').fillna(0)

# Ensure 'published' year is integer
df['published'] = pd.to_numeric(df['published'], errors='coerce').astype('Int64')

# Group by 'published' year
grouped = df.groupby('published')...",7,"The generated code correctly addresses most of the task requirements.

**Strong Points:**
- **Correct Data Loading:** Successfully loads the CSV.
- **Accurate 'Video views' Cleaning:** Handles comma removal, conversion to numeric, and NaN filling correctly.
- **Correct Grouping and Metric Calculation:** Accurately groups by year and calculates both average views and unique video counts.
- **Proper Plotting:** Uses two subplots, correct data for each, and includes titles and labels.

**Areas for Improvement (Why not higher):**
- **Inaccurate 'published' Column Handling:** The task specifies the 'published' column (year) should be treated as numeric. The dataset typically has a `publishedAt` column with full datetime strings, not just a `published` column with years. The ground truth correctly infers this and extracts the year from `publishedAt`. The generated code attempts to convert a non-existent 'published' column directly to numeric, which would likely fail or produce all NaNs depending on the actual CSV columns, or if 'published' refers to `publishedAt` it would fail `to_numeric` directly. This is a critical logical flaw based on the typical structure of such data. If `youtube_trendings.csv` has a column specifically named `published` which contains only years, then this part would be correct, but it's an uncommon naming convention for such a task. Assuming `publishedAt`, it's incorrect.
- **Lack of Error Handling:** No `try-except` block for file loading, which is good practice.
- **Implicit Column Name:** Relies on a column named `published`, which might not be present or might mean `publishedAt` (as in the ground truth). If the intent was for a column named `publishedAt`, then extract the year from `publishedAt`.
- **Minor Plotting Enhancements:** While functional, the plots could benefit from minor touches like setting proper x-axis ticks for years (as the ground truth does by `set_xticks(yearly_trends['published_year'].unique())`) and removing `sharex=True` if years are not continuous or need independent spacing. For clarity, a `plt.suptitle` for the entire figure would also be beneficial.

Overall, it's a good attempt, but the ambiguity/incorrectness in handling the 'published' column based on common YouTube dataset structures is a significant drawback.",gpt-4.1-nano,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

# Load dataset
df = pd.read_csv('youtube_trendings.csv')

# 1) Create 'title_word_count' feature
df['title_word_count'] = df['Video'].apply(lambda x: len(str(x).split()))

# 2) Clean 'Video views' column
df['Video views'] = df['Video views'].astype(str).str.replace(',', '', regex=False)
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerc...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Correctness:** All steps (loading data, `title_word_count`, `Video views` cleaning, correlation, filtering by year, word processing, most common words) are implemented and produce correct results.
- **Completeness:** All specified features and transformations are included.
- **Code Quality:** Imports are proper. The code is well-structured and easy to follow.
- **Error Handling (Implicit):** Uses `errors='coerce'` for `pd.to_numeric` and `fillna(0)` for `Video views`, which is robust. Converts 'Video' column to string before processing.

**Areas for Minor Improvement:**
- **NLTK Download:** The `nltk.download('stopwords')` call should ideally be wrapped in a `try-except` block with `nltk.data.find()` as shown in the ground truth. This prevents unnecessary downloads if stopwords are already present and handles potential download errors more gracefully.
- **Missing `isalpha()` check:** In the word processing for common words, the ground truth includes an `isalpha()` check (`word.isalpha()`) when filtering words. This helps to remove any numeric strings or mixed alphanumeric strings that might remain after punctuation removal but are not meaningful ""words"" in a text analysis context. The generated code only checks `if w not in stop_words`. This is a minor point, but it can refine the word list.
- **Output Formatting:** The Pearson correlation printing does not format to four decimal places as in the ground truth, nor does it include the separator line, which is a minor aesthetic difference. The most common words output uses a different formatting. This is purely stylistic and doesn't impact correctness.

Overall, it's a very good solution, close to the ground truth.",gpt-4.1-nano,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score

# 1) Load data
df = pd.read_csv('mobiles_2025.csv')

# 2) Clean 'Launched Price (USA)'
def clean_price(price):
    if pd.isnull(price):
        return np.nan
    # Remove 'USD ' and commas
    price_str = str(price).replace('USD ', '').replace(',', '').strip()
    try:
        return float(pr...",9,"The generated code correctly implements most of the requirements.

**Strong Points:**
*   **Correct Implementation of Core Logic:** The code successfully loads data, cleans the 'Launched Price (USA)' column to create the `is_expensive_usa` binary target, handles NaNs correctly for the target, cleans all specified numerical features (`Mobile Weight`, `RAM`, `Battery Capacity`, `Screen Size`), and extracts MP values for `Front Camera` and `Back Camera`.
*   **One-Hot Encoding:** It correctly one-hot encodes `Company Name` and `Processor`.
*   **Feature and Target Definition:** `X` and `y` are correctly defined using the cleaned numerical and one-hot encoded categorical features.
*   **Data Splitting and Model Training:** The data is split correctly, and a `RandomForestClassifier` is trained with the specified parameters.
*   **Evaluation:** F1-score (macro) and ROC AUC are correctly calculated and printed.
*   **Robustness:** Includes checks for column existence before cleaning or one-hot encoding, which is good practice (though the ground truth might assume column existence).
*   **Readability:** The code is well-structured with comments matching the task's numbered steps.

**Areas for Improvement (minor issues / deviations from ground truth):**
*   **`is_expensive_usa` NaN Handling:** The generated code defines `is_expensive_usa` as `1 if pd.notnull(x) and x > 1000 else 0`. This means if `clean_price` returns `np.nan`, `pd.notnull(x)` will be `False`, and `is_expensive_usa` will become `0`. The ground truth explicitly states: ""Drop rows where `is_expensive_usa` is NaN."" This implies that rows with unparseable prices should result in `is_expensive_usa` being `NaN` *first*, and then those rows are dropped. The generated code effectively creates 0s for unparseable prices rather than dropping them, which deviates from the precise instruction ""Drop rows where `is_expensive_usa` is NaN."" However, it does drop rows where the `Launched Price (USA)_clean` (the intermediate step) is NaN, which achieves a similar outcome if 'Launched Price (USA)_clean' becomes NaN for unparseable entries. The ground truth's approach of setting `is_expensive_usa` to `np.nan` for unparseable original prices and then dropping rows based on *that* NaN value is more explicit about the problem statement's intent.
*   **`Launched Year` Handling:** The generated code includes `Launched Year` in `numerical_features` but then has a conditional `if 'Launched Year' not in df.columns:` block that sets it to `np.nan`. This is unnecessary because `Launched Year` is an input column and implicitly handled by `dropna` later. It's also not a cleaned feature; it's used as is.
*   **`re` import:** The `re` module is imported inside the `extract_camera_mp` function, which is inefficient. It should be imported at the top level with other imports.
*   **No error handling for CSV loading:** The ground truth includes a `try-except` block for `FileNotFoundError` during CSV loading, which is a good practice for robustness. The generated code lacks this.
*   **Renaming conventions:** The generated code uses `col_clean` for some features and `col_MP` for camera features, which is consistent. The ground truth uses `col_cleaned`. This is a minor stylistic difference, not a correctness issue.

These are mostly minor deviations or small improvements that could be made, but the core functionality is excellent.",gpt-4.1-nano,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Define the price cleaning function
def clean_price(price):
    if pd.isnull(price):
        return np.nan
    if isinstance(price, str):
        # Remove currency symbols
        for symbol in ['PKR', 'INR', 'CNY', 'USD', 'AED']:
            price = price.replace(symbol, '')
        # Remove commas
        price = price.replace(',', '')
        # Strip whitespace
        price = price.str...",8,"The generated code correctly addresses most of the requirements. It loads the dataset, defines and applies a cleaning function for price columns, calculates the average global price, and identifies the top 5 models.

**Strong points:**
- The `clean_price` function correctly handles NaN values, removes currency symbols and commas, and converts to numeric. It also includes error handling for non-string inputs or conversion failures.
- It correctly iterates through the specified price columns and applies the `clean_price` function.
- The calculation of `Average Global Price` correctly considers only available (non-NaN) price data for each model.
- It correctly identifies and displays the top 5 models sorted by `Average Global Price` in descending order.
- The use of `to_string(index=False)` for output is a nice touch for clean display.

**Areas for improvement:**
- **Efficiency in calculating `Average Global Price`:** The ground truth code uses `df[price_columns].mean(axis=1, skipna=True)`, which is a highly optimized, vectorized pandas operation. The generated code uses an explicit loop (`for idx, row in df.iterrows()`) to calculate the average price for each row. While functionally correct, this approach is significantly less efficient for large datasets compared to the vectorized method.
- **Error Handling for File Not Found:** The ground truth includes `try-except FileNotFoundError` for `pd.read_csv`, which is good practice for robust code. The generated code lacks this.
- **Redundancy in `clean_price`:** The `else` block in `clean_price` that attempts `pd.to_numeric(price)` is somewhat redundant if the initial `isinstance(price, str)` already covered the common cases and the `if pd.isnull(price)` handles NaNs. A simpler `try-except` structure might be more concise, as seen in the ground truth's `clean_price` function which handles the conversion in one `try-except` block after initial string processing.

Overall, it's a solid solution, but the efficiency of the mean calculation and file loading robustness are areas where it could be improved to match the ground truth's robustness and idiomatic pandas usage.",gpt-4.1-nano,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean 'RAM'
def clean_ram(ram):
    try:
        return float(ram.replace('GB', '').strip())
    except:
        return pd.NA

# Clean 'Battery Capacity'
def clean_battery(battery):
    try:
        return float(battery.replace('mAh', '').replace(',', '').strip())
    except:
        return pd.NA

# Clean 'Launched Price (USA)'
def clean_price(price):
    tr...",6,"The generated code correctly addresses most of the requirements but has a significant flaw in the plotting section that misinterprets the task's instructions for comparative analysis.

**Correctness:**
The data loading and cleaning steps are mostly correct. The use of `pd.NA` is acceptable for handling NaNs. The filtering for Apple products and ensuring 'Launched Year' is numeric are also handled. However, the plotting logic is incorrect. The task asks for box plots *per year* for each metric, meaning for each `Launched Year`, there should be three box plots (one for RAM, one for Battery Capacity, one for Price). The generated code attempts to create three subplots (one for each metric), but then tries to plot all years on a single x-axis within each metric's subplot which conflicts with the ""For each `Launched Year`, generate box plots"" instruction. This effectively shows the distribution of a metric across *all* years on one plot, grouped by year, which is different from showing *three distinct plots for each year*.

The `clean_battery` and `clean_price` functions attempt to remove commas in their `try` blocks, which is good.

**Code Quality:**
- **Imports:** Proper imports are included.
- **Readability:** Functions for cleaning are a good approach, although `df.apply` can be slower than vectorized string operations, for this scale of data, it is acceptable.
- **Error Handling:** The `try-except` blocks in cleaning functions are good for robustness. It correctly `dropna` for the cleaned columns.
- **Redundancy/Efficiency:** The `df.apply` for cleaning columns creates new columns (`RAM_Clean`, etc.) which is fine, but the intermediate `df_clean` assignment is also valid.
- **Plotting:** The plotting section is where quality significantly drops. The `plt.figure` and `plt.subplot` loop is set up for creating three subplots (one for each metric), but then it incorrectly draws a single box plot for each metric using `sns.boxplot(x=apple_df['Launched Year'].astype(str), y=apple_df[metric])`. This means for each of the three subplots, you get a view of that metric's distribution *across all years*, rather than three separate plots *for each specific year*. The `data` list collected inside the plotting loop is also unused.
- **Missing `plt.show()` per year:** The task asks for box plots ""For each `Launched Year`"". This implies showing plots for year X, then year Y, etc., requiring `plt.show()` within a loop over years, or generating separate figures per year. The generated code produces one figure with three subplots summarizing all years together.

**Completeness:**
- **Load dataset:** Yes.
- **Clean columns and handle NaNs:** Yes, mostly. The `RAM` cleaning function misses the comma removal that `clean_price` includes, but `RAM` typically doesn't have commas. Battery capacity cleaning function also doesn't explicitly remove commas, but the ground truth example suggests a comma can be in mAh. Still, the `replace('mAh', '').replace(',', '')` is a good practice.
- **Filter for Apple products:** Yes.
- **Group by Launched Year:** Yes, implicitly by using `apple_df['Launched Year'].astype(str)` for `x` in the box plot.
- **Generate box plots per year for all three metrics:** This is where it fails to meet the requirement. It generates plots for *all years grouped by year* on a single figure, not separate sets of plots *for each year*. The requirement is to show the distribution of RAM, Battery, and Price *for a specific year*, then for another specific year, and so on.
- **Appropriate titles and labels:** Titles are general (`{title} Distribution by Year`) and y-labels are correct. However, they don't indicate the *specific year* in the title, which is crucial if the individual plots were indeed going to be per year. If it were truly plotting distributions *for each year*, the year should be in the title or figure title.",gpt-4.1-nano,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_val_score, learning_curve, validation_curve
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.feature_selectio...",6,"The generated code attempts to address many aspects of the prompt but falls short in several critical areas, particularly regarding the advanced nature of preprocessing, the robustness of the pipeline design, and the depth of the evaluation.

**Strengths:**
- Includes comprehensive imports.
- Attempts multiple imputation with `IterativeImputer`.
- Incorporates polynomial features and interaction terms.
- Includes Random Forest, Gradient Boosting, SVM, and Neural Network models, and attempts GridSearchCV.
- Creates both Voting and Stacking ensembles.
- Attempts feature selection (SelectKBest, RFE).
- Includes learning curves and validation curves.
- Implements SHAP for model interpretability.
- Provides a summary of metrics.

**Areas for Improvement:**
1.  **Data Preprocessing Pipeline:**
    *   **Lack of a unified pipeline:** The biggest flaw is the lack of a proper `ColumnTransformer` and `Pipeline` for preprocessing *before* the model fitting. The generated code performs all preprocessing steps (imputation, scaling, one-hot encoding, feature engineering) *before* `train_test_split`. This leads to data leakage (e.g., `IterativeImputer` and `StandardScaler` are fitted on the entire dataset, including test data) and makes it impossible to correctly integrate feature engineering *within* a scikit-learn pipeline for hyperparameter tuning and cross-validation. The ground truth correctly uses a `ColumnTransformer` and a `Pipeline` for this.
    *   **Feature Engineering:** While it attempts feature engineering (Title, FamilySize, Age_Fare_Interaction, Surname), the `FeatureEngineerTransformer` in the ground truth is a more robust and pipeline-compatible way to manage this, ensuring these steps are applied consistently within preprocessing steps for any model. Surname extraction is also less impactful than the ground truth's text mining for `Name` (e.g., NameLength, Is_Married from title).
    *   **Handling `PassengerId`, `Ticket`, `Cabin`:** The generated code doesn't explicitly drop or process `PassengerId` or `Ticket`, and `Cabin` is entirely ignored, whereas the ground truth processes `Cabin` into `Deck` and drops `Ticket`.
    *   **Multiple Imputation:** The `IterativeImputer` is applied to `df[numeric_features]` directly before `train_test_split`, which is data leakage. It should be part of a pipeline to be fitted *only* on training data.

2.  **Model Tuning and Integration:**
    *   **XGBoost not included:** The prompt specifically requested XGBoost, but it's missing from the base models and ensembles.
    *   **`IterativeImputer` and `PolynomialFeatures` outside pipeline:** Because these steps are run independently on the full `X` before splitting, they cannot be part of the `GridSearchCV`/`RandomizedSearchCV` process or cross-validation in a robust way, leading to potential data leakage and inconsistent results. In the generated code, `X` is already heavily preprocessed before being passed to `GridSearchCV`. The ground truth integrates all preprocessing steps (including `IterativeImputer` and `PolynomialFeatures`) into a `Pipeline` that is then passed to `RandomizedSearchCV`, ensuring proper cross-validation without leakage.

3.  **Feature Selection:**
    *   **Incorrect `RecursiveFeatureElimination` import:** It uses `RecursiveFeatureElimination` instead of `RFE`.
    *   **Application to full `X`:** Feature selection (`SelectKBest`, `RFE`) is applied to `X` (the full preprocessed dataset) before splitting, again leading to data leakage. Feature selection should generally be applied within a pipeline, or at least only on the `X_train` data.
    *   **Comparison of impact:** While it notes ""Evaluate impact,"" it only selects one (`X_kbest`) and retrains a single RF model. The prompt requested comparing the *impact* of multiple techniques, which implies a more structured comparison of model performance *after* using different feature selection methods.

4.  **Extensive Model Evaluation:**
    *   **Limited CV for base models:** The `evaluate_model` function correctly performs CV, but it does so on `X` (the *full* preprocessed data, including test data), not just `X_train`. This is data leakage.
    *   **Missing Statistical Significance Tests:** The prompt heavily emphasizes ""Statistical Significance Tests"" to compare models. The generated code mentions `DeLong test` but does not implement it, which was a specific requirement. The ground truth correctly implements paired t-tests on CV scores.
    *   **No Validation Curves for other models/hyperparameters:** Only one validation curve for `n_estimators` of `RandomForestClassifier` is shown, which is okay for demonstration, but the prompt implies exploring this more extensively.

5.  **Model Interpretability:**
    *   **Missing LIME:** The prompt specifically requested ""SHAP or LIME"". While SHAP is implemented, LIME is entirely missing.
    *   **SHAP for single instance:** The `shap.force_plot` is correctly called, but its results usually need proper rendering setup.

6.  **Code Quality/Minor issues:**
    *   The `extract_title` function uses specific title strings in the regex like `[Miss]\.`, `[Ms]\.`, which should just be `Miss\.`, `Ms\.` etc., without the brackets if they are literal. Also, `Master` is included in `rare_titles` when it's a very common title for young boys and often treated as its own group.
    *   The `Surname` feature seems incomplete and its utility isn't integrated into the model or evaluated.
    *   The way `X` is constructed by `pd.concat` could become problematic if column order isn't strictly maintained or if there are index mismatches (though `reset_index` helps). A `ColumnTransformer` is much safer.
    *   The `models` dictionary is inconsistent, storing `GridSearchCV` objects initially, then extracting `best_estimator_`.

The generated code demonstrates an understanding of the components required but struggles significantly with the robust and leakage-free integration of these components using `scikit-learn`'s best practices, especially with pipelines and proper splitting. The lack of statistical tests and LIME, along with issues in preprocessing, pulls the score down.",gpt-4.1-nano,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Define features and target
X = df[['Year', 'Runtime']]
y = df['Rating']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Initialize and train the Linear Regres...",8,"The generated code correctly implements all the explicit requirements of the task: loading the data, defining features and target, splitting data, training a Linear Regression model, and evaluating it with R2 and MAE. The imports are proper, and the code structure is clear and logical.

However, the ground truth code includes crucial data preprocessing steps (handling ' min' in 'Runtime', converting types, and handling missing/invalid values) which are essential for a robust regression model with this specific dataset. The generated code implicitly assumes the data is perfectly clean and ready for use, which is often not the case in real-world scenarios, especially with free-form text columns like 'Runtime'. While the prompt did not explicitly ask for robust error handling or data cleaning for specific column formats, a robust solution would include it. The lack of these preprocessing steps would likely lead to errors if the `imdb_top_250.csv` file contains values like ""100 min"" in the 'Runtime' column or non-numeric values in 'Year' or 'Rating', which it often does.",gpt-4.1-nano,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Select features
features = df[['Rating', 'Runtime']]

# Standardize features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
clusters = kmeans.fit_predict(scaled_features)

# Ad...",7,"The generated code correctly addresses most of the requirements. It loads the data, selects and standardizes features, applies K-Means with the specified parameters, and plots the results. It also calculates and prints the mean rating and runtime per cluster.

However, there are a few areas for improvement:
1.  **Data Cleaning:** The most significant missing piece is handling the `Runtime` column. The prompt implies that `Runtime` might be a string (e.g., '142 min') and requires conversion to numeric. The ground truth code includes robust `Runtime` and `Rating` cleaning (removing ' min', converting to numeric, handling `NaN`s). The generated code assumes `Runtime` and `Rating` are already in a clean, numeric format, which is often not the case with raw CSV data from sources like IMDb. This can lead to errors or incorrect results if the 'Runtime' column is not purely numeric.
2.  **Plotting:**
    *   The `plt.legend(*scatter.legend_elements(), title='Clusters')` line is a good attempt at creating a legend, but a `colorbar` is usually more appropriate and informative for cluster labels in a scatter plot. The ground truth uses `plt.colorbar`.
    *   The `Runtime` column might contain non-numeric data if the data cleaning isn't done, which would also break the scatter plot.
3.  **Robustness:** The ground truth includes `try-except FileNotFoundError` and checks for column existence, which makes the code more robust to common issues. The generated code lacks these checks.
4.  **Clarity of Output:** While the mean calculation is correct, the ground truth's `groupby()` approach is generally more concise and idiomatic for this task in pandas than iterating through unique cluster labels.

Overall, it's a good attempt, but the lack of data cleaning for `Runtime` and `Rating` is a critical omission that would prevent it from running correctly with the expected `imdb_top_250.csv` format described in the context of such datasets.",gpt-4.1-nano,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import re

# Ensure NLTK stopwords are downloaded
try:
    stop_words = set(stopwords.words('english'))
except LookupError:
    nltk.download('stopwords')
    stop_words = set(stopwords.words('english'))

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# Load dataset
df = pd.read_csv('imdb_top_250.csv')...",7,"The generated code correctly addresses most of the requirements.

**Strong points:**
*   Correctly loads the dataset and creates the `title_length` feature.
*   Calculates and prints the Pearson correlation.
*   Handles NLTK downloads for `stopwords` and `punkt`.
*   The preprocessing function `preprocess_title` correctly performs lowercasing, punctuation removal (using `re.sub`), and tokenization.
*   Removes stopwords successfully.
*   Identifies and prints the 10 most common words with their frequencies using `collections.Counter`.

**Areas for improvement:**
1.  **Correlation Calculation Robustness:** The ground truth code handles potential non-numeric values in the `Rating` column by using `pd.to_numeric` with `errors='coerce'` and then dropping NaN values. The generated code directly calls `corr()` without this validation, which might fail or produce incorrect results if `Rating` contains non-numeric entries or NaNs that aren't automatically excluded by `corr()`'s default `min_periods` behavior. While `corr()` typically ignores NaNs, explicit handling makes the code more robust and clear about assumptions.
2.  **`title_length` `astype(str)`:** For `str.len()`, `astype(str)` is redundant if the 'Title' column is already guaranteed to be strings. It doesn't hurt, but isn't strictly necessary.
3.  **Order of Checks for NLTK Downloads:** The `try-except` block for downloading stopwords in the generated code runs `set(stopwords.words('english'))` *before* the `try-except` for download, which means the `LookupError` for stopwords would be caught by the *first* `try-except`, and then `nltk.download('stopwords')` would execute. The `nltk.data.find('tokenizers/punkt')` check is good. Rearranging the stopwords check to be similar to `punkt` (i.e., `nltk.data.find('corpora/stopwords')`) or using the `try-except` block around the actual `stopwords.words` call for the initial check is more idiomatic. The ground truth's approach of `try-except` directly around `stopwords.words('english')` is simpler and effective.
4.  **Efficiency of Preprocessing Loop:** The generated code uses a direct `for` loop to iterate through `df['Title']` and append to `all_tokens`. While functional, applying a function to a DataFrame column (as shown in the ground truth with `df.apply`) is often morePandas-idiomatic and potentially more optimized for larger datasets, especially if not pre-calculating intermediate columns, but for this specific problem, both are acceptable. The ground truth creates intermediate columns which can be useful for debugging or further analysis, but the generated code's more direct approach to `all_tokens` is fine.

Overall, the generated code fulfills the core requirements well, with minor improvements possible for robustness and Python/Pandas idiomatic practices. If the input data is guaranteed clean, the current correlation calculation is sufficient.",gpt-4.1-nano,LLM_judge_config
H43,hard,Classification,penguins.csv,"import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Load dataset
df = pd.read_csv('penguins.csv')

# Define features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island...",6,"The generated code attempts to address all requirements but has several critical issues that prevent it from being fully correct and robust.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly identifies numerical and categorical features.
*   Uses `SimpleImputer` for numerical features (median strategy).
*   Correctly uses `LabelEncoder` for the target variable.
*   Uses `RandomForestClassifier` with specified parameters.
*   Performs 5-fold cross-validation and prints the mean accuracy.
*   Proper imports are included.

**Weaknesses and Areas for Improvement:**

1.  **Imputation of Categorical Features (Direct Modification of X):**
    *   The `fillna` for categorical features modifies `X` directly *before* the `ColumnTransformer`. This is generally discouraged in `scikit-learn` pipelines for consistency and to avoid potential `SettingWithCopyWarning` issues.
    *   More importantly, the `SimpleImputer` is designed to handle this within a pipeline. The problem explicitly asks for preprocessing *with a pipeline* if possible or standard `sklearn` practices.
    *   The special handling for `island` (dropping rows if still NA) is not fully compliant with the prompt's condition: ""if mode imputation results in NA for island, drop those rows."" The generated code's `X = X[~X['island'].isna()]` only works if `fillna` with 'Unknown' wasn't applied, or if 'Unknown' was not introduced, which it was. This means it might drop valid 'Unknown' entries or miss actual NAs if 'Unknown' is treated as a valid category. The logic is a bit convoluted and doesn't exactly match the requirement's handling for *true NAs* after imputation. The `mode_value.empty` check for all-NA categories followed by `fillna('Unknown')` doesn't fully align with the instruction to *drop rows* if mode imputation yields NA (which means the mode itself is NA, a rare edge case for strings but possible with an empty `mode()` result). The ground truth's handling for an entirely NA `island` column (dropping all rows) is more precise to the prompt.

2.  **Incorrect Pipeline `ColumnTransformer` (Scaling and One-Hot Encoding):**
    *   The `ColumnTransformer` is defined with `'cat', OneHotEncoder(drop='first', handle_unknown='ignore')`. `drop='first'` is *not* requested by the prompt. The prompt asks for one-hot encoding, implying full one-hot encoding without dropping the first column unless specified.
    *   Crucially, `StandardScaler` is placed *after* the `ColumnTransformer` in the main pipeline, but only for `remainder='passthrough'`. This means that numerical features, after passing through `remainder='passthrough'`, would be scaled, but the *new* one-hot encoded columns (from categorical features) would *not* be scaled. Scaling applied to one-hot encoded features is generally incorrect and can lead to misleading results, but more critically, the numerical features *already in `X`* are what need scaling, not the *remainder*. The `remainder='passthrough'` sends the numerical features *through* the `ColumnTransformer`, but `StandardScaler` is then applied globally. This setup is flawed for applying `StandardScaler` only to numerical features. The `numerical_features` first get median imputed, but then they are passed through `remainder='passthrough'` and subsequently scaled. The `ColumnTransformer` should typically handle *all* preprocessing steps for different column types if using this structure. `StandardScaler` should ideally be applied *only* to the pre-identified numerical features.

3.  **Order of Operations and `ColumnTransformer` vs. Manual Preprocessing:**
    *   The generated code manually imputes `X` *before* passing it to the `ColumnTransformer` and pipeline. While technically possible, it defeats the purpose of encapsulating steps within a `scikit-learn` pipeline for cleaner code and more robust handling (e.g., within `cross_val_score`, each fold would correctly perform imputation if it was in the pipeline). The prompt suggests an ""advanced preprocessing"" setup, which often implies using pipelines to manage transformations.
    *   The `OneHotEncoder` setup is within `ColumnTransformer`, but simple imputation is done manually outside. This creates a mixed preprocessing approach that isn't ideal. A more advanced pipeline would integrate imputation into the `ColumnTransformer`.

4.  **Implicit Assumption of `OneHotEncoder` (`from sklearn.preprocessing import OneHotEncoder`):**
    *   The code uses `OneHotEncoder` but doesn't import it. This would lead to a `NameError`. (Self-correction: I missed it in the import list of *my provided code*, the actual generated code *did* import it. Apologies for this oversight. However, the `drop='first'` choice remains.)

5.  **Handling of `island` NA after mode imputation:**
    *   The prompt specifically notes: ""(Note: if a row still has NA in `island` after mode imputation, e.g. if all were NA, it can be dropped, or a placeholder like 'Unknown' can be used if mode is NA itself. For this task, if mode imputation results in NA for island, drop those rows)."" The generated code chooses `fillna('Unknown')` if `mode_value.empty`, which is a placeholder, *not* dropping the rows as specified. The subsequent `X = X[~X['island'].isna()]` will *not* drop rows where `island` was entirely NA and received 'Unknown', which means the requirement isn't fully met for this edge case.

Overall, the code performs some, but not all, preprocessing steps correctly within the intended framework. The pipeline construction for preprocessing is flawed, specifically in how scaling and one-hot encoding interact, and the categorical imputation logic doesn't fully conform to the prompt's explicit handling of NA 'island' values.",gpt-4.1-nano,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Select features and drop rows with missing values
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_selected = df[features + ['species']].dropna()

# Extract features and scale
X = df_selected[features...",8,"The generated code correctly addresses most of the task requirements. It loads the data, selects and cleans the specified features, standardizes them, applies K-Means with the correct parameters, and performs PCA. It then generates two scatter plots, one colored by K-Means labels and another by original species labels.

Strong points:
- Correct use of `StandardScaler`, `KMeans` (with `n_init='auto'` and `random_state=42`), and `PCA`.
- Correct feature selection and NA handling using `dropna()`.
- Plots are generated correctly with appropriate titles and labels.
- The use of `cmap='viridis'` and `cmap='tab10'` for K-Means and species plots respectively is good practice for categorical data.
- The manual handling of species for legend creation in the second plot is a good solution for showing discrete categories.

Areas for improvement (to achieve a higher score):
- **Clarity and Readability of Plotting:** The ground truth uses `seaborn` and creates a `DataFrame` (`pca_df`) which significantly improves the readability and maintainability of the plotting code. Directly using `X_pca[:, 0]` and `X_pca[:, 1]` works but is less descriptive. Using `seaborn` also handles legends and color mapping more elegantly for categorical data by default, reducing the need for manual mapping and `Line2D` objects for the legend.
- **Handling of `species` with `dropna()`**: While `df_selected = df[features + ['species']].dropna()` correctly removes NAs, the ground truth explicitly creates `df_selected = df[selected_features + ['species']].copy()` and then `df_cleaned = df_selected.dropna(subset=selected_features)`. This ensures that even if `species` itself had NAs (which it doesn't in this dataset for the relevant rows), the `dropna(subset=selected_features)` would only consider the chosen features, preventing accidental loss of rows based on `species` NA if that wasn't intended. In this specific case, it doesn't make a practical difference, but it's a subtle point about robustness.
- **Plotting Aesthetics:** While the plots are functional, `seaborn` in the ground truth often provides more aesthetically pleasing defaults and easier customization (e.g., `sns.set_style(""whitegrid"")`). The ground truth's use of `s=100` and `alpha=0.8` also makes the scatter points slightly clearer.",gpt-4.1-nano,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingClassifier

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Impute missing values
# Fill missing numerical values with mean
num_cols = ['culmen_length_m...",7,"The generated code correctly addresses most of the task requirements, including data loading, imputation, feature engineering, model training, and feature importance extraction. The visualization part is also largely correct.

However, there are a few areas for improvement:

1.  **One-Hot Encoding `drop='first'`**: The generated code uses `OneHotEncoder(drop='first', sparse=False)`. While `drop='first'` is often used to prevent multicollinearity in models, it removes one category from each categorical feature. This means that for feature importance analysis, you will not see an importance value for the dropped category, which can be misleading if the goal is to evaluate the importance of *all* distinct categories. The ground truth avoided `drop_first` for this reason. This is a subtle point but crucial for comprehensive feature importance evaluation involving one-hot encoded variables.

2.  **`X_num.values` in `np.hstack`**: When combining `X_num` (which is a DataFrame) and `X_cat` (which is a NumPy array), the code uses `X_num.values`. While this works, it converts `X_num` to a NumPy array, losing the DataFrame structure (column names) before stacking. The `feature_names` list is then constructed correctly, but `pd.concat` (as used in the ground truth) is often a cleaner and more direct way to combine DataFrames while preserving column names and types, especially when dealing with mixed data types or when column order needs to be rigorously maintained. This is more of a style and robustness point than a functional error in this specific case.

3.  **Use of `SimpleImputer` and `ColumnTransformer` (unused)**: The generated code imports `SimpleImputer`, `ColumnTransformer`, and `Pipeline` but then implements imputation manually (mean for numerical, mode for sex). While manual imputation is acceptable and correct here, the unused imports suggest perhaps an initial intent for a more robust sklearn-pipeline approach that wasn't fully implemented. This doesn't affect correctness but can be considered a minor code quality point.

4.  **`df['species_label'] = y` and `df['species_name'] = le_species.inverse_transform(df['species_label'])`**: While functionally correct for plotting, adding these temporary columns to the original `df` for plotting purposes is not strictly necessary and sometimes not desired if `df` is meant to remain clean. The ground truth directly used the original `df_processed['species']` column for plotting, which was already cleaned and imputed. This is a minor point related to dataset modification for visualization.

Overall, the code is well-structured, fulfills the primary requirements, and produces the correct output for feature importances and visualizations, except for the nuance of `drop='first'` in one-hot encoding impacting feature importance interpretation.",gpt-4.1-nano,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer

# Load the dataset
df = pd.read_csv('penguins.csv')

# List of numerical columns to impute
num_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']

# Extract numerical data
num_data = df[num_cols]

# Initialize KNNImputer
knn_imputer = KNNImputer(n_neighbors=3)

# Fit and transform the numerical data
imputed_num_data = knn_imputer.fit_transform(num_data)

# Assign the imputed values back to the da...",8,"The generated code correctly performs the requested data cleaning steps: loading the dataset, applying KNN imputation to the specified numerical columns, and imputing NA values in the 'sex' column with its mode. It also prints the final NA counts, fulfilling the verification step.

Strong points:
- Correctly identifies and imputes the numerical columns using KNNImputer.
- Correctly imputes the 'sex' column using the mode.
- Uses `inplace=True` for `fillna`, which is efficient.
- The `if df['sex'].isnull().any():` check before imputing mode is a good, although not strictly necessary, addition for robustness.

Areas for improvement:
- The initial `print` statements regarding the loaded data and initial NA counts from the ground truth provided better context and immediate verification that the data loaded correctly.
- The ground truth included error handling for `FileNotFoundError`, which is good practice in real-world scenarios but was not strictly required by the prompt's core task of data cleaning.
- The ground truth also explicitly printed individual NA counts for the imputed columns at the end, offering more direct verification of success for those specific columns, rather than just the general `isnull().sum()`. While `isnull().sum()` covers all columns, explicitly verifying the target ones adds clarity.",gpt-4.1-nano,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# 1. Load the dataset
df = pd.read_csv('penguins.csv')

# 2. Filter for 'Adelie' and 'Gentoo'
filtered_df = df[df['species'].isin(['Adelie', 'Gentoo'])]

# 3. Drop rows with NA in 'culmen_length_mm'
clean_df = filtered_df.dropna(subset=['culmen_length_mm'])

# Separate data for each species
adelie_culmen = clean_df[clean_df['species'] == 'Adelie']['culmen_length_mm']
gento_culmen = clean_df[clean_df['species'] == 'Gentoo']['culmen_...",9,"The generated code correctly addresses all the requirements of the task. It loads the data, filters for the specified species, handles NA values, performs the t-test, and interprets the results based on the p-value and significance level.

**Strong points:**
*   **Correctness:** All steps are performed accurately, leading to the correct statistical output and conclusion.
*   **Conciseness:** The code is straightforward and efficient, directly addressing the task without unnecessary complexity.
*   **Readability:** Variable names are clear, and the flow of operations is easy to follow.
*   **Completeness:** All 6 requirements are met.

**Areas for minor improvement (to reach a perfect 10):**
*   **Error Handling:** The code assumes the file `penguins.csv` exists. Adding a `try-except` block for `FileNotFoundError` when loading the CSV would make it more robust, similar to the ground truth.
*   **Docstrings/Comments:** While the code is relatively self-explanatory for this simple task, adding a brief docstring or comments for each step would enhance maintainability and understanding for more complex scenarios or collaborative environments.
*   **Function Encapsulation:** Wrapping the logic in a function (as done in the ground truth) would make the code reusable and more modular, following good programming practices.
*   **Informative Prints:** The ground truth includes more verbose print statements (e.g., confirming data loading, filtering, and NA handling), which can be very helpful for debugging and understanding data flow, especially with larger datasets or more complex tasks. While not strictly required by the prompt, they improve the user experience.
*   **Variable Name Consistency:** A minor point, but `gento_culmen` perhaps should be `gentoo_culmen` for consistency with `Adelie`. (Typo, but acceptable).

Overall, the generated code is a solid and correct solution to the problem. The suggested improvements are primarily for robustness, modularity, and user-friendliness, rather than fundamental correctness flaws.",gpt-4.1-nano,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Define features and target
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
target = 'species'...",9,"The generated code successfully completes the task by loading the data, performing the specified preprocessing steps, splitting the data with stratification, training a RandomForestClassifier, and evaluating its performance.

**Strong Points:**
- All required libraries are imported.
- Data loading and feature/target definition are correct.
- Numerical features are correctly imputed with the median.
- 'sex' is correctly imputed with the mode.
- One-hot encoding for 'island' and 'sex' is correctly applied using `pd.get_dummies`.
- Target variable 'species' is correctly label encoded.
- Data splitting adheres to the specified test size, stratification, and random state.
- RandomForestClassifier is initialized and trained with the correct parameters.
- Predictions are made, and accuracy and classification report are printed as required. The use of `target_names=label_enc.classes_` in the classification report is excellent for readability.

**Areas for Minor Improvement (to reach a perfect 10):**
- **In-place modification of `X`:** The code modifies `X` in place (e.g., `X[num_features] = num_imputer.fit_transform(X[num_features])` and `X['sex'] = sex_imputer.fit_transform(X[['sex']])`). While it works for this simplified case, it's generally safer and more robust to use `sklearn.compose.ColumnTransformer` and `sklearn.pipeline.Pipeline` as shown in the ground truth. This approach isolates preprocessing steps and prevents potential `SettingWithCopyWarning` issues. It also makes the code more scalable and maintainable for complex preprocessing pipelines.
- **Handling of `sex` imputation before `get_dummies`:** Although the generated code gets it right by first imputing `sex` and then one-hot encoding, a `ColumnTransformer` approach inherently handles the sequence better when combining multiple operations on the same column.

Overall, this is a very strong and functional solution that meets almost all requirements. The minor points are more about best practices and robustness in larger ML projects.",gpt-4.1-nano,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Calculate Pearson correlation be...",9,"The generated code correctly defines the DataFrame, calculates the specified Pearson correlation coefficients, prints them, and correctly determines and prints which feature has a stronger absolute correlation.

**Strong Points:**
- **Correctness:** All parts of the task are solved correctly. The DataFrame is created accurately, correlations are calculated, and the comparison logic is sound.
- **Conciseness:** The code is clean and to the point, avoiding unnecessary complexity.
- **Readability:** Variable names are clear, and comments (though minimal) guide the user through the steps.
- **Pandas Usage:** Correctly uses `df.corr()` for correlation calculation which defaults to Pearson.

**Areas for Minor Improvement (for a perfect 10):**
- **Explicit `method='pearson'`**: While `corr()` defaults to Pearson, explicitly stating `method='pearson'` makes the code more robust and self-documenting, especially if the default were to change or if other correlation methods were ever considered. (This is a very minor point, as `corr()` is indeed Pearson by default).
- **Output Formatting:** The ground truth code formats the correlation coefficients to a specific number of decimal places (`:.4f`), which can improve readability for numerical outputs. The generated code prints the full precision, which is not incorrect but less polished.
- **Detail in ""Else"" Case:** The `else` statement for equal correlations is slightly less descriptive than the ground truth. The ground truth provides the actual correlation value in the message (e.g., ""Both features have the same absolute correlation with PerformanceScore (0.1234).""), which is more informative.

Overall, this is a very strong solution that completely fulfills the task requirements effectively. The suggested improvements are minor and mostly related to output presentation and explicit coding practices rather than functional correctness.",gpt-4.1-nano,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.metrics import pairwise
import pymc3 as pm
import theano.tensor as tt
from scipy.stats import chi2_c...",6,"The generated code attempts to address all requirements but falls short in key areas, particularly regarding the interpretation of ""time-to-event"" for the Titanic dataset, the implementation of propensity score matching, and the Bayesian survival model.

**Correctness:**
*   **Kaplan-Meier & CoxPH `duration` and `event`:** This is the most critical correctness issue. The prompt explicitly states ""treating the voyage as time-to-event data (with survival as event indicator)"". The ground truth correctly interprets this, setting `T=1` for all observations (fixed voyage duration) and `E=1-Survived` (death as event). The generated code incorrectly uses `Age` as `duration` and `Survived` as `event`, which is a fundamental misinterpretation in the context of the Titanic task. 'Age' is not a time-to-event here; it's a covariate. 'Survived' is the outcome, not the event. This makes the KM and CoxPH results biologically and statistically nonsensical for the stated problem.
*   **Propensity Score Matching (PSM):** The implementation of PSM has a critical flaw. It matches `treated` units to `control` units by finding the nearest neighbor in `pscore` *without ensuring that the matched control unit is not already used*. While it tries `control.loc[control.index != idx]`, `idxmin()` for `dist` is still likely to re-select already matched units if it finds a better match in subsequent iterations, leading to incorrect paired matching. The ground truth uses `NearestNeighbors` from `sklearn` which is designed for this task and matches more robustly. Furthermore, the PSM evaluation (log-rank test and plotting) is based on the flawed `duration` and `event` definitions.
*   **Bayesian Analysis:** The PyMC3 model uses an `Exponential` likelihood with `observed=observed_time * observed_event`. The exponential distribution is typically used for survival analysis, but its `lam` parameter is the rate, and `observed` should be the time-to-event. Multiplying `observed_time` by `observed_event` (which is `Survived` in this case, so 0 or 1) means that for survivors, the `observed` time is 0, which doesn't make sense for exponential survival. A more robust Bayesian survival model would be needed, e.g., using AFT (Accelerated Failure Time) models or discrete-time survival. The prior choices are very wide (`sigma=10`), which might be problematic given the feature scaling isn't explicitly handled (though `df[Age]` might be scaled if it's the `age` feature used in the `df_cox` which is standardized).
*   **Model Diagnostics (Hosmer-Lemeshow):** The custom `hosmer_lemeshow_test` function is a good attempt, but `exp_freq * (1 - exp_freq)` can be 0 or very close to 0, leading to division by zero or large numbers. The `+ 1e-6` is a patch but might not be robust. Standard implementations or packages should be preferred.

**Code Quality:**
*   **Imports:** Mostly good.
*   **Structure:** Follows the requested numerical sections reasonably well, but the overall data preprocessing for different models is a bit scattered, leading to potential inconsistencies (e.g., `df_cox` for CoxPH, `df` re-used for PSM without clear, consistent prep for `Age` and `Survived`).
*   **Readability:** Generally readable, but variable names like `df_cox` or `df_sens` are somewhat ambiguous without careful tracing.
*   **Missing Imports:** `Pymc3` typically works better with `arviz` for summary and plotting, which is present in the ground truth but missing here.

**Completeness:**
*   **Kaplan-Meier:** Implemented, but the `duration`/`event` definition is incorrect. Plotting by Pclass is done.
*   **CoxPH:** Implemented, but the `duration`/`event` definition is incorrect.
*   **Advanced Statistical Tests:** Multivariate analysis (Logistic Regression) is present. Interaction effects are tested. Hosmer-Lemeshow for diagnostics is attempted. Log-rank tests added between KM plots.
*   **Propensity Score Matching:** Implemented, but with the matching logic flaw and reliance on the incorrect `duration`/`event`.
*   **Bootstrap Resampling:** Implemented for CoxPH HR, which is good.
*   **Bayesian Analysis:** Attempted with PyMC3, but the survival model specification is incorrect for the problem and would require a more specialized `pymc3` survival package or a correct manual implementation.
*   **Model Validation/Sensitivity:** Proportional hazards test is included. A basic sensitivity analysis by perturbing 'Age' is attempted, which is a good idea, though the chosen perturbation might be too simplistic for a full sensitivity analysis often involving parameter uncertainty or varying model structure.

Overall, the code demonstrates an understanding of the concepts but struggles with the nuanced application to the Titanic dataset, especially the time-to-event definition, and some advanced statistical implementations. The use of 'Age' as 'duration' makes the survival analysis sections fundamentally flawed for the given problem context.",gpt-4.1-nano,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# Create the DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
    'Final_Score': [90, 88, 82...",9,"The generated code correctly creates the DataFrame, calculates and prints the Pearson correlation coefficient, generates a complete scatter plot with coloring by subject, legend, title, and axis labels, and assigns grades based on the 'Final_Score' while printing the grade distribution.

The code adheres to all requirements of the prompt.
- **Correctness**: All calculations and visualizations are correct. The scatter plot correctly uses `hue` for subject coloring, and the `assign_grade` function correctly implements the specified grading logic.
- **Code Quality**: Imports are correct and necessary. The code is well-structured and readable. Using `sns.scatterplot` makes the plotting more concise and aesthetically pleasing. Using `apply` for assigning grades is a common and acceptable pandas pattern, though `np.select` (used in the ground truth) can sometimes be more performant for many conditions.
- **Completeness**: All parts of the task are addressed.

**Minor Improvement for a perfect score (similar to Ground Truth):**
- The prompt mentioned ""Include necessary imports (pandas, matplotlib.pyplot, seaborn for potentially better aesthetics but not required for coloring, `scipy.stats` for correlation)"". The ground truth also imported `numpy`. While `numpy` wasn't explicitly used for the grading in the generated code (using `apply` instead of `np.select`), it's a very common dependency in data science tasks, and explicitly importing it might be considered good practice by some. However, `np.select` is a more performant way to handle multiple conditions for column creation and aligns with standard vectorized operations in pandas which internally uses `numpy`. The current `apply` method is also perfectly valid.

Overall, it's a very strong submission that meets the requirements well.",gpt-4.1-nano,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Data creation
data = {
    'Date': pd.to_datetime([
        '2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05',
        '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12'
    ]),
    'Ticker': ['TECHCO']*10,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0],
    'Low': [149.5, 150....",10,"The generated code is excellent. It correctly implements all requirements of the task.
1.  **DataFrame Creation and Indexing**: The DataFrame is created correctly, and 'Date' is set as the index efficiently using `.set_index()`.
2.  **Daily Percentage Return**: `df['Close'].pct_change() * 100` is the correct and idiomatic way to calculate daily returns.
3.  **SMA Calculation**: Both 3-day and 5-day Simple Moving Averages are calculated accurately using `rolling().mean()`.
4.  **Plotting**:
    *   The plot correctly displays 'Close Price', 'SMA_3Day', and 'SMA_5Day' on the same chart.
    *   The title, x-label, y-label, legend, and grid are all present and correctly configured.
    *   `plt.figure(figsize=(10,6))` and `plt.tight_layout()` are good practices for plot presentation.
5.  **Bullish Crossover Identification**:
    *   The logic for identifying bullish crossovers (`(df['SMA_Diff'].shift(1) < 0) & (df['SMA_Diff'] > 0)`) is accurate and robust. It correctly checks for the prior condition (SMA_3Day below SMA_5Day) and the current condition (SMA_3Day above SMA_5Day).
    *   The dates are then extracted and printed in a readable format.

The code is well-structured, uses clear variable names, and follows best practices for pandas and matplotlib, making it very readable and maintainable.",gpt-4.1-nano,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1) Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics'] * 7,
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST001']
}

df = pd.DataFrame(data)

# 2) Calcu...",9,"The generated code is highly correct and complete, addressing all aspects of the task. It correctly creates the DataFrame, calculates total sales revenue per product, generates and displays the horizontal bar chart, calculates the average order value, and identifies repeat customers with their purchase counts.

Strengths:
- **Correctness:** All calculations and data manipulations are accurate.
- **Completeness:** Every part of the task prompt is addressed.
- **Plotting:** The horizontal bar chart is correctly generated with appropriate labels and title.
- **Clarity:** The code is well-structured and easy to understand.
- **Efficiency:** Uses pandas functionalities effectively for aggregations and data manipulation.

Minor improvements:
- In the `Category` column definition, `['Electronics'] * 7` is a valid and concise way to define the list, which is good.
- For the `product_revenue.sort_values().plot(kind='barh')`, explicitly sorting in descending order before plotting (e.g., `product_revenue.sort_values(ascending=True).plot(kind='barh')` or just plot `product_revenue` directly if it's already sorted by sum in the previous step and the default order is fine for visualization) could sometimes make the chart more intuitive if higher values are preferred at the top, but `sort_values()` without `ascending=False` means it plots in ascending order, which is also a valid presentation. The ground truth sorts in `descending=False` before plotting, leading to the largest bar at the top, which is a common convention for bar charts to show values ordered by magnitude. The generated code sorts in `ascending` order, so the smallest bar is at the top. This is a stylistic choice more than an error.
- The print output for repeat customers could be slightly more formatted for readability, similar to the ground truth's table-like output, but this is a minor aesthetic point.
- The ground truth includes `plt.grid(axis='x', linestyle='--', alpha=0.7)` and `plt.figure(figsize=(10, 6))` (which the generated code also implements) for better plot readability. Adding a grid would be a small enhancement.",gpt-4.1-nano,LLM_judge_config
H53,hard,Feature Engineering,,"import pandas as pd

# Original DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 1) Map DeviceType to numerical values
df['DeviceType_Numeric'] = df['DeviceType'].map({'Desktop': 0, 'Mobile': 1})...",10,"The generated code is excellent. It correctly defines the initial DataFrame, accurately creates the `DeviceType_Numeric` feature using `map`, and correctly calculates `PagesPerMinute` while robustly handling potential division by zero by replacing `inf`, `-inf`, and `NaN` values with 0. Finally, it correctly selects the specified feature set and prints its head. The code adheres to all requirements of the task, is structured well, and uses appropriate pandas functionalities. The use of `float('inf')` and `-float('inf')` instead of `np.inf` and `-np.inf` is also perfectly acceptable and achieves the same result given a direct import of `pandas` is not strictly `numpy`.",gpt-4.1-nano,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

# Define the data
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"",
                  ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26...",10,"The generated code perfectly addresses all requirements of the task. It correctly defines the DataFrame, calculates the mean and standard deviation for 'TemperatureCelsius', identifies outliers based on the 1.5 standard deviation threshold, and prints the specified details for each outlier. The code is clean, well-structured, and directly implements the requested logic. The variable names are clear, and the use of boolean indexing for outlier detection is efficient and idiomatic pandas.",gpt-4.1-nano,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"import pandas as pd

# Define the data
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [10, 5, 25, 15, 8...",10,"The generated code is entirely correct and effectively solves all parts of the task. It correctly defines the DataFrame, calculates the EngagementScore as specified, and accurately identifies the PostType with the highest average EngagementScore for each platform. The printing format is also clear and follows the requirements. The code quality is good, using appropriate pandas functions for grouping and aggregation. The use of `idxmax()` after grouping is an efficient and clean way to achieve the desired result, comparable to the sorting and dropping duplicates method used in the ground truth.",gpt-4.1-nano,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

# Define the data
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"",...",9,"The generated code correctly performs all the required tasks:
1. It accurately defines the DataFrame.
2. It correctly calculates the ""LengthOfStay"" column.
3. It correctly computes the average ""LengthOfStay"" and patient count per diagnosis.
4. It correctly identifies the diagnosis with the longest average stay.

The code is concise and uses appropriate pandas functions (`dt.days`, `groupby`, `agg`, `idxmax`).

Areas for minor improvement (preventing a perfect 10):
- **Output clarity for task 3:** While the output `print(diagnosis_stats)` does show the required information, it's a raw DataFrame printout. The Ground Truth code provides a more formatted and human-readable print statement (using an f-string loop), which aligns better with ""Print this information"" as a distinct step.
- **Explicit import placement:** `import pandas as pd` is correctly placed at the top, which is good practice.
- **Function encapsulation:** The ground truth code encapsulates the logic within a function, which is generally good practice for reusability and organization, especially in larger scripts. However, for this specific task, a simple script is also acceptable if not explicitly asked for a function.
- **Comments/Docstrings:** The generated code has minimalistic comments compared to the Ground Truth's more detailed comments and docstring. For a benchmark, thorough comments are often preferred for clarity.

Overall, a very strong submission that fulfills the task requirements effectively.",gpt-4.1-nano,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",9,"The generated code correctly defines the DataFrame, calculates the ""WeightedRating"", adds it as a new column, and identifies the movie with the highest ""WeightedRating"", printing its title and weighted rating. This addresses all parts of the task.

The code quality is good, using appropriate pandas functions and a clear structure. The only minor improvement would be handling the edge case of `total_votes` being zero, although for the given dataset, this is not an issue. The ground truth code includes this check, which makes it slightly more robust. Also, the print format for the final output could be slightly more descriptive, similar to the ground truth's f-string print. However, the current print statement still conveys the required information.",gpt-4.1-nano,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

# Define the data
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 1000...",9,"The generated code correctly defines the DataFrame, calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"", and prints the resulting grouped DataFrame. It uses an efficient `groupby().agg()` method which is suitable for this task.

The only minor point of improvement is the column naming in the final output. The ground truth explicitly renames the columns to `Average_PriceUSD` and `Average_Mileage` using named aggregations, which makes the output more descriptive. The generated code keeps the original column names (`PriceUSD`, `Mileage`) for the averages, which is technically correct but slightly less clear in output context. However, it does apply `reset_index()`, which transforms 'Brand' back into a column from an index, making the output format potentially easier to work with downstream than if it remained an index. Overall, it's a very good solution.",gpt-4.1-nano,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

# Define the data
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48...",10,"The generated code is excellent. It correctly defines the DataFrame, adds the ""TotalStats"" column, identifies the Pokemon with the highest ""TotalStats"", and calculates the average ""Speed"" for each ""Type1"". The code is clean, concise, and uses appropriate pandas functionalities. Specifically:
- **DataFrame Creation:** The DataFrame is created accurately with the provided data.
- **TotalStats Column:** The calculation `df[['HP', 'Attack', 'Defense', 'Speed']].sum(axis=1)` is a robust and common pandas idiom for summing across multiple columns, arguably cleaner than individual column additions.
- **Highest TotalStats Pokemon:** The logic `df.loc[df['TotalStats'] == max_total, 'Name'].values[0]` correctly identifies the name of the Pokemon with the highest total stats. Using `.values[0]` is appropriate here because we expect a single Pokemon to have the maximum, or we only care about the first one if there's a tie. While `idxmax()` from the ground truth is often preferred for its directness, this solution is also very valid and correct.
- **Average Speed by Type1:** The `groupby('Type1')['Speed'].mean()` is the standard and correct way to perform this aggregation.
- **Code Quality:** Imports are correct, and the code is well-structured and easy to read.
- **Completeness:** All aspects of the task are addressed perfectly.",gpt-4.1-nano,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt
import seaborn as s...",6,"The generated code attempts to address all requirements but falls short in several key areas, leading to a lower score.

**Positives:**
*   **Imports:** Most necessary imports are present.
*   **Feature Engineering:** Includes an attempt at advanced features like `FamilySize`, `TicketShareCount`, `Title`, and `TicketPrefix`.
*   **Preprocessing:** Utilizes `OneHotEncoder` and `StandardScaler`.
*   **Multiple Algorithms:** Attempts K-means, Hierarchical, DBSCAN, and GMM.
*   **Optimal K:** Tries to use Silhouette, CH index, and Gap Statistic.
*   **Dimensionality Reduction:** Includes PCA, t-SNE, and UMAP.
*   **Survival Analysis:** Includes basic survival rate calculation per cluster.
*   **Visualizations:** Offers basic scatter plots and bar plots.

**Areas for Improvement (Why it lost points):**

1.  **Correctness & Robustness of Feature Engineering:**
    *   **FarePerPerson/Ticket sharing calculation:** The ground truth calculates `FarePerPerson = Fare / TicketGroupSize` and handles potential division by zero. The generated code calculates `AvgFarePerPassenger` using `df.groupby('Ticket')['Fare'].transform('mean')`, which is different and doesn't explicitly create a per-person fare. The prompt specifically asked for ""ticket sharing"" (implying the number of people on a ticket, like in GT) and ""fare per person"". The GT approach `Fare / TicketGroupSize` is more direct for ""fare per person"".
    *   **`TicketPrefix`:** The regex `re.match(r'([A-Za-z]+)', str(ticket))` for `TicketPrefix` is less robust than the GT's `split(' ')` and checking `isdigit()`. It might miss prefixes that are not at the very beginning or have numbers intertwined. GT's consolidates rare prefixes.
    *   **Title mapping:** While it attempts to group rare titles, it's not as comprehensive or well-defined as the `replace` logic in the GT for common title restructuring (e.g., 'Mlle'/'Ms' to 'Miss').
    *   **`Cabin` feature:** The prompt asked for ""text-based features from names and tickets"" which the generated code partially covers. However, the ground truth also includes `CabinDeck` and `HasCabin` from the 'Cabin' column, which are highly relevant and were likely implicitly expected under ""text-based features"" or ""socioeconomic indicators"" (cabin class). This was missed.
    *   **Interaction features:** GT includes `Age_Class`, `Fare_Class`, `FarePerPerson_Class`. Generated code does not. This was a key requirement for ""advanced features"".

2.  **Preprocessing Issues:**
    *   **Handling `NaN`s:** The generated code does not explicitly handle `NaN` values in `Age` or `Embarked` before scaling, which will cause `StandardScaler` to fail or produce NaNs. The GT correctly imputes these.
    *   **Missing Features in `X`:** The `features` list created in the generated code misses several generated features like `Title`, `TicketPrefix`, `HasCabin`, `CabinDeck`, and interaction features (if they were created). This means these features would not be used in clustering, undermining the advanced feature engineering part.
    *   **Improper `X_processed` creation:** `X_num = X.drop(columns=categorical_features).values` will drop original columns like 'Age', 'Fare', 'FamilySize' which might have NaNs, and converts to NumPy array, which `StandardScaler` can handle but without imputation. The subsequent `np.hstack` then combines it. This structure is less robust than `ColumnTransformer` (used in GT) which applies different transformers to different columns systematically and handles missing names.

3.  **Optimal Cluster Determination:**
    *   **Gap Statistic:** Relies on an external library `gap_statistic` that might not be standard or universally available, and the prompt implies using common libraries like `sklearn`, `scipy`. While the prompt didn't forbid it, it's a slight deviation from common practice.
    *   **Consensus `best_k`:** Taking a `round(np.mean([...]))` of `optimal_k_sil`, `optimal_k_ch`, and `best_k_gap` is a very simplistic way to determine `best_k`. In practice, one would analyze plots of these metrics (as shown in GT) to make an informed decision, or use more sophisticated consensus methods. The code does not plot these metrics, which is crucial for interpreting ""optimal k"" visually.

4.  **Cluster Stability and Consensus Clustering:**
    *   **Stability analysis:** The implementation of stability analysis is incomplete. It mentions bootstrapping and `adjusted_rand_score` but includes placeholder comments like `# Placeholder: store ARI for each iteration # actual computation requires matching indices # For brevity, skipped detailed implementation`. This means the requirement for ""Implement cluster stability analysis"" is not met. The GT provides a functional and illustrative example of stability analysis using ARI.
    *   **Consensus Clustering:** The prompt explicitly asked for ""consensus clustering,"" which implies combining results from multiple runs or algorithms (e.g., using a co-occurrence matrix). This is not present in the generated code at all.

5.  **Comprehensive Visualizations:**
    *   **Missing Plots:** The generated code plots UMAP for K-Means but not the other dimensionality reduction techniques (PCA, t-SNE) or other clustering algorithms. The GT provides a comprehensive set of plots for all.
    *   **Feature Distribution Plots:** The generated code only provides `countplot` for 'Survived' per cluster. It lacks detailed feature distribution plots (e.g., violin plots for Age, Fare, count plots for Pclass, Sex) for each cluster which are vital for ""Analyzing cluster characteristics and survival patterns within each segment"". The GT provides these.
    *   **Heatmap of Scaled Feature Means:** A crucial visualization for understanding cluster profiles is a heatmap of scaled feature means, which the GT includes but is missing from the generated code.

6.  **External Validation:**
    *   The external validation part is very basic (just printing survival rate). The GT calculates survival distribution per cluster and visualizes it, which is more comprehensive.

**Overall:** The code demonstrates an understanding of the components involved but lacks the depth, robustness, and completeness compared to professional-grade analysis and what was explicitly asked for in the prompt. Many sections are started but not fully implemented to the level of detail and quality expected. The omission of proper `NaN` handling and systematic multi-feature preprocessing with `ColumnTransformer` is a significant flaw.",gpt-4.1-nano,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Define the data and create the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""...",9,"The generated code correctly defines the DataFrame, calculates the book count and average price per author, and identifies the author with the most books in stock. All functional requirements are met.

The code quality is good, using appropriate pandas functions (`groupby`, `agg`, `size`, `mean`, `sum`, `idxmax`, `max`). The variable names are clear and descriptive.

The only minor area for improvement is in the printing of the author statistics. The ground truth code provided a more formatted and readable output for the author statistics (`Author: ... | Books: ... | Avg Price: ...`), whereas the generated code simply prints the `author_stats` DataFrame directly, which is functional but less presentation-friendly for a final report. However, this is a minor aesthetic difference and does not impact the correctness or completeness of the required calculations.",gpt-4.1-nano,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np

from sklearn.model_selection import (
    train_test_split, StratifiedKFold, GridSearchCV,
    learning_curve, validation_curve, cross_val_score
)
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, FunctionTransformer
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.compose import ColumnTransformer
from sklearn.feature_selection import SelectKBest, f_classif

...",5,"The generated code attempts to address all requirements but falls significantly short in implementation details, correctness, and robustness.

**Critical Issues and Missing Functionality:**

1.  **Data Preprocessing:**
    *   **Multiple Imputation Strategies:** The ground truth uses `IterativeImputer` (MICE), which is a robust multiple imputation method. The generated code proposes `SimpleImputer` (mean/median) and `KNNImputer` but then bundles them all into a single `ColumnTransformer` ( `preprocessor` )  without selecting one, effectively discarding the benefit of multiple strategies or creating an unusable pipeline. The `FeatureUnion` mentioned is not used. This section is conceptually broken.
    *   **Outlier Detection/Treatment:** The generated code performs outlier capping via an `outlier_capper` function outside the pipeline, directly on `X`, creating `X_no_outliers`. This means the `preprocessor` pipeline steps (imputation, scaling, poly features) are applied *after* outlier treatment, which is fine, but the outlier treatment itself is not part of the `Pipeline` passed to `GridSearchCV` or ensemble models, making it non-reproducible within the model optimization loop. The ground truth correctly integrates a custom `OutlierCapper` into the preprocessing pipeline.
    *   **Feature Engineering:** The `PolynomialFeatures` are applied within the `build_pipeline` function, which is good. However, the `poly.fit_transform(X)` in the preprocessing section is redundant and not used.

2.  **Model Building and Optimization:**
    *   **GridSearchCV Parameters:** The parameter grids are reasonable but lack depth for ""advanced"" optimization. Bayesian optimization was an option in the prompt but not implemented by either.
    *   **Pipeline Setup:** `build_pipeline` creates a new `ColumnTransformer` with `preprocessor` from the global scope, which was already flawed as described above.

3.  **Ensemble Models:**
    *   **Blending:** The blending implementation is incomplete and doesn't actually train and use base models on a ""hold-out"" set for meta-features, which is the core idea of blending. It simply averages probabilities from `optimized_models`, which is more akin to a 'soft' voting classifier than a proper blending approach. The ground truth provides a correct custom blending implementation.

4.  **Advanced Evaluation:**
    *   **`evaluate_model` (Stratified K-Fold):** While a good intention, `cross_val_score(model, X_train, y_train, cv=skf)` inside `evaluate_model` is not what's needed for the paired t-test. For paired t-tests, you need the *individual fold scores* from the *same folds* across different models, not just overall mean CV scores. The ground truth correctly re-runs CV for each best model to get these fold-wise scores.
    *   **Bootstrap Sampling:** The `bootstrap_evaluate` function is designed to *retrain* the model `n_bootstrap` times on resampled *training data*, then evaluate on `X_test`. This is computationally expensive and not the standard way to compute confidence intervals on *test set performance*. The standard way is to resample the *test set* and calculate metrics on those resamples (as done in the ground truth). The generated code's `bootstrap_evaluate` is also not called or used to calculate confidence intervals in the dashboard.
    *   **Confidence Intervals in Dashboard:** The generated code attempts to calculate confidence intervals but then doesn't display them or integrate them correctly into the dashboard plot.

5.  **Model Interpretability (SHAP/LIME):**
    *   **SHAP:** The `explainer = shap.Explainer` without specifying the background dataset (`X_train`) is less robust for non-tree models or when using `KernelExplainer` (though it defaults to `TreeExplainer` if possible). The `shap_values` is `shap_values = explainer.shap_values(X_test)`. This is correct for tree models where explainer.shap_values directly returns the values. The plot types are okay.
    *   **LIME:** While LIME is imported and an explainer initialized, the `predict_proba` function passed to LIME must be a callable that takes a 2D array and returns class probabilities. `stacking_ensemble.predict_proba` is correct, but the output `exp.show_in_notebook(show_table=True)` indicates it's meant for a Jupyter environment, not a standalone script, and won't display anything.

6.  **Statistical Significance Testing:**
    *   The `ttest_rel` relies on the flawed `evaluate_model` scores (average CV score, not per-fold scores tied to specific folds across models). This makes the t-test unreliable and not truly paired.

7.  **Model Comparison Dashboard:**
    *   **Feature Importance Plot:** Attempts to plot feature importance but uses `X_train.columns` directly as labels, which will be incorrect after `PolynomialFeatures` and `ColumnTransformer` (the number of `importances` won't match `len(X_train.columns)`). The ground truth correctly extracts processed feature names.
    *   **Prediction Confidence Intervals:** `predict_with_confidence` retrains the model multiple times on resampled `X_test`, which is extremely inefficient and incorrect for prediction confidence intervals. CIs for predictions are usually for the prediction itself for a given input, not for the entire test set's predictions. The ground truth implements bootstrap for *metric* confidence intervals, which is different.

8.  **Model Calibration:**
    *   This section is quite good. It attempts calibration and plots curves both before and after, which is aligned with the prompt.
    *   `CalibratedClassifierCV(stacking_ensemble, method='sigmoid', cv='prefit')` might cause issues if `stacking_ensemble` was not explicitly `fit` without `cv` previously. It's safer to use `cv=5` or `cv='prefit'` after training with a full `fit`. The ground truth directly fits `CalibratedClassifierCV` on training data, which handles the internal cross-validation correctly.

**Code Quality:**
*   Many classes are imported but not used (e.g., `FunctionTransformer`, `FeatureUnion`, `SelectKBest`, `f_classif`, `precision_recall_curve`, `resample`).
*   Variable names are mostly clear.
*   Comments explain intentions but often the implementation is flawed.
*   Lack of error handling for file not found (though the prompt did provide it, the generated code doesn't).
*   The `treat_outliers` function is called outside the pipeline, breaking the pipeline's end-to-end nature. This is a significant design flaw.

In summary, the generated code outlines the major components requested but fails to correctly implement many of the advanced requirements, especially in data preprocessing, proper ensemble blending, accurate evaluation metrics for statistical testing, and meaningful confidence interval calculation and visualization.",gpt-4.1-nano,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.impute import KNNImputer, IterativeImputer
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metri...",6,"The generated code attempts to address all requirements of the task, but falls short in several areas, particularly regarding the depth and correctness of implementation for advanced statistical methods and diagnostics.

**Strengths:**
*   **Structure:** Follows the numbered requirements, making it easy to follow the intent.
*   **Imputation:** Correctly uses `KNNImputer` and `IterativeImputer` (MICE-like) and provides a basic comparison.
*   **Model Building:** Implements Logistic Regression, PyGAM (though syntax seems truncated or simplified), and Bayesian Logistic Regression using PyMC3.
*   **Anomaly Detection:** Correctly implements Isolation Forest, LOF, and One-Class SVM.
*   **Dimensionality Reduction:** Implements PCA and Factor Analysis.
*   **Hypothesis Testing:** Attempts MANOVA, permutation tests, and bootstrap CIs.

**Areas for Improvement / Weaknesses:**

1.  **Missing Data Imputation Comparison:** The comparison `imputation_effects` only compares the number of nulls, which is not a measure of effectiveness. Effectiveness should be evaluated based on downstream model performance or preservation of statistical properties. The ground truth provides descriptive statistics which is a better start.
2.  **PyGAM Implementation:** The `gam` initialization `s(0) + s(1) + ...` is hardcoded and assumes 8 features, which is brittle. It should dynamically build the formula from column names. The `summary()` call for `pygam` is correct but the output (print) isn't shown.
3.  **Bayesian Logistic Regression (PyMC3):**
    *   The `pm.math.dot(X_train, beta)` expects `X_train` to be a Theano variable, but it's a Pandas DataFrame. PyMC3 models usually work with numpy arrays or Theano shared variables.
    *   The posterior predictive check (`pm.sample_posterior_predictive`) is called, but no analysis of the predictions (e.g., ROC AUC, classification report) is performed, which is critical for evaluating a Bayesian model's predictive power.
    *   No trace plots or summary statistics (`az.plot_trace`, `az.summary`) are provided for the MCMC chains, which are essential for diagnosing model convergence and interpreting parameters in Bayesian contexts.
4.  **Advanced Hypothesis Testing:**
    *   **MANOVA:** The use of `statsmodels.stats.anova.MANOVA` is more common for ANOVA-like tests on a single dependent variable rather than multivariate. The `statsmodels.multivariate.manova.MANOVA` is the correct class for true MANOVA (as used in ground truth). The current implementation's `mv_test()` might not produce the expected output for true MANOVA.
    *   **Permutation Test:** The permutation test is implemented correctly for mean difference, which is good.
    *   **Bootstrap CI:** Correctly implemented for the mean.
5.  **Robust Statistical Methods:**
    *   Using MAD for outlier detection and then replacing outliers with the median is a reasonable simple robust method, but it's not a ""robust statistical model"" in the sense of RLM (Robust Linear Model) or GLMs with robust variance estimators. The prompt specifically asked for ""robust statistical methods to handle outliers and non-normal distributions,"" and RLM from `statsmodels.robust` is a much more appropriate example. The current approach is more of a robust preprocessing step.
    *   Shapiro-Wilk for non-normal distributions is good, but it's often a precursor to choosing robust methods, not a robust method itself.
6.  **Model Diagnostics:**
    *   **Residual Analysis for Logistic Regression:** `sns.residplot` is designed for OLS regression. For logistic regression, residuals are not normally distributed, and the plotting method should typically be on deviance or Pearson residuals vs. fitted *probabilities* or original features. The ground truth correctly uses deviance residuals.
    *   **Influence Measures:** It identifies `leverage` and `cooks` but the plot for leverage vs. residuals is not the standard influence plot (Cook's distance vs. Leverage or Fitted Values vs. Standardized Residuals). The `influence_plot` from `statsmodels.graphics.regressionplots` is the standard and more informative.
    *   **Goodness-of-Fit:** Hosmer-Lemeshow is a good G-o-F test for logistic regression. However, the `sm.stats.hl_test` function is from `statsmodels` but not typically used in this way. One usually needs to implement it or use a specific function that takes `y_true` and `y_pred_proba`. The usage `sm.stats.hl_test(sm.stats.gof.Chi2Test, y_test, preds)` is incorrect as `Chi2Test` is not directly usable as the first argument in this context. It should be a proper GLM result.
7.  **Advanced Statistical Visualizations:**
    *   Q-Q plots are generated, but often for residuals of a model or for a specific feature, not necessarily for all features.
    *   The ""Leverage Plot"" is just a scatter plot of leverage vs residuals, not the standard `plot_leverage_resid2` from `statsmodels.graphics.regressionplots` as requested.
    *   The ""Diagnostic dashboard"" is a basic grid of plots, but doesn't meet the ""diagnostic dashboards"" implied by the prompt, which usually integrates model-specific diagnostics.

**Overall:** The generated code provides a reasonable scaffold but lacks the advanced detail and specialized function usage expected for an ""expert code evaluator."" Many implementations are basic or slightly off from the most appropriate advanced statistical methods and typical diagnostic practices. It misses crucial parts of the PyMC3 and `statsmodels` diagnostics.",gpt-4.1-nano,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.manifold import TSNE
import umap
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.cluster import SpectralClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.model_selection import t...",6,"The generated code attempts to address all aspects of the request but falls short on correctness, completeness, and robustness in several key areas.

**Strong points:**
*   **Structure:** It follows a logical flow as outlined in the prompt.
*   **Multiple Algorithms & DR:** It correctly implements various clustering algorithms and dimensionality reduction techniques.
*   **Basic Validation:** Includes basic methods for optimal K determination (silhouette, Calinski-Harabasz, elbow).

**Areas for Improvement:**

1.  **Missing Data Handling (Critical):** The generated code assumes `fillna(features.median(), inplace=True)` will work universally. However, the `water_potability.csv` dataset explicitly has missing values that need proper imputation (e.g., using `sklearn.impute.SimpleImputer`) before feature engineering or scaling, as the prompt's ground truth demonstrates. This is a critical oversight that can lead to errors or incorrect results. The provided dataset often raises `ValueError: Input contains NaN, Infinity or a value too large for dtype('float64').` if not handled correctly early on.
2.  **Feature Engineering (Partial & Incorrect):**
    *   **""Potability_flag"" from ""Potability"":** The prompt says `df['Potability']` is already numeric (0 or 1). The generated code assumes it's 'Positive' and 'Negative', which may not be true for the given dataset.
    *   **Contaminant Columns:** The code `contaminant_cols = ['Lead', 'Nitrates', 'Fluoride', 'Chloramine']` implicitly assumes these columns exist in the dataset. `water_potability.csv` does not contain 'Lead', 'Nitrates', or 'Fluoride'. This will cause `KeyError` or result in an empty `contamination_score` feature. The ground truth correctly identifies existing columns for risk scoring.
    *   **Lack of Specific Water Quality Indices:** The engineered features are very generic and lack the depth and specific examples (like pH deviation, Hardness quality score, Sulfate/Chloramines ratio) that a robust water quality analysis would require, as demonstrated in the ground truth.
    *   **Temporal Patterns:** The code attempts temporal patterns but the `water_potability.csv` dataset is static and does not contain a 'Date' column. This part of the code is irrelevant and will raise an error if these columns are assumed to exist.
3.  **Optimal K Determination (Incomplete):** While three metrics are used, the essential Davies-Bouldin index for optimal K is missing from the plotting section, and the analysis of the plots is superficial (e.g., ""assuming 3 from above""). The ground truth provides all four plots and encourages visual inspection.
4.  **DBSCAN Parameter Tuning:** The generated code uses fixed `eps=0.5` and `min_samples=5` for DBSCAN. This is highly problematic as DBSCAN is very sensitive to these parameters, which typically require visual inspection of a k-distance graph (or similar methods) for proper tuning. The ground truth correctly includes code for generating this plot.
5.  **Cluster Stability Analysis (Very Weak):**
    *   The `bootstrap_cluster_stability` function is flawed. It resamples the *data* and then fits KMeans, but then tries to `np.mean(stability_scores)` which is a list of *labels*, not scores. It incorrectly states: ""For simplicity, measure stability via adjusted rand index"" but doesn't actually implement or use it. Comparing resampled labels directly to original labels requires a sophisticated matching algorithm (e.g., Hungarian algorithm) for Adjusted Rand Index to be meaningful when cluster labels are arbitrary.
    *   It completely omits the consensus clustering approach using co-occurrence matrices and dendrograms, which is a key part of robust stability analysis and is present in the ground truth.
6.  **Anomaly Detection (Limited & Incorrect):**
    *   It uses ""distance to cluster centroid"" but the calculation `distances = cdist(X_scaled, kmeans.cluster_centers_[labels_kmeans], metric='euclidean')` is slightly off. `kmeans.cluster_centers_[labels_kmeans]` creates a matrix where each row is the centroid corresponding to the point at that index in `labels_kmeans`. The `cdist` function is used to calculate distances between *sets* of points. Here, `cdist` will calculate pairwise distances for all points in `X_scaled` against all chosen centroids. A simpler and more direct way is to iterate or use `sklearn.metrics.pairwise_distances_argmin_min`.
    *   The visualization of anomalies (`reducer_umap.transform(anomalies.iloc[:, :-2])[0,0]`) looks incorrect; `anomalies` is a DataFrame, and `transform` expects a 2D array. This line `reducer_umap.transform(anomalies.iloc[:, :-2])[0,0]` would likely fail or produce an incorrect single point.
    *   It only implemented one basic anomaly detection method. The prompt explicitly asks for ""cluster-based anomaly detection to identify unusual water samples,"" which can include more than just centroids. The ground truth provides two methods: distance to centroid and Isolation Forest within clusters.
7.  **Plotting and Visualization:** While `plotly.express` is used, the radar chart generation is incorrect. It tries to plot a separate figure for each cluster, which is not ideal, and it doesn't normalize the features for a comparative radar chart across clusters effectively. The ground truth correctly plots all clusters on one normalized radar chart.
8.  **General Errors/Warnings:** The code lacks `SimpleImputer` for handling `NaN`s which exist in the input, leading to potential crashes or incorrect results. It also lacks a robust configuration section, error handling for file loading, and suppression of warnings for cleaner output.

Overall, the generated code provides a basic framework but lacks the robustness, precision, and complete implementation of advanced techniques and proper error handling expected of an ""expert expert code evaluator"" level solution.",gpt-4.1-nano,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr
from sklearn.linear_model import LinearRegression

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Compute correlation with 'quality'
correlations = data.corr()['quality'].drop('quality')

# Find strongest positive and negative correlations
strongest_positive_feature = correlations.idxmax()
strongest_negative_feature = correlations.idxmin()

posi...",9,"The generated code correctly addresses all requirements of the task. It calculates correlations, identifies top positive and negative features, generates a heatmap, and creates scatter plots with regression lines for the most correlated features.

**Strong Points:**
*   Correctly calculates correlations and identifies top positive/negative features.
*   Successfully generates a correlation heatmap.
*   Creates scatter plots and visualizes linear regression lines using `sklearn.linear_model.LinearRegression`, which is an acceptable alternative to `scipy.stats.linregress`.
*   The code is well-structured and uses appropriate imports.
*   Prints out the correlations clearly.

**Areas for Improvement (minor):**
*   The scatter plots don't include R-squared values on the legend, which the ground truth code did. This isn't a critical omission but would enhance the plots.
*   The `plot_feature_vs_quality` function recreates the plot for each feature, which is functional but could be slightly optimized by using `plt.subplots` to generate them side-by-side if desired, or just making sure each plot is a new figure.
*   Error handling for file loading (e.g., `try-except FileNotFoundError`) is missing, which is a good practice for robust code, though not explicitly requested in the prompt beyond ""assume the CSV file is in the current directory"".
*   The chosen `strongest_positive_feature` and `strongest_negative_feature` are based on `correlations.max()` and `correlations.min()`, which is technically correct. However, `top3_positive.index[0]` and `top3_negative.index[0]` (as used in the ground truth) are semantically clearer as they directly refer to the *first* item in the sorted lists. In this case, `idxmax()` and `idxmin()` will yield the same result since `quality` was dropped.",gpt-4.1-nano,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, chi2_contingency, mannwhitneyu

# 1) Load dataset and clean missing values
df = pd.read_csv('video_games_sales.csv')
df.dropna(inplace=True)

# 2) Test whether Nintendo games have significantly higher global sales compared to all other publishers
nintendo_sales = df[df['Publisher'] == 'Nintendo']['Global_Sales']
other_sales = df[df['Publisher'] != 'Nintendo']['Global_Sales']
t_stat_nintendo, p_value_nintendo = tt...",6,"The generated code addresses most of the requirements but has significant issues regarding data cleaning, robustness, and the completeness of interpretation, which are critical for an ""expert code evaluator"" benchmark.

**Strong Points:**
*   Correctly uses `ttest_ind`, `chi2_contingency`, and `mannwhitneyu` from `scipy.stats`.
*   Applies `equal_var=False` for t-tests, which is generally a good practice (Welch's t-test).
*   Correctly identifies top 5 genres and platforms for the chi-square test.
*   Performs the Mann-Whitney U test with the correct alternative.
*   Prints p-values and a basic interpretation.

**Areas for Improvement/Critical Missing Points:**

1.  **Data Cleaning (Critical):**
    *   `df.dropna(inplace=True)` at the beginning will drop *any* row with *any* missing value in *any* column. This is often too aggressive and can lead to loss of valuable data. The ground truth selectively drops NaNs based on the columns required for specific tests (`Global_Sales`, `Year_of_Release`, `Publisher`, `Genre`, `Platform`).
    *   The `Year_of_Release` column is immediately converted to numeric *after* this aggressive `dropna`. If `Year_of_Release` had non-numeric values (like 'tbd') or NaNs, they wouldn't be handled gracefully *before* the Mann-Whitney test, which relies on this column, potentially leading to errors or skewed results if not all relevant NaNs were cleared by the initial `dropna`. The ground truth explicitly cleans `Year_of_Release` and converts it to `int` for proper comparison.
    *   No handling for 'tbd' in `User_Score` (though not directly used in tests, it's a common cleaning step for this dataset).

2.  **Robustness and Error Handling (Critical):**
    *   No `try-except` block for `pd.read_csv`, meaning it would crash if the file is not found.
    *   No checks for empty dataframes or groups before performing statistical tests (e.g., if `nintendo_sales` or `other_sales` have less than 2 samples). This would cause `ttest_ind` to raise an error. The ground truth includes `if len(group) < 2` checks.
    *   No check for the validity of the contingency table (e.g., if it's all zeros or empty) before the chi-square test. This could lead to errors if filtering for top genres/platforms results in an invalid table.

3.  **Completeness of Interpretation:**
    *   The interpretation is very brief (""Significant"" or ""Not Significant""). The ground truth provides full null and alternative hypotheses, explains what ""significant"" means in context, and states the direction of the effect for the one-tailed test (e.g., ""Nintendo games have significantly higher..."").
    *   It doesn't display the T-statistic or Chi-square statistic, which are useful outputs.

4.  **One-sided vs. Two-sided Test:**
    *   For the Nintendo vs. Other Publishers test, the prompt explicitly asks: ""Test whether Nintendo games have significantly *higher* global sales"". This implies a one-tailed test. The generated code uses the default `two-sided` alternative for `ttest_ind` for this specific test, while the ground truth correctly uses `alternative='greater'`.

5.  **Hardcoded Significance Level:**
    *   The `ALPHA` value (0.05) is hardcoded repeatedly (`0.05`) instead of being defined as a constant at the top as good practice dictates.

6.  **Readability/Structure:**
    *   Mixing parts of the cleaning and testing (e.g., `df['Year_of_Release'] = pd.to_numeric(...)` right before the Mann-Whitney test) can make the flow less clear compared to having a dedicated cleaning section.",gpt-4.1-nano,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values by dropping rows with missing target or features
df.d...",7,"The generated code correctly addresses most of the requirements and demonstrates a good understanding of the task.

**Strong points:**
*   Correctly loads data and handles missing values.
*   Accurately creates the 'High_Sales' target variable.
*   Properly encodes categorical features using `LabelEncoder`.
*   Successfully creates the derived features 'Total_Regional_Sales' and 'Years_Since_Release'.
*   Correctly splits data using `train_test_split`.
*   Trains and evaluates the Random Forest Classifier using the specified metrics (accuracy, precision, recall, F1, confusion matrix).
*   Displays feature importance.

**Areas for improvement:**
*   **Feature Selection for Model Training:** The generated code defines `feature_cols = ['Platform', 'Genre', 'Publisher', 'Total_Regional_Sales', 'Years_Since_Release']`. However, the 'Total_Regional_Sales' is derived from 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales'. It is generally better to include the original regional sales features (`NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`) along with or instead of just the derived `Total_Regional_Sales` if they are intended to be part of the predictive process. The ground truth code correctly includes both the individual regional sales and the derived total. This omission in the generated code might lead to less comprehensive feature importance or model performance.
*   **Missing Value Handling (Year Conversion):** The generated code drops NaNs for 'Year' but doesn't explicitly convert 'Year' to integer type *after* dropping. While pandas often handles this implicitly for numerical columns after dropping, explicit conversion (`df['Year'] = df['Year'].astype(int)`) ensures consistency, especially before calculating `Years_Since_Release` (though it likely wouldn't cause an error here if `Year` is already numeric).
*   **Feature Importance Display:** While it displays feature importance, the use of `matplotlib.pyplot` and `seaborn` for visualization was not explicitly requested in the prompt. The prompt only asked to ""Display feature importance"", which could be a simple printout of the sorted values (as in the ground truth). Including plotting libraries adds a dependency that wasn't strictly necessary for the problem statement.
*   **Readability/Comments:** The ground truth code includes more granular comments for each step, which improves readability and understanding for others (or oneself in the future). The generated code has section headers but fewer inline comments.
*   **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.",gpt-4.1-nano,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures, StandardScaler

# 1) Load dataset and handle missing values
df = pd.read_csv('video_games_sales.csv')
df.fillna(0, inplace=True)

# Ensure relevant columns are numeric
numeric_cols = ['Global_Sales', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']
for col in numeric_cols:
    df[col] = pd.to_numeric(df[col], er...",6,"The generated code addresses most of the requirements, demonstrating an understanding of the task. However, there are several inaccuracies and areas for improvement, particularly concerning missing value handling, feature engineering logic, and the implementation of polynomial and interaction features.

**Strengths:**
*   **Most Features Attempted:** The code attempts to create all specified engineered features, perform correlation analysis, PCA, polynomial features, interaction features, and outlier handling.
*   **Correct PCA Steps:** The PCA implementation correctly scales features, fits PCA, and plots the cumulative explained variance.
*   **Correct Outlier Capping Logic:** The IQR-based outlier capping method is generally correct.
*   **Visualization:** Includes correct plots for correlation heatmap and PCA explained variance.

**Areas for Improvement/Corrections:**
1.  **Missing Value Handling (Critical):**
    *   `df.fillna(0, inplace=True)` is generally a poor strategy for all missing values without considering the column type (e.g., Year, Publisher, Genre). This can drastically alter the data's meaning and lead to incorrect calculations. The ground truth uses `SimpleImputer` with appropriate strategies (median for Year, mode for Publisher/Genre), which is much more robust.
    *   The `Year` column is not properly handled for missing values; `df.fillna(0)` would make `Year` 0, which is incorrect. The `pd.to_numeric` part handles non-numeric 'Year' but then fills NaNs with 0, which is still problematic.
    *   For `Sales_Ratio_NA_EU`, replacing `EU_Sales` 0s with NaN *then* replacing NaNs with 0 again effectively means division by zero results in 0, which is not standard. A small epsilon as in the ground truth is a more robust way to handle this for ratios.
2.  **Engineered Features - Logical Errors:**
    *   **`Sales_Ratio_NA_EU`:** The `replace(0, np.nan)` followed by `replace(np.nan, 0)` could lead to unexpected behavior if `EU_Sales` is legitimately 0. The purpose is to avoid `inf` values, but setting them to 0 isn't ideal. The ground truth's approach using `(EU_Sales + 1e-6)` denominator is better.
    *   **`Platform_Popularity`:** `df['Platform'].value_counts()` creates a Series mapping platform names to counts. `df['Platform'].map(platform_counts)` is correct for mapping these counts back to the DataFrame. This is generally fine, but `transform('count')` is slightly more idiomatic for broadcasting grouped results back to the original index.
    *   **`Sales_Momentum`:** Requires `Year` to be properly imputed first. Its reliance on the initial `fillna(0)` for `Year` can produce incorrect results.
3.  **Polynomial Features:**
    *   The polynomial features are generated and put into a new DataFrame `sales_poly_df`. However, this DataFrame is *not* concatenated back to the main `df`. The task states ""analyze their impact"" and the implied understanding is that these features should be added to the dataset for further analysis, which was done by the ground truth. The `corr_with_global` is a partial analysis but doesn't fully integrate them.
4.  **Interaction Features:**
    *   **`Year_Genre_Interaction` (`df['Year'].astype(str) + '_' + df['Genre']`) is merely a categorical string concatenation, not a numerical interaction feature as typically understood (e.g., Year * one-hot encoded Genre).** The task likely intended numerical interaction, which is what the ground truth implemented using one-hot encoding for genre and then multiplying by year. This is a significant deviation from the requirement.
5.  **Data Type Consistency:** The ground truth code makes explicit efforts to ensure data types after imputation (e.g., converting `Year` to `int`). The generated code is less explicit; this can lead to issues downstream.
6.  **Readability/Best Practices:**
    *   Lack of comments for complex sections.
    *   Missing `import warnings` and `warnings.filterwarnings('ignore')` which is good practice for benchmark scripts.
    *   Error handling for `pd.read_csv` is missing (though minor).
    *   `Rank` column is not explicitly removed from numerical features for correlation or PCA, though it should be. The ground truth correctly identifies and removes it as it's an identifier, not a true numerical feature for such analyses.
7.  **Outlier Handling Column List:** The `numeric_cols` defined at the beginning only covers sales related columns; the outlier handling loops over `numeric_cols + ['Sales_Ratio_NA_EU', ...]` but `numeric_cols` already contains the sales columns. It's not a bug, but can be cleaner. The `numeric_cols` might not include all the *newly engineered* numeric features, which the ground truth included.",gpt-4.1-nano,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
import statsmodels.api as sm

# Load dataset
df = pd.read_csv('diabetes.csv')

# List of columns to replace zeros with NaN (assuming these are features where zero is invalid)
zero_as_nan_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

# Replace zeros with NaN
for col in zero_as_nan_cols:
    df[col] = df[col].replace(0, np.nan)

# Median imputation
for col in zero_as_nan_cols:
    median_value = df[c...",7,"The generated code correctly implements all the requested statistical tests and handles the data cleaning as specified. The variable names are clear, and the code follows a logical flow.

Areas for improvement:
1.  **Interpretation Detail**: While the code prints ""Significant"" or ""Not Significant"", the ground truth provides a much more detailed interpretation, explaining *what* is significant (e.g., ""diabetic patients have significantly higher glucose levels""). This level of detail was requested in the prompt (""interpret results with significance level =0.05"").
2.  **Output Clarity**: The ground truth code includes print statements to preface each test's results and interpretations, making the output easy to follow. The generated code presents the p-values sequentially at the end, which is less user-friendly for understanding each test's context.
3.  **Robustness (File Handling)**: The ground truth includes a `try-except` block for file loading, which is good practice. The generated code assumes the file will always be present.
4.  **Minor Logic in BMI Categories**: The prompt specifies BMI categories as `Underweight: <18.5, Normal: 18.5-24.9, Overweight: 25-29.9, Obese: >=30`. The `right=False` in `pd.cut` creates intervals like `[0, 18.5)`, `[18.5, 24.9)`, etc. This means 18.5 would fall into 'Normal' but 24.9 would fall into 'Overweight', which is inconsistent with `Normal: 18.5-24.9` (inclusive of 24.9). The ground truth's `right=False` for ANOVA was correctly used for `[0, 18.5)`, `[18.5, 25)`, `[25, 30)`, `[30, inf)`. This is a subtle difference, but precise binning is important.
5.  **Unused Import**: `statsmodels.api as sm` is imported but not used.",gpt-4.1-nano,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import ...",7,"The generated code successfully tackles most of the requirements. It loads data, handles missing values, creates new features, performs scaling, splits data, and trains multiple models. It also includes hyperparameter tuning and basic visualizations.

However, there are several areas for improvement:
- **Feature Engineering Issues:**
    - The `SimpleImputer` is applied correctly for numerical missing values, but the subsequent handling of `Glucose_BMI_ratio` division by zero (filling with 0) is problematic. If a BMI was 0 (and replaced with NaN, then imputed to median), then if Glucose was 0 (and replaced with NaN, then imputed to median), the ratio could still theoretically be 0/median, which is fine, but the `np.inf` handling and then `fillna(0)` is confusing and potentially incorrect. It suggests there might have been a 0 BMI *before* imputation in the original data or an oversight in how `replace` and `impute` interact. The ground truth avoids this by only replacing 0s with NaNs and then imputing, meaning no 0s would remain for division issues *after* imputation.
    - The `Age_group` bins are incorrect. The ground truth uses `[0, 20, 30, ...]` and `right=True`, which creates standard age ranges. The generated code uses `[20, 30, ...]` and `right=False`, meaning a 20-year-old would fall into '20-30', whereas typically 0-20, 21-30 are used. The ground truth approach is more common for initial age binning when considering a full age range.
    - `Insulin_level` tertile calculation is correctly implemented for the bins, but applying the `insulin_category` function in a loop might be less efficient than `pd.qcut` which directly handles quantile-based categorization and is used in the ground truth.
- **Preprocessing Pipeline:** The generated code performs scaling separately from the categorical encoding and then scales the entire `X` dataframe. The ground truth uses `ColumnTransformer` and `Pipeline` which is a more robust and standard way to handle mixed data types (numerical and categorical) together during preprocessing, preventing data leakage and errors in feature alignment. This is crucial for maintaining proper feature names and ensuring correct scaling/encoding.
- **Completeness of Evaluation:**
    - The generated code evaluates models, but the `results` dictionary is only populated based on the last fit. The tuning for Random Forest is applied to `best_rf`, but the general `models` dictionary is not updated with the best model's fit results directly, potentially leading to plotting/reporting the untuned RF results unless manually overridden. The printing loop then re-evaluates `Random Forest` in the loop or uses the `best_rf`. The ground truth explicitly creates `Random Forest (Tuned)` as a new entry.
    - Confusion matrices are plotted individually, making comparison harder. The ground truth plots them in subplots for better visual comparison.
    - The feature importance for tree-based models is only shown for Random Forest. The task explicitly asks for ""tree-based models"" (plural), meaning Gradient Boosting's feature importance should also be displayed (which the ground truth does).
- **Redundant Code:** The `SimpleImputer` is used, which is good, but the initial loop replacing 0s with `np.nan` is still necessary if 0s are meant to signify missing values, as `SimpleImputer` does not inherently treat 0 as NaN unless specified in its `missing_values` parameter. This part is fine, but the method used in the ground truth (manual `median` imputation based on columns identified as having 0s as missing) is simpler if only median imputation is needed for *those specific 0s*.
- **Code Structure and Readability:** The ground truth code uses more comments, print statements for progress tracking, and better variable naming, which contributes to overall readability and maintainability. It also uses `StratifiedKFold` for cross-validation, which is better for imbalanced datasets, though not strictly required by the prompt, it's a best practice in ML.",gpt-4.1-nano,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

from imblearn.over_sampling import SMOTE

# Load dataset
df ...",6,"The generated code attempts to address all parts of the task but has several issues that affect its correctness and completeness, particularly in the handling of missing values, the order of operations for feature selection/dimensionality reduction, and the application of transformations.

**Strengths:**
*   **Good intent on feature engineering:** Creates the requested engineered features.
*   **Applies various techniques:** Includes polynomial features, different scaling, PCA, SelectKBest, RFE, SMOTE, and feature importance.
*   **Clear steps for each section:** The comments generally guide the user through the process.
*   **Visualization for feature importance:** Correctly generates a bar plot for feature importance.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling (Critical Issue):**
    *   The ground truth explicitly states that 0s in certain columns represent missing values and need to be imputed. The generated code *assumes* missing values are '?' or NaN and uses `dropna()`, which is incorrect for this dataset and will remove valid rows, potentially leading to a much smaller and biased dataset. This is a fundamental error for this specific dataset.
    *   The ground truth also uses `SimpleImputer` with 'median' strategy, which is absent in the generated code.

2.  **Order of Operations (Major Issue):**
    *   **PCA and Feature Selection:** The generated code applies PCA *before* *any* feature selection (SelectKBest, RFE, correlation filtering) and then applies feature selection *on the PCA components*. This is not standard practice for feature selection. Feature selection (like SelectKBest, RFE, and correlation filtering) is typically performed on the *original or scaled features* to identify important and non-redundant features *before* dimensionality reduction like PCA, or sometimes after feature selection.
    *   The task explicitly asks for correlation-based feature filtering to remove highly correlated *features*, which implies operation on the actual features, not on abstract PCA components.
    *   The `X_pca` is used as input for `SelectKBest` and `RFE`, and then `X_df = pd.DataFrame(X_pca)` is created for correlation filtering. This means these feature selection methods are operating on principal components, not the original engineered features, which makes the interpretation of selected features or dropped correlated features difficult (as PCA components are linear combinations of original features).

3.  **Polynomial Feature Application:**
    *   The generated code applies `PolynomialFeatures` to a *subset* of the features (`poly_features = ['Glucose', 'BMI', 'Insulin', 'Age']`). While this is a choice, the chosen method of dropping the original `poly_features` *after* concatenating the polynomial features means that the original features are permanently removed from `X` before scaling, which might not be desired. The ground truth approaches this by dropping the original poly\_cols *after* creating new polynomial features and then concatenating, which is a cleaner way to *replace* selected features with their polynomial expanded versions.

4.  **Inconsistent Data Flow:**
    *   Scaling is done twice (`X_scaled_std`, `X_scaled_minmax`), but only `X_scaled_std` is used for PCA (`X_pca_input`). This is fine, but the subsequent steps for feature selection (`SelectKBest`, `RFE`) are applied to `X_pca`, while correlation filtering is applied to `X_pca`'s DataFrame, and then SMOTE is applied to `X_filtered_scaled` which is *again* scaled (`X_filtered_scaled = scaler_std.fit_transform(X_filtered)`). This implies redundant or confused scaling. The ground truth uses `X_processed = X_scaled_standard_df` as the consistent input for subsequent steps.
    *   `X_pca` is split into `X_train, X_test`, but then `SelectKBest` and `RFE` are fit on `X_train` which is *already* PCA-transformed. The task asked for these techniques *alongside* PCA, implying they should be applicable to the original feature space or a consistently preprocessed one.

5.  **Feature Importance Interpretation:**
    *   The feature importance visualization uses `X_filtered.columns` for `feature_names`. Since `X_filtered` itself was derived from `X_pca`, the column names are just numerical indices (e.g., 0, 1, 2...) for the PCA components, making the feature importance plot uninterpretable in terms of original or engineered features. The ground truth correctly applies feature importance to a feature set *before* PCA for interpretability.

6.  **Missing `LogisticRegression` for RFE:** The RFE implementation for the ground truth uses `LogisticRegression` as the estimator, which is typical. The generated code uses `RandomForestClassifier`, which is also a valid choice but makes the comparison slightly different.

Overall, the generated code demonstrates an understanding of the individual components but struggles with their strategic integration and proper application within a coherent data science pipeline, especially given the specific requirements of the dataset and task. The misinterpretation of missing values is a crucial flaw for this specific problem.",gpt-4.1-nano,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
import statsmodels.api as sm
import statsmodels.formula.api as smf

# 1) Load dataset and handle missing values
df = pd.read_csv('world-happiness-report-2021.csv')
df = df.dropna()

# Display basic info to understand structure
# Assuming the necessary columns based on typical dataset structure
# For example:
# 'Country or region', 'Regional indicator', 'Ladder score' (happiness score),
# 'GDP per capita', 'Social support', ...",7,"The generated code addresses most of the tasks and correctly implements the requested statistical tests. However, there are significant areas for improvement, particularly regarding data handling and the detail of interpretations.

**Strengths:**
- Correctly uses `pandas`, `numpy`, and `scipy.stats` for data manipulation and statistical tests.
- Implements all requested tests: t-test (multiple times), Mann-Whitney U, ANOVA, Pearson correlation, and Chi-square.
- Defines and uses the significance level (alpha).
- Computes and displays p-values for all tests.

**Areas for Improvement (Why it's not higher):**

1.  **Missing Value Handling:**
    *   The generated code uses `df.dropna()` without specifying `subset` of critical columns. This is less robust than the ground truth, which identifies vital columns and only drops rows if *any* of those specific columns have NaNs. Dropping all NaNs might lead to information loss if non-critical columns have missing values.
    *   It doesn't print any information about how many rows were dropped, which is good practice for transparency in data cleaning.

2.  **Dataset Column Names:**
    *   The generated code assumes `GDP per capita` and `Freedom`. The actual column names in the `world-happiness-report-2021.csv` are `Logged GDP per capita` and `Freedom to make life choices`. This will cause `KeyError` unless the dataset columns were manually renamed or the code was run on a modified dataset. This is a critical error for correctness. I have to assume the actual column names in the provided dataset.

3.  **Interpretation of Results (Crucial for a ""expert code evaluator"" task):**
    *   The interpretation section (point 8) is very generic. It only states ""Statistically significant (reject null hypothesis)"" or ""Not statistically significant (fail to reject null hypothesis)"".
    *   The task explicitly asks for an interpretation *with* the results (e.g., ""Western European countries have significantly HIGHER happiness scores..."", ""The top 25% happiest countries have significantly HIGHER GDP per capita...""). The generated code does not provide this specific direction in its final output, which is a key part of the prompt's requirement for ""interpret results with significance level \u03b1=0.05"".
    *   The ground truth code does this for each test immediately after the p-value is calculated, providing much clearer and more practical insights.

4.  **Redundancy/Clarity in ANOVA preparation:**
    *   `anova_df = df.groupby('Regional indicator')['Ladder score'].apply(list).reset_index()` is created but never used. The subsequent `anova_groups` correctly prepares the data, making the `anova_df` line redundant.

5.  **Chi-square Happiness Category Definition:**
    *   While using `pd.cut` is a valid approach, using quantiles `0.33` and `0.66` might not necessarily create groups that are *roughly* equal in size if there are many duplicate values at the cut-off points. The ground truth's `1/3` and `2/3` also relies on this assumption but is a common practice. The main issue isn't the method, but ensuring it results in meaningful categories, which it likely does.

6.  **Minor Code Comments/Structure:**
    *   The comments like `# Assuming the necessary columns...` and `# For example:` indicate uncertainty about the actual column names or structure, which points to a potential lack of prior data exploration/understanding. While good for human understanding, they indicate a potential fragility.

To improve, the code should:
*   Use the exact column names from the dataset or implement robust error handling if they are not found.
*   Provide detailed, directional interpretations for each statistical test result.
*   Improve the missing data handling by explicitly mentioning the affected columns and reporting the number of rows removed.",gpt-4.1-nano,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import C...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being fully correct and high-quality.

**Correctness Issues:**
*   **Feature/Target Definition:** The ground truth correctly identifies the specific features (e.g., 'Logged GDP per capita', 'Freedom to make life choices'). The generated code attempts to derive features dynamically by excluding 'Ladder score' and 'Country/Regional indicator' from numeric/categorical columns. This is problematic because the prompt explicitly lists the required features. More critically, the prompt specifies 'Freedom' which maps to 'Freedom to make life choices' in the dataset, and the generated code expects a 'Freedom' column which isn't directly available or handled correctly. Similarly for 'GDP per capita' vs 'Logged GDP per capita'. This leads to incorrect feature selection and will cause errors or incorrect models.
*   **Missing Value Handling:** The generated code uses `df.dropna()` on the entire DataFrame immediately after loading. This is too aggressive and will drop rows even if the missing values are in columns not relevant to the specified features or target. The ground truth correctly identifies and drops NaNs only from the *relevant* columns (`FEATURE_COLUMNS + [TARGET_COLUMN]`).
*   **Categorical Variable Handling for Features:** The prompt explicitly states the features are 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom', 'Generosity', and 'Perceptions of corruption'. All these are *numerical*. The generated code incorrectly assumes there are categorical features among the *specified* ones and attempts to apply `OneHotEncoder` to them. It includes generic `categorical_cols` in `X` which can lead to columns like 'Country name' or 'Regional indicator' being used as features, which were not specified in the prompt's feature list. The ground truth correctly notes that no specified features are categorical.
*   **Hyperparameter Tuning Target:** The generated code hardcodes hyperparameter tuning for `RandomForestRegressor`, stating ""assumed Random Forest here"". The prompt requires tuning the *best* model. While Random Forest is often a good candidate, dynamically selecting the best performing model from the initial training round (as the ground truth does) is more aligned with the prompt.
*   **Predicting for New Data (Feature Issues):** The `new_data` DataFrame in the generated code uses simplified column names like 'Logged GDP per Capita' (capital 'P') and 'Freedom', which may not match what the `ColumnTransformer` expects after processing the original dataset column names. The `new_data` also includes 'Country' and 'Regional indicator' initially, which are then dropped. This indicates a misunderstanding that these columns are not features required for prediction based on the model trained on the specified features. The model was trained on the *exact* column names from the dataset and it expects those names for new predictions. The ground truth defines the `hypothetical_countries_data` with the *exact* original column names.

**Code Quality Issues:**
*   **Clarity and Structure:** While it uses pipelines, the feature selection logic is convoluted and error-prone due to the generic `numeric_cols` and `categorical_cols` definitions combined with dropping specific names. The ground truth's explicit `TARGET_COLUMN` and `FEATURE_COLUMNS` lists are much clearer and less prone to errors based on implicit column types.
*   **Redundant Warnings Filter:** `import warnings; warnings.filterwarnings('ignore')` is generally bad practice, hiding potentially useful warnings.
*   **Lack of Intermediate Output/Debugging:** The ground truth includes `df.info()`, `df.head()`, and checks for missing values *before* handling, which is crucial for understanding the data and verifying preprocessing steps. The generated code lacks such informative prints.

**Completeness Issues:**
*   **Residual Plots for ""Best Model"":** The residual plot is generated for `best_rf_model` regardless of whether RF was actually the best overall. The ground truth ensures the plot is for the actual best model.
*   **Missing Analysis for Hypothetical Countries:** The ground truth provides a brief analysis of the predictions for hypothetical countries, fulfilling a requirement. The generated code just prints the scores.
*   **Specific Feature Names:** The prompt specifically listed features like ""GDP per capita"", ""Freedom"", ""Perceptions of corruption"". The ground truth explicitly maps this to the dataset's 'Logged GDP per capita', 'Freedom to make life choices', 'Perceptions of corruption', which is a crucial detail. The generated code does not handle this mapping, assuming simplified names like 'Freedom' would work which they don't in the context of the dataset's actual column names.

In summary, the generated code demonstrates an understanding of pipelines and basic machine learning steps, but fails significantly in precise data preprocessing, feature selection according to the prompt's specifications, and dynamic selection of the best model for tuning and plotting.",gpt-4.1-nano,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import mutual_info_regression
from sklearn.base import clone
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt...",7,"The generated code addresses most of the requirements of the task, including feature engineering, polynomial features, multiple scaling methods, dimensionality reduction, feature selection, and feature importance validation. However, there are several areas where it falls short compared to the ground truth and best practices, affecting its correctness and completeness.

**Areas for Improvement:**

1.  **Preprocessing and Data Consistency:**
    *   The `features` list initially defined (`Life Ladder`, `Logged GDP per capita`, etc.) doesn't align with the column names (`Ladder score`, `Logged GDP per capita` in Ground Truth). This led to errors needing manual correction. The task provided `world-happiness-report-2021.csv`, which uses 'Ladder score', 'Freedom to make life choices', 'Perceptions of corruption' etc. The generated code incorrectly assumes different column names and tries to fix them, leading to `KeyError` unless modified. This is a significant issue.
    *   Missing value handling is less robust. The ground truth explicitly fills NaNs in 'Perceptions of corruption' with the mean and then drops any remaining NaNs in core columns. The generated code only uses `dropna` on a subset, which might leave NaNs in other columns, impacting subsequent operations like `X_combined` or `X_final`.
    *   The target variable (y) is created using 'Life Ladder', which is also present in `X`. This is incorrect, as the target should not be part of the features. The ground truth correctly removes the target from numerical features for `X`.

2.  **Engineered Features:**
    *   `Social_Wellness_Index`: The ground truth uses `addition` (`+`) for combination, while the generated code uses `multiplication` (`*`). Both are valid interpretations, but the request was ""combination of Social support and Healthy life expectancy"", and addition is a more common initial approach for indices unless a specific multiplicative relationship is implied.
    *   `Governance_Score`: The ground truth correctly uses `(1 - Perceptions of corruption)` to make 'low corruption' (which means a higher score) contribute positively. The generated code similarly uses `1 - Perceptions of corruption`, but the `Freedom` column name differs from the ground truth.
    *   The generated code uses 'Ladder rank' in its `dropna` subset, which was not explicitly mentioned or used in the ground truth for core processing.

3.  **Polynomial Features:**
    *   The `key_factors` list has the same column name mismatch issue as general features (`Freedom` vs `Freedom to make life choices`).
    *   The `X_poly_df` and `X_combined` are created using `X` which still contains the target column 'Life Ladder' (as `feature_cols` includes it). This means polynomial features are generated for the target variable as well as other features, which is incorrect. The ground truth correctly separates X and y before this step.

4.  **Dimensionality Reduction:**
    *   PCA: The ground truth provides output about explained variance ratio, which is good practice for understanding PCA results. The generated code lacks this.
    *   t-SNE: While the visualization is present, the ground truth includes a note about applying t-SNE on PCA-reduced data for efficiency, and checks `n_components_tsne_input` dynamically, which is a better approach for larger datasets. The generated code applies t-SNE directly on the full `X_scaled`.

5.  **Feature Selection:**
    *   `SelectKBest`: The output of SelectKBest (`X_kbest`) is not used or linked back to feature names, making it hard to see which features were selected. The ground truth explicitly prints the selected feature names. Also, `kbest_scores` refers to `X.columns`, which is problematic because `X` doesn't contain the polynomial and interaction features.
    *   `RFE`: Similar to SelectKBest, the `selected_features_rfe` uses `X.columns` which again misses the full set of features.
    *   `mutual_info_regression`: The ground truth explicitly calls `mutual_info_regression` for scores, while the generated code uses `SelectKBest` with `f_regression` and then just the `mutual_info_regression` function. The process is a bit disjointed. The ground truth displays all mutual information scores.

6.  **Interaction Features (`PolynomialFeatures` vs direct multiplication):**
    *   The prompt asked for interaction features ""between economic and social factors"". The generated code uses `PolynomialFeatures(interaction_only=True)` on *specific* features (`Logged GDP per capita`, `Social support`). The ground truth implements two specific direct multiplications (`X * Y`), which is more direct for the specified task of creating *interaction features*. Both are valid, but the ground truth's approach is simpler for the given specific interactions.
    *   The interaction features are added to `X_final` by concatenating with `X_scaled`. This implies that `X_scaled` didn't already have these, which is inconsistent if `X_final` is supposed to contain *all* features. The ground truth incorporates interaction features earlier into the `df_processed` which then flows into `X`.

7.  **Overall Code Structure and Clarity:**
    *   The ground truth explicitly states what each step does (e.g., ""--- 1. Load and Preprocess the Dataset ---""). This makes the code easier to follow. The generated code lacks such clear sectioning.
    *   The ground truth handles potential `FileNotFoundError` for the CSV, which is robust. The generated code does not.
    *   The generated code uses `df.dropna(subset=features + ['Country', 'Regional indicator', 'Ladder rank'])`, which is overly specific and potentially problematic if `Ladder rank` or `Country` has NaNs, as these might not be relevant for dropping *all* data for the core analysis. The ground truth's `dropna` after filling ""Perceptions of corruption"" is more focused.

8.  **Output and Visualization:**
    *   While visualizations are present, the ground truth provides more informative print statements throughout the process, showing the shape of dataframes after transformations, selected features, and explained variance, which aids in debugging and understanding the results.

In summary, the generated code attempts all parts of the task but struggles with data consistency, correct variable usage (especially regarding the target variable and feature names), and providing comprehensive outputs. The column name mismatch is a critical error that makes the code fail to execute as-is with the provided dataset without manual intervention.",gpt-4.1-nano,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# 1) Load the dataset, parse 'date', filter for 'USA'
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
usa_data = df[df['location'] == 'USA'].copy()

# 2) Set 'date' as index
usa_data.set_index('date', inplace=True)

# 3) Resample to weekly frequency (sum of new...",6,"The generated code correctly performs most of the required steps, including loading and filtering data, resampling, ADF test, and plotting ACF/PACF. However, there are several issues that reduce its score:

1.  **Decomposition Plotting (Major Issue):** The generated code attempts to plot the decomposed components manually using `plt.subplot` and `plt.plot`. While this is technically possible, `statsmodels.tsa.seasonal.seasonal_decompose` returns an object that has an inherent `plot()` method (e.g., `decomposition.plot()`). The task specifically asks to ""Plot the original time series and its decomposed components,"" which is best achieved and commonly understood to mean using the built-in `plot()` method for clarity and standard presentation. The manual plotting also misses setting a `period` for decomposition, which is crucial for seasonal decomposition of weekly data (e.g., 52 for yearly seasonality). Without `period`, `seasonal_decompose` might default to an incorrect or too small period, making the decomposition results potentially meaningless.

2.  **Missing `period` in `seasonal_decompose` (Critical):** As mentioned above, the `seasonal_decompose` function *must* be provided with a `period` argument, especially for weekly data, to correctly identify seasonality. A value of 52 (weeks in a year) is typical for annual seasonality in weekly data. Without it, the decomposition (especially seasonality and residuals) will likely be incorrect.

3.  **Error Handling/Robustness (Missing):** The generated code lacks any error handling for file not found, empty dataframes, or insufficient data points for statistical tests/plots (e.g., ADF test requires a minimum number of observations, as do ACF/PACF plots). This makes the code fragile.

4.  **Incomplete ADF Plotting/Interpretation:** While it prints the ADF results, it doesn't explicitly state the conclusion about stationarity (e.g., ""Series is stationary"" or ""Series is non-stationary and differenced""). The ground truth provides more verbose and helpful output.

5.  **Plotting Redundancy:** The decomposition plot and ACF/PACF plots are shown without titles or clear separation from the output, making them less user-friendly than the ground truth's approach.

6.  **Readability/Structure:** The code is functional but presented as a direct script. Encapsulating the logic within a function (as in the ground truth) would improve reusability and organization for a ""comprehensive time series analysis.""",gpt-4.1-nano,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import f_oneway

# Load dataset
df = pd.read_csv('winequality-red.csv')

# Categorize pH into groups
def categorize_ph(pH):
    if pH < 3.2:
        return 'Low pH'
    elif pH <= 3.4:
        return 'Medium pH'
    else:
        return 'High pH'

df['pH_group'] = df['pH'].apply(categorize_ph)

# Calculate average quality for each group
group_stats = df.groupby('pH_group')['quality'].agg(['mean', 's...",6,"The generated code correctly performs most of the required tasks but has significant issues in implementation and completeness, particularly regarding the confidence intervals and plotting.

**Strengths:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Correct Data Loading:** The CSV file is loaded correctly.
*   **Correct pH Categorization:** The custom function accurately categorizes pH values.
*   **Correct Group Mean Calculation:** It correctly calculates the mean and standard deviation for each group.
*   **Correct ANOVA:** The `f_oneway` function is correctly used to perform the ANOVA test.

**Weaknesses and Areas for Improvement:**
*   **Incomplete pH Group Labels:** The generated code uses ""Low pH"", ""Medium pH"", ""High pH"", which are not as descriptive as the requested labels (""Low pH (< 3.2)"", ""Medium pH (3.2 to 3.4)"", ""High pH (> 3.4)""). This affects adherence to the prompt and clarity in output/plots.
*   **Incorrect Confidence Interval Calculation:** The prompt specifically requested 95% confidence intervals using the t-distribution for small sample sizes, which is standard practice when the population standard deviation is unknown. The generated code uses `1.96 * ses`, which is a z-score approximation (valid for very large samples or known population standard deviation) and does not account for degrees of freedom, making the CI less accurate, especially for groups with smaller sample sizes.
*   **Missing Ordering of pH Groups:** The `groupby` and plotting do not explicitly order the pH groups, which can lead to ""High pH"", ""Low pH"", ""Medium pH"" order in plots and prints (alphabetical order by default from `groupby`). The Ground Truth code explicitly orders the categorical variable.
*   **Incomplete Visualization:**
    *   The `yerr` in `plt.bar` expects a single value (or array) per bar, representing the distance from the mean to the confidence interval boundary. The generated code passes `confidence_interval` which is `1.96 * ses`. While this provides some error bars, it's not truly representative of the full confidence interval as calculated by a t-distribution and does not explicitly create the lower and upper bounds for the plot's Y-axis or internal calculations.
    *   There's no printing of the calculated CI values, which was part of the reporting requirement in the ground truth.
    *   No labels on the bars showing the exact mean quality.
*   **Lack of Readability/Robustness:** The use of a simple `if/elif/else` for categorization is functional but less robust and harder to manage for more complex binning scenarios than `pd.cut`, which also handles edge cases (`right=False`, `include_lowest=True`) more precisely and explicitly.
*   **ANOVA Output Detail:** The ANOVA output is minimal. The ground truth includes a conclusion based on the p-value against a significance level, which is a good practice for interpreting statistical tests.

In summary, while the code gets the main components done, the details of the confidence interval calculation and presentation of results, especially the visualization, are below standard best practices for statistical analysis.",gpt-4.1-nano,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load dataset and parse 'date'
df = pd.read_csv('syntheti...",9,"The generated code correctly addresses almost all requirements of the task. It loads the data, defines features and target, handles missing values, encodes categorical features, scales numerical features, splits data, trains both specified models, and evaluates them with the required metrics.

**Strong points:**
- Excellent use of `Pipeline` and `ColumnTransformer` for preprocessing. This is a very clean and robust way to handle multiple preprocessing steps and prevents data leakage by applying `fit_transform` only on the training set within the pipeline. This approach is arguably superior to the ground truth which scales after the split, but outside of a pipeline, creating a slightly less integrated workflow.
- Correctly uses `SimpleImputer` for handling missing values.
- Includes all specified models and evaluation metrics.
- Output formatting is clear.

**Areas for improvement (leading to a score of 9 instead of 10):**
- **Missing value handling for `continent`:** The prompt specifically requested handling missing values in ""selected features and target by imputing with the median."" While the ground truth implicitly handled `continent` by letting `get_dummies` create columns with `NaN` if there were `NaN`s in `continent` and then the median imputation would not apply to them, the generated code doesn't explicitly address missing values in the `continent` column. `OneHotEncoder(handle_unknown='ignore')` does not handle `NaN` values directly; it would treat them as a separate category if they are passed as `NaN` (depending on the scikit-learn version and if `sparse_output` is True). For robustness, one might want to impute `NaN`s in `continent` (e.g., with ""Unknown"" or the mode) *before* one-hot encoding, as `OneHotEncoder` expects non-missing categorical data. The current setup will likely raise an error if `continent` contains `NaN`s (unless `handle_unknown='ignore'` allows `NaN`s to be treated as an unknown category, which is not its primary function). The ground truth implicitly skirts this by not explicitly imputing `continent` *if* `get_dummies` handles `NaN` as a distinct category or if there are no `NaN`s. The problem statement asked to impute all selected features with median, which is inappropriate for `continent`. The ground truth implements this correctly by only imputing *numerical* columns with median. The generated code's imputer for `X[features]` only covers numerical features, which is correct given the median strategy, but it misses `continent` entirely. A simple `X['continent'].fillna('missing', inplace=True)` before `ColumnTransformer` would make it more robust.

- **Missing value handling for `target`:** The prompt specified imputing missing values in the target by imputing with the median. The ground truth does this correctly. The generated code also handles this with `SimpleImputer` for `y`, but the `revel()` and `reshape()` operations are a bit verbose. While functional, it's a minor point.

- **Imputation in `X[features] = imputer_features.fit_transform(X[features])`**: While `fit_transform` is used, this operation is done *before* the train-test split and outside of the pipeline. This is a potential data leakage issue. Ideally, all preprocessing (including imputation and scaling) should happen *inside* the pipeline, applied only on the training data (`fit_transform`) and then on the test data (`transform`). The current code imputes the *entire dataset* before splitting, which means information from the test set's median values is used to impute the training set. The ground truth correctly handles this by imputing the full dataset, which aligns with common practice for datasets with missing values *before* splitting, especially if the imputation strategy is median/mean globally. However, if the imputation was more complex (e.g., k-NN imputer), it would be a major data leakage concern. For `median`, it's less critical but still potentially problematic in strict academic settings. The ground truth explicitly states ""Scaling is performed after splitting to prevent data leakage from the test set into the training process,"" but then imputes *before* splitting, leading to a subtle inconsistency. The optimal approach (if adhering strictly to ""no leakage"") would be to include `SimpleImputer` for numerical features within the `ColumnTransformer` inside the pipeline.

Overall, the generated code demonstrates a strong understanding of best practices (pipelining) which is commendable. The minor issues around missing value handling for `continent` and imputation before split are the only drawbacks.",gpt-4.1-nano,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"import pandas as pd
import numpy as np
from scipy import stats
import statsmodels.api as sm
import statsmodels.formula.api as smf

# Load dataset
data = pd.read_csv('synthetic_covid19_data.csv')

# Handle missing values with median imputation
columns_to_impute = ['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']
for col in columns_to_impute:
    if col in data.columns:
        median_value = data[col].median()
        data[col].fillna(median_value,...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being fully correct and robust.

**Correctness Issues:**

1.  **ANOVA Data Preparation:** The ground truth code correctly uses all data points for `reproduction_rate` for the ANOVA test within the specified continents. The generated code, however, incorrectly filters the data for ANOVA to `latest_data = data_filtered.sort_values('date').groupby('country').tail(1)`. This means it only takes the *latest entry per country* within the filtered continents, which is not what ANOVA requires for comparing means across general continent data. ANOVA should use all available data points for reproduction rate within those continents, not just the latest per country. This fundamentally alters the data used for the ANOVA test, making it incorrect for the task's stated objective.
2.  **Missing Value Handling for Death Rate Ratio and T-test:**
    *   When calculating `latest_country_data['death_rate'] = latest_country_data['total_deaths'] / latest_country_data['population']`, the generated code does not handle potential `np.inf` or `-np.inf` values that can arise from division by zero or NaN `population` values explicitly. The ground truth handles this by using `replace([np.inf, -np.inf], np.nan, inplace=True)` and then `dropna`. This can lead to `inf` values polluting the `death_rate` column and subsequently affecting the `quantile` calculation and the `high_death_rate_country` classification.
    *   The generated code uses `median_age_groups = latest_country_data[['country', 'median_age', 'high_death_rate_country']].dropna()`. While `dropna()` is used, it loses the `death_rate` calculation's potential `inf` issue mentioned above. More importantly, it should ensure `median_age` itself is not NaN *before* the t-test (though median imputation should have handled this, a robust check is good). The ground truth correctly drops NaNs from `death_rate_ratio` before proceeding.

**Code Quality and Completeness Issues:**

1.  **Missing Imports and Unused Imports:** `statsmodels.api` and `statsmodels.formula.api` are imported but not used. The prompt requests `statsmodels`, but since it's not used, it's unnecessary clutter.
2.  **Lack of Output Clarity:** The prompt explicitly asks to ""state the null hypothesis, print the p-value, and conclude whether to reject the null hypothesis at =0.05"". While the generated code prints the p-value and conclusion, it doesn't explicitly state the null hypothesis for each test in the output, which the ground truth does. The prompt also asks for a consistent alpha of 0.05, but this is hardcoded (0.05) instead of defined as a variable (e.g., `ALPHA = 0.05`), which affects maintainability.
3.  **Intermediate Prints/Feedback:** The ground truth includes helpful print statements for debugging and understanding data processing steps (e.g., ""Data Loading and Imputation Complete,"" handling missing values warnings, ANOVA warnings), which are missing in the generated code.
4.  **Robustness Checks:** The ground truth includes checks like `if len(reproduction_rates_by_continent) >= 2:` for ANOVA and `if len(df_corr) > 1:` for Pearson correlation to prevent errors from insufficient data, which the generated code lacks. Similarly, checks for `empty()` dataframes before t-tests are important.

**Strengths:**

*   **Correct Imputation Logic:** The median imputation logic is correctly implemented.
*   **Correct Test Selection:** ANOVA, Pearson, and T-test are correctly identified and used for the respective tasks, albeit with data preparation issues for ANOVA.
*   **Correct Interpretation of P-values:** The logic for rejecting/failing to reject the null hypothesis based on alpha is correct.
*   **High Death Rate Category Calculation:** The logic to derive the `high_death_rate_country` seems mostly correct, assuming the `death_rate` calculation is clean and doesn't hit inf issues.",gpt-4.1-nano,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select the specified numerical columns
cols = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
selected_df = df[cols]

# Calculate the correlation matrix
corr_matrix = selected_df.corr()

# Plot the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt="".2f"")
plt.title('Correlation Matrix...",9,"The generated code correctly implements all specified requirements: loading the dataset, selecting the numerical columns, calculating the correlation matrix, and visualizing it with a heatmap. It includes all necessary imports and follows a logical flow. The visualization parameters (`annot`, `cmap`, `fmt`) are well-chosen.

Minor points for improvement (which do not detract significantly from the score for this specific task):
- The ground truth code includes error handling (e.g., `FileNotFoundError`, `KeyError`), which is good practice for more robust applications, but not strictly required by the prompt's core task.
- The ground truth also uses a function and explicit variable names for dataset path and column names, which can improve modularity and readability in larger projects. However, for a concise script like this, the direct approach is perfectly acceptable.
- The `figsize` in the generated code is slightly smaller than the ground truth, but this is a subjective aesthetic choice and both are reasonable.",gpt-4.1-nano,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Group by 'rarity' and calculate the required averages
grouped = df.groupby('rarity').agg({
    'elixirCost': 'mean',
    'Win Rate': 'mean',
    'Usage': 'mean'
})

# Display the results
print(grouped)",10,"The generated code perfectly addresses all requirements of the task. It correctly loads the CSV, groups the data by 'rarity', calculates the mean for 'elixirCost', 'Win Rate', and 'Usage' for each rarity, and then displays the results. The use of `.agg()` is a good and explicit way to perform multiple aggregations with different functions, though `.mean()` on selected columns after grouping works just as well. The code also includes the necessary import and is well-structured.",gpt-4.1-nano,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Create target column
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)

# Select features
features = df[['elixirCost', 'Usage', 'rarity']]
target = df['is_high_win_rate']

# One-hot encode 'rarity' column
encoder = OneHotEncoder(sparse=False, drop='first')
rarity_encoded = encoder.fit_transform(features...",7,"The generated code correctly performs most of the required steps: loading data, creating the target column, and splitting data. However, there are a couple of issues related to feature selection and one-hot encoding that prevent it from being a perfect match to the efficient and idiomatic approach used in the ground truth.

**Areas for improvement:**

1.  **Feature Selection and `X` Initialization:** The line `features = df[['elixirCost', 'Usage', 'rarity']]` creates a DataFrame `features` that is then partially duplicated (`features[['elixirCost', 'Usage']]`) during concatenation. It's more common and cleaner to define `X` and `y` upfront. The `features` variable itself is a DataFrame, not just a list of column names, which can be slightly confusing.
2.  **`OneHotEncoder` vs. `pd.get_dummies`:** While `OneHotEncoder` is a valid tool, `pd.get_dummies` is generally simpler and more straightforward for in-place one-hot encoding within a DataFrame, especially when the task description doesn't explicitly require `sklearn.preprocessing.OneHotEncoder` for this step (it just says ""apply one-hot encoding""). The use of `drop='first'` in `OneHotEncoder` also changes the output compared to `pd.get_dummies` default behavior (which keeps all created dummy variables) and what's implicitly expected if the ground truth uses `drop_first=False`. For this type of benchmark, sticking to the most direct interpretation of the requirements is best.
3.  **Index Resetting during Concatenation:** Using `reset_index(drop=True)` before `pd.concat` is a common fix when indices might not align, but it adds an unnecessary step if `X` was prepared directly from `df` with aligned indices. The ground truth avoids this by applying `get_dummies` directly to the `X` DataFrame.

The code works and produces the desired outcome in terms of data split, but the `OneHotEncoder` implementation is slightly more convoluted than needed for this specific problem, and the `drop='first'` parameter results in a subtly different output for the `rarity` columns.",gpt-4.1-nano,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv('iris.csv')

# Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = df['Species']

# Encode target labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Split data into training and ...",9,"The generated code correctly implements all the requirements of the task: it loads the data, defines features and target, encodes the target, splits the data with the specified parameters, trains a KNN classifier with `n_neighbors=3`, makes predictions, and reports the accuracy. The imports are correct, and the code structure is clean and easy to follow.

One minor improvement, which is not strictly specified in the prompt but is good practice for classification tasks, would be to add `stratify=y_encoded` to the `train_test_split` function. This ensures that the class distribution in the training and testing sets is similar to the original dataset, which is especially important for imbalanced datasets, though less critical for a balanced dataset like Iris. The ground truth code includes this.

Additionally, the ground truth code includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good robustness measure. The generated code assumes the file exists. However, for a benchmark where the file presence is guaranteed, this isn't a critical omission based on the prompt's explicit requirements.",gpt-4.1-nano,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
iris_df = pd.read_csv('iris.csv')

# Filter data for the two species
versicolor = iris_df[iris_df['Species'] == 'Iris-versicolor']
virginica = iris_df[iris_df['Species'] == 'Iris-virginica']

# Perform an independent two-sample t-test on PetalLengthCm
t_stat, p_value = ttest_ind(versicolor['PetalLengthCm'], virginica['PetalLengthCm'])

# Null hypothesis (H0): The mean PetalLengthCm for Iris-versicolor equals that...",9,"The generated code correctly performs all the required steps: loading the data, filtering, performing the t-test, stating hypotheses, printing results, and interpreting them. The code is clean and uses appropriate libraries.

Areas for improvement:
1.  **Hypotheses Clarity**: The hypotheses are stated as comments rather than printed explicitly as requested by the prompt (""Clearly state the null and alternative hypotheses for this test""). The ground truth prints them out, which is a better fulfillment of ""clearly state"".
2.  **Error Handling**: The generated code lacks error handling for `FileNotFoundError` if 'iris.csv' is missing, which the ground truth includes, making it more robust.
3.  **Numerical Precision**: The output for t-statistic and p-value could benefit from formatting to a specific number of decimal places (e.g., `.4f`) for consistency and readability, as done in the ground truth.

Overall, it's a very good solution that fulfills almost all requirements. The missing explicit print of hypotheses and `FileNotFoundError` handling are minor omissions compared to the core logic.",gpt-4.1-nano,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('iris.csv')

# List of features to plot
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

for feature in features:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='Species', y=feature, data=df)
    plt.title(f'Box Plot of {feature} grouped by Species')
    plt.xlabel('Species')
    plt.ylabel(feature)
    plt.tight_layout()
    plt.show()
```",9,"The generated code correctly loads the dataset, iterates through the numerical features, and generates a box plot for each, grouped by `Species`. Each plot has an appropriate title. It successfully meets all the core requirements of the task.

**Strong points:**
*   Correctly uses pandas for data loading.
*   Correctly uses seaborn for generating box plots.
*   Successfully iterates through all four specified numerical features.
*   Each plot is grouped by the `Species` column.
*   Titles are included for each plot, clearly indicating the feature.
*   Proper imports are included.
*   `plt.tight_layout()` is a good practice for plot aesthetics.

**Minor areas for improvement (to reach a perfect 10):**
*   **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions during file loading, which makes the code more robust. The generated code assumes the file will always be present.
*   **Configuration:** The ground truth separates configurations (like `NUMERICAL_FEATURES`, `GROUPING_COLUMN`, `DATASET_PATH`) into distinct variables and a configuration section. While not strictly necessary for this small script, it enhances readability, maintainability, and makes the code more adaptable to changes (e.g., if feature names change).
*   **Plot Aesthetics:** The ground truth adds a grid and sets font sizes for titles and labels, which are minor aesthetic touches that can improve readability. These are not critical for functionality but contribute to polish.

Overall, the generated code performs exactly as requested for the core task of generating the plots. The missing error handling is the most significant difference from the ground truth in terms of robustness.",gpt-4.1-nano,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv('heart.csv')

# Select features and target
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']
target = 'HeartDisease'

X = df[features]
y = df[target]

# Define categ...",9,"The generated code correctly addresses all aspects of the task, including loading data, selecting features, one-hot encoding categorical variables, splitting data, training a Logistic Regression model, and reporting accuracy. The imports are correct, and the code structure is logical and easy to follow.

The main difference from the ground truth is how the preprocessing is handled. The ground truth uses a `Pipeline` which is generally considered best practice as it encapsulates the preprocessing and modeling steps, ensuring that the same transformations are applied to both training and test data consistently within the `Pipeline`'s `fit` and `predict` calls. The generated code `fit_transform`s `X` *before* the train-test split, which is a common mistake that can lead to data leakage from the test set into the training set if `fit_transform` were used for scaling (e.g., StandardScaler). For `OneHotEncoder`, the impact of fitting `preprocessor` on the full `X` then splitting is minimal for training, but it's still cleaner and safer to handle the transformation either with a `Pipeline` or by fitting the `preprocessor` only on the training data (`X_train`) and then transforming both `X_train` and `X_test`.

However, for one-hot encoding specifically with `ColumnTransformer` and `remainder='passthrough'`, fitting on the full `X` and then splitting, or fitting within a `Pipeline`, tends to yield the same result for the feature columns. Given `handle_unknown='ignore'`, this approach works correctly for this specific preprocessing task.

Overall, it's a very good solution that successfully completes the task requirements. The minor difference in preprocessing strategy (not using a `Pipeline` and applying `fit_transform` before splitting) is a subtle point in best practices but doesn't lead to incorrect results for this specific problem (OneHotEncoding).",gpt-4.1-nano,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
df = pd.read_csv('heart.csv')

# Create groups based on 'Sex'
males = df[df['Sex'] == 'M']
females = df[df['Sex'] == 'F']

# Null hypothesis (H0): The mean Cholesterol levels are equal for males and females.
# Alternative hypothesis (H1): The mean Cholesterol levels differ between males and females.

# Perform an independent two-sample t-test
t_stat, p_value = ttest_ind(males['Cholesterol'], females['Cholesterol'...",8,"The generated code correctly performs all the required steps: loading the data, creating groups based on 'Sex', stating hypotheses, performing the t-test on 'Cholesterol' levels, printing the t-statistic and p-value, and interpreting the result based on alpha = 0.05. It uses the correct libraries and functions.

Areas for improvement to achieve a higher score:
1.  **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and missing columns. The generated code assumes the file exists and has the correct columns.
2.  **Robustness for Missing Data:** The ground truth uses `.dropna()` when creating the cholesterol groups, ensuring that `NaN` values in the 'Cholesterol' column do not cause issues for the t-test. While `ttest_ind` can often handle NaNs by default (or raise an error depending on the `nan_policy`), explicit handling is generally safer and clearer.
3.  **Function Encapsulation:** Encapsulating the logic within a function (as in the ground truth) makes the code more reusable and testable.
4.  **Clarity of Output:** While the output is correct, the ground truth provides more verbose and user-friendly print statements, including the number of samples in each group, which is helpful for verification.
5.  **Hypotheses Statement Location:** The generated code states the hypotheses as comments, which is acceptable, but printing them to the console (as done in the ground truth) directly fulfills the requirement ""Clearly state the null and alternative hypotheses for this test.""
6.  **`equal_var=False`:** The generated code correctly uses `equal_var=False` for Welch's t-test, which is a good choice for robustness. The ground truth explains why this is often preferred, making it more informative.",gpt-4.1-nano,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('winequality-red.csv')

# Define columns to analyze
columns = ['volatile acidity', 'residual sugar']

# Initialize dictionaries to store IQR bounds and outlier counts
outlier_info = {}

for col in columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
 ...",8,"The generated code correctly performs all the requested tasks: identifying outliers using the IQR method, reporting their count, winsorizing them by capping at the 1.5*IQR bounds, and comparing the statistics before and after treatment. It uses proper imports and pandas/numpy functionalities effectively.

Strong points:
- Correct application of IQR method for outlier detection.
- Accurate counting of outliers.
- Effective winsorization using `.clip()` which is a concise and idiomatic pandas way to perform this operation.
- Clear comparison of mean and standard deviation before and after treatment.

Areas for minor improvement:
- **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.
- **Output Verbosity/Formatting:** While it reports the necessary information, the output is less detailed and formatted than the ground truth's (e.g., no explicit display of Q1, Q3, IQR, bounds, or original dataset info). The task asked for ""reporting"" the number of outliers, which it does, but the ground truth provided more comprehensive insights into the calculated bounds, which can be helpful for verification.
- **Clarity of Winsorization:** In the winsorization step, `df_winsorized[col] = df[col].clip(...)` correctly caps the values. However, `df[col]` itself is clean (the original data), so it is creating the capped version from the original data, which is fine. The ground truth uses `np.where` on `df_winsorized[col]` which is also perfectly valid. Both achieve the same result. The ground truth explicitly shows the bounds it's using during the winsorization loop, which makes the flow slightly more readable without having to refer back to the `outlier_info` dictionary.

Overall, it's a very good solution that directly answers the question's core requirements. The minor points above are largely about user experience and robustness rather than core correctness.",gpt-4.1-nano,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
df = pd.read_csv('heart.csv')

# Filter for specific ChestPainType categories if needed
categories = ['TA', 'ATA', 'NAP', 'ASY']
df_filtered = df[df['ChestPainType'].isin(categories)]

# Count of HeartDisease for each ChestPainType and HeartDisease status
counts = df_filtered.groupby(['ChestPainType', 'HeartDisease']).size().unstack()

# Plotting
x = np.arange(len(categories))
width = 0.35

fig, ax = plt.s...",8,"The generated code correctly loads the dataset and creates a grouped bar chart showing the count of HeartDisease (0 and 1) for each category in the ChestPainType. It fulfills the core requirements of displaying the two bars for each ChestPainType and includes proper labels, title, and legend.

Strong points:
- Correctly uses `groupby().size().unstack()` to get the counts for each combination.
- Manually constructs the bars `ax.bar(x - width/2, ...)` and `ax.bar(x + width/2, ...)` which is a valid way to create grouped bars when `seaborn.countplot` with `hue` is not used.
- Handles cases where a `HeartDisease` value might be missing for a `ChestPainType` using `counts.get(0, [0]*len(categories))`.
- Includes a title, labeled axes, and a legend.

Areas for improvement to achieve a higher score:
- **Robustness:** The code assumes that 'TA', 'ATA', 'NAP', 'ASY' are the only relevant or existing categories. While `df_filtered` addresses ensuring only these categories are plotted, the `counts.get(0, [0]*len(categories))` relies on `categories` being comprehensive and in a specific order which might be brittle if the actual data contains other `ChestPainType` values or the order changes, leading to misaligned labels or missing data. A more robust approach would be to properly handle potential categorical data issues or use a library that handles this automatically (like seaborn).
- **Hardcoded Categories:** The list `categories = ['TA', 'ATA', 'NAP', 'ASY']` is hardcoded. It would be more robust to either derive this from the dataset's unique values or use a categorical type conversion as in the ground truth for consistent ordering.
- **Error Handling:** There is no error handling for `FileNotFoundError` or if the required columns are missing, which makes the code less robust for general use.
- **Readability/Conciseness:** While the `ax.bar` approach works, using `seaborn.countplot` with the `hue` parameter, as in the ground truth, is often more concise and idiomatic for this type of grouped count plot in data science, making the intent clearer and often handling positioning/colors more automatically.
- **Function Encapsulation:** Not encapsulating the plotting logic in a function makes it less reusable.",gpt-4.1-nano,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Clean the 'in_shazam_charts' column
df['in_shazam_charts_clean'] = df['in_shazam_charts'].astype(str).str.replace(',', '', regex=False)
df['in_shazam_charts_num'] = pd.to_numeric(df['in_shazam_charts_clean'], errors='coerce')

# Report count of NaNs after conversion
nan_count = df['in_shazam_charts_num'].isna().sum()
print(f""Number of NaNs in 'in_shazam_charts' after cleaning: {nan_count}"")

# Group by 'key...",9,"The generated code correctly addresses all parts of the task. It loads the CSV, cleans the `in_shazam_charts` column by removing commas and converting to numeric with error coercion, and correctly reports the NaN count. It then groups the data by `key` and `mode` and calculates the average `danceability_%` and `valence_%`.

Strengths:
- Correctly uses pandas for data loading and manipulation.
- Accurately cleans and converts the `in_shazam_charts` column, including handling errors with `coerce`.
- Reports the count of NaNs as requested.
- Correctly performs the grouping and aggregation.
- The output format is clear.

Minor improvements:
- The generated code creates two new columns (`in_shazam_charts_clean` and `in_shazam_charts_num`) instead of directly modifying `in_shazam_charts` as done in the ground truth. While functionally correct, it adds unnecessary columns if the original `in_shazam_charts` column is not needed for other purposes. Modifying in place is slightly more efficient/cleaner for this specific task.
- The prompt asks to ""display"" the averages. While `print(result)` does display it, the ground truth uses a more descriptive print statement for the final output, which is generally good practice for clarity.
- The error handling for `FileNotFoundError` present in the ground truth is a good practice for robust code, though not explicitly required by the prompt's functional requirements.",gpt-4.1-nano,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Ensure 'streams' is numeric
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# Select features and target
features = ['in_spotify_playlists', 'energy_%']
df = df.dropna(subset=features + ['streams'])
X = df[features]
y = df['streams']

# Split in...",9,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, converts 'streams' to numeric, handles NaNs correctly for the specified columns, selects features and target, splits the data, trains a Linear Regression model, and reports R2 and MSE.

The only minor area for improvement is the lack of a `try-except` block for `pd.read_csv`, which was present in the ground truth for robust file loading. While the prompt implied the file would be present, robust code often includes this. Also, the MSE is printed with 4 decimal places, whereas the ground truth used 2 decimal places. These are minor stylistic differences and do not affect the core functionality or correctness. Overall, the code is very good and complete.",gpt-4.1-nano,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Combine released_year, released_month, and released_day into a single datetime column
def create_release_date(row):
    try:
        return pd.to_datetime(f""{row['released_year']}-{row['released_month']:02d}-{row['released_day']:02d}"")
    except (ValueError, TypeError):
        return pd.NaT

df['release_date'] = df.apply(create_release_date, axis=1)

# Calculate days since release from a fixed date
fixed_...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Correctness:** The code successfully loads the dataset, creates the `release_date` column, and calculates `days_since_release` as requested. The use of `pd.NaT` for invalid dates is also correctly implemented.
- **Error Handling for Dates:** The `create_release_date` function correctly handles `ValueError` and `TypeError` when parsing dates to set invalid dates to `NaT`, which fulfills the requirement.
- **Feature Calculation:** The calculation of `days_since_release` using `(fixed_date - df['release_date']).dt.days` is accurate and efficient.
- **Output:** The final display of the first 5 tracks with the specified columns is correct.
- **Readability:** The code is well-structured and easy to understand.

**Areas for Improvement (minor):**
- **Performance:** While the `apply` method with a custom function works, it can be significantly slower for very large datasets compared to vectorized operations. The ground truth code uses vectorized string operations (`.astype(str)`, `.str.zfill(2)`) and then `pd.to_datetime` with `errors='coerce'`, which is generally more performant and idiomatic for pandas. For this specific task, the `apply` approach is acceptable but worth noting for scalability.
- **`days_since_release` data type:** The task explicitly asked for `days_since_release` to be an integer. When `pd.to_datetime` results in `NaT` (Not a Time), the subsequent subtraction `(fixed_date - df['release_date'])` will result in `NaT` (Not a TimeDelta), and `dt.days` will yield `NaN`. By default, pandas will convert a Series containing `NaN` to a float type (e.g., `float64`), which conflicts with the ""integer"" requirement. To strictly enforce an integer type while allowing for `NaN` (which corresponds to `NaT` in `release_date`), one should use pandas' nullable integer type (`'Int64'` or `pd.Int64Dtype()`). Without this, the `days_since_release` column will be `float64` if NaNs are present, which is a subtle deviation from the ""integer"" requirement. The current code will produce `float64` for `days_since_release` if any `release_date` is `NaT`.

Overall, the code is very good and provides a correct solution to the problem, with only minor optimizations and a data type consideration for strictness.",gpt-4.1-nano,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Function to clean numeric columns
def clean_numeric_column(series):
    return pd.to_numeric(series.str.replace(',', '', regex=False), errors='coerce').fillna(0)

# Clean 'Video views' and 'Likes' columns
df['Video views'] = clean_numeric_column(df['Video views'])
df['Likes'] = clean_numeric_column(df['Likes'])

# Fill missing values in 'Category'
df['Category'] = df['Category'].fillna('Unknown')

# Group by 'Ca...",9,"The generated code correctly implements all the requirements of the task: loading the CSV, cleaning 'Video views' and 'Likes' columns (removing commas, converting to numeric, handling NaNs), filling missing 'Category' values, and then grouping and summing the specified columns. The use of a helper function `clean_numeric_column` is a good practice for reducing code duplication and improving readability.

The only minor area for improvement (and why it's not a 10) is the lack of robust error handling for file not found or if expected columns are missing, which the ground truth code includes. While the task description didn't explicitly ask for `try-except` blocks or column existence checks, in a real-world scenario, these would increase the robustness of the code. However, given the prompt's focus on the core data manipulation logic, the generated code is highly effective and correct.",gpt-4.1-nano,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# 1. Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# 2. Clean the 'Likes' column
# Remove commas, convert to float, handle conversion errors as NaN
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce')
# Fill NaNs with 0
df['Likes'].fillna(0, inplace=True)

# 3. Fill missing 'Category' values with 'Unknown'
df['Category'].fillna('Unknown', inplace=True)

# 4. Create groups for 'Music' and 'Spo...",9,"The generated code is largely correct and addresses most of the requirements.

**Strong Points:**
- **Correct Data Loading:** It correctly loads the `youtube_trendings.csv` dataset.
- **Correct 'Likes' Cleaning:** It properly removes commas, converts `Likes` to numeric (handling errors), and fills NaNs with 0. It correctly uses `inplace=True` for this operation.
- **Correct 'Category' Cleaning:** It correctly fills missing `Category` values with 'Unknown' using `fillna('Unknown', inplace=True)`.
- **Correct Group Creation:** It correctly creates `likes_music` and `likes_sports` series using `df.loc[]` which is a good practice.
- **Correct T-test Execution:** It correctly performs the independent two-sample t-test using `ttest_ind` and importantly sets `equal_var=False` as specified in the ground truth for Welch's test.
- **Hypotheses Stated:** It implicitly states the null and alternative hypotheses through comments, although not identical to the ground truth's verbose print statements, it conveys the correct information.
- **Correct Output of T-statistic and P-value:** It prints the required metrics.
- **Correct Interpretation:** It correctly interprets the p-value against `alpha=0.05` and provides the appropriate conclusion.

**Areas for Improvement (Why not a 10):**
- **Robustness/Error Handling for File Loading:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which makes the code more robust if the file is missing. The generated code assumes the file will always be present, which is less robust in a real-world scenario.
- **Explicit Hypothesis Statement:** While comments state the hypotheses, the ground truth explicitly prints them as part of the output, which was a specific requirement (""Clearly state the null and alternative hypotheses for this test""). The generated code fulfills the spirit but not the exact output format requested by the prompt for this point.
- **Clarity of `Likes` Cleaning Destination:** The prompt said ""create a new column"" or ""convert it to a numeric type (e.g., float)"". The generated code overwrites the original 'Likes' column. While this is acceptable and memory-efficient, the ground truth's creation of `Likes_cleaned` as a separate column might be marginally clearer based on the prompt's wording ""create two groups of cleaned `Likes` values"". However, overwriting is fine.
- **No verbose feedback:** The ground truth includes print statements at every step indicating progress and confirming data cleaning. While not strictly required for functionality, it's good practice for debugging and user feedback. The prompt didn't explicitly ask for this level of verbosity beyond the final results and interpretation.

Overall, the generated code fulfills the core requirements of the task very well and is highly functional. The minor deductions are for robustness, adherence to explicit output formatting for hypotheses, and slight differences in variable handling.",gpt-4.1-nano,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# 1) Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Helper function to clean and convert Likes and Dislikes columns
def clean_numeric_column(series):
    # Remove commas
    cleaned_series = series.str.replace(',', '', regex=False)
    # Convert to numeric, coerce errors to NaN
    numeric_series = pd.to_numeric(cleaned_series, errors='coerce')
    # Fill NaNs with 0
    numeric_series_filled = numeric_series.fillna(0)
    return n...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, cleans the 'Likes' and 'Dislikes' columns by removing commas, converting to numeric, and filling NaNs with 0. It then correctly calculates 'TotalEngagement' and 'LikeDislikeRatio' as specified. Finally, it displays the required columns. The use of a helper function `clean_numeric_column` for `Likes` and `Dislikes` is a good practice for code reusability and readability.

The only minor area for improvement, which is not critical, is that the ground truth explicitly casts the cleaned columns back to `float` after filling NaNs, though `pd.to_numeric` with `errors='coerce'` usually results in a float dtype when NaNs are present. However, the generated code's approach (returning a numeric series from `clean_numeric_column`) is also perfectly valid and Pandas will handle the numeric types correctly for calculations. The ground truth also included robust error handling for `FileNotFoundError` during loading, which the generated code omits. While not strictly part of the ""feature engineering"" task, robust loading is generally good practice. Given the focus on feature engineering, the generated code's omission of advanced error handling for file loading is acceptable.",gpt-4.1-nano,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean 'Launched Price (USA)' column
df['Launched Price (USA)'] = pd.to_numeric(
    df['Launched Price (USA)']
        .str.replace('USD ', '', regex=False)
        .str.replace(',', '', regex=False),
    errors='coerce'
)

# Clean 'Screen Size' column
df['Screen Size'] = pd.to_numeric(
    df['Screen Size']
        .str.replace(' inches', '', regex=False),
    errors='coerce'
)

# Group by 'Company Name' and 'Launch...",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans both specified columns to numeric types, handles conversion errors by coercing to NaN values, correctly groups the data, and calculates the required averages. The `as_index=False` in the `groupby` is good practice, and the chained `str.replace` calls are concise.

Minor improvements could include:
- Adding error handling for `pd.read_csv` (e.g., `FileNotFoundError`) for robustness, although the prompt didn't explicitly ask for it beyond ""assume the CSV file is in the current directory.""
- Renaming the cleaned columns to something like `Cleaned Price (USD)` and `Cleaned Screen Size (inches)` as done in the ground truth for clarity, rather than overwriting the original columns. While overwriting works and is concise, it might lose original data for debugging or later use if not explicitly intended.
- Explicitly showcasing the column types after cleaning to confirm the numeric conversion, similar to the ground truth's detailed print statements.

Overall, it's a very good solution that fulfills the prompt's core requirements effectively and efficiently.",gpt-4.1-nano,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

def extract_camera_mp(camera_str):
    if pd.isnull(camera_str):
        return 0
    match = re.search(r'(\d+)', camera_str)
    if match:
        return int(match.group(1))
    else:
        return 0

# Clean Front Camera column
df['Cleaned Front Camera'] = df['Front Camera'].apply(extract_camera_mp)

# Clean Back Camera column
df['Cleaned Back Camera'] = df['Back Camera'].apply(extract_camera_mp)...",9,"The generated code correctly solves the task. It loads the dataset, defines a function to extract the first sequence of digits as the MP value from camera specifications, handles NaN values by returning 0, applies this function to create cleaned camera columns, and then calculates the `Total Camera MP`. Finally, it displays the required columns for the first 10 rows.

**Strong points:**
*   Correctly loads the CSV file.
*   The `extract_camera_mp` function is well-defined and correctly extracts the first numerical sequence.
*   Handles `pd.isnull` for camera strings, correctly returning 0 if no value is present.
*   Correctly creates `Cleaned Front Camera`, `Cleaned Back Camera`, and `Total Camera MP` columns.
*   Displays the requested columns for the first 10 rows.

**Minor areas for improvement (not critical for correctness, but good practice):**
*   The `extract_camera_mp` function could benefit from type hints for better readability and maintainability, similar to the ground truth.
*   The `re.search(r'(\d+)', camera_str)` uses a capturing group `(\d+)` but then references `match.group(1)`. While `match.group(0)` would also work here and would be more direct as it refers to the entire match, using `match.group(1)` is not incorrect since it's the only capturing group.
*   Adding `try-except FileNotFoundError` for loading the CSV is a good practice for robustness, although not explicitly required by the prompt's simplicity.
*   Adding print statements for progress tracking (e.g., ""Dataset loaded successfully"") as in the ground truth can enhance user experience for larger scripts.",gpt-4.1-nano,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean RAM
df['RAM'] = df['RAM'].str.replace('GB', '').str.strip()
df['RAM'] = pd.to_numeric(df['RAM'], errors='coerce')

# Clean Battery Capacity
df['Battery Capacity'] = df['Battery Capacity'].str.replace('mAh', '').str.strip()
df['Battery Capacity'] =...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, performs the specified cleaning operations on the `RAM`, `Battery Capacity`, `Screen Size`, and `Launched Price (USA)` columns, handling conversion errors by coercing to NaN and dropping rows with NaNs in these specific columns. It then correctly defines features and target, splits the data, trains a Linear Regression model, and evaluates it using R-squared and MAE.

Minor improvement: The ground truth code includes error handling for `FileNotFoundError` when loading the CSV and checks if `df` is empty after cleaning, which adds robustness. It also casts columns to `astype(str)` before string operations, which is good practice to avoid warnings/errors if a column contains mixed types initially. The generated code implicitly assumes the columns are string-like before `str.replace` which works most of the time but can sometimes lead to issues. However, given the prompt implicitly implies that the data types in the specified columns are such that the `.str` accessor would work, this is a minor point. The explicit `df_clean = df.dropna(...)` creates a new DataFrame, which is fine, but the in-place operation (`df.dropna(..., inplace=True)`) used in the ground truth is also a common and equally valid approach. Overall, it's a solid solution.",gpt-4.1-nano,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load dataset
df = pd.read_csv('titanic.csv')

# 1) Chi-square test between Pclass and Survived
# Create contingency table
contingency_table = pd.crosstab(df['Pclass'], df['Survived'])
chi2, p_chi2, dof, expected = stats.chi2_contingency(contingency_table)

# Calculate Cramr's V for effect size
n = contingency_table.sum().sum()
phi2 = chi2 / n
min_dim = min(contingency_table.shape) - 1
cramers_v = np.sqrt(phi2 / min_dim)...",8,"The generated code correctly performs all three statistical tests (chi-square, t-test, ANOVA) and calculates their respective effect sizes. It also interprets the results based on the p-value and a hardcoded alpha of 0.05.

**Strong points:**
*   Correctly implements `chi2_contingency`, `ttest_ind`, and `f_oneway`.
*   Effect size calculations (Cramer's V, Cohen's d, Eta-squared) are mostly correct.
*   Handles missing data for Age and Fare appropriately using `.dropna()`.
*   Includes clear print statements for statistics, p-values, and conclusions.

**Areas for improvement:**
*   **Variable Alpha:** The prompt specified using an ""appropriate alpha level (0.05)"" and the ground truth defines it as a variable. While 0.05 is used, it's hardcoded into each `if` statement rather than using a defined `alpha` variable, making it less flexible and harder to change if needed.
*   **T-test `equal_var`:** The generated code hardcodes `equal_var=False` for the t-test. While often a safer general approach, the ground truth explicitly performed Levene's test to statistically determine whether variances were equal, which is a more robust and statistically sound method for deciding on `equal_var`. This is a subtle but important detail in statistical practice.
*   **Cohen's d Calculation:** The calculation for Cohen's d uses a pooled standard deviation formula that assumes equal variances (`pooled_sd = np.sqrt(((n_surv -1)*sd_surv**2 + (n_non_surv -1)*sd_non_surv**2) / (n_surv + n_non_surv - 2))`). However, the `ttest_ind` itself was performed with `equal_var=False` (Welch's t-test), which implies unequal variances. For Welch's t-test, a specific Cohen's d formula that does not assume equal variances (often called Hedges' g or a modified Cohen's d) is more appropriate. The current combination is a minor inconsistency.
*   **Dataset Loading Robustness:** The ground truth includes a `try-except` block for loading the CSV, which makes the code more robust against `FileNotFoundError`. The generated code assumes the file will always be present, which is less ideal for general deployment.
*   **Clarity/Comments:** While the code is functional, adding comments explaining the hypotheses and parts of the effect size calculations (similar to the ground truth) would improve readability and explain the statistical reasoning.
*   **Output Format:** The prompt asks for ""Display p-values, test statistics, and conclusions for each hypothesis test."" While done, the ground truth's formatting and additional context (like degrees of freedom for chi-square) are more comprehensive for statistical reporting. For ANOVA, mentioning the need for post-hoc tests when rejecting the null hypothesis (as in ground truth) is also good statistical practice.",gpt-4.1-nano,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Group by Year and calculate mean Rating and Runtime
grouped = df.groupby('Year').agg({'Rating': 'mean', 'Runtime': 'mean'}).reset_index()

# Plot the trend of average Rating over Year
plt.figure(figsize=(10, 6))
plt.plot(grouped['Year'], grouped['Rating'], marker='o')
plt.title('Average IMDb Rating Over Years')
plt.xlabel('Year')
plt.ylabel('Average Rating')
plt.grid(True)
plt.show()",6,"The generated code correctly loads the dataset, groups by year, calculates the mean rating and runtime, and plots the average rating over the years with a title and labels. This addresses the core requirements of the task.

However, there are two significant shortcomings compared to the ground truth:
1.  **Data Cleaning/Robustness:** The generated code implicitly assumes that the `Year`, `Runtime`, and `Rating` columns are perfectly clean and in a format suitable for direct numerical operations. The `imdb_top_250.csv` dataset often has 'Year' in formats like `(2000)` and 'Runtime' in `X min`. The ground truth includes essential data cleaning steps (using regex for Year and Runtime, handling errors for Rating), which makes it much more robust and reusable for real-world data that might not be perfectly pre-cleaned. Without this, the generated code would likely fail or produce incorrect results on the actual dataset.
2.  **Function Encapsulation:** The ground truth encapsulates the logic within a function (`analyze_imdb_data`) and includes error handling (`try-except` blocks for `FileNotFoundError`, `KeyError`, and general exceptions). This significantly improves code reusability, testability, and robustness in a production or analytical environment. The generated code is a script, which is less ideal for larger projects.

To improve the score, the generated code should incorporate data cleaning steps for `Year` and `Runtime` (and potentially `Rating`) to handle common data inconsistencies found in such datasets, and ideally, encapsulate the logic within a function with basic error handling.",gpt-4.1-nano,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create two groups based on the Year
group_before_1990 = df[df['Year'] < 1990]['Rating']
group_1990_or_later = df[df['Year'] >= 1990]['Rating']

# Perform an independent two-sample t-test
t_stat, p_value = ttest_ind(group_before_1990, group_1990_or_later, equal_var=False)

# Hypotheses
# Null hypothesis (H0): The mean Ratings before 1990 and in or after 1990 are equal.
# Alt...",8,"The generated code correctly loads the dataset, creates the two specified groups, extracts the 'Rating' for each, and performs an independent two-sample t-test. It correctly prints the t-statistic and p-value, and interprets the result based on the significance level of 0.05.

However, there are a few areas for improvement:
1.  **Hypotheses Statement:** While the hypotheses are commented in the code, the task explicitly asks to ""Clearly state the null and alternative hypotheses for this test."" and ""Print the calculated t-statistic and the p-value."" This implies printing the hypotheses to the console as part of the output, not just as internal comments.
2.  **Robustness:** The ground truth code includes error handling for `FileNotFoundError` and explicit conversion of 'Year' and 'Rating' to numeric types with `errors='coerce'` and subsequent `dropna`. This makes the code more robust to common data issues (e.g., non-numeric data in columns, missing file). The generated code assumes clean data. While it fulfills the core requirements, robustness is a sign of higher quality production-ready code.
3.  **Clarity of Interpretation:** The interpretation ""Reject the null hypothesis: There is a statistically significant difference in ratings between the two groups."" is correct, but providing the specific p-value and alpha in the output (as done in the ground truth) enhances clarity and reproducibility.
4.  **`equal_var` parameter:** The generated code uses `equal_var=False` for the t-test (Welch's t-test). While often a safer default as it doesn't assume equal variances, the prompt doesn't specify which variant to use. The ground truth used `equal_var=True` and commented on it (Student's t-test). For a benchmark, consistency or justification is good. For general purposes, `equal_var=False` is often preferred unless assumptions for `equal_var=True` are met or tested. Without specification, either is technically permissible, but acknowledging it is good.

Overall, it's a very good attempt that addresses the core requirements. The improvements would primarily be around robustness, explicit output of hypotheses, and a slightly more verbose interpretation and output format.",gpt-4.1-nano,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create the 'Decade' column
def determine_decade(year):
    start = (year // 10) * 10
    return f""{start}s""

df['Decade'] = df['Year'].apply(determine_decade)

# Display the first 10 movies with Title, Year, and Decade
print(df[['Title', 'Year', 'Decade']].head(10))",9,"The generated code correctly loads the dataset, creates the `Decade` column as requested, and displays the `Title`, `Year`, and `Decade` for the first 10 movies. The logic for determining the decade `(year // 10) * 10` is accurate and efficient.

Strong points:
- Correctly implements all core requirements of the task.
- Uses pandas for data manipulation effectively.
- Code is concise and readable.

Areas for minor improvement (to achieve a perfect 10):
- The output format for displaying the first 10 rows is slightly different from the ground truth. The ground truth uses `to_string(index=False)` for cleaner output, which is a minor aesthetic point.
- The generated code doesn't include any error handling (e.g., `try-except` for `FileNotFoundError`) which is present in the ground truth and generally good practice for robust scripts that deal with file I/O.
- While using a function for `determine_decade` is okay, a direct vectorized operation (`((df['Year'] // 10) * 10).astype(str) + 's'`) as seen in the ground truth is often more performant in pandas for large datasets. For this small example, the difference is negligible.",gpt-4.1-nano,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Impute missing numerical columns with mean
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numerical_cols:
    df[col].fillna(df[col].mean(), inplace=True)

# Impute missing 'sex' values with mode
df['sex'].fillna(df['sex'].mode()[0], inplace=True)

# Group by 'species' and 'sex' and calculate mean of 'body_mass_g'
grouped = df.groupby(['species', 'sex'])[...",10,"The generated code is correct, complete, and follows good coding practices. It successfully loads the dataset, correctly imputes missing numerical values with the mean of their respective columns, imputes missing categorical 'sex' values with the mode, and then groups the data by 'species' and 'sex' to calculate and display the average 'body_mass_g'. All specified requirements are met. The code is concise and efficient.",gpt-4.1-nano,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Filter the data for 'Adelie' or 'Gentoo' species
filtered_df = df[df['species'].isin(['Adelie', 'Gentoo'])]

# 3) Remove rows with missing values in 'flipper_length_mm' or 'species'
clean_df = filtered_df.dropna(subset=['flipper_length_mm', 'species'])

# Separate the data for each species
adelie_flippers = clean_df[clean_df['species'] == 'Adelie']['flipper_length_mm']
ge...",9,"The generated code correctly performs all the requested steps: loading data, filtering, handling NAs, stating hypotheses, performing the t-test, printing results, and interpreting the p-value. The use of `ttest_ind(..., equal_var=False)` is a good choice for real-world data, matching the ground truth.

Areas for minor improvement (not critical enough to significantly lower the score, but good practice):
1.  **Error Handling for File Loading:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions during file loading, which makes the script more robust. The generated code assumes the file will always be present.
2.  **Hypotheses Printing:** The hypotheses are commented out in the code, which is acceptable but printing them to the console (as the ground truth does) would fulfill the requirement of ""Clearly state the null and alternative hypotheses"" more explicitly in the output.
3.  **Variable Naming (minor):** `gento_flippers` instead of `gentoo_flippers` is a small typo, but doesn't affect functionality.
4.  **Clarity of Steps:** While the comments indicate the steps, adding more print statements to show intermediate data shapes or counts (like the ground truth) can make the execution flow clearer to the user.

Overall, it's a very good solution that fulfills the core requirements.",gpt-4.1-nano,LLM_judge_config
M45,medium,Classification,penguins.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Select features and target
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
X = df[features]
y = df['species']

# 3) Drop rows with missing values in selected columns or target
X = X.dr...",7,"The generated code correctly implements most of the requested steps: loading data, selecting features and target, encoding the target, splitting the data, training a Logistic Regression model, and reporting accuracy. All necessary imports are present.

However, there is a critical flaw in the data cleaning step (step 3). The code separates `X` and `y` *before* dropping NAs, and then calls `dropna()` independently on `X` and `y`. This can lead to misalignments between `X` and `y` if `y` has NAs where `X` does not, or vice-versa, or if different rows are dropped from `X` and `y`. While `y = y.loc[X.index]` attempts to realign, it assumes that `X.dropna()` correctly identifies all rows that need to be dropped for both `X` and `y` simultaneously, which is not guaranteed. The requirement was to drop rows where *any* of the specified columns (`species` or the four features) have missing values. The correct approach is to drop NAs from a combined dataframe or explicitly on the subset of columns required for `X` and `y`, ensuring that the same rows are removed from both.

Specifically, the ground truth explicitly selects the relevant columns first and then performs `dropna()` on this combined subset, ensuring consistency. The generated code also does not handle the case where the CSV file might be missing gracefully, which is a minor point but good practice for robustness.",gpt-4.1-nano,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Drop rows where 'body_mass_g' or 'species' is NA
df_clean = df.dropna(subset=['body_mass_g', 'species'])

# 3) Calculate the average 'body_mass_g' for each species
avg_body_mass = df_clean.groupby('species')['body_mass_g'].mean().reset_index()

# 4) Create a bar plot
plt.figure(figsize=(8,6))
sns.barplot(x='species', y='body_mass_g', data=avg_body_mass...",9,"The generated code correctly addresses all requirements of the task. It successfully loads the dataset, drops NA values from the specified columns (`body_mass_g` and `species`), calculates the average body mass per species, and generates a clear bar plot with appropriate titles and labels. The code is concise and follows standard practices for data analysis with pandas and matplotlib/seaborn.

**Strong points:**
*   **Correct logic:** All steps are implemented accurately according to the task description.
*   **Conciseness:** The code is efficient and to the point.
*   **Proper imports:** All necessary libraries are imported.
*   **Clear plot:** The generated plot is well-labeled and understandable.

**Areas for minor improvement (not critical for current task, but good practice):**
*   **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError`, which is robust practice for loading files. The generated code assumes the file exists.
*   **Verbosity/Prints:** The ground truth includes print statements for initial and final data shapes and NA counts, which is very helpful for debugging and understanding data transformations during development. While not strictly required by the prompt, it adds to the robustness of a solution.
*   **Plot aesthetics:** Adding `palette` to `sns.barplot` and `plt.grid` can enhance readability and visual appeal, as shown in the ground truth.
*   **Comments:** While the code is relatively self-explanatory, more detailed comments (like in the ground truth) can improve maintainability for complex tasks.",gpt-4.1-nano,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Drop rows where specified columns have NA
df_clean = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])

# 3) Create new feature 'culmen_area'
df_clean['culmen_area'] = df_clean['culmen_length_mm'] * df_clean['culmen_depth_mm']

# 4) Calculate Pearson correlation matrix for selected columns
cols_of_interest = ...",9,"The generated code correctly performs all the requested steps, including loading the data, dropping NA values from the specified columns, creating the new `culmen_area` feature, calculating the correlation matrix for the specified columns, and visualizing it as an annotated heatmap with the correct title. The imports are proper, and the code structure is clear and follows the task's steps sequentially.

The only minor points for improvement, which prevent a perfect score, are:
*   **Error Handling**: The generated code lacks explicit error handling for `FileNotFoundError` when loading the CSV, unlike the ground truth. While not strictly required by the prompt, it's good practice for robust code.
*   **`SettingWithCopyWarning`**: The line `df_clean['culmen_area'] = ...` might trigger a `SettingWithCopyWarning` in pandas, as `df_clean` is a view created by `dropna` without an explicit `.copy()`. The ground truth handles this by using `.copy()`. This is a common pandas quirk and not a functional error in this context, but it's an area for refined code quality.
*   **Default `corr()` method**: The prompt explicitly asks for ""Pearson correlation matrix"". While `corr()` defaults to 'pearson', explicitly setting `method='pearson'` as in the ground truth makes the intent clearer and the code more robust against potential future pandas changes or misinterpretations.

Overall, it's a very good solution that fully addresses the core requirements of the task.",gpt-4.1-nano,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Select features and target
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
target = 'species'

df = df[features + [target]]

# 3) Impute NA values for numerical features with mean
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numerical_cols:
    df[col].fillna(df[col].mean(), inplace=True)

# ...",9,"The generated code successfully completes almost all aspects of the task. It correctly loads the data, selects the specified features and target, imputes numerical NaNs with the mean, handles the 'sex' column imputation and mapping accurately, and applies one-hot encoding to 'island'. It also correctly prints the shape and head of the resulting feature matrix.

Areas for minor improvement:
1.  **File Not Found Handling:** The `try-except` block for `FileNotFoundError` when loading the CSV, as present in the ground truth, is a good practice for robustness. The generated code assumes the file exists.
2.  **`SettingWithCopyWarning`:** While not explicitly violated in this specific code due to intermediate selections, using `.copy()` after feature selection (`df = df[features + [target]].copy()`) can prevent potential `SettingWithCopyWarning` in more complex scenarios where chained assignments might occur. The generated code implicitly handles this by reassigning `df`.
3.  **Clarity of variable names:** The ground truth separates `X` and `y` more explicitly from `df_processed` after all transformations, which can be slightly clearer, though `df.drop(columns=target)` achieves the same outcome effectively.

Overall, the code is highly correct and functional for the given task. The minor points are about robust coding practices rather than functional errors.",gpt-4.1-nano,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004'...",9,"The generated code correctly performs all requested operations: DataFrame creation, calculation of total spending per customer, identification of the top 3 customers by spending, and calculation of total quantity sold per category.

**Strong Points:**
- Correctly imports `pandas`.
- Accurately calculates `LineTotal` (named `Total_Spend` in the generated code) and uses it for customer spending.
- Correctly uses `groupby()` for both customer spending and category quantity.
- Correctly identifies the top 3 customers using `sort_values().head(3)` which is an effective alternative to `nlargest()`.
- Prints the results clearly in a readable format.

**Area for minor improvement:**
- The ground truth code includes helpful print statements for each step, which enhances readability and debugging. The generated code is more concise, but adding some descriptive print statements (e.g., ""Calculating total spending..."", ""Top 3 customers are..."") would align it even closer with the ground truth's best practices for clarity in benchmark tasks. This is a stylistic point rather than a functional error.",gpt-4.1-nano,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, roc_curve
)
from sklearn.preprocessing import StandardScaler, LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('titanic.csv')

# Data preprocessing
# ...",7,"The generated code addresses most of the requirements but has several areas for improvement, particularly in data preprocessing, feature engineering, and the robustness of its model evaluation/interpretation.

**Strong Points:**
*   Correctly loads the dataset.
*   Handles missing values for 'Age', 'Fare', and 'Embarked'.
*   Splits data into training and testing sets with a correct ratio and `stratify`.
*   Trains a Logistic Regression model.
*   Evaluates the model using all requested metrics (accuracy, precision, recall, F1-score, ROC-AUC).
*   Visualizes Confusion Matrix and ROC Curve.
*   Performs cross-validation.
*   Attempts to display and interpret feature coefficients.

**Areas for Improvement:**

1.  **Data Preprocessing (Categorical Variables and Pipelines):**
    *   **Label Encoding for Pclass and Embarked:** The use of `LabelEncoder` for `Pclass` and `Embarked` implicitly assumes an ordinal relationship, which is not appropriate for these nominal categorical features. One-hot encoding (as used in the ground truth) is the correct approach to avoid misleading the model about such relationships.
    *   **Lack of `Pipeline` or `ColumnTransformer`:** The preprocessing steps (scaling, encoding) are performed globally on `X` before `train_test_split`. This leads to data leakage as the scaler and encoders learn from the test set data. The ground truth correctly uses `ColumnTransformer` and `Pipeline` to encapsulate preprocessing within the training process, ensuring that scaling and encoding transformations are learned *only* from the training data and then applied consistently to both training and test sets. This is a critical best practice in machine learning.
    *   `Cabin`, `PassengerId`, `Name`, `Ticket` columns are not dropped. While some are irrelevant, the ground truth explicitly handles them.

2.  **Feature Engineering:**
    *   The generated code does not perform any feature engineering (e.g., creating `FamilySize` from `SibSp` and `Parch`), which was present in the ground truth and often beneficial for this dataset.

3.  **Coefficient Interpretation:**
    *   While coefficients are displayed, their interpretation is more straightforward for one-hot encoded features. With `LabelEncoder`, the meaning of coefficients for `Sex` and `Embarked` is less clear and potentially misleading (e.g., the coefficient for `Sex_enc` represents the change in log-odds for a 1-unit increase in the label-encoded value, which has no inherent meaning beyond distinguishing a male from a female). The ground truth's approach of showing coefficients for specifically named one-hot encoded features (`Gender_male`, `Embarked_S`, etc.) combined with its detailed interpretation is superior.
    *   The coefficients displayed are for the *scaled* features. While this is mathematically correct, interpreting them in terms of original units (e.g., ""a one-year increase in Age"") becomes more complex. Many data scientists prefer to see interpretations on original scales for direct business insights, but doing so would require significantly more complex steps. However, the ground truth's method provides the named generated features which is better.

4.  **Cross-Validation Metrics:**
    *   The cross-validation only reports 'accuracy'. The prompt specifically asked for evaluation using *multiple metrics* (accuracy, precision, recall, F1-score, ROC-AUC) and the ground truth correctly shows how to gather all these metrics during cross-validation using `cross_validate` with a `scoring` dictionary. This gives a more comprehensive view of model stability.

5.  **Imports and Display:**
    *   `ConfusionMatrixDisplay` and `RocCurveDisplay` from `sklearn.metrics` (used in ground truth) provide more convenient and standardized plotting functions compared to manual `seaborn.heatmap` and `matplotlib.pyplot.plot` for these specific visualizations.
    *   No explicit `np.random.seed(42)` at the beginning for pure reproducibility beyond `random_state` in `train_test_split` and `LogisticRegression`. While not a major issue here, it's good practice.",gpt-4.1-nano,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', 
                            '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']),
    'Ticker': ['TECHCO']*8,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
    'Close': [152.0, 151.0, 1...",10,"The generated code is excellent. It correctly implements all the requirements of the task:
1. **DataFrame Creation:** The pandas DataFrame is created accurately with the provided data, and 'Date' is correctly set as the index.
2. **SMA Calculation:** The 3-day simple moving average of the 'Close' price is calculated using `df['Close'].rolling(window=3).mean()`, and initial `NaN` values for the first two days are handled correctly by this method.
3. **Volume Percentage Change:** The percentage change in 'Volume' from the previous day is correctly calculated using `df['Volume'].pct_change()`, and it appropriately sets `NaN` for the first day.
4. **Output:** The resulting DataFrame with the new columns is printed as required.
5. **Imports:** Necessary imports (pandas) are included.
The code is clean, efficient, and directly addresses every part of the problem statement.",gpt-4.1-nano,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

# Create the DataFrame with student performance data
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.9...",9,"The generated code correctly creates the DataFrame, calculates the 'Score_Improvement' column, and then calculates and prints the average improvement per subject. It also correctly identifies students who meet the specified criteria (Final_Score > 90 and Attendance_Rate >= 0.95). All necessary imports are included, and the code structure is logical and readable.

The only minor difference from the ground truth is the output format for the identified students. The ground truth iterates and prints each student ID on a new line, along with a message if no students meet the criteria. The generated code uses `tolist()` and prints the list directly. While functionally correct and easy to read, it's a slight deviation from the output presentation of the ground truth (which is not explicitly defined as a strict requirement but is often part of benchmark evaluation). The ground truth also includes more `print` statements for intermediate steps, which are not strictly required for the solution but can be helpful for debugging/demonstration. The generation is concise and directly answers the prompt.",gpt-4.1-nano,LLM_judge_config
M52,medium,Data Manipulation,,"import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

# Filter for Tech department with PerformanceScore > 3
filtered_df...",9,"The generated code correctly defines the DataFrame, filters it according to the specified conditions, and accurately calculates the average ""ProjectsCompleted"" for the filtered subset. All core requirements of the task are met.

The only minor area for improvement is for robustness: the ground truth code includes a check for an empty filtered DataFrame, which is good practice to prevent potential errors or to provide more informative output if no data matches the filtering criteria. While the current input data guarantees a non-empty result, in a production setting with various inputs, handling the empty case is important. Also, the print statement formatting in the ground truth is slightly more descriptive. These are minor stylistic and robust additions, not critical failures.",gpt-4.1-nano,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Filter for Desktop users and calculate conversion rate
desktop_users = df[df[""DeviceType""] == ""Desktop...",9,"The generated code correctly defines the DataFrame, calculates the conversion rates for both Desktop and Mobile users, and prints them clearly labeled. The use of `.mean()` on the boolean 'Converted' series is an efficient and correct way to get the proportion of True values, which then scaled by 100 gives the percentage. The code is well-structured and follows Python best practices.

One minor improvement, as seen in the ground truth, would be to add a check for `df.empty` or for `desktop_users.empty` and `mobile_users.empty` before calculating the mean, to handle edge cases where a particular device type might not be present in the dataset. While it works correctly for the given data, robust code often accounts for such possibilities to avoid potential `NaN` or errors if the filtered DataFrame happens to be empty. However, given the prompt's specific data, this isn't a critical flaw.",gpt-4.1-nano,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

# Define the data
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

# Create the DataFrame
df = pd.DataFrame(data)

# Calculate and print average AirQual...",10,"The generated code is excellent. It correctly defines the pandas DataFrame as specified. It then accurately calculates and prints the average ""AirQualityIndex"" for each unique ""SensorID"". The output is clear, and the use of an f-string with formatting (`:.2f`) makes the output clean and readable, clearly indicating which average belongs to which sensor. The code is concise, efficient, and directly addresses all requirements of the task.",gpt-4.1-nano,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

# Define the data
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}

# Create DataFrame
df = pd.DataFrame(data)

# 1) Create EngagementScore
df['EngagementScore'...",9,"The generated code correctly performs all requested operations: it defines the DataFrame, calculates the ""EngagementScore"", filters for Instagram posts, and computes their average engagement score. The code is clean, concise, and uses appropriate pandas functionalities. The only minor point for improvement, which keeps it from a perfect 10, is the output format. The ground truth code formats the average score to two decimal places and provides a more descriptive print statement, which is good practice for user-facing output. However, the core logic and correctness are excellent.",gpt-4.1-nano,LLM_judge_config
M56,medium,Data Manipulation,,"import pandas as pd

# Create the DataFrame with patient admissions data
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
  ...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the ""LengthOfStay"" column, filters for ""Flu"" diagnosis, and accurately calculates and prints the average ""LengthOfStay"" for these patients. The code is concise, well-structured, and directly addresses all parts of the task. It utilizes pandas functionalities effectively for date calculations and filtering. The import statement is present, and the variable names are clear. No improvements are necessary for task completion.",gpt-4.1-nano,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}
df = pd.DataFrame(data)

# Filter for Sci-Fi movies
sci_fi_movi...",9,"The generated code correctly performs all requested operations: it defines the DataFrame, filters for 'Sci-Fi' movies, and accurately calculates and prints their average rating. The code is concise and uses appropriate pandas functionalities.

Areas for minor improvement:
1.  **Output Clarity:** The ground truth includes a descriptive print statement (`print(f""The average rating for Sci-Fi movies is: {average_sci_fi_rating:.2f}"")`) which makes the output more user-friendly. The generated code simply `print(average_rating)`, which while correct, is less informative.
2.  **Robustness/Edge Cases:** The ground truth includes a check for `if sci_fi_movies_df.empty:`, which handles the case where there might be no Sci-Fi movies (though not strictly necessary for this specific dataset). While not explicitly requested by the prompt, considering such edge cases can sometimes be beneficial for robustness in real-world scenarios. However, for a direct comparison against the prompt's requirements, the generated code satisfies them fully.",gpt-4.1-nano,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

df = pd.DataFrame(data)

# Filter to include only Toyota cars
toyota_c...",9,"The generated code correctly defines the DataFrame, filters for Toyota cars, and accurately calculates their average mileage, addressing all core requirements of the task. The code is clean, concise, and uses standard pandas operations efficiently.

Minor improvements could include:
1.  **Output Formatting:** The ground truth code formats the output string to be more user-friendly (e.g., ""The average Mileage for Toyota cars is: 22500.00""). The generated code simply prints the numerical value.
2.  **Robustness for Empty Filters (Optional but good practice):** The ground truth includes a check (`if not toyota_cars_df.empty:`) to handle cases where no 'Toyota' cars might be found, preventing potential errors or providing a more informative message. While not strictly necessary for *this specific dataset* as Toyota cars are present, it's a good practice for more robust code in general data processing.

Overall, it's a very good solution that fulfills the prompt effectively.",gpt-4.1-nano,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

# Define the data
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, ...",9,"The generated code correctly performs all requested operations: initializing the DataFrame, filtering for Fire-type Pokemon, and calculating their average Attack stat. The code is clean, concise, and uses appropriate pandas functionalities. The main point for improvement, which leads to a slightly lower score than 10, is the output format. The ground truth code provides a more user-friendly and informative print statement for the average attack, including a descriptive string and formatting to two decimal places. The generated code simply prints the numerical value, which is less polished for a user-facing output. Additionally, while not strictly required by the prompt, the ground truth includes an edge case check for an empty filtered DataFrame, which is good practice.",gpt-4.1-nano,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from matplotlib.gridspec import GridSpec

# Load dataset
df = pd.read_csv('titanic.csv')

# Data preprocessing
# Create age groups
bins = [0, 12, 18, 25, 35, 60, 80]
labels = ['Child', 'Teen', 'Young Adult', 'Adult', 'Senior', 'Elder']
df['AgeGroup'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False)

# Map survival to numeric
# Assuming 'Survived' column is 0 or 1
# If not, ma...",6,"The generated code addresses most of the requirements but has several areas for improvement in terms of robustness, clarity, and specific visualization choices.

**Correctness:**
- **Data Preprocessing:** The code doesn't handle missing `Age` or `Fare` values, which can lead to errors or incorrect visualizations, especially when calculating means or plotting distributions. The ground truth fills missing age with median and missing fare.
- **`ci=None` in bar plots:** Using `ci=None` for survival rates is generally fine for average survival rates, but it hides potential uncertainty if we were looking at sample means. The ground truth explicitly calculates the means beforehand, which is clearer.
- **Plotly Integration:** The interactive scatter plot uses `size='Survived'`, which is not what was requested. The request was ""different colors for passenger classes,"" and the ground truth uses `color='Pclass_Cat'`. Also, the interactive plot in the dashboard is converted to a static seaborn scatter plot, which loses interactivity and is not an ""embedding"" as suggested in the comment. The task specifically asked for an *interactive* scatter plot.
- **Dashboard Layout:** While `GridSpec` is used, the layout of the dashboard is not as cohesive or informative as the ground truth. Missing are insights, specific annotations, or an attempt to use `gridspec` for more complex layouts (e.g., spanning multiple rows/columns for a specific plot).

**Code Quality:**
- **Readability:** Some variable names (`fig_dash`, `gs_dash`) are okay, but some preprocessing steps could be more explicit (e.g., handling missing data).
- **Hardcoding:** No configuration for `plt.rcParams` or `sns.set_style`, which the ground truth does to enhance aesthetics and resolution.
- **Error Handling:** No `try-except` for file loading as in the ground truth, which is good practice.

**Completeness:**
- **Missing Preprocessing:** Fails to address missing values in 'Age' and 'Fare' columns, leading to potential data inaccuracies in plots.
- **Interactive Scatter Plot:** While it attempts an interactive scatter plot, it deviates from the requested color scheme (using size instead of color for class) and then replaces it with a static plot in the dashboard, losing the interactive aspect. The task asked for an interactive plot *and* a dashboard combining multiple visualizations.
- **Dashboard Sophistication:** The dashboard is a basic grid of plots. It lacks the advanced combined visualization strategy, custom annotations, or textual insights that the ground truth provides to tell a ""story."" The dashboard only presents plots without interpretation.
- **Minor Details:** Small issues like labels on legends (e.g., ""Did not Survive"" vs ""No"") or consistency in categorical mapping (`Pclass_Cat`) are missing. The ground truth also explicitly orders categories, which helps with consistent plotting.

Overall, the code provides basic implementations for most visualization types but lacks the polish, robustness, and deeper storytelling elements present in the ground truth, particularly in data preprocessing, interactive plot details, and the dashboard's design.",gpt-4.1-nano,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price""...",9,"The generated code correctly defines the DataFrame, filters for the Dystopian genre, and accurately calculates the average stock for that genre, fulfilling all the task requirements. The code structure is clean and follows standard pandas practices.

The only minor point for improvement, which is not critical for correctness but good practice, would be to consider adding a check for `dystopian_books.empty` before calculating the mean, to handle cases where no Dystopian books are present (though for this specific dataset, there are Dystopian books). Also, the output could be formatted more descriptively, e.g., using an f-string like the ground truth. However, as the task only asked to ""print the average 'Stock'"", `print(average_stock)` is technically sufficient.",gpt-4.1-nano,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load dataset
df = pd.read_csv('water_potability.csv')

# List of numerical features to analyze (excluding 'Potability' target)
features = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 
            'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']
target = 'Potability'
alpha = 0.05

# Separate data into potable and non-potable groups
potable = df[df[target] == 1]
non_potable = df[df[target] == 0]
...",7,"The generated code correctly addresses most of the requirements of the task. It performs t-tests, chi-square test, and Shapiro-Wilk tests, and calculates Cohen's d. Imports are correct and the structure is logical.

However, there are several areas for improvement:
1.  **Missing Value Handling:** The generated code does not explicitly handle missing values *before* running the tests. It uses `.dropna()` within the loops, which means each test for each feature will drop NaNs independently. While this prevents errors, it doesn't impute missing values as suggested by the ground truth, nor does it provide a robust global strategy for handling them, which is crucial for statistical analysis to avoid data loss or bias depending on the missingness mechanism. The task implicitly asked for handling missing values, and the ground truth demonstrated a good imputation strategy.
2.  **Cohen's d Calculation:** The calculation for Cohen's d uses the pooled standard deviation formula for *equal variances*, assuming `equal_var=True`. However, the `ttest_ind` is performed with `equal_var=False` (Welch's t-test). For Welch's t-test, alternative effect size measures (like Hedges' g) or a different Cohen's d calculation (using standard deviations directly or a more complex approach considering unequal variances) would be more appropriate. The current mismatch can lead to slightly misleading effect sizes. More importantly, the pooled standard deviation calculation lacks robustness for `len(potable_data) + len(non_potable_data) - 2` being zero or negative (which means only 0 or 1 item in total). The ground truth provides more robust handling for these edge cases (e.g., if pooled_std is 0).
3.  **Interpretation Detail:** The interpretations are somewhat brief. For t-tests, it could explicitly state the direction of the difference (e.g., ""potable water has significantly higher/lower X than non-potable""). For chi-square and normality tests, it could be more explicit about what the null hypothesis means in the context of the results. The ground truth provides more verbose and helpful interpretations.
4.  **Error Handling:** There's no `try-except` block for `pd.read_csv`, which would make the script more robust if the file is not found.
5.  **Output Clarity:** The output titles are good, but the overall presentation could be improved with more explicit statements about the significance level being used (e.g., ""Results for  = 0.05"").

Overall, it's a solid attempt that performs the core statistical tests, but lacks some of the robustness, meticulousness in effect size calculation, and detailed interpretation and error handling present in a truly expert solution.",gpt-4.1-nano,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import matplot...",6,"The generated code addresses most of the requirements but has critical issues in the data preprocessing steps, particularly regarding the train-test split and cross-validation, leading to potential data leakage.

**Areas for Improvement:**

1.  **Data Leakage in Preprocessing:**
    *   **Missing Value Imputation:** The code performs `SimpleImputer().fit_transform(X)` on the *entire* dataset `X` *before* the train-test split. This means information from the test set is used to impute missing values in the training set, which is a form of data leakage. The imputer should be fitted *only* on the training data and then used to transform both training and testing data.
    *   **Feature Scaling:** Similarly, `StandardScaler().fit_transform(X_imputed)` is applied to the *entire* dataset `X_imputed` before the split. The scaler should be fitted *only* on the training data.

    This issue is significant because it can lead to overly optimistic performance estimates on the test set and poor generalization to truly unseen data.

2.  **Cross-Validation Implementation:**
    *   The `cross_val_score` is called using `X_scaled` and `y`, which are the *entire* fully preprocessed datasets where imputation and scaling were done *before* the train-test split or CV folds. This means each fold in `cross_val_score` uses the already scaled data, derived from the full dataset.
    *   **Lack of Pipeline:** To correctly perform cross-validation without data leakage, the preprocessing steps (imputation and scaling) should be integrated into a `Pipeline` object. This pipeline should then be passed to `cross_val_score` so that `fit_transform` (for imputer and scaler) is applied *only* to the training folds, and `transform` is applied to the validation fold within each CV iteration. The ground truth code correctly uses a `Pipeline` for this purpose.
    *   **Limited Metrics in CV:** The generated code only calculates accuracy for cross-validation (`scoring='accuracy'`). The requirement was to assess model stability, implying a need to check the stability of *all* relevant metrics (precision, recall, F1, ROC-AUC) during cross-validation, not just accuracy.

3.  **Visualization:**
    *   Confusion matrices and individual ROC curves are plotted, but currently, they are all squashed into a 3x3 subplot layout, which looks cluttered and isn't very readable. A separate `plt.figure()` for each plot type (confusion matrix, individual ROC, combined ROC, feature importance) would be much clearer, as done in the ground truth. The subplot layout is also not optimal; for example, `(idx-1)*3 + 1` should be `(idx-1)*3 + 1` in `plt.subplot(3, 3, ...)` will lead to very small figures for each plot. It would be better to iterate and create new figures for each plot *or* use subplots per model for CM/ROC and one large subplot for combined ROC.

4.  **Best Model Selection:** While it states selection based on ROC-AUC, it only uses the test set results to select the best model. The prompt specifically asked to perform k-fold cross-validation ""to assess model stability"" and ""compare model performances"" and select the best based on those more robust CV results, not just a single test set evaluation.

5.  **Handling Missing Values in Feature Importance:** The feature importance display assumes that `df` has the same column names as `X_scaled`. After `fit_transform(X)` directly on `X`, `X_scaled` (or `X_imputed`) becomes a NumPy array if `pd.DataFrame` is not explicitly used with column names. While the generated code correctly converts it back to a DataFrame with column names `X.columns`, for robustness, it's good practice to ensure column names persist through preprocessing.

6.  **Minor Code Style/Completeness:**
    *   No error handling for file not found.
    *   No clear print statements for each section (e.g., ""1. Preprocessing..."", ""2. Training...""), which makes the output less informative.
    *   The subplot indexing for plotting is slightly off; `(idx - 1)*3 + 1` if `idx` starts from 1 and is used in a 3x3 grid, leads to `(0, 1, 2)` then `(3, 4, 5)` then `(6, 7, 8)` but then the code uses `plt.subplot(3,3,3)` for the combined ROC. This is messy and might not display as intended.

Overall, the code structure for training, evaluation, and plotting is mostly there, but the fundamental flaw of data leakage in preprocessing and CV execution makes it less reliable for a real-world scenario.",gpt-4.1-nano,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split

# Load dataset
df = pd.read_csv('water_potability.csv')

# Encode target variable if not already encoded
if df['Potability'].dtype == 'object':
    df['Potability'] = df['P...",6,"The generated code addresses most of the requirements but has several issues that reduce its correctness and completeness.

**Correctness Issues:**
*   **Missing Value Handling:** The code attempts to handle missing values by replacing zeros with NaN for ratios, but it doesn't explicitly address the pre-existing NaNs in `ph`, `Sulfate`, and `Trihalomethanes` columns which are present in the original dataset. The ground truth correctly identifies and handles these.
*   **Polynomial Features:** The polynomial feature generation is incorrect. `poly.get_feature_names()` (deprecated, now `get_feature_names_out`) returns names like 'x0', 'x1', etc. not the original column names. The `pd.concat` then creates new columns with generic names and duplicates original columns if they already exist, rather than adding truly *new* polynomial (interaction or squared) terms. The ground truth carefully avoids this by keeping original columns separate and only adding bona fide new polynomial terms. Moreover, it applies `fillna(0)` *after* polynomial features which might hide issues if `ph`, `Chloramines`, etc. had NaNs *before* poly transformation.
*   **Ratio Denominator Zero Handling:** Using `.replace(0, np.nan)` and then `pd.fillna(0)` effectively replaces division-by-zero results with 0, which might not always be the desired behavior and can distort feature values. The ground truth uses a small `epsilon` value which is a more robust way to handle potential division by zero results.
*   **Feature Scaling:** The generated code completely omits feature scaling (e.g., using `StandardScaler`) before applying `SelectKBest` and `RFE`. This is crucial for these methods, especially for models like RandomForest (which generally doesn't strictly *require* scaling, but RFE applies the estimator iteratively on subsets of features, so general scaling is good practice). This is a significant omission for a data science benchmark.
*   **Feature Importance Plot:** The x-axis labels (`X.columns[indices]`) in the feature importance plot are cut off unless the plot is made very large, making it unreadable. The ground truth sorts and takes `head(25)` for better visualization.
*   **Pairplot Features:** The `top_features` list should be applied to the DataFrame *before* concatenation with 'Potability' to avoid issues if 'Potability' isn't explicitly in the original `df`. The ground truth does this more robustly.

**Completeness Issues:**
*   **Meaningful Features:** While it creates some ratios and interactions, the selection of engineered features is less comprehensive than the ground truth. The ground truth generates more diverse ratios and interaction terms.
*   **Polynomial Features Selection:** The ground truth carefully selects a subset of features for polynomial expansion to avoid excessive dimensionality, which is a good practice. The generated code applies it to `ph`, `Chloramines`, `Hardness`, `Solids` without much justification.
*   **Top 5 Correlated Features:** The ground truth correctly excludes 'Potability' itself from the list of top 5 correlated features (as it will always have a correlation of 1 with itself and would wrongly appear in the list). The generated code's `drop('Potability')` correctly handles this.
*   **Comparison of Feature Sets:** The comparison of feature sets (`set_kbest.intersection(set_rfe)`) is very basic. The ground truth provides a more thorough analysis, including common features, unique features for each method, and overlap with top correlated features, which is valuable for understanding the strengths of different selection methods.
*   **Vis readability:** The ground truth uses `sns.set_style(""whitegrid"")` and more elaborate `plt.figure(figsize=...)` settings for better presentation and readability compared to the generated code.

The absence of feature scaling and the incorrect polynomial feature handling are the most critical shortcomings.",gpt-4.1-nano,LLM_judge_config
