task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1. List all column names and identify data types
column_info = df.dtypes
print(""Column Names and Data Types:"")
print(column_info)

# 2. Calculate descriptive statistics for numerical columns
numerical_stats = df.describe()
print(""\nDescriptive Statistics for Numerical Columns:"")
print(numerical_stats)

# 3. Total number of rows and columns
total_rows, total_columns = df.shape
print(""\nTotal Rows:"", total...",6,"The generated code correctly loads the dataset, calculates descriptive statistics for numerical columns, and reports the dimensions. However, it falls short in accurately identifying data types and categorical/ordinal columns as specifically requested in the prompt.

**Areas for Improvement:**
1.  **Data Type Identification:** The prompt asked to ""identify the data type of each column (numerical, categorical, etc.)"" and then list them. The generated code `print(column_info)` prints the pandas dtypes, but doesn't explicitly categorize them as ""Numerical,"" ""Categorical,"" etc., nor does it list them alongside specific column names as clearly as the ground truth. It also lumps all numeric columns as numerical and all object/category columns as categorical/ordinal, which is an oversimplification for this particular dataset where 'quality' is numeric but often treated as ordinal.
2.  **Categorical/Ordinal Identification Specificity:** While `df.select_dtypes(include=['object', 'category'])` is the standard way to find object/category columns, for the `winequality-red.csv` dataset, there are no 'object' or 'category' dtypes initially. All columns are numerical. The prompt specifically adds a nuance for `quality` column, which is numerical but often treated as ordinal. The generated code will incorrectly report an empty list for categorical/ordinal columns because it only looks for specific pandas dtypes and not for column semantics (e.g., discrete numerical values that might represent categories). The prompt asks to ""identify any columns that *appear* to be categorical or ordinal,"" implying semantic consideration beyond just strict pandas dtypes.
3.  **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError`, which is good practice for loading datasets. The generated code lacks this.
4.  **Clarity of Output:** The ground truth code provides more structured and labeled output that clearly distinguishes between different sections of the task, making the results easier to interpret. The generated code's output is functional but less explicit.",gpt_4o_mini,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
df = pd.read_csv('video_games_sales.csv')
print(df.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(df.shape)

# 3) List all column names and their data types
print(df.dtypes)

# 4) Display the top 10 games by Global_Sales
top_10_games = df.nlargest(10, 'Global_Sales')
print(top_10_games[['Name', 'Global_Sales']])

# 5) Show the unique values in the Platform and Genre columns
unique_platforms...",7,"The generated code correctly addresses most of the tasks. It successfully loads the data, displays the shape, column types, top 10 games by sales, unique platforms/genres, summary statistics for sales columns, and missing values.

However, there are several areas for improvement:
- **Error Handling**: The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is crucial for robust applications.
- **Output Clarity**: The ground truth code includes print statements to clearly indicate what each section of output represents (e.g., ""--- 1. Loading Dataset and Displaying First 10 Rows ---""). The generated code simply prints the output without any descriptive headers, making it harder to read and understand the workflow.
- **`df.info()` vs. `df.dtypes`**: While `df.dtypes` provides column names and data types, `df.info()` from the ground truth also provides non-null counts and memory usage, which is more comprehensive for EDA. The prompt specifically asked to ""List all column names and their data types,"" and both satisfy this, but `df.info()` is generally preferred in professional EDA.
- **Output for Top 10 Games**: When displaying the top 10 games, the ground truth explicitly selects `['Name', 'Platform', 'Genre', 'Publisher', 'Global_Sales']`, providing more context than just `['Name', 'Global_Sales']` from the generated code, which might be more useful for the user.
- **Safeguards for Column Existence**: The ground truth includes checks (e.g., `if 'Global_Sales' in df.columns:`) to prevent errors if a critical column is missing, making the code more robust. The generated code assumes all specified columns (e.g., `Global_Sales`, `Platform`, `Genre`, *and all sales columns*) will always exist, which can lead to runtime errors if the dataset structure changes.
- **Visibility of Sales Columns**: In `summary_statistics`, the ground truth ensures that only *existing* sales columns are used, preventing key errors if one of the `sales_columns` is not present in the dataframe. The generated code would crash if, for example, 'NA_Sales' was missing.
- **Missing Values Output**: The prompt asked to ""Identify any missing values"". While `df.isnull().sum()` identifies them, the ground truth's approach of filtering to `missing_values[missing_values > 0]` is clearer as it only shows columns with actual missing values, enhancing readability for large datasets.

Overall, the code is functional but could benefit significantly from improved robustness, clarity, and adherence to common EDA best practices regarding output presentation and error handling.",gpt_4o_mini,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd

# Step 1: Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Step 2: Handle missing values in the Year column
median_year = df['Year'].median()
df['Year'].fillna(median_year, inplace=True)

# Step 3: Remove games with missing Publisher information
df = df[df['Publisher'].notna()]

# Step 4: Create a new column 'Years_Since_Release'
df['Years_Since_Release'] = 2023 - df['Year']

# Step 5: Remove duplicate games (based on Name and Platform)
df.drop_duplica...",7,"The generated code addresses most of the requirements but has several areas for improvement.

Strong points:
- Correctly loads the dataset.
- Handles missing 'Year' values by filling with the median.
- Correctly removes games with missing 'Publisher' information.
- Creates the 'Years_Since_Release' column correctly.
- Correctly removes duplicate games based on 'Name' and 'Platform'.
- Filters out low-selling games based on 'Global_Sales'.
- Displays the final shape and head of the DataFrame.

Areas for Improvement:
- **Robustness in 'Year' column handling**: The ground truth converts 'Year' to numeric with `errors='coerce'` before calculating the median and filling NaNs, then converts to `int`. The generated code assumes 'Year' is already numeric enough for `median()` and directly fills, which might lead to errors if 'Year' contains non-numeric strings.
- **Robustness in 'Global_Sales' column handling**: Similar to 'Year', the generated code assumes 'Global_Sales' is numeric. The ground truth explicitly converts 'Global_Sales' to numeric using `pd.to_numeric` with `errors='coerce'` and then drops NaNs created by this conversion, making it more robust to non-numeric entries in this column.
- **Error Handling**: The generated code lacks error handling for `FileNotFoundError` or other potential issues during data loading and processing, which the ground truth includes using a `try-except` block.
- **Data Type Consistency**: After filling median for 'Year', it's good practice to ensure the column is an integer type, which the ground truth does. The generated code does not explicitly cast 'Year' to `int` after filling.
- **Negative `Years_Since_Release`**: While less common, if a game's release year is after the `CURRENT_YEAR` (e.g., future releases or data entry errors), `Years_Since_Release` could be negative. The ground truth includes a step to ensure this column is non-negative, which is a good practice for derived metrics.
- **Clarity/Step-by-step**: The ground truth prints intermediate shapes, which is very helpful for debugging and understanding the impact of each cleaning step. This is missing in the generated code.
- **Constants**: The `CURRENT_YEAR` (2023) is hardcoded in the generated code, whereas the ground truth defines it as a constant, improving maintainability.",gpt_4o_mini,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('video_games_sales.csv')

# 1. Bar chart showing the top 10 gaming platforms by total global sales
top_platforms = data.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False).head(10)
plt.figure(figsize=(10, 5))
top_platforms.plot(kind='bar', color='skyblue')
plt.title('Top 10 Gaming Platforms by Total Global Sales')
plt.xlabel('Platform')
plt.ylabel('Total Global Sales (millions)...",7,"The generated code successfully creates all the requested visualizations.

Strengths:
- All required chart types (bar, pie, histogram, scatter, line) are generated.
- Plots generally have correct titles and labels.
- Uses pandas and matplotlib appropriately.

Areas for Improvement:
- **Data Loading and Error Handling**: The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth. This makes the code less robust.
- **Data Cleaning/Preprocessing**: The ground truth includes more robust data cleaning (e.g., converting 'Year_of_Release' and sales columns to numeric, handling NaNs) which is crucial for real-world data and ensures plots are accurate. The generated code assumes clean numeric data for sales and year which might not always be the case.
- **Pie Chart Readability**: The ground truth intelligently handles the ""other"" category for the pie chart to improve readability by aggregating small genres. The generated code does not do this, which can make the pie chart cluttered if there are many small categories.
- **Histogram Range**: The ground truth sets a range for the histogram using a percentile, which is good practice to focus on the bulk of the data and avoid extreme outliers skewing the visualization. The generated code uses default bins, which might not be optimal for skewed sales data.
- **Line Plot X-axis**: The `xticks` for the line plot in the generated code tries to set all years as ticks, which can lead to overlapping labels if there are many years. The ground truth relies on matplotlib's auto-tick placement which is often better or uses rotation/skipping, though it explicitly filters out extreme years.
- **Figure Sizing**: The ground truth consistently uses `plt.figure(figsize=...)` before each plot, which is good practice for controlling plot dimensions. The generated code also uses it but some sizes could be optimized.
- **Plot Customization**: The ground truth has more refined customization (e.g., `plt.grid`, `plt.tight_layout`, `alpha` for scatter plot, more descriptive y-labels like ""Millions""). These small details improve the overall presentation.
- **Consistency in Column Names**: The ground truth code correctly identifies `Year_of_Release`, while the generated code uses `Year` which might depend on the exact column name in the CSV (`Year_of_Release` is more common for this dataset). This could cause an error if the column name is indeed `Year_of_Release`. (Assuming `Year_of_Release` from the common dataset name).",gpt_4o_mini,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Display the first 10 rows
print(df.head(10))

# Show the shape of the dataset
print(""Shape of the dataset:"", df.shape)

# List all column names and their data types
print(""Column names and data types:\n"", df.dtypes)

# Calculate the percentage of diabetic vs non-diabetic patients
diabetic_count = df['Outcome'].value_counts(normalize=True) * 100
print(""Percentage of diabetic vs non-diabetic patients:\n"", diabeti...",6,"The generated code addresses most of the requirements but has several areas for improvement in terms of robustness, completeness, and clarity.

**Strengths:**
*   Correctly loads the CSV and displays the first 10 rows, shape, and dtypes.
*   Correctly calculates the percentage of diabetic vs. non-diabetic patients.
*   Correctly identifies missing values and displays summary statistics.

**Areas for Improvement:**
*   **Error Handling:** The ground truth includes `try-except` blocks for file loading, which is crucial for robust code. The generated code lacks this.
*   **Clarity and Readability:** The ground truth uses section headers (`--- 1. ... ---`) and dividers (`=""*50`) to clearly delineate each step, making the output much more readable and user-friendly. The generated code just prints outputs one after another.
*   **Missing Values Recheck (Zero as Missing):** The prompt specifically asked to ""Identify any columns with zero values that might represent missing data."" The generated code's approach `(df == 0).any()` identifies *any* column that contains a zero, which is not specific enough. For example, `Pregnancies` can legitimately be zero. The ground truth correctly identifies specific columns where zero values are problematic (`Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`) and calculates the count of *only* those suspicious zeros, making it a more intelligent and accurate check for potential missing data. This is a significant difference in interpretation and implementation of this specific requirement.
*   **Import `numpy`:** Although not strictly used in every line, `numpy` is often imported alongside `pandas` for numerical operations and is a good practice for data science tasks. The ground truth includes it.
*   **No Explicit `DATASET_PATH` variable:** While minor, defining the dataset path at the top makes the code more configurable.

Overall, the generated code performs the core EDA steps but falls short in robustness, output presentation, and precise fulfillment of the ""zero as missing"" requirement compared to the more thorough ground truth.",gpt_4o_mini,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Step 1: Load the dataset
df = pd.read_csv('diabetes.csv')

# Step 2: Identify columns with zero values that represent missing data
columns_with_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_with_zeros] = df[columns_with_zeros].replace(0, np.nan)

# Step 3: Fill missing values with the median of each respective column
df[columns_with_zeros] = df[columns_with_ze...",6,"The generated code correctly addresses most of the task requirements but has several significant deficiencies.

**Strengths:**
- Correctly loads the dataset.
- Identifies and replaces zero values in specified columns with NaN.
- Fills missing values with the median.
- Removes duplicate rows.
- Creates the `BMI_Category` column with appropriate logic.
- Uses `StandardScaler` for numerical features.

**Weaknesses and Areas for Improvement:**

1.  **Missing Error Handling:** The generated code lacks error handling for `FileNotFoundError` when loading `diabetes.csv`. The ground truth includes a `try-except` block, which is crucial for robustness.
2.  **Scalability Issue in Zero Replacement/Median Imputation:** The generated code performs `df[columns_with_zeros] = df[columns_with_zeros].fillna(df[columns_with_zeros].median())`. While this works, it computes the median across *all* specified columns simultaneously for each column (effectively `df[['col1','col2']].median()`), then tries to use that to fill NAs in a broadcasted manner. It's more explicit and safer to iterate through the columns and fill each one individually with its specific median, as shown in the ground truth. This also ensures that if one column was *not* found (though the ground truth already handles that warning), the rest would still be processed correctly.
3.  **Incomplete Scaling Method (Critical Flaw):**
    *   `numerical_features = df.drop(columns=['BMI_Category'])`: This line incorrectly assumes that *all* other columns are numerical and should be scaled. `Outcome` is a numerical column in this dataset but typically represents the target variable and should not be scaled. The ground truth correctly identifies `Outcome` and excludes it from scaling.
    *   The scaled data `scaled_features` is put into a *new* DataFrame `df_scaled`, but the original `df` is never updated with these scaled values. This creates a disconnect. The task asks to ""Display the cleaned dataset's shape and first 5 rows"", implying a single, coherent cleaned `df`. The ground truth scales in-place on the original `df` for the numerical columns, which is a better approach for maintaining a single dataset object.
    *   The `Outcome` column is completely lost in `df_scaled` after the current scaling approach, which is a major data integrity issue.
4.  **Implicit Assumptions about Column Existence:** The code assumes that 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI' columns exist. While the ground truth also makes this assumption, it adds `if col in df.columns:` checks, which are good practice.
5.  **Lack of Informative Output:** The generated code only prints the shape and head. The ground truth provides more context, such as the original shape, confirmation of duplicates removed, and columns scaled, which are very helpful for debugging and understanding the cleaning process.
6.  **Minor Categorization Edge Case:** `18.5 <= bmi < 25` and `25 <= bmi < 30`. While functionally correct for this dataset, `pd.cut` with `bins` and `labels` is generally more robust and concise for binning numerical data. `pd.cut` also handles the bin edges more systematically (e.g., `right=True`). For example, `bmi < 25` would put 24.99 in 'Normal', while a range up to 24.9 (as in medical definitions) would be better represented by `24.9`. The ground truth's `bins = [0, 18.5, 24.9, 29.9, np.inf]` is more precise according to the problem description (18.5-24.9, 25-29.9). The generated code's `25` is less precise for the 'Normal' upper bound if medical definitions of normal BMI (up to 24.9) are strictly followed.",gpt_4o_mini,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('diabetes.csv')

# 2) Create a correlation heatmap
plt.figure(figsize=(12, 8))
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, fmt="".2f"", cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap of Diabetes Dataset')
plt.show()

# 3) Generate histograms for all numerical features
data.hist(bins=15, figsize=(15, 10), edgecolor='black')
plt...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
- All required visualization types (heatmap, histograms, box plots, pie chart, scatter plots, pair plot) are generated.
- Proper imports are included.
- For most plots, titles and labels are present, contributing to good readability.
- The use of `subplot` for multi-plot figures is appropriate.
- The selection of key features for the pair plot is correct.

**Areas for improvement:**
- **Correlation Heatmap:** The ground truth excludes the 'Outcome' column from the correlation to focus only on numerical features for their interrelationships, while the generated code includes it. While not strictly incorrect, it's generally better practice to correlate features with each other rather than the target variable.
- **Histograms:** The generated code uses `data.hist()`, which is a quick way to plot histograms but offers less control over individual plot aesthetics (like `kde` line, specific colors, and `xlabel/ylabel` for each subplot) compared to iterating and using `sns.histplot` as in the ground truth. While it fulfills the basic requirement, the ground truth's approach is more explicit and customizable.
- **Box Plots:** The `xlabel` for the box plots could be more descriptive for the 'Outcome' axis (e.g., ""0: Non-Diabetic, 1: Diabetic"") as in the ground truth. The current `xlabel` is just 'Outcome'. Also, the `features` variable excludes the 'Outcome' column (`data.columns[:-1]`), which is correct logic for plotting features against the outcome, but the `numerical_cols` in the ground truth (which also excludes Outcome) is used to iterate over numerical features for histograms and heatmap, providing a more consistent and robust definition of ""numerical features.""
- **Pie Chart Labels:** The labels for the pie chart are directly hardcoded `['Non-Diabetic', 'Diabetic']`. While functional, the ground truth uses `outcome_counts.index` to dynamically assign labels based on the actual outcome values, which is more robust if the outcome values in the dataset were to change (e.g., to 'Yes'/'No' instead of 0/1). Also, the ground truth includes `explode` for emphasis and `shadow=True` for a slightly more polished look, though these are minor aesthetic points.
- **Scatter Plot Legends:** The scatter plot legends simply show '0' and '1'. The ground truth explicitly labels them as 'Non-Diabetic' and 'Diabetic' in the legend, which is much more informative.
- **Pair Plot `diag_kind`:** The ground truth specifies `diag_kind='kde'` for the pair plot, which shows kernel density estimates on the diagonal, providing a smoother distribution visualization compared to the default histograms generated on the diagonal by the generated code. This is a minor subtlety but adds to the quality of the visualization.",gpt_4o_mini,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

# 1. Load the dataset and display the first 10 rows
df = pd.read_csv('world-happiness-report-2021.csv')
print(df.head(10))

# 2. Show the shape of the dataset (rows and columns)
print(""Shape of dataset:"", df.shape)

# 3. List all column names and their data types
print(""Column names and data types:\n"", df.dtypes)

# 4. Display the unique regional indicators and count of countries per region
region_counts = df['Regional indicator'].value_counts()
print(""Unique regio...",7,"The generated code correctly addresses most of the requirements but lacks robustness and adherence to some best practices demonstrated in the ground truth.

**Strong Points:**
*   Correctly loads the dataset and displays the first 10 rows.
*   Correctly displays the shape of the dataset.
*   Correctly lists column names and data types (though `df.dtypes` is less informative than `df.info()`).
*   Correctly displays unique regional indicators and counts.
*   Correctly identifies and displays the top 10 happiest countries.
*   Correctly calculates summary statistics for the specified columns.
*   Correctly identifies missing values.

**Areas for Improvement:**
*   **Robustness (File Handling):** The generated code does not include error handling for `FileNotFoundError` when loading the CSV, which is crucial for a real-world scenario. The ground truth includes a `try-except` block for this.
*   **Clarity and Readability:** The ground truth adds separators and clear headings for each task, making the output much easier to follow. The generated code prints results sequentially without much demarcation.
*   **Task 3 (`df.dtypes` vs `df.info()`):** While `df.dtypes` fulfills the request for column names and data types, `df.info()` from the ground truth provides a more comprehensive overview including non-null counts, which is very useful in EDA.
*   **Task 6 (`GDP per capita` vs `Logged GDP per capita`):** The ground truth correctly uses 'Logged GDP per capita' which is the actual column name in the dataset, enhancing accuracy. The generated code uses 'GDP per capita', which might lead to `KeyError` if the column name isn't exactly matched (though in this specific dataset 'GDP per capita' might implicitly work if it's an alias or if the dataset was slightly different, the ground truth is more precise).
*   **Error Handling for Column Existence:** The ground truth wisely includes checks (`if 'Column Name' in df.columns:`), which makes the code more robust against variations in dataset schema or typos, preventing `KeyError`. The generated code assumes all columns exist.

Overall, it's a functional solution but could be improved significantly by adding error handling and more structured output, aligning with production-ready code.",gpt_4o_mini,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder

# 1) Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# 2) Handle missing values
for column in df.select_dtypes(include=[np.number]).columns:
    df[column].fillna(df[column].mean(), inplace=True)
    
for column in df.select_dtypes(include=[object]).columns:
    df[column].fillna(df[column].mode()[0], inplace=True)

# 3) Create new derived features
df['Happiness_...",7,"The generated code addresses most of the requirements but has a few inaccuracies and omissions that prevent it from being perfect.

**Strengths:**
*   Correctly loads the dataset and handles missing numerical and categorical values.
*   Correctly implements the IQR method for outlier removal on 'Ladder score'.
*   Correctly uses `StandardScaler` for numerical features and `LabelEncoder` for 'Regional indicator'.
*   Correctly displays the final shape and head of the DataFrame.
*   The overall structure is clean and follows the steps outlined in the task.

**Areas for Improvement/Correction:**
1.  **""GDP\_Category"" Feature Creation:** The task specified `Logged GDP per capita` for the GDP_Category, but the generated code uses `GDP per capita`. This is an older column name from previous happiness reports and is not present in the `world-happiness-report-2021.csv` dataset, which uses `Logged GDP per capita`. This will cause a `KeyError`.
2.  **""Well\_being\_Score"" Calculation:** The calculation `(df['Social support'] + df['Healthy life expectancy'] + df['Freedom']) / 3` is correct for an average, but the ground truth code used `df[['Social support', 'Healthy life expectancy', 'Freedom to make life choices']].mean(axis=1)`. The provided column name `'Freedom'` is also incorrect; the dataset uses `'Freedom to make life choices'`. This will cause a `KeyError` unless the column name is corrected.
3.  **""Happiness\_Category"" Logic:** The prompt specified the ranges for 'Happiness\_Category' as: High: >6.5, Medium: 4-6.5, Low: <4. The `pd.cut` implementation correctly assigns 'Low' for <4 and 'High' for >6.5, but the 'Medium' range is `(4, 6.5]` or `(4-6.5]`. The `pd.cut` approach covers this generally but the specific ranges in the prompt (`4-6.5`) could be slightly ambiguous if `4` or `6.5` fall precisely on a boundary when using `pd.cut`. While `pd.cut` is a valid approach, `np.select` (as in the ground truth) directly maps conditions with clear inequalities, which might be a more precise fit for the exact ranges specified in the prompt. This is a minor point, but worth noting for exact adherence.
4.  **Error Handling for File Loading:** The generated code lacks explicit error handling for `FileNotFoundError` when loading the CSV, which the ground truth included. While not explicitly asked for, it's good practice for robustness.
5.  **Descriptive Category Labels for GDP:** The prompt left the `GDP_Category` labels flexible by simply stating 'based on quartiles'. The generated code uses 'Low', 'Medium', 'High', 'Very High', which is a reasonable choice, but deviates from the ground truth's 'Q1', 'Q2', 'Q3', 'Q4' which directly reflects quartiles. This is a stylistic choice more than an error, but worth noting.

The critical errors regarding incorrect column names (`GDP per capita` and `Freedom`) for feature creation will lead to the code failing to run as intended, significantly reducing its score.",gpt_4o_mini,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# 2) Create a world map visualization showing happiness scores by country
fig_map = px.choropleth(df,
                         locations='Country name',
                         locationmode='country names',
                         color='Ladder score',
                         color_continuous...",6,"The generated code attempts to address all requirements but falls short in several aspects, primarily related to data handling and specific visualization details.

**Correctness (6/10):**
*   **Map visualization**: Correctly uses `plotly.express.choropleth`.
*   **Bar chart**: Correctly groups by region and plots. However, the column for 'Region' is incorrect in the dataset (it should be 'Regional indicator'). This causes a `KeyError`.
*   **Correlation Heatmap**: Correctly identifies relevant columns and generates the heatmap.
*   **Scatter Plots**: Correctly generates two scatter plots using `matplotlib` and `seaborn`.
*   **Box Plots**: Correctly attempts to generate box plots but suffers from the same 'Region' column issue as the bar chart.
*   **Radar Chart**: This is the most problematic section.
    *   The features used for the radar chart are not normalized, which is crucial for a meaningful radar chart comparison across different scales of metrics. Without normalization, features with larger magnitudes (e.g., 'GDP per capita') will dominate the chart, rendering smaller-scale features (e.g., 'Generosity') almost invisible or insignificant.
    *   The `labels` array for the radar chart includes 'Ladder score' which is not a factor but the outcome, making the chart less intuitive for ""happiness profile.""
    *   The `top_5_happiest_values` array concatenation is incorrect for filling the radar chart and setting x-tick labels.
    *   `ax.set_yticklabels([])` removes all y-tick labels, making it harder to interpret the values on the chart.
    *   The column names like 'GDP per capita' are not consistent with the typical transformations needed for a radar chart (e.g., 'Logged GDP per capita').

**Code Quality (7/10):**
*   Includes proper imports.
*   Uses `plt.show()` after each plot, which is good.
*   Labels and titles are present but could be more descriptive or consistent (e.g., specifying the year).
*   No error handling for file loading, which the ground truth includes.
*   Does not rename columns for consistency, leading to longer column names in visualizations.

**Completeness (6/10):**
*   **Loads dataset**: Yes.
*   **World Map**: Yes, with Plotly.
*   **Horizontal Bar Chart**: Yes, but fails due to `Region` column issue.
*   **Correlation Heatmap**: Yes.
*   **Scatter Plots**: Yes.
*   **Box Plots**: Yes, but fails due to `Region` column issue.
*   **Radar Chart**: Attempts to create, but the implementation is flawed and doesn't meet the intent of showing a ""happiness profile"" effectively due to lack of normalization and incorrect feature selection/plotting logic. The ground truth correctly uses `MinMaxScaler` which is a critical omission here.
*   **Proper titles/labels**: Mostly yes, but can be improved (e.g., rotation for x-axis labels on box plots).
*   **matplotlib, seaborn, plotly usage**: All three libraries are used.

**Areas for Improvement:**
1.  **Data Preprocessing/Renaming**: Implement column renaming as done in the ground truth for consistency and easier plotting.
2.  **Column Names**: Fix the `Regional indicator` vs `Region` issue. The dataset uses `Regional indicator`, not `Region`.
3.  **Radar Chart Normalization**: Crucially, normalize the data for the radar chart using `MinMaxScaler` (or similar) to ensure all features are on a comparable scale (e.g., 0-1).
4.  **Radar Chart Features**: Refine the features included in the radar chart to represent actual contributing factors rather than the 'Ladder score' itself.
5.  **Radar Chart Plotting Logic**: Review the plotting logic for the radar chart, especially how values are mapped to angles and how the plot is filled. Ensure the y-axis labels on the radar chart are present and meaningful (e.g., 0.2, 0.4).
6.  **Error Handling**: Add `try-except` block for file loading.
7.  **Plot Aesthetics**: Consider rotating x-axis labels for readability in box plots and other charts where they might overlap. Add `tight_layout()`.",gpt_4o_mini,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd

# 1) Load the dataset and parse the 'date' column as datetime objects
data = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2) Display the first 10 rows
print(data.head(10))

# 3) Show the shape of the dataset (rows and columns)
print(data.shape)

# 4) List all column names and their data types
print(data.dtypes)

# 5) Calculate and display the total sum of 'total_cases' and 'total_deaths' grouped by 'continent'
total_sum = data.groupby('contine...",8,"The generated code provides a mostly correct and complete solution to the task.

**Strong points:**
*   Correctly loads the CSV and parses the date column.
*   Correctly displays the first 10 rows and dataset shape.
*   The grouping and summation for `total_cases` and `total_deaths` by `continent` is correct.
*   The logic for finding the top 5 locations based on the latest date and total cases is efficient and correct.
*   The summary statistics for `new_cases` and `new_deaths` are calculated and displayed appropriately, including the renaming of '50%' to 'median'.
*   Missing values are correctly counted and displayed.

**Areas for improvement:**
1.  **Error Handling (Critical omission):** The generated code lacks any error handling for `FileNotFoundError` or other potential issues during CSV loading, which is present in the ground truth. This is a significant point of failure for real-world scenarios.
2.  **Verbosity of Output Headers:** The ground truth includes `print` statements with descriptive headers (e.g., ""--- First 10 rows of the dataset ---"") before each section of output, which significantly improves readability for an exploratory data analysis script. The generated code only prints the dataframes directly, making the output harder to interpret.
3.  **Data Type Display (Minor Difference):** While `data.dtypes` gives the data types, `df.info()` from the ground truth provides a more comprehensive overview including non-null counts, which is generally more useful for EDA as it immediately hints at missing values and memory usage.
4.  **Robustness for Missing Columns:** The ground truth includes checks (`if col in df.columns:`) before attempting operations on specific columns (like `continent`, `total_cases`, `new_cases`), making it more robust if the dataset structure deviates unexpectedly. The generated code does not include these checks.",gpt_4o_mini,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1) Calculate and report the average (mean) 'quality' of the red wines
average_quality = df['quality'].mean()
print(f'Average quality of red wines: {average_quality:.2f}')

# 2) Group the wines by their 'quality' score and calculate the average 'alcohol' content for each quality group
average_alcohol_by_quality = df.groupby('quality')['alcohol'].mean()...",8,"The generated code correctly addresses all three main requirements: calculating the mean quality, grouping by quality to find average alcohol, and visualizing this relationship with a bar chart. Imports are correct, and the structure is logical.

Areas for improvement to get a higher score:
1.  **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code lacks this.
2.  **Printing of Grouped Data:** The ground truth explicitly prints the `average_alcohol_by_quality` DataFrame, which was a subtle part of the second requirement (""report the average 'alcohol' content for each quality group""). The generated code calculates it but doesn't print it.
3.  **Plot Enhancements:**
    *   **Labels on Bars:** The ground truth adds specific alcohol values (labels) on top of each bar in the chart, which significantly enhances readability and provides precise data points directly on the visualization. The generated code doesn't include this.
    *   **Clarity of Labels/Titles:** While adequate, the plot titles and labels in the ground truth are slightly more descriptive (e.g., 'Quality Score' vs. 'Wine Quality', 'Average Alcohol Content (%)' vs. 'Average Alcohol Content').
    *   **`sns.set_style()`:** The ground truth sets a seaborn style, which is a nice touch for consistent aesthetics.
4.  **`reset_index()` for Seaborn:** When `groupby()` is called, it returns a Series with the 'quality' as the index. While `sns.barplot(x=average_alcohol_by_quality.index, y=average_alcohol_by_quality.values)` works, it's often more idiomatic and readable in `seaborn` to use a DataFrame with named columns. The ground truth's use of `.reset_index()` makes the `average_alcohol_by_quality` a DataFrame, allowing for `sns.barplot(x='quality', y='alcohol', data=average_alcohol_by_quality)`, which is generally cleaner.

Overall, it's a very good attempt that fulfills the core requirements. The improvements would mainly be in robustness and making the visualization more informative and polished.",gpt_4o_mini,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset and parse the 'date' column
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Handle missing values
df['hosp_patients'].fillna(0, inplace=True)
df['icu_patients'].fillna(0, inplace=True)
df['positive_rate'].fillna(df['positive_rate'].mean(), inplace=True)
df['reproduction_rate'].fillna(df['reproduction_rate'].mean(), inplace=True)

# Create 'mortality_rate' column
df['mortality_rate'] = (df['total_deaths']...",8,"The generated code correctly addresses most of the task requirements. It loads the data, parses the 'date' column, handles missing values as specified for `hosp_patients`, `icu_patients`, `positive_rate`, and `reproduction_rate`. It also correctly creates the `mortality_rate` and `vaccination_rate_per_capita` columns, explicitly handling division by zero cases with `np.inf` and `fillna(0)`. The filtering for the year 2021 is also correct.

Areas for improvement:
1.  **Robustness for `pd.to_numeric`**: The ground truth explicitly converts columns like `positive_rate`, `reproduction_rate`, `total_deaths`, `total_cases`, `people_fully_vaccinated`, and `population` to numeric types using `pd.to_numeric(..., errors='coerce')` before calculations or mean imputation. While pandas often infers types correctly on load, ensuring this conversion makes the code more robust against potential non-numeric data in these columns, which could lead to errors or incorrect calculations (e.g., trying to calculate a mean on an object column). The generated code implicitly relies on correct type inference.
2.  **File Not Found Handling**: The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for user-facing code. The generated code does not include this.
3.  **Clarity of Output**: The ground truth includes more descriptive print statements for the shape and head, which enhances readability and understanding of the output. The generated code just prints the shape and head without labels.
4.  **`SettingWithCopyWarning`**: While not strictly incorrect, the ground truth uses `.copy()` when filtering `df_2021 = df[df['date'].dt.year == 2021].copy()` to prevent a `SettingWithCopyWarning` in future operations on `df_2021`. This is a minor but good practice for robustness.",gpt_4o_mini,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset and parse the 'date' column
data = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 1. Time series line plot of 'new_cases' for 'India'
india_data = data[data['location'] == 'India']
plt.figure(figsize=(12, 6))
plt.plot(india_data['date'], india_data['new_cases'], marker='o', linestyle='-')
plt.title('New COVID-19 Cases in India Over Time')
plt.xlabel('Date')
plt.ylabel('New ...",7,"The generated code correctly addresses most of the visualization requirements.

**Strong Points:**
*   Correctly loads the data and parses the 'date' column.
*   Successfully generates a time series line plot for India, a histogram for 'reproduction_rate', and a box plot for 'stringency_index' by 'continent' with appropriate titles and labels.
*   Uses `matplotlib.pyplot` and `seaborn` correctly.
*   Includes `tight_layout()` for better plot presentation.

**Areas for Improvement:**
*   **Bar Chart for `total_deaths` by `continent`:** The prompt asked for ""sum of 'total_deaths' for each 'continent'"". The generated code computes `data.groupby('continent')['total_deaths'].sum()`. This is problematic because `total_deaths` is a cumulative metric and summing all entries for a continent will lead to a vastly inflated and incorrect total. The correct approach (as seen in the ground truth) is to take the *latest* `total_deaths` reading for each *country* within a continent, and then sum *those* country-level latest totals. This is a significant logical flaw for this specific visualization.
*   **Scatter Plot for `gdp_per_capita` vs `total_vaccinations`:**
    *   Similar to the bar chart, `total_vaccinations` is also a cumulative metric. The generated code correctly attempts to use `latest_data = data.loc[data.groupby('location')['date'].idxmax()]` to get the latest entry per location, which is good.
    *   However, it doesn't handle `NaN` values, which can lead to errors or warning messages if these columns contain missing data for the latest entries. The ground truth explicitly uses `dropna()` after selecting the latest data.
    *   The prompt asked for a scatter plot for ""all locations"". While `idxmax()` does this, the ground truth also adds `hue='continent'` and `size='population'` for better insights, which were not strictly asked but improve the visualization significantly and are commonly expected in such analysis.
*   **Minor Plotting Details/Robustness:**
    *   The time series plot uses `marker='o'`, which can make the plot look cluttered for a long time series. A simple line without markers is generally preferred for this type of plot unless specific points need emphasis. The ground truth uses `sns.lineplot` without markers by default, which usually looks cleaner.
    *   Error handling for file loading is missing, which is good practice in a complete script.
    *   No explicit `sns.set_theme()` call for consistent styling across plots, although `sns.barplot`, `sns.histplot`, `sns.boxplot` implicitly apply some seaborn styling.

The primary reason for not scoring higher is the logical error in calculating ""total deaths per continent"" and the lack of robustness or refinement (like `dropna` and richer scatter plot details) compared to an expert solution.",gpt_4o_mini,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 1) List all column names and identify the data type of each column
print(df.dtypes)

# 2) Calculate and report basic descriptive statistics for numerical columns
numerical_columns = ['elixirCost', 'Win Rate', 'Usage']
descriptive_stats = df[numerical_columns].describe()

print(descriptive_stats)

# 3) Provide the total number of cards (rows) in the dataset
total_cards = len(df)
print(f'Total number of...",7,"The generated code correctly loads the dataset, identifies column data types, calculates descriptive statistics for the specified numerical columns using `describe()`, and reports the total number of rows. This fulfills the core requirements of the task.

However, there are a few areas for improvement:
- **Output Clarity for Column Names:** The ground truth explicitly lists column names, which can be helpful for quick scanning, whereas the generated code only shows them as part of `df.dtypes`. While `df.dtypes` indirectly shows column names, a direct listing was requested and is generally clearer.
- **Robustness (Error Handling):** The generated code lacks error handling for `FileNotFoundError` or other potential issues during CSV loading, which the ground truth gracefully handles.
- **Specificity of `describe()`:** While `describe()` works and provides the required stats (mean, median, std, min, max, though median is labeled as 50%), the prompt specifically asked for these individual statistics (`mean`, `median`, `std`, `min`, `max`) and could be interpreted as needing a more direct calculation or explicit naming of these. The `describe()` output includes other metrics (count, 25%, 75%) that were not explicitly asked for. The ground truth's use of `.agg(['mean', 'median', 'std', 'min', 'max'])` is more precise and directly fulfills the request.
- **Clarity of Output Messages:** The ground truth provides more descriptive print statements for each section, making the output easier to read and understand what each block of information represents. The generated code's output is more concise but less informative for the end-user.",gpt_4o_mini,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Filter the dataset for cards with elixirCost <= 3
filtered_df = df[df['elixirCost'] <= 3]

# Select the Card name and Win Rate columns
result_df = filtered_df[['Card', 'Win Rate']]

# Sort the resulting data by Win Rate in descending order
sorted_result = result_df.sort_values(by='Win Rate', ascending=False)

# Display the result
print(sorted_result)
```",9,"The generated code correctly solves the task by loading the dataset, filtering by `elixirCost`, selecting the specified columns, and sorting by `Win Rate` in descending order. It includes the necessary import. The code is concise and easy to understand.

Areas for minor improvement (not critical for this specific task, but good practice):
1.  **Error Handling:** The ground truth includes `try-except` blocks for file loading, which makes the code more robust against `FileNotFoundError` or other reading issues. While not explicitly asked for in ""Provide only the Python code without explanations,"" it's a good practice for production-ready code.
2.  **Configuration:** The ground truth also defines constants for column names and file paths. This makes the code more maintainable if column names change or the file path needs to be updated, and reduces ""magic strings.""

Despite these minor points, for the direct requirements of the prompt, the generated code is excellent.",gpt_4o_mini,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Generate histogram for the elixirCost column
plt.figure(figsize=(10, 6))
sns.histplot(df['elixirCost'], bins=10, kde=False)
plt.title('Distribution of Elixir Cost')
plt.xlabel('Elixir Cost')
plt.ylabel('Frequency')
plt.grid(axis='y')
plt.show()
```",8,"The generated code correctly loads the CSV, generates a histogram for the `elixirCost` column, and includes a title and labeled axes, fulfilling the core requirements of the task. The use of seaborn's `histplot` is standard and good practice.

However, there are a few minor areas for improvement:
1.  **Error Handling**: The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth. While not explicitly asked for in the prompt, robust code typically includes this.
2.  **Binning for Discrete Data**: The `bins=10` argument for `histplot` is a general approximation. For discrete data like `elixirCost` (which typically ranges from 1 to 10), it's better to explicitly define bins to ensure each integer cost has its own bar, as done in the ground truth (`bins=range(1, 12)`). This ensures a more accurate and interpretable representation of the distribution.
3.  **Y-axis Label Clarity**: ""Frequency"" is correct but ""Number of Cards"" (as in the ground truth) might be slightly more descriptive in this specific context.
4.  **Aesthetics/Readability**: The ground truth includes `xticks` setting and `edgecolor` for the bars, which improve the readability and appearance of the plot. These are minor but contribute to a polished visualization.

Overall, the generated code is highly functional and addresses the prompt's explicit requirements well. The noted areas are about robustness and finer points of visualization best practices.",gpt_4o_mini,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# Load the iris dataset
df = pd.read_csv('iris.csv')

# 1) List all column names and identify the data type of each column
print(""Column Names and Data Types:"")
print(df.dtypes)

# 2) Calculate and report basic descriptive statistics for numerical columns
numerical_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
descriptive_stats = df[numerical_columns].describe().T[['mean', '50%', 'std', 'min', 'max']]
descriptive_stats.rename(columns=...",8,"The generated code correctly addresses all requirements of the task. It loads the CSV, displays column data types, calculates the required descriptive statistics for the specified numerical columns, and reports unique species counts.

**Strong points:**
*   **Correctness:** All parts of the task are functionally correctly implemented.
*   **Conciseness:** The code is quite concise, especially the descriptive statistics part which cleverly uses `.describe().T` and then selects specific statistics and renames '50%' to 'median'. This is an efficient way to get the desired output.
*   **Proper Imports:** `pandas` is correctly imported.

**Areas for improvement (to reach a 10):**
*   **Robustness:** The ground truth code includes error handling for `FileNotFoundError` when loading the CSV and checks for the existence of columns before proceeding. The generated code assumes the `iris.csv` file exists and that all specified columns (`SepalLengthCm`, etc., and `Species`) are present. In a real-world scenario, this could lead to errors (e.g., `FileNotFoundError`, `KeyError`) if the file is missing or column names are slightly different.
*   **Output Clarity:** While the output is correct, the ground truth code adds more descriptive print statements (`""--- Column Names and Data Types ---""`, etc.) and clear separators (`""-"" * 40`) which make the output more readable and professional. The generated code is a bit more minimalistic in its output formatting.
*   **Specific Column Names vs. `df.dtypes`:** For requirement 1 (""List all column names and identify the data type of each column""), the generated code only prints `df.dtypes` which implicitly includes column names. The ground truth code explicitly prints `df.columns.tolist()` first, which more directly fulfills ""List all column names"" and then `df.dtypes` for data types. It's a small difference but contributes to more explicit fulfillment of the prompt.",gpt_4o_mini,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('iris.csv')

# Filter for 'Iris-setosa' species
setosa_df = df[df['Species'] == 'Iris-setosa']

# Select and display only the SepalLengthCm and SepalWidthCm columns
result = setosa_df[['SepalLengthCm', 'SepalWidthCm']]
print(result)
```",9,"The generated code correctly addresses all functional requirements of the task: it loads the `iris.csv` dataset, filters it to include only 'Iris-setosa' species, and then selects and displays the 'SepalLengthCm' and 'SepalWidthCm' columns. The imports are correct, and the code is clean and readable.

The only minor area for improvement, preventing a perfect 10, is the lack of error handling for file not found or missing columns, which is present in the ground truth. While error handling wasn't explicitly requested in the prompt, robust production-ready code typically includes it. However, for a data science benchmark focusing on core logic, the generated code is excellent.",gpt_4o_mini,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('iris.csv')

# Create a scatter plot
plt.figure(figsize=(10, 6))
scatter = plt.scatter(df['SepalLengthCm'], df['SepalWidthCm'], c=df['Species'].astype('category').cat.codes, cmap='viridis')

# Add title and labels
plt.title('Sepal Length vs Sepal Width')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')

# Create a legend
legend1 = plt.legend(*scatter.legend_elements(), title=""Species""...",7,"The generated code correctly loads the dataset and creates a scatter plot with the specified axes and title. It attempts to color points by species, but the legend is not properly implemented for categorical data when using `c=df['Species'].astype('category').cat.codes`. This approach assigns numerical codes to species, which `cmap='viridis'` then maps to a continuous color scale, rather than distinct colors for each category with a meaningful legend. The x-axis and y-axis labels have extra text (""(cm)"") which was not requested. While it fulfills most requirements, the coloring and legend for `Species` are not as clear or correctly represented as they could be for distinct categories, which is a key requirement of the task. Improvements would involve using a distinct color for each species and ensuring the legend clearly maps these colors to species names.",gpt_4o_mini,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# 1) List all column names and identify the data type of each column
print(df.dtypes)

# 2) Calculate and report basic descriptive statistics for numerical columns
numerical_columns = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']
descriptive_stats = df[numerical_columns].describe().loc[['mean', '50%', 'std', 'min', 'max']]
descriptive_stats.rename(index={'50%': 'median'}, inplace=True)
print(descriptive_stats)

# 3) ...",7,"The generated code correctly loads the dataset, identifies column data types, calculates descriptive statistics for the specified numerical columns, and reports value counts for the `HeartDisease` column.

**Strengths:**
*   **Correctness:** All key requirements are met.
*   **Conciseness:** The code is straight to the point and avoids unnecessary verbosity.
*   **Pandas Usage:** Utilizes appropriate pandas functions like `read_csv`, `dtypes`, `describe`, `loc`, `rename`, and `value_counts` effectively.

**Areas for Improvement (Why it's not a 10):**
*   **Column Names Listing:** The task explicitly asked to ""List all column names AND identify the data type of each column."" While `print(df.dtypes)` shows data types (which inherently lists column names next to them), it doesn't explicitly print the column names formatted as ""Column Names: - col1, - col2..."" as requested and shown in the ground truth. This is a minor stylistic missing piece.
*   **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and general exceptions during CSV loading. The generated code lacks this, which is crucial for robust applications. If `heart.csv` is not present, the script will crash.
*   **Readability/Clarity of Output:** While the output itself is correct, the ground truth adds print statements and headers (e.g., ""--- Column Names and Data Types ---"", ""--- Descriptive Statistics for Numerical Columns ---"") which significantly improve the readability and understanding of the output for the user. The generated code just prints the raw outputs, which can be less clear.
*   **Missing Interpretation for HeartDisease:** The ground truth includes a helpful interpretation for the `HeartDisease` column's value counts (0 = No Heart Disease, 1 = Heart Disease). This was missed in the generated code.

Overall, it's a functionally correct solution for the core data processing, but could be improved significantly by adding error handling, better output formatting, and a more explicit printing of column names as requested.",gpt_4o_mini,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# Filter the dataset
filtered_df = df[(df['Age'] > 50) & (df['HeartDisease'] == 1)]

# Select and display specific columns
result = filtered_df[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]
print(result)
```",9,"The generated code correctly performs all the required tasks: loading the CSV, filtering based on Age > 50 and HeartDisease == 1, and selecting/displaying the specified columns. It's concise and directly addresses the prompt.

**Strong points:**
*   Correctly loads the CSV using `pandas.read_csv`.
*   Applies the exact filtering conditions (`Age > 50` AND `HeartDisease == 1`).
*   Accurately selects the required columns (`Age`, `Sex`, `ChestPainType`, `Cholesterol`).
*   Displays the final result.
*   Includes the necessary `import pandas as pd`.

**Areas for improvement (minor):**
*   **Error Handling:** The code lacks robust error handling for `FileNotFoundError` (if `heart.csv` doesn't exist) or `KeyError` (if a specified column is missing). While the prompt didn't explicitly ask for error handling, it's good practice in real-world scenarios.
*   **Encapsulation/Modularity:** The code is a series of global statements. Encapsulating it within a function (as in the ground truth) would make it more reusable, testable, and organized, especially for larger scripts. Given the specific request for ""Provide only the Python code without explanations,"" a simple script is acceptable, but a function would generally be preferred for ""production-ready"" code.

Overall, it's an excellent solution that meets the core requirements of the task. The minor points are about best practices rather than direct failures to meet the prompt's explicit requirements.",gpt_4o_mini,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Filter the wines based on the specified conditions
filtered_wines = data[(data['alcohol'] > 12) & (data['pH'] < 3.3)]

# Identify the wine with the highest quality score
highest_quality_wine = filtered_wines.loc[filtered_wines['quality'].idxmax()]

# Display all features of this specific wine
print(highest_quality_wine)
```",9,"The generated code correctly implements all the core requirements of the task: filtering with compound conditions, finding the maximum quality in the subset, and displaying the features of the selected wine. It uses appropriate `pandas` functions (`read_csv`, boolean indexing, `idxmax`, `loc`). The code is concise and directly addresses the problem.

**Strengths:**
*   **Correctness:** All three main requirements are met accurately.
*   **Conciseness:** The solution is compact and efficient.
*   **Proper Imports:** `pandas` is correctly imported.

**Areas for Improvement (minor, not critical for core functionality):**
*   **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError`, `KeyError`, and general exceptions. The generated code lacks this, which is crucial for real-world applications where the file might not exist or the columns might be misspelled.
*   **Edge Case Handling:** The generated code doesn't explicitly check if `filtered_wines` is empty. If no wines meet the criteria, `idxmax()` on an empty Series would raise an error. The ground truth handles this gracefully.
*   **Docstrings/Comments:** While not explicitly requested in the prompt, good code typically includes comments or docstrings for clarity, especially in more complex functions.
*   **Function Encapsulation:** Encapsulating the logic within a function (as in the ground truth) would make the code more reusable and testable.
*   **Output Formatting:** The ground truth uses `to_string()` for displaying the final wine, which is better for ensuring all features are shown without truncation in larger dataframes. `print(highest_quality_wine)` might truncate output in some environments.

Overall, it's a very good attempt that nails the core logic. The improvements are primarily related to robustness and best practices rather than core problem-solving capability.",gpt_4o_mini,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('heart.csv')

# Plotting the histogram for Age column
plt.figure(figsize=(10, 5))
plt.hist(data['Age'], bins=20, color='skyblue', edgecolor='black')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.grid(axis='y')
plt.show()

# Plotting the bar chart for Sex column
plt.figure(figsize=(10, 5))
sns.countplot(x='Sex', data=data, palette='pastel')
pl...",7,"The generated code correctly loads the dataset and generates both required plots. The histogram for Age is well-labeled, and the bar chart for Sex also has appropriate labels. However, there are a few areas for improvement. The bar chart for Sex could benefit from mapping the numerical '0' and '1' to 'Female' and 'Male' respectively, as done in the ground truth, for better readability. Also, the error handling for `FileNotFoundError` and other potential issues is missing, which is present in the ground truth and makes the code more robust. The ground truth also uses `sns.histplot` which is generally preferred over `plt.hist` when using Seaborn for more aesthetic and informative plots (e.g., KDE).",gpt_4o_mini,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 1. List all column names and identify the data type of each
print(df.dtypes)

# 2. Convert the 'streams' column to a numeric data type, coercing errors to NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# Report any NaNs in the 'streams' column
nan_streams = df['streams'].isna().sum()
print(f'Number of NaN values in ""streams"": {nan_streams}')

# 3. Calculate and report descriptive statist...",7,"The generated code correctly performs most of the required tasks: loading the CSV, converting `streams` to numeric with error handling, reporting NaNs, calculating descriptive statistics for the specified columns, and counting unique artists.

However, there are a few areas for improvement where the generated code is less complete or robust compared to the ground truth:

1.  **Error Handling for File Loading:** The generated code lacks a `try-except` block for `FileNotFoundError` when loading the CSV, which is crucial for robust real-world applications.
2.  **Clarity of Column/Type Reporting:** While `df.dtypes` provides the data types, `df.info()` as used in the ground truth gives a more comprehensive overview including non-null counts, memory usage, and a clear header, which is generally preferred for ""listing all column names and identifying the data type of each.""
3.  **Descriptive Stats Output Clarity:** The `descriptive_stats = df[numerical_columns].describe().T[['mean', '50%', 'std', 'min', 'max']]` is a correct way to get the stats, but the ground truth's approach using `loc` on the original `describe()` output often leads to a slightly more readable table for these specific statistics, especially when transposed like this. Also, the ground truth explicitly mentions that '50%' is the median, which adds clarity.
4.  **Reporting NaNs:** The ground truth provides more detailed output about the `streams` conversion, including initial vs. new dtype and a sample of rows with NaNs if they exist, which aids in debugging and understanding the data transformation.
5.  **Robustness for Missing Columns:** The ground truth includes checks (`if col in df.columns:` and `if 'artist(s)_name' in df.columns:`) to ensure that the specified columns for statistics and unique artist counting actually exist in the DataFrame, preventing `KeyError` if the dataset structure changes. The generated code assumes all columns will be present.

Overall, it's a solid attempt that accomplishes the core tasks, but could be improved in terms of robustness, output clarity, and comprehensiveness of reporting.",gpt_4o_mini,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Filter for songs released in 2023
filtered_df = df[df['released_year'] == 2023]

# Select relevant columns and ensure 'streams' is numeric
result_df = filtered_df[['track_name', 'artist(s)_name', 'streams']]
result_df['streams'] = pd.to_numeric(result_df['streams'], errors='coerce')

# Sort by 'streams' in descending order
result_df = result_df.sort_values(by='streams', ascending=False)

# Display the resul...",9,"The generated code correctly addresses all the requirements: loading the CSV, filtering by `released_year` == 2023, selecting the specified columns, coercing the `streams` column to numeric, and sorting by `streams` in descending order. The code is concise and readable.

However, there's a potential `SettingWithCopyWarning` issue that could arise from the line `result_df['streams'] = pd.to_numeric(result_df['streams'], errors='coerce')`. While it might work in many cases, it's generally safer to use `.loc` for explicit modifications to avoid this warning and ensure modifications are made on a copy rather than a view of the original DataFrame. The ground truth handles this by performing the `to_numeric` call on the original DataFrame or ensuring modifications are made on a derived copy using explicit `.loc` or `.copy()`. The ground truth also includes error handling for `FileNotFoundError`, which is good practice for robustness in real-world scenarios, though not explicitly requested in the problem description for this benchmark. Given the core logic is correct and fulfills all specified requirements, it gets a high score.",gpt_4o_mini,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 1) Generate a histogram for the 'bpm' column
plt.figure(figsize=(10, 6))
plt.hist(df['bpm'], bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of BPM')
plt.xlabel('BPM')
plt.ylabel('Frequency')
plt.grid(axis='y')
plt.show()

# 2) Create a scatter plot for 'danceability_%' vs 'energy_%'
plt.figure(figsize=(10, 6))
plt.scatter(df['danceability_%'], df['energy...",9,"The generated code successfully loads the dataset, generates a histogram for `bpm`, and a scatter plot for `danceability_%` vs `energy_%`. Both plots have titles and appropriately labeled axes, and proper imports are included. This fully addresses all requirements of the task.

Minor points for improvement, not critical for correctness or completeness but for robustness or better adherence to general good practices:
- The `plt.show()` calls between the plots will display the first plot and close it before displaying the second. While this fulfills the 'generate' requirement for both, often in a single script, you'd want all plots to appear simultaneously at the very end of the script (i.e., one `plt.show()` at the very end to display all open figures).
- Error handling for `FileNotFoundError` during CSV loading or `KeyError` for missing columns (as seen in the ground truth) would make the code more robust, though it wasn't explicitly requested in the prompt.
- The axis labels for the scatter plot use `(%)` which is good, but the ground truth was slightly more descriptive, e.g., 'Danceability % (Percentage)'. This is a very minor detail.

Overall, the code is very good and directly solves the task.",gpt_4o_mini,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# 1) List all column names and their inferred data types
print(df.dtypes)

# 2) Clean 'Video views', 'Likes', and 'Dislikes' columns
for col in ['Video views', 'Likes', 'Dislikes']:
    df[col] = df[col].str.replace(',', '').astype(float, errors='coerce').fillna(0)

# 3) Calculate and report basic descriptive statistics
print(df[['Video views', 'Likes', 'Dislikes']].describe())

# 4) Fill missing values ...",8,"The generated code correctly addresses all core requirements of the task.

**Strong Points:**
- **Loading:** Loads the CSV correctly.
- **Data Type Reporting:** `df.dtypes` effectively reports column names and their data types, fulfilling part 1.
- **Numeric Column Cleaning:** The chained operations `str.replace(',', '').astype(float, errors='coerce').fillna(0)` are concise and correctly implement the cleaning, conversion, and NaN filling for 'Video views', 'Likes', and 'Dislikes'. This is a very efficient way to handle this step.
- **Descriptive Statistics:** `df[['Video views', 'Likes', 'Dislikes']].describe()` is a perfect and concise way to get all required descriptive statistics for the specified columns.
- **Category NaN Handling:** `df['Category'].fillna('Unknown')` correctly fills missing values in the 'Category' column.
- **Unique Category Count:** `df['Category'].value_counts()` effectively shows the unique values and their counts, which implies the count of unique values, fulfilling the requirement.

**Areas for Improvement (to reach a 10):**
- **Error Handling for File Loading:** The ground truth includes `try-except FileNotFoundError` for loading the CSV, which is good practice. The generated code lacks this, making it less robust if the file is missing.
- **Explicit Reporting of Unique Categories:** While `value_counts()` implicitly shows the unique count, `df['Category'].nunique()` explicitly gives *only* the count, which was asked for. `value_counts()` provides more information than strictly requested for ""count of unique values"".
- **Clarity of Output for Step 1:** While `df.dtypes` provides the information, `df.info()` as used in the ground truth often provides a more comprehensive and readable summary including non-null counts, which can be very insightful when initially inspecting a dataset.
- **Output Verbosity/Formatting:** The task asked to ""list all column names and their inferred data types"" and ""report the count of unique values"". The ground truth prints clear headers for each step, which improves readability for a benchmark where outputs are evaluated. The generated code prints the `dtypes` and `describe()` outputs directly, which is functional but less explicitly formatted.

Overall, it's a very good solution, demonstrating solid pandas skills and correctly addressing the problem. The suggested improvements are primarily about robustness, minor output exactness, and presentation, rather than functionality.",gpt_4o_mini,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '').astype(float, errors='coerce').fillna(0)

# Filter the dataset for videos with views greater than 50,000,000
filtered_df = df[df['Video views'] > 50000000]

# Select and display the 'Video' and cleaned 'Video views' columns
result_df = filtered_df[['Video', 'Video views']]

# Sort the resulting data by 'Video view...",9,"The generated code correctly addresses all requirements of the task: loading the CSV, cleaning the 'Video views' column (commas removal, type conversion with error handling, NaN filling with 0), filtering by view count, selecting the specified columns, and sorting the output. The approach for cleaning the 'Video views' column in a single line is efficient and elegant, combining string replacement, type conversion with `errors='coerce'`, and `fillna(0)`.

A minor point of improvement, though not critical for correctness, is the absence of `try-except` block for `pd.read_csv`, which is included in the ground truth for robustness against `FileNotFoundError`. While the prompt assumes the file is in the current directory, robust code typically accounts for such scenarios. However, given the prompt's specific instructions, this isn't a critical omission that fails the core task. The code is clean, readable, and directly solves the problem.",gpt_4o_mini,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Likes' and 'Video views' columns
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)

# Create scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(df['Video views'], df['Likes'], alpha=0.5)
plt.title('Video Views vs. Likes')
pl...",9,"The generated code correctly performs all the requested tasks: loading the CSV, cleaning the 'Likes' and 'Video views' columns by removing commas, converting to numeric, coercing errors to NaN, and filling NaNs with 0. It also accurately generates the scatter plot with the specified title and axis labels.

Strengths:
- **Correctness:** All primary requirements of the task are met. The data cleaning logic works as intended.
- **Conciseness:** The cleaning steps for each column are implemented very efficiently in a single line, taking advantage of method chaining.
- **Proper Imports:** Necessary libraries (`pandas`, `matplotlib.pyplot`) are correctly imported.
- **Plotting:** The scatter plot is correctly generated with the specified title, x-axis label, and y-axis label.

Minor area for improvement (not critical for this task, but good practice):
- **Error Handling for File Loading:** The ground truth includes `try-except` blocks for handling `FileNotFoundError` during CSV loading. While the prompt implies the file will be present, including this would make the code more robust in a real-world scenario. However, the prompt did not explicitly ask for error handling, so its omission doesn't detract from the score significantly for this specific task.
- **Consistency in Plotting Parameters:** `alpha` argument in `plt.scatter` could be slightly different (0.5 vs 0.6), but this is a minor stylistic choice and doesn't affect correctness.
- **Figure Size and Tight Layout:** The ground truth includes `plt.figure(figsize=(10, 6))` and `plt.tight_layout()`. While not strictly required by the prompt, these improve readability and presentation of the plot. The generated code does set `figsize`, which is good.",gpt_4o_mini,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the 'Mobile Weight' column
df['Mobile Weight'] = pd.to_numeric(df['Mobile Weight'].str.replace('g', '', regex=True), errors='coerce')

# Clean the 'RAM' column
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', '', regex=True), errors='coerce')

# Clean the 'Battery Capacity' column
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', '', regex=True), errors='coerce')

#...",8,"The generated code successfully accomplishes most of the required tasks. It loads the dataset, cleans the specified columns by removing units and converting them to numeric types (handling errors by coercing to NaN), prints the data types, calculates descriptive statistics for the cleaned columns, and reports the count of unique company names.

Strengths:
- Correctly loads the CSV.
- Successfully cleans all four specified columns (`Mobile Weight`, `RAM`, `Battery Capacity`, `Launched Price (USA)`) by removing units/currency/commas and converting to numeric, correctly using `errors='coerce'`.
- Prints the `dtypes` after cleaning, fulfilling requirement 5.
- Calculates and prints descriptive statistics for the correct columns, fulfilling requirement 6.
- Correctly reports the count of unique values in 'Company Name', fulfilling requirement 7.
- Uses `regex=True` in `str.replace` which is good practice where applicable, though for simple fixed strings like 'g', 'GB', 'mAh', `regex=False` (or omitting it as `False` is default) is also fine and sometimes marginally faster.

Areas for improvement to reach a higher score:
- **Descriptive Statistics Format:** While `describe()` is called, the task specifically asked for ""mean, median, std, min, max"". `describe()` includes count and 25%/75% quartiles which were not explicitly requested. The ground truth demonstrated how to filter `describe()` output (`.loc[['mean', '50%', 'std', 'min', 'max']]`) to match the exact requirement, using '50%' for median. A more precise `describe()` selection or manual calculation for each would have been ideal.
- **Reporting of Unique Company Names:** The generated code simply prints the number `unique_companies_count`. The ground truth provided a more descriptive output (e.g., ""Number of unique companies: X""). While functional, a more user-friendly output is always preferred for reporting.
- **Robustness/Error Handling:** The ground truth included basic error handling for `FileNotFoundError` when loading the CSV, and checks if columns exist before performing operations (though this was less critical for this specific task where column existence is assumed). The generated code assumes the file exists and column names are exact.

Overall, it's a solid solution that correctly performs the core data manipulation and analysis steps. The points for improvement are mostly around output formatting and slightly more robust exception handling/column validation.",gpt_4o_mini,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the 'Launched Price (India)' column
df['Launched Price (India)'] = pd.to_numeric(df['Launched Price (India)'].str.replace('INR ', '').str.replace(',', ''), errors='coerce')

# Clean the 'RAM' column
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')

# Filter the dataset
filtered_df = df[(df['Company Name'] == 'Apple') & (df['Launched Price (India)'] < 100000)]

# Select and d...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
*   **Correctness:** It successfully loads the CSV, cleans both 'Launched Price (India)' and 'RAM' columns by removing specified strings and converting to numeric, handling errors by coercing to NaN. It correctly filters for 'Apple' phones with prices less than 100,000, selects the required columns and sorts them by price in ascending order by default.
*   **Conciseness:** The cleaning steps are performed efficiently in a single line for each column, which demonstrates a good understanding of pandas string methods and `to_numeric`.
*   **Completeness:** It completes steps 1-5 of the task.

**Areas for minor improvement:**
*   **Robustness in cleaning:** While the cleaning logic works for the provided task, the ground truth uses `astype(str)` before string operations. This makes the cleaning more robust to cases where a column might contain non-string types initially, preventing potential `AttributeError` if some entries are not strings. For example, if 'RAM' column had a float `8.0` instead of a string `'8GB'`, `.str.replace()` would fail without prior conversion. For typical CSV files, this might not be an issue if the data types are consistent, but it's a good practice for robustness.
*   **Error handling for file loading:** The generated code lacks explicit error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. This makes the code less robust in a real-world scenario where the file might not exist.
*   **No explicit `regex=False`:** In the 'Launched Price (India)' cleaning, the string replacement methods implicitly use `regex=True` by default in some pandas versions if the pattern could be interpreted as a regex. While 'INR ' and ',' are literal strings and work fine, for robust code and clarity, specifying `regex=False` is often recommended when dealing with literal strings to avoid unintended regex behavior or future conflicts. The ground truth uses it.
*   **No print statements or comments:** Beyond displaying the final result, the code doesn't include any print statements for progress or comments, which can be useful for debugging or understanding the flow, though the prompt asked for ""only the Python code without explanations"" so this was not explicitly required, but the ground truth example shows detailed output and comments which is good practice for a benchmark.

Overall, it's a very good solution that achieves the core task effectively and efficiently within pandas.",gpt_4o_mini,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the 'RAM' column
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', '', regex=False), errors='coerce')

# Clean the 'Battery Capacity' column
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', '', regex=False), errors='coerce')

# Create a scatter plot
plt.figure(figsize=(10, 6))
scatter = plt.scatter(df['RAM'], df['Battery Capacity'], c...",7,"The generated code correctly loads the dataset and implements the cleaning steps for 'RAM' and 'Battery Capacity' columns, handling errors by coercing to NaN as requested. It also creates a scatter plot with a title and axis labels.

However, there are a few areas for improvement:
1.  **Coloring by Company Name**: While it attempts to color by `Company Name` using `cat.codes` and a colorbar, this approach is less intuitive and harder to interpret than using `seaborn.scatterplot` with the `hue` argument, which automatically handles categorical legends. The current approach also doesn't map company names to the colorbar ticks, making the colorbar values meaningless without manual lookup. The ground truth leverages Seaborn effectively for this task.
2.  **`Company Name` Cleaning**: The task implicitly asks for proper visualization based on company name. The generated code uses `cat.codes` which is a numerical representation, but the plot's legend (via colorbar) doesn't clearly map these codes back to actual company names, making the color coding hard to interpret. Seaborn's `hue` parameter automatically creates a meaningful legend.
3.  **Missing `seaborn` import**: Although `matplotlib.pyplot` is sufficient for a basic scatter plot, the task description specifically mentioned `matplotlib.pyplot or seaborn`, and seaborn often offers more convenient and aesthetically pleasing ways to handle categorical data in plots, as demonstrated in the ground truth.
4.  **No error handling for file loading**: The generated code assumes the CSV file exists, which is a minor point but something the ground truth handles gracefully.

Overall, it's a good attempt, but the visualization aspect for categorical data could be significantly improved for clarity and interpretability.",gpt_4o_mini,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 5 rows
titanic_df = pd.read_csv('titanic.csv')
print(titanic_df.head())

# 2) Show the shape of the dataset (rows and columns)
print(""Shape of the dataset:"", titanic_df.shape)

# 3) List all column names and their data types
print(""Column names and data types:"")
print(titanic_df.dtypes)

# 4) Calculate the survival rate (percentage of passengers who survived)
survival_rate = titanic_df['Survived'].mean() * 100
print(f""Sur...",9,"The generated code correctly implements all specified tasks for exploratory data analysis on the titanic dataset. It properly loads the data, displays the head, shows the shape and data types, calculates the survival rate accurately using `mean()`, and counts missing values.

Strengths:
- **Correctness:** All tasks are executed correctly.
- **Conciseness:** The calculation of survival rate using `df['Survived'].mean()` is very concise and accurate for a 0/1 encoded column.
- **Readability:** The code is well-structured and easy to understand.
- **Completeness:** All requirements from the prompt are met.

Areas for minor improvement (not critical, hence the high score):
- **User-friendly output:** The ground truth adds more descriptive print statements (e.g., ""1. First 5 rows of the dataset:""). While not strictly required by the prompt ""Provide only the Python code without explanations,"" adding these brief descriptive prints can enhance the user experience when running the script for exploratory analysis.
- **Error Handling:** The ground truth includes `try-except FileNotFoundError`, which is good practice for robust code, though the prompt didn't explicitly ask for error handling. For a simple EDA script, this might be considered overkill, but for production-grade code, it's a plus.",gpt_4o_mini,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 1) List all column names and their data types
print(df.dtypes)

# 2) Calculate and report basic descriptive statistics for numerical columns
numerical_columns = ['Year', 'Rating', 'Runtime']
descriptive_stats = df[numerical_columns].describe().loc[['mean', '50%', 'std', 'min', 'max']]
descriptive_stats.rename(index={'50%': 'median'}, inplace=True)
print(descriptive_stats)

# 3) Report the total number of mo...",6,"The generated code correctly performs the core tasks of loading the CSV, displaying column types, calculating descriptive statistics for the specified numerical columns, and reporting the total number of movies. It's concise and directly addresses most requirements.

However, it has significant weaknesses in error handling and data type robustness, which are crucial for real-world data science tasks.

**Areas for Improvement:**
1.  **Robustness for `Runtime` Column:** The ground truth code correctly anticipates that the 'Runtime' column might be a string (e.g., '142 min') and includes preprocessing to convert it to a numeric type. The generated code assumes 'Runtime' is already numeric, which is often not the case for this specific dataset and could lead to a `TypeError` if the column is detected as an object type when `describe()` is called, or at least `NaN`s for statistics on that column if `pd.to_numeric` is used. If `Runtime` column is not numeric, the `describe()` call for the `numerical_columns` will fail or produce incorrect output.
2.  **Error Handling (File Not Found):** The generated code lacks a `try-except` block for `FileNotFoundError` when loading the CSV. This is a critical omission for robust code, as it would crash if the file is not present.
3.  **Clarity for `df.dtypes`:** While `print(df.dtypes)` does show data types, `df.info()` as used in the ground truth provides a more comprehensive and readable summary including non-null counts and memory usage, which is generally preferred for quick dataset overview.
4.  **Assumptions on Column Existence:** The generated code assumes that 'Year', 'Rating', and 'Runtime' columns will always exist and be in a suitable format. The ground truth includes checks to ensure columns exist before performing operations and adapts the list of columns for stats if one is missing.

These improvements prevent common runtime errors and make the code more resilient across different data states.",gpt_4o_mini,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Filter the dataset for movies with Rating >= 9.0
filtered_df = df[df['Rating'] >= 9.0]

# Select and display the Title and Rating columns
result = filtered_df[['Title', 'Rating']]

# Sort the resulting data by Rating in descending order
sorted_result = result.sort_values(by='Rating', ascending=False)

# Display the result
print(sorted_result)
```",8,"The generated code correctly implements all specified task requirements: it loads the CSV, filters by rating, selects the correct columns, and sorts the data in descending order. It includes the necessary import. The code is concise and easy to understand.

Areas for improvement (which would lead to a higher score closer to 10) include:
1.  **Error Handling**: The ground truth includes robust error handling for `FileNotFoundError`, `KeyError`, and general exceptions. The generated code lacks this, making it less robust in a real-world scenario where the file might be missing or columns might be misnamed.
2.  **Constant for Path**: While not strictly a functional requirement, defining the dataset path as a constant (`DATASET_PATH = 'imdb_top_250.csv'`) in the ground truth is a good practice for maintainability, especially if the path needs to be used multiple times or changed later.
3.  **Comments/Clarity**: The ground truth adds more descriptive comments explaining each step (e.g., ""This creates a boolean mask..."", ""The 'ascending=False' argument ensures descending order""). While the generated code is clear enough for this simple task, more comprehensive comments are beneficial for complex scripts or team environments.
4.  **.copy() usage**: The ground truth includes a `.copy()` after `sort_values` to explicitly avoid `SettingWithCopyWarning`, which is a good defensive programming practice, although not strictly necessary for this specific final display step.",gpt_4o_mini,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create a histogram for the Rating column
plt.figure(figsize=(10, 5))
plt.hist(df['Rating'], bins=20, color='blue', alpha=0.7)
plt.title('Distribution of IMDB Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.grid(axis='y')
plt.show()

# Create a scatter plot for Rating vs. Runtime
plt.figure(figsize=(10, 5))
plt.scatter(df['Runtime'], df['Rating...",7,"The generated code correctly addresses most of the task requirements. It successfully loads the CSV, creates a histogram for 'Rating', and a scatter plot for 'Rating' vs. 'Runtime', both with appropriate titles and axis labels. Imports are correct.

However, a significant issue is the lack of robust handling for the 'Runtime' column. The `imdb_top_250.csv` dataset often has 'Runtime' as a string (e.g., ""120 min""). The generated code directly tries to plot `df['Runtime']` which will likely cause a `TypeError` because it expects numeric data. The ground truth code includes robust preprocessing to extract numeric values from the 'Runtime' string, which is crucial for this specific dataset and a common challenge in real-world data.

Areas for improvement:
1.  **Robust 'Runtime' Column Handling:** The most critical improvement is to preprocess the 'Runtime' column to extract numerical values. This often involves string manipulation (e.g., using `.str.extract()`) and conversion to numeric types, handling potential `ValueError` or `TypeError` if the column is not purely numeric.
2.  **Error Handling for File Not Found:** While not explicitly requested in the prompt, a `try-except` block for `pd.read_csv` would make the code more robust against missing files, as shown in the ground truth.
3.  **Consistency in Plotting Library:** While both `matplotlib` and `seaborn` are allowed, the ground truth uses `seaborn.histplot` which is generally more aesthetically pleasing and offers more features (like KDE) for histograms, which could be a minor aesthetic improvement.",gpt_4o_mini,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# 1) Report the number of missing values (NA) for each column
missing_values = df.isna().sum()
print(""Missing values for each column:"")
print(missing_values)

# 2) Drop rows with missing values in specific numerical columns
numerical_columns = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_cleaned = df.dropna(subset=numerical_columns)

# Calculate descriptive statistics for the cle...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, reports missing values, correctly filters the numerical columns for NA values before calculating descriptive statistics (mean, median, std, min, max), and accurately counts unique values for 'species' and 'island'.

**Strengths:**
*   **Correctness:** All parts of the task are functionally correct.
*   **Conciseness:** The code is more concise than the ground truth while still achieving the desired results.
*   **Proper Filtering:** The `dropna(subset=numerical_columns)` is correctly used to filter rows only based on NAs in the specified columns.
*   **Descriptive Statistics:** `describe().T` followed by column selection and renaming (`'50%'` to `'median'`) is an efficient way to get the requested statistics.

**Minor Areas for Improvement (for an even higher score or robustness):**
*   **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` when loading the CSV, which is good practice for more robust code. The generated code assumes the file will always be present.
*   **Output Clarity:** While the output is correct, adding more descriptive print statements (e.g., ""--- Missing Values Report ---"" as in the ground truth) could enhance readability for the user.
*   **Column Existence Checks:** For `species` and `island`, the ground truth checks if the columns exist before attempting `nunique()`. While highly unlikely for this specific dataset, robust code might include such checks for general applicability. In this context, it's less critical.

Overall, it's a very good solution that successfully completes the task. The minor improvements are mostly about robustness and output formatting rather than core functionality errors.",gpt_4o_mini,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the dataset
filtered_df = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')]

# Select specific columns
selected_columns = filtered_df[['culmen_length_mm', 'culmen_depth_mm']]

# Drop rows with missing values
cleaned_df = selected_columns.dropna()

# Display the first 5 rows of the resulting DataFrame
print(cleaned_df.head())
```",9,"The generated code correctly performs all the required steps: loading the data, filtering by species and island, selecting the specified columns, dropping rows with NaNs in those columns, and displaying the first 5 rows. The imports are correct, and the code structure is clean and easy to follow.

The only minor area for improvement, preventing a perfect 10, is the `dropna()` call. While `selected_columns.dropna()` works correctly in this specific scenario because `selected_columns` *only* contains the two columns relevant for dropping NaNs, the ground truth's explicit `dropna(subset=['culmen_length_mm', 'culmen_depth_mm'])` is slightly more robust and explicit. If `selected_columns` happened to contain other columns (which it doesn't in this case, but could in a more complex scenario), `dropna()` without a subset would drop rows if *any* column had a NaN, whereas the requirement was specifically about `culmen_length_mm` and `culmen_depth_mm`. However, given the prior column selection, the effect is the same.

Additionally, the ground truth includes error handling for `FileNotFoundError`, which is good practice for production-ready code, although not explicitly requested for this specific benchmark task. The prompt did not ask for robust error handling, so its absence does not detract from the score for this specific task.",gpt_4o_mini,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
penguins = pd.read_csv('penguins.csv')

# Filter the dataset for non-missing values
filtered_penguins = penguins.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])

# Create the scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(data=filtered_penguins, x='culmen_length_mm', y='culmen_depth_mm', hue='species')

# Set the title and labels
plt.title('Scatter Plot of Culmen Le...",9,"The generated code correctly loads the dataset, filters out rows with missing values in `culmen_length_mm`, `culmen_depth_mm`, and `species`, and generates the specified scatter plot. It includes a title and axis labels as requested. The imports are proper.

**Strong Points:**
*   Correctly uses `pd.read_csv` to load the data.
*   Correctly uses `dropna(subset=...)` to filter missing values for the specified columns.
*   Correctly uses `seaborn.scatterplot` with `hue='species'` to color points by species.
*   Includes a title and axis labels using `plt.title`, `plt.xlabel`, and `plt.ylabel`.
*   Includes `plt.show()` to display the plot.

**Minor Improvement:**
*   The ground truth also handled `FileNotFoundError` and other general exceptions during CSV loading, which is good practice for robust code. The generated code assumes the file exists. While the prompt implied the file is there, robust production-level code would include this. However, for a coding task like this, it's often not strictly required unless specified.
*   The ground truth wrapped the logic in a function and used an `if __name__ == '__main__':` block, which is better for reusability and script execution. The generated code is more of a direct script. For a benchmark, a direct script is acceptable if it meets the core requirements.",gpt_4o_mini,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Step 1: Read the dataset
df = pd.read_csv('penguins.csv')

# Step 2: Fill NA values with the mean of their respective columns
df['culmen_length_mm'].fillna(df['culmen_length_mm'].mean(), inplace=True)
df['culmen_depth_mm'].fillna(df['culmen_depth_mm'].mean(), inplace=True)

# Step 3: Calculate and print mean, median, and standard deviation
mean_culmen_length = df['culmen_length_mm'].mean()
median_culmen_length = df['culmen_length_mm'].median()
std_culmen_length =...",9,"The generated code correctly addresses all requirements outlined in the task: it loads the dataset, fills NA values in the specified columns with their respective means, calculates and prints the mean, median, and standard deviation for the cleaned columns, and prints the total number of rows and columns.

**Strong points:**
*   **Correctness:** All calculations and data manipulations are accurate.
*   **Completeness:** All four steps outlined in the task are fully implemented.
*   **Conciseness:** The code is quite concise while remaining readable.
*   **Directness:** It directly addresses the problem without unnecessary complexity.

**Areas for minor improvement (preventing a perfect 10):**
*   **Hardcoding Column Names:** The column names (`culmen_length_mm` and `culmen_depth_mm`) are hardcoded multiple times. While acceptable for a small, fixed task, using a list of columns (as shown in the ground truth) would make the code more maintainable and scalable if more columns needed similar processing. This allows for iteration rather than repeated lines of code.
*   **Error Handling:** There's no error handling for `FileNotFoundError` when reading the CSV, which is a good practice for robust code.
*   **Output Formatting:** The output for statistics is on a single line per column, which is functional but less readable than the multi-line, formatted output of the ground truth (e.g., separating mean, median, std dev on different lines with consistent decimal formatting).

Overall, it's a very good solution that successfully completes the task. The suggested improvements are primarily about robustness, maintainability, and presentation, rather than core functionality being incorrect.",gpt_4o_mini,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows where flipper_length_mm is NA
df = df.dropna(subset=['flipper_length_mm'])

# Create a histogram of the flipper_length_mm column
plt.figure(figsize=(10, 6))
plt.hist(df['flipper_length_mm'], bins=20, color='skyblue', edgecolor='black')
plt.title('Distribution of Flipper Length (mm)')
plt.xlabel('Flipper Length (mm)')
plt.ylabel('Frequency')
plt.gri...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, handles NA values in `flipper_length_mm` by dropping rows, creates a correctly labeled histogram for `flipper_length_mm`, and generates a labeled bar chart for `island` counts.

Strong points:
- Correctly uses pandas for data loading and manipulation.
- Appropriately uses matplotlib for the histogram and seaborn for the bar chart.
- All required titles and axis labels are present and correct.
- The use of `dropna(subset=[...])` is correct for handling NAs in a specific column.
- Includes `plt.show()` after each plot, which is good practice.

Areas for minor improvement (to achieve a perfect 10):
- The histogram's y-axis label is 'Frequency' in the generated code, while the ground truth uses 'Count'. Both are acceptable, but 'Count' is often more intuitive for a basic histogram.
- The generated code doesn't include error handling for `FileNotFoundError` when loading the CSV, which could make the code more robust.
- The ground truth code uses `sns.histplot` which is generally preferred over `plt.hist` when Seaborn is imported, as it integrates better with Seaborn's styling and offers more features (like KDE). However, `plt.hist` is perfectly valid.
- The ground truth includes `sns.set_theme` for consistent styling, which is a nice touch for professional plots but not strictly required by the prompt.",gpt_4o_mini,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Filter the data to include only penguins from the 'Torgersen' island
torgersen_penguins = df[df['island'] == 'Torgersen']

# 3) Count and print the number of penguins belonging to each species
species_count = torgersen_penguins['species'].value_counts()
print(species_count)
```",8,"The generated code correctly solves the task by loading the dataset, filtering for 'Torgersen' island, and counting species. It is concise and directly addresses all requirements.

**Strong Points:**
*   **Correctness:** The code produces the correct output as per the task requirements.
*   **Conciseness:** It achieves the goal with minimal and clear code.
*   **Readability:** The code is easy to understand.

**Areas for Improvement (why not a 10):**
*   **Error Handling:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, or for cases where columns like 'island' or 'species' might be missing. The ground truth includes robust error handling, which is crucial for real-world applications.
*   **Edge Cases:** While not strictly part of the core output, not checking if `torgersen_penguins` is empty before attempting to count species (though highly unlikely for this specific dataset) could lead to less graceful behavior in different data contexts.
*   **Informative Output:** The ground truth provides more informative print statements describing each step, which can be useful for debugging or understanding the code's flow, especially in a benchmark context. For this specific prompt, the generated code satisfies the ""print"" requirement but without additional context.

Overall, it's a very good solution that directly tackles the problem, but it could be more robust and user-friendly with added error checks and descriptive outputs.",gpt_4o_mini,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

# Print the shape of the DataFrame
print(df.shape)

# Li...",10,"The generated code is correct, complete, and of good quality. It precisely follows all instructions: it correctly defines the DataFrame, accurately prints its shape, and correctly lists the data types of all columns. The imports are proper, and the code structure is clean and efficient. There are no unnecessary elements, and all requirements from the prompt are met perfectly.",gpt_4o_mini,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
titanic_df = pd.read_csv('titanic.csv')

# 1) Bar chart showing the count of survivors vs non-survivors
survival_counts = titanic_df['Survived'].value_counts()
plt.figure(figsize=(8, 6))
survival_counts.plot(kind='bar')
plt.title('Count of Survivors vs Non-Survivors')
plt.xlabel('Survival Status (0=No, 1=Yes)')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

# 2) Bar chart showing the distribution of passeng...",7,"The generated code correctly creates four distinct plots with appropriate titles and labels for each, fulfilling a major part of the task. Imports are correct, and the structure is logical.

However, there are a few areas for improvement:
- **Missing Age Value Handling:** The generated code uses `dropna()` for age, which removes rows with missing age values. While this *handles* missing values, a more common and often preferred approach for distribution visualization is to impute them (e.g., with median or mean), preserving more data. The ground truth code correctly imputes the median, which is a better demonstration of ""handling missing values appropriately"" in this context.
- **Axis Labels for Survival Plot:** The x-axis label for the survival plot (`Survival Status (0=No, 1=Yes)`) is functional but less clear than mapping the numerical values to descriptive labels like ""Non-survivor"" and ""Survivor"", as done in the ground truth. This improves readability for a general audience.
- **Error Handling:** The generated code lacks error handling for `FileNotFoundError` if 'titanic.csv' is not present. While not explicitly requested in the prompt's output format, it's a good practice, as shown in the ground truth.
- **Clarity of `Pclass` Plot:** Similar to the survival plot, the pclass plot uses numerical labels. While acceptable, mapping them to '1st Class', '2nd Class', '3rd Class' would enhance readability. Though the prompt didn't explicitly ask for this, it's a detail that adds polish.

Overall, it's a good attempt, but attention to the nuances of data handling (imputation vs. dropping) and plot readability would significantly improve it.",gpt_4o_mini,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]
}

df = pd.DataFrame(data)

# Calculate and print the average Midterm_Score
av...",10,"The generated code correctly creates the DataFrame as specified, accurately calculates and prints both the average Midterm_Score and average Final_Score for all students, and correctly identifies and prints the 'Subject' with the highest average 'Final_Score'. It uses standard pandas operations efficiently and includes the necessary import. The output format for scores is slightly different (no formatting to two decimal places), but this does not impact correctness or completeness of the required calculations, matching the ground truth's core logic. The code is clean, well-structured, and directly addresses all parts of the task.",gpt_4o_mini,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

# Creating the DataFrame
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
    'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
    'Volume': [100000...",10,"The generated code is correct, complete, and follows good code quality practices. It successfully creates the DataFrame, sets the index, calculates the 'Daily_Range' column, identifies the date with the highest 'Close' price, and prints the updated DataFrame as required by the task. All necessary imports are included, and the code is structured logically. It correctly uses `idxmax()` to find the date with the highest close price and calculates the new column accurately. The output format for the highest close date is also acceptable, even if it's not explicitly formatted like in the ground truth, it still provides the correct date.",gpt_4o_mini,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics'] * 7,
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST001']
}

df = pd.DataFrame(data)

# Calculate Total Revenue
df['Total_Revenue']...",7,"The generated code correctly creates the DataFrame, calculates the 'Total_Revenue' column, and prints the first 5 rows of the DataFrame. However, there's a crucial error in how it identifies the ""Product sold the most in terms of Quantity"".

The line `most_sold_product = df.loc[df['Quantity'].idxmax(), 'Product']` finds the product associated with the *single highest quantity in a single transaction*, not the product that has the highest *cumulative quantity sold across all transactions*. For example, if 'Laptop' was sold in quantities of (1, 1) and 'Mouse' in a quantity of (2), this code would incorrectly identify 'Mouse' as the most sold because one of its transactions had a higher quantity (2) than any single Laptop transaction (1). The ground truth correctly `groupby('Product')['Quantity'].sum()` to aggregate quantities per product before finding the maximum.

To improve, the generated code should adapt the grouping and summing approach used in the ground truth to accurately determine the best-selling product by total quantity. Also, the `print(df.head())` without specifying `(5)` still achieves the correct output for 5 rows but explicitly passing `5` is generally clearer.",gpt_4o_mini,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}

df = pd.DataFrame(data)

# Calculate and print the average ""SessionDurationMinutes""
average_session_duration = df[""SessionDurationMinutes""].mean()
prin...",9,"The generated code correctly defines the DataFrame, calculates the average session duration, and counts users by device type, fulfilling all requirements of the task. The imports are correct, and the structure is clear and logical.

**Strong points:**
*   **Correctness:** Both calculations (average session duration and device type counts) are performed accurately using appropriate pandas methods (`.mean()` and `.value_counts()`).
*   **Completeness:** All parts of the task are addressed.
*   **Code Quality:** The code is clean, readable, and directly implements the required operations.

**Areas for minor improvement (to reach a perfect 10):**
*   **Print statements:** While functional, the formatting of the print statements is slightly less polished than the ground truth. For example, the ground truth explicitly labels the output ""1. Average Session Duration:"" and ""2. Number of users per Device Type:"", and formats the float to two decimal places, which can improve readability and presentability in a benchmark context. The ground truth also adds a separator for clarity. These are minor stylistic choices, but they contribute to a more professional and user-friendly output.

Overall, the generated code is excellent and fully accomplishes the task.",gpt_4o_mini,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

df = pd.DataFrame(data)

# 1) Find and print the maximum ""TemperatureCelsius""
max_temperature = df[""Tem...",9,"The generated code successfully defines the DataFrame as specified and accurately finds and prints the maximum ""TemperatureCelsius"" and minimum ""HumidityPercent"". The imports are correct, and the code is well-structured and easy to understand.

The only minor points for improvement, which are primarily stylistic and not impacting correctness or functionality, are:
*   The print statements in the generated code are slightly less descriptive than the ground truth's f-string formatting, which includes units and more explicit labels (e.g., ""Maximum TemperatureCelsius recorded: 22.8°C"").
*   The ground truth includes extensive comments explaining each step, which is good practice for readability and maintainability, though not strictly required by the prompt's output format.

Overall, the generated code fully satisfies all the requirements of the task.",gpt_4o_mini,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# 1) Print the total number of ""Likes"" across all posts.
total_likes = df[""Likes""].sum()
print(""Total Likes:"", total_likes)

# 2)...",10,"The generated code is correct and complete. It accurately defines the DataFrame, calculates the total number of likes, and correctly identifies the PostType with the highest average shares. The code is well-structured and uses appropriate pandas methods for the task. The comments clearly delineate the different parts of the task.",gpt_4o_mini,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",9,"The generated code correctly defines the DataFrame, calculates the average age, and counts patients by gender, fulfilling all requirements of the task. The code is well-structured and uses appropriate pandas methods.

The only minor point for improvement, which is less about correctness and more about presentation, is the average age printing. The ground truth code formats the average age to two decimal places, which is generally good practice for numerical outputs like averages. The generated code prints the raw float. This is a very minor stylistic difference and does not impact functionality.

Overall, an excellent submission.",gpt_4o_mini,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}

df = pd.DataFrame(data)

# 1) Print the ""Title"" of the movie with the highest ""Rating"".
highest_rated_mov...",9,"The generated code correctly defines the DataFrame, identifies the title of the highest-rated movie, and calculates the average release year. It imports pandas correctly and the structure is clear and concise.

Compared to the ground truth, the generated code uses `idxmax()` which is a more direct and often more efficient way to find the index of the maximum value, then `loc` to directly retrieve the 'Title'. This is a perfectly valid and good approach.

The only minor area for improvement that prevents a perfect 10 is the lack of descriptive print statements for the results, which the ground truth includes (e.g., ""The title of the movie with the highest rating is: 'The Dark Knight'""). While not explicitly requested to be descriptive, it generally makes the output more user-friendly and understandable. However, the core logic is sound and efficient.",gpt_4o_mini,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

df = pd.DataFrame(data)

# 1) Print the total number of cars in the dataset
total_cars = df.s...",9,"The generated code correctly defines the DataFrame, accurately calculates and prints the total number of cars, and correctly calculates and prints the average ""PriceUSD"". It addresses all requirements of the task. The code is well-structured and uses appropriate pandas methods.

One minor area for improvement that would bring it closer to the ground truth (and a perfect score) is the formatting of the average price. The ground truth formats the currency to two decimal places, which is good practice for financial figures. While not strictly required by the prompt's output specification, it enhances readability and demonstrates a slightly higher level of polish. However, given the prompt only asked to ""print the average PriceUSD"", the generated code fully satisfies that.",gpt_4o_mini,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}

df = pd.DataFrame(data)

# 1) Print the name of the Pokemon with the highest ""HP...",10,"The generated code is correct, complete, and of high quality. It accurately defines the DataFrame and correctly identifies and prints the names of the Pokemon with the highest HP and lowest Defense using efficient pandas methods (`idxmax()` and `idxmin()`). The import statements are correct, and the code structure is clear and follows the instructions.",gpt_4o_mini,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('titanic.csv')

# Identify columns with missing values
missing_values = df.isnull().sum()
print(""Missing values in each column:\n"", missing_values[missing_values > 0])

# Fill missing Age values with the median age
df['Age'].fillna(df['Age'].median(), inplace=True)

# Fill missing Embarked values with the most frequent port
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# Drop the Cabin column due to excessive mis...",7,"The generated code correctly addresses most of the task requirements: loading data, handling missing 'Age' and 'Embarked' values, dropping the 'Cabin' column, and displaying the shape and head. The `str.extract` method for 'Title' extraction is concise and effective for *some* titles, which is a good use of pandas string methods. However, the regex used for title extraction (`r',\s*(\w+)'`) is not robust enough to capture all common titles (e.g., 'Mr.', 'Mrs.', 'Miss.') accurately as presented in the 'Name' column format in the Titanic dataset. It correctly identifies the word immediately following a comma and space, which often works for cases like ""Braund, Mr. Owen Harris"" returning ""Mr"", but might fail or return incorrect results for more complex names or names without a comma before the title. The ground truth's regex `re.search(' ([A-Za-z]+)\.', name)` is more specific and robust for capturing the desired titles by looking for a space then a word followed by a period. The output in the prompt also asks for ""Mr., Mrs., Miss., Master., etc."" while the generated code only extracts the word itself, not including the period. This discrepancy in title extraction accuracy and robustness is the primary reason for not achieving a higher score. Also, the ground truth includes a `try-except` block for file loading, which is good practice but not explicitly requested.",gpt_4o_mini,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}
df = pd.DataFrame(data)

# 1) Print the ""T...",9,"The generated code correctly defines the DataFrame, identifies the title of the book with the highest price, and calculates the total stock. Imports are correct, and the code structure is logical and easy to follow.

The only minor area for improvement is in the printing of the results. While the code correctly prints the required values, the ground truth uses f-strings with descriptive text, making the output more user-friendly and informative. For example, instead of just `print(highest_price_title)`, `print(f""The title of the book with the highest price is: '{highest_price_title}'"")` provides better context. This is a stylistic preference rather than a correctness issue, hence the high score.",gpt_4o_mini,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Display the first 10 rows
print(df.head(10))

# Show the shape of the dataset
print(""Shape of the dataset:"", df.shape)

# List all column names and their data types
print(""Column names and data types:\n"", df.dtypes)

# Calculate the percentage of potable vs non-potable water samples
potable_count = df['potability'].value_counts(normalize=True) * 100
print(""Percentage of potable vs non-potable water samp...",7,"The generated code addresses most of the requirements correctly and provides a good starting point for EDA.

**Strengths:**
*   Correctly loads the dataset.
*   Displays the first 10 rows.
*   Shows the shape of the dataset.
*   Lists column names and data types (though `df.dtypes` is less comprehensive than `df.info()`).
*   Calculates potability percentages correctly.
*   Displays missing values counts.
*   Shows summary statistics.
*   Uses `normalize=True` for percentages, which is concise.

**Areas for Improvement (Why it didn't get a higher score):**

1.  **Readability/Clarity of Output:** The ground truth code includes clear print statements and separators (`--- 1. First 10 rows...`, `===`) to delineate each section of the output, making it much easier for a user to understand what each printed block represents. The generated code just prints the results without much context.
2.  **Robustness:** The ground truth includes `try-except` blocks for file loading, which is crucial for real-world applications to handle `FileNotFoundError` gracefully. The generated code lacks this.
3.  **Column Names and Data Types:** While `df.dtypes` works, `df.info()` (used in ground truth) provides a more comprehensive summary, including non-null counts, which is very useful for initial data exploration.
4.  **Specific Requirement Detail:** The prompt asked to ""List all column names and their data types"". `df.dtypes` does this, but `df.info()` satisfies this *and more* (non-null counts), which is why it's often preferred in EDA.
5.  **Missing Values Display:** The ground truth specifically filters to `missing_values[missing_values > 0]` and adds a message if no missing values are found, which is more refined than just printing all columns' missing counts (which will show 0s for many columns).
6.  **Case Sensitivity of 'Potability' Column:** The generated code uses `'potability'` (lowercase) while the ground truth accounts for potential case sensitivity by checking if the column exists (`if 'Potability' in df.columns:`). While in this specific dataset 'Potability' is capitalized, making the code more robust against variations is a good practice.",gpt_4o_mini,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Identify columns with missing values
missing_values = df.isnull().sum()

# Fill missing values for numerical columns with the mean
for column in df.select_dtypes(include=['float64', 'int64']).columns:
    df[column].fillna(df[column].mean(), inplace=True)

# Check for and remove duplicate rows
duplicates_before = df.duplicated().sum()
df.drop_duplicates(inplace=True)
duplicates_after = df.duplicated().s...",6,"The generated code addresses most of the requirements but has several shortcomings.

**Strengths:**
*   **Loading and Initial Missing Value Identification:** Correctly loads the dataset and identifies initial missing values.
*   **Missing Value Imputation:** Correctly fills missing numerical values with the mean.
*   **Duplicate Handling:** Correctly identifies and removes duplicate rows.
*   **Final Display:** Correctly displays the first 5 rows of the cleaned dataset.

**Areas for Improvement:**
1.  **Before/After Summary:** The `summary_before` and `summary_after` dictionaries are flawed. `summary_before['Dataset Shape Before']` is taken *after* missing values have been filled, and `summary_after['Dataset Shape After']` is taken *after* range checks, which significantly alters the shape. The missing values count in `summary_after` should ideally verify that all missing values are gone, but simply printing `df.isnull().sum()` might still show columns that *had* missing values but are now 0. It should explicitly show what *was* missing before and what *is* missing after. The `duplicates_before` and `duplicates_after` are calculated correctly but stored in the `summary_after` dictionary, which is slightly inconsistent. The summary should be a more coherent narrative of the cleaning steps and their impact.
2.  **Range Verification Logic:**
    *   **Hardcoded `max_values`:** While a reasonable attempt, it's problematic as some columns (like `ph`) have both a minimum and maximum range (0-14), not just a maximum. Other parameters (like `Hardness`, `Solids`) cannot be negative. The current implementation only checks an upper bound, completely ignoring lower bounds, especially for non-negative physical quantities.
    *   **Arbitrary Max Values:** The `max_values` chosen are heuristic and don't necessarily reflect the actual physical or regulatory limits (e.g., `hardness` 500 mg/L might be too general; `solids` 20000 mg/L is a very high upper bound and might not be typical for *potable* water). The problem statement implies ""reasonable ranges"" rather than just ""max values"".
    *   **Data Loss:** Applying `df = df[df[column] <= max_value]` iteratively for each column can lead to cascading data loss, without any reporting on how many rows were removed at each step. The ground truth handles pH ranges and then other positive parameters separately, reporting removal count.
3.  **Error Handling:** The ground truth includes `try-except FileNotFoundError`, which is good practice for loading files. The generated code lacks this.
4.  **Clarity of Output:** The output for the summary is just a print of dictionaries, which is less user-friendly than the detailed print statements in the ground truth that explain each step (e.g., ""Filling missing values:"", ""Removed X duplicate rows."", etc.).
5.  **Completeness of Checks:** The `ph` range check in the ground truth is `(df['ph'] >= 0) & (df['ph'] <= 14)`, covering both bounds. The generated code only checks `<= 14`. For other numerical columns, the ground truth checks `>= 0`. The generated code only checks upper bounds defined in `max_values`.

**Overall:** The code performs the core cleaning actions but lacks robustness in range validation and clarity in reporting the cleaning process, especially the before/after comparison. The range validation is the weakest point, potentially leading to incorrect data filtering.",gpt_4o_mini,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('water_potability.csv')

# 1) Bar chart showing the distribution of potable vs non-potable water samples
plt.figure(figsize=(8, 6))
sns.countplot(x='potability', data=data, palette='viridis')
plt.title('Distribution of Potable vs Non-Potable Water Samples')
plt.xlabel('Potability (0 = Non-Potable, 1 = Potable)')
plt.ylabel('Number of Samples')
plt.xticks([0, 1], ['Non-Potabl...",8,"The generated code successfully addresses all the requirements of the task by creating four distinct visualizations: a bar chart, histograms, a boxplot, and a correlation heatmap. It uses the correct libraries (pandas, matplotlib, seaborn) and includes proper titles and axis labels.

Strong points:
- All four requested plots are generated correctly.
- Missing values for plotting are handled adequately by `.dropna()` for histograms and `seaborn`'s default behavior for boxplots and correlation matrices.
- Plots generally have appropriate titles and axis labels.
- The use of `plt.tight_layout()` for histograms is good for preventing overlaps.

Areas for improvement (to reach a higher score):
- **Consistency in column names:** The generated code uses `'potability'` and `'ph'` in lowercase, while the ground truth (and likely the CSV column names based on standard practice) uses `'Potability'` and `'pH'` (with initial caps). While Python is case-sensitive and this might lead to a `KeyError` if the CSV columns are indeed capitalized, typically data loading would normalize names. Assuming the CSV uses capitalized names, this would cause an error. The ground truth correctly assumes capitalized names.
- **Improved histogram structuring:** The ground truth code uses `fig, axes = plt.subplots(1, 3, figsize=(18, 6))` which is a more robust and common way to manage multiple subplots, allowing for easier, individual styling of each subplot. The generated code uses `plt.subplot` within a loop, which works but can be less flexible for post-hoc adjustments.
- **Minor labeling improvements:** The ground truth adds more comprehensive labels, e.g., specifying units for 'Hardness (mg/L)' and 'Chloramines (mg/L)', which is good practice in data visualization. Also, the x-tick labels for potability on the bar chart and boxplot are slightly more explicit in the ground truth ('0: Not Potable, 1: Potable').
- **Standard error handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust.
- **Code clarity and organization:** The ground truth breaks down the code into clearly commented sections for each visualization, enhancing readability and maintainability. It also sets a global `sns.set_style(""whitegrid"")` for consistent aesthetics.

Overall, the generated code performs well and would likely produce the desired plots, given correct column casing in the CSV. The areas for improvement are mostly about robustness, best practices, and minor aesthetic/labeling enhancements.",gpt_4o_mini,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.preprocessing import PolynomialFeatures

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Feature engineering: Create new features
df['alcohol_acidity'] = df['alcohol'] / df['fixed acidity']  # Ratio of alcohol to fixed acidity
df['total_sulfur_dioxide...",6,"The generated code addresses most of the requirements but has several significant issues that prevent it from scoring higher.

**Correctness:**
- **Feature Engineering:** While it creates three new features, two are direct ratios, and one uses `PolynomialFeatures`. However, `PolynomialFeatures` without degree > 1 and `interaction_only=True` on two features only creates their product and the features themselves, which is not truly a novel ""polynomial"" feature typically implying powers greater than 1. The choice of features (`citric acid`, `residual sugar`) might not be the most ""meaningful"" for interaction; the ground truth provides more domain-informed choices. Also, `total_sulfur_dioxide_ratio` has `+ 1` in the denominator, which is an arbitrary constant and not robust against zero values, unlike the ground truth's `EPSILON`.
- **Feature Selection:** RFE and SelectKBest are used correctly in principle. However, the use of `X_train` for fitting the selectors is good practice from a modeling perspective but makes comparing the selected features directly to the original feature set (which includes the engineered features) slightly muddled, especially when trying to get importances for *all* initial features later.
- **Reporting Feature Importances:** This is a major correctness issue. The code attempts to display importance scores for selected features by first fitting a `RandomForestRegressor` on the *entire training set* (`rf.fit(X_train, y_train)`), then taking its `feature_importances_`. This approach essentially calculates the importance of all original and engineered features, *not* just the ones selected by RFE or SelectKBest, within their respective selected subsets. Crucially, the scores reported for `final_features_rfe` and `final_features_kbest` are the global importances from the `rf` model trained on *all* features, not the importances *within the context of the features selected by RFE or KBest*. This means the reported importances/rankings for the selected features are incorrect for the specific feature selection method. The ground truth correctly retrains the estimator on the RFE-selected features to get their relevant importances. For SelectKBest, the scores are already provided by the `score_func`.
- **Handling NaNs/Infs:** The generated code does not handle potential NaN/Inf values that could be introduced during feature engineering (e.g., division by zero). The ground truth specifically adds an EPSILON and handles NaNs.

**Code Quality:**
- **Imports:** Correct and necessary imports are present.
- **Structure:** The code is logically structured, moving from data loading to feature engineering and then feature selection.
- **Readability:** Generally readable. Print statements are clear.
- **Constants:** No constants defined for feature selection count or epsilon, making it less flexible.

**Completeness:**
- **At least three new meaningful features:** Achieved, but the ""meaningfulness"" could be improved (see above).
- **At least two different feature selection techniques:** Achieved (RFE and SelectKBest).
- **Identify top 5-7 most important features:** Achieved by setting `k=7`.
- **Compare and contrast:** The code prints the lists of selected features but lacks detailed comparison or contrasting explanations like the ground truth does. The comparison is implicit.
- **Display final selected features with importance scores/rankings:** This part is largely incorrect due to how importances are calculated and presented (as detailed above). The ground truth correctly obtains importance/scores specific to each selection method.
- **Proper imports (pandas, sklearn, numpy):** Achieved.
- **Assume CSV in current directory:** Achieved, implicitly by `pd.read_csv`.

**Areas for Improvement:**
1.  **Correctly calculate and display importance scores for selected features:** For RFE, retrain the `RandomForestRegressor` on *only* the RFE-selected features, then take `feature_importances_`. For SelectKBest, directly use the `selector_kbest.scores_` for the selected features.
2.  **Enhance feature engineering:** Consider more domain-specific or insightful interactions/polynomials. Implement robust handling of potential division by zero (e.g., small epsilon) and NaNs for engineered features.
3.  **Explicit comparison:** Add structured print statements to compare common and unique features, similar to the ground truth, along with brief explanations of why different methods might yield different results.
4.  **Consider scaling:** While RFE with RandomForest and SelectKBest with f_regression are less sensitive to scaling compared to other methods, for a production-ready feature selection pipeline, scaling the `X_train` data is a good practice, especially if the subsequent models are sensitive to feature scales. The ground truth does this.",gpt_4o_mini,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
...",6,"The generated code attempts to address all requirements but falls short in completeness and robustness, leading to some correctness issues.

**Strong Points:**
*   **Multiple Ensemble Methods:** Implementation of Random Forest, Gradient Boosting, XGBoost, and AdaBoost.
*   **Cross-validation:** Uses `cross_val_score` for individual models.
*   **Voting Ensemble:** Implements a VotingRegressor.
*   **Hyperparameter Tuning:** Uses `GridSearchCV` for XGBoost.
*   **Feature Importance:** Attempts to plot feature importances.
*   **Evaluation Metrics:** Calculates MAE, MSE, RMSE, and R² for individual models.
*   **Residual Plots:** Creates a residual plot.

**Areas for Improvement / Missing/Incorrect Aspects:**

*   **Data Preprocessing and Pipelines (Critical):**
    *   **Incomplete Imputation:** `df.dropna(subset=['Global_Sales'], inplace=True)` is used for `Global_Sales`, but `User_Score` and `Critic_Score`/`Critic_Count`/`User_Count` also have missing values that are not properly handled. The `imputer = SimpleImputer(strategy='mean')` for 'Year' is less robust than `SimpleImputer` within pipelines tailored for numerical and categorical features.
    *   **Preprocessing Pipeline Mismatch:** The `preprocessor` is fit on `X` but then `X_processed` is created as a standalone array. Subsequent model training (e.g., `cross_val_score(model, X_train, y_train)`) operates directly on `X_train` without the `preprocessor` being integrated into a `Pipeline`. This is a significant flaw as the models are trained on unscaled/unencoded data in the cross-validation steps, which will lead to incorrect results. The ground truth correctly uses `Pipeline(steps=[('preprocessor', preprocessor), ('regressor', model)])`.
    *   **Encoding `handle_unknown`:** `OneHotEncoder()` without `handle_unknown='ignore'` can cause errors if new categories appear in test sets not seen during training, which is a common issue in real-world data and cross-validation folds.
*   **Cross-Validation Consistency:**
    *   The code uses `cross_val_score` for individual models and the voting ensemble, but `train_test_split` is used *before* this. `cross_val_score` inherently performs its own splits. This means the `train_test_split` is redundant for the initial cross-validation and only the `X_test` from this initial split is used for final evaluation metrics, which is not true cross-validation of the entire model. The ground truth correctly defines a `KFold` object and uses it for all cross-validation steps, ensuring consistency and proper evaluation.
    *   The evaluation metrics (MAE, MSE, RMSE, R²) are calculated only on the *single* `X_test` split, not as averaged cross-validation scores, which contradicts requirement 4 (""Use cross-validation with 5 folds to evaluate each model""). The results produced for `results` are only RMSE (from `neg_mean_squared_error`), not the full suite of metrics across folds.
*   **Feature Importance Plotting:**
    *   The `plt.xticks` line for feature names `np.array(['']*X_train.shape[1])[indices]` makes the x-axis labels empty, effectively making the feature importance plots useless. The model is trained on `X_processed` which is a NumPy array, so the original column names are lost. Proper handling involves extracting feature names after one-hot encoding, as shown in the ground truth.
    *   AdaBoost feature importances are not directly available unless its base estimator (defaulting to DecisionTreeRegressor) is explicitly set and has `feature_importances_`.
*   **Stacking Ensemble Implementation (Critical):**
    *   The custom `StackingModel` class is a reinvented wheel, missing the robust features of `sklearn.ensemble.StackingRegressor` (e.g., `cv` parameter for generating out-of-fold predictions for the meta-learner, handling of base model fitting). The custom implementation fits base models on the *entire* `X_train` and then uses their predictions on `X_train` to train the meta-model, which is prone to **data leakage**. The meta-model should be trained on *out-of-fold predictions* from the base models to avoid overfitting. The ground truth correctly uses `sklearn.ensemble.StackingRegressor` and its `cv` parameter.
*   **Hyperparameter Tuning Target:** The problem statement asks for tuning ""for the best individual model."" The generated code hardcodes XGBoost for tuning, regardless of which model performed best initially. The ground truth dynamically identifies the best model before tuning.
*   **Output and Readability:** The print statements and overall structure are less detailed and less organized than the ground truth, making the analysis summary less clear. For example, the `results` dictionary only stores RMSE, not all metrics for CV.
*   **Minor Issues:**
    *   `random_state` not consistently used for `train_test_split` (though `cross_val_score` and `KFold` are more relevant here).
    *   `n_jobs=-1` for `XGBRegressor` and `VotingRegressor` is good practice for parallel processing, missing in the generated code.

The most critical failures are the incorrect preprocessing integration with model training (models not properly using the preprocessor in CV and final evaluation), the data leakage in the custom stacking implementation, and the ineffective feature importance plotting.",gpt_4o_mini,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error
import pymc3 as pm
from changepoint.utils import det...",3,"The generated code attempts to address many requirements but falls short significantly in correctness, completeness, and various aspects of code quality, especially when compared to the ground truth.

**Major Issues:**

1.  **Data Loading and Preprocessing:**
    *   `df['Year'] = pd.to_datetime(df['Year'], format='%Y')`: The ground truth uses `Year_of_Release`, which is the correct column name based on common `video_games_sales.csv` datasets. The generated code assumes 'Year', leading to `KeyError` or incorrect data handling.
    *   `annual_sales = df.groupby(df['Year'].dt.year)['Global_Sales'].sum()`: This will fail if 'Year' column doesn't exist or isn't processed correctly. The ground truth correctly uses `Year_of_Release` and converts index to datetime.
    *   **Missing Data Cleaning:** The generated code completely skips handling missing values in `Year_of_Release` or `Global_Sales`, which is crucial for real-world datasets and robust analysis. The ground truth correctly addresses this.

2.  **ARIMA Modeling:**
    *   `seasonal_order=(1, 1, 1, 12)`: For `annual_sales` (yearly data), a monthly seasonality (period=12) makes no sense. This will lead to an incorrect model or an error. The ground truth correctly uses `m=1` and `seasonal=False` or `SARIMAX(..., seasonal_order=(0,0,0,0))` for yearly data.
    *   **No `auto_arima` equivalent:** The ground truth uses `pmdarima.auto_arima` to automatically find the best ARIMA order, which is a standard practical approach. The generated code hardcodes `(1,1,1)`, which is unlikely to be optimal.
    *   **Confidence Intervals:** While `forecast.conf_int()` is called, the output isn't printed or effectively used, diminishing its value.

3.  **Seasonal Decomposition:**
    *   `seasonal_decompose(annual_sales, model='additive')`: Similar to ARIMA, applying seasonal decomposition on yearly data without specifying a reasonable period (e.g., 5-year cycle if there's a suspected business cycle) will default to a 1-year period, resulting in a trivial or misleading decomposition (trend will match original, seasonal will be zeros). The ground truth correctly explains this limitation and tries a logical period.

4.  **Multivariate Regression:**
    *   `df['Year'] = X['Year'].dt.year`: This modifies `df` *after* `get_dummies` and `X, y` split, potentially leading to inconsistent states or `SettingWithCopyWarning`.
    *   **Interaction Term:** `X['Genre_Platform'] = X['Genre_Adventure'] * X['Platform_PS4']` is just one hardcoded example. The ground truth demonstrates a more robust way to incorporate multiple interaction terms, especially within the context of a `statsmodels.formula.api` model, which is much more suitable for complex formulas with categorical variables and interactions.
    *   **Lack of `statsmodels`:** The ground truth correctly uses `statsmodels` for multivariate regression, which natively provides p-values, R-squared, and confidence/prediction intervals, all crucial for statistical modeling. The generated code uses `sklearn.LinearRegression` which lacks these direct statistical outputs.
    *   **Confidence/Prediction Intervals:** There's no implementation for CIs/PIs for the multivariate regression, which was a core requirement.

5.  **Polynomial Regression with Regularization:**
    *   `train_test_split` is used, which is generally not appropriate for time series data. The ground truth correctly uses `TimeSeriesSplit`.
    *   No implementation of confidence or prediction intervals for these models, only prediction. The ground truth correctly points out `sklearn` limitations and provides an appropriate (though approximate) workaround.

6.  **Bayesian Regression:**
    *   **PyMC3 Usage:** The `PyMC3` model is highly simplified and unlikely to train effectively or provide meaningful results on the given data and features. `pm.math.dot(X_train, betas)` assumes `X_train` is purely numerical or has been transformed appropriately, and `betas` handles categorical effects properly. It likely will fail due to mismatched dimensions or type errors, or lack interpretability without proper one-hot encoding for categorical variables before passing to `PyMC3` (or using appropriate PyMC3 types like `pm.Categorical`). The ground truth implements a more robust PyMC3 model with categorical variable handling.
    *   **No Summary/CI/PI:** While `trace` is sampled, there's no call to `az.summary` or any attempt to extract meaningful results like coefficients, CIs, or prediction intervals, which are key benefits of Bayesian modeling.

7.  **Change Point Detection:**
    *   `from changepoint.utils import detect_changes`: This import is highly suspicious. `changepoint` is not a standard, well-maintained library for change point detection in Python. `ruptures` (used by GT) is the industry standard. This will likely result in an `ImportError`. Even if it existed, `detect_changes` is not a robust or standard function call.

8.  **Survival Analysis:**
    *   `df['Release_Year']`: This column likely does not exist based on the dataset structure (it's `Year_of_Release`).
    *   `lifetimes = df['Year'].dt.year - df['Release_Year']`: This calculation is flawed. It computes a difference between current year and release year *for each row* instead of `last_observed_year - release_year` for a given game. The ground truth implements this more carefully by grouping by game name.
    *   `event_observed=df['Global_Sales'] > 0`: This is a very weak proxy for an event. A game being ""profitable"" or ""active"" needs a more specific definition than just having global sales greater than 0, as almost all games would have sales > 0 unless they completely flopped. The ground truth uses `Observed_Event=1` as a simplification for ""observed until a certain year"".
    *   **No Cox PH Model:** The prompt specifically asked for Cox PH model to include covariates (`Genre`, `Platform`, `Publisher`). The generated code only implements Kaplan-Meier.

9.  **Confidence and Prediction Intervals:**
    *   The generated code has a placeholder comment `Assuming 'results' is the fitted ARIMA model` and `pred_conf_int = forecast.conf_int()`, but doesn't *use* or *display* these intervals anywhere. All other models completely lack CI/PI implementation. This is a critical failure of requirement #9.

10. **Time Series Cross-Validation:**
    *   `TimeSeriesSplit` is called, which is good. However, it's only applied to the multivariate regression model (implicitly, as `X_train` and `y_train` are used). It should be applied to *all* time series-relevant models (ARIMA, possibly polynomial regression if sales are yearly).
    *   The implementation is only for `lm` (Linear Regression), not for Ridge/Lasso within the TS-CV loop.

In summary, the code has numerous fundamental issues with data handling, model selection, implementation correctness, and fails to meet many core requirements, especially concerning the proper calculation and display of confidence/prediction intervals and the use of appropriate libraries for specific tasks (like `statsmodels` for regression, `ruptures` for change point detection, `PyMC3` for sophisticated Bayesian models).",gpt_4o_mini,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
from yellowbrick.cluster import SilhouetteVisualizer
from sklearn.metrics import pairwi...",5,"The generated code attempts to address many parts of the prompt but falls short significantly in several critical areas, leading to incorrect or incomplete implementations.

**Primary Reasons for Failure/Critical Missing Elements:**

1.  **Incorrect Data Preprocessing/Feature Engineering:**
    *   `Regional_Sales_Distribution`: The calculation `df[['NA_Sales', 'EU_Sales', 'JP_Sales']].div(df[['NA_Sales', 'EU_Sales', 'JP_Sales']].sum(axis=1), axis=0)` is fundamentally flawed. It only distributes within regional sales, not relating them to `Global_Sales` as implied by the task and implemented in the ground truth. It also misses `Other_Sales`. This is a major misunderstanding of the feature.
    *   `Publisher_Portfolio_Size`: `df.groupby('Publisher')['Publisher'].transform('count')` simply counts the number of games per publisher, not the *unique* games or a measure of portfolio *diversity* as the task implied and ground truth implemented. It's a general count, not a true portfolio size.
    *   Missing handling of `Year_of_Release` in feature engineering.
    *   The `Total_Sales` feature is created, but the task asked for `Global_Sales`, which is already in the dataset.
    *   Only a subset of numerical features (`NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`, `Total_Sales`, `Market_Position', 'Publisher_Portfolio_Size'`) are chosen for scaling/clustering, omitting `Year_of_Release` and the regionally distributed sales proportions, which were explicitly requested as features.
    *   Crucially, **categorical features are not handled at all** during preprocessing for clustering (e.g., OneHotEncoding for 'Genre', 'Platform', 'Publisher'). Clustering on purely numerical features (even incorrectly derived ones) ignores large parts of the dataset's information crucial for segmentation. This is a severe error.

2.  **Clustering Algorithm Implementation Issues:**
    *   **Fixed `n_clusters=5` for all algorithms:** The task explicitly required *determining optimal number of clusters* for KMeans, GMM, and Hierarchical. The code hardcodes `n_clusters=5`, which directly contradicts this requirement.
    *   **DBSCAN:** No attempt is made to determine `eps` and `min_samples` dynamically or via common heuristics. It defaults to fixed values, which rarely work well without tuning.
    *   **Outputting Cluster Labels:** The code calculates labels but doesn't explicitly store them efficiently or consistently for all algorithms in a way that directly supports subsequent steps like consensus clustering or profiling on the *original* dataframe rows. (Minor point, but impacts usability).

3.  **Optimal Cluster Determination - Incomplete/Incorrect:**
    *   Only the Elbow method is applied. Silhouette analysis and Gap Statistic (or similar advanced methods) are completely missing.
    *   The Elbow method implementation plots the result but doesn't programmatically *determine* the optimal k.

4.  **Consensus Clustering - Seriously Flawed:**
    *   The `consensus_clustering` function's logic is incorrect. It calculates a simple co-occurrence count but then uses `AgglomerativeClustering` on this *count matrix* directly. This matrix is not a distance or similarity matrix suitable for direct clustering. It also incorrectly uses `n_clusters=5` again. Consensus clustering typically involves a much more robust approach, often building a similarity matrix (e.g., average pairwise co-occurrence) and then re-clustering on that matrix. The current implementation is unlikely to produce meaningful results.
    *   DBSCAN labels (-1 for noise) are simply included, which will skew the consensus matrix.

5.  **Cluster Profiling and Statistical Testing - Minimal/Incorrect:**
    *   Only `groupby().mean()` is used for numerical features. This is a basic profile but lacks depth.
    *   **No statistical significance testing is implemented for cluster profiling.** This was a specific and important requirement (ANOVA, Chi-squared).

6.  **Anomaly Detection - Overly simplistic and potentially incorrect:**
    *   Using Z-score on `Total_Sales` is a very basic univariate anomaly detection method, not a robust and multivariate anomaly detection technique like Isolation Forest (as suggested by the task and used in ground truth). It only finds outliers based on one feature, overlooking complex patterns.
    *   It also assumes that `Total_Sales` is the only feature sensitive to anomalies.

7.  **Association Rule Mining - Incorrect Data Preparation:**
    *   `game_attributes = df[['Genre', 'Publisher', 'Platform']]` directly feeds categorical columns to `apriori`. `apriori` expects a transactional format, typically one-hot encoded or boolean data. The generated code implicitly assumes `apriori` will handle this, which it won't correctly in a general case. The `game_attributes['count'] = 1` hack is not how association rule mining is typically done, and it will lead to an error or nonsensical results when `apriori` is called on a DataFrame with mixed types in this way.

8.  **Recommendation System - Basic, but Functional (given existing clusters).**

9.  **Validation - Extremely Limited:**
    *   Only one internal validation metric (`silhouette_score`) is calculated, when multiple (Davies-Bouldin, Calinski-Harabasz) were requested.
    *   **External validation metrics are completely missing.**
    *   The dendrogram plot is included but not linked to optimal cluster selection or further analysis. It also uses `linkage(scaled_features, method='ward')` which makes sense for the dendrogram itself, but the Hierarchical Clustering was performed with `AgglomerativeClustering(n_clusters=5)`, which doesn't directly use this linkage object.

**Missing Imports and General Code Quality:**
*   Missing `KneeLocator` for elbow method.
*   Missing `f_oneway`, `chi2_contingency` for statistical testing.
*   Missing `IsolationForest`.
*   Unnecessary imports like `yellowbrick.cluster.SilhouetteVisualizer`, `sklearn.metrics.pairwise_distances` (used twice), `LabelEncoder`, `dendrogram` (imported twice), `zscore`, `IterativeImputer` (not used).
*   The `DBSCAN` implementation with `eps=0.5` and `min_samples=5` often produces no meaningful clusters (many -1 labels) or a single cluster depending on the dataset scale. No handling for that.
*   The input data (`video_games_sales.csv`) usually has `Year_of_Release` as float/object and sometimes missing values. The code attempts to drop nulls for 'Genre' and 'Publisher', but crucial numerical columns like sales might still have issues or benefit from imputation rather than simple dropping if general. The ground truth handles this more robustly.

Overall, the code structure shows an attempt to follow the high-level steps, but the low-level implementation for many key components is either incorrect, incomplete, or uses overly simplistic methods that fail to meet the ""advanced"" requirement of the task.",gpt_4o_mini,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostCla...",6,"The generated code attempts to address most of the requirements but falls short in several aspects, particularly in the implementation details and comprehensiveness compared to the ground truth.

**Correctness:**
- **Feature Engineering:** The `PolynomialFeatures` are used without creating meaningful interaction terms or handling missing values (0s). The ground truth specifically handles 0s as NaNs and imputes them, and creates custom features like BMI_Age_Interaction and Glucose_Insulin_Ratio. The generated code also scales `X_poly` directly, where `X_poly` might contain zeros from the original dataset which would be `NaN`s in the ground truth pre-processing.
- **Ensemble Methods:**
    - **Bagging:** The `create_bagging_classifier` is generic but not demonstrated with different base estimators as explicitly requested (e.g., DT, LR, SVC). The ground truth shows examples.
    - **Boosting:** While the models are instantiated, their usage is not as integrated into the full evaluation pipeline (nested CV) as in the ground truth.
    - **Voting Classifier:** The `create_voting_classifier` is defined but only used for nested CV calculation, not explicitly evaluated against other models as a primary ensemble.
- **Bayesian Optimization:** Optuna is used, but it's only optimizing XGBoost on `n_estimators` and `max_depth`. The ground truth does a much more comprehensive optimization including learning_rate, subsample, colsample_bytree, gamma, lambda, alpha, min_child_weight, using `eval_set` and `early_stopping_rounds`, which is a more robust approach.
- **Nested Cross-Validation:** The nested CV is implemented, but it only evaluates the `create_voting_classifier()` using `cross_val_score`, not all the individual models and other ensembles, which is a major missing piece for unbiased evaluation as demanded. The ground truth iterates through models and ensembles within each outer fold.
- **Custom Ensemble with Dynamic Weight Assignment:** The `DynamicWeightEnsemble` class is a placeholder. The `predict` method currently just averages predictions (which is effectively hard voting or soft voting without actual probability handling), and the comment ""Dynamic weight assignment logic can go here"" indicates it's incomplete. The ground truth provides a concrete example using validation AUC to derive weights.
- **CatBoost:** The generated code initializes CatBoost but doesn't explicitly highlight its handling of categorical features (which isn't strictly necessary for this dataset if all features are numerical, but the requirement was to *apply* it).
- **SHAP Interpretation:** SHAP `explainer` is used, but the visualization part (`shap.summary_plot`, `shap.dependence_plot`, `shap.force_plot`) and local interpretation demonstration are missing. The ground truth includes these.
- **Model Calibration:** `CalibratedClassifierCV` is used, but `method` is set to `isotonic` and `cv` to `prefit`. The ground truth demonstrates both `isotonic` and `sigmoid` and also plots reliability curves with Brier scores, which is crucial for reliability analysis.
- **Ensemble Diversity Analysis/Combination Strategies:**
    - **Diversity:** `ensemble_diversity` function calculates correlation, but the visualization (`sns.heatmap`) and interpretation of correlation are missing. The ground truth provides both.
    - **Combination Strategies:** The advanced combination strategy (meta-learner on OOF predictions) is not implemented in the generated code.

**Code Quality:**
- Overall structure is decent, but functions like `create_voting_classifier` are defined but not extensively used or integrated into the main evaluation loop.
- Imports are mostly correct.
- Lack of comments and explanations in certain complex parts (e.g., custom ensemble placeholder).
- The `use_label_encoder=False` is used, but `eval_metric='logloss'` is missed in the general XGBoost instantiation at the beginning.

**Completeness:**
- **Missing Requirements:**
    - Comprehensive feature engineering (custom features, robust missing value handling).
    - Bagging with *different* base estimators.
    - Full nested cross-validation across *all* models and ensembles, not just one.
    - Comprehensive Optuna tuning parameters and early stopping.
    - Full implementation of custom ensemble with dynamic weights.
    - Comprehensive model interpretation with SHAP visualizations.
    - Full model calibration analysis (e.g., reliability curve plotting, Brier score comparison).
    - Advanced model combination strategies (meta-learner on OOF).
    - Ensemble diversity analysis visualizations and detailed printouts.
    - While it uses `CatBoost`, it doesn't explicitly demonstrate `cat_features` handling.

In summary, the generated code outlines many of the requested components but often provides a basic or incomplete implementation compared to the comprehensive nature and detailed evaluation specified in the task and present in the ground truth. It lacks the depth and rigor required for an expert-level solution in several key areas.",gpt_4o_mini,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import arviz as az
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from scipy import stats

# 1. Load and preprocess the dataset with comprehensive missing value handling
df = pd.read_csv('diabetes.csv')
imputer = SimpleImputer(strategy='mean')
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Scaling the data...",4,"The generated code attempts to address several requirements but falls short in implementation and correctness for most advanced sections.

**Primary Reasons for Failure/Critical Missing/Incorrect Elements:**

*   **Hierarchical Bayesian Model (Section 2/3):** The implementation of the hierarchical model is critically flawed. It defines `a` as a random intercept for *each individual* (`shape=len(X)`) but does not include any grouping variable. A hierarchical model requires grouping observations (e.g., by age group, region, etc.) to model variability *between* groups and *within* groups. The current setup is essentially treating every individual as their own group, making the ""hierarchical"" aspect meaningless in a practical sense and identical to a non-hierarchical model with an individual-level intercept. The ""mixed-effects model"" (Section 3) also suffers from this; `group_effect` is again defined per individual without any actual grouping structure. It just adds individual-level noise, not true mixed effects.
*   **Survival Analysis (Section 5):** The code only simulates `time` and `event` columns and then leaves a comment ""Further time-to-event modeling can be done with lifelines or similar libraries"". It **does not implement any survival analysis techniques** as required by the prompt (e.g., Cox Proportional Hazards Model). This is a critical omission.
*   **Gaussian Processes (Section 7):** The GP model is incomplete and likely incorrect. It defines a `gp.Marginal` but then calls `y_gp = gp.marginal_likelihood(y)`. This is just setting up the likelihood without defining the actual inputs (`X_gp` in the ground truth) for the GP or specifying *what* relationship it's modeling (e.g., Y vs X, or Y vs time). There's no input variable for the GP. The ground truth correctly uses `X_gp = df_scaled['Age'].values[:, None]` and `y_gp = df_scaled['Glucose'].values`. The generated code's GP doesn't connect features to outcomes.
*   **Causal Inference (Section 8):** The code explicitly states ""(for example, propensity score matching could be done separately)"" and *provides no implementation* for either propensity score matching or instrumental variables. This is a complete failure on this requirement.
*   **Comprehensive Missing Value Handling:** The prompt specified ""comprehensive missing value handling"" including identifying columns where '0' might represent missing values (common in this dataset), which the ground truth correctly addresses. The generated code only uses `SimpleImputer(strategy='mean')` which is insufficient for this dataset.
*   **Informative Priors:** While the logistic regression mentions ""informative prior"" for `betas` `(mu=0, sigma=1)`, this is a relatively generic prior. The ground truth provides more specific and medically informed priors (e.g., `beta_glucose = pm.Normal('beta_glucose', mu=0.7, sigma=0.3)`) based on expected impact, which is a key aspect of ""informative priors based on medical knowledge.""
*   **Posterior Predictive Checks and Credible Intervals (Section 10):** The code generates PPCs and credible intervals but only for the `logistic_model`. The prompt asks for them for ""model validation"" implying for the potentially more complex hierarchical/mixed models as well. The ground truth demonstrates this for its hierarchical model.
*   **Code Quality/Structure:**
    *   No warnings were suppressed, which can lead to noisy output in PyMC3/ArviZ.
    *   `return_inferencedata=False` was used for `pm.sample`, which is deprecated and makes analysis with ArviZ (like `az.waic` and `az.plot_ppc`) more cumbersome or potentially less robust. The ground truth uses `return_inferencedata=True`.
    *   The `X` and `y` for each model are not explicitly passed or used correctly in the `with pm.Model() as ...` blocks, relying implicitly on global variables. While this often works, it's less robust.
    *   The hierarchical model's `p = pm.invlogit(a[X.index])` is problematic; `a` is defined in the shape of `len(X)`, and `X.index` is a range from 0 to `len(X)-1`, so `a[X.index]` is simply `a` itself. This doesn't leverage any grouping.

In summary, the generated code contains fundamental misunderstandings of hierarchical/mixed models, completely omits causal inference and a large part of survival analysis, and has an incomplete/incorrect Gaussian process implementation. While it correctly uses PyMC3 for basic Bayesian regression and addresses some preprocessing steps, the complexity and specific requirements of the advanced statistical modeling as outlined in the prompt are not met.",gpt_4o_mini,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering, OPTICS
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.pipeline import make_pipeline
from sklearn.pipeline import Pipeline
from skle...",5,"The generated code attempts to address many parts of the prompt but falls short in correctness, completeness, and sometimes code quality for an ""expert"" evaluation.

Primary reasons for critical failure:

1.  **Missing/Incorrect Imports and Setup:** The generated code imports `fn` and `hashlib` which are not used and `_` is from `fn` but not explained or used correctly. The `MLPRegressor` is imported but then used incorrectly as an autoencoder (it's not an autoencoder; it's a regressor, and the `transform` method as implemented doesn't provide meaningful embeddings). `yellowbrick` and `statsmodels` are imported but barely utilized. A `df_transformed` is mentioned for association rule mining but never defined.
2.  **Inadequate Data Preprocessing & Feature Engineering:** The ""advanced feature engineering"" for metabolic syndrome is an oversimplification and potentially incorrect (`(data['BMI'] >= 30) & (data['BloodPressure'] >= 90)|(data['Glucose'] > 126) & (data['Insulin'] > 24)` is logically flawed and too basic for ""advanced""). Crucially, it doesn't handle the common issue of '0' values in certain columns of this dataset (e.g., Glucose, BloodPressure) which often indicate missing data, leading to incorrect calculations and clustering. The ground truth correctly replaces 0s with NaN and imputes.
3.  **Clustering Algorithm Implementation:**
    *   `KMeans()`, `DBSCAN()`, `AgglomerativeClustering()`, `SpectralClustering()`, `OPTICS()` initializations are missing crucial parameters (e.g., `n_clusters` for K-Means, Agglomerative, Spectral; `eps` and `min_samples` for DBSCAN and OPTICS). Without proper parameter tuning, these will produce suboptimal or even meaningless results. The ground truth demonstrates how to find optimal parameters.
    *   The `silhouette_score` loop uses `labels = model.fit_predict(X)` which means `labels` will only hold the result of the *last* model in the dictionary. It doesn't store labels for all models separately, which is critical for consensus clustering.
4.  **Dimensionality Reduction:**
    *   The Autoencoder implementation is entirely wrong. An `MLPRegressor` is not an autoencoder. An autoencoder needs an encoder and decoder structure (`Input`, `Dense` layers in Keras/TensorFlow for example), which projects data to a lower dimension and then reconstructs it. The current implementation tries to map X to X using a regression model, which won't provide meaningful embeddings for visualization.
5.  **Consensus Clustering:** The provided `consensus_clustering` function is incorrect. `pairwise_distances(labels_list)` will not create a meaningful similarity matrix from a list of label arrays. Consensus clustering typically involves creating a co-occurrence matrix (how many times samples are clustered together) across different algorithms, which then forms a similarity matrix to be re-clustered. The ground truth provides a robust method for this.
6.  **Patient Phenotype & Statistical Significance:**
    *   `phenotype_profiles` uses `mean()` on ALL columns, including potentially inappropriate ones.
    *   `perform_t_tests` conducts t-tests comparing each feature in *each group* to *all data*, which isn't the standard approach for comparing features *across* multiple clusters (ANOVA/Kruskal-Wallis is more appropriate). The ground truth correctly uses Kruskal-Wallis/Chi-squared.
7.  **Anomaly Detection:** Uses `IsolationForest` but doesn't visualize anomalies on a reduced dimension plot, nor does it include other specified anomaly detection techniques like OneClassSVM or LOF for a comprehensive approach.
8.  **Time-Series Clustering:** Correctly identifies the dataset as cross-sectional but doesn't elaborate on *how* one would apply it if temporal data *were* present, or what libraries/methods are typically used, beyond a commented `tslearn` line. The ground truth provides a good conceptual explanation.
9.  **Association Rule Mining:** Refers to `df_transformed` which is not defined. More importantly, it doesn't provide a way to discretize continuous features for ARM, which is essential for this dataset, making the ARM section incomplete and likely erroneous in practice. The ground truth explicitly handles discretization.
10. **Risk Stratification:** The `risk_stratification` function simply computes the mean of all columns per cluster, which is a start but lacks the clinical interpretation and risk mapping strategy demonstrated in the ground truth.

In summary, while the generated code includes many of the requested module imports and function names, the underlying implementations are often fundamentally flawed, incomplete, or incorrectly applied, demonstrating a lack of expert understanding of several advanced techniques.",gpt_4o_mini,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import VotingRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
import lightgbm as lgb
from catboost import CatBoostRegressor
from skl...",4,"The generated code attempts to address many aspects of the prompt but falls short in correctness, completeness, and code quality in several critical areas.

**Correctness & Completeness Issues:**

1.  **Preprocessing and Feature Engineering:**
    *   The `preprocess` function incorrectly assumes the presence of a 'Population' column for `social_support_ratio` which is not in the `world-happiness-report-2021.csv` dataset. This will cause an error.
    *   It also attempts to drop `Country code` which is not in the dataset.
    *   The `preprocessor` setup for `ColumnTransformer` is applied to `X_train` *before* the full pipeline is defined and used, leading to potential inconsistencies. The `numeric_features` and `categorical_features` are found on the *original* `X_train` after dropping columns but before applying the `preprocess` function's assumed feature engineering.
    *   The generated pipeline usually should encapsulate the entire `preprocessor` and the model. This is not strictly done for all models, leading to potential issues with `cross_val_score` `cross_val_predict`, SHAP, and LIME which might expect raw data or a consistent pipeline.

2.  **Ensemble Methods:**
    *   **Voting Regressor:** The `VotingRegressor` is instantiated but not wrapped in a `Pipeline` with the `preprocessor`, nor is it trained or evaluated.
    *   **Bagging:** Only instantiated, not integrated into a pipeline or trained. The `base_estimator` for `BaggingRegressor` should be an unfitted estimator, not a base model that would be preprocessed separately. `GradientBoostingRegressor` is a poor choice for a `base_estimator` in `BaggingRegressor` as GBR is already an ensemble itself. A simpler model like `DecisionTreeRegressor` is more typical.
    *   **Boosting Algorithms:** `adaboost`, `gradient_boost`, `xgboost`, `lightgbm`, `catboost` are only instantiated, not trained or evaluated within a pipeline. `AdaBoostRegressor` with `LinearRegression` as `base_estimator` is prone to issues if residuals are not well-behaved.
    *   **Stacking:** `StackingRegressor` is instantiated but not integrated into a `Pipeline` with the `preprocessor`, nor is it trained or evaluated.

3.  **Bayesian Optimization:**
    *   The `objective` function for Optuna creates an `XGBRegressor` directly. It does *not* include the `preprocessor` within the `model` pipeline passed to `cross_val_score`. This means `X_train` is passed as raw data, which will cause the `XGBoost` model to fail because `XGBoost` expects numerical input, while `X_train` contains categorical columns.

4.  **Nested Cross-Validation:**
    *   The inner loop (Hyperparameter Tuning for each fold) uses `GridSearchCV` (`KFold` is defined, but `GridSearchCV` takes precedence) instead of Optuna, contradicting the prompt which specified Optuna for Bayesian Optimization.
    *   More importantly, the `GridSearchCV` model is applied directly to `X_train`, once again skipping the crucial preprocessing step. This will lead to errors due to categorical features and non-scaled numerical features.
    *   The prompt asked for robust evaluation using nested cross-validation, but the implementation for the `scores` list just performs one pass. It does not clearly illustrate how the *nested* aspect is used to prevent data leakage in a true HPO setup.

5.  **Custom Ensemble with Dynamic Weight Assignment:**
    *   The `DynamicWeightEnsemble` class is rudimentary; it performs simple averaging. The prompt specifically asked for dynamic weight assignment *based on model performance*. The current implementation does not calculate weights based on performance.
    *   The `explainer` for SHAP and `lime_explainer` for LIME are incorrectly instantiated using the `DynamicWeightEnsemble` class, which is a custom `BaseEstimator` and `TransformerMixin` but not a standard Scikit-learn model that `shap.Explainer` or `lime.lime_tabular.LimeTabularExplainer` can directly interpret for feature importance. It’s also trying to explain an ensemble that averages predictions, not a single model, which complicates interpretation and is likely not what was intended for direct SHAP/LIME explanation. The base models *within* the ensemble would need to be explained.
    *   The `dynamic_models` are not wrapped in pipelines with the preprocessor, so they would fail to train on the raw `X_train`.

6.  **Model Interpretation (SHAP & LIME):**
    *   As noted above, SHAP and LIME are incorrectly applied to the `dynamic_weight_ensemble`. SHAP's `TreeExplainer` or `KernelExplainer` would be more appropriate for specific models. LIME `predict_fn` would need to handle preprocessing within it.
    *   `shap.summary_plot` and `lime_explanation.show_in_notebook` are called, but `matplotlib.pyplot` or `seaborn` are not imported or used to actually display the plots, which are critical for visualization and interaction.

7.  **Model Calibration and Uncertainty Quantification:**
    *   `CalibratedClassifierCV` is for classification, not regression. The prompt asked for model calibration and uncertainty quantification for regression. This is a critical error. For regression, calibration typically involves analyzing residuals or using methods like quantile regression (as in the ground truth).
    *   The ground truth used Quantile Regression; the generated code uses `CalibratedClassifierCV` (a direct misinterpretation).

8.  **Ensemble Diversity Analysis and Model Combination Optimization:**
    *   Diversity analysis uses `np.corrcoef` on predictions from `dynamic_models`. However, these `dynamic_models` were never individually trained (only within the `DynamicWeightEnsemble` which isn't used for individual model predictions here).
    *   Model combination optimization (finding optimal weights) was completely missed. The code states ""Model combination optimization is implemented separately based on evaluation criteria,"" implying it's not present.

**Code Quality & Structure:**

*   Missing `matplotlib.pyplot` and `seaborn` imports, which are essential for plotting.
*   The `preprocess` function modifies the DataFrame in-place (`data.drop(columns=..., inplace=True)`), which can lead to unexpected side effects if the DataFrame is used elsewhere. Generally, it's better to return a new DataFrame.
*   Lack of clear separation between defining models, training them, and evaluating them.
*   Many parts of the code are just instantiated objects without fitting or evaluation.
*   The overall structure is a sequence of blocks, but without calls to integrate them (e.g., `pipeline.fit()`, `model.predict()`), most of the defined functionality is never used.
*   The `Population` column and `Country code` were assumed as features and dropped/engineered on, but they don't exist in the provided dataset.

In summary, while the code attempts to cover the breadth of the requirements, the lack of careful integration of preprocessing with models, the incorrect application of certain techniques (e.g., `CalibratedClassifierCV` for regression, `GridSearchCV` instead of Optuna in nested CV, incorrect SHAP/LIME usage), and critical logical errors (e.g., assuming non-existent columns, not training all instantiated models) make it largely non-functional and unreliable for the given task.",gpt_4o_mini,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from scipy import stats
import arviz as az
import causal_inference as ci
from causal_inference import propensity_score

# Load and preprocess the dataset
data = pd.read_csv('world-happiness-report-2021.csv')

# Co...",3,"The generated code attempts to address several requirements but falls short in correctness, completeness, and adherence to specific instructions.

**Primary reasons for low score:**

*   **Missing Value Handling (Incorrect):** `data.fillna(data.mean(), inplace=True)` is not a comprehensive solution. It only handles numerical missing values by imputing with the mean, ignoring categorical columns and more sophisticated strategies like median/mode imputation, or more robust methods. The ground truth demonstrates a `ColumnTransformer` and `SimpleImputer` for numerical and categorical data separately, followed by standardization.
*   **Hierarchical Bayesian Model (Incorrect/Incomplete):**
    *   The generated code uses `data['Region']` which likely does not exist (the dataset has 'Regional indicator').
    *   It assumes `data['Happiness_Score']` and `data['GDP_per_capita']` exist, but these are not the column names in the dataset ('Ladder_score', 'Logged_GDP_per_capita'). This will cause `KeyError`.
    *   The `regional_effect` is based on `len(data['Region'].unique())`, but `regional_idx` is then mapped, which is okay, but it lacks the full model structure (e.g., global intercept, other predictors). The `sigma=1` for `happiness` is a fixed value, not a learned parameter. It's a very basic model, not a true hierarchical model as requested, which should include random effects with hyperpriors for the variance.
    *   `return_inferencedata=False` was used, which prevents `Arviz` from working seamlessly for subsequent steps like `plot_trace` and `summary`.
*   **Structural Equation Models (SEM) (Incorrect/Misinterpreted):** The request specifically asks for building SEMs using PyMC3/PyMC4 to analyze causal pathways. The generated code uses `statsmodels.formula.api.ols`. While OLS can estimate relationships in a path model, it does not build a 'structural equation model' in the sense of a full SEM framework (e.g., path analysis with observed/latent variables, multiple equations estimated simultaneously, fit indices). The ground truth cleverly approximates SEM by chaining PyMC3 models, which is a valid Bayesian approach to this task if a dedicated SEM library for PyMC3 is not direct.
*   **Causal Inference - Instrumental Variables (IV) (Incorrect/Incomplete):**
    *   The IV model uses `smf.iv.IV2SLS.from_formula`, but it defines `GDP_instrument` as a variable in the formula; however, there is no `GDP_instrument` column in the data. The prompt implies creating or simulating one if not available, which the ground truth does. This code would fail.
    *   There's no implementation of Propensity Score Matching (PSM), which was explicitly requested.
*   **Bayesian Regression with Informative Priors (Incorrect Column Names):** Again, `data['Happiness_Score']` and `data['GDP_per_capita']` are wrong column names.
*   **Gaussian Processes (Incomplete/Incorrect application within a Bayesian context):** While `sklearn.gaussian_process` can train a GP, the task specifically asks for *Bayesian* Gaussian processes likely within PyMC3, not a frequentist sklearn implementation. The ground truth correctly implements it using `pm.gp`. The provided `sklearn` code does not connect to the Bayesian framework or produce posteriors/credible intervals for GP parameters.
*   **Mediation Analysis (Incorrect/Incomplete):** The generated code uses `statsmodels.mediation.Mediation`, but this specific submodule `mediation` is not part of standard `statsmodels`. This line would likely cause an `AttributeError`. The ground truth implements mediation analysis by combining the posteriors of the individual path coefficients from its PyMC3-approximated SEMs, which is the correct Bayesian approach.
*   **Difference-in-Differences (DiD) (Incomplete/Incorrect Simulation):** `data['Year'] = np.random.choice([2020, 2021], size=len(data), p=[0.5, 0.5])` creates random years for each observation, but it does not simulate a ""pre"" and ""post"" period for each country, nor does it create a panel dataset. The `Treated` column is incorrectly defined as `np.where(data['Year'] == 2021, 1, 0)`, which means *all* countries are treated in 2021, not a specific treated group. A proper DiD simulation requires duplicating the data for two time points and assigning a treatment group.
*   **Bayesian Model Comparison (Missing):** The prompt requested WAIC, LOO-CV, and BayesFactors. The generated code only shows `az.plot_trace` and `az.summary`, which are diagnostics for a single model, not comparison *between* models using criteria like WAIC/LOO.
*   **Credible Intervals, PPC, Sensitivity Analysis (Partial/Incorrect):** Credible intervals are covered by `az.summary`, and a PPC plot is attempted, but `posterior_predictive = pm.sample_posterior_predictive(trace)` and `az.plot_ppc(posterior_predictive)` are not guaranteed to work correctly given the issues with `trace` and the model definition. Sensitivity analysis is implemented by simply modifying data and re-sampling, which is a very rudimentary approach, not a true prior sensitivity analysis as demonstrated in the ground truth (e.g., re-running with different priors and comparing posteriors).
*   **Imports and Structure:** Incorrect column names are a recurring issue. The `causal_inference` library and its `propensity_score` module are not standard and would likely fail to import without specific installation instructions, which contradicts the ""provide only the Python code"" instruction. `pm.sample` is used with `return_inferencedata=False`, which then creates issues for `arviz`.

In summary, the code demonstrates an attempt to use the specified libraries and methods but suffers from fundamental errors in understanding the dataset's column names, misinterpreting the specific requirements for certain statistical methods (SEM, Bayesian GP, Mediation), and failing to properly simulate or construct data for causal inference techniques like IV and DiD. Many parts of the code would simply not run or produce incorrect results due to these issues.",gpt_4o_mini,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import umap
import hdbscan
from sklearn.metrics import silhouette_score, silhouette_samples
from yellowbrick.cluster import KElbowVisualizer
from scipy import stats
import seaborn as sns
import matplotlib.pyplot as plt...",4,"The generated code attempts to address several aspects of the prompt but falls significantly short in completeness, correctness, and advanced implementation compared to the ground truth.

**Primary Reasons for Failure & Critical Missing/Incorrect Aspects:**

1.  **Completeness (Major Issue):** Many critical requirements outlined in the prompt are either completely missing or are implemented superficially.
    *   **Advanced Feature Engineering:** The prompt explicitly asks for ""advanced feature engineering including happiness profiles and regional characteristics."" The generated code only adds ""Happiness Score"" as a derived feature and scales existing features, completely ignoring regional indicators, interaction terms, or other advanced transformations (like `GDP_x_SocialSupport`, `Health_Freedom_Ratio`, `Generosity_minus_Corruption` in ground truth).
    *   **Advanced Dimensionality Reduction:** While PCA, t-SNE, and UMAP are mentioned, autoencoders and manifold learning (specifically Isomap in the ground truth) are missing. There is also no visualization of these embeddings with cluster labels as requested.
    *   **Optimal Clustering Parameters & Stability Analysis:** The generated code only implements silhouette analysis and elbow method and *prints* the scores, but doesn't genuinely *determine* optimal parameters by selecting the best K and using it downstream. Gap statistic and stability analysis are completely missing. The ground truth has a dedicated method `_find_optimal_k_silhouette` that selects the optimal K and uses it for the KMeans, Agglomerative, GMM, and Spectral models.
    *   **Consensus Clustering:** The implementation is extremely simplistic (""Majority vote"" without considering the quality or type of clustering algorithms, and only for discrete labels) and incorrect for a robust consensus clustering approach (which typically involves a co-association matrix and hierarchical clustering on it, as seen in the ground truth). It doesn't correctly aggregate diverse clustering outcomes into a robust grouping.
    *   **Detailed Country Profiles & Statistical Significance/Effect Size:** The profiling is very basic (only mean happiness and count) and completely misses statistical significance testing (ANOVA) and effect size analysis (Eta-squared) for each feature across clusters. The provided `shapiro` test is for normality, not for comparing group means.
    *   **Anomaly Detection:** Only Isolation Forest is implemented. LOF and One-Class SVM are missing. There's no visualization of anomalies which the ground truth includes.
    *   **Network Analysis:** The network centrality measures (degree, betweenness) are missing. The edge weighting (`weight=np.abs(row['Happiness Score'] - data.iloc[j]['Happiness Score'])`) is problematic as it makes less difference mean higher centrality, which is counterintuitive for ""relationships based on happiness factors"". Normally, you'd want *similarity* for edges. The visualization is also minimal.
    *   **Association Rule Mining:** The implementation is fundamentally flawed. It tries to do ARM on a boolean column `above_average` only (`happiness_data[['above_average']]`). Association Rule Mining targets patterns *between multiple items/features*. The ground truth correctly bins multiple numerical features and applies ARM on them.
    *   **Recommendation System & Policy Insights:** The recommendation system is very basic (random selection from a cluster). The policy insights section is non-existent beyond printing profile stats. Geopolitical validation is missing.

2.  **Correctness (Significant Issues):**
    *   **Missing Values:** `data.fillna(0, inplace=True)` is a poor choice for imputation as `0` for metrics like 'Healthy life expectancy' can drastically skew results. The ground truth uses median for numerical and mode for categorical, which is more robust.
    *   **Feature Scaling Scope:** `features = data[['GDP per capita', ..., 'Perceptions of corruption']]` followed by `features['Happiness Score'] = data['Ladder score']` and then `features_scaled = StandardScaler().fit_transform(features)`. The `Ladder score` is the outcome variable, not typically used as an independent feature for clustering unless the goal is to cluster *based on* happiness values, which is ambiguous. The `Ladder score` is `Happiness Score`. The prompt asks for clustering on ""happiness factors"" (`Ladder score` is an aggregation, not a factor).
    *   **Clustering Parameter Selection:** Most clustering algorithms (KMeans, Hierarchical, GMM, Spectral) are initialized with a fixed `n_clusters=5` without any attempt to determine an optimal `k` based on the silhouette/elbow analysis performed *after* the clustering. This contradicts the prompt's requirement (point 4).
    *   **Hierarchical Clustering:** `AgglomerativeClustering` typically does not fit and then return labels by itself if `n_clusters` is given. It just forms the clusters directly.
    *   **Spectral Clustering:** Missing `assign_labels='kmeans'` which is common practice for Spectral Clustering in scikit-learn to make the results more robust.
    *   **HDBSCAN:** The generated code uses `HDBSCAN(min_cluster_size=5).fit(features_scaled)` but then uses `silhouette_score` over K-means. HDBSCAN doesn't strictly need `n_clusters`.
    *   **Consensus Clustering Logic:** `row.mode()[0]` for consensus is an oversimplification and often inappropriate. It doesn't handle algorithms that produce noise clusters (like DBSCAN, HDBSCAN with -1 labels) and ignores the nuances of different clustering results.
    *   **Network Analysis Edge Weight:** As stated above, `np.abs(row['Happiness Score'] - data.iloc[j]['Happiness Score'])` for edge weight implies that the *difference* in happiness is the ""strength"" of the relationship, which is usually inverse to the goal of connecting similar countries. A similarity measure (like cosine similarity as in ground truth) is more appropriate.

3.  **Code Quality/Structure:**
    *   No class structure, leading to a flat script that doesn't encapsulate functionality, making it less modular and harder to manage. The ground truth uses a class, which is much better for such a complex task.
    *   Lack of clear function separation for different tasks.
    *   Many imports are at the top, but `SpectralClustering` and `IsolationForest` are imported in the middle of the script, which is inconsistent.
    *   No error handling or robust checks (e.g., if a dataset is too small for certain clustering algorithms).
    *   Hardcoded parameters (e.g., `n_clusters=5` for most algorithms) rather than allowing for dynamic determination or user input.

In summary, the generated code provides a skeletal solution, touching upon various methods but failing to implement them with the required depth, correctness, and adherence to the ""advanced"" and ""expert"" claims in the prompt's task description. It is a very basic attempt that would not yield genuinely insightful results for the given problem.",gpt_4o_mini,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from statsmodels.tsa.statespace.sarimax import SARIMAX
from pmdarima import auto_arima
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Step 1: Load data, parse 'date', filter for 'Germany', set 'date' as index
data = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
germany_data = data[data['location'] == 'Germany'].set_index('date')

# Step 2: Handle missing values
germany_data['new_cases'].fillna(m...",7,"The generated code implements the core requirements of the task efficiently. It correctly loads and filters the data, handles missing values (though somewhat simplistically), determines SARIMA orders using `auto_arima`, trains the model with exogenous variables, forecasts, evaluates using RMSE, and plots the results.

However, there are several areas for improvement to reach a higher score:

1.  **Missing Value Handling:** The generated code only uses `ffill` for missing values and directly applies it to `new_cases`, `stringency_index`, and `people_fully_vaccinated`. The ground truth includes `bfill` and `interpolate` as additional safeguards, which is more robust, especially for time series where leading NaNs or gaps can exist after `ffill`. For `new_cases`, `ffill` might not always be the best approach for a count variable; interpolation or other methods might be more suitable in some cases. The ground truth also explicitly checks for remaining NaNs and handles potential critical scenarios (e.g., empty dataframe).
2.  **`auto_arima` Usage and Robustness:**
    *   The `model = auto_arima(...)` is called on the training data, which is correct.
    *   However, the `exogenous` variable for `auto_arima` should ideally be `X_train` (the exogenous variables from the training set), not the full `exogenous` dataframe derived from `train`. The current implementation `exogenous = train[['stringency_index', 'people_fully_vaccinated']]` correctly extracts these from the training set, so this point is minor.
    *   The generated code doesn't explicitly handle potential issues with `auto_arima` like `trace=True` printing a lot of output, `suppress_warnings`, or `error_action='ignore'`, which the ground truth incorporates for cleaner execution.
    *   The ground truth also explicitly sets `start_p`, `start_q`, `max_p`, `max_q`, `start_P`, `start_Q`, `max_P`, `max_Q` in `auto_arima` and uses `test='adf'`, `stepwise=True`, and `n_jobs=-1` for more robust and efficient hyperparameter search. The generated code uses default values for most of these, which might not always find the optimal or most efficient solution.
3.  **SARIMAX Parameters:** When initializing `SARIMAX` for training, the ground truth sets `enforce_stationarity=False` and `enforce_invertibility=False`. While `auto_arima` often handles this implicitly, explicitly setting these can prevent convergence issues, especially when `d` or `D` are greater than 0 as determined by `auto_arima`.
4.  **Error Handling and User Feedback:** The ground truth includes robust error handling for file not found, insufficient data length for the test set, and more detailed print statements for user feedback during execution. The generated code lacks these.
5.  **Forecast Indexing:** The ground truth explicitly sets the index of the `forecast_values` to `y_test.index` to ensure proper alignment for plotting and evaluation. While `forecast` often retains the correct index (especially if `steps` matches `len(test)`), explicit assignment is a good practice.
6.  **Negative Forecasts:** The ground truth explicitly handles potential negative forecasts (`forecast_values[forecast_values < 0] = 0`), which are illogical for 'new_cases'. This is a valuable addition for count data.
7.  **Separation of Variables:** While the generated code correctly identifies `new_cases` as the target and the others as exogenous, the ground truth explicitly creates `y_train`, `X_train`, `y_test`, `X_test` variables, leading to slightly cleaner and more explicit code for model fitting and forecasting. The generated code directly indexes into `train` and `test` within the model calls, which is functional but less explicit.
8.  **Plotting Details:** The ground truth plot is more detailed with training data plotted, explicit title with RMSE, customized linewidths, grid, and `tight_layout()`, improving readability and presentation.

Overall, the generated code provides a functional solution to the problem, but it could be improved by adding more robustness, better error handling, and more refined parameter choices for `auto_arima` and `SARIMAX` fitting, as demonstrated in the ground truth.",gpt_4o_mini,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats import multitest
from statsmodels.formula.api import ols
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scikit_posthocs import posthoc_dunn

# Load dataset
df = pd.read_csv('winequality-red.csv')

# Define quality groups
conditions = [
    (df['quality'] <= 4),
    (df['quality'].between(5, 6)),
    (df['quality'] >= 7)
]
group_labels = ['Low Qual...",4,"The generated code has several critical issues that make it largely incorrect for the given task.

1.  **Incorrect Normality Test and Decision Logic:** The `normaltest` function is applied to the entire column, not to individual quality groups as required for ANOVA/Kruskal-Wallis assumptions. Futhermore, `scipy.stats.normaltest` is designed for a single sample, and its p-value is only interpretable if the sample size is sufficient (typically > 20). More importantly, the decision to use ANOVA or Kruskal-Wallis should be based on *all* groups being normal and homogeneous in variance (for ANOVA), or *any* group violating normality/homogeneity (for Kruskal-Wallis). The current logic only checks overall normality and then proceeds incorrectly. It also completely misses the check for homogeneity of variances (Levene's test), which is crucial for ANOVA.

2.  **Incorrect ANOVA Implementation:** It imports `ols` from `statsmodels.formula.api` but then uses `stats.f_oneway`, which doesn't directly leverage the `ols` model. While `f_oneway` is correct for one-way ANOVA, the decision criteria for its use are flawed.

3.  **Missing or Incorrect Post-hoc Tests:**
    *   For ANOVA, it *should* perform Tukey's HSD when ANOVA was used. The code doesn't do this.
    *   For Kruskal-Wallis, it correctly imports `posthoc_dunn` but the way it uses it for post-hoc for *all* significant properties *after* the initial main test decision is problematic. Dunn's test is *only* appropriate when Kruskal-Wallis was chosen. The code applies Dunn's without distinguishing whether Kruskal-Wallis or ANOVA was performed.
    *   The `summary_df = summary_df[summary_df['p-adjust'] < 0.05]` line after `posthoc_results[prop] = posthoc_dunn(...)` is problematic. `posthoc_dunn` returns a DataFrame where the p-values are already adjusted for multiple comparisons. The `summary_df` logic is not correctly extracting and displaying information about *which* pairs are significant. It's expecting a `p-adjust` column which `posthoc_dunn` doesn't directly label in the returned DataFrame like that. It provides a matrix of corrected p-values.

4.  **Incomplete Summarization:** The final summary prints `summary_df` which is a DataFrame of *all* post-hoc results for *all* significant properties combined and filtered, rather than a clear summary for *each* property indicating which *pairs* are significant. It misses the required summary from ""5. Summarize which features significantly differ between groups."" outlining the primary test and post-hoc results clearly per feature.

5.  **Quality Group Definition:** While `pd.cut` can be used, the specific bins `[0, 4, 6, 10]` and labels `['Low Quality', 'Medium Quality', 'High Quality']` result in 'Low Quality' being `<=4`, 'Medium Quality' being `(4, 6]`, and 'High Quality' being `(6, 10]`. This differs slightly from the ground truth's definition (`score <= 4`, `5-6`, `score >= 7`), especially for wines with quality 4, 6, and 7. The ground truth's `apply` function provides a more precise and robust grouping.

6.  **Code Structure and Readability:** The code lacks comments and clear separation of concerns (e.g., distinct sections for data loading, a primary analysis loop, post-hoc loop, plotting loop). Error handling for file not found is also missing.

7.  **Feature Identification:** `properties = df.columns[:-2]` assumes 'quality' and 'quality_group' are always the last two columns, which might not be robust if column order changes.

The fundamental flaw in statistical test selection and post-hoc application makes the statistical analysis presented largely unreliable.",gpt_4o_mini,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"Here's a Python code snippet that covers the requested functionalities, from loading the data to training an XGBoost classifier and evaluating the model using ROC-AUC and F1-score. It also includes the use of SHAP for interpreting feature importance.

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline ...",6,"The generated code attempts to address all requirements of the task but has several significant issues that affect its correctness and completeness, especially regarding the target variable definition and proper handling of time-series data for feature engineering and splitting.

**Key Issues:**

1.  **Incorrect Target Variable Definition:** The most critical error is in step 2.
    ```python
    data = data[data['new_deaths'] > 0]  # Exclude days with zero new deaths
    data['high_daily_death_increase'] = data.groupby('location')['new_deaths'].transform(
        lambda x: x > np.percentile(x, 90)
    )
    ```
    *   **Filtering `data` *before* percentile calculation:** By `data = data[data['new_deaths'] > 0]`, the code *removes* all rows where `new_deaths` is zero. This means the target variable can only be defined for days with *non-zero* new deaths. The task explicitly states: ""A 'high_daily_death_increase' is defined as a day where 'new_deaths' for a 'location' is in the top 10% of its own historical 'new_deaths' (excluding days with zero new deaths)."" This implies that the *percentile calculation* should exclude zeros, but the target variable `high_daily_death_increase` should still be assigned to *all* days (including days with zero deaths, where it would be 0). By filtering the DataFrame, you lose all days with zero deaths, which significantly alters the dataset's composition and the target variable's meaning, potentially leading to data leakage or incorrect evaluation if those zero-death days are relevant. The ground truth correctly defines the percentile from non-zero deaths but keeps all rows for target assignment.
    *   The `transform` lambda `x > np.percentile(x, 90)` directly assigns `True`/`False` to the `new_deaths` column where rows with 0 new deaths might exist. This should be a direct comparison against the location's 90th percentile *calculated from non-zero deaths*, and assigned to the original DataFrame containing all days. The current code calculates the percentile based on the *already filtered* dataframe, which biases the percentile itself.

2.  **Improper Time-Series Split:**
    ```python
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)
    ```
    While `shuffle=False` is used, the `train_test_split` function does not guarantee a time-based split unless the data is explicitly sorted by `date` *before* the split. The code implicitly relies on `data` being sorted by date, but this is not explicitly enforced, and for time-dependent operations (like feature engineering and evaluation), sorting is paramount. The ground truth explicitly sorts by date before the split.

3.  **Missing `population` for `people_fully_vaccinated` ratio:** The task explicitly asks to use `'people_fully_vaccinated' / 'population'` ratio. The generated code uses `'people_fully_vaccinated'` and `'population'` as separate features, which is not what was requested.

4.  **Incomplete Handling of Missing Values:**
    `data.fillna(0, inplace=True)`: While this addresses the request to fill NaNs with 0, it applies indiscriminately to *all* columns. For feature engineering, especially rolling calculations and lags, NaNs often appear at the beginning of groups. A simple `fillna(0)` might not be the most robust approach. The ground truth uses `ffill()` per group followed by `fillna(0)`, which is generally better for time-series features. Also, some original features (like `stringency_index`, `median_age`, etc.) might have NaNs, and filling them with 0 might not always be the best strategy for those specific features, although the prompt allowed it for FE NaNs. The `people_fully_vaccinated_ratio` calculation also needed specific NaN/Inf handling which was missed by a blanket `fillna(0)`.

5.  **SHAP Feature Names/Type Issue:**
    ```python
    shap.summary_plot(shap_values, features=X_test, max_display=10)
    ```
    The `X_test` here is `X_scaled`, which is a NumPy array. SHAP plots can benefit significantly from having feature names, but `X_test` (as a NumPy array) won't carry the feature names. The one-hot encoded features introduce new column names, and these are not correctly passed to SHAP. The ground truth correctly extracts feature names after preprocessing. The `shap.Explainer` also needs to be fitted on the preprocessed training data, which means `X_train_scaled` not implicitly on `grid_search.best_estimator_` which is the entire pipeline. The ground truth's approach using `explainer.shap_values(X_train_processed_df)` where `X_train_processed_df` has correct column names is more robust.

6.  **Missing `continent` in `features` list and OHE application:**
    The `features` list does not include `'continent'`, so it's not being used in the model. Then `pd.get_dummies(X, columns=['continent'], drop_first=True)` is called *on `X` directly*, which is already missing 'continent' at that point. The standard `ColumnTransformer` approach is much more robust for handling mixed-type data and ensures all selected categorical features are processed together.

7.  **`use_label_encoder=False` for XGBoost:** While the code includes this, it's a deprecation warning fix. The ground truth also explicitly sets `eval_metric='logloss'` as a good practice.

**Improvements to achieve a higher score:**

*   **Correct Target Variable Definition:** Ensure the target variable is defined for *all* rows, and the 90th percentile is derived *only* from non-zero `new_deaths`.
*   **Robust Time-Series Handling:** Explicitly sort the DataFrame by `date` before any time-series operations (rolling, lags) and especially before the train/test split.
*   **Feature Engineering for `people_fully_vaccinated_ratio`:** Implement the actual ratio as a feature.
*   **Advanced Missing Value Imputation for FE:** Use `groupby('location').ffill().fillna(0)` for rolling/lag features, and consider if `fillna(0)` is truly appropriate for other numerical columns or if other imputation strategies would be better.
*   **Proper Preprocessing Pipeline with `ColumnTransformer`:** This is crucial for handling categorical and numerical features cleanly and ensures feature names are maintained for SHAP.
*   **Correct SHAP Integration:** Ensure SHAP Explainer gets the correct feature names and processed data.
*   **Completeness on `rolling_icu_patients_std`:** The prompt asked for both mean and std for `icu_patients`, but only mean was implemented. `hosp_patients` also only had mean.",gpt_4o_mini,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import mixedlm
import statsmodels.formula.api as smf
from sklearn.impute import SimpleImputer

# Step 1: Load data, parse 'date', and calculate 'vaccination_percentage'
data = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
data['vaccination_percentage'] = data['people_fully_vaccinated'] / data['population'] * 100

# Step 2: Handle missing values
imputer = SimpleImputer(stra...",4,"The generated code attempts to address the task but falls short in several critical aspects, primarily due to incorrect implementation of key statistical modeling requirements and poor missing value handling.

**Major Issues:**

1.  **Incorrect Mixed-Effects Model Specification (Random Effects):**
    *   The prompt explicitly asks for ""Random effects: random intercepts for 'location' nested within 'continent'.""
    *   The generated code uses `groups=data['continent'], re_formula='~0 + location'`. This specifies random slopes for `location` within `continent`, not random intercepts for `location` nested within `continent`. To achieve random intercepts for each unique (continent, location) combination, one should create a unique `id` for each `(continent, location)` pair and use `groups=data['continent_location_id']` with `re_formula='~1'`. This is a fundamental misunderstanding of the specified random effects structure.

2.  **Incorrect Missing Value Handling:**
    *   The prompt asks for handling missing values (e.g., interpolation or median imputation grouped by location).
    *   The generated code uses `SimpleImputer(strategy='median')` and applies it with `transform(lambda x: imputer.fit_transform(x.values.reshape(-1,1)))` grouped by location. While it attempts to impute by location, `SimpleImputer` is designed for array-like input, and applying `fit_transform` repeatedly within a `transform` lambda can lead to issues. More importantly, it doesn't align with more robust time-series imputation methods like interpolation, which are often preferred for `reproduction_rate` or `stringency_index` that have a time-series nature. The ground truth demonstrates a superior approach using `ffill`/`bfill` for cumulative data and `interpolate` for continuous data, followed by a global median fallback. The generated code's imputation is simplistic and likely ineffective for the dataset's nature.
    *   It also imputes `population` and `people_fully_vaccinated` implicitly when calculating `vaccination_percentage`, but does not explicitly handle their missing values before or after calculation, which can lead to `NaN`s in `vaccination_percentage` if `population` is 0 or `NaN`.

3.  **Incomplete Preprocessing for Vaccination Percentage:**
    *   The prompt asks to calculate `vaccination_percentage = people_fully_vaccinated / population`. The generated code does this.
    *   However, it misses a crucial step: capping `vaccination_percentage` at 100% as it cannot exceed the total population. This is a common data cleaning step for such a variable.

4.  **Reporting of Random Effects Variance:**
    *   The prompt (point 6) asks to ""Report the variance components for the random effects.""
    *   The generated code prints `fit_model.random_effects`, which shows the *estimated random effects values* for each group, not the *variance components* (e.g., the variance of the random intercepts). The variance component is a single value representing the variability *between* the random effects.

5.  **Lack of Interpretation of Fixed Effects:**
    *   The prompt (point 5) asks for ""Interpret the significance and direction of these effects.""
    *   The generated code prints the summary table and then lists coefficients, SEs, and p-values, but it provides no actual human-readable interpretation of what those values mean in the context of the problem, unlike the ground truth which explicitly discusses the implications of positive/negative coefficients and significance.

6.  **Missing Error Handling and Robustness:**
    *   The code lacks any error handling for file not found (for CSV loading) or model convergence issues, making it less robust.

7.  **Minor Issues/Improvements:**
    *   Duplicate import `smf` (imported twice).
    *   `sm.mixedlm` is from `statsmodels.formula.api` which is already imported as `smf`, so `sm.mixedlm` isn't accessible via `sm` unless `import statsmodels.api as sm` is actually `from statsmodels.formula.api import mixedlm`. However, it's better to explicitly use `smf.mixedlm`. The current import structure `import statsmodels.api as sm` followed by `from statsmodels.formula.api import mixedlm` is a bit confusing. Using `smf` for formula-based models is clearer.
    *   Doesn't sort data by location and date, which is good practice for time-series operations like more advanced interpolation if it were used.",gpt_4o_mini,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score

# Step 1: Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# Step 2: Select the features for clustering
features = data[['elixirCost', 'Win Rate', 'Usage']]

# Step 3: Standardize the features
scaler = StandardScaler()
features_scaled = scaler.fit_transfor...",7,"The generated code correctly implements most parts of the task, demonstrating a good understanding of the required libraries and concepts.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly selects and standardizes features.
*   Successfully attempts to determine K using both WCSS (Elbow Method) and Silhouette scores, and plots both, which is a good practice.
*   Applies K-Means clustering with a chosen K.
*   Performs PCA to reduce dimensionality to 2 components.
*   Generates a scatter plot of the PCA reduced features, colored by cluster labels.
*   Good use of `_predict` and `fit_transform`.

**Areas for improvement (why not higher than 7):**
*   **Minor Missing Functionality in K-Means (n_init parameter):** In `sklearn` versions 1.1 and above, the `n_init` parameter for `KMeans` defaults to `'auto'` and explicitly setting it to `10` (as in the ground truth) is needed to replicate older behavior and suppress a future warning. While the code might run without issues in older versions, it's good practice to include it for future compatibility and to avoid warnings. The generated code implicitly uses the default, which is not strictly wrong but not as robust as explicitly setting `n_init=10`.
*   **Hardcoded `optimal_k` without justification:** While the code plots WCSS and Silhouette scores, it then hardcodes `optimal_k = 4` without any programmatic or commented justification for this choice. The task asked to ""justify your choice of K,"" which implies either a visual interpretation of the plots (which should be mentioned) or programmatically selecting based on the highest silhouette score or clearest elbow. The ground truth explicitly states ""Based on the Elbow plot, visually inspect... We will choose K=3 for demonstration purposes."" which is a better approach given the ambiguity of the elbow method.
*   **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and missing columns. The generated code lacks this, which makes it less production-ready and more prone to crashing if the dataset is not found or has different column names.
*   **Axis Labeling for PCA Plot:** The PCA plot's axes labels could be improved to include the explained variance ratio for each principal component, as done in the ground truth, which provides context for the visualization.

Overall, it's a very good attempt, fulfilling most of the requirements. The minor points relate to robustness, thoroughness in justification, and handling library specific nuances.",gpt_4o_mini,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder

# 1. Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# 2. Select features and target
features = data[['elixirCost', 'Usage', 'rarity']]
target = data['Win Rate']

# 3. Preprocess the rarity column (one-hot encoding)
encoder = OneHotEncoder(spars...",8,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, selects the specified features and target, preprocesses the `rarity` column using one-hot encoding, splits the data, trains a RandomForestRegressor, and evaluates it using MSE and R-squared.

**Strengths:**
- Correctly implements data loading and feature/target selection.
- Successfully performs one-hot encoding using `OneHotEncoder` and `pd.concat` to integrate the processed feature.
- Correctly splits the data using `train_test_split` with the specified `test_size` and `random_state`.
- Trains a `RandomForestRegressor` and evaluates it with `mean_squared_error` and `r2_score`.
- Prints the evaluation metrics as requested.

**Areas for Improvement (Why not a 10):**
- **Robustness/Error Handling:** The ground truth code includes basic error handling for `FileNotFoundError` and checks for missing columns. The generated code lacks these robust checks, which are good practice in real-world scenarios.
- **Missing Value Handling:** The prompt doesn't explicitly mention handling missing values, but `Win Rate` or `Usage` might have NaN values in `clash_royale_cards.csv`, which could lead to issues later (e.g., `fit` and `predict` failing if `y_train` or `y_test` contain NaNs, or `features_final` containing NaNs from `elixirCost` or `Usage`). The ground truth implicitly handles this by dropping NaNs, which is a simple yet effective approach if not specified otherwise.
- **Clarity in Preprocessing:** While correct, the one-hot encoding approach with `OneHotEncoder` and then `pd.concat` is slightly more verbose than using `pd.get_dummies` directly on `X`, which is often preferred for its conciseness when working with `CategoricalDtype` or just `object` columns in pandas. The ground truth demonstrates `pd.get_dummies`.
- **Code Structure and Comments:** While simple, the ground truth uses comments to clearly delineate each step, which improves readability and helps meet the ""completeness"" criteria by explicitly showing how each requirement is addressed. The generated code has fewer structuring comments.",gpt_4o_mini,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# Identify the unique categories in the rarity column
rarity_groups = data['rarity'].unique()

# Perform ANOVA test
anova_result = stats.f_oneway(*(data[data['rarity'] == rarity]['Win Rate'] for rarity in rarity_groups))

# Null and alternative hypotheses
null_hypothesis = ""There is no significant difference in mean Win Rate across different rarity categories.""
alternative_hypo...",7,"The generated code correctly addresses most of the requirements but misses some crucial aspects related to data cleaning, hypothesis statement, and robustness.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly identifies unique rarity categories.
*   Uses `scipy.stats.f_oneway` for ANOVA, which is appropriate.
*   Correctly prints the F-statistic and p-value.
*   Correctly interprets the p-value against the alpha level.

**Areas for Improvement:**
*   **Data Cleaning/Handling Missing Values:** The generated code does not explicitly handle potential missing values or non-numeric values in the 'Win Rate' column, which can lead to errors or incorrect results if the data isn't perfectly clean. The ground truth explicitly converts 'Win Rate' to numeric and drops NaNs.
*   **Robustness:** There's no error handling for `FileNotFoundError` if the CSV isn't present.
*   **Clear Hypothesis Statement:** While the hypotheses are mentioned in comments, the task explicitly asks to *state* (print) them. The ground truth clearly prints the null and alternative hypotheses, making the output more informative.
*   **Group Size for ANOVA:** ANOVA requires at least two observations per group for valid testing. The generated code doesn't check if individual rarity groups have enough data before passing them to `f_oneway`. If a category has only one or zero win rates, `f_oneway` might error or produce misleading results. The ground truth includes checks for the number of groups and observations within groups.
*   **Output Clarity:** The output of the generated code is a bit terse; adding labels like ""F-statistic:"", ""p-value:"", and a clear concluding sentence for the interpretation would improve readability, similar to the ground truth.",gpt_4o_mini,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
data = pd.read_csv('iris.csv')

# Define features and target
X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm...",9,"The generated code is highly correct and addresses almost all requirements of the task.

**Strong points:**
- All necessary libraries are correctly imported.
- Data loading, feature/target definition (`X`, `y`), and target encoding are handled correctly.
- Data splitting with the correct `test_size` and `random_state` is implemented.
- All three specified classifiers (Logistic Regression, SVC with linear kernel, Random Forest with `n_estimators=100`) are initialized correctly.
- The code iterates through models, trains them, makes predictions, and calculates all required metrics (accuracy, macro precision, macro recall, macro F1-score).
- The results are printed clearly for each model.
- It correctly identifies the best model based on the highest F1-score (macro average).

**Minor areas for improvement (preventing a perfect 10):**
- **Logistic Regression `max_iter`:** The default `max_iter` for `LogisticRegression` can sometimes lead to `ConvergenceWarning` with small datasets or certain default solvers (like 'liblinear' or older 'lbfgs'). The ground truth code specifically increased `max_iter` to `200` to avoid this. While not strictly a ""bug"" for this specific dataset and problem, it's good practice to be aware of and potentially handle convergence issues for logistic regression.
- **`stratify` argument in `train_test_split`:** The ground truth code used `stratify=y_encoded` in `train_test_split`. For imbalanced or even multi-class classification problems, using `stratify` ensures that the proportion of classes in the training and testing sets is the same as in the original dataset. While the Iris dataset is balanced, it's a good practice to include `stratify` for robust model evaluation, especially if the underlying problem had class imbalance. For a general solution to machine learning problems, stratify is an important consideration.

Overall, the generated code is excellent and robust for the given task. The minor points are more about best practices and robustness for general machine learning tasks rather than critical flaws for *this specific* problem.",gpt_4o_mini,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('iris.csv')

# Select the features
features = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Standardize the features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Apply K-M...",9,"The generated code correctly addresses all requirements of the task. It loads the data, selects and standardizes features, applies K-Means with the specified parameters, performs PCA, and generates both required scatter plots with appropriate coloring and titles. Imports are proper.

Minor improvements could include:
1.  **Error Handling for File Loading:** While not explicitly requested, adding a `try-except` block for `pd.read_csv` as in the ground truth is good practice for robustness.
2.  **Feature Selection:** The ground truth uses `X = df[features]` after defining a list of features, which is a slightly cleaner way to select columns than `features = df[['SepalLengthCm', ...]]` directly. Both work identically, but the list approach is often more flexible.
3.  **Color Bar Labels for True Species:** The ground truth code provides a more informative color bar for the true species plot, showing the actual species names instead of just numerical labels. While the generated code meets the requirement of coloring, this addition enhances usability.
4.  **Plot Aesthetic Consistency:** The ground truth includes `alpha=0.8` and `grid=True` for both plots, which are nice aesthetic touches that the generated code omits.

Overall, the code is highly functional and fulfills the prompt effectively. The points above are minor enhancements rather than critical failures.",gpt_4o_mini,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# Load the dataset
data = pd.read_csv('iris.csv')

# Create new features
data['PetalArea'] = data['PetalLengthCm'] * data['PetalWidthCm']
data['SepalArea'] = data['SepalLengthCm'] * data['SepalWidthCm']

# Define features and target
X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'Pe...",9,"The generated code is highly correct and nearly complete. It successfully loads the dataset, creates the specified features, defines the correct X and y, encodes the target, initializes the Gradient Boosting Classifier with the correct parameters, and performs 5-fold cross-validation, reporting the mean accuracy.

**Strong points:**
*   Correctly implements all specified steps.
*   Proper use of pandas for data manipulation.
*   Correct application of `LabelEncoder`.
*   Correct `GradientBoostingClassifier` initialization and `cross_val_score` usage.

**Area for minor improvement (preventing a perfect 10):**
*   The `cross_val_score` call implicitly uses 'accuracy' as the scoring metric for classification tasks, which is correct. However, explicitly adding `scoring='accuracy'` (as in the ground truth) is a good practice for clarity and robustness, even if it's the default in this context. While not strictly required by the prompt, it makes the intent clearer and avoids potential future ambiguity if `scoring` defaults were to change or the problem involved a different metric.
*   The `train_test_split` import is present but not used. While harmless, it's unnecessary and could be removed for cleaner imports.",gpt_4o_mini,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load the dataset
df = pd.read_csv('heart.csv')

# Define features and target
X = df[['Age', 'Sex', 'ChestPainType',...",9,"The generated code successfully completes all specified tasks. It correctly loads the dataset, defines features and target, preprocesses categorical and numerical features using a ColumnTransformer within a Pipeline, splits the data, trains a RandomForestClassifier, and evaluates it using the requested metrics (accuracy, macro-averaged precision, recall, F1-score, and ROC-AUC).

Specific strengths include:
- Correct use of `ColumnTransformer` for preprocessing.
- Correct application of `Pipeline` to streamline preprocessing and model training.
- Proper calculation of all specified evaluation metrics, including macro-averaged F1-score and ROC-AUC.
- Adherence to `random_state` for reproducibility.

Minor areas for improvement to achieve a perfect score:
- The `OneHotEncoder` within `ColumnTransformer` doesn't explicitly set `handle_unknown='ignore'`. While not strictly required for this dataset if all categories are present in both train and test, it's good practice for robustness with `ColumnTransformer`. The ground truth includes this, showing a slightly more robust implementation.
- The numerical features list `numerical_features = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']` is hardcoded. While correct for the problem description, a more robust/dynamic approach (like in the ground truth) would infer them from the difference between all features and categorical features, making the code more adaptable to slight changes in feature sets without manual updates.
- Error handling for file loading (`try-except` block) is missing, which is a good practice for deployed code but not explicitly requested by the prompt for benchmark.
- No `stratify=y` in `train_test_split`. While `random_state` helps with reproducibility, `stratify=y` is generally recommended for classification tasks, especially with imbalanced datasets, to ensure that the class distribution is maintained in both training and test sets. The ground truth includes this.

Overall, it's a very solid and complete solution addressing all core requirements of the task.",gpt_4o_mini,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score

# 1. Load the dataset
data = pd.read_csv('heart.csv')

# 2. Select the numerical features for clustering
features = data[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']]

# 3. Standardize the selected features
scaler = StandardScaler()
scaled_features = scaler.f...",8,"The generated code correctly addresses almost all aspects of the task. It loads the dataset, selects specific features, standardizes them, performs the elbow method and silhouette score calculations for K-selection (and plots them), applies K-Means with the specified parameters, reduces dimensionality with PCA, and visualizes the results.

Areas for improvement to achieve a higher score:
1.  **Hardcoded `optimal_k`**: While the prompt states ""assuming K=3 for this example"", implying it's acceptable, a more robust solution would dynamically select the optimal K based on the plots (e.g., finding the K with the highest silhouette score, or a more sophisticated elbow point detection). The ground truth code makes a decision and explicitly states it's based on visual inspection.
2.  **Missing Error Handling**: The generated code lacks error handling for `pd.read_csv` if 'heart.csv' is not found, which was present in the ground truth.
3.  **Plot Visualization Details**:
    *   The `plt.scatter` in the final plot uses `c=cluster_labels` which relies on `cmap='viridis'`. While functional, the ground truth code iterates through clusters to assign distinct colors and create a legend, which is generally better for clarity and professionalism in cluster visualizations, particularly when cluster labels themselves don't have inherent numerical order.
    *   The subplot titles are less descriptive than the ground truth.
    *   The plots in the generated code lack grid lines and specific X-tick labels for K, which are present in the ground truth and improve readability.
4.  **Print Statements/Feedback**: The ground truth code includes helpful print statements to trace the execution and confirm steps like data loading, feature selection, and standardization. The generated code is silent on these aspects.

Overall, it's a very good attempt that meets most requirements, with minor refinements needed for robustness, presentation, and user feedback.",gpt_4o_mini,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Step 1: Create binary target variable
data['target'] = np.where(data['quality'] >= 7, 'good', 'bad')

# Step 2: Preprocess the data
# Handle missing values ...",7,"The generated code correctly addresses most of the task requirements.

**Strong Points:**
- Correctly loads the data.
- Correctly implements the binary target variable creation (though the type is not explicitly set to int, it's handled by `np.where`).
- Handles missing values using mean imputation (reasonable for numerical data, though median can sometimes be more robust).
- Correctly splits the data into train/test sets and scales features.
- Trains a Logistic Regression model.
- Evaluates performance using Accuracy, F1-score, and Classification Report.
- Identifies and reports feature importance based on model coefficients.

**Areas for Improvement (Why not higher score):**
- **Binary Target Variable Type:** The target variable `y` is created with string values ('good', 'bad'). While `LogisticRegression` can handle this sometimes, it's generally better practice and more robust to convert it to numerical (0 or 1) as explicitly stated in the ground truth (`.astype(int)`). This caused the `pos_label` argument to be necessary for `f1_score`.
- **Missing Value Handling:** The imputation method `data.fillna(data.mean(), inplace=True)` will impute values in the `quality` and `target` columns as well, which is not ideal. The `quality` column is dropped later, but the target column should not have NaNs anyway. It's safer to impute only numeric feature columns before dropping the target. The ground truth correctly separates features before checking for and handling missing values.
- **Scaling and DataFrame Conversion:** The scaled features `X_scaled` are a NumPy array. While this works for model training, the ground truth converts them back to a DataFrame (`X_train_scaled_df`, `X_test_scaled_df`) to maintain column names. This becomes important when reporting feature importance, as it ensures the `X.columns` corresponds directly to the scaled features. In this generated code, `X.columns` is used directly with `importance = model.coef_[0]`, which implicitly assumes `X_scaled` retains the column order of `X`. While this is true for most cases, it's more robust to work with DataFrames or explicitly get features names from the scaler if needed.
- **Stratification:** The `train_test_split` does not use `stratify=y`, which is important for imbalanced datasets (like wine quality) to ensure target class proportions are maintained in train and test sets.
- **Robustness (File Not Found):** The ground truth includes error handling for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.
- **Clarity/Readability:** The ground truth adds more print statements and comments to make the flow and results clearer, which is beneficial for a benchmark task. The generated code is functional but less verbose in its output.",gpt_4o_mini,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

# Load the dataset
df = pd.read_csv('heart.csv')

# Create a box plot
plt.figure(figsize=(10, 6))
sns.boxplot(x='HeartDisease', y='Oldpeak', data=df)
plt.xlabel('Heart Disease (0 = No, 1 = Yes)')
plt.ylabel('Oldpeak (ST depression induced by exercise relative to rest)')
plt.title('Distribution of Oldpeak by Heart Disease Status')
plt.show()

...",7,"The generated code correctly addresses most of the task requirements: loading data, creating a box plot, performing logistic regression and extracting the coefficient and odds ratio, discretizing 'Oldpeak', and calculating proportions of HeartDisease=1 per bin.

However, there are a few areas for improvement:
1.  **Interpretation of Odds Ratio:** While the code calculates and prints the odds ratio, it *does not* provide a meaningful interpretation of its meaning in the context of odds ratios, which was explicitly requested in the prompt. This is a critical component of the data science task.
2.  **Robustness (Error Handling/Data Quality Checks):** The ground truth includes `try-except` for file loading and checks for missing values, making it more robust. The generated code lacks these, which is a minor but good practice for real-world scenarios.
3.  **Visualization Enhancement:** The box plot and final proportion calculation are done correctly, but the Ground Truth adds a bar plot for the proportions, which enhances the analysis.
4.  **Clarity of Output:** The interpretation of the coefficient and odds ratio is crucial. The generated code only prints the numbers, whereas the ground truth provides a worded explanation, which is far more useful for understanding the results.

Overall, it's a solid attempt that gets most of the technical steps right, but falls short on the interpretation and robustness aspects.",gpt_4o_mini,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Step 1: Load data
data = pd.read_csv('spotify_2023.csv')

# Step 2: Convert `streams` to numeric (coerce errors to NaN) and drop NaN rows
dat...",7,"The generated code largely addresses the requirements but has a significant issue in how it handles feature selection and one-hot encoding, leading to incorrect feature sets.

**Strengths:**
*   Correctly loads data and handles `streams` conversion to numeric and dropping NaNs.
*   Correctly creates the `is_popular` target variable based on the 75th percentile.
*   Correctly selects the specified numerical features and categorical features.
*   Correctly splits the data into training and testing sets with `random_state=42`.
*   Correctly trains a Logistic Regression model with `max_iter=1000`.
*   Correctly calculates and prints F1-score (macro), ROC AUC score, and the confusion matrix.
*   Properly includes all necessary imports.

**Areas for Improvement / Issues:**

1.  **Incorrect Feature Combination Before One-Hot Encoding (Major Issue):**
    *   The line `X = preprocessor.fit_transform(data[categorical_features].join(features))` is incorrect.
    *   `data[categorical_features].join(features)` will only join the `features` DataFrame to `data[categorical_features]` *if their indices align*. This could lead to a mix-up of features if the index is not distinct or if rows were dropped (which they were with `dropna`).
    *   More critically, `ColumnTransformer` with `remainder='passthrough'` should operate on the *entire* input `X` that contains *all* features (numerical and categorical). The current approach passes only `categorical_features` joined with `features` (which is problematic) to the `preprocessor`, not `data[all_features]`. This means the numerical features might not be passed through correctly, or the resulting `X` array structure will not match the expected combined features.
    *   The intent was to apply `OneHotEncoder` only to specified categorical columns and keep numerical columns as they are. The `ColumnTransformer` should typically operate on a DataFrame containing *all* initial features. The ground truth's `pd.get_dummies` directly on `X` (which is already `df[all_features]`) is a more straightforward and common approach for this simple scenario. If `ColumnTransformer` is used, `X` should be `data[numerical_features + categorical_features]`, and the `ColumnTransformer` defined over `numerical_features` and `categorical_features`.

2.  **Missing `stratify` in `train_test_split`:** The ground truth used `stratify=y` to ensure the class distribution of `is_popular` is maintained in both training and testing sets. This is good practice for imbalanced datasets and was not included in the generated code. While not explicitly asked for, it's a best practice for classification tasks.

3.  **No `random_state` for Logistic Regression:** While not explicitly required for the model training itself, setting `random_state` in `LogisticRegression` would ensure full reproducibility of the model, similar to `train_test_split`.

Addressing the first point is crucial for the correctness of the model's features. The second and third points are minor improvements.",gpt_4o_mini,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Step 1: Load data
df = pd.read_csv('spotify_2023.csv')

# Step 2: Select audio features and drop NaNs
audio_features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%', 
                  'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm']
df_selected = df[audio_features].dropna()

# Step 3: ...",8,"The generated code correctly addresses almost all requirements of the task. It loads the data, selects the specified audio features, handles NaNs, standardizes the features, applies K-Means clustering, performs PCA, and generates the scatter plot. It also calculates and displays the mean values of the original (unstandardized) audio features per cluster.

Strong points:
- Correctly identifies and uses all required libraries.
- Implements all core steps (data loading, feature selection, NaN handling, standardization, K-Means, PCA, plotting, mean calculation).
- `random_state` is correctly used for reproducibility in both KMeans and PCA.
- `n_init='auto'` is correctly used for K-Means.
- The plot is generated with correct components and coloring.
- The mean calculation is performed on the correct (original) features and grouped by cluster.

Areas for minor improvement (leading to a score of 8 instead of 9 or 10):
- **NaN Handling and Original Data for Analysis:** The generated code performs `df_selected = df[audio_features].dropna()`. This creates `df_selected` as a subset of the original `df` where NaNs are dropped. When it later computes `df_selected['cluster'] = kmeans.fit_predict(X_scaled)`, it adds the cluster labels to this `df_selected`. Then, for calculating means, it uses `df_selected.groupby('cluster')[audio_features].mean()`. This is **correct in principle** because `df_selected` retains the original, unstandardized values and their indices align properly after `dropna()`. The ground truth code explicitly creates `df_original_for_analysis = df_features.copy()` *before* standardization and appends labels to it, which is a slightly more robust way to ensure that the mean calculation is always on the original (unstandardized) values, especially if there were intermediate steps that accidentally modified `df_selected`. However, for this specific task, the generated code's approach is functionally correct.
- **PCA `n_components`:** The generated code initializes `pca = PCA(random_state=42)` without specifying `n_components=2`. While `fit_transform` implicitly uses `n_components=min(n_samples, n_features)` by default if not specified, leading to 2 components for a 2D plot of 8 features, it's best practice to explicitly state `n_components=2` as per the prompt's requirement ""Reduce standardized features to 2 principal components"".
- **File Not Found Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robustness. The generated code assumes the file exists. While the prompt stated ""Assume CSV is in current directory"", including error handling is generally better.",gpt_4o_mini,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load data
data = pd.read_csv('spotify_2023.csv')

# Step 2: Convert 'streams' to numeric (coerce errors to NaN)
data['streams'] = pd.to_numeric(data['streams'], errors='coerce')

# Step 3: Filter for songs with 'streams' > 10,000,000 and 'released_year' between 2010 and 2023, inclusive
filtered_data = data[(data['streams'] > 10000000) & (data['released_year'].between(2010, 2023))]

# Drop rows with NaN in 'streams' after co...",8,"The generated code correctly addresses all requirements of the task. It loads the data, converts 'streams' to numeric with error coercion, filters the data correctly based on 'streams' and 'released_year', groups the data by year, calculates the average audio features, and generates line plots for each feature.

**Strong points:**
- Correctly handles `streams` conversion and `NaN` values.
- Applies all specified filtering conditions (`streams` > 10M, `released_year` between 2010 and 2023).
- Uses `groupby().agg()` effectively for calculating yearly averages.
- Generates separate plots for each audio feature with appropriate titles and labels.
- Includes `plt.xticks(grouped_data['released_year'])` which helps in displaying all years clearly on the x-axis.

**Areas for improvement (to achieve a higher score):**
- **Plotting efficiency:** The generated code creates three separate figures using `plt.figure()` and `plt.show()` three times. While functionally correct, it's generally better practice for presentation and efficiency to put all related plots into a single figure using `plt.subplots()` as demonstrated in the ground truth. This avoids opening multiple plot windows and allows for easier comparison.
- **Error Handling (File Not Found):** The generated code does not include error handling for `FileNotFoundError` when loading the CSV, unlike the ground truth. This is a good practice for robust code.
- **Clarity of filters:** The generated code merges the `streams` filtering and year filtering into one line, which is effective but slightly less readable than breaking it into two steps as done in the ground truth, especially for debugging purposes. Also, the `dropna(subset=['streams'])` is applied *after* the initial filtering, which means that `NaN` entries in streams that already fulfill the year filter (but not the streams > 10M filter) might be dropped unnecessarily if they had no other issues. However, given the prompt's instruction to drop *after* conversion, and the subsequent `streams > 10M` filter which wouldn't apply to NaNs anyway, its current placement is functionally fine. The ground truth's approach of `dropna` immediately after `to_numeric` ensures only valid `streams` values proceed.

Overall, it's a very good solution that successfully completes the task. The minor improvements relate more to best practices and code elegance than to correctness.",gpt_4o_mini,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean Likes, Dislikes, and Video views
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', '').fillna(0), errors='coerce').fillna(0)
df['Dislikes'] = pd.to_numeric(df['Dislikes'].str.replace(...",7,"The generated code correctly addresses most of the requirements. It loads the dataset, performs the necessary cleaning for 'Likes', 'Dislikes', and 'Video views' (commas, numeric, NaN to 0), handles the 'published' year, fills missing 'Category' values and applies one-hot encoding, defines features and target, splits the data, trains a Linear Regression model, and reports R2 and MAE.

However, there are a couple of points for improvement:
1.  **Handling `published` column NaNs**: The instruction was to ""Treat the `published` column (year) as a numeric feature."" The ground truth ensures `published` is converted to datetime, then year is extracted, and any resulting NaNs (from conversion errors) are filled with 0. The generated code directly converts to `pd.to_datetime(df['published']).dt.year`. If there are invalid date strings, this might result in `NaT` (Not a Time) or `NaN` values that are then propagated, potentially causing issues when used as a numeric feature in `X`. The ground truth explicitly handles `errors='coerce'` and `fillna(0)` which is safer.
2.  **`fillna(0)` on `str.replace` output for numeric columns**: For the cleaning of `Likes`, `Dislikes`, `Video views`, the generated code uses `df[col].str.replace(',', '').fillna(0)`. The `fillna(0)` here is on the potentially empty string result of `str.replace` before `pd.to_numeric` with `errors='coerce'`. This `fillna(0)` before `to_numeric` doesn't make sense for actual NaN values that `to_numeric` would then create. It should be applied *after* `pd.to_numeric(..., errors='coerce')` to fill the NaNs created by the coercion. The current implementation works because `pd.to_numeric` on an empty string `''` will produce `NaN` (which is then filled by the *second* `fillna(0)`) but it's less robust. For example, if a cell `pd.to_numeric` receives is `''`, it would generate a `NaN`, then the *second* `fillna(0)` would catch it. The ground truth separates the `str.replace` and `pd.to_numeric(errors='coerce').fillna(0)` steps, which is clearer and more aligned with the logical flow.
3.  **Use of `sparse=False` for OneHotEncoder**: While `sparse=False` is used, it's generally good practice to explicitly convert the output of `OneHotEncoder` to a DataFrame with column names using `get_feature_names_out` as done in the ground truth. The generated code does this, which is good.

Overall, the code works and fulfills the main requirements, but some minor improvements to robustness and logical flow around data cleaning steps could make it more resilient to varied input data.",gpt_4o_mini,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the 'Video views' column
df['Video views'] = df['Video views'].replace(',', '', regex=True).astype(float).fillna(0)

# Ensure 'published' column is treated as numeric or integer type
df['published'] = pd.to_numeric(df['published'], errors='coerce')

# Group the data by the published year
grouped = df.groupby('published').agg(
    average_views=('Video views', 'mean...",7,"The generated code correctly addresses most of the requirements.

**Strong Points:**
*   Correctly loads the dataset.
*   Successfully cleans the 'Video views' column by removing commas, converting to float, and filling NaNs with 0.
*   Groups the data by the 'published' year and calculates both the average video views and the count of unique videos per year.
*   Creates two subplots with appropriate titles and labels for the axes.
*   Uses `plt.tight_layout()` for better plot presentation.

**Areas for Improvement (Why it's not higher):**
*   **""published"" column handling:** The prompt specifically mentions ""Ensure the `published` column (year) is treated as a numeric or integer type."" The dataset typically has a `publishedAt` column with full datetime strings, not just a `published` year column. The generated code attempts to convert a non-existent `published` column directly, which will lead to a `KeyError` or result in all NaNs if `published` refers to something else entirely in the actual dataset. The ground truth correctly infers this and handles the `publishedAt` column, extracting the year. This is a significant point of failure if the CSV structure isn't exactly as assumed (which it usually isn't for a simple column name like `published` for a year).
*   **Robustness:** No error handling for file not found or potential issues during data loading.
*   **Clarity on `published`:** The prompt mentions ""published column (year)"", which implies the *year part* of a publication date. The most common column name for a publication date/time in such datasets is `publishedAt` or `published_date`. The generated code assumes a simple `published` column exists which directly contains the year, which is less likely. The ground truth correctly assumes `publishedAt` and extracts the year.
*   **Minor Plotting Details:** While the plots are functional, the ground truth includes more specific formatting like setting `xticks` to show only integer years, handling scientific notation for views on the y-axis, and better overall plot aesthetics (e.g., specific colors, grid style).

Overall, it's a good attempt that gets most of the core logic right, but a critical assumption about the 'published' column name and content makes it less robust for a real-world `youtube_trendings.csv` dataset.",gpt_4o_mini,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
from nltk.corpus import stopwords

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Create title_word_count feature
df['title_word_count'] = df['Video'].str.split().str.len()

# Clean Video views column
df['Video views'] = df['Video views'].str.replace(',', '').astype(float).fillna(0)

# Calculate Pearson correlation coefficient
correlation = df['title_word_count'].corr(df['Video views'])
print(f""Pearson corre...",8,"The generated code successfully addresses most of the requirements.

**Strong points:**
- Correctly loads the dataset.
- Accurately creates `title_word_count`.
- Cleans the `Video views` column as specified (removes commas, converts to numeric, fills NaNs).
- Calculates and prints the Pearson correlation coefficient correctly.
- Filters the dataset for 2023/2024 videos and processes titles (lowercase, removes punctuation, removes stop words).
- Correctly identifies and prints the 5 most common words.
- Uses appropriate imports.

**Areas for improvement:**
- **NLTK Download:** The ground truth includes a robust way to ensure NLTK stopwords are downloaded, which the generated code omits. This could lead to a `ResourceCorpusReader` error if 'stopwords' are not already present. This is a common issue with `nltk` and handling it makes the code more robust.
- **Error Handling for File:** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is good practice. The generated code does not.
- **Handling of 'published' column for filtering:** The generated code `df['published'].str.contains('2023|2024')` works for this specific date format but is less robust than converting to datetime objects and extracting the year, as done in the ground truth. This is more resilient to variations in date string formats.
- **Clarity of Output:** The output for the correlation and most common words could be more descriptive, similar to the ground truth. For example, `print(""5 most common words in video titles:"", most_common_words)` is correct but less informative than `for word, count in most_common_words: print(f""- '{word}': {count} occurrences"")`.
- **Handling of non-string 'Video' titles:** The ground truth adds `df['Video'] = df['Video'].astype(str)` before splitting to prevent errors if some entries in the 'Video' column are not strings. While often not an issue with well-formatted CSVs, it adds robustness.
- **Removing numbers/non-alpha characters from words:** The ground truth includes `.isalpha()` in the list comprehension for word processing (`word.isalpha()`) to ensure only actual words (letters) are considered after punctuation removal, preventing numbers or empty strings from being counted. The generated code processes `\w` (word characters including numbers) and only checks against stop words, meaning words like ""2023"" or ""youtube2.0"" could remain. This is a minor refinement but contributes to cleaner word processing.",gpt_4o_mini,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score

# 1) Load data
data = pd.read_csv('mobiles_2025.csv')

# 2) Clean `Launched Price (USA)`
data['Launched Price (USA)'] = pd.to_numeric(
    data['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''), 
    errors='coerce'
)
data['is_expensive_usa'] = np.where(data['Launched Price...",7,"The generated code addresses most of the requirements but has several subtle issues that impact correctness and robustness.

**Strengths:**
*   Correctly loads the data.
*   Correctly cleans `Launched Price (USA)` and creates `is_expensive_usa`.
*   Correctly cleans `Mobile Weight`, `RAM`, `Battery Capacity`, and `Screen Size`.
*   Attempts to extract camera values, though the regex could be more robust.
*   Correctly one-hot encodes `Company Name` and `Processor`.
*   Correctly defines `X` and `y` and splits the data.
*   Correctly trains the `RandomForestClassifier`.
*   Correctly evaluates F1-score (macro) and ROC AUC score.

**Areas for Improvement / Issues:**

1.  **Target `is_expensive_usa` Creation and NaN Handling:**
    *   The ground truth explicitly states: ""Create a binary target `is_expensive_usa`: 1 if cleaned `Launched Price (USA)` > 1000, else 0. Drop rows where `is_expensive_usa` is NaN.""
    *   The generated code uses `np.where(data['Launched Price (USA)'] > 1000, 1, 0)`. This handles `True` and `False` cases, but if `data['Launched Price (USA)']` is `NaN`, `data['Launched Price (USA)'] > 1000` evaluates to `False`, so `np.where` assigns `0` to `is_expensive_usa`. This is *not* equivalent to dropping NaNs specifically from the target *after* its creation.
    *   The subsequent `data.dropna(subset=['is_expensive_usa'], inplace=True)` will *never* drop rows unless the original `Launched Price (USA)` was entirely unparseable *before* the `np.where` command, in which case `pd.to_numeric` would have made it NaN. But even then, `np.where` would turn that NaN into a `0`. This means rows with unparseable prices will incorrectly get `is_expensive_usa = 0` instead of being dropped. The task implicitly implies that unparseable prices should lead to dropping the row for the target, which the ground truth does by checking `pd.notna(x)` for the 0 case and then explicitly dropping target NaNs.

2.  **Camera Feature Cleaning Regex:**
    *   The `extract_camera_value` function uses `r'(\d*\.?\d+)'` which extracts *any* number. The requirement was to extract ""the first numerical MP value"". This means it could extract other numbers present in the string that are not MP values. For example, ""Dual 12MP + 5MP"" would just extract ""12"" and not necessarily ensure it's linked to `MP`. The more robust regex `r'(\d+(\.\d+)?)\s*MP'` from the ground truth ensures it's specifically an MP value.

3.  **Feature Cleaning Robustness (`.str.strip()` consideration):**
    *   When replacing 'g', ' GB', ' mAh', ' inches', the generated code does not use `.str.strip()` after replacement. While `pd.to_numeric` often handles leading/trailing spaces, it's good practice to `.strip()` for consistency and to avoid potential issues if other non-numeric characters remain.

4.  **NaN Handling for Features (X and y alignment):**
    *   The code does `X.dropna(inplace=True)` and then `y = y[X.index]`. This is generally acceptable for aligning `y` with `X` after dropping, but it's a bit less explicit or robust than concatenating `X` and `y`, dropping NaNs from the combined DataFrame, and then separating them again, which ensures consistent row drops across both. If `y` had NaNs that `X` didn't, or vice-versa (unlikely for `y` in this case due to prior cleaning but possible), this approach could lead to subtle misalignments or issues if not careful.

5.  **Missing `re` Import:**
    *   The generated code uses `re` implicitly in the `extract_camera_value` regex but doesn't import the `re` module. This would lead to a `NameError` if `re` were explicitly used (e.g., `re.search`), but `str.extract` is a pandas string method directly using regex, so it doesn't need the `re` import for itself, but it's good practice to import `re` if complex regex patterns are being defined and used within a function that hints at `re` module usage. For simple `str.replace` or `str.extract`, it's not strictly necessary, but when specifying ""extract the first numerical MP value"" and hinting at `re.search` capabilities, it's a common oversight. In this specific case, for `str.extract`, `re` import is not needed.

The main critical point is the target variable creation and its NaN handling, which doesn't perfectly match the explicit instruction (`Drop rows where `is_expensive_usa` is NaN`) if the original price was unparseable.",gpt_4o_mini,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Function to clean price columns
def clean_price(price):
    try:
        return pd.to_numeric(price.replace('PKR', '').replace('INR', '').replace('CNY', '')
                             .replace('USD', '').replace('AED', '').replace(',', '').strip(), errors='coerce')
    except Exception:
        return np.nan

# Apply cleaning function to relevant columns
price_columns = ['Launched Price...",9,"The generated code correctly addresses most of the task requirements.

**Strong Points:**
- **Correctness:** It successfully loads the data, defines a functional `clean_price` function that handles multiple currency symbols and commas, applies it to the correct columns, calculates the average global price, and identifies the top 5 models.
- **Conciseness:** The `clean_price` function is quite concise and uses `pd.to_numeric` with `errors='coerce'` effectively for error handling.
- **Use of Pandas features:** `df[price_columns].mean(axis=1)` and `nlargest` are used appropriately.

**Areas for Improvement (minor issues):**
1.  **Robustness of `clean_price` for non-string types:** The ground truth meticulously converts `price` to `str(price_str)` and handles `pd.isna(price_str)` at the beginning. The generated code directly calls string methods (`.replace`, `.strip`) on `price`. While `pd.to_numeric(..., errors='coerce')` will handle non-string inputs that can't be converted, it's safer to ensure the input is a string before calling string methods, especially if the column happens to contain numbers or NaNs directly sometimes without prior string conversion. However, for typical CSV data read by `pd.read_csv`, price columns often come in as object/string type, so it might not be a critical issue in practice.
2.  **Handling of `mean(skipna=True)`:** The ground truth specifically mentions and uses `skipna=True` for the mean calculation (`df[price_columns].mean(axis=1, skipna=True)`). While `pd.DataFrame.mean()` defaults to `skipna=True`, explicitly stating it or acknowledging it (as the ground truth does in its comments) reinforces the requirement to ""only consider countries for which price data is available (not NaN)"". The generated code implicitly relies on the default.
3.  **Error Handling for File Not Found:** The ground truth includes a `try-except FileNotFoundError` block for `pd.read_csv`, which is good practice for robust code, especially in a benchmark scenario where the file might not always be present. The generated code lacks this.
4.  **Display Format:** The prompt requests displaying ""Model Name and their calculated Average Global Price... sorted by Average Global Price descending"". While `nlargest` ensures the sort order for the top N, the output format is slightly different. The `top_5_models` DataFrame will be printed, which is correct, but sometimes the ""display"" requirement can imply a more tailored print statement. This is a minor stylistic difference.

Overall, the generated code performs very well and correctly solves the core problem. The minor points are about robustness, explicitness, and best practices rather than fundamental correctness issues for the given dataset.",gpt_4o_mini,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the columns
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''), errors='coerce')

# Drop rows with NaN valu...",4,"The generated code has significant issues regarding the plotting requirements.

**Correctness:**
The most critical error lies in the plotting approach. The task explicitly asks to ""Group the filtered Apple data by `Launched Year`"" and ""For each `Launched Year`, generate box plots to visualize and compare the distributions"". This means that for *each year*, there should be a set of three box plots (RAM, Battery, Price) showing the distributions *within that specific year*. The generated code attempts to plot all years on a single subplot for each metric, overlaying box plots based on `positions=[year]`. While this approach could be used for showing trends *across years* on a single plot, it does not fulfill the requirement of showing ""For each `Launched Year`"" and generating specific box plots *for that year*. The task implies separate figures or subplots per year, not aggregated plots across years.

Furthermore, the `xticks` line `plt.xticks(ticks=grouped.groups.keys())` only applies to the *last* `axes` object modified, and it's attempting to set ticks for a range of years on a plot that's actually intended to show yearly distributions *within* each year. Even if the plotting approach was to show trends across years, the x-axis would need to be re-evaluated to clearly distinguish the box plots per year.

The cleaning for 'RAM' and 'Battery Capacity' misses stripping whitespace, which means '16 GB' would become '16 ', and `pd.to_numeric` might still parse it, but it's not as robust as stripping. More importantly, for 'Launched Price (USA)', it doesn't chain `str.replace` calls. It should be chained as `str.replace('USD ', '').str.replace(',', '')`. The current code `str.replace('USD ', '').str.replace(',', '')` is correct in its second attempt (it seems I misread this during the initial evaluation, but the point stands about general robustness lacking for all cleaning steps). It also implicitly assumes that `pd.to_numeric` will handle leading/trailing spaces, which it generally does, but explicit stripping is better practice.

**Code Quality:**
- Imports are correct.
- Variable naming is generally good.
- There's no error handling for file loading.
- The comments are sparse compared to the ground truth and don't fully explain the logic, especially for the plotting.

**Completeness:**
- Fails to meet the core plotting requirement of generating box plots *for each launched year separately*. Instead, it tries to put all years on a single set of plots.
- Titles and labels are provided, but they are generic across all years, rather than being specific to ""Year X"" as requested by the prompt. The request was ""appropriated titles (indicating the year and metric)"", which the generated code does not do for the year.
- It doesn't handle all cleaning nuances (e.g., stripping whitespace for `RAM` and `Battery Capacity` after removing the unit).

Overall, the data loading and initial cleaning are partially correct, but the fundamental interpretation and implementation of the plotting requirement are incorrect, leading to a visual output that doesn't match the prompt's intent.",gpt_4o_mini,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import PolynomialFeatures, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier
from sklearn.svm imp...",5,"The generated code attempts to address many aspects of the prompt but falls short in implementing several ""advanced"" features and comprehensive evaluation metrics requested.

Here's a detailed breakdown:

**Correctness:**
*   **Data Preprocessing:** The feature engineering from names and creation of `FamilySize`/`IsAlone` is correctly implemented. However, the `ColumnTransformer` for imputation and encoding is defined *before* the engineered features are part of `X`. The custom transformer is missing. `X_transformed` loses column names, which affects interpretability tools later.
*   **Imputation:** Only `SimpleImputer` is used, explicitly missing the ""multiple imputation strategies"" like `IterativeImputer`.
*   **Polynomial Features/Interaction Terms:** These are completely absent from the preprocessing pipeline.
*   **Model Tuning:** `GridSearchCV` is used, but the prompt requested `GridSearchCV *or* RandomizedSearchCV`. The parameter grids are extremely limited, not representing a comprehensive tuning.
*   **Ensemble Models:** Stacking and Voting classifiers are defined, but the `final_estimator` for StackingClassifier is hardcoded to `XGBClassifier` without tuning, and the pipeline integration of these ensembles with the preprocessor is missing (they receive `X_transformed` directly, but the models in `best_models` were fit on it, means the models were fitted with original data.
*   **Evaluation:** Only `roc_auc_score` is used for evaluation during KFold. The prompt explicitly asked for ""comprehensive performance metrics"" (precision, recall, F1, ROC AUC, confusion matrix) and ""statistical significance tests"". `learning_curve` is implemented, but `validation_curve` is missing. Stratified K-fold is used but not dynamically applied to the pipelines.
*   **Feature Selection:** `RFE` and `SelectFromModel` are used, but the request was ""multiple techniques *and compare their impact*"". The comparison of impact (e.g., training a model on selected features and comparing performance) is missing. The `selected_features` are obtained, but not integrated into model training for impact comparison.
*   **Interpretability:** SHAP is applied, but LIME is entirely missing. SHAP is applied to `X_transformed` which is a NumPy array losing feature names, impacting the clarity of the SHAP plots. The `explainer` for SHAP is created in a way that generates warnings in recent SHAP versions if feature names are not provided.
*   **Statistical Tests:** Completely missing.

**Code Quality:**
*   **Imports:** Mostly correct, but `lime` is missing, and some other `sklearn` modules for advanced evaluation/imputation are missing.
*   **Structure:** Relatively linear. The preprocessing steps could be better encapsulated within a `Pipeline` for clarity and reproducibility across training, testing, and new data. The custom transformer for feature engineering is missing.
*   **Readability:** Generally readable, but the lack of a full pipeline for the entire process (preprocessing + model) makes the code less robust for testing and deployment.
*   **Modularity:** Not very modular. Feature engineering logic is mixed into the main script. The `ColumnTransformer` is defined but not explicitly integrated with the rest of the preprocessing as a cohesive pipeline.

**Completeness:**
*   **Comprehensive Data Preprocessing:**
    *   Advanced feature engineering: Partially done (FamilySize, IsAlone, Title), but **polynomial features, interaction terms** are explicitly missing. `Deck` and `Is_Married` from name were also missing compared to ground truth.
    *   Missing values: Only `SimpleImputer` is used (`mean`/`most_frequent`), **multiple imputation strategies (e.g., IterativeImputer)** are missing.
*   **Base Models & Tuning:** Random Forest, Gradient Boosting, SVM, Neural Network are included. Tuning uses `GridSearchCV` but for limited parameters and not `RandomizedSearchCV`. XGBoost is used only as a `final_estimator` in Stacking, not as a base model for tuning.
*   **Ensemble Classifiers:** Stacking and Voting are included.
*   **Feature Selection:** `RFE` and `SelectFromModel` are used, but **comparison of their impact** (by re-training models on selected features and evaluating performance) is missing. `SelectKBest` is also missing.
*   **Extensive Model Evaluation:**
    *   Stratified k-fold cross-validation: Done for base models, but not explicitly shown for ensembles in the same cross-validation loop.
    *   Learning curves: Implemented for one model.
    *   Validation curves: **Completely missing.**
    *   Comprehensive performance metrics: Only ROC AUC is used in KFold. `classification_report` is imported but not used. Precision, recall, F1, confusion matrix are not explicitly calculated or reported.
    *   **Statistical significance tests:** **Completely missing.**
*   **Model Interpretability:** SHAP is implemented but LIME is **completely missing**. SHAP explanation suffers from lack of feature names due to `X_transformed` being a NumPy array.

The solution provides a basic framework but misses many specific ""advanced"" requirements (e.g., multiple imputation, polynomial features, validation curves, statistical tests, LIME, robust comparison of feature selection impact, comprehensive metrics). The integration of preprocessing, feature engineering, and modeling into robust pipelines is also weaker than expected for an ""expert"" level.",gpt_4o_mini,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Define features and target
X = df[['Year', 'Runtime']]
y = df['Rating']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Linear Regression model
mod...",7,"The generated code correctly implements the core requirements of the task: loading data, selecting features/target, splitting data, training a Linear Regression model, and evaluating with R2 and MAE. All specified imports are present.

However, it lacks crucial data preprocessing steps that would be necessary for a robust solution, especially for a real-world dataset like `imdb_top_250.csv`. The `Runtime` column typically contains ' min' suffixes and might need cleaning (e.g., `df['Runtime'].str.replace(' min', '').astype(float)`). Also, it doesn't handle potential non-numeric values or missing data in `Year`, `Runtime`, or `Rating` columns, which could lead to errors or incorrect model training if the data is not perfectly clean. The ground truth code includes robust preprocessing to handle these potential issues (e.g., `errors='coerce'` in `pd.to_numeric` and `dropna`).

Furthermore, it does not include error handling for `FileNotFoundError` when loading the CSV, which is a good practice for user-friendly code.

To improve, add robust data cleaning and type conversion for `Year`, `Runtime`, and `Rating` columns, and consider general error handling for file loading.",gpt_4o_mini,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('imdb_top_250.csv')

# Select the features for clustering
X = data[['Rating', 'Runtime']]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
data['Cluster'] = kmeans.fit_predict(X_scaled)...",6,"The generated code correctly implements K-Means clustering, standardizes features, and produces a scatter plot with cluster labels. All required imports are present and correctly used.

However, a critical omission is the handling of the 'Runtime' column, which is typically a string (e.g., '142 min') in the `imdb_top_250.csv` dataset. The generated code directly selects 'Runtime' without any string-to-numeric conversion or error handling, which will cause a `TypeError` or `ValueError` when `pd.to_numeric` or `fit_transform` is implicitly called or when matplotlib attempts to plot non-numeric data. The ground truth code includes robust data cleaning for 'Runtime' and handles potential `FileNotFoundError` and missing columns, which the generated code lacks.

Additionally, the calculation of mean rating and runtime for each cluster in the generated code is done via a loop, which is less efficient and less ""pandas-idiomatic"" than using `groupby()` as shown in the ground truth. While functionally correct, it's a minor point for improvement. The scatter plot could also benefit from minor refinements like adding a `s` parameter for point size and `alpha` for transparency, and clarifying axis labels with units (e.g., 'Runtime (minutes)').

To improve, the code needs to:
1.  Add robust data cleaning for the 'Runtime' column to convert it from string to numeric (e.g., `'142 min'` to `142`).
2.  Add error handling for `FileNotFoundError` and missing columns.
3.  Use `df.groupby().mean()` for calculating cluster means for better efficiency and readability.",gpt_4o_mini,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter

# Ensure that the NLTK stopwords resource is available
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create a new feature called title_length
df['title_length'] = df['Title'].str.len()

# Calculate and print the Pearson correlation coefficient betwe...",7,"The generated code largely addresses the requirements but has some critical flaws in the preprocessing and correlation calculation that prevent it from being a perfect solution.

**Strong Points:**
*   Correctly loads the dataset.
*   Correctly calculates `title_length`.
*   Correctly uses `Counter` for word frequency.
*   Correctly removes stopwords.
*   The download of NLTK resources is handled, albeit slightly differently from the ground truth.
*   The overall structure is logical and easy to follow.

**Areas for Improvement (Why it's not higher):**

1.  **Punctuation Removal Inaccuracy:** The generated code uses `str.maketrans('', '', string.punctuation)` for punctuation removal. While this works for standard `string.punctuation`, it might miss some characters considered punctuation (e.g., em-dashes, certain special symbols not in `string.punctuation`). More importantly, `word_tokenize` is applied *after* this removal. `word_tokenize` often relies on punctuation to distinguish words (e.g., ""U.S.A."" or ""rock-n-roll""). Removing punctuation *before* tokenization can merge words that should be separate, leading to incorrect tokenization. The ground truth uses `re.sub(r'[^a-z0-9\s]', '', text)` which is more robust and specifically targets non-alphanumeric/whitespace characters, applied *after* lowercasing and *before* tokenization, which is a better approach for this kind of text cleaning task (though the ground truth then tokenizes, which assumes we want ""rocknroll"" not ""rock n roll""). However, the core issue with the generated code is that `word_tokenize` is applied *after* punctuation removal, which is less ideal.

2.  **Correlation Robustness:** The generated code calculates `correlation = df['title_length'].corr(df['Rating'])` directly. It does not explicitly handle potential non-numeric values or NaNs in the 'Rating' column, which could lead to an incorrect correlation or an error if the 'Rating' column is not purely numeric or contains NaNs. The ground truth robustly handles this by coercing 'Rating' to numeric (`pd.to_numeric(errors='coerce')`) and then dropping NaNs (`dropna`) before calculating the correlation. This makes the correlation calculation more reliable.

3.  **Missing `word_tokenize` download handling:** While `nltk.download('punkt', quiet=True)` is present, the prompt specifically asks to ensure `nltk.download('punkt')` is available, similar to stopwords. The ground truth demonstrates best practice with `try-except LookupError` for robustness. The generated code simply downloads unconditionally.

4.  **Minor Output Formatting:** The correlation output format slightly differs from the prompt's request for ""Pearson correlation coefficient between `title_length` and the `Rating` column"". The ground truth uses `f""Pearson correlation between title_length and Rating: {correlation:.4f}""` which matches the requested phrasing and provides better formatting.

These issues, particularly the punctuation handling and correlation robustness, affect the correctness and quality of the output, hence the score of 7.",gpt_4o_mini,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import numpy as np

# Load the dataset
data = pd.read_csv('penguins.csv')

# Define features and target
X = data[['culmen_length_mm', 'culmen_dep...",7,"The generated code correctly addresses most of the requirements.

**Strong Points:**
- Correctly loads the dataset.
- Defines X and y correctly.
- Uses `SimpleImputer` with 'median' for numerical features, which is good practice.
- Uses `SimpleImputer` with 'most_frequent' for categorical features, which is also correct.
- Correctly uses `OneHotEncoder` for categorical features.
- Correctly uses `StandardScaler` for numerical features.
- Correctly uses `LabelEncoder` for the target variable.
- Initializes `RandomForestClassifier` with specified parameters.
- Uses `cross_val_score` for 5-fold cross-validation and prints the mean accuracy.

**Areas for Improvement / Missing Aspects:**
1.  **Handling missing values in the target `y`:** The prompt implies handling missing values in features. The ground truth code robustly handles cases where `species` might be `NaN`. The provided solution does not explicitly address this. While `fit_transform` on `y` might implicitly drop NAs, it's safer to explicitly handle it and ensure `X` aligns with `y`.
2.  **Order of Imputation and Dropping NA in `island`:** The ground truth code handles the `island` NA dropping *after* its mode imputation, but *also* considers the edge case where *all* `island` values might be NA, in which case mode imputation would fail. The generated code `X = X.dropna(subset=['island'])` is applied *after* mode imputation *and* outside the specific logic that checks if the mode itself resulted in NA or if all were NA. This can lead to issues if `most_frequent` imputation doesn't fix all NAs (e.g., if the mode itself was NA, or if there were no valid values to get a mode from) or if the `island` column was entirely NA initially. The ground truth's handling of the ""if mode imputation results in NA for island, drop those rows"" is more precise.
3.  **Combining features:** The method of combining `X_scaled` and `X_encoded` using `scipy.sparse.hstack` is functional but `pd.DataFrame` concatenation is often more robust and maintainable, especially if column names are needed later (though for model training it won't matter immediately). A `ColumnTransformer` (as typically part of a `Pipeline`) is the most robust and elegant way to orchestrate these steps, keeping track of column names and applying transformations to specific subsets of columns, which the code has imported but not utilized.
4.  **Clarity of Feature Tracking:** The `X` dataframe is repeatedly modified in place (`X[num_features] = num_imputer.fit_transform(X[num_features])`, etc.), which can be less clear than building up preprocessed components or using `ColumnTransformer` / `Pipeline`.
5.  **`OneHotEncoder(drop='first')`:** While often used to avoid multicollinearity, the prompt did not specify `drop_first`. The ground truth used `drop_first=False`. This is a minor deviation but could affect the model slightly.
6.  **`reset_index`:** If rows are dropped (as in `X.dropna(subset=['island'])`), the indices of `X` and `y` might become misaligned. The ground truth code handles `reset_index` after dropping rows, which is good practice to ensure `X` and `y` always have aligned and consecutive indices. The provided code does not ensure this, which could lead to issues, especially if the target `y` also needs to be adjusted after dropping. Before `LabelEncoder` on `y`, `y` needs to correspond to `X` after dropping. This is a potential bug if `y` had missing values that were not dropped, or if `y` wasn't adjusted to match the dropped rows from `X`.",gpt_4o_mini,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# 1) Load the dataset.
data = pd.read_csv('penguins.csv')

# 2) Select the features.
features = data[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]

# 3) Drop any rows with missing values.
data_cleaned = data.dropna(subset=features.columns)

# 4) Standardize the features using Standar...",7,"The generated code correctly addresses most of the task requirements: loading data, selecting features, handling NAs (with a minor issue), standardizing, performing K-Means, applying PCA, and generating two scatter plots.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly selects the specified features.
*   Applies `StandardScaler`, `KMeans` (with correct parameters), and `PCA` (with correct number of components).
*   Generates two plots with correct PCA components on axes.
*   Plots are titled and axes labeled.

**Areas for Improvement (leading to a score of 7 instead of higher):**
*   **NA handling and data alignment:** The crucial detail is in step 3. The ground truth carefully aligns `species` with the `data_cleaned` after dropping NAs. The generated code's NA handling is slightly off. It does `data_cleaned = data.dropna(subset=features.columns)`. This will drop rows from the original `data` dataframe based on NAs in the selected features. However, the `features` variable itself was created as `data[['culmen_length_mm', ...]]` *before* this `dropna` operation. While `scaled_features` is correctly based on `data_cleaned[features.columns]`, the `species` used for the second plot is `data_cleaned['species']`, which *does* align. So, while the line `features = data[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]` is slightly confusing in its naming (`features` here is a DataFrame, not just a list of column names), the eventual `data_cleaned[features.columns]` correctly selects the columns from the cleaned dataframe. The code *does* end up using the correct `species` from `data_cleaned`. This is more a point of clarity and robustness in the `dropna` step. The groundtruth ensures `species` is part of the `df_selected` so that `dropna` applies directly to the dataframe that will be `df_cleaned`, ensuring perfect alignment.
*   **Plotting `species` labels:** While the code uses `c=species.astype('category').cat.codes`, which will correctly color points based on species, the `cmap='viridis'` is not ideal for categorical data. `viridis` is a sequential colormap designed for numeric data. For categorical data, a qualitative colormap like `tab10`, `Set1`, or `Paired` would be more appropriate for distinguishing discrete categories. The `KMeans` plot usage of `viridis` is acceptable as K-Means clusters are often treated as ordinal or semi-quantitative, but for `species`, clarity dictates a categorical colormap.
*   **Legends:** The generated plots lack legends. While K-Means plot might implicitly have legend through `c` (if a colorbar was added), for species, a legend mapping colors to names is crucial for interpretation. The ground truth uses `seaborn` which automatically handles legends for `hue`. For `matplotlib.pyplot.scatter`, you'd typically have to iterate through unique categories and plot them separately to generate a legend or use a custom legend. This makes the plots less informative.
*   **Code Structure/Clarity:** The ground truth code has clearer step numbers and comments, making it easier to follow the execution flow corresponding to the task requirements. The generated code is functional but less verbose in its self-documentation.
*   **Error Handling:** No error handling for `FileNotFoundError`. The ground truth includes a `try-except` block.

Overall, the code is almost entirely correct in its execution logic but misses some finer points regarding plotting best practices (colormaps, legends) and robust data handling/clarity in the NA step.",gpt_4o_mini,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer

# Load the dataset
df = pd.read_csv('penguins.csv')

# Impute missing values
imputer_num = SimpleImputer(strategy='mean')
df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm'...",6,"The generated code addresses most of the requirements but has several issues in correctness and completeness.

**Correctness Issues:**
1.  **Imputation Order and `drop_first` for 'sex' encoding:** The `island` NA rows are dropped *after* the numerical and 'sex' imputations. If a row had an `island` NA, its numerical/sex values would still have contributed to the mean/mode calculation, which is incorrect. `island` NA rows should be dropped first.
2.  **Incorrect `X` construction**: The lines `X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]` followed by `X = pd.get_dummies(X, columns=['island'])` are problematic. The `island` column was never put *into* `X` to begin with, so `pd.get_dummies(X, columns=['island'])` will raise a `KeyError` because `island` is not in `X`. The `X['sex'] = df['sex']` line also adds `sex` to `X`, but `island` is completely missed from the feature set `X`.
3.  **`drop_first=True` for One-Hot Encoding**: The task explicitly asks for feature importances of *all* features, including those derived from one-hot encoding. `drop_first=True` removes one category to prevent multicollinearity, which is common in modeling but prevents getting an importance for that dropped category. For feature importance analysis as requested, all dummy variables should be present.
4.  **Box Plot Grouping and Title**: The box plot command `sns.boxplot(data=df, x='sex', y='flipper_length_mm', hue='species')` displays `flipper_length_mm` grouped by `sex` with `species` as hue. The requirement was ""For each `species`, create box plots to visualize the distribution of `flipper_length_mm` (using imputed data) grouped by the imputed `sex` column."" This implies `species` on the x-axis and `sex` as hue, or separate plots for each species, which the ground truth correctly implements. The title is also ""Distribution of Flipper Length by Sex and Species"" which is not what was requested.

**Completeness Issues:**
1.  The `island` column was not included in the feature set `X` due to the incorrect `X` construction.
2.  While the code extracts and prints feature importances, the `feature_names` for `island` are missing because `island` was not correctly added to `X`.

**Minor Improvement Areas:**
*   It's good that `SimpleImputer` is used, but direct `fillna` methods are also perfectly acceptable and sometimes more readable for simple cases.
*   The `train_test_split` import is unused.
*   The `numpy` import is unused.

Overall, the code attempts to fulfill the requirements but fails on critical steps like feature definition and plotting specifications, which are central to the task.",gpt_4o_mini,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer
from sklearn.impute import SimpleImputer

# Step 1: Load the dataset
df = pd.read_csv('penguins.csv')

# Step 2: Impute numerical columns with KNN imputer
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
knn_imputer = KNNImputer(n_neighbors=3)
df[numerical_cols] = knn_imputer.fit_transform(df[numerical_cols])

# Step 3: Impute categorical column 'sex' with mode
sex_imputer = SimpleImpu...",9,"The generated code successfully completes almost all aspects of the task. It loads the dataset, correctly applies KNN imputation to the specified numerical columns, and imputes the 'sex' column with its mode. The use of `SimpleImputer` for the mode imputation is a valid and good approach. The final verification step (`df.isna().sum()`) is also correctly implemented.

Strengths:
- Correct library imports.
- Clear and concise steps for each part of the task.
- Correct application of `KNNImputer` with the specified `n_neighbors`.
- Correct application of `SimpleImputer` with `strategy='most_frequent'` for mode imputation.
- Correct verification of remaining NA values.

Area for minor improvement:
- The task explicitly asked to ""Print the number of remaining NA values in each column after imputation to verify."" and the generated code prints `na_counts`. While this is correct, adding a small descriptive print statement before it (e.g., ""Number of remaining NA values after imputation:"") would make the output slightly more user-friendly and align even closer with the prompt's explicit wording for verification.
- The ground truth includes error handling for `FileNotFoundError`. While not strictly required by the prompt's 'Provide only Python code' and 'Assume the CSV is in the current directory' clauses, it's good practice for robust code. However, given the prompt constraints, its absence is not a major deduction.",gpt_4o_mini,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Step 1: Load the dataset
df = pd.read_csv('penguins.csv')

# Step 2: Filter the dataset for 'Adelie' and 'Gentoo' species
filtered_df = df[df['species'].isin(['Adelie', 'Gentoo'])]

# Step 3: Handle NA values in the 'culmen_length_mm' column
filtered_df = filtered_df.dropna(subset=['culmen_length_mm'])

# Step 4: Perform an independent two-sample t-test
adelie = filtered_df[filtered_df['species'] == 'Adelie']['culmen_length_mm']
gentoo = f...",9,"The generated code correctly addresses all the requirements of the task. It successfully loads the data, filters for the specified species, handles NA values appropriately, performs the t-test, and interprets the p-value to state the conclusion.

Strengths:
- **Correctness:** All steps are implemented accurately, leading to the correct t-test result and conclusion.
- **Conciseness:** The code is straightforward and to the point, avoiding unnecessary complexity.
- **Completeness:** All 6 specified steps in the task description are covered.

Areas for Improvement (minor, for a perfect score):
- **Error Handling:** The generated code lacks explicit error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. While the task didn't explicitly ask for it, robust code usually includes this.
- **Readability/Clarity:** Adding comments for each step, similar to the ground truth, would enhance readability, especially for someone unfamiliar with the code.
- **Modularity:** Wrapping the logic in a function would make the code reusable and testable, which is good practice for larger scripts or applications.
- **Output Formatting:** The ground truth provides more verbose and formatted output, which is helpful for understanding the processing steps and intermediate results (e.g., number of rows after filtering/dropping NAs, mean values). However, the generated code satisfies the printing requirement.",gpt_4o_mini,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

# Step 1: Load the dataset
data = pd.read_csv('penguins.csv')

# Step 2: Define features and target
features = ['culmen_length_mm'...",6,"The generated code correctly addresses most of the requirements but has a significant issue with the preprocessing steps, particularly how it merges the imputation and one-hot encoding with the feature selection and `train_test_split`.

**Strengths:**
*   Correctly loads the data.
*   Correctly defines features and target.
*   Correctly handles numerical NA imputation with the median.
*   Correctly handles 'sex' NA imputation with the mode.
*   Correctly uses `LabelEncoder` for the target variable `species`.
*   Correctly splits data with stratification and `random_state`.
*   Correctly trains `RandomForestClassifier` with specified parameters.
*   Correctly makes predictions and prints accuracy and classification report.

**Weaknesses and Areas for Improvement:**

1.  **Preprocessing Order and Data Leakage Concerns:** The major flaw is how preprocessing is integrated.
    *   The generated code performs all imputation and one-hot encoding on the *entire* `data` DataFrame *before* the `X` and `y` separation and `train_test_split`.
    *   When `pd.get_dummies` is called, it directly modifies the `data` DataFrame.
    *   This means when `X = data[features]` is executed, the `features` list no longer correctly reflects the columns available in `data` after one-hot encoding. `island` and `sex` columns technically no longer exist, having been replaced by their one-hot encoded versions (e.g., `island_Biscoe`, `sex_Male`). This will lead to a `KeyError` or incorrect feature selection if the `features` list is not updated or if the `X` DataFrame is not properly constructed from the `get_dummies` output. The `drop_first=True` argument used in `get_dummies` further complicates this as it changes the exact column names. Because of this, the code produced an error `KeyError: ""['island', 'sex'] not in index""` when executed.
    *   A more robust approach (as shown in the ground truth) is to use `ColumnTransformer` within a `Pipeline`. This ensures that transformations are applied *after* the train-test split (or within a pipeline that first splits then transforms), preventing data leakage from the test set into the training data during preprocessing steps like imputation and one-hot encoding `fit` calls.

2.  **Lack of `Pipeline` or `ColumnTransformer` for Robust Preprocessing:** The prompt specifically implies a robust preprocessing setup by asking for median imputation for numerical, mode for 'sex', and one-hot encoding for 'island' and 'sex'. While the individual steps are performed, they are done in a less robust and potentially error-prone manual way that doesn't scale well or prevent data leakage. The use of `ColumnTransformer` (as in the ground truth) is the standard and recommended way to handle heterogeneous data types and transformations in scikit-learn.

3.  **Ambiguity in `drop_first=True`:** While `drop_first=True` is often used to avoid multicollinearity, the prompt did not specify this, and it deviates slightly from a direct one-hot encoding. This will result in fewer columns than expected for the categorical features.

To improve the score, the code should integrate the preprocessing within an `sklearn` `Pipeline` and `ColumnTransformer` to ensure correct application and prevent data leakage, aligning with standard ML best practices. This also resolves the `KeyError` issue by correctly handling the dynamic column names after one-hot encoding.",gpt_4o_mini,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Calculate the Pearson correlatio...",9,"The generated code correctly defines the DataFrame, calculates the specified Pearson correlation coefficients, and prints them. It also correctly determines and prints the feature with the stronger absolute correlation.

**Strong points:**
- Correctly initializes the DataFrame as specified.
- Accurately calculates Pearson correlation coefficients using the `.corr()` method (which defaults to Pearson).
- Correctly prints the calculated correlations.
- Successfully identifies the feature with the stronger *absolute* correlation using a concise conditional expression.

**Areas for minor improvement:**
- While `method='pearson'` is the default for `.corr()`, explicitly specifying it (`df['PerformanceScore'].corr(df['YearsExperience'], method='pearson')`) can make the code more explicit and readable for those less familiar with `pandas` defaults.
- The output format for the final comparison could be slightly more informative, similar to the ground truth, by printing the actual absolute correlation values for both to provide context, especially if the absolute values are very close. For example, instead of just saying ""X is stronger"", it could say ""X (0.Y) is stronger than Y (0.Z)"".
- No specific handling for the edge case where the absolute correlations are exactly equal, though for floating-point numbers, this is rarely an issue. The current code would default to ""TrainingHours"" if they are equal, which is a minor detail.

Overall, it's a very good and correct solution that directly addresses all requirements.",gpt_4o_mini,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test
from statsmodels.formula.api import ols
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.neighbors import NearestNeighbors
import pymc3 as pm
from scipy import ...",4,"The generated code attempts to address many of the requirements but fails significantly in correctness and completeness, particularly in the core survival analysis and advanced statistical modeling sections.

**Fatal Flaws:**

*   **Incorrect Kaplan-Meier and Cox Model `T` and `E`:** The prompt explicitly stated, ""Implement Kaplan-Meier survival analysis treating the voyage as time-to-event data (with survival as event indicator)"". The ground truth correctly interprets this by setting `T` (duration) to a constant (e.g., 1) and `E` (event) as `1 - Survived` (death).
    *   The generated code uses `df['Survived']` for both `durations` and `event_observed` in K-M and `Survived` as the `duration_col` and `event_col` in Cox. This is fundamentally incorrect. `Survived` is a binary outcome (0 or 1), not a duration. This renders the Kaplan-Meier and Cox models meaningless and incorrectly applied for survival analysis. A binary variable cannot be a duration.
*   **Propensity Score Matching (PSM) implementation is incomplete and incorrect**:
    *   The prompt asked for PSM to ""analyze treatment effects (e.g., passenger class impact)"". The generated code calculates propensity scores for `Survived` (the outcome), not for a treatment variable (like Pclass). PSM is designed to balance covariates between treatment and control groups *before* analyzing an outcome.
    *   It then calculates `NearestNeighbors` on `X_test` based on `y_test == 1` (survived patients), which doesn't perform matching between a treated group and a control group based on propensity scores. There's no actual matching of treated and control subjects or subsequent analysis of treatment effect.
*   **Bayesian Analysis Error**: The Bayesian model attempts to include `df['Pclass'].astype(float)` directly in the linear predictor without proper encoding (it's loaded as a string and converted to float, but this isn't sufficient for categorical representation in a regression model in this context). The model also doesn't fit the 'survival' aspect well conceptually; it's a simple linear regression model on `Survived` (which is binary here) rather than a probabilistic model for survival factors more akin to a logistic regression or a Bayesian survival model.
*   **Missing or Incorrect Advanced Statistical Methods:**
    *   **Multivariate Analysis:** ""Multivariate analysis"" in this context typically refers to analyzing multiple dependent variables or more complex relationships than just a single OLS model. While `OLS` is a valid regression, ""multivariate analysis"" usually implies techniques like MANOVA, PCA, or more complex regression setups. An OLS model on a binary `Survived` outcome is ill-suited; Logistic Regression would be more appropriate for binary outcomes.
    *   **Model Diagnostics:** The single `residplot` is a very basic diagnostic for OLS, but it's not comprehensive for a binary outcome regression and entirely misses diagnostics for non-OLS models like the Cox model (e.g., proportional hazards assumption, which `lifelines` supports but was incorrectly used).
    *   **Sensitivity Analysis**: The ground truth provided a good example of sensitivity analysis for imputation. The generated code's only sensitivity analysis is `cph.check_assumptions()`, which is a diagnostic check, not sensitivity analysis of model parameters or assumptions.

**Minor Issues / Areas for Improvement:**

*   **Imports:** The generated code imports `statsmodels.formula.api.ols` but not `statsmodels.api` which is more standard for full regression.
*   **Completeness:** While some requirements are superficially touched upon (e.g., 'Bayesian analysis'), their implementation is deeply flawed, as detailed above.
*   **Code Structure:** Missing comments, especially for complex sections.
*   **Data Preparation:** While some imputation is done, `df['Pclass']` is converted to string and then directly to float in the Bayesian model, which is an unusual and problematic way to handle a categorical variable for regression. One-hot encoding is necessary for most models as shown in the ground truth. Sex also remains unencoded for the Cox model.
*   **Bootstrap Resampling:** The bootstrap solely calculates confidence intervals for the mean of 'Fare', which is a very limited application. The prompt asked for ""confidence intervals and statistical inference"" related to model parameters or effects, suggesting more complex bootstrapping of model coefficients or CIs for treatment effects.

The fundamental misunderstanding of survival analysis (`T` and `E`) and the incorrect application of PSM are critical failures that make the code not fit for the stated purpose.",gpt_4o_mini,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# Create the DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 
                  'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 
                'Math', 'English', 'Science', 'Math', 'English', 
                'Math', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82...",9,"The generated code correctly performs all the required tasks:
1.  **DataFrame Creation:** The DataFrame is created correctly.
2.  **Pearson Correlation:** The correlation coefficient is calculated and printed accurately.
3.  **Scatter Plot:** The scatter plot is generated with the correct axes, colored by subject, and includes a title and labels. It successfully uses seaborn for coloring, which was hinted at in the prompt.
4.  **Overall Grade Assignment:** The `Overall_Grade` column is correctly added based on the specified criteria.
5.  **Grade Distribution:** The count of students in each grade category is correctly calculated and printed.

**Strong Points:**
*   Correct use of pandas for DataFrame creation and manipulation.
*   Correct use of `scipy.stats.pearsonr` for correlation.
*   Effective use of seaborn for the scatter plot, including `hue` for coloring by subject.
*   Clear and functional assignment of grades using a function and `.apply()`.
*   Proper use of `value_counts()` for grade distribution.

**Areas for Minor Improvement (to reach a perfect 10):**
*   **Verbosity in Output:** The ground truth code includes helpful print statements to indicate which step is being performed. While not strictly required by the prompt, it enhances user experience and clarity for multi-step tasks.
*   **`np.select` vs. `.apply()` for Grading:** While the `.apply()` method with a custom function works perfectly fine for assigning grades, `np.select` (as used in the ground truth) is generally more performant for large DataFrames and often considered a more ""pandas-idiomatic"" way for applying multiple conditional logic, especially if the conditions are mutually exclusive. For this small dataset, the performance difference is negligible, but it's a good practice point.
*   **Legend Placement:** The legend placement `bbox_to_anchor=(1.05, 1), loc='upper left'` in the ground truth places the legend outside the plot area, which can be useful to prevent it from overlapping with data points, especially with more categories. The generated code places it inside, which is acceptable but could lead to overlap in some cases.
*   **Error Handling/Robustness:** The ground truth briefly mentions checking for missing values before correlation, which is a good practice, though not necessary for this specific clean dataset.

Overall, the generated code is highly functional and accurate, meeting almost all requirements excellently. The minor suggestions are more about best practices and slightly more robust or user-friendly output rather than critical errors.",gpt_4o_mini,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Creating the DataFrame
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', 
                            '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']),
    'Ticker': ['TECHCO'] * 10,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0],
    '...",9,"The generated code correctly performs all the requested tasks: creates the DataFrame, calculates daily returns and SMAs, generates a well-labeled plot, and identifies the bullish crossover dates. The code is clean and well-structured, using appropriate pandas and matplotlib functions.

Strengths:
- Correct DataFrame creation and index setting.
- Accurate calculation of daily returns and both SMAs.
- Plot generation is complete with correct labels, title, and legend.
- The identification of bullish crossovers is correctly implemented using boolean indexing and `shift()`.

Areas for minor improvement:
- The `bullish_signals.index` output for the crossover dates could be slightly more user-friendly, perhaps by iterating and printing each date individually as done in the ground truth, for better readability.
- The use of `marker='o'` for the plots is not strictly necessary and sometimes makes the lines look less smooth, especially for time-series data. The ground truth opted for cleaner lines. This is a minor stylistic choice and doesn't affect correctness.",gpt_4o_mini,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics'] * 7,
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST001']
}

df = pd.DataFrame(data)

# 1) Calculat...",9,"The generated code correctly addresses all requirements of the task. It successfully creates the DataFrame, calculates and visualizes total sales revenue by product, calculates the average order value, and identifies customers with more than one purchase. The plot is generated as requested.

Strong points:
- **Correctness:** All calculations and data manipulations are accurate.
- **Completeness:** All four parts of the task (DataFrame creation, sales visualization, average order value, repeat customers) are implemented.
- **Clarity:** The code is well-structured and easy to understand.
- **Plotting:** The horizontal bar chart is correctly generated with appropriate labels and title.

Minor points for improvement (not affecting the score significantly for this task, but good practice):
- **Category data definition:** Using `['Electronics'] * 7` for the 'Category' column is a clever shortcut and works correctly.
- **Print statements:** The ground truth code includes more descriptive print statements and formatting (e.g., separating sections with `---` and `===`) which improves readability for a comprehensive analysis output. The generated code's print statements are more concise but still clear.
- **Figure size:** While not strictly required by the prompt, adding `plt.figure(figsize=(...))` before plotting can improve the readability of the generated graph, especially with more data points.
- **`tight_layout()`:** `plt.tight_layout()` is good practice to prevent labels from overlapping and ensuring all elements fit within the figure area.

Overall, the generated code is very good and fully satisfies the requirements.",gpt_4o_mini,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd

# Define the initial DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Create DeviceType_Numeric feature
df[""DeviceType_Numeric""] = df[""DeviceType""].map({""Desktop"": ...",9,"The generated code correctly solves the task. It successfully defines the DataFrame, creates both `DeviceType_Numeric` and `PagesPerMinute` features, correctly handling division by zero for the latter, selects the specified feature set, and prints the head of the resulting DataFrame.

The only minor point of improvement is the handling of infinities in `PagesPerMinute`. While `float('inf')` works, using `np.inf` (which would require `import numpy as np`) is generally more idiomatic and robust when working with numerical data in pandas/numpy contexts. However, the current implementation still achieves the desired outcome. The output also doesn't exactly match the ground truth's print statement (""Head of the prepared feature DataFrame (X):""), but this is a very minor stylistic difference.",gpt_4o_mini,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

# Define the data
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", 
                  ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 2...",9,"The generated code correctly defines the DataFrame, calculates the mean and standard deviation for ""TemperatureCelsius"", identifies outliers based on the 1.5 standard deviation threshold, and prints the required information for each outlier. The code is clean, well-structured, and directly addresses all parts of the task.

One minor area for improvement, though not strictly required by the prompt, would be to convert the 'Timestamp' column to datetime objects using `pd.to_datetime()`. While the current string format works for printing, converting it is generally good practice when working with time-series data in pandas and allows for more robust time-based operations if the task were to expand. The ground truth code includes this, which is a good example of robust coding. However, for the given task, it works perfectly without it. Another minor point is that the ground truth provided more detailed print statements including formatting. The generated code is functionally correct but could be slightly more user-friendly in its output presentation.",gpt_4o_mini,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [10, 5, 25, 15, 8, 30, 9,...",9,"The generated code accurately defines the DataFrame, calculates the EngagementScore, and correctly identifies and prints the PostType with the highest average EngagementScore for each platform. The approach using `idxmax()` after grouping is a concise and efficient way to achieve the desired result, potentially more robust than sorting and dropping duplicates for this specific problem (though both are valid). The output formatting is slightly less refined than the ground truth in terms of floating-point precision, but the core functionality is identical and correct. Adding `:.2f` to the printed `EngagementScore` would make it perfect, aligning with typical presentation standards for floats.",gpt_4o_mini,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""C...",9,"The generated code correctly performs all the required steps: defines the DataFrame, calculates ""LengthOfStay"", computes average stay and patient count per diagnosis, prints the summary, and identifies the diagnosis with the longest average stay.

**Strong points:**
- **Correctness:** All calculations and aggregations are accurate. The `idxmax()` method is correctly used to find the diagnosis with the longest average stay.
- **Conciseness:** The code is direct and efficient.
- **Completeness:** All aspects of the task prompt are addressed.

**Minor areas for improvement (to achieve a perfect 10):**
- **Output formatting:** While it prints the summary, the ground truth uses f-strings and loops to create a more readable, formatted table output for the per-diagnosis summary (Step 4 in the ground truth). The generated code just prints the `diagnosis_summary` DataFrame directly, which is functional but less tailored for human readability as requested in ""Print this information (Diagnosis, Average LengthOfStay, Patient Count)"".
- **Clarity of final print statement:** The final print statement for the longest stay is good, but could explicitly mention ""Average Length of Stay"" in the output for better clarity, similar to the ground truth's `f""Average Length of Stay for '{longest_diagnosis}': {max_avg_stay:.2f} days""`.
- **Modularity:** Encapsulating the logic within a function (as in the ground truth) would improve code organization and reusability, although not strictly required by the prompt's output format, it's good practice.",gpt_4o_mini,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",9,"The generated code correctly defines the DataFrame, calculates the ""WeightedRating"", adds it as a new column, and identifies the movie with the highest ""WeightedRating"". All requirements of the prompt are met accurately.

The one minor point for improvement, which prevents a perfect 10, is that the final print statement, while it displays the correct information, prints a pandas Series object directly. The prompt specifically asked to ""Print the 'Title' and 'WeightedRating' of the movie with the highest 'WeightedRating'"", and the ground truth handles this by printing formatted strings (`print(f""Title: {title_highest}"")` and `print(f""WeightedRating: {weighted_rating_highest:.6f}"")`). While the generated code's output is functionally correct and easily readable, explicitly printing the values in a more descriptive format would align perfectly with the output style implied by ""Print the 'Title' and 'WeightedRating'"".

The code is clean, concise, and uses efficient pandas operations. The `idxmax()` function is correctly applied to find the highest weighted movie.",gpt_4o_mini,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 10000, 8000]
}

df = p...",9,"The generated code correctly defines the DataFrame and accurately calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"". It also correctly creates and prints the resulting grouped DataFrame, fulfilling all task requirements.

The use of `.reset_index()` is a good practical choice as it turns 'Brand' back into a column, which is often more convenient for subsequent operations or display than having it as an index.

The only minor area for improvement is in the naming of the aggregated columns. The ground truth code used named aggregations (e.g., `Average_PriceUSD=(""PriceUSD"", ""mean"")`) which results in clearer column names in the output DataFrame. The generated code's output columns will simply be ""PriceUSD"" and ""Mileage"" in the grouped dataframe, which might be less explicit about them being averages. However, this is a minor style preference and doesn't impact correctness or functionality.",gpt_4o_mini,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 8...",10,"The generated code is excellent. It correctly defines the DataFrame, adds the 'TotalStats' column, identifies the Pokemon with the highest total stats, and calculates the average speed per 'Type1' for each group. The code is concise, efficient, and directly addresses all parts of the prompt. The use of `df[['HP', 'Attack', 'Defense', 'Speed']].sum(axis=1)` for 'TotalStats' is a clean and flexible way to sum multiple columns. The use of `idxmax()` to find the Pokemon with the highest stats is also very efficient. All requirements are met with high code quality.",gpt_4o_mini,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
from scipy.spatial.distance import cdist
from scipy.cluster.hierarch...",5,"The generated code attempts to address many aspects of the prompt but falls significantly short in implementation details, advanced features, and comprehensive analysis, leading to a low score.

Here's a breakdown:

**1. Correctness (4/10):**
*   **Feature Engineering:** The feature engineering is extremely basic compared to the prompt's requirement for ""advanced features including social network analysis (family groups, ticket sharing), socioeconomic indicators, and text-based features from names and tickets."" The code only creates `Family_Size`, `IsAlone`, and `Ticket_Group_Size`, and a simplified `Title` extraction. Missing are `FarePerPerson`, `TicketPrefix`, `CabinDeck`, `HasCabin`, and various interaction features.
*   **Preprocessing:** `LabelEncoder` is used for `Embarked`, `Sex`, and `Title`. This is generally inappropriate for features that don't have an ordinal relationship (e.g.,Embarked locations or titles). `OneHotEncoder` (as in ground truth) is more suitable.
*   **Missing Values:** Uses `features.fillna(features.mean())` which is simplistic and can be problematic (e.g., mean for `Age` or `Fare` might not be robust, and it applies mean across all columns, even categorical ones where it's nonsensical). The ground truth specifically addresses `Age` with median and `Embarked` with mode, and handles `Fare` before `FarePerPerson`.
*   **Clustering:** While K-means, hierarchical, GMM, and DBSCAN are used, the choice of `n_clusters=3` for all is arbitrary, ignoring the requirement to ""Determine optimal number of clusters using multiple validation metrics.""
*   **Optimal Cluster Determination:** Only the Elbow method and Silhouette score are plotted, and only for K-Means. The prompt explicitly asks for ""multiple validation metrics (silhouette score, elbow method, gap statistic, calinski-harabasz index)"" and for ""multiple clustering algorithms."" Gap statistic is entirely missing.
*   **DBSCAN:** `eps` and `min_samples` are fixed values without any reasoning or attempt at hyperparameter tuning, which is crucial for DBSCAN.
*   **UMAP Import:** `import umap` is vague. `umap.umap_ as umap` is the correct way to import the `UMAP` class.
*   **Scoring:** Silhouette and Calinski-Harabasz are used, but Davies-Bouldin and Gap Statistic are missing.

**2. Code Quality (6/10):**
*   **Imports:** Mostly correct, but `cdist` and `dendrogram` from `scipy.spatial.distance` and `scipy.cluster.hierarchy` are imported but not used. `LabelEncoder` is used incorrectly.
*   **Structure:** The code flows logically from data loading to feature engineering, preprocessing, clustering, and visualization.
*   **Readability:** Generally readable.

**3. Completeness (3/10):**
*   **Feature Engineering:** Weak, as noted above. Many advanced features are missing.
*   **Multiple Clustering Algorithms:** Implemented, but not with optimal cluster determination or robust parameter tuning, especially for DBSCAN.
*   **Optimal Cluster Determination:** Only partially implemented (limited metrics, only for K-Means). Gap statistic is missing.
*   **Dimensionality Reduction:** PCA, t-SNE, and UMAP are used, but UMAP import is slightly off (`import umap` instead of `umap.umap_ as umap` for the class, although `umap.UMAP` still worked for some versions). The visualizations of clusters are basic.
*   **Analyze Cluster Characteristics:** Basic `groupby().mean()` is used. The ground truth provides more detailed insights like median/mode for specific features and survival rates.
*   **Cluster Stability Analysis and Consensus Clustering:** Mentioned in a comment as a ""placeholder"" but entirely absent from the code. This is a critical deficiency given the prompt's explicit requirement.
*   **Comprehensive Visualizations:** Lacks the depth and variety requested (e.g., feature importance for each segment, violin plots, heatmaps of scaled feature means). Only basic scatter plots and bar plots are provided.
*   **External Validation:** Only a simple box plot for survival outcomes is given, which is a start but not as thorough as needed for ""validating clustering results using external criteria.""

In summary, the generated code provides a skeletal solution that touches upon many requirements but fails to implement them with the necessary depth, robustness, and advanced techniques specified in the prompt. The lack of advanced feature engineering, proper categorical encoding, comprehensive cluster validation metrics (especially gap statistic and detailed evaluation for all algorithms), and the complete absence of stability/consensus clustering are major shortcomings.",gpt_4o_mini,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dystopian""],
    ""Stock"": [...",8,"The generated code correctly performs all the required tasks, including DataFrame creation, calculating book counts and average prices per author, and identifying the author with the most books in stock.

**Strong Points:**
- **Correct DataFrame Creation:** The DataFrame is correctly initialized as per the task description.
- **Accurate Aggregation:** The `groupby().agg()` method is used effectively to calculate the count of books and average price for each author.
- **Correct Author Identification for Most Stock:** The logic to find the author with the most stock is clever and correct, utilizing `transform('sum')` to get the total stock per author back into the original DataFrame structure, and then finding the author with the maximum of these sums. This works perfectly.

**Areas for Improvement (to achieve a higher score):**
- **Output Formatting for Author Stats:** While `print(author_stats)` correctly displays the information, the ground truth uses an f-string loop to format the output for ""Author Book Statistics"" in a more human-readable and specific way (e.g., ""Author: X | Books: Y | Avg Price: $Z.YY""). The generated code's printout is less user-friendly for this specific part, simply showing the DataFrame representation.
- **Clarity of Output:** The ground truth includes clear headers and separators (""--- Author Book Statistics ---"", ""--- Author with Most Books in Stock ---"") which makes the output much easier to follow and understand. The generated code lacks these, making the transition between the two printed sections abrupt.

Overall, the code is functionally correct and demonstrates a good understanding of pandas operations. The main improvements would be in presentation and user-friendliness of the output.",gpt_4o_mini,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklear...",4,"The generated code attempts to address many aspects of the prompt but fails significantly in implementing several key requirements, especially concerning advanced data preprocessing, model optimization, ensemble techniques, and comprehensive evaluation.

**Primary Reasons for Failure & Critical Missing/Incorrect Implementations:**

1.  **Data Preprocessing (Critical Failure):**
    *   **Multiple Imputation Strategies:** The code uses `SimpleImputer(strategy='mean')`, which is *not* a multiple imputation strategy (like MICE/IterativeImputer as in the ground truth). This is a fundamental misunderstanding of the requirement.
    *   **Outlier Detection & Treatment:** While it attempts Z-score based outlier removal, it removes rows (`data_imputed = data_imputed[z_scores < 3]`) rather than capping/transforming, which significantly alters the dataset's size and can lead to data leakage if not handled carefully before splitting. The ground truth uses Winsorization (capping).
    *   **Advanced Feature Engineering:** It uses `PolynomialFeatures(interaction_only=True)`, only generating interaction terms, NOT polynomial *and* interaction terms as requested. The ground truth explicitly uses `interaction_only=False` for polynomial features.
    *   **Scaling:** The generated code completely *misses* feature scaling (e.g., `StandardScaler`) after imputation and before model training. This is crucial for models like SVM and Neural Networks. The ground truth correctly implements this within a `Pipeline`.

2.  **Model Optimization (Significant Weakness):**
    *   **Bayesian Optimization:** The prompt explicitly mentions ""GridSearchCV *or* Bayesian optimization."" The generated code only uses `GridSearchCV` and doesn't even attempt Bayesian optimization.
    *   **Parameter Grids:** The parameter grids used are very limited (e.g., SVM `C` only, MLP `hidden_layer_sizes` only). This suggests superficial optimization compared to ""optimized multiple base models.""

3.  **Ensemble Models (Incomplete & Incorrect):**
    *   **Stacking and Blending:** The generated code *completely skips* Stacking and Blending. It only implements a basic `VotingClassifier`. This is a major omission given the prompt's explicit requirement for ""stacking, voting, and blending techniques.""

4.  **Advanced Evaluation (Mostly Missing or Superficial):**
    *   **Learning Curves:** Not implemented.
    *   **Validation Curves:** Not implemented.
    *   **Bootstrap Sampling:** Not implemented for confidence intervals. The ground truth correctly uses `resample` for this.
    *   **Comprehensive Metrics:** While it prints `accuracy_score`, `roc_auc_score`, and `classification_report`, it doesn't provide the detailed breakdown for all metrics as in the ground truth or structured for a dashboard.

5.  **Model Interpretability (Partial):**
    *   **SHAP/LIME:** It uses SHAP but doesn't implement LIME. The SHAP plots generated (`shap.summary_plot`) are rudimentary compared to the Ground Truth which also includes `shap.dependence_plot` and handles tree vs. non-tree explainers.

6.  **Statistical Significance Testing:**
    *   It performs `ttest_rel` but the approach is flawed. It compares *all* pairs of models using the training data's cross-validation scores (`cross_val_score(model, X, y, cv=skf)` where X, y are the full dataset), which isn't ideal. The ground truth focuses on comparing against the *best* model using CV scores from the *training* process to avoid data leakage from the test set for this comparison, and it also clones models properly for each fold.

7.  **Model Comparison Dashboard (Missing):**
    *   The prompt asks for a ""comprehensive model comparison dashboard with performance metrics, feature importance, and prediction confidence intervals."" The generated code only prints evaluation reports and a single SHAP plot summary. There is no dashboard, no confidence intervals integrated into performance display, and no feature importance visualization *across models* (only SHAP for one model).

8.  **Model Calibration (Partial & Misleading):**
    *   It uses `CalibratedClassifierCV`, which is a good start. However, it uses `method='sigmoid'`, while `isotonic` is often preferred when data is sufficient, as in the ground truth.
    *   The ""Probability Calibration Plot"" is just a histogram of probabilities, *not* a proper reliability diagram showing predicted probability vs. true fraction of positives, which is the standard calibration plot. The ground truth correctly implements a reliability diagram.

**Code Quality and Structure Issues:**
*   Lack of comments in critical sections explaining logic.
*   The `evaluate_model` function uses `print` statements, making it unsuitable for collecting results programmatically for a dashboard.
*   The use of `HalvingGridSearchCV` was imported but not used.
*   No robust error handling for file not found (e.g., `try-except`).
*   Global variable modification for `data_imputed` during outlier treatment is a bad practice.
*   The custom `OutlierCapper` class and `ColumnTransformer` from the ground truth were absent, simplifying and reducing the extensibility/correctness of the preprocessing pipeline.",gpt_4o_mini,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import KNNImputer, IterativeImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import IsolationForest, LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from statsmodels.stats.multicomp import pairwise_tukeyhsd
...",4,"The generated code attempts to address several requirements but falls short in implementation, correctness, and completeness for an ""expert"" level task.

**Primary Reasons for Failure & Critical Missing/Incorrect Aspects:**

*   **Missing Data Imputation Comparison:** While three imputation methods are mentioned, the comparison is superficial (`isnull().sum().sum()`). There's no actual assessment of their *effectiveness* on the imputed columns (e.g., comparing distributions, variance, or impact on subsequent model performance) as requested. MICE specifically refers to the IterativeImputer in `sklearn`, not a separate method like `df_mice = pd.DataFrame(mice_imp.fit_transform(df), columns=df.columns)`. The ground truth correctly shows how to use `IterativeImputer` for MICE.
*   **Bayesian Logistic Regression (PyMC3):** This implementation is severely flawed.
    *   `with pm.Model() as model: pm.Fit() pm.sample(...)` is syntactically incorrect for PyMC3. `pm.Fit()` is not a valid function call in this context for model fitting.
    *   There is no model definition, no priors defined, no likelihood, and no observed data passed to the model. This is a critical failure.
    *   The `return_inferencedata=False` is deprecated.
    *   No analysis of the PyMC3 results (trace plots, summary, posterior predictive checks) is performed, which is fundamental to Bayesian modeling.
*   **Generalized Additive Models (GAM):** **Completely missing.** This was a specific requirement.
*   **Advanced Hypothesis Testing:**
    *   **Multivariate ANOVA (MANOVA):** The provided `sm.formula.ols('potability ~ C(feature1) + C(feature2)', data=df).fit()` and `sm.stats.anova_lm()` performs a standard ANOVA, not a *Multivariate* ANOVA (MANOVA). MANOVA tests multiple dependent variables against a categorical independent variable, which `statsmodels` has a dedicated class for (`statsmodels.multivariate.manova.MANOVA`). The provided code also refers to non-existent `feature1`, `feature2`.
    *   **Permutation Tests:** The implementation is acceptable for a univariate permutation test, but the prompt asks for *advanced* hypothesis testing implying a broader application or more complex scenarios.
    *   **Bootstrap Confidence Intervals:** The implementation is acceptable for a univariate CI.
*   **Robust Statistical Methods:** **Completely missing.** This was a specific requirement (""handle outliers and non-normal distributions""). The ground truth uses `statsmodels.robust.robust_linear_model.RLM` as an example.
*   **Factor Analysis:** **Completely missing.** Only PCA is implemented for dimensionality reduction.
*   **Model Diagnostics:**
    *   **Residual Analysis:** Only a QQ-plot of residuals is shown. It's unclear what `residuals = y_test - log_reg.predict(X_test)` represents for a classification model; for logistic regression, basic residuals are not appropriate. Deviance residuals or Pearson residuals are needed.
    *   **Influence Measures:** The `sm.OLS(y_train, X_train).fit().get_influence()` attempts to use OLS for a logistic regression problem, which is incorrect for assessing influence in a GLM context. Influence measures for GLMs are different.
    *   **Goodness-of-Fit Tests:** **Completely missing.** No Hosmer-Lemeshow, deviance, AIC/BIC comparison, or ROC curve visualizations/metrics are included.
*   **Advanced Statistical Visualizations:**
    *   While some visualizations (Pairplot, Correlation Heatmap, Box plots) are present, the requirement for ""diagnostic dashboards"" and ""leverage plots"" was partially addressed but not fully realized or correctly implemented for all models. The overall visualization suite is basic compared to the ""advanced"" requirement.

**General Code Quality & Structure:**

*   **Imports:** Some necessary imports are missing (e.g., `pygam`, several specific `statsmodels.graphics` or `statsmodels.genmod` imports).
*   **Completeness:** Many explicitly requested features are missing or incorrectly implemented, as detailed above.
*   **Clarity and Structure:** The code lacks comments, clear sectioning, and print statements to explain what's happening at each step, making it less readable for a complex task.
*   **Error Handling:** No robust error handling (e.g., for file not found).
*   **Reproducibility:** No `np.random.seed()` or `random.seed()` is set for various stochastic processes (imputation, permutation, bootstrap, PyMC3 sampling), which impacts reproducibility.

In summary, the code provides a superficial attempt at some parts of the task but fails to deliver on the ""expert"" level requirements for advanced statistical modeling, particularly in Bayesian modeling, GAMs, robust methods, and comprehensive model diagnostics for GLMs.",gpt_4o_mini,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import silhouette_samples
from sklearn.manifold...",5,"The generated code attempts to address several requirements but falls short in correctness, completeness, and overall sophistication compared to the ground truth and the task description.

**Correctness:**
- **Missing Value Handling:** The code completely overlooks handling missing values in the dataset. This is a critical first step in most data analyses, especially for clustering, and is correctly handled by the ground truth.
- **Feature Engineering:** The feature engineering is extremely basic (""Water_Quality_Index"" as a simple average, ""Risk_Score"" as a max). The ground truth provides much more elaborate and domain-specific feature engineering (pH deviation, hardness quality, contamination risk, chemical balance ratios, and a more comprehensive WQI). The generated code's 'Nitrates', 'Sulfate', 'Fluoride' for 'Risk_Score' are hardcoded and some columns might not even exist in the dataset (e.g., 'Nitrates', 'Fluoride' are not in `water_potability.csv`).
- **Optimal Cluster Determination:** The generated code plots `silhouette_scores`, `davies_bouldin_scores`, and `calinski_scores` to determine the optimal number of clusters, but it incorrectly assumes that plotting these together is an ""Elbow Method."" The Elbow Method is specifically for SSE (Sum of Squared Errors), which is not calculated or plotted. More importantly, the plotting combines all three scores on the same Y-axis without normalization, making the plot uninterpretable. The `evaluate_clustering` function also calls `kmeans.fit_predict(X)` multiple times, which is inefficient.
- **DBSCAN Parameter Tuning:** DBSCAN is notoriously sensitive to `eps` and `min_samples`. The generated code uses arbitrary values (0.5 and 5) without any method for estimating or tuning them, which is a major flaw for DBSCAN application. The ground truth correctly demonstrates a method for estimating `eps` using the k-distance graph.
- **Cluster Stability Analysis:** The bootstrap resampling and consensus clustering implementations are very basic. The consensus matrix calculation, in particular, is not robust; `cluster_stability[i][j]` implies direct indexing even though the sample is taken with `frac=0.8`, meaning the indices won't align directly. The ground truth provides a more robust (though still simplified) approach for consensus that iterates over the full data and aggregates co-occurrences of points _within_ the consensus runs.
- **External Validation:** The code only includes a comment about external validation and assumes an `external_labels.csv` file, which is not provided, making this section incomplete and non-executable. The ground truth at least mentions how this *would* be done and discusses referring to external domain knowledge.
- **3D Visualization:** The 3D PCA plot uses `labels` as the Z-axis, which is incorrect. `labels` are categorical cluster assignments, not a continuous third dimension of the data. The ground truth correctly uses the third PCA component.

**Completeness:**
- **Clustering Algorithms:** While it lists K-means, Hierarchical, DBSCAN, and GMM, it *only* performs K-means training and visualization. Spectral Clustering is missing.
- **Dimensionality Reduction:** Factor Analysis and UMAP are completely missing from the implementation. The ground truth includes all specified DR techniques.
- **Cluster Validation Techniques:** Gap Statistic is missing. Elbow method for SSE is missing (and incorrectly labeled).
- **Cluster Stability Analysis:** The ground truth includes bootstrap resampling and consensus clustering with a proper dendrogram visualization, while the generated code's implementation is flawed and lacks the dendrogram.
- **Anomaly Detection:** The generated code performs anomaly detection using `IsolationForest`, but not the cluster-based anomaly detection (distance from centroid) as requested and implemented in the ground truth.
- **Dashboard:** The ""dashboard"" aspect is minimal, lacking the comprehensive interactive visualizations (beyond basic scatter plots) and radar charts shown in the ground truth. The radar chart function is broken in the generated code (`data.columns[:-1]` will exclude the 'cluster' column, but then `data.loc[cluster_id].values.flatten().tolist()` might not get the correct features for radar).
- **Domain Knowledge Integration:** While it mentions domain knowledge in comments, it doesn't demonstrate its use in interpreting results beyond basic statements, unlike the ground truth which explicitly references WHO guidelines and uses them to interpret the cluster profiles.

**Code Quality:**
- **Imports:** While generally good, some imports are present but unused (e.g., `GridSearchCV`, `make_pipeline`, `train_test_split`, `silhouette_samples`).
- **Function Naming:** `evaluate_clustering` is confusing as it returns the silhouette, DB, and CH scores for K-means, not a general evaluation for any clustering algorithm.
- **Variable Names:** `features` and `features_scaled` are okay but generic.
- **Redundant Calculations:** As noted, `kmeans.fit_predict(X)` is called multiple times unnecessarily within the loop for optimal K determination.
- **Plotting:** Plots are static `matplotlib` plots, largely lacking the interactivity requested for a ""dashboard"" (which `plotly` excels at). The radar chart implementation is flawed and would need significant correction to work as intended.
- **Comments/Explanations:** Comments are sparse and don't elaborate on the complexity or rationale behind certain choices (e.g., arbitrary DBSCAN parameters).

In summary, the generated code provides a skeletal attempt that touches upon some requirements but lacks the depth, accuracy, and completeness expected from an ""expert"" level task. Many critical components are either missing, incorrectly implemented, or overly simplified.",gpt_4o_mini,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
import numpy as np

# Load dataset
data = pd.read_csv('winequality-red.csv')

# Calculate the correlation matrix
correlation_matrix = data.corr()

# Find the correlation of each feature with 'quality'
quality_corr = correlation_matrix['quality']

# Identify top 3 positively and negatively correlated features
top_positive = quality_corr.nlargest(4)[1:]  # Top 3 exc...",8,"The generated code correctly addresses most of the task requirements. It loads the data, calculates correlations, identifies true strongest positive and negative correlations with 'quality', generates a heatmap, and creates scatter plots with regression lines.

**Strong Points:**
- Correctly uses `pandas` for data loading and correlation calculation.
- Appropriately uses `seaborn` for the heatmap and `matplotlib` for plotting.
- Correctly identifies the top 3 positive and negative correlations (though the `nlargest(4)[1:]` is a clever workaround for the self-correlation of 'quality' and `nsmallest(3)` correctly gets the negative ones).
- Uses `sklearn.linear_model.LinearRegression` correctly for fitting the regression lines and `predict` for plotting them.
- Provides all required plots.

**Identified Areas for Improvement:**
- **Robustness/Error Handling:** The ground truth includes `try-except` blocks for file loading, which is good practice. The generated code lacks this, making it less robust if the file is missing.
- **Clarity in Top 3:** While `nlargest(4)[1:]` works, explicitly dropping 'quality' from `quality_correlations` before taking `head(3)` and `tail(3)` as in the ground truth is arguably clearer and more conventional.
- **Plotting Aesthetics/Completeness:**
    - The heatmap in the generated code doesn't explicitly rotate x-axis labels, which can lead to overlap for many features.
    - The scatter plots do not explicitly show the R-squared value on the plot, which was included in the ground truth's regression line label (though the problem description didn't explicitly ask for R-squared on the plot, it's a good touch for regression analysis).
    - The scatter plots use `plt.scatter` and `plt.plot` directly. While functional, `sns.regplot` or `sns.lmplot` often handle both scatter and regression line plotting more concisely and can add confidence intervals which is a common practice. Using `sklearn.linear_model` explicitly and then plotting `predict` is also a valid approach, but `scipy.stats.linregress` directly gives the slope and intercept which can be slightly simpler for plotting the line manually.
    - Lack of `tight_layout()` for plots means potential clipping of labels.
- **Clarity in Regression Plotting:** For the scatter plots, using `X_pos = data[[most_positive_feature]]` results in a DataFrame. While `sklearn` can handle this, `plt.scatter` and `plt.plot` typically expect 1D arrays or Series for `x` and `y`. It works due to implicit conversion, but explicitly converting to a Series (e.g., `data[most_positive_feature]`) can be cleaner. Also, creating a distinct range `x_pos = np.array([df[most_positive_feature].min(), df[most_positive_feature].max()])` and then plotting `intercept + slope * x_pos` for the regression line (as done in the ground truth) results in a smooth line across the data range, whereas plotting `model.predict(X)` where `X` are the original feature values might make the line appear less continuous if the data points are sparse or have large gaps. However, for a simple linear model, both approaches achieve a visually similar result.

Overall, it's a very good solution that fulfills the core requirements. The improvements are mainly about robustness, minor plotting details, and slightly more conventional data handling.",gpt_4o_mini,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import chi2_contingency, mannwhitneyu

# Load dataset
df = pd.read_csv(""video_games_sales.csv"")

# Clean missing values
df.dropna(subset=['Global_Sales', 'Publisher', 'Genre', 'Platform', 'Year'], inplace=True)

# 1. Test if Nintendo games have significantly higher global sales compared to all other publishers combined
nintendo_sales = df[df['Publisher'] == 'Nintendo']['Global_Sales']
other_sales = df[df['P...",6,"The generated code addresses most of the requirements but has several issues that affect its correctness, robustness, and completeness compared to the ground truth.

**Correctness & Robustness Issues:**
1.  **Chaining `dropna`:** The generated code uses `df.dropna(subset=['Global_Sales', 'Publisher', 'Genre', 'Platform', 'Year'], inplace=True)`. While `Year` correctly refers to `Year_of_Release`, this single `dropna` means that if any of these columns have a NaN, the row is dropped. The ground truth code handles `Global_Sales` first, then `Year_of_Release`, then `Publisher`, `Genre`, `Platform` separately. This might or might not lead to the same final dataset, but depending on the order of importance of columns, it's a detail. More importantly, the `Year` column is not explicitly converted to `int` before use for the Mann-Whitney U test, which could cause issues if it's float or string. The `errors='coerce'` in `pd.to_numeric` for `User_Score` in the ground truth is good practice, though this column isn't directly used in the tests.
2.  **`Year` vs `Year_of_Release`:** The CSV usually contains `Year_of_Release`, not `Year`. The generated code uses `df['Year']` which would likely cause a `KeyError`. The ground truth uses `Year_of_Release` consistently.
3.  **Chi-square contingency table:** The filtering for the chi-square test is flawed. `pd.crosstab(df[df['Genre'].isin(top_genres)]['Genre'], df[df['Platform'].isin(top_platforms)]['Platform'])` will create a contingency table from two independently filtered series, which is incorrect. It should be filtered first, then crosstabbed for the intersection of top genres AND top platforms. The ground truth handles this correctly: `df_filtered_chi2 = df[df['Genre'].isin(top_5_genres) & df['Platform'].isin(top_5_platforms)]` then `pd.crosstab(df_filtered_chi2['Genre'], df_filtered_chi2['Platform'])`. This is a critical error for the chi-square test.
4.  **T-test assumptions (`equal_var`):** The ground truth uses `equal_var=False` (Welch's t-test) which is generally recommended when population variances are not assumed to be equal, making the test more robust. The generated code uses `equal_var=True` (Student's t-test by default). This slight difference can impact the p-value.
5.  **Alternative Hypotheses:** The ground truth correctly uses `alternative='greater'` for the Nintendo test as the prompt states ""significantly *higher*"", while the generated code uses the default `two-sided`, which is incorrect for that specific part of the problem. For ""significantly different"", `two-sided` is correct. The prompt also states ""games released after 2000 have significantly *different* sales"", for which `two-sided` for Mann-Whitney U is correct, and the generated code gets this right.

**Completeness Issues:**
1.  **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code does not.
2.  **Detailed Interpretation:** The ground truth provides much more detailed interpretations of the results, including repeating the Null and Alternative Hypotheses, displaying the T-statistic/Chi2-statistic/U-statistic, and a more verbose explanation of the conclusion (e.g., ""Reject the Null Hypothesis (p < ALPHA). Interpretation: ...""). The generated code only prints the p-value and a boolean, which is less informative for evaluating the results of a statistical test.
3.  **Data Cleaning Summary:** The ground truth includes print statements for DataFrame shape before and after cleaning and a null count summary, which aids in understanding the data. The generated code lacks this.
4.  **Checks for insufficient data:** The ground truth includes `if len(group) < 2` checks before performing tests, which prevents errors on small or empty groups. The generated code omits these crucial checks.

**Code Quality:**
1.  **Imports:** The generated code specifically imports `chi2_contingency` and `mannwhitneyu` from `scipy.stats` which is good practice, though `stats.ttest_ind` is still used, implying `from scipy import stats` is still needed.
2.  **Comments/Readability:** The ground truth is much more heavily commented and uses clear section headers, improving readability and maintainability. The generated code is more concise but less self-explanatory.

Overall, while the generated code attempts to solve all parts of the problem, the critical error in the chi-square test, lack of robustness checks, less detailed output, and minor assumption disagreements make it less reliable and complete compared to the ground truth.",gpt_4o_mini,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values
df.dropna(subset=['Global_Sales'], inplace=True)

# Crea...",7,"The generated code correctly addresses most of the task requirements, including loading data, creating the target variable, encoding features, building a Random Forest model, and evaluating it. It also attempts to display feature importance.

However, there are several areas for improvement:

1.  **Missing Value Handling:** The ground truth code specifically handles missing values for 'Year', 'Publisher', and regional sales columns, which are crucial for feature creation and model training. The generated code only drops `Global_Sales` NaNs. This omission could lead to errors (e.g., `df['Year']` might contain NaNs, causing issues with `2023 - df['Year']`) or less robust models. The `astype(str)` for LabelEncoder is a workaround for NaNs in categorical columns but doesn't explicitly address the underlying missing data problem.
2.  **Feature Selection:** The ground truth explicitly defines the `features` list, ensuring only relevant columns are used and providing clarity. The generated code uses `X = df.drop('High_Sales', axis=1)`, which is less precise as it might implicitly include columns that should have been dropped (e.g., `Name`, `Year`, `Global_Sales` *before* dropping them, potentially leading to issues if those drops happen later or differently).
3.  **Missing Values in Created Features:** The `Years_Since_Release` calculation `2023 - df['Year']` will fail if `df['Year']` contains NaN values, which is not handled beyond the initial `Global_Sales` drop. The ground truth handles this more thoroughly by dropping NaNs from 'Year' earlier and converting it to integer.
4.  **Stratified Split:** The ground truth uses `stratify=y` in `train_test_split`, which is important for multi-class or imbalanced binary classification to ensure that both training and testing sets have a representative proportion of the target classes. The generated code misses this, which could lead to skewed splits, especially if 'High_Sales' is imbalanced.
5.  **Robustness (File Not Found):** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust. The generated code lacks this.
6.  **Minor code quality:** The `np.where` for creating `High_Sales` is fine but `(df['Global_Sales'] > 1.0).astype(int)` is often slightly more idiomatic and direct in pandas. The visual display of feature importance is a nice addition, but the prompt only asked to ""display feature importance"" not necessarily visualize it.",gpt_4o_mini,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler

# Load dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values
df.fillna({'NA_Sales': 0, 'EU_Sales': 0, 'JP_Sales': 0, 'Other_Sales': 0, 'Global_Sales': 0}, inplace=True)
df.dropna(subset=['Name', 'Platform', 'Year'], inplace=True)

# Cr...",6,"The generated code addresses most of the task requirements but has several issues regarding correctness, robustness, and completeness.

**Strengths:**
*   **Imports:** Proper imports are included.
*   **Structure:** The code generally follows the requested structure of the task.
*   **Feature Engineering:** Most of the requested engineered features are attempted, along with correlation analysis, PCA, and polynomial features.

**Areas for Improvement / Issues:**

1.  **Missing Value Handling:**
    *   The ground truth uses `SimpleImputer` more robustly for different data types ('Year' with median, 'Publisher'/'Genre' with mode). The generated code uses `fillna` with 0 for sales, which is problematic as 0 sales might not be a good imputation value and can negatively impact calculations like ratios.
    *   `dropna(subset=['Name', 'Platform', 'Year'])` is used, which might discard too much data, especially if `Year` has missing values (which it often does in such datasets). The ground truth's median imputation for 'Year' is better.

2.  **Engineered Features - Correctness:**
    *   **Sales_Ratio_NA_EU:** `df['EU_Sales'].replace(0, np.nan)` followed by division will result in `NaN`s where `EU_Sales` was zero, rather than handling potential division by zero gracefully (e.g., by adding a small epsilon or assigning 0/NaN in such cases if that's the intent). The ground truth's approach using `(df['EU_Sales'] + 1e-6)` is more robust for direct calculation.
    *   **Genre_Market_Share:** `transform(lambda x: x / x.sum())` calculates individual game sales as a proportion of *that genre's total sum*, not the total market share of the *genre itself* relative to global sales. The ground truth correctly aggregates genre sales first then divides by total global sales to get the genre's market share.
    *   **Sales_Momentum:** `df['Year'].map(df.groupby('Year')['Global_Sales'].median())` is a compact way to do the median mapping, but the functionality seems correct.

3.  **Correlation Analysis:** `df.corr()` without explicitly selecting numerical columns (like the ground truth does by checking `select_dtypes(include=np.number)`) might include non-numeric columns if pandas can coerce them, or fail, or silently ignore them, leading to less precise analysis.

4.  **PCA:** `scaled_data = scaler.fit_transform(df.select_dtypes(include=[np.number]))` is good, but if `Rank` (or other non-sales numeric columns) exists, it should ideally be excluded as a direct feature for PCA to focus on predictive numerical features. The ground truth is more deliberate in selecting features.

5.  **Polynomial Features:**
    *   The sales columns are correctly selected.
    *   The `poly_features_df` is created but **not explicitly merged back into the original `df`**, which is a critical missing step for ""analyzing their impact"" or using them downstream.

6.  **Interaction Features:**
    *   `df['Year_Genre_Interaction'] = df['Year'].astype(str) + '_' + df['Genre']` creates a *string concatenation* of Year and Genre. The task typically implies *numerical interaction features* (e.g., Year multiplied by one-hot encoded Genre features), which are often used in regression models. The ground truth correctly generates numerical interaction features by multiplying `Year` with `get_dummies` features. This is a significant conceptual difference.

7.  **Outlier Handling:**
    *   The generated code uses a *row-wise deletion* approach (`df = df[~((...) | (...)).any(axis=1)]`). This means if *any* of the specified sales columns in a row has an outlier, the *entire row* is dropped. This is a very aggressive form of outlier handling that can lead to significant data loss, especially in a dataset with many outliers.
    *   The request was to ""handle outliers using IQR method,"" which more commonly implies capping or transforming rather than outright deletion of entire rows. The ground truth's approach of **capping** (setting values to the lower/upper bounds) is much more robust and preserves data.
    *   Only sales columns are considered for outlier handling, while other engineered numerical features (like `Sales_Ratio_NA_EU`, `Publisher_Avg_Sales`, etc.) which might also contain outliers are ignored. The ground truth included a broader set of relevant numerical columns.

8.  **Output/Clarity:** The ground truth also provides informative print statements and plots to show the progress and results at each step, which is good practice for data analysis tasks. The generated code lacks these.",gpt_4o_mini,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Replace zero values with NaN for specific columns and perform median imputation
cols_to_impute = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[cols_to_impute] = df[cols_to_impute].replace(0, np.nan)
df[cols_to_impute] = df[cols_to_impute].fillna(df[cols_to_impute].median())

# 1. T-test for gluc...",7,"The generated code correctly performs most of the requested statistical tests and handles the data cleaning as specified. The interpretation of results is present, although basic.

However, there are several areas for improvement:

1.  **Robustness in Data Loading & Cleaning**: The generated code assumes `diabetes.csv` exists without error handling. The ground truth includes a `try-except` block for `FileNotFoundError`.
2.  **Median Imputation Application**: The generated code uses `df[cols_to_impute] = df[cols_to_impute].fillna(df[cols_to_impute].median())`. While this fills NaNs, it calculates the median for each column across the *entire dataframe* first and then fills. The ground truth code iterates through each column individually and calculates the median for *that specific column* before filling, ensuring the median is correctly derived from the column's data. This is a subtle difference but might be more robust if there are complexities. More importantly, the ground truth correctly points out columns like 'Pregnancies' and 'Outcome' can legitimately be 0 and should *not* be imputed, while the generated code's broad imputation might inadvertently affect these if `cols_to_impute` were defined differently. The ground truth correctly defines `cols_to_impute` for biologically implausible zeros.
3.  **T-test `equal_var` parameter**: The generated code uses the default `equal_var=True` for the `ttest_ind`. The ground truth explicitly uses `equal_var=False` (Welch's t-test), which is often a safer choice when equal variances are not assumed or tested. This reflects a more statistically sound approach.
4.  **BMI Category Binning**: The BMI category definitions in the generated code are slightly inconsistent with standard definitions (e.g., 25-29.9 for overweight, >=30 for obese). Specifically, ""Normal: 18.5-24.9"" and ""Overweight: 25-29.9"" as implemented might have slight overlaps or gaps at boundaries depending on floating point precision and exact interpretation. The ground truth's `pd.cut` with `right=False` (e.g., `[a, b)`) is a more standard and robust way to define bins, explicitly addressing the boundaries.
5.  **ANOVA Empty Group Handling**: The generated code directly passes lists to `f_oneway`. If any of the BMI categories happen to be empty after filtering (which is unlikely in this dataset but good practice), `f_oneway` might raise an error. The ground truth includes a check `if len(glucose_groups) < 2:` to handle cases where ANOVA cannot be performed due to insufficient groups.
6.  **Interpretations**: While the generated code prints p-values and a generic ""Significant/Not Significant"" message, the ground truth provides a more detailed interpretation for each test, explaining what rejecting/failing to reject the null hypothesis means in the context of the specific variables being tested. This adds significant value for a data science benchmark.
7.  **Extra Imports**: `matplotlib.pyplot` and `seaborn` are imported but not used, making them unnecessary imports.",gpt_4o_mini,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classificatio...",6,"The generated code addresses most of the requirements but has several crucial issues that prevent it from being production-ready or completely correct.

**Strengths:**
- Correctly loads and preprocesses data (handling zeros with median).
- Creates the specified new features (`Glucose_BMI_ratio`, `Age_group`, `Insulin_level`).
- Correctly uses `ColumnTransformer` for preprocessing numerical and categorical features.
- Splits data into training and testing sets.
- Trains multiple classification models.
- Performs hyperparameter tuning using `GridSearchCV`.
- Evaluates the *best* model using all specified metrics.
- Visualizes confusion matrix and ROC curve for the best model.

**Weaknesses and Areas for Improvement:**
1.  **Missing `fit_transform` for `X` in preprocessing**: The `X_processed = preprocessor.fit_transform(X)` line processes the *entire* dataset `X` before splitting. This leads to **data leakage**, as the scaler and one-hot encoder learn from the test set data. The correct approach is to fit the preprocessor on the training data (`X_train`) and then `transform` both `X_train` and `X_test`. *The ground truth code correctly handles this by applying preprocessing after the train/test split.*
2.  **Limited Model Evaluation (Missing comprehensive evaluation for *all* models)**: The code only evaluates the `best_model` (Random Forest after tuning) comprehensively. The task specifically asks to ""Evaluate all models using accuracy, precision, recall, F1-score, and ROC-AUC"". The initial loop for `models` only performs cross-validation for F1-score and doesn't store/display the test set metrics for *all* models before tuning.
3.  **Cross-Validation Scope**: The prompt asks to ""Use cross-validation to evaluate each model"". While the code does this for the F1 score, it doesn't print or store a comprehensive set of CV metrics (accuracy, precision, recall, F1-score, ROC-AUC) for each model.
4.  **Incomplete Visualization**:
    *   **Confusion Matrices**: Only generates one confusion matrix for the *best* model, not for *all* models as required.
    *   **ROC Curves**: Only generates one ROC curve for the *best* model, not for *all* models as specified.
    *   **Feature Importance Display**: The `feature_names` for plotting importance are incorrect. After `ColumnTransformer` and `OneHotEncoder`, the original `X.columns` no longer directly map to the features in `X_processed`. The one-hot encoded features need to be properly named/tracked to display meaningful feature importances. The `numeric_features` are correctly identified, but the names of the one-hot encoded categorical features are lost, making the x-axis labels of the feature importance plot incorrect (`X_processed` has many more columns than `X`).
5.  **Hardcoded Age Group Bins**: The `Age_group` bins in the generated code `[0, 30, 50, 70, 100]` differ from a more nuanced set of bins that would typically be used for age, and also from the ground truth. While not strictly an error, it's a minor deviation from a good standard approach.
6.  **Readability/Structure**: The ground truth code has better structuring with headers for each section (e.g., ""1. Load dataset"", ""2. Preprocess data""), making it easier to follow. The generated code is more monolithic.
7.  **Minor**: No explicit `random_state` set for models during training, which reduces reproducibility compared to the ground truth. `GridSearchCV` only explores a very limited set of hyperparameters for Random Forest.
8.  **Output**: Missing structured print statements for evaluation results for *all* models. The ground truth provides nice summary tables/sections.",gpt_4o_mini,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, chi2, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset...",6,"The generated code attempts to address all requirements but falls short in several key areas, leading to incorrect or incomplete implementation of certain steps.

**Correctness:**
*   **Missing Value Handling:** The generated code uses `df.fillna(df.mean(), inplace=True)` which is a generic approach. The Ground Truth correctly identifies that 0s in specific columns `['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']` represent missing values and handles them appropriately by replacing with `NaN` and then imputing with the median. The generic mean imputation will fail for columns where 0 is a valid value (like 'Pregnancies' or 'Outcome').
*   **Polynomial Features:** The generated code applies `PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)`. The task specified `degree 2` without `interaction_only`. The `interaction_only` limits it to products of different features, missing the `x^2` terms. Also, it adds the polynomial features back to the `df` without dropping the original columns used to create them, leading to redundancy and potential collinearity issues not intended by polynomial transformation combined with feature selection/PCA. The Ground Truth correctly drops the original columns and then concatenates.
*   **Feature Scaling Alignment:** The scaling is performed on `df.select_dtypes(include=[np.number])` which includes the 'Outcome' column, which should not be scaled. It also creates two separate scaled DataFrames (`scaled_standard_df`, `scaled_minmax_df`) but doesn't consistently use one for subsequent steps. The Ground Truth correctly separates `X` and `y` before scaling `X`.
*   **PCA Application:** PCA is applied on `scaled_standard_df` which still contains more features than necessary. The Ground Truth applies PCA *after* correlation filtering, which is a better pipeline. Also, the generated code doesn't specify `n_components` for PCA, making it less direct to interpret. It also doesn't explicitly print the explained variance ratios as requested.
*   **Correlation Filtering:** The implementation `df = df.loc[:, ~df.columns.isin(high_corr_var)]` is problematic. `high_corr_var` contains the *names of columns that have high correlation with ANY other column*, not necessarily the ones to be dropped to resolve multicollinearity. A correct approach involves iterating through the upper triangle of the correlation matrix and systematically dropping one of the highly correlated pairs, as done in the Ground Truth.
*   **Feature Selection (S-KBest & RFE) Integration:** Both SelectKBest and RFE are applied, but the results (`X_new`, `selector.fit(X,y)`) are not explicitly used to update the feature set for subsequent steps. They seem to be standalone operations. The task implies applying these and *using* the selected features.
*   **SMOTE Application:** SMOTE is applied on the original `X` and `y` (`smote.fit_resample(X, y)`), not on a pre-processed/filtered `X` as would be typically done in a pipeline (e.g., after scaling, correlation filtering). The Ground Truth performs SMOTE on `X_final` (which is `X_filtered_corr`) and *after* a train-test split, which is crucial to prevent data leakage.
*   **Feature Importance Visualization:** The visualization plots `top_features` but the `feature_importance_df` is created using `X.columns`, which is the *original* feature set before any advanced processing (polynomial features, correlation filtering, etc.). This makes the feature importance less meaningful in the context of the full pipeline. The Ground Truth uses `X_train_smote.columns`, ensuring the importance is for the actually used features.

**Code Quality & Completeness:**
*   **Imports:** Proper imports are present.
*   **Structure:** The code is structured somewhat sequentially, but the modularity and reusability of intermediate results are not always optimal.
*   **Completeness (Addressing Requirements):**
    *   **Loading & Preprocessing:** Handles missing values, but incorrectly.
    *   **Engineered Features:** All specified features are created.
    *   **Polynomial Features:** Applied, but with a different `interaction_only` parameter and incorrect blending back into the DataFrame.
    *   **Feature Scaling:** Done, but on the wrong data subset and not consistently used.
    *   **PCA:** Applied, but explanations are missed (variance ratios). No `n_components` specified.
    *   **SelectKBest & RFE:** Applied but not integrated into the main feature set for subsequent steps.
    *   **Correlation Filtering:** Attempted, but the logic for dropping features is likely incorrect.
    *   **SMOTE:** Applied, but incorrectly (before split, on original data).
    *   **Feature Importance:** Calculated and visualized, but on the wrong feature set.
    *   **Explanation of Variance Ratios:** Requested, but not printed by the generated code for PCA.
    *   **Evaluation (classification report):** Included but performed on `X_resampled` and `y_resampled`, which is training data and overfitted.

Overall, while the code attempts most of the steps, the implementation details often deviate from best practices and lead to issues that would hinder the effectiveness and correctness of a real-world feature engineering pipeline. The pipeline flow is not always logical, and intermediate results are not consistently used.",gpt_4o_mini,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import scipy.stats as stats
import numpy as np
from scipy.stats import mannwhitneyu, f_oneway, pearsonr, chi2_contingency

# Load the dataset
data = pd.read_csv('world-happiness-report-2021.csv')

# Handle missing values
data.fillna(data.mean(), inplace=True)

# 2) T-test for happiness scores of Western European countries vs. others
western_european_countries = ['Country1', 'Country2', 'Country3']  # Replace with actual country names
western_happiness = data[data['C...",3,"The generated code attempts to address the prompt but has critical errors and omissions that make it largely non-functional or incorrect.

**Key Issues:**

1.  **Incorrect Column Names:** The generated code uses incorrect column names throughout (e.g., 'Happiness Score', 'Country', 'GDP per capita', 'Region', 'Freedom', 'Generosity') which are not present in the `world-happiness-report-2021.csv` dataset. The actual column names are 'Ladder score', 'Regional indicator', 'Logged GDP per capita', 'Social support', 'Freedom to make life choices', and 'Generosity'. This makes almost all statistical tests fail immediately when run.

2.  **Missing Value Handling:** The code uses `data.fillna(data.mean(), inplace=True)`. While `fillna` is a valid approach, filling with the mean for all columns (including categorical ones implicitly if not careful, though not the case here) is often not the best strategy for `Ladder score`, `GDP per capita`, etc., especially if the missingness is not random or if the distribution is skewed. More importantly, the ground truth code identified specific ""critical columns"" and dropped rows with NaNs in *those* to ensure complete data for statistical tests, which is a more robust approach for this context. The generated code's approach could introduce bias or issues if missing values are extensive in specific columns.

3.  **Hardcoded Western European Countries:** For the first T-test, the code hardcodes `western_european_countries = ['Country1', 'Country2', 'Country3']` and explicitly tells the user to replace them. This is a significant flaw; the task requires *using the dataset's 'Regional indicator' column*. The ground truth correctly filters by `'Regional indicator' == 'Western Europe'`.

4.  **Incomplete Interpretation:** The prompt specifically asks to ""Display p-values and interpret results with significance level α=0.05"". The generated code only prints a dictionary of boolean values (`p_value < alpha`), which is not an interpretation of the results. It misses the t-statistics, F-statistics, correlation coefficients, chi-square statistics, and the textual interpretation based on the p-value and the direction of the effect (e.g., higher/lower, positive/negative correlation).

5.  **Missing Imports and Context:** While `scipy.stats` is imported, specific functions like `mannwhitneyu`, `f_oneway`, `pearsonr`, `chi2_contingency` are imported individually but then sometimes called via `stats.` prefix and other times directly. This is inconsistent, though not a critical error. More importantly, the print statements for intermediary results and a clear structure for each test are absent, making the output less informative.

6.  **Happiness Category Definition:** The arbitrary bins `[-np.inf, 4, 6, np.inf]` for happiness categories might not be appropriate for the actual data distribution of 'Ladder score'. The ground truth uses quantiles (1/3 and 2/3), which is a much more robust and data-driven way to create balanced categories for a chi-square test.

7.  **`equal_var` in t-test:** The generated code performs `stats.ttest_ind` without `equal_var=False`, implying an assumption of equal variances. The ground truth correctly uses `equal_var=False` (Welch's t-test), which is generally more robust and recommended when variances are unknown or unequal, as is typical in real-world sociological data.

8.  **Output Format:** The prompt asks for ""only the Python code without explanations"". However, the critical output requirement is ""Display p-values and interpret results"". The generated code's final output is a simple dictionary of booleans which does not sufficiently display p-values nor provide proper interpretation.",gpt_4o_mini,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Preprocessing: Han...",6,"The generated code addresses most of the requirements but has significant issues in data preprocessing and some omissions or less robust implementations compared to the ground truth.

**Correctness:**
- **Missing Value Handling:** `df.fillna(df.mean(), inplace=True)` is generally a poor choice for handling missing values for specific columns if not all columns are numerical and if the `mean()` operation might include 'Ladder score' in its calculation for other columns, potentially leading to data leakage or incorrect imputations. More importantly, the ground truth correctly identified that only relevant columns should be considered for dropping NaNs and that the specified features had few or no NaNs.
- **Categorical Encoding:** `pd.get_dummies(df, drop_first=True)` is applied to the entire DataFrame *before* selecting features. This is problematic because 'Country name' and 'Regional indicator' are high cardinality and not specified as features. Applying one-hot encoding globally without a `ColumnTransformer` is a less robust approach, especially when many columns might be unsuitable for direct encoding or are not features. The ground truth correctly notes that none of the *specified features* are categorical, hence no encoding is needed for X.
- **Feature Selection:** The generated code uses `'GDP per capita'`, `'Freedom'`, `'Perceptions of corruption'` as feature names. However, the exact column names in the dataset are `Logged GDP per capita`, `Freedom to make life choices`, and `Perceptions of corruption`. This will cause a `KeyError` unless the CSV columns were manually renamed or are different from the standard `world-happiness-report-2021.csv`. The ground truth explicitly addresses this naming discrepancy and uses the correct column names from the dataset. This is a critical error that makes the code non-executable as written for the specified dataset.

**Code Quality:**
- Imports are mostly correct.
- Structure is logical, following the task requirements in order.
- Lack of error handling for file loading.
- `random_state` is not consistently set for all models and GridSearchCV, which is good practice for reproducibility.

**Completeness:**
- **Preprocessing:** Fails on the specified feature names (`KeyError`) and has inadequate missing value and categorical encoding strategies.
- **Model Training and Comparison:** All models are trained and evaluated correctly.
- **Hyperparameter Tuning:** Performed, but the selection of `RandomForestRegressor` as the ""best model"" for tuning is hardcoded without comparing initial R2 scores of all models. The best model should be determined programmatically.
- **Evaluation Metrics:** All requested metrics (MAE, MSE, RMSE, R²) are computed and printed.
- **Visualizations:** Residual plots and feature importance plots are generated.
- **Cross-validation:** Implemented for the best model, but only `cross_val_score` is used without specific `KFold` setup or reporting mean/std dev for different metrics (like RMSE besides R2).
- **Predictions for new countries:** Implemented and displayed, but lacks analysis/interpretation. The hypothetical feature values are also simplified and might not match the scale/distribution of the actual data, making predictions potentially less meaningful.

**Specific Improvements for a Higher Score:**
1.  **Correct Feature Names:** Crucially, fix the feature names to match the dataset's column names (`Logged GDP per capita`, `Freedom to make life choices`). This is a blocking issue.
2.  **Robust Preprocessing:**
    *   Handle missing values by dropping rows (as per ground truth) or more sophisticated imputation for *only the relevant columns*.
    *   Revisit categorical encoding. Since the specified features are all numerical, `pd.get_dummies` is not needed for `X`. If `Regional indicator` were to be used, then `ColumnTransformer` would be appropriate.
3.  **Determine Best Model for Tuning Programmatically:** Instead of hardcoding `RandomForestRegressor` for tuning, find the model with the highest R² from the initial evaluation phase and tune that one.
4.  **Prediction Analysis:** Add a small analytical section explaining the implications of the hypothetical country predictions.
5.  **Random State:** Consistently apply `random_state=42` to all models and `GridSearchCV` for reproducibility.
6.  **Cross-validation Details:** Provide mean and standard deviation for cross-validation scores, and consider calculating RMSE for cross-validation as well using appropriate scoring.",gpt_4o_mini,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mutual_info_score
from sklearn.pipeline import Pipeline
from sklearn.compose imp...",6,"The generated code attempts to address all requirements but falls short in several aspects related to correctness, completeness, and best practices.

**Correctness & Completeness Issues:**

1.  **Preprocessing and Column Names:** The ground truth correctly identifies `Logged GDP per capita` and `Perceptions of corruption` as the relevant column names. The generated code uses `GDP per capita` and `Perceptions of corruption` which might exist in the raw dataset but are not the 'logged' version as often used in such analyses. More critically, it drops `Regional indicator` early (`data.drop(columns=['Country name', 'Regional indicator'], inplace=True)`), which is then needed for `Regional_Happiness_Rank` and `GDP_vs_Regional_Average` calculations (`data.groupby('Region')`). This will lead to an error or incorrect behavior, as there's no 'Region' column after dropping 'Regional indicator'. The ground truth correctly handles missing values specifically for 'Perceptions of corruption' before a general `dropna()`, which is more robust than a direct `fillna(mean())` across all columns, especially if non-numeric columns are present (though not the case here).

2.  **Engineered Features:**
    *   `Happiness_Efficiency`: Correctly implemented.
    *   `Social_Wellness_Index`: The ground truth uses `+` for combination, the generated code uses `/2`, which implies an average. While valid, the prompt implies ""combination"" which addition perfectly achieves. The division by 2 is a slight deviation from the expectation of a simple combination.
    *   `Governance_Score`: Similar to `Social_Wellness_Index`, it uses `/2` (average) instead of simple addition as in GT, and the critical `(1 - Perceptions of corruption)` is correctly used.
    *   `Regional_Happiness_Rank` and `GDP_vs_Regional_Average`: As noted above, these will fail because `Regional indicator` (referred to as 'Region' in GC) is dropped prematurely. The ground truth correctly keeps it for these calculations.
    *   The generated code also includes `GDP per capita` in `key_features` for polynomial features, but the ground truth uses `Logged GDP per capita`, maintaining consistency with the initial data handling.

3.  **Polynomial Features:**
    *   The generated code correctly applies `PolynomialFeatures` and concatenates them.
    *   It uses `key_features = data[['Ladder score', ...]]`. The ground truth uses factors *contributing* to happiness, not the target variable itself (`Ladder score`). Including `Ladder score` in the polynomial features means you're creating features from the target, which isn't standard practice for feature engineering for a predictor model, and can lead to data leakage if not handled carefully (though `include_bias=False` and no direct prediction is done here, it's conceptually off).

4.  **Feature Scaling:** The generated code creates separate `scaled_features` dictionaries but then only uses `StandardScaler`'s output for PCA and t-SNE. The prompt asks to ""Perform feature scaling using multiple methods"", implying that the results of these scalers should potentially be used or demonstrated, not just computed and then discarded in favor of one. The ground truth clearly states it will use `StandardScaler` for subsequent steps, which is good practice.

5.  **Dimensionality Reduction (PCA and t-SNE):**
    *   PCA and t-SNE are correctly applied and new columns are added to the `data` DataFrame, which is a good way to integrate them.
    *   However, no visualization is generated directly, which was explicitly requested (""Apply PCA and t-SNE for dimensionality reduction and visualization""). The ground truth includes plots for both.

6.  **Feature Selection:**
    *   `SelectKBest`: Correctly applied using `f_regression`.
    *   `RFE`: The generated code uses `RandomForestRegressor` as the estimator for RFE. While valid, the prompt implies that the chosen estimator should be `LinearRegression` as this is typical for consistency. The ground truth used `LinearRegression`, which is often preferred for simple RFE with many features.
    *   **Missing `mutual_info_regression`:** The generated code uses `mutual_info_score` from `sklearn.metrics` which is for *classification*, not regression. The task specifically requires `mutual_info_regression` (which is present in `sklearn.feature_selection`). This is a critical error. The ground truth correctly uses `mutual_info_regression`.

7.  **Interaction Features:** Correctly implemented for one type of interaction.

8.  **Feature Importance (Random Forest):**
    *   Correctly implemented using `RandomForestRegressor`.
    *   The visualization for feature importance *is* included, which is a strong point.

**Code Quality and Structure:**

*   **Imports:** Mostly correct, but the `mutual_info_score` import is incorrect for the task's requirement. Unnecessary `Pipeline`, `ColumnTransformer`, `SimpleImputer`, `train_test_split` imports given how they are used (or not used).
*   **Variable Naming:** Generally clear.
*   **Comments/Prints:** Lacks informative print statements or comments to indicate progress or validation steps, which the ground truth includes.
*   **Error Handling:** The ground truth includes a `try-except` block for file loading, which is a good practice. The generated code does not.
*   **Warning Suppression:** The ground truth includes warning suppression, which can make the output cleaner, especially when dealing with many sklearn operations.

In summary, while the generated code attempts to cover all aspects, critical errors in data preprocessing (dropping `Regional indicator`), incorrect use of `mutual_info_score` instead of `mutual_info_regression`, and omission of required visualizations significantly reduce its correctness and completeness. The choice of `LinearRegression` for RFE by the GT is also a minor point of deviation for best consistency.",gpt_4o_mini,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Step 1: Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Step 2: Filter for 'location' == 'USA' and set 'date' as the index
usa_data = df[df['location'] == 'USA'].set_index('date')

# Step 3: Resample to weekly frequency, summing ...",7,"The generated code addresses most of the requirements of the task and produces a working solution.

**Strong Points:**
*   Correctly loads data, parses dates, and filters for 'USA'.
*   Correctly resamples 'new_cases' to weekly frequency using summation.
*   Performs and plots time series decomposition.
*   Correctly implements the Augmented Dickey-Fuller (ADF) test for stationarity.
*   Applies first-order differencing if the initial ADF test indicates non-stationarity.
*   Plots ACF and PACF for the (potentially differenced) series.
*   Includes all necessary imports.

**Areas for Improvement (Why it's not higher):**

1.  **Hardcoded Period for Decomposition:** The decomposition period is not specified in the `seasonal_decompose` function within the generated code (`decomposition = seasonal_decompose(weekly_cases, model='additive')`). `seasonal_decompose` by default tries to infer the period, which can sometimes be unreliable for weekly data where a 52-week (yearly) seasonality is typically expected. The ground truth explicitly sets `period=52`. This is a crucial detail for accurate decomposition of weekly data.
2.  **Robustness/Error Handling:**
    *   There's no error handling for `FileNotFoundError` if `synthetic_covid19_data.csv` is missing.
    *   No checks for empty dataframes after filtering or resampling (e.g., if 'USA' or 'new_cases' data is missing or all zeros after resampling). This could lead to errors in subsequent steps (e.g., `seasonal_decompose` or `adfuller` might fail on empty or very short series).
    *   No handling if `weekly_cases` has too few data points for decomposition or the ADF test; `seasonal_decompose` and `adfuller` can raise errors if the series is too short.
3.  **Clarity in Output:** The output for the ADF test is just the statistic and p-value. The ground truth provides critical values and a more comprehensive conclusion about stationarity, which is very helpful for interpretation.
4.  **No `main` function or encapsulation:** The code is written as a script, which is fine for a small task, but encapsulating it within a function (as in the ground truth) enhances reusability and modularity.
5.  **Plotting Details:** The plots produced by the generated code are functional but lack specific titles or refined layouts that would improve their presentation (e.g., `plt.tight_layout()` for better spacing, `fig.suptitle` for decomposition plots). The `plot_acf` and `plot_pacf` calls are also a bit simple, lacking specific titles, though they default to reasonable ones.
6.  **Redundant `differenced_cases` assignment:** If the initial series is stationary, `differenced_cases` is assigned `weekly_cases`, but the variable name itself becomes slightly misleading then. A more generic name, like `stationary_series`, would be better as used in the ground truth.

To score higher, the generated code would need to incorporate the hardcoded decomposition period or dynamic period detection, add robust error handling and checks for data availability, and improve the clarity and detail of its output/plots.",gpt_4o_mini,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Categorize wines based on 'pH' values
conditions = [
    (data['pH'] < 3.2),
    (data['pH'] >= 3.2) & (data['pH'] <= 3.4),
    (data['pH'] > 3.4)
]
labels = ['Low pH', 'Medium pH', 'High pH']
data['pH_Category'] = np.select(conditions, labels)

# Calculate average 'quality' for each pH group
average_quality = data.groupby('pH_C...",7,"The generated code correctly performs most of the required tasks but has some limitations and areas for improvement.

**Strong Points:**
*   **Correct Categorization:** The `np.select` method correctly categorizes pH values into the specified groups.
*   **Mean Calculation:** Correctly calculates the average quality for each pH group.
*   **ANOVA Test:** Successfully performs and prints the ANOVA test results (F-statistic and p-value).
*   **Basic Visualization:** Generates a bar plot showing average quality by pH category.

**Areas for Improvement/Limitations:**
*   **Confidence Interval Calculation (Major Issue):** The confidence interval calculation `1.96 * (std_devs / np.sqrt(n))` assumes the population standard deviation is known or `n` is large enough for Z-distribution. For smaller `n`, it should ideally use the t-distribution (`scipy.stats.t.interval`), which is more appropriate here as `n` for each group might vary. The current implementation uses a fixed 1.96 (for 95% CI with Z-dist) which is less robust than using `stats.t.interval` as in the ground truth. Also, the error bars represent symmetric intervals based on `std_devs`, but the visualization requires specific calculation of lower and upper bounds of CI from the mean to show accurate error bars, not just a symmetric `std_devs`.
*   **Missing Confidence Interval Reporting:** The calculated confidence intervals are used for plotting but not explicitly printed or reported to the user as requested by ""Calculate and report the average 'quality' for each pH group"". The ground truth prints the full CI (lower and upper bounds).
*   **Lack of Error Handling:** No `try-except` block for file loading, which could lead to a crash if the CSV is not found.
*   **No Interpretation of ANOVA Results:** While the p-value is printed, there's no interpretation of its significance (e.g., comparing to alpha=0.05) as done in the ground truth.
*   **Missing Ordering:** The pH categories are created directly (`np.select`) but not explicitly ordered as a pandas categorical type. This means the plotting order might be alphabetical rather than the intended logical order ('Low', 'Medium', 'High'), leading to inconsistent visualizations if not handled carefully (though for these labels, alphabetical happens to match). The ground truth explicitly sets the categorical order.
*   **Less Descriptive Labels for pH:** The pH labels are ""Low pH"", ""Medium pH"", ""High pH"", which are less descriptive than the ground truth's ""(< 3.2)"", ""(3.2 to 3.4)"", ""(> 3.4)"".
*   **Plotting Details:**
    *   No Y-axis limit for quality (quality scores typically range 0-10).
    *   No grid lines.
    *   No text labels for values on top of bars.
    *   Arbitrary colors used instead of a single consistent color with clear edges.
    *   Does not show the average quality values on the bars, which is a good practice.",gpt_4o_mini,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Step 1: Load the dataset and parse 'date'.
dat...",6,"The generated code has some significant issues that prevent it from being fully correct and score higher.

**Issues:**

1.  **Incorrect Imputation and Encoding Order/Method:**
    *   The `SimpleImputer` is applied to `X` directly, which contains both numerical and categorical data ('continent'). `SimpleImputer(strategy='median')` is only suitable for numerical data. It will raise an error or produce incorrect results if applied to a string column like 'continent'.
    *   Even if the imputer was applied only to numerical columns, the output `X_imputed` is a NumPy array, which loses the column names.
    *   The subsequent `pd.get_dummies(pd.DataFrame(X_imputed), columns=['continent'], drop_first=True)` will fail because `X_imputed` is already a NumPy array and won't have a 'continent' column. This indicates a fundamental misunderstanding of how to correctly integrate imputation and one-hot encoding, especially when using `ColumnTransformer` is often the preferred, robust way (even though the task doesn't explicitly require it, the ground truth achieves this with manual steps).

2.  **Improper Scaling (and subsequent failure):**
    *   `X_encoded` is created after imputation but before scaling. `X_encoded = pd.get_dummies(pd.DataFrame(X_imputed), columns=['continent'], drop_first=True)` attempts to one-hot encode, but as mentioned, `X_imputed` is already a NumPy array and doesn't contain a 'continent' column for `get_dummies` to work on. This step will likely cause an error or produce an empty `X_encoded` if it doesn't error out directly.
    *   Even if the encoding somehow worked, scaling must happen *after* the train-test split to prevent data leakage. The generated code scales `X_encoded` *before* splitting, which is incorrect. The target `y` is also imputed and then `X_scaled` and `y_imputed` are split, which is fine, but the order of operations for `X` is critical and wrong.

3.  **Redundant/Problematic Imputation of Target:**
    *   `y_imputed = imputer.fit_transform(y.values.reshape(-1, 1)).ravel()` This line imputes the target variable `y`. While imputing target variables *can* be done, it's less common than feature imputation and not explicitly requested to be done with the median strategy. The request states ""Handle missing values in selected features and target by imputing with the median."" The ground truth handles missing values in the target by dropping rows containing missing values in the target or by median imputing specific numerical columns, but the generated code's approach of directly applying a median imputer on the target *vector* without careful consideration might yield unexpected results if `y` contains non-numeric types or if this isn't the desired handling. In this specific dataset, `new_deaths` are numerical, so it might work, but it's less standard than the way the ground truth handles it within `df_selected`.

4.  **Mismanagement of Column Information:**
    *   When `SimpleImputer` `fit_transform`s `X`, `X_imputed` becomes a NumPy array. This loses all column names, which then makes the attempt to one-hot encode 'continent' on `X_imputed` impossible. The `ColumnTransformer` (which was imported but not used, perhaps intended?) is the way to handle mixed data types and transformations while preserving column context.

**Strengths:**

*   **Correct Imports:** All necessary libraries are imported.
*   **Correct Model Initialization and Training:** Linear Regression and Random Forest Regressor models are correctly initialized and trained with the specified parameters.
*   **Correct Evaluation Metrics:** MAE, MSE, and R-squared are correctly calculated and printed for both models.
*   **Adherence to Data Splitting Ratio:** The 80/20 train-test split is correctly implemented.",gpt_4o_mini,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
from scipy import stats
import statsmodels.api as sm

# Load the dataset
data = pd.read_csv('synthetic_covid19_data.csv')

# Handle missing values with median imputation
columns_to_impute = ['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']
data[columns_to_impute] = data[columns_to_impute].fillna(data[columns_to_impute].median())

# 2 - ANOVA test for 'reproduction_rate' across continents (Asia, Europe, North America)
...",6,"The generated code addresses most parts of the task requirement but has several inaccuracies and missing elements that prevent it from being fully correct and robust.

**Correctness Issues:**

1.  **ANOVA Data Preparation:** The line `anova_data = [continent_data[continent_data['continent'] == continent]['reproduction_rate'] for continent in continent_data['continent'].unique()]` incorrectly extracts unique continents from `continent_data`. `continent_data['continent'].unique()` would include *all* continents present in the `continent_data` DataFrame, not just the specified 'Asia', 'Europe', 'North America'. This means if a continent other than these three (e.g., 'Africa') was present in `data` and then filtered for `continent_data` (which it wouldn't be by `isin`), this approach would still extract unique continent names. More critically, it doesn't handle the case where one of the *target* continents might have all NaN values, which `f_oneway` expects to be handled. The ground truth correctly iterates through `continents_of_interest`.
2.  **Date Conversion for Latest Entry:** The code `latest_data = data.loc[data.groupby('country')['date'].idxmax()]` relies on `date` being sortable, but it doesn't explicitly convert the 'date' column to datetime objects. While `idxmax()` *might* work on string dates if they are in a sortable format (like 'YYYY-MM-DD'), it's not robust and prone to errors if the format varies. The ground truth correctly converts `df['date']` to `datetime`.
3.  **High Death Rate Calculation (75th Percentile):** The line `latest_data['high_death_rate_country'] = (latest_data['total_deaths'] / latest_data['population']) > latest_data['total_deaths'].quantile(0.75)` is incorrect. The 75th percentile should be of the *ratio* (`total_deaths / population`), not just `total_deaths`. This is a critical logical error for classifying `high_death_rate_country`.
4.  **Handling `total_deaths / population` NaNs:** The code does not explicitly handle potential division by zero if `population` is 0, or if `total_deaths` or `population` are NaNs, which could lead to `inf` or `NaN` values in the ratio. The ground truth handles this via `replace([np.inf, -np.inf], np.nan, inplace=True)`.
5.  **Output Clarity:** The output for each test is minimal. It prints the p-value and a conclusion, but it doesn't explicitly state the null hypothesis for each test before the conclusion, as requested by the prompt.

**Completeness and Quality Issues:**

1.  **Missing `numpy` import:** `numpy` is often useful for numerical operations and is part of standard data science imports, especially when dealing with `inf` or `nan` (though not strictly needed given current errors). The ground truth includes it.
2.  **Missing `statsmodels` import usage:** The prompt explicitly asks for `statsmodels` to be imported, but it's not used in the generated code. While `scipy.stats` is sufficient for the requested tests, adhering to the prompt is important.
3.  **Hardcoded Alpha Value:** The significance level (alpha) is hardcoded as `0.05` instead of being defined as a constant, which is better practice for maintainability and readability.
4.  **Error Handling:** The code does not include any error handling for file not found (e.g., `try-except` block for `pd.read_csv`).
5.  **Docstrings/Comments:** Lacks comments or explanations for key steps, which would improve readability and maintainability.
6.  **Redundant `statsmodels` import:** The import `import statsmodels.api as sm` is present but not used.

Overall, while the code attempts the tasks, the inaccuracies in calculations and logical errors in the ""high death rate"" category derivation significantly impact its correctness. The lack of robustness and clearer output also contributes to a lower score.",gpt_4o_mini,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select the numerical columns
numerical_columns = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
selected_data = df[numerical_columns]

# Calculate the pairwise correlation matrix
correlation_matrix = selected_data.corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_m...",9,"The generated code correctly solves all aspects of the task: it properly loads the CSV, selects the specified numerical columns, calculates the correlation matrix, and visualizes it as a heatmap using seaborn. The imports are correct and standard. The visualization parameters (`annot`, `fmt`, `cmap`) are well-chosen for a clear heatmap.

The only minor difference from the ground truth is the lack of robust error handling (e.g., `try-except` blocks for `FileNotFoundError` or `KeyError` if columns are missing or the file doesn't exist) and the encapsulation within a function. However, the prompt only asked for ""Python code without explanations"" and focused on the core functionality, which was fully delivered. For a benchmark, the functional correctness is paramount, and this code achieves that.",gpt_4o_mini,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Group by rarity and calculate averages
averages = df.groupby('rarity').agg(
    average_elixirCost=('elixirCost', 'mean'),
    average_win_rate=('Win Rate', 'mean'),
    average_usage=('Usage', 'mean')
).reset_index()

# Display the averages
print(averages)
```",10,"The generated code is excellent. It correctly loads the CSV, groups the data by 'rarity', calculates the mean for 'elixirCost', 'Win Rate', and 'Usage', and displays the results. The use of `.agg()` with named aggregations is a modern and readable approach, and `.reset_index()` makes the output clean with 'rarity' as a regular column instead of an index. This is a perfectly correct and efficient solution.",gpt_4o_mini,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Create the new binary target column
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)

# Select the features
features = df[['elixirCost', 'Usage', 'rarity']]
target = df['is_high_win_rate']

# Apply one-hot encoding to the categorical 'rarity' column
features = pd.get_dummies(features, columns...",8,"The generated code successfully completes the task by loading the data, creating the target column, selecting features, applying one-hot encoding, and splitting the data. The imports are correct, and the code structure is logical.

However, there is one minor issue:
- In the `pd.get_dummies` call, `drop_first=True` was used. While this is often a good practice to avoid multicollinearity, the prompt did not specify this. The ground truth, to be strictly compliant with the prompt (which only asked for ""Apply one-hot encoding""), used `drop_first=False` (or implied it by not specifying). For a benchmark, strict adherence to parameters is important.

Additionally, while not an error, the lines `features = df[['elixirCost', 'Usage', 'rarity']]` and `target = df['is_high_win_rate']` directly assign slices of the dataframe. Using `.copy()` as in the ground truth (`X = df[features].copy()`) is a better practice to avoid `SettingWithCopyWarning` in more complex scenarios, though it doesn't cause a direct error here.",gpt_4o_mini,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('iris.csv')

# Define features and target
X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = data['Species']

# Encode the categorical target variable
label_encoder = LabelEncoder()
y_encoded = label_encoder...",9,"The generated code is highly correct and addresses almost all requirements from the task.

**Strong Points:**
- **Correct Imports:** All necessary libraries are imported.
- **Data Loading:** Correctly loads `iris.csv`.
- **Feature and Target Definition:** `X` and `y` are correctly defined using the specified columns.
- **Target Encoding:** `LabelEncoder` is correctly used to encode the target variable.
- **Data Splitting:** `train_test_split` is used with the correct `test_size` (0.3) and `random_state` (42).
- **KNN Training:** A `KNeighborsClassifier` is initialized with `n_neighbors=3` and trained on the training data.
- **Prediction and Accuracy:** Predictions are made on the test set, and accuracy is calculated and printed.

**Areas for Minor Improvement:**
- **File Not Found Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robust code. The generated code does not include this. While not strictly required by the prompt, it makes the code more robust.
- **`stratify` parameter in `train_test_split`:** The ground truth includes `stratify=y_encoded`, which is important for maintaining class distribution across splits, especially with imbalanced datasets. While the Iris dataset is balanced, it's a good practice for classification tasks. The generated code omits this.
- **Output Formatting:** The ground truth formats the accuracy to 4 decimal places and provides a more descriptive print statement. This is a minor aesthetic point.

Overall, the generated code performs the task correctly and efficiently. The missing `FileNotFoundError` handling and `stratify` parameter are minor omissions that don't fundamentally break the core logic but could improve robustness and best practices.",gpt_4o_mini,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
iris_data = pd.read_csv('iris.csv')

# Filter the data for the two species
versicolor = iris_data[iris_data['Species'] == 'Iris-versicolor']['PetalLengthCm']
virginica = iris_data[iris_data['Species'] == 'Iris-virginica']['PetalLengthCm']

# Perform an independent two-sample t-test
t_statistic, p_value = stats.ttest_ind(versicolor, virginica)

# Null and alternative hypotheses
# H0: There is no significant difference in th...",8,"The generated code correctly performs the core task of loading the dataset, filtering for the specified species, running an independent two-sample t-test, and interpreting the p-value. It correctly imports necessary libraries and the overall structure is logical.

Areas for improvement to achieve a higher score:
1.  **Hypotheses Statement Clarity:** While the hypotheses are stated as comments, the ground truth explicitly prints them for clarity and traceability, which is good practice for a data science benchmark. The generated code could benefit from printing these out.
2.  **Output Detail for Interpretation:** The interpretation only states ""Reject the null hypothesis"" or ""Fail to reject the null hypothesis."" The ground truth provides a more comprehensive conclusion tied back to the specific context of the problem (e.g., ""There is a statistically significant difference...""), which is more informative for a data science report.
3.  **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` and checks for empty groups, which makes the code more robust. The generated code lacks this.
4.  **Formatting of Output:** The ground truth uses f-strings with formatting (e.g., `:.4f`) for better readability of the t-statistic and p-value.
5.  **Adherence to ""Clearly state..."" requirement:** The prompt asked to ""Clearly state the null and alternative hypotheses for this test."" While commenting them states them, printing them as the ground truth does adheres more directly to ""clearly state"".

Overall, it's a solid attempt that gets the main statistical analysis correct, but it could be enhanced with better output presentation, clearer hypothesis communication, and basic error handling for robustness.",gpt_4o_mini,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
iris_data = pd.read_csv('iris.csv')

# List of numerical features
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

# Set the aesthetics for the plots
sns.set(style=""whitegrid"")

# Create box plots for each feature
for feature in features:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='Species', y=feature, data=iris_data)
    plt.title(f'Box Plot of {feature} by...",9,"The generated code correctly loads the dataset, iterates through each numerical feature, and generates a box plot grouped by 'Species' with appropriate titles. It uses `pandas`, `seaborn`, and `matplotlib.pyplot` effectively. The code is concise and directly addresses all requirements of the task.

Strengths:
- Correctly loads the CSV file.
- Generates a separate box plot for each of the four specified numerical features.
- Each box plot correctly groups data by 'Species'.
- Plots have appropriate titles.
- Uses `seaborn` for plotting, which is suitable for statistical visualizations.
- Includes proper imports.

Areas for minor improvement (not critical for task completion but good practice):
- The ground truth code includes error handling for `FileNotFoundError`, which is a robust practice for file operations. The generated code lacks this.
- While `sns.set(style=""whitegrid"")` is called, other aesthetic improvements like `plt.grid` or `plt.tight_layout()` as seen in the ground truth are absent. This is a minor point as the basic plots are correct.
- Explicitly defining constants for `DATASET_PATH`, `NUMERICAL_FEATURES`, and `GROUPING_COLUMN` as in the ground truth improves readability and maintainability for larger projects, though for this specific task, direct string usage is acceptable.",gpt_4o_mini,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('heart.csv')

# Select features and target variable
features = data[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']]
target = data['HeartDisease']

# Preprocess categorical features using one-hot encoding
features ...",9,"The generated code correctly implements all the requirements of the task. It loads the dataset, selects the specified features and target, performs one-hot encoding on the specified categorical features using `pd.get_dummies`, splits the data, trains a Logistic Regression model, and reports the accuracy score.

One minor point for improvement, although not strictly a failure, is the use of `pd.get_dummies` directly on `features` *before* the train-test split. While this works for this specific task, a more robust and generally recommended approach for production-ready code is to use `ColumnTransformer` and `Pipeline` (as seen in the ground truth). This approach prevents data leakage during preprocessing and ensures that the same preprocessing steps are applied consistently to both training and test data, especially when dealing with unforeseen categories in the test set or more complex preprocessing pipelines. For this basic task, `pd.get_dummies` is acceptable. The `drop_first=True` in `pd.get_dummies` is also a good practice to avoid multicollinearity.

The formatting of the accuracy printout is also slightly different (2 decimal places vs. 4), but this is a minor stylistic difference and doesn't affect correctness. Overall, a very good solution.",gpt_4o_mini,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

# Step 1: Load the dataset
data = pd.read_csv('heart.csv')

# Step 2: Create two groups based on the `Sex` column
males = data[data['Sex'] == 'M']['Cholesterol']
females = data[data['Sex'] == 'F']['Cholesterol']

# Step 3: Perform an independent two-sample t-test
t_statistic, p_value = stats.ttest_ind(males, females)

# Step 4: Null and alternative hypotheses
# Null Hypothesis (H0): There is no difference in mean Cholesterol levels between m...",8,"The generated code correctly performs all the required steps, including loading the data, separating groups, performing the t-test, stating hypotheses, printing results, and interpreting the p-value. The imports are correct, and the structure is straightforward.

Areas for improvement to reach a higher score:
1.  **Robustness**: The ground truth includes error handling for `FileNotFoundError` and checks for the existence of required columns and sufficient data in groups. The generated code assumes the file exists and has the correct columns and sufficient data, which can lead to runtime errors if these assumptions are not met.
2.  **`equal_var` parameter**: The `ttest_ind` function by default assumes equal variances (`equal_var=True`). The ground truth specifically uses `equal_var=False` (Welch's t-test), which is often a safer choice when population variances are not known or tested to be equal, as it's more robust to violations of that assumption. While not explicitly stated in the prompt, using Welch's t-test is generally good practice in real-world scenarios.
3.  **Clarity**: The ground truth separates the print statements for hypotheses, t-test results, and interpretation with headings (`--- Hypotheses ---`), which improves readability. The generated code outputs text directly.
4.  **Formatting of output**: The ground truth formats the t-statistic and p-value to a specific number of decimal places, which can improve readability and consistency.",gpt_4o_mini,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Function to identify outliers using IQR
def identify_outliers(column):
    Q1 = column.quantile(0.25)
    Q3 = column.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = column[(column < lower_bound) | (column > upper_bound)]
    return outliers

# Identify outliers in 'volatile acidity' and 'residual sugar'
volatile_acidity...",7,"The generated code correctly implements the core logic for identifying and winsorizing outliers using the IQR method. It handles both specified columns and compares statistics before and after.

However, there are several areas for improvement:
- **Redundant Calculations:** The Q1, Q3, and IQR calculations are repeated in both `identify_outliers` and `winsorize_column` functions. This is inefficient and prone to errors if the logic needs to change. It would be better to calculate these once per column and pass them or store them.
- **Reporting of Outliers:** The `identify_outliers` function returns the actual outlier values, but the task asked to report the *number* of outliers. While `len()` is used, the function name itself could be more descriptive of its return value.
- **Output Format:** The final comparison of mean and standard deviation is printed as a dictionary, which is not very user-friendly compared to the ground truth's formatted print statements, which explicitly label ""Original Data"" and ""Winsorized Data"" stats.
- **Comparison of Statistics:** While the data is gathered, there's no explicit printing of the *difference* or *change* in statistics from before to after treatment, which is often crucial for evaluating the impact of outlier treatment.
- **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError`, which is good practice for data loading tasks. The generated code lacks this.
- **Code Structure and Readability:** While functions are used, the repetition and the way data is passed/recalculated could be improved for better overall structure and readability. The ground truth, by iterating through the columns in a loop and performing all steps for each column, presents a very clear and logical flow.",gpt_4o_mini,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('heart.csv')

# Create the grouped data
grouped_data = data.groupby(['ChestPainType', 'HeartDisease']).size().unstack(fill_value=0)

# Create the plot
grouped_data.plot(kind='bar', figsize=(10, 6))

# Adding title and labels
plt.title('Count of Heart Disease by Chest Pain Type')
plt.xlabel('Chest Pain Type')
plt.ylabel('Count')
plt.xticks(rotation=0)

# Adding legend
plt.leg...",7,"The generated code correctly loads the dataset and creates a grouped bar chart as requested. It includes proper imports, a title, labeled axes, and a legend.

However, there are a few areas for improvement:
1.  **Hardcoded Legend Labels:** The legend labels `['No', 'Yes']` are hardcoded. While conceptually correct for 0 and 1, it's better practice to either map these values to more descriptive strings (`'No Heart Disease'`, `'Heart Disease'`) earlier in the data processing or ensure the legend reflects the actual `HeartDisease` (0 and 1) values directly. The current approach assumes a direct mapping.
2.  **`ChestPainType` Order:** The `ChestPainType` categories (`TA`, `ATA`, `NAP`, `ASY`) are not explicitly ordered. While pandas `groupby` might preserve some order, it's not guaranteed to be the desired order, especially if any categories are missing or if the default sort order isn't the specified one. The Ground Truth code explicitly sets the categorical order, which is more robust.
3.  **Error Handling:** The generated code lacks any error handling for file not found or other issues during CSV loading, which is present in the ground truth and generally good practice.
4.  **Clarity for `HeartDisease` legend:** The legend shows ""No"" and ""Yes"" but it's not immediately clear if ""No"" represents 0 or ""Yes"" represents 1 without checking the code. Using descriptive labels in the data itself (like the Ground Truth does with `HeartDisease_Label`) makes the plot more self-explanatory.
5.  **Plot type:** While `grouped_data.plot(kind='bar')` works, `seaborn.countplot` is generally more idiomatic and concise for this specific type of task (counting occurrences of categories, with hue for grouping) and often produces a more aesthetically pleasing default. The generated code's approach requires an explicit `groupby().size().unstack()` which is more verbose for this common plotting pattern.",gpt_4o_mini,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Clean the `in_shazam_charts` column
df['in_shazam_charts'] = pd.to_numeric(df['in_shazam_charts'].str.replace(',', ''), errors='coerce')

# Report count of NaNs
nan_count = df['in_shazam_charts'].isna().sum()
print(f'Count of NaNs in in_shazam_charts: {nan_count}')

# Group by `key` and `mode`, then calculate the averages
average_danceability_valence = df.groupby(['key', 'mode'])[['danceability_%', 'valence...",9,"The generated code correctly performs all the required tasks: loading the dataset, cleaning and converting the `in_shazam_charts` column (including reporting NaNs), and grouping by `key` and `mode` to calculate the average `danceability_%` and `valence_%`. The code is concise and efficient.

One minor suggestion for improvement, which is not critical and why it's not a perfect 10, is that `.reset_index()` is used on the final output, which is not strictly necessary for displaying the grouped data and makes the output slightly less aligned with the ground truth's direct display of the groupby result. However, it doesn't affect correctness. Also, the ground truth included `try-except FileNotFoundError`, which is good practice for robustness, but not explicitly requested. The generated code also didn't explicitly print the descriptive string for the final output like the ground truth did, just the dataframe. Despite these minor points, the core requirements are met effectively.",gpt_4o_mini,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
data = pd.read_csv('spotify_2023.csv')

# Ensure the streams column is numeric, coerce errors to NaN, then drop NaN rows
data['streams'] = pd.to_numeric(data['streams'], errors='coerce')
data = data.dropna(subset=['streams', 'in_spotify_playlists', 'energy_%'])

# Select features and targe...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, handles the `streams` column conversion and NaN values appropriately, selects the correct features and target, splits the data, trains a Linear Regression model, and reports both R-squared and MSE. The imports are correct and the code structure is logical and easy to follow.

The only minor points for improvement, which prevent a perfect score, are:
1.  **Error Handling for File Loading:** The generated code doesn't include a `try-except` block for `pd.read_csv`, meaning it would crash if the file is not found. The ground truth includes this, which is good practice for robust code.
2.  **Output Formatting for Metrics:** The ground truth formats the R2 and MSE outputs to a specific number of decimal places, which makes the output cleaner and more consistent. The generated code prints the full float precision.

Overall, it's a very good solution that successfully completes the task.",gpt_4o_mini,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Combine released_year, released_month, and released_day into a single datetime column named release_date
df['release_date'] = pd.to_datetime(df[['released_year', 'released_month', 'released_day']].astype(str).agg('-'.join, axis=1), errors='coerce')

# Create a new feature named days_since_release
fixed_date = pd.Timestamp('2023-12-31')
df['days_since_release'] = (fixed_date - df['release_date']).dt.days.fil...",8,"The generated code correctly performs the requested feature engineering tasks: loading the data, combining date components into a `release_date` column with error handling, and calculating `days_since_release`. It uses appropriate pandas functionalities for these operations.

**Strong points:**
*   **Concise release_date creation:** The method `df[['released_year', 'released_month', 'released_day']].astype(str).agg('-'.join, axis=1)` is a very clever and concise way to combine the year, month, and day columns into a string, which `pd.to_datetime` can then parse. This is an efficient alternative to string concatenation.
*   **Correct error handling for dates:** `errors='coerce'` is correctly applied, setting invalid dates to NaT.
*   **Correct `days_since_release` calculation:** The calculation `(fixed_date - df['release_date']).dt.days` is accurate.

**Areas for improvement:**
*   **`fillna(0)` for `days_since_release`:** The task specifies that invalid dates should be handled by setting them to `NaT` for `release_date`. For `days_since_release`, when `release_date` is `NaT`, `(fixed_date - NaT).dt.days` will result in `NaN`. The ground truth used `Int64` dtype, which allows `NaN` values, maintaining fidelity to the original NaT. The generated code uses `.fillna(0)` and `.astype(int)`. While `astype(int)` requires filling `NaN` first, `fillna(0)` implies that unreleased or invalid-date tracks were released 0 days ago, which is not strictly accurate or desired based on the problem statement's `NaT` handling for `release_date`. A better approach would be to ensure the column can hold null values (e.g., using `pd.Int64Dtype()`) or to explicitly state how `NaN`s in `days_since_release` should be treated if not `NaN`. Given the problem statement just asks for an integer, `Int64` is the most robust way to fulfill this while maintaining NaN propagation.
*   **Robustness in file loading:** The ground truth includes `try-except` blocks for file loading, which is good practice for production-ready code. The generated code assumes the file exists. While the prompt implied the file would be present, robust code typically handles this.
*   **Completeness on `days_since_release` type:** The task asked for `days_since_release` to be an integer. While `.astype(int)` makes it an integer, if there are `NaN` values, `astype(int)` would raise an error if not for the `fillna(0)`. Using `pd.Int64Dtype()` (or just letting it be float if NaNs are present and not coercing to `int`) would be more aligned with how pandas handles missing integers. However, for an output display, this is minor.",gpt_4o_mini,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Video views' and 'Likes' columns
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)

# Fill missing values in 'Category' column
df['Category'].fillna('Unknown', inplace=True)

# Group by 'Category' and calculate sums
category_sums = df.groupby('Category').agg...",9,"The generated code correctly performs all the required operations: loading the CSV, cleaning 'Video views' and 'Likes' columns as specified (removing commas, converting to numeric, handling errors with NaN and then filling NaNs with 0), filling missing 'Category' values, and finally grouping by 'Category' to calculate the sum of cleaned views and likes. The code is concise and uses appropriate pandas methods effectively.

A minor improvement is that the generated code assumes 'Video views', 'Likes', and 'Category' columns always exist. While this is often the case for benchmark datasets, robust production-ready code (like the ground truth) might include checks for column existence or `try-except` blocks for file operations. However, for a data science benchmark scenario where the dataset structure is usually guaranteed, this is a very strong and efficient solution. The ground truth also explicitly prints a descriptive header before the result, which is good practice for clarity. The generated code just prints the DataFrame, which is acceptable but less descriptive.",gpt_4o_mini,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the Likes column
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)

# Fill missing values in the Category column
df['Category'] = df['Category'].fillna('Unknown')

# Create groups based on Category
music_likes = df[df['Category'] == 'Music']['Likes']
sports_likes = df[df['Category'] == 'Sports']['Likes']

# Perform an independent two-...",8,"The generated code successfully performs the core logic of the task, including loading the data, cleaning the `Likes` column, filling missing `Category` values, creating the groups, and performing the t-test. It correctly uses `pd.to_numeric` with `errors='coerce'` and `fillna(0)` for the `Likes` column, and `fillna('Unknown')` for `Category`. The `ttest_ind` call with `equal_var=False` is also correct.

Areas for improvement:
*   **Hypotheses Statements Clarity:** While the code includes comments for the hypotheses, they are very brief. The detailed and formal statements provided in the ground truth regarding ""statistically significant difference"" and mathematical equality (μ_music = μ_sports) are much better. The prompt specifically asked to ""Clearly state the null and alternative hypotheses"".
*   **Interpretation Clarity:** The interpretation ""Reject the null hypothesis."" or ""Fail to reject the null hypothesis."" is correct but lacks the subsequent conclusive statement about what that means in the context of the problem, as provided in the ground truth (""Conclusion: There is a statistically significant difference..."").
*   **Robustness:** The generated code does not include error handling for `FileNotFoundError` when loading the CSV, which could make the script crash if the file is not present.
*   **Output Formatting/Verbosity:** The ground truth code provides more informative print statements throughout the execution, indicating progress and confirmation of completed steps (e.g., ""Dataset loaded successfully."", "" 'Likes' column cleaned...""). This is good practice for user-facing scripts but not strictly required by the prompt.
*   **Edge Case Handling:** The ground truth includes a check for an insufficient number of samples in the `music_likes` or `sports_likes` groups, which is a good robustness check for t-tests. This was not explicitly requested but adds to the completeness.

Overall, it's a solid solution that accomplishes the main objective, but could be enhanced with better output, more formal hypothesis statements, and error handling.",gpt_4o_mini,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Likes' and 'Dislikes' columns
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)
df['Dislikes'] = pd.to_numeric(df['Dislikes'].str.replace(',', ''), errors='coerce').fillna(0)

# Create TotalEngagement column
df['TotalEngagement'] = df['Likes'] + df['Dislikes']

# Create LikeDislikeRatio column
df['LikeDislikeRatio'] = df['Likes'] / (df['Dislikes'] + 1)

# D...",9,"The generated code correctly addresses all requirements of the task. It successfully loads the dataset, cleans the `Likes` and `Dislikes` columns by removing commas, converting to numeric, and filling NaNs with 0. It then correctly calculates `TotalEngagement` and `LikeDislikeRatio` as specified. Finally, it displays the required columns.

The strong points of the solution are its conciseness and efficiency in performing the cleaning steps. The chaining of `str.replace(',', '')`, `pd.to_numeric`, and `fillna(0)` is an elegant and efficient way to handle the data cleaning.

The only minor area for improvement, which doesn't detract significantly from its correctness but is present in the ground truth, is the lack of explicit error handling for file loading (`FileNotFoundError`). While not strictly required by the prompt, robust code often includes such handling. However, given the prompt's focus on feature engineering, this is a very minor point. The result display also doesn't include the introductory print statement, but functionally it displays the correct information.",gpt_4o_mini,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the 'Launched Price (USA)' column
df['Launched Price (USA)'] = df['Launched Price (USA)'].replace({'USD ': '', ',': ''}, regex=True)
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'], errors='coerce')

# Clean the 'Screen Size' column
df['Screen Size'] = df['Screen Size'].replace({' inches': ''}, regex=True)
df['Screen Size'] = pd.to_numeric(df['Screen Size'], errors='coerce')

# G...",9,"The generated code correctly addresses all requirements of the task. It loads the CSV, cleans both specified columns by removing unwanted strings and converting them to numeric (handling errors with `errors='coerce'`), groups the data as required, and calculates the correct averages. The output format is also correct. The use of `replace` with `regex=True` is concise for cleaning the price column.

A minor improvement could be to use `.str.replace` instead of `.replace` on the series for string operations, as `.replace` on a Series with regex set to true might behave differently with non-string dtypes if not handled carefully, though in this case `pd.to_numeric` would likely handle initial non-string types on the subsequent line. Also, it might be slightly more robust to explicitly define new columns for the cleaned data rather than overwriting the original columns, although for this specific task, overwriting is acceptable and common. Error handling for file loading is also missing, but the prompt didn't explicitly ask for it in the output code, so it's not a deduction against core functionality. Overall, it's a very good solution.",gpt_4o_mini,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Function to clean camera columns
def clean_camera(column):
    match = re.search(r'(\d+)', column)
    return int(match.group(1)) if match else 0

# Apply the function to create cleaned camera columns
df['Cleaned Front Camera'] = df['Front Camera'].apply(clean_camera)
df['Cleaned Back Camera'] = df['Back Camera'].apply(clean_camera)

# Create a new feature Total Camera MP
df['Total Camera MP'] = d...",9,"The generated code correctly addresses all aspects of the task. It loads the dataset, defines and applies a function to clean camera specifications by extracting the first sequence of digits, creates the 'Total Camera MP' feature, and displays the specified columns for the first 10 rows. The use of regex `r'(\d+)'` correctly captures the digits, and `if match else 0` handles cases where no digits are found.

Minor improvements could include:
1. Adding error handling for `FileNotFoundError` when loading the CSV, similar to the ground truth, to make it more robust.
2. The `clean_camera` function does not explicitly handle `NaN` or non-string inputs from the DataFrame beyond what `re.search` would implicitly do (which is usually to raise a `TypeError` if `column` isn't a string-like object). While `apply` often works well with mixed types, explicitly converting to string or handling `NaN` as shown in the ground truth's `pd.isna` check makes the function more robust. However, for this specific task context, `apply` on a typical column with strings/NaNs usually handles it gracefully enough that `re.search` will treat `NaN` as `None` or `str(NaN)` will work.
3. Adding docstrings or comments for better readability, although the task did not explicitly require it.

Overall, it's a very good solution that meets the requirements effectively.",gpt_4o_mini,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, r2_score

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the specified columns
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', '', regex=False), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', '', regex=False), errors='coerce')
df['Screen Size'...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, performs the necessary cleaning operations on the specified columns, handles conversion errors by coercing to NaN and then dropping those rows, defines features and target, splits the data, trains a Linear Regression model, and reports the R-squared and MAE scores.

**Strong points:**
*   **Correct Cleaning:** The cleaning process for all specified columns (RAM, Battery Capacity, Screen Size, Launched Price (USA)) is correctly implemented, including removing multiple patterns for 'Launched Price (USA)' and handling `errors='coerce'` for `pd.to_numeric`.
*   **Correct NaN Handling:** `dropna(subset=...)` is correctly used to remove rows with NaNs only in the cleaned columns, as specified.
*   **Correct Model Workflow:** The steps for defining X and y, splitting data, training the model, and evaluating it are all correctly followed.
*   **Proper Imports:** All necessary libraries are imported.
*   **Reproducibility:** `random_state=42` is used for `train_test_split`.

**Minor areas for improvement (to achieve a perfect 10):**
*   **Robustness against non-string types:** The ground truth converts columns to `str` first before `str.replace`. While `df[column].str.replace` generally works fine, if a column were to contain non-string numeric types (e.g., `NaN`s, or actual numbers) before the cleaning step, directly calling `.str` can sometimes lead to errors or warnings. Explicitly converting to string first (`.astype(str)`) makes it more robust. However, for a typical CSV from a web scraper where these fields are often strings, it might not be strictly necessary.
*   **Error handling for file loading:** The ground truth includes a `try-except FileNotFoundError` block for `pd.read_csv`, which is good practice for robust code but not explicitly requested by the prompt.
*   **Code readability for `str.replace`:** While functional, using a loop or a dictionary for cleaning patterns as in the ground truth might be slightly cleaner/more maintainable if the list of columns or their cleaning patterns were to grow significantly. However, for this specific task, the direct approach is perfectly clear.",gpt_4o_mini,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency, ttest_ind, f_oneway
from statsmodels.stats.weightstats import DescrStatsW

# Load dataset
titanic_data = pd.read_csv('titanic.csv')

# 1) Chi-square test for association between Pclass and Survival
contingency_table = pd.crosstab(titanic_data['Pclass'], titanic_data['Survived'])
chi2_stat, p_val, _, _ = chi2_contingency(contingency_table)
alpha = 0.05
print(f""Chi-square Test:\n Statistic: {chi2_stat}, p-va...",6,"The generated code correctly performs the chi-square, t-test, and ANOVA. However, it falls short on several critical aspects concerning best statistical practices and completeness.

**Areas for Improvement:**

1.  **Missing Effect Size for Chi-square:** The request explicitly asked for effect sizes for *each* test. Cramer's V for the chi-square test is completely missing, which is a significant omission.
2.  **Incorrect Cohen's d Calculation:** The formula used for Cohen's d in the generated code `(np.var(survivors_ages, ddof=1) + np.var(non_survivors_ages, ddof=1)) / 2` calculates the *average variance*, not the pooled standard deviation, which is essential for Cohen's d. This will lead to an incorrect Cohen's d value, especially if variances are different. The ground truth correctly implements the pooled standard deviation for Cohen's d.
3.  **Incorrect Eta-squared Calculation:** The formula for Eta-squared `f_stat / (f_stat + (len(titanic_data) - len(fare_groups)))` is incorrect and does not represent the standard Eta-squared formula (SS_between / SS_total). This will yield a very different (and likely incorrect) effect size for ANOVA. The ground truth correctly calculates Eta-squared based on Sum of Squares.
4.  **Lack of Robustness/Assumptions Testing (T-test & ANOVA):** While the generated t-test uses `equal_var=False` (Welch's t-test), which is good for unequal variances, it doesn't explicitly check for variance equality (e.g., using Levene's test) or report on it. For ANOVA, checking the assumption of homogeneity of variances (e.g., with Levene's test) is crucial for interpreting the results, which the ground truth includes.
5.  **Output Readability and Interpretation:** The output is very minimal. The ground truth provides more context, including hypotheses, clearer conclusions, and values formatted to a reasonable number of decimal places, making the output more professional and easier to interpret for someone who isn't an expert in statistical tests.
6.  **Error Handling (File Not Found):** The ground truth includes basic error handling for `FileNotFoundError`, which is good practice. The generated code lacks this.
7.  **Clarity on `alpha`:** The `alpha` is defined later in the generated code. It's better to define it upfront for all tests.

To achieve a higher score, the generated code needs to correct the effect size calculations, include Cramer's V, and improve the statistical rigor by considering assumptions and providing clearer output.",gpt_4o_mini,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Group by Year and calculate averages
average_stats = df.groupby('Year').agg({'Rating': 'mean', 'Runtime': 'mean'}).reset_index()

# Create a line plot for average Rating over the Year
plt.figure(figsize=(10, 5))
plt.plot(average_stats['Year'], average_stats['Rating'], marker='o', color='b', label='Average Rating')
plt.title('Average IMDb Rating Trend Over the Years')
plt.xlab...",6,"The generated code correctly loads the dataset, groups by year, calculates the average rating and runtime, and plots the average rating vs. year with a title and labels. This addresses the core requirements of the task.

However, there are significant issues that prevent a higher score:
1.  **Robustness (Data Cleaning):** The `imdb_top_250.csv` dataset often has 'Year' formatted as 'Year (Release Year)' (e.g., '1994 (I)') and 'Runtime' as 'X min'. The generated code *does not* include any data cleaning steps for these columns. It assumes they are directly ready for numeric conversion and grouping, which will likely lead to errors or incorrect results when run with the actual dataset. The ground truth code handles this effectively using regex.
2.  **Completeness (Plotting `xticks`):** While `plt.xticks(average_stats['Year'], rotation=45)` is an attempt to improve x-axis labels, it will likely create too many ticks or lead to an unreadable plot if there are many unique years, or if the years are not sequential. A better approach might involve setting fewer, more spaced-out ticks or letting matplotlib determine them, as the ground truth does by just rotating existing ticks if needed.
3.  **Encapsulation/Modularity:** The generated code lacks a function to encapsulate the logic, making it less reusable and testable compared to the ground truth's `analyze_imdb_data` function.
4.  **Error Handling**: There is no error handling for issues like `FileNotFoundError` as seen in the ground truth.

To improve the score, the generated code needs to:
*   Add robust data cleaning for the `Year` and `Runtime` columns as done in the ground truth. This is critical for the code to run successfully on the actual dataset.
*   Consider wrapping the logic in a function.
*   Improve `xticks` handling for better plot readability.",gpt_4o_mini,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create two groups of movies based on the Year column
group_before_1990 = df[df['Year'] < 1990]['Rating']
group_after_1990 = df[df['Year'] >= 1990]['Rating']

# Perform an independent two-sample t-test
t_statistic, p_value = stats.ttest_ind(group_before_1990, group_after_1990)

# Null and alternative hypotheses
# Null Hypothesis (H0): There is no difference in mean Rating between movi...",7,"The generated code correctly performs the core task of loading data, creating groups, and running the t-test. It also correctly interprets the p-value and states the hypotheses.

However, there are a few areas for improvement:
1.  **Robustness (Data Loading):** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth and good practice.
2.  **Data Type Handling:** The generated code directly uses `df['Year']` and `df['Rating']` for filtering and the t-test without explicitly converting them to numeric types or handling potential errors (`errors='coerce'`). If 'Year' or 'Rating' columns contain non-numeric data, this could lead to unexpected behavior or errors. The ground truth explicitly handles this.
3.  **Completeness of Hypotheses:** While the hypotheses are stated in comments, the task explicitly asked to ""Clearly state the null and alternative hypotheses for this test"" and the ground truth prints them out, making them part of the direct output rather than just internal notes.
4.  **Output Formatting:** The output for the t-statistic and p-value is less formatted than the ground truth (`:.4f`), which is a minor aesthetic point but contributes to readability.
5.  **Assumption for T-test:** The `equal_var` parameter in `ttest_ind` is not explicitly set. By default, `scipy.stats.ttest_ind` assumes `equal_var=True`. While this might be correct based on implicit understanding, explicitly setting it (`equal_var=True` or `False`) makes the intent clearer and avoids potential issues if the default changes or if the user's assumption of variance equality is different. The ground truth provides context for this.",gpt_4o_mini,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create a new column named `Decade`
df['Decade'] = (df['Year'] // 10 * 10).astype(str) + 's'

# Display the Title, Year, and the new Decade column for the first 10 movies
print(df[['Title', 'Year', 'Decade']].head(10))
```",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, accurately creates the `Decade` column using the described logic (e.g., 1990s from 1990-1999), and displays the requested columns for the first 10 rows. The code is concise and directly implements the stated steps.

The only minor area for improvement, especially for a ""benchmark"" context, would be to include error handling (e.g., `try-except` for `FileNotFoundError`) and potentially encapsulate the logic within functions for better modularity and reusability, similar to the ground truth. However, for the specific task requirements of providing ""only the Python code without explanations"" for the core logic, it performs excellently. The conversion of `Year` to numeric before division isn't explicitly done, but `pd.read_csv` often infers numeric types correctly for columns like 'Year', making the `// 10` operation work as expected.",gpt_4o_mini,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Impute missing values for numerical columns with their mean
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numerical_cols:
    df[col].fillna(df[col].mean(), inplace=True)

# Impute missing values for the categorical 'sex' column with the mode
df['sex'].fillna(df['sex'].mode()[0], inplace=True)

# Group the data by 'species' and 'sex' and calculate the av...",9,"The generated code correctly performs all the required steps: loading the dataset, imputing numerical columns with means, imputing the categorical `sex` column with its mode, grouping by `species` and `sex`, and calculating the average `body_mass_g`. The code is concise and clear.

One minor point is that the ground truth code includes error handling (e.g., `FileNotFoundError`), which is good practice for robustness in a real-world scenario. While the prompt did not strictly require error handling, its absence makes the generated code slightly less robust than the ground truth. Additionally, the ground truth explicitly prints a descriptive string before the results, which is a nice touch for clarity, whereas the generated code just prints the DataFrame, which is acceptable but less descriptive. However, these are minor details and do not detract from the core functionality being correct.",gpt_4o_mini,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Step 1: Load the dataset
df = pd.read_csv('penguins.csv')

# Step 2: Filter the data for Adelie and Gentoo species
filtered_df = df[df['species'].isin(['Adelie', 'Gentoo'])]

# Step 3: Remove rows with missing flipper_length_mm or species
filtered_df = filtered_df.dropna(subset=['flipper_length_mm', 'species'])

# Step 4: Perform an independent two-sample t-test
adelie_flipper_length = filtered_df[filtered_df['species'] == 'Adelie']['flipp...",9,"The generated code correctly addresses almost all requirements of the task. It successfully loads the data, filters for the specified species, handles missing values, performs the t-test, and interprets the results based on the p-value and alpha.

Strong points:
- Correctly loads the dataset.
- Filters the data as required.
- Correctly uses `dropna` on the specified subset.
- Splits the data for the two groups correctly.
- Performs the `ttest_ind` correctly.
- Prints the t-statistic and p-value.
- Interprets the p-value against alpha and states the conclusion correctly.

Areas for minor improvement/consideration:
- **Hypotheses Statement:** The ground truth explicitly prints the null and alternative hypotheses, which is a better fulfillment of ""Clearly state the null and alternative hypotheses for this test"" than just having them as comments. While commenting them fulfills the ""state"" part, printing them makes them part of the output, which is generally preferred for clarity in such tasks.
- **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and other general exceptions during file loading. While not strictly required by the prompt's explicit steps, it's good practice for production-ready code.
- **`equal_var` parameter in `ttest_ind`**: The ground truth explicitly discusses the `equal_var` parameter for `ttest_ind` and chooses `False` for robustness. The generated code uses the default (`True`), which implies an assumption of equal variances. While the prompt didn't specify which one to use, explicitly considering or mentioning this choice would enhance the solution's completeness from a statistical standpoint. However, simply using the default is not an error given the prompt's wording.
- **Copying filtered_df:** The ground truth uses `.copy()` after filtering (`filtered_df = df[species_filter].copy()`) to prevent `SettingWithCopyWarning` when subsequent operations are performed on `filtered_df`. While not strictly necessary for this specific code given how `dropna` is used (it reassigns `filtered_df`), it's good practice.

Overall, the code is highly functional and accurate for the given task. The minor points are more about best practices and explicit communication rather than incorrectness.",gpt_4o_mini,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1. Load the dataset
data = pd.read_csv('penguins.csv')

# 2. Select the features and target
X = data[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]
y = data['species']

# 3. Drop rows with missing values
data_cleaned = data.dropna(subset=['sp...",9,"The generated code correctly implements all specified steps for the task. It loads the data, selects the features and target, handles missing values (though in a slightly less efficient way than the ground truth), encodes the target, splits the data, trains the model, and reports accuracy.

**Strong Points:**
- All required libraries are imported correctly.
- All steps outlined in the task are addressed.
- The `max_iter=1000` for Logistic Regression is correctly set.
- The `random_state=42` for `train_test_split` is correctly used.
- The accuracy calculation and printing format are correct.

**Areas for Minor Improvement (preventing a perfect 10):**
- **Efficiency in Handling Missing Values:** In step 3, the code re-creates `X` and `y` *after* dropping NAs from `data_cleaned`. It would be slightly more efficient to derive `X` and `y` directly from `data_cleaned` once, rather than selecting them from the original `data` first and then re-selecting after cleaning. The current approach works but involves an unnecessary intermediate `X` and `y` definition from the uncleaned `data` DataFrame. The ground truth approaches this by first selecting the relevant columns into `df_selected` and then dropping NAs from `df_selected` to create `df_cleaned`, ensuring that `X` and `y` are always derived from the truly cleaned subset.
- **Robustness (Error Handling):** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is good practice for production code. While not explicitly asked for, adding such robustness improves code quality.
- **Clarity/Prints:** The ground truth includes print statements to indicate progress and data shapes, which is helpful for debugging and understanding what the code is doing. The generated code is functional but lacks these informative outputs.

Despite these minor points, the code is very good and fully accomplishes the core requirements of the task.",gpt_4o_mini,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load the dataset
df = pd.read_csv('penguins.csv')

# 2. Drop rows where `body_mass_g` or `species` is NA
df_clean = df.dropna(subset=['body_mass_g', 'species'])

# 3. For each `species`, calculate the average `body_mass_g`
average_body_mass = df_clean.groupby('species')['body_mass_g'].mean().reset_index()

# 4. Create a bar plot showing the average `body_mass_g` for each `species`
plt.figure(figsize=(10, 6)...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, correctly handles NAs in the specified columns, calculates the average body mass per species, and generates a well-labeled bar plot. The use of seaborn for the bar plot is appropriate and the aesthetic choices like `palette='viridis'` and `plt.tight_layout()` enhance readability. The only minor point of improvement could be adding a `try-except` block for file loading, as present in the ground truth, to make the code more robust against `FileNotFoundError`. However, given the prompt's simplicity and typical benchmark setup where the file is assumed to exist, its omission doesn't detract significantly from the core task completion. The generated code is concise and clear, demonstrating a good understanding of both pandas and matplotlib/seaborn functionalities for data analysis and visualization.",gpt_4o_mini,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Step 1: Load the dataset
df = pd.read_csv('penguins.csv')

# Step 2: Drop rows where specific columns are NA
df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'], inplace=True)

# Step 3: Create a new feature culmen_area
df['culmen_area'] = df['culmen_length_mm'] * df['culmen_depth_mm']

# Step 4: Calculate the Pearson correlation matrix
correlation_columns = ['culmen_le...",9,"The generated code successfully completes all tasks outlined in the prompt: loading the data, dropping NA values in the specified columns, creating the new `culmen_area` feature, calculating the Pearson correlation matrix for the required columns, and visualizing it as an annotated heatmap with the correct title.

**Strengths:**
- **Correctness:** All an
steps are implemented correctly and achieve the desired output.
- **Completeness:** All five requirements from the task description are met.
- **Conciseness:** The code is direct and to the point, avoiding unnecessary lines.
- **Pandas and Seaborn Usage:** Correctly uses `pd.read_csv`, `df.dropna`, new column creation, `df.corr()`, and `sns.heatmap`.

**Areas for Minor Improvement (prevented a perfect 10):**
- **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError` and checks for an empty DataFrame after dropping NAs. While not explicitly requested in the prompt, robust error handling is good practice for production-ready code. The generated code lacks these checks, which could lead to exceptions if the file is missing or if all rows are dropped.
- **`inplace=True`:** While `inplace=True` works, it's often recommended to avoid it and instead assign the result to a new DataFrame (e.g., `df = df.dropna(...)`) or to the same DataFrame (like `df = df.dropna(...)`) to make it explicit that the DataFrame is being modified. The `inplace=True` can sometimes lead to unexpected side effects or chained assignments warnings in more complex scenarios. The ground truth example correctly uses `.copy()` after `dropna` which is a safer approach for this kind of operation to avoid `SettingWithCopyWarning`. However, for a simple `dropna` directly modifying `df`, it's generally acceptable.

Overall, it's a very good solution that directly addresses the prompt's requirements. The minor points are mostly about robustness and best practices rather than functional correctness for the given task.",gpt_4o_mini,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd

# Step 1: Load the dataset
df = pd.read_csv('penguins.csv')

# Step 2: Select features and target variable
features = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']]
target = df['species']

# Step 3: Impute NA values for numerical features with the mean
numerical_features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
features[numerical_features] = features[numerical_features].fillna(fe...",9,"The generated code successfully addresses almost all aspects of the task.

**Strong Points:**
- **Correctness:** It correctly loads the data, selects the specified features, imputes numerical NAs with the mean, handles the `sex` column as required (imputation and mapping), and applies one-hot encoding to `island`.
- **Conciseness:** The code is quite concise and efficient in its implementation of the steps.
- **Completeness:** All explicit requirements from the prompt are met.

**Areas for Improvement (minor):**
- **SettingWithCopyWarning:** The line `features[numerical_features] = features[numerical_features].fillna(features[numerical_features].mean())` and subsequent assignments to `features['sex']` directly modify a slice of the DataFrame (`features`) that was created without `.copy()`. This can lead to a `SettingWithCopyWarning` in pandas, indicating that you might be operating on a view rather than a copy, and changes might not propagate back to the original DataFrame (though in this specific case, it often works as intended due to assignment). Using `df.copy()` after selecting features would prevent this warning and is generally better practice.
- **`drop_first=True` in `get_dummies`:** While `drop_first=True` is common in one-hot encoding to avoid multicollinearity (for modeling), the prompt did not explicitly ask for it. The ground truth typically leaves all dummy variables unless specified. This is a minor deviation but worth noting.
- **No error handling for `FileNotFoundError`:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust. The generated code assumes the file is always present.
- **Explicit target separation:** The prompt states ""The goal is to predict `species`... The target variable is `species`."" and later asks for ""the shape of the resulting feature matrix"". The generated code correctly creates `features` (which acts as X) but also creates `target` (y) which is good practice for classification tasks, though it's not explicitly printed. The final print statements only show `features.shape` and `features.head()`, which is consistent with the ""feature matrix"" request.

Overall, it's a very good solution that directly addresses the prompt's requirements effectively. The points for improvement are mostly about robustness and best practices in pandas, rather than functional correctness for the given task.",gpt_4o_mini,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

# Create the DataFrame with the provided data
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics'] * 7,
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST001']
}

df = pd.DataFrame(data)

# 1) Calculate total s...",9,"The generated code correctly creates the DataFrame, calculates the total spending per customer, identifies the top 3 customers by spending, and calculates the total quantity sold per category. The import is correct, and the code structure is logical.

The only minor point for improvement, which prevents a perfect score, is that the output format for the ""Top 3 Customers by Spending"" task is slightly different from the ground truth. While technically correct (it prints a DataFrame), the ground truth explicitly formatted the output with ""CustomerID: ..., Total Spending: $..."" and this small stylistic difference, although not incorrect, is the only distinction. Similarly, the ground truth prints the category total quantity directly as a Series, while the generated code uses `reset_index()` which results in a DataFrame. Both are valid, but the ground truth's approach might be considered slightly more direct for a single-column output. Otherwise, the code is very good.",gpt_4o_mini,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('tit...",6,"The generated code addresses most of the requirements but has several issues in correctness and completeness compared to the ground truth.

**Strengths:**
*   Correctly uses a `Pipeline` for preprocessing and modeling.
*   Includes common evaluation metrics (though not all requested are explicitly printed).
*   Correctly visualizes confusion matrix and ROC curve.
*   Performs cross-validation.

**Areas for Improvement / Issues:**

1.  **Data Preprocessing: Feature Engineering:** The generated code misses the feature engineering step of creating `FamilySize` by combining `SibSp` and `Parch`. The ground truth creates this important feature, which often improves model performance on this dataset.
2.  **Data Preprocessing: Handling Irrelevant Columns:** The generated code only drops 'Cabin' and 'Ticket'. It should also drop 'PassengerId' and 'Name' as done in the ground truth, as these are not relevant for prediction.
3.  **Preprocessing Pipeline - `ColumnTransformer` (Correctness):**
    *   The `ColumnTransformer` syntax is incorrect: `transformers=[('num', 'passthrough', ...)]` should be `transformers=[('num', 'passthrough', ...)]`. It's missing `(` and `)`. This would cause a syntax error.
    *   It treats 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare' as numerical and uses `'passthrough'` for them in the preprocessor, THEN incorrectly adds a `StandardScaler()` after the `ColumnTransformer` in the pipeline. `StandardScaler` should be applied *within* the numerical part of the `ColumnTransformer` to ensure only numerical features are scaled and before one-hot encoding. In the ground truth, Pclass is treated as categorical, which is generally a better approach for this dataset (as its numerical value doesn't imply a linear relationship).
4.  **Evaluation Metrics Completeness/Explicit Output:** While `classification_report` prints accuracy, precision, recall, and F1-score, it doesn't explicitly print ROC-AUC. The request specifically asks for all these metrics to be displayed individually with their values, which the ground truth does.
5.  **Feature Coefficients Interpretation:** The generated code displays feature coefficients but lacks the interpretation aspect as requested and provided in the ground truth.
6.  **Feature Coefficients Mapping (Correctness):** The way `feature_names_out()` is accessed is problematic. `preprocessor.transformers_[0][1].get_feature_names_out()` will try to get names from 'passthrough', which doesn't have a `get_feature_names_out()` method in the same way `OneHotEncoder` does for example. It also assumes `preprocessor.transformers_[0]` is the numerical one and `preprocessor.transformers_[1]` is the categorical one, which is brittle. The ground truth's approach using `model_pipeline.named_steps['preprocessor'].get_feature_names_out()` after the preprocessor has been fitted on the training data is more robust.
7.  **Reproducibility:** The ground truth includes `np.random.seed(42)` which is good practice for reproducibility.
8.  **Output Clarity:** The ground truth provides more descriptive print statements regarding data loading, training status, and interpretation of results.",gpt_4o_mini,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']),
    'Ticker': ['TECHCO'] * 8,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0, 157.0, 154.0],
    'Vol...",10,"The generated code is entirely correct and complete. It accurately addresses all parts of the task:
- It correctly initializes the DataFrame with the specified data and sets 'Date' as the index.
- It correctly calculates the 3-day Simple Moving Average ('SMA_3Day') for 'Close' price, properly handling the initial NaN values for the first two days due to the `rolling()` window.
- It correctly calculates the percentage change for 'Volume' ('Volume_Pct_Change') from the previous day, also correctly producing a NaN for the first entry.
- It prints the resulting DataFrame as requested.
- The code uses standard pandas functions effectively and efficiently.
- Imports are correct.
- The structure is clean and readable.",gpt_4o_mini,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]
}

df = pd.Data...",9,"The generated code correctly performs all requested operations: it creates the DataFrame, calculates the 'Score_Improvement' column, computes the average improvement per subject, and identifies students meeting the specified criteria. The code is clean and uses appropriate pandas functionalities.

One minor area for improvement is the printing of results. While functional, the ground truth provides clearer labels and separation for each output, making the results easier to interpret. For example, instead of just `print(average_improvement)`, adding a descriptive string like `print(""Average Score Improvement per Subject:"")` would enhance readability. Similarly, for the final student list, printing it as a list by default is fine, but the ground truth's loop with `if not student_ids_meeting_criteria.empty` provides more robust handling for cases where no students meet the criteria. However, for the given dataset, both approaches yield the correct output. These are minor stylistic points and do not detract from the correctness or core functionality of the code.",gpt_4o_mini,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

# Filter for Tech department employees with PerformanceS...",9,"The generated code correctly defines the DataFrame, filters it according to the specified conditions, and accurately calculates the average number of ""ProjectsCompleted"" for the filtered group. The code is concise and directly addresses all parts of the prompt.

One minor area for improvement, though not critical to correctness for this specific task, would be to add a check for an empty `filtered_df` before calculating the mean, similar to the ground truth code. This prevents errors if no employees match the criteria. Additionally, the ground truth code formats the output to two decimal places and provides a more descriptive print statement, which enhances readability and user understanding. However, for core functionality, the generated code is excellent.",gpt_4o_mini,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate conversion rate for Desktop
desktop_users = df[df['DeviceType'] == 'Desktop']
desktop_conver...",9,"The generated code correctly defines the DataFrame and accurately calculates and prints the conversion rates for Desktop and Mobile users, fulfilling all the task requirements. The code is clean, readable, and uses appropriate pandas features (filtering, `.mean()`).

The only very minor point for improvement (and why it's not a perfect 10) is the lack of explicit handling for edge cases where `desktop_users` or `mobile_users` might be empty. While this specific input data guarantees non-empty subsets, robust code would typically include checks like `if not desktop_users.empty:` before calling `.mean()` to prevent potential `NaN` or errors if a `DeviceType` filter yielded no rows in different datasets. The ground truth code demonstrates this robustness with `if not ... empty:` checks and fallback values. However, for this specific problem context where the data is fixed and guaranteed to have both device types, the generated code is perfectly functional and correct.",gpt_4o_mini,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

df = pd.DataFrame(data)

# Calculate and print the average AirQualityIndex for each unique SensorID
ave...",10,"The generated code correctly defines the DataFrame as specified in the task. It then accurately calculates the average ""AirQualityIndex"" for each unique ""SensorID"" using `groupby()` and `mean()`, which is an efficient and standard pandas operation. Finally, it iterates through the results and prints them clearly, fulfilling all requirements of the task. The code is concise, readable, and directly addresses all parts of the prompt.",gpt_4o_mini,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

# Create the EngagementScore
df['EngagementScore'] = df['Likes'...",9,"The generated code correctly performs all the required tasks: it defines the DataFrame, calculates the EngagementScore, filters for Instagram posts, and computes their average EngagementScore. The code is clean, concise, and uses proper pandas operations. The primary reason for not giving a perfect 10 is a minor formatting difference in the final print statement compared to the ground truth (no f-string formatting to two decimal places). While functionally correct, the output presentation differs slightly from the ground truth's specific output format.",gpt_4o_mini,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the ""LengthOfStay"" column accurately, filters the DataFrame to include only patients with a ""Flu"" diagnosis, and correctly calculates and prints their average ""LengthOfStay"". The code is concise, follows good practices for pandas operations, and fully addresses all requirements of the task. The use of `.dt.days` is correct for converting timedelta to an integer number of days. The logic flow is clear and efficient.",gpt_4o_mini,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}

df = pd.DataFrame(data)

# Filter for Sci-Fi movies
...",9,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates their average rating. All key requirements of the task are met. The code is clean, concise, and follows standard pandas practices.

The only minor point for improvement, which is not a correctness issue for this specific dataset but good practice in general, would be to add a check for an empty filtered DataFrame (`sci_fi_movies`) before attempting to calculate the mean. If `sci_fi_movies` were empty (i.e., no Sci-Fi movies in the data), `sci_fi_movies['Rating'].mean()` would return `NaN` (or `None` in some contexts), which might require specific handling depending on downstream operations. However, for this problem, the dataset guarantees Sci-Fi movies will be present, so it's not a critical flaw. Additionally, the output format for printing the average rating could be slightly more user-friendly, perhaps using an f-string to explain what the number represents, as done in the ground truth. But this is a minor presentation detail.",gpt_4o_mini,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

df = pd.DataFrame(data)

# Filter the DataFrame for Toyota cars
toyota_cars = df[df['Brand'] ...",9,"The generated code is highly correct and successfully completes all parts of the task: it properly defines the DataFrame, filters it to include only Toyota cars, and accurately calculates their average mileage. The code quality is good, with clear variable names and logical flow.

The only minor area for improvement, preventing a perfect 10, is the lack of explicit formatting in the print statement for the average mileage, as seen in the ground truth (`.2f`). While not strictly required by the prompt, it usually makes the output more user-friendly in data science contexts. Also, the ground truth includes a robust check for an empty DataFrame before calculating the mean, which is good practice, though not strictly necessary given the test case.",gpt_4o_mini,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 80...",9,"The generated code correctly defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat. It correctly imports pandas and follows a logical flow to accomplish the task.

The only minor area for improvement that prevents a perfect score is the output format. The ground truth code prints the average with a more descriptive f-string and formats it to two decimal places, which is generally good practice for numerical results in data science. The generated code just prints the raw float value, which is less user-friendly. Additionally, the ground truth includes an `if not fire_pokemon_df.empty:` check, which handles an edge case where no Fire-type Pokemon might exist, preventing potential errors or providing more informative output in such scenarios. While not strictly necessary for this specific dataset, robust code often considers such edge cases.",gpt_4o_mini,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Load the dataset
titanic = pd.read_csv('titanic.csv')

# Preprocessing
titanic['AgeGroup'] = pd.cut(titanic['Age'], bins=[0, 12, 20, 40, 60, 80], 
                              labels=['Child', 'Teen', 'Adult', 'Middle Age', 'Senior'])

# 1. Create a multi-panel figure showing survival rates by different demographic groups
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Class vs...",7,"The generated code addresses most of the tasks, but several aspects could be improved for higher quality and completeness, especially regarding the ""advanced"" and ""storytelling"" aspects.

**Strengths:**
*   **Correctness:** All plots are correctly generated and visualize the requested relationships.
*   **Imports:** Proper imports are included.
*   **Basic Task Completion:** It successfully creates the multi-panel figure, heatmap, violin plots, and interactive scatter plot.

**Areas for Improvement:**
*   **Data Preprocessing and Robustness:** The ground truth includes more robust preprocessing (handling missing 'Age' and 'Fare' values, creating more descriptive categorical columns like `Pclass_Cat` and `Survived_Cat`, and properly ordering categories). The generated code's `AgeGroup` bins are a bit arbitrary and not as refined as the ground truth. Missing value handling (especially for Age) is critical for Titanic datasets.
*   **Advanced Visualization & Storytelling (Dashboard):**
    *   The dashboard in the generated code is very basic. It simply re-uses three bar plots from the first task and one heatmap, arranging them in a 2x2 grid. This falls short of a ""comprehensive dashboard-style figure combining multiple visualizations with proper annotations and insights"" as requested.
    *   The ground truth's dashboard uses `gridspec` for a more flexible layout, combines different types of plots (survival rates, density plots for age/fare), and crucially includes a dedicated panel for ""Key Insights"" with descriptive text annotations. This adds significant storytelling value, which is a key requirement of the task. The generated code lacks any textual insights or interpretations.
*   **Plot Customization and Aesthetics:** While the generated plots are functional, they lack the polished appearance and detailed annotations found in the ground truth.
    *   Y-axis limits `(0, 1)` for survival rates are missing in the generated code, which can make comparisons across plots less consistent.
    *   Lack of shared y-axis labels in multi-panel plots for readability.
    *   Less descriptive labels for `Pclass` (e.g., using `1`, `2`, `3` instead of '1st Class', '2nd Class', '3rd Class').
*   **Interactive Plot:** The Plotly scatter plot is functional, but the ground truth enhances it by mapping `Survived` to symbols (`Survived_Str`) and providing more extensive `hover_data`, which significantly improves the interactive experience and data exploration. The generated code only uses `color='Pclass'`.",gpt_4o_mini,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, ...",9,"The generated code correctly defines the DataFrame, filters for the 'Dystopian' genre, and accurately calculates the average stock for that genre, fulfilling all task requirements. The code is clean, well-structured, and uses appropriate pandas operations. The only minor improvement would be to include formatting for the printed average stock, similar to the ground truth (e.g., `:.2f`). Additionally, while not strictly required by the prompt, the ground truth's inclusion of an `if not dystopian_books_df.empty:` check for robustness is a good practice for production-level code, although for this specific predefined dataset, it's not critical.",gpt_4o_mini,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Separate potable and non-potable samples
potable = df[df['potability'] == 1]
non_potable = df[df['potability'] == 0]

# List of water quality parameters
parameters = ['pH', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']

# Function to calculate Cohen's d
def cohen_d(x1, x2):
    return (np.mean(...",6,"The generated code performs the requested statistical tests but has several significant issues and omissions that prevent it from being a high-quality solution.

**Strengths:**
*   Correctly identifies and uses `scipy.stats` for t-tests, chi-square, and Shapiro-Wilk tests.
*   Attempts to calculate Cohen's d.
*   Includes all requested types of tests.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling:** This is a critical omission. The ground truth explicitly addresses missing values using mean imputation. The generated code only uses `.dropna()` when performing tests, which is problematic:
    *   It drops rows for each parameter independently, which can lead to different sample sizes for different tests, making comparisons difficult.
    *   It doesn't impute, so information is lost if rows with NaNs are dropped.
    *   The prompt asked for proper handling of missing values.

2.  **Case Sensitivity:** The generated code uses `df['potability']` (lowercase 'p'), while the dataset (and ground truth) likely has `df['Potability']` (uppercase 'P'). This will cause a `KeyError` unless the actual CSV has a lowercase column, but generally, the column name is 'Potability'. (Correction: The ground truth uses `Potability`, so this is a potential issue).

3.  **Chi-square Test Misinterpretation:**
    *   The prompt asks for a ""chi-square goodness-of-fit test to check if potability follows expected distribution"". The generated code calculates `expected_dist = [len(potable), len(non_potable)]`. This means the ""expected"" distribution is *exactly* the observed distribution, which will trivially result in a chi-square statistic of 0 and a p-value of 1 (unless there's a floating-point error), making the test meaningless.
    *   A goodness-of-fit test requires an *a priori* expected distribution (e.g., 50/50 split, or based on prior research). The ground truth correctly assumes a 50/50 split as a reasonable default for a binary variable when no expectation is specified.
    *   The parameters `observed_dist` and `expected_dist` are identical.

4.  **Cohen's d Calculation:**
    *   The formula used for Cohen's d calculates the pooled standard deviation `sqrt(np.var(x1) + np.var(x2) / 2)` which is then used `np.sqrt(((len(x1) - 1) * np.var(x1) + (len(x2) - 1) * np.var(x2)) / (len(x1) + len(x2) - 2))`. This is correct if `ddof=0` for `np.var`, but `np.var` uses `ddof=0` by default (population variance). For sample variance, `ddof=1` is needed. Standard Cohen's d uses sample standard deviations. It's better to use `np.std(data, ddof=1)`. The ground truth implements this more robustly.
    *   More importantly, the `cohen_d` function doesn't account for cases where `len(x1) + len(x2) - 2` is zero or less (e.g., very small sample sizes leading to division by zero) or where standard deviations are zero. This will cause runtime errors.

5.  **Interpretation:** The code calculates p-values and significance but lacks clear, human-readable interpretations for each test as requested (""interpretation of results for each test""). For example, for t-tests, it just prints ""Significant: True/False"" rather than a sentence explaining what that means. The prompt specifically asked for this.

6.  **Alpha Value:** The significance level (`α = 0.05`) is hardcoded in the T-test and Normality test display, but not defined as a constant at the top as good practice dictates and as the ground truth demonstrates.

7.  **Robustness (Shapiro-Wilk):** The Shapiro-Wilk test is unreliable for very large sample sizes (typically N > 5000), as highlighted in the ground truth. The generated code doesn't account for this.

8.  **Output Clarity:** The output format is concise but less informative than desired (e.g., doesn't show calculated means for t-tests).

To improve, the generated code needs robust missing value imputation, correct chi-square setup, more defensive programming for Cohen's d, better output interpretation, and attention to statistical best practices (like `alpha` constant and Shapiro-Wilk limitations).",gpt_4o_mini,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
import matplotlib.pyplot as plt

# Load dataset
data = pd.read_cs...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being a high-quality solution.

**Strengths:**
*   **Data Preprocessing (Partial):** Handles missing values (though simplistically) and performs feature scaling and train-test split.
*   **Multiple Algorithms:** Implements Logistic Regression, Random Forest, and SVM.
*   **Comprehensive Metrics:** Calculates accuracy, precision, recall, F1-score, and ROC-AUC.
*   **ROC Curve Visualization:** Generates a combined ROC curve.
*   **K-Fold Cross-Validation:** Performs k-fold cross-validation.
*   **Model Comparison:** Selects a best model.
*   **Feature Importance (Partial):** Attempts to display feature importance for Random Forest.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling (Incorrect Data Leakage):** The `data.fillna(data.mean(), inplace=True)` is performed on the *entire dataset* before the train-test split. This leads to **data leakage**, as information from the test set's mean is used to impute missing values in the training set. Missing value imputation should only be fitted on the training data and then applied to both training and test sets. The ground truth code handles this correctly using `SimpleImputer` *after* the split and fitting only on the training set.
2.  **Order of Preprocessing:** Feature scaling (`X_scaled = scaler.fit_transform(X)`) is done on the *entire dataset* *before* the train-test split. This also causes **data leakage**. Scaling should be fitted on the training data (`X_train`) and transformed on both `X_train` and `X_test`. The ground truth demonstrates the correct approach.
3.  **Confusion Matrices:** The task explicitly asks for ""Create confusion matrices... for each model"". The generated code calculates the confusion matrix (`results[model_name]['confusion_matrix'] = confusion`) but **does not display them**.
4.  **ROC Curve Display:** While a combined ROC curve is plotted, the task asked for ""Create ... ROC curves for each model"". The ground truth code correctly uses `RocCurveDisplay.from_estimator` for a cleaner display and proper labeling of each curve within the plot.
5.  **Cross-Validation Implementation:**
    *   The `cross_val_score` is applied directly to `X_scaled, y`. Since `X_scaled` was scaled *before* the train-test split (data leakage), the CV results are also potentially biased.
    *   Crucially, `cross_val_score` should ideally be performed within a `sklearn.pipeline.Pipeline` that encapsulates the imputation and scaling steps. This ensures that *each fold* of cross-validation correctly applies these preprocessing steps independently (fit on training fold, transform on test fold), preventing data leakage during CV. The generated code does not use a pipeline, which is a major oversight for robust CV.
    *   Only `accuracy` is implicitly used for CV scores (`cv_scores = cross_val_score(model, X_scaled, y, cv=kf)`). The task asks to ""Evaluate models using accuracy, precision, recall, F1-score, and ROC-AUC metrics"" and apply CV to ""assess model stability"". The ground truth code uses `scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc']` within the pipeline for a comprehensive CV evaluation.
6.  **Comparison of Model Performances:** The comparison logic is very basic, just using `results[x]['roc_auc']`. It prints the `results` dictionary (which contains test set metrics and a single CV score). It doesn't clearly display the comprehensive comparison of *all* metrics as a table (like the ground truth's `pd.DataFrame(results).T`).
7.  **Feature Importance for Non-Tree Models:** The code only handles Random Forest feature importance. It completely misses Logistic Regression's coefficients and correctly identifying that SVMs don't have direct feature importances, as provided in the ground truth. The implementation for Random Forest also lacks proper plot titles/labels.
8.  **Imports:** Some useful imports like `seaborn` for better plotting or `ConfusionMatrixDisplay` for direct display are missing, which makes the plotting part less robust.
9.  **Output Readability:** The final `print(results)` dumps a large dictionary without proper formatting, making it hard to interpret.
10. **Stratified Split:** While `stratify=y` is an option for `train_test_split`, it's not used. Given classification and potential class imbalance, stratification is good practice. The ground truth correctly uses `stratify=y` for `train_test_split` and `StratifiedKFold` for CV, which is important for maintaining class distribution in splits.
11. **Random State:** While some models have `random_state`, not all do, and `KFold` is used instead of `StratifiedKFold`. Consistency in `random_state` helps reproducibility.",gpt_4o_mini,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import PolynomialFeatures

# Load the dataset
data = pd.read_csv('water_potability.csv')

# Feature Engineering
# Create new meaningful features
data['pH_Chloramines'] = data['pH'] / (data['Chloramines'] + 1e-9)  # Adding a small value to avoid divisio...",6,"The generated code addresses most of the task requirements but has several significant deficiencies and areas for improvement.

**Strengths:**
*   **Imports:** Includes necessary libraries.
*   **Basic Structure:** Follows a logical flow of data loading, feature engineering, correlation, feature selection, and visualization.
*   **Missing Value Handling (Implicit):** Loads data, but explicitly handles missing values by computing correlation matrix on the raw data which has NaN. This would lead to NaNs in the correlation matrix and subsequent errors in feature selection if not handled. The ground truth correctly imputes missing data first.
*   **Feature Engineering - Ratios:** Attempts to create ratio features.
*   **Feature Engineering - Polynomial Features:** Attempts to create polynomial features, acknowledging the high dimensionality.
*   **Correlation Analysis:** Calculates and identifies top correlated features.
*   **Feature Selection:** Uses both SelectKBest and RFE as requested.
*   **Visualizations:** Generates a correlation heatmap, feature importance plot, and pairwise scatter plots.
*   **Comparison of Feature Sets:** Compares selected feature sets.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling:** The most critical flaw. The `water_potability.csv` dataset has missing values in 'ph', 'Sulfate', and 'Trihalomethanes'. The generated code does *not* explicitly handle these missing values before proceeding. This will cause `data.corr()` to have NaNs and subsequent `SelectKBest` and `RFE` to fail or produce incorrect results due to `X` containing NaNs. The ground truth correctly imputes these.
2.  **Polynomial Feature Generation (Incorrect/Incomplete):**
    *   `PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)`: While `interaction_only=True` is a good choice to manage dimensionality, the ground truth uses `degree=2` (which implicitly includes interactions) without `interaction_only` and also includes squared terms, which are themselves meaningful. The prompt asked for ""polynomial features"", which typically includes squared terms.
    *   `poly.fit_transform(data.select_dtypes(include=[np.number]))`: This applies polynomial features to *all* numerical columns, including the target 'Potability' and potentially the newly created ratio features *before* they are properly integrated, leading to redundant or nonsensical polynomial terms. The ground truth carefully selects a subset of *original* features for polynomial expansion to avoid excessive dimensionality and redundancy.
    *   `pd.concat([data, poly_features_df], axis=1)`: This will lead to duplicate columns where `poly_features_df` recreates original columns (e.g., 'ph', 'Hardness') as polynomial features. The ground truth carefully checks to add only *new* terms.
    *   No handling of column names for the polynomial features, which can become long and messy, especially if not careful about the input to `PolynomialFeatures`.
3.  **Feature Engineering - Meaningful Ratios and Interaction Terms:**
    *   The generated code creates only two ratio features (`pH_Chloramines`, `Hardness_Solids`). The prompt asked for ""new meaningful features such as ratios between chemical parameters (e.g., pH/Chloramines, Hardness/Solids), interaction terms, and polynomial features"". The ground truth provides a much richer set of ratios and explicitly defined interaction terms. The generated code also includes `Turbidity_Hardness` and `Phosphate_Sulfate` as multiplication, which aligns with 'interaction terms' but doesn't explicitly label them as such.
4.  **Standardization before Feature Selection:** The generated code does not scale features before `SelectKBest` or `RFE`. While `SelectKBest` with `f_classif` is generally robust to scaling, RFE with `RandomForestClassifier` (though less sensitive than distance-based models) typically benefits from scaled data, and it's good practice for consistency, especially when comparing different selection methods. The ground truth correctly applies `StandardScaler`.
5.  **`RFE` Estimator Fitting:** The Random Forest classifier used for RFE `model = RandomForestClassifier()` is initialized *outside* the `RFE` call. Then `fit_rfe = rfe.fit(X, y)` is called. Later, for the feature importance plot, `feature_importances = model.fit(X, y).feature_importances_` is called. This means the `model` object is re-fitted on the entire `X` dataset *again* just for feature importance, and crucially, this `model` might not reflect the RFE-trained model's internal state. It's better to get feature importances from a model trained for that purpose or ensure the model used for importance is the same one used in RFE (which might not be ideal as RFE trains it internally). The ground truth specifically trains a `rf_full` model for feature importance.
6.  **`k` parameter for Feature Selection:** Both `SelectKBest` and `RFE` are set to `k=5`, which means they select only 5 features. The prompt asked for ""optimal features"" and then lists ""top 5 features most correlated"". Selecting only 5 features for *all* selection methods seems arbitrary and might not be optimal, nor does it provide a good comparison with the top 5 *correlated* features without having a larger pool from the selection methods. The ground truth chose `k=15` for the selection methods, which gives more room for comparison.
7.  **Comparison of Feature Sets:** The comparison is very basic, only showing the overlap. The ground truth provides a more comprehensive comparison showing overlapping, unique, and union sets, which is more insightful.
8.  **Visualization Details:**
    *   Heatmap annotation is `annot=True` for all features, which makes it very cluttered and unreadable for a large matrix. The ground truth uses `annot=False`, which is appropriate.
    *   Feature importance plot only shows the top 10. The ground truth plots the top 25, giving a broader view.
    *   Error handling for `water_potability.csv` not found is missing.

To improve the score, the code needs robust missing value handling, more thoughtful and correct polynomial feature generation, a richer set of explicitly defined interaction terms, and standardization of data before feature selection methods. Details in visualizations and comparison of feature sets could also be refined.",gpt_4o_mini,LLM_judge_config
