task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: winequality-red.csv not found in the current directory."")
    exit()

# 1. List column names and data types
print(""Column Names and Data Types:"")
for col in df.columns:
    print(f""{col}: {df[col].dtype}"")

# 2. Calculate descriptive statistics for numerical columns
print(""\nDescriptive Statistics for Numerical Columns:"")
for col in df.select_dtypes(inclu...",9,"The generated code is highly correct and directly addresses most of the requirements. It successfully loads the dataset, lists column names and data types, calculates descriptive statistics for numerical columns, and reports dataset dimensions. The use of `df.select_dtypes(include=['number'])` for numerical columns is good.

**Strong points:**
- Proper handling of `FileNotFoundError`.
- Clear output for each section.
- Correct calculation of all requested descriptive statistics.

**Areas for minor improvement:**
- **Identifying data types (Part 1):** The generated code only prints the pandas dtype (`df[col].dtype`). The ground truth extends this by *identifying* whether the column is ""Numerical"", ""Categorical/Ordinal"", or ""Unknown"" based on `pd.api.types`. While all columns in `winequality-red.csv` are numerical, the task asked to ""identify the data type of each column (numerical, categorical, etc.)"", implying a more explicit classification beyond just printing the raw dtype string.
- **Categorical/Ordinal identification (Part 4):** The generated code identifies columns that are `exclude=['number']`, which is a good start. However, the ground truth provides a more nuanced explanation that even numerical columns like 'quality' can be *treated* as ordinal due to context, which is highly relevant for this specific dataset. The generated code simply lists non-numerical columns, and since all columns are numerical in this dataset, it outputs nothing, which could be confusing without further explanation. The ground truth explicitly states that there are no non-numerical columns and then explains the ""quality"" column.
- **`describe()` method:** The ground truth effectively uses `df[numerical_columns].describe()` which provides all required statistics (mean, median/50th percentile, std, min, max, quartiles) in a concise and commonly used format. The generated code manually calculates each statistic and prints it, which is correct but less idiomatic/concise when `describe()` offers it all.

Overall, it's a very good attempt. The minor points are about adhering more closely to the *spirit* of ""identifying"" types and providing complete context for categorical/ordinal columns, especially for this specific dataset.",gemma-3-12b-it,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""video_games_sales.csv"")

# Display the first 10 rows
print(""First 10 rows:\n"", df.head(10))

# Show the shape of the dataset
print(""\nShape of the dataset:"", df.shape)

# List all column names and their data types
print(""\nColumn names and data types:\n"", df.info())

# Display the top 10 games by Global_Sales
print(""\nTop 10 games by Global_Sales:\n"", df.sort_values(by=""Global_Sales"", ascending=False).head(10))

# Show the uniqu...",7,"The generated code correctly performs most of the requested EDA tasks. It successfully loads the data, displays the head, shape, unique values, summary statistics, and missing values.

However, there are a few areas for improvement:
1.  **Error Handling**: The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. This makes the code less robust.
2.  **`df.info()` output**: `df.info()` prints its output directly and returns `None`. Wrapping `df.info()` in `print()` as done in the generated code (`print(""\nColumn names and data types:\n"", df.info())`) results in an extra `None` being printed, which is not ideal. The ground truth correctly calls `df.info()` directly.
3.  **Clarity of Output**: The output is not as clearly delineated or labeled as in the ground truth, which uses multiple `print` statements and separators (e.g., `=""*80`) to improve readability and distinguish between different sections of the analysis.
4.  **Robustness for Missing Columns**: The ground truth includes checks (e.g., `if 'Global_Sales' in df.columns:`) before attempting operations on specific columns. While the provided dataset guarantees these columns, in a more general scenario, these checks improve robustness.
5.  **Display of Top 10 Games**: While the generated code prints the sorted DataFrame, the ground truth explicitly selects relevant columns (`[['Name', 'Platform', 'Genre', 'Publisher', 'Global_Sales']]`) for clearer display of the top games, which is a good practice.",gemma-3-12b-it,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd

# 1. Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 2. Handle missing values in the Year column
if df['Year'].isnull().any():
    median_year = df['Year'].median()
    df['Year'].fillna(median_year, inplace=True)

# 3. Remove games with missing Publisher information
df.dropna(subset=['Publisher'], inplace=True)

# 4. Create a new column 'Years_Since_Release'
current_year = 2023
df['Years_Since_Release'] = current_year - df['Year']

# 5. Remove duplic...",7,"The generated code addresses most of the requirements correctly, achieving a good level of functionality.

**Strong points:**
- Correctly loads the dataset.
- Handles missing 'Year' values by filling with the median.
- Correctly removes rows with missing 'Publisher'.
- Calculates 'Years_Since_Release' correctly, although it implicitly assumes Year is an integer.
- Correctly removes duplicates based on 'Name' and 'Platform'.
- Filters out low-selling games based on 'Global_Sales'.
- Displays the shape and head of the cleaned dataset.

**Areas for improvement to achieve a higher score:**
- **Robustness in 'Year' column handling:** The `Year` column can contain non-numeric data, which `median()` and subsequent subtraction might fail on or produce incorrect results. The ground truth explicitly uses `pd.to_numeric(errors='coerce')` to handle this, which is crucial for real-world datasets. The generated code implicitly assumes Year is numeric, which is a potential failure point.
- **Robustness in 'Global_Sales' filtering:** Similar to 'Year', 'Global_Sales' might not always be numeric. The ground truth converts it to numeric with `errors='coerce'` and then drops NaNs, making the filtering more robust. The generated code assumes 'Global_Sales' is numeric, which could cause errors if not.
- **Edge case for 'Years_Since_Release':** If a game's release year is in the future (e.g., 2024), `current_year - df['Year']` would result in a negative value for `Years_Since_Release`. While less common for release years, it's a potential edge case. The ground truth handles this by ensuring values are non-negative.
- **Clarity and robustness of `Year` conversion:** After filling NaNs, the `Year` column's data type might not be an integer. The ground truth explicitly converts it to `astype(int)`, which is good practice for year columns.
- **Conditional check for `df['Year'].isnull().any()`:** While not critically wrong, the `if df['Year'].isnull().any():` check is redundant. `fillna` will simply do nothing if there are no NaNs, so the `if` statement adds unnecessary verbosity without significant benefit.
- **Completeness of output:** The request asked for ""Cleaned dataset's shape and first 5 rows"". The ground truth prints ""Cleaned dataset shape:"" and ""First 5 rows of the cleaned dataset:"", which matches the wording slightly better. The generated code printing ""Cleaned dataset head:"" is also acceptable but less precise.",gemma-3-12b-it,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 1. Bar chart: Top 10 gaming platforms by total sales
platform_sales = df.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False)
top_10_platforms = platform_sales.head(10)

plt.figure(figsize=(10, 6))
top_10_platforms.plot(kind='bar')
plt.title('Top 10 Gaming Platforms by Global Sales')
plt.xlabel('Platform')
plt.ylabel('Global Sales (Millions)')
plt.xtick...",7,"The generated code addresses most of the requirements but has some critical omissions and potential issues that prevent it from being a perfect solution.

**Strong Points:**
*   Correctly generates all requested plot types (bar, pie, histogram, scatter, line).
*   Includes proper titles and labels for most charts.
*   Uses `matplotlib.pyplot` and `pandas` as requested.
*   Handles the grouping and aggregation correctly for the bar and line charts.

**Areas for Improvement/Missing Aspects:**
1.  **Data Loading Robustness:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for user-friendly code. The generated code lacks this.
2.  **Data Preprocessing/Cleaning:** The most significant omission is the lack of any data cleaning or preprocessing, especially for 'Year_of_Release' and sales columns. The `video_games_sales.csv` dataset often has 'N/A' or non-numeric values in 'Year_of_Release' and sometimes in sales columns. The ground truth correctly converts these to numeric and handles NaNs. The generated code directly uses these columns, which could lead to errors or incorrect plots if the data isn't clean.
    *   Specifically, the line plot for ""Trend of total games released per year"" assumes a 'Year' column in the DataFrame, but the dataset typically has 'Year_of_Release', which might not be clean. If 'Year_of_Release' has non-numeric values, `groupby('Year')` would fail or be incorrect. The ground truth handles this effectively.
    *   Sales columns (NA\_Sales, EU\_Sales, Global\_Sales) can also have `NaN` or non-numeric entries, and direct plotting without `dropna` or `astype(float)` could cause issues.
3.  **Plot Refinements:**
    *   **Bar Chart:** While functional, the ground truth adds `color`, `grid`, and `tight_layout()` for better aesthetics and readability. `plt.tight_layout()` is present in generated code but more extensive formatting could be added.
    *   **Pie Chart:** The ground truth includes a strategy to handle ""Other"" categories for genres with too few counts, making the pie chart more readable. The generated code plots all genres, which can make the chart very cluttered if there are many minor genres. `plt.axis('equal')` should be added to ensure the pie chart is a circle.
    *   **Histogram:** The ground truth considers limiting the range of the histogram (`range=(0, df['Global_Sales'].quantile(0.99))`) because sales data is often heavily skewed, and most games have very low sales, making a basic histogram hard to interpret. The generated code uses default bins and range, which might not be optimal for this dataset.
    *   **Scatter Plot:** The ground truth adds `alpha` and `s` (marker size) for better visualization of dense scatter plots.
    *   **Line Plot:** The ground truth filters years to avoid outliers or data entry errors, which is a good analytical step. The generated code uses `groupby('Year')` which is an incorrect column name (it should be `Year_of_Release`) and doesn't handle the potential `NaN` values or non-numeric types in that column.

Overall, the generated code fulfills the basic requirements but lacks the robustness, data handling, and fine-tuning seen in an expert-level solution.",gemma-3-12b-it,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd

# Load the dataset
diabetes_data = pd.read_csv('diabetes.csv')

# Display the first 10 rows
print(""First 10 rows of the dataset:"")
print(diabetes_data.head(10))
print(""\n"")

# Show the shape of the dataset
print(""Shape of the dataset (rows, columns):"", diabetes_data.shape)
print(""\n"")

# List column names and data types
print(""Column names and data types:"")
print(diabetes_data.info())
print(""\n"")

# Calculate the percentage of diabetic vs non-diabetic patients
tota...",7,"The generated code correctly addresses most of the requirements but has a few areas for improvement in terms of robustness, detail, and adherence to common best practices for EDA.

**Strong Points:**
*   Correctly loads the CSV and displays the first 10 rows.
*   Correctly shows the dataset shape.
*   Uses `df.info()` for column names and types, which is excellent.
*   Calculates and displays missing values correctly.
*   Shows summary statistics correctly.

**Areas for Improvement:**
*   **Percentage of Diabetic vs Non-diabetic Patients:** While the calculation itself is correct, using `value_counts(normalize=True)` (as in the ground truth) is more concise, robust, and idiomatic for this task. The current implementation is a bit more verbose. It also prints ""Percentage of diabetic patients"" and ""Percentage of non-diabetic patients"" separately instead of a combined view, which is less informative.
*   **Identifying Zero Values:**
    *   The generated code iterates through *all* columns and checks for zeros, which is overly broad. For datasets like `diabetes.csv`, some columns (e.g., `Pregnancies`, `Outcome`) can legitimately have zero values. The ground truth explicitly lists `COLUMNS_TO_CHECK_FOR_ZERO_AS_MISSING` to target only those columns where zero is likely a placeholder for missing data. This distinction is crucial for effective EDA.
    *   It also checks `df[col].dtype != 'object'`, which is a good filter, but still doesn't narrow it down to the specific columns of interest.
    *   It just prints the count, not the percentage, which is less informative than the ground truth's output.
*   **Error Handling:** The ground truth includes basic `try-except` for file loading, which adds robustness. This is missing in the generated code.
*   **Output Formatting:** The ground truth uses separators (`---`, `===`) to clearly delineate each section, making the output much more readable. The generated code has simpler `print(""\n"")` separators. While not a functional error, better formatting aids in understanding the output.
*   **Redundant Import:** `numpy` is imported in the ground truth, but not used. While not a big deal, removing unused imports is good practice. (The generated code *doesn't* import `numpy`, so it doesn't have this issue).

Overall, it's a good attempt that covers the core requirements, but could be refined for better detail, specificity, and robustness, especially regarding the interpretation of zero values and outcome percentages.",gemma-3-12b-it,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# 1. Load the dataset
data = pd.read_csv('diabetes.csv')

# 2 & 3. Identify and replace zero values with NaN in specified columns
cols_to_check = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
for col in cols_to_check:
    data[col] = data[col].replace(0, np.nan)

# 4. Handle missing values by filling with the median
for col in cols_to_check:
    data[col] = data[col].fillna(data[c...",7,"The generated code correctly addresses most of the requirements. It loads the dataset, replaces zeros with NaNs, fills missing values with the median, removes duplicates, creates the BMI_Category column, and scales numerical features. However, there are a few areas for improvement.

The code assumes the CSV file exists; it would be more robust to include error handling for `FileNotFoundError` as seen in the ground truth.

For scaling numerical features, the generated code hardcodes `numerical_features`. This assumes these are the only numerical features to scale and explicitly excludes the 'Outcome' column without checking if it's the target and should be excluded. A more dynamic approach, like the ground truth's `select_dtypes(include=np.number).columns.tolist()` and then removing 'Outcome' if present, would be more adaptable and less prone to errors if the dataset structure changes.

The BMI categorization logic is correct but could be more concise using `pd.cut` as demonstrated in the ground truth, which is generally preferred for binning numerical data.

Finally, the code doesn't explicitly print the original shape vs. cleaned shape or mention if duplicates were found, which were small but helpful additions in the ground truth for understanding the cleaning process.",gemma-3-12b-it,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the dataset
try:
    df = pd.read_csv('diabetes.csv')
except FileNotFoundError:
    print(""Error: diabetes.csv not found in the current directory."")
    exit()

# 2. Correlation Heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Numerical Variables')
plt.show()

# 3. Histograms for all numerical features
df.hist(figsize=(12, 10))
pl...",7,"The generated code largely addresses the requirements but has several areas for improvement in terms of detail and formatting.

**Strengths:**
*   **Correctness:** All required visualization types are generated.
*   **Completeness:** All 7 tasks are attempted.
*   **Imports:** Proper libraries (pandas, matplotlib.pyplot, seaborn) are imported.
*   **Error Handling:** Includes basic `FileNotFoundError` handling for the CSV.

**Areas for Improvement:**

1.  **Correlation Heatmap:** The generated code correctly creates a heatmap. However, the ground truth explicitly uses `df[numerical_cols]` to ensure only numerical variables are correlated (though in this specific dataset, `df.corr()` would effectively do the same as 'Outcome' is numerical).
2.  **Histograms:** The generated code uses `df.hist()`, which is a quick way to get histograms but offers less flexibility in customization (e.g., KDE plots, specific colors, x-axis labels, better titles for individual plots) compared to iterating and using `sns.histplot` as in the ground truth. The current `df.hist()` generates plots that are a bit small and squished without careful layout.
3.  **Box Plots:**
    *   **Looping:** The generated code uses `df.columns[:-1]` to iterate, which assumes the last column is 'Outcome'. While true for this dataset, it's less robust than explicitly defining numerical columns as in the ground truth.
    *   **Subplot Layout:** The `subplot(6, 2, ...)` assumes a fixed number of features and might not dynamically adjust well if features are added or removed. The ground truth calculates `n_rows` and `n_cols` dynamically.
    *   **Labels:** The x-axis label for Outcome in the box plots (`0: Non-Diabetic, 1: Diabetic`) is missing in the generated code, which is a good detail for clarity.
4.  **Pie Chart:** Correctly generated. The ground truth adds `explode` and `shadow` for slight aesthetic improvement, and clearer labels (`Non-Diabetic (0)`, `Diabetic (1)`).
5.  **Scatter Plots:**
    *   **Order of Variables:** The generated code plots 'Glucose' on the x-axis and 'BMI' on the y-axis for the first scatter plot. The prompt and ground truth specify ""BMI vs Glucose"" (meaning BMI on x, Glucose on y), which is a minor but noticeable difference.
    *   **Labels/Titles:** The titles are concise but could be more descriptive (e.g., ""BMI vs Glucose, Colored by Diabetes Outcome"").
    *   **Legends:** The legends are basic (`0`, `1`). The ground truth provides more descriptive labels (`Non-Diabetic`, `Diabetic`) within the legend.
6.  **Pair Plot:**
    *   **Selection of Features:** The generated code correctly selects the specified columns `df[['Glucose', 'BMI', 'Age', 'Outcome']]`.
    *   **Customization:** The ground truth includes `hue='Outcome'` and `diag_kind='kde'` as parameters to `sns.pairplot`, which are crucial for visualizing relationships *by outcome* and showing kernel density estimates on the diagonal, making the pair plot much more informative. The generated code's pair plot is more basic, lacking the `hue` and `diag_kind` parameters.
    *   **`plt.figure(figsize=(12, 10))` before `sns.pairplot`:** This line actually has no effect on `sns.pairplot`'s figure size, as `pairplot` creates its own figure. Setting `height` and `aspect` within `pairplot` is the correct way to control its size.

Overall, the generated code produces the requested plots but often lacks the polish, robustness, and detailed customization that make the ground truth code more effective for analysis and presentation. The use of `df.hist()` and the basic `sns.pairplot` are the primary weaknesses regarding visual quality and information density.",gemma-3-12b-it,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""world-happiness-report-2021.csv"")

# Display the first 10 rows
print(""First 10 rows of the dataset:\n"", df.head(10))

# Show the shape of the dataset
print(""\nShape of the dataset (rows, columns):"", df.shape)

# List all column names and their data types
print(""\nColumn names and data types:\n"", df.dtypes)

# Display unique regional indicators and count of countries per region
print(""\nRegional indicators and country counts:\n"",...",6,"The generated code addresses most of the requirements but has several issues that prevent it from scoring higher.

**Strengths:**
- Correctly loads the dataset.
- Displays the first 10 rows correctly.
- Shows the shape of the dataset correctly.
- Correctly shows the top 10 happiest countries.
- Correctly calculates summary statistics (assuming correct column names).
- Correctly identifies missing values.

**Weaknesses and Areas for Improvement:**

1.  **Missing Error Handling (Critical):** The generated code lacks any `try-except` block for `FileNotFoundError` when loading the CSV. This is a critical omission, as the entire script will crash if the file is not present. The ground truth includes robust error handling.
2.  **Incorrect Column Name for Regional Indicator (Critical):** The task specified ""Regional indicator"". The generated code uses `df.groupby(""Region"").size()`. Based on the typical naming conventions for these reports, ""Regional indicator"" is the correct column name. This means the regional analysis will fail.
3.  **Inaccurate Column Data Type Display:** `df.dtypes` only shows the data types. `df.info()` (used in the ground truth) is superior as it also provides non-null counts and memory usage, which is more comprehensive for understanding data types and completeness.
4.  **Assumptions about Column Existence:** The generated code directly accesses columns like ""Region"", ""Ladder score"", ""GDP per capita"", etc., without checking if they exist. While these columns are highly likely to be present in this specific dataset, robust code (like the ground truth) would include checks before accessing them to prevent `KeyError` if the dataset structure changes or is unexpected.
5.  **Output Clarity/Formatting:** The ground truth adds separators and more descriptive print statements for better readability of the output, making it easier to distinguish between different task results.

Overall, it's a solid attempt that performs the core data manipulations but falls short on robustness, accuracy of specific column names, and output presentation compared to the meticulousness of the ground truth.",gemma-3-12b-it,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, LabelEncoder
import numpy as np

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Handle missing values
numerical_cols = ['GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom', 'Generosity', 'Corruption perception']
categorical_cols = ['Regional indicator']

numerical_imputer = SimpleImputer(strategy='mean')
categorical_imputer = Sim...",7,"The generated code addresses most of the requirements but has several issues that prevent it from being a perfect solution.

Positive aspects:
*   Correctly loads the dataset.
*   Handles missing numerical and categorical values using appropriate strategies (mean/most frequent).
*   Correctly calculates 'Well_being_Score' as an average of the three specified features.
*   Correctly implements IQR method for outlier removal on 'Ladder score'.
*   Correctly standardizes numerical features using `StandardScaler`.
*   Correctly encodes 'Regional indicator' using `LabelEncoder`.
*   Displays the shape and head of the cleaned dataset.

Areas for improvement/Corrections:
1.  **Missing Value Handling (Dynamic Column Selection):** The generated code hardcodes `numerical_cols` and `categorical_cols` for missing value imputation. This is less robust than the ground truth which dynamically selects these columns using `select_dtypes(include=np.number)` and `select_dtypes(include='object')`. If the dataset schema changes, the hardcoded list would break. Also, `GDP per capita` does not exist in the dataset; it should be `Logged GDP per capita`. This is a critical error in column selection for imputation.
2.  **`Happiness_Category` Definition:** The binning for `Happiness_Category` is slightly different. The ground truth specifies `(High: >6.5, Medium: 4-6.5, Low: <4)`. The generated code uses `pd.cut` with `bins=[0, 4, 6.5, float('inf')]` and `labels=['Low', 'Medium', 'High']`. This makes `0-4` 'Low', `4-6.5` 'Medium', and `6.5-inf` 'High'. The lower bound `0` for 'Low' might be acceptable, but the ground truth's `np.select` provides more precise control over the exact boundaries, especially if `6.5` should be exclusive for 'High' and inclusive for 'Medium'. The chosen `pd.cut` setup makes `4` and `6.5` exclusive for the lower bin, which is slightly off the ground truth's inclusive `4-6.5` range.
3.  **`GDP_Category` Definition:** The task specifies 'GDP_Category' based on quartiles. The generated code uses `pd.qcut` but sets `labels=False`, which returns integer codes (0, 1, 2, 3) instead of descriptive labels like 'Q1', 'Q2', 'Q3', 'Q4' (as in the ground truth). While functionally similar for some downstream tasks, using descriptive labels is generally preferred for clarity and aligns better with the spirit of creating a ""category"".
4.  **Column Name Mismatch:** The generated code uses `'GDP per capita'` in `numerical_cols` for imputation and `numerical_cols_to_scale`. The actual column name in the dataset is `'Logged GDP per capita'`. This will cause `KeyError` or operate on empty columns if the dataset structure was slightly different and didn't gracefully handle non-existent columns. This is a significant bug.
5.  **Lack of Robustness (File Loading):** The generated code does not include error handling for `FileNotFoundError` when loading the CSV, unlike the ground truth. This is good practice for real-world scenarios.
6.  **`Well_being_Score` Variable Naming:** The prompt asks for `Well_being_Score` as an average. The code calculates it correctly but does not *explicitly* include it in the `numerical_cols_to_scale` list, although `Scaler` will typically pick it up if not specified. However, for clarity and explicit compliance, it's better to include it in the scaling list if it's new and numerical. (Upon review, it is implicitly scaled as numerical_cols_to_scale does not differentiate.)
7.  **Minor missing error in missing value imputation:** The generated code initializes `numerical_cols` and `categorical_cols` prior to imputing. While it correctly identifies `Regional indicator` as categorical, it only includes a partial list of numerical columns. For example `Ladder score` and `Well-being score` (which is derived, but depends on other numerical columns) are not explicitly mentioned for imputation if *they* had missing values. The ground truth's dynamic selection is superior here due to `select_dtypes`.

The significant issues are the hardcoded, incorrect column names (`GDP per capita` vs `Logged GDP per capita`) for imputation and scaling, and the somewhat limited selection of columns for imputation compared to dynamic selection. These flaws impact correctness and robustness.",gemma-3-12b-it,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import plotly.express as px
import folium
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv(""world-happiness-report-2021.csv"")

# Data cleaning – drop rows with missing values
df = df.dropna()

# 1. World Map Visualization
fig_map = px.choropleth(df, locations=""Country name"",
                         color=""Ladder score"",
                         hover_name=""Country name"",
                         color_continuous_scale=px.co...",7,"The generated code addresses most of the requirements and produces various visualizations. However, there are several areas for improvement:

1.  **Correctness (Minor issues):**
    *   **Column Renaming:** The generated code does not rename columns for better readability as specified in the ground truth and as good practice. This impacts readability and consistency, especially in the radar chart where a column like ""D. Freedom to make life choices"" is used, which is not present after loading. The ground truth handles this via `df.rename()`.
    *   **Radar Chart Features:** The features used for the radar chart in the generated code (`""D. Freedom to make life choices""`) is an incorrect column name. It should be `""Freedom to make life choices""`. This leads to a `KeyError`. Even if it were correct, the ground truth uses a more comprehensive set of factors and normalizes them correctly for a meaningful radar chart.
    *   **Radar Chart Implementation:** The radar chart implementation in the generated code creates 5 separate small radar charts in a subplot, which is less effective for comparison than the single overlaid radar chart in the ground truth. Also, the normalization `(data - data.min()) / (data.max() - data.min())` performed only on the top 5 countries' data is not ideal; it should be normalized against the min/max of the *entire dataset* for a more accurate representation of how these countries compare globally on each factor, which the ground truth does.

2.  **Code quality:**
    *   **Error Handling:** Lacks file not found error handling for `pd.read_csv`.
    *   **Hardcoded Column Names:** The code directly uses original column names like ""Ladder score"", ""Logged GDP per capita"", ""Social support"", etc., which makes the code less readable compared to using renamed columns.
    *   **Redundant `df.dropna()`:** The instruction doesn't mention handling missing values, and the dataset typically doesn't have many NaNs that would break these visualizations. Dropping them without context might inadvertently remove useful data. The ground truth doesn't explicitly drop NaNs, relying on the robustness of the plotting functions.
    *   **`folium` import:** `folium` is imported but not used. The task specified `plotly or folium`, and `plotly` was used, so `folium` is unnecessary here.

3.  **Completeness:**
    *   **Titles and Labels:** While present for most plots, some `plt.show()` calls are directly after the plot generation without `plt.tight_layout()` or additional formatting that would improve presentation.
    *   **Matplotlib defaults:** The ground truth sets `sns.set_style(""whitegrid"")`, `plt.rcParams['figure.dpi'] = 100`, and `plt.rcParams['figure.figsize'] = (12, 8)` for better aesthetics and consistent sizing. The generated code lacks these, using default sizes which can vary.
    *   **Legends for Scatter Plots:** The scatter plots in the generated code do not include legends to distinguish regions, which the ground truth correctly adds using `hue='Regional indicator'`.

Overall, it's a good attempt that covers most plot types, but the quality of implementation, especially for the radar chart, and attention to detail in formatting and data preparation (like column renaming and robust scaling) could be improved.",gemma-3-12b-it,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Parse the 'date' column as datetime objects
df['date'] = pd.to_datetime(df['date'])

# Display the first 10 rows
print(""First 10 rows:\n"", df.head(10))

# Show the shape of the dataset
print(""\nShape of the dataset:"", df.shape)

# List all column names and their data types
print(""\nColumn names and data types:\n"", df.info())

# Calculate the total sum of 'total_cases' and 'total_deaths' grouped by...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being fully correct and robust.

**Correctness Issues:**

1.  **Date Parsing:** The `pd.read_csv('synthetic_covid19_data.csv')` then `df['date'] = pd.to_datetime(df['date'])` is suboptimal. The `parse_dates=['date']` argument directly in `read_csv` is more efficient and idiomatic. While the generated code's approach works, it's not the best practice.
2.  **`df.info()` output:** `print(""\nColumn names and data types:\n"", df.info())` is incorrect. `df.info()` itself prints to the console and returns `None`. Printing `None` will be confusing. It should just be `df.info()`.
3.  **Top 5 locations logic (Critical Flaw):**
    *   `latest_data = df.sort_values(by='date', ascending=False).groupby('location').first()`: This logic is fundamentally flawed for finding the latest data for each location. `groupby().first()` will take the *first* row in each group after sorting the entire DataFrame by `date` *overall*. If the data for a location is not contiguous, or if sorting by `date` globally rearranges locations, `first()` will not consistently pick the *latest* date *for that specific location*. The correct approach is to sort by `location` then `date`, then `drop_duplicates(subset=['location'], keep='last')` as shown in the ground truth.
    *   The generated code also tries to print `top_5_locations[['location', 'total_cases']]`. However, after `groupby('location')`, 'location' becomes the index, so `top_5_locations['location']` will throw an error or result in an empty series if the index is used as a column. The ground truth explicitly selects `location` from the non-grouped DataFrame before sorting and dropping duplicates, or resets the index.
4.  **Error Handling:** The generated code lacks any robust error handling for `FileNotFoundError` or other potential issues during CSV loading, which the ground truth includes.
5.  **`describe()` for specific stats**: While `describe()` provides mean, min, max, and std, it also provides count, 25%, 50%, and 75% percentiles. The prompt specifically asked for ""mean, median, min, max, std"". While `describe()` includes these, a more targeted approach like `df[['new_cases', 'new_deaths']].agg(['mean', 'median', 'min', 'max', 'std'])` would be more precise to the prompt's request. However, `describe()` is generally acceptable for ""basic summary statistics.""

**Code Quality/Completeness:**

*   **Readability:** The print statements are a bit less descriptive compared to the ground truth (e.g., ""First 10 rows:"" vs. ""--- First 10 rows of the dataset ---"").
*   **Robustness:** Lacks checks for column existence before operations (e.g., `continent` in groupby, or `total_cases` in latest data calculation). If a column is missing, it will raise a `KeyError`. The ground truth includes these checks, making it more robust.
*   **Comments:** Sparse comments compared to the ground truth.

Despite these issues, the code correctly loads the data, identifies some basic stats, and counts missing values. The critical flaw in the ""top 5 locations"" logic significantly impacts its correctness for that specific task, and the lack of robust error handling is a notable deficiency for a ""production-ready"" piece of code.",gemma-3-12b-it,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: winequality-red.csv not found in the current directory."")
    exit()

# 1. Calculate and report the average 'quality'
average_quality = df['quality'].mean()
print(f""Average quality: {average_quality}"")

# 2. Group by 'quality' and calculate average 'alcohol'
quality_alcohol = df.groupby('quality')['alc...",8,"The generated code correctly addresses all three main requirements: calculating the average quality, grouping by quality to find average alcohol, and creating a bar chart. It uses appropriate libraries (pandas, matplotlib, seaborn) and includes basic error handling for file not found.

Areas for improvement to achieve a higher score (e.g., 9 or 10):
- **Reporting of Grouped Data:** The ground truth explicitly prints the `average_alcohol_by_quality` DataFrame, which is useful for verifying the grouped calculation. The generated code calculates `quality_alcohol` but doesn't print it.
- **Chart Labeling and Aesthetics:** While the generated chart is functional, the ground truth includes more detailed formatting:
    - Clearer print statements (""1. Average 'quality' of red wines: ..."", ""2. Average 'alcohol' content..."").
    - More professional plot title and axis labels (e.g., ""Average Alcohol Content by Wine Quality"" vs. ""Average Alcohol Content vs. Quality"", ""Quality Score"" vs. ""Quality"", ""Average Alcohol Content (%)"" vs. ""Average Alcohol Content"").
    - Setting a `sns.set_style(""whitegrid"")` for better aesthetics.
    - Adding data labels on top of the bars for precise values, which is a significant addition for readability and insight.
    - Using a `palette` in `sns.barplot` for better visual appeal.

Overall, it's a solid attempt that fulfills the core requirements, but could benefit from more robust output reporting and enhanced visualization details for better user experience and clarity.",gemma-3-12b-it,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd

def clean_and_preprocess_covid_data(file_path='synthetic_covid19_data.csv'):
    """"""
    Cleans and preprocesses the synthetic COVID-19 data.

    Args:
        file_path (str): Path to the CSV file.

    Returns:
        pandas.DataFrame: The cleaned and preprocessed DataFrame.
    """"""

    df = pd.read_csv(file_path)
    df['date'] = pd.to_datetime(df['date'])

    # Handle missing values
    df['hosp_patients'].fillna(0, inplace=True)
    df['icu_patients'].fill...",7,"The generated code correctly addresses most of the task requirements and is well-structured within a function, which improves reusability. It loads the data, handles missing values for `hosp_patients` and `icu_patients`, fills mean for `positive_rate` and `reproduction_rate`, calculates `mortality_rate` and `vaccination_rate_per_capita`, and filters for the year 2021.

However, there are a few areas for improvement:
1.  **Handling `NaN` in `mortality_rate` and `vaccination_rate_per_capita`:** The `replace` method with `float('nan')` only works if the NaN values are exactly `np.nan`. Post-division by zero (0/0), pandas usually produces `np.nan`, but `float('nan')` is not equivalent to `np.nan` for `replace`. `fillna(0)` is the more robust way to handle `NaN` values that arise from division by zero, as demonstrated in the ground truth. The current `replace` also misses `-np.inf`.
2.  **`total_deaths`/`total_cases` and `people_fully_vaccinated`/`population` type coercion:** The ground truth explicitly converts these columns to numeric using `pd.to_numeric(..., errors='coerce')` before calculation. While pandas often handles basic numeric operations, if these columns contain non-numeric strings, it would lead to errors or unexpected `NaN`s which then might not be handled correctly by the `replace` logic.
3.  **Import:** The `import numpy as np` is missing, which isn't strictly needed for the `float('inf')` but is often a good practice when dealing with numerical operations and potentially `np.nan` (though `float('nan')` was used here, less robustly).
4.  **Error Handling for File Not Found:** The generated code does not include any error handling for `FileNotFoundError` as present in the ground truth, which is a good practice for robustness.
5.  **Clarity for Display:** The print statements for shape and head don't specify ""records from 2021"" as the ground truth does, which adds a bit more context.

Overall, it's a solid attempt that performs the core task well, but can be made more robust and explicit in its type handling and NaN replacement.",gemma-3-12b-it,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Parse the 'date' column
df['date'] = pd.to_datetime(df['date'])

# Time series line plot of 'new_cases' for 'India'
india_data = df[df['location'] == 'India']
plt.figure(figsize=(12, 6))
sns.lineplot(x='date', y='new_cases', data=india_data)
plt.title('New Cases in India Over Time')
plt.xlabel('Date')
plt.ylabel('New Cases')
plt.xticks(rotation...",7,"The generated code correctly addresses most of the plotting requirements and provides a functional solution.

**Strong points:**
- All required plot types (time series, bar, scatter, histogram, box plot) are generated.
- Proper libraries (pandas, matplotlib, seaborn) are imported.
- `date` column is correctly parsed.
- Titles and labels are present for all charts.
- `tight_layout()` is used for better plot presentation.

**Areas for improvement:**
- **Incomplete handling of `total_deaths` for bar chart:** The requirement was to show the sum of 'total_deaths' for each continent, which implicitly means the *latest* total deaths for each country summed up by continent. The generated code simply sums `total_deaths` across all entries for each continent, which is likely to be an overcount if a country appears multiple times. The ground truth correctly addresses this by getting the latest `total_deaths` per location first and then summing by continent.
- **Improved scatter plot data selection:** The scatter plot correctly attempts to use the latest entry per location, but it doesn't handle `NaN` values for `gdp_per_capita` or `total_vaccinations` which can lead to empty plots or warnings if such data exists. The ground truth explicitly drops NaNs for these columns before plotting.
- **Missing error handling:** The ground truth includes `try-except` blocks for file loading, which is good practice. The generated code assumes the file will always be present.
- **Plotting aesthetics:** While basic labels are present, the ground truth includes more enhanced aesthetics like `sns.set_theme`, specific colors, grid lines, and `xscale/yscale('log')` for the scatter plot, which are often beneficial for data science visualizations. The generated code is functional but less refined visually.
- **Detailed plot requirements:** The scatter plot for `gdp_per_capita` vs `total_vaccinations` was requested to use the latest entry for *each location*. While `groupby('location').tail(1)` partially achieves this, it's better to explicitly sort and drop duplicates as in the ground truth for robustness, especially if there are multiple entries for the same date for a location. The ground truth also explicitly adds `hue='continent'` and `size='population'` for more informative scatter plots based on common practices, which wasn't explicitly asked but is a common enhancement.
- **Histogram bins:** The number of bins for the histogram is hardcoded to 20; auto-determination or more consideration of the data distribution might be beneficial, though 20 is a reasonable default. The ground truth uses 30 bins and also includes `kde=True` which is a nice addition.
- **Data preprocessing for plots:** For the box plot and histogram, the ground truth explicitly drops `NaN` values from the respective columns, preventing potential plotting issues or warnings. The generated code plots directly, which might pass, but it's safer to handle NaNs.",gemma-3-12b-it,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

try:
    df = pd.read_csv(""clash_royale_cards.csv"")
except FileNotFoundError:
    print(""Error: clash_royale_cards.csv not found. Make sure the file is in the current directory."")
    exit()

# 1. List column names and data types
print(""Column Names and Data Types:"")
for col, dtype in df.dtypes.items():
    print(f""{col}: {dtype}"")

# 2. Calculate descriptive statistics for numerical columns
numerical_cols = ['elixirCost', 'Win Rate', 'Usage']

for col in numerical...",8,"The generated code successfully completes almost all aspects of the task. It correctly loads the dataset, lists column names and data types, calculates descriptive statistics for the specified numerical columns, and reports the total number of rows.

**Strong Points:**
*   **Correctness:** All required calculations (mean, median, std, min, max, total rows) are accurate.
*   **Completeness:** Addresses all specified requirements from the prompt.
*   **Error Handling:** Includes basic `FileNotFoundError` handling for the CSV.
*   **Readability:** The code is well-structured and easy to understand.

**Areas for Improvement (minor, to reach a perfect score):**
*   **Descriptive Statistics Output Formatting:** While the generated code correctly calculates and prints statistics for each column individually, the ground truth uses `df[existing_numerical_cols].agg(['mean', 'median', 'std', 'min', 'max'])` which provides a more condensed and tabular output, making it easier to compare statistics across different columns. The ground truth's `print(descriptive_stats)` is superior in presentation.
*   **Column Listing Format:** The ground truth explicitly iterates and prints column names first, then provides `df.dtypes`. The generated code combines them, which is acceptable but the ground truth's separation is slightly clearer for listing all names then showing types.
*   **Output Clarity:** The ground truth adds more descriptive print statements (e.g., ""--- Column Names and Data Types ---"", ""--- Total Number of Cards ---"") which improves the overall readability and user experience of the output. The generated code's output is functional but less verbose.
*   **Robustness of `exit()`:** The ground truth also includes a general `except Exception as e:` block for broader error handling during file loading, and uses `exit()` more consistently after an error. The generated code could benefit from a similar general exception block.",gemma-3-12b-it,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv(""clash_royale_cards.csv"")
except FileNotFoundError:
    print(""Error: clash_royale_cards.csv not found in the current directory."")
    exit()

# Filter by elixirCost
filtered_df = df[df['elixirCost'] <= 3]

# Select and display Card and Win Rate columns
displayed_df = filtered_df[['Card', 'Win Rate']]

# Sort by Win Rate in descending order
sorted_df = displayed_df.sort_values(by='Win Rate', ascending=False)

# Print the...",9,"The generated code correctly addresses all the requirements of the task: it loads the CSV, filters by 'elixirCost' less than or equal to 3, selects 'Card' and 'Win Rate' columns, and sorts the results by 'Win Rate' in descending order. The error handling for `FileNotFoundError` is also present, which is good practice.

Areas for minor improvement (to reach a perfect 10):
1.  **Configuration:** The ground truth uses constants for file names and column names. While not strictly necessary for this small script, it's a good practice for maintainability and readability in larger projects, as it centralizes configuration.
2.  **Error Handling (General):** The ground truth includes a general `except Exception as e:` block for broader error handling during file loading. While `FileNotFoundError` is the most common, other issues (e.g., permission errors, corrupted CSV) could occur.
3.  **Column Existence Check:** The ground truth includes a check to ensure that the required columns (`Card`, `Win Rate`) actually exist in the DataFrame before attempting to select them. This adds robustness to the code, preventing `KeyError` if the CSV structure changes.

Overall, it's a very solid and correct solution for the given task.",gemma-3-12b-it,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
try:
    df = pd.read_csv(""clash_royale_cards.csv"")
except FileNotFoundError:
    print(""Error: clash_royale_cards.csv not found in the current directory."")
    exit()

# Generate histogram of elixirCost
plt.figure(figsize=(10, 6))
plt.hist(df['elixirCost'], bins=20, color='skyblue', edgecolor='black')
plt.title('Distribution of Elixir Cost')
plt.xlabel('Elixir Cost')
plt.ylabel('Frequency')
plt.grid(axis='y', alph...",7,"The generated code correctly loads the dataset and generates a histogram for the `elixirCost` column with a title and labeled axes. This fulfills the core requirements of the task. However, it does not use `seaborn` as suggested in the ground truth code and its chosen `bins` for the histogram are not optimal for the discrete nature of elixir cost, which typically ranges from 1 to 10. The `bins=20` might create bins for non-existent fractional elixir costs, making the visualization less intuitive for discrete data. The ground truth code uses `bins=range(1, 12)` which is a better approach for discrete values like elixir cost. The error handling is basic compared to the ground truth which accounts for missing columns and other general exceptions.",gemma-3-12b-it,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# Load the iris dataset
iris_data = pd.read_csv('iris.csv')

# 1. List column names and data types
print(""Column Names and Data Types:"")
for col in iris_data.columns:
    print(f""{col}: {iris_data[col].dtype}"")

# 2. Calculate descriptive statistics for numerical columns
numerical_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
print(""\nDescriptive Statistics for Numerical Columns:"")
for col in numerical_cols:
    print(f""\n{col}:"")
    pr...",7,"The generated code correctly performs all the required operations: loading the CSV, listing column names and data types, calculating descriptive statistics for the specified numerical columns, and reporting unique species counts.

**Strengths:**
*   Correctly uses `pandas` to load the dataset.
*   Accurately extracts and prints column names and data types.
*   Correctly calculates and prints mean, median, standard deviation, min, and max for each specified numerical column.
*   Successfully uses `value_counts()` to report unique species and their counts.

**Areas for Improvement (Why not higher):**
*   **Robustness (Loading):** The code lacks error handling for `FileNotFoundError` or other exceptions during file loading. This is a crucial robustness aspect in real-world applications. The `try-except` block in the ground truth is a good example.
*   **Output Formatting (Descriptive Stats):** While correct, the output for descriptive statistics is printed column by column in a loop. Using `df[numerical_cols].describe()` or `df[numerical_cols].agg(...)` would provide a more concise, readable, and structured output (e.g., a table) for these statistics, which is generally preferred for data exploration. The ground truth demonstrates this better by using `agg()`.
*   **Column Existence Checks:** There are no checks to ensure that the specified numerical columns (`SepalLengthCm`, etc.) or the 'Species' column actually exist in the DataFrame. If a column name were misspelled or missing in the dataset, the code would raise a `KeyError`. The ground truth includes checks for this.
*   **Readability/Conciseness (Data Types):** While the loop for printing data types works, `print(iris_data.dtypes)` or `print(iris_data.info())` provides a more direct and often preferred way to get this information from a DataFrame.",gemma-3-12b-it,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

# Load the iris dataset
iris_data = pd.read_csv('iris.csv')

# Filter the dataset for Iris-setosa
setosa_data = iris_data[iris_data['Species'] == 'Iris-setosa']

# Select and display SepalLengthCm and SepalWidthCm
setosa_sepal = setosa_data[['SepalLengthCm', 'SepalWidthCm']]
print(setosa_sepal)
```",8,"The generated code correctly performs all the required steps: loading the CSV, filtering by 'Iris-setosa' species, and selecting/displaying the specified sepal columns. The imports are correct, and the code structure is clear and readable.

However, it lacks robust error handling. The ground truth code includes `try-except` blocks to catch `FileNotFoundError`, `KeyError` (for missing columns), and general `Exception`s. While the task brief did not explicitly ask for error handling, for a real-world scenario or a robust solution as implied by an ""expert code evaluator"" benchmark, including such handling significantly improves code stability and usability. The generated code would crash if 'iris.csv' is not found or if the column names were slightly different, which is a potential point of failure.",gemma-3-12b-it,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the iris dataset
iris = pd.read_csv('iris.csv')

# Create the scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(iris['SepalLengthCm'], iris['SepalWidthCm'], c=iris['Species'].map({'setosa': 0, 'versicolor': 1, 'virginica': 2}))

# Add title and labels
plt.title('Scatter Plot of Sepal Length vs. Sepal Width')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')

# Add a legend
plt.legend(handles=[plt.plot([],[], ma...",6,"The generated code correctly loads the dataset and creates a scatter plot with a title and labels. It also attempts to color the points and create a legend based on species. However, the coloring by species using `.map()` with hardcoded integer values is problematic. Matplotlib's `scatter` function with `c=iris['Species'].map(...)` will interpret the numbers (0, 1, 2) as values for a colormap, not distinct categories with distinct colors, leading to a gradient rather than clear categorical coloring. Additionally, the legend creation is overly complex and incorrect for how `scatter` manages colors, requiring manual handle creation which won't reflect the actual colors used by the plot in this setup.

To improve, the code should iterate through unique species and plot each 'subset' with a different color, similar to the ground truth, allowing for a more natural and direct legend generation. This would properly address the requirement of coloring points according to the `Species` column.",gemma-3-12b-it,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv(""heart.csv"")
except FileNotFoundError:
    print(""Error: heart.csv not found in the current directory."")
    exit()

# 1. List column names and data types
print(""Column Names and Data Types:"")
for col in df.columns:
    print(f""{col}: {df[col].dtype}"")

# 2. Calculate descriptive statistics for specified numerical columns
numerical_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']
print(""\nDescriptive Statistics for Nu...",8,"The generated code correctly loads the CSV, lists column names and data types, calculates descriptive statistics for the specified numerical columns, and reports value counts for the `HeartDisease` column. It effectively addresses all the core requirements of the task.

**Strong points:**
- Correctly uses pandas for data loading and manipulation.
- Accurately identifies and displays column names and their data types.
- Correctly calculates mean, median, standard deviation, min, and max for the numerical columns.
- Correctly uses `value_counts()` for the `HeartDisease` column.
- Includes basic error handling for `FileNotFoundError`.

**Areas for improvement (to achieve a higher score):**
- **Output formatting for descriptive statistics:** While individual statistics are printed, the ground truth uses `df[columns].describe()` which provides a more consolidated and readable table. The generated code's output for statistics is verbose and less scannable.
- **Robustness for descriptive statistics:** The generated code assumes all numerical columns ('Age', 'RestingBP', 'Cholesterol', 'MaxHR') will always be present. The ground truth includes a check (`existing_numerical_cols`) to warn if a specified column is missing, making it more robust.
- **Clarity of `HeartDisease` interpretation:** The ground truth explicitly states ""0 = No Heart Disease, 1 = Heart Disease"" which adds helpful context for the `value_counts` output.
- **General output clarity and separation:** The ground truth uses section headers and separators (like `---` and `print(""-"" * 40 + ""\n"")`) which significantly improve the readability and organization of the output. The generated code's output is a bit more compact but less clearly demarcated.",gemma-3-12b-it,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# Filter the dataset
filtered_df = df[(df['Age'] > 50) & (df['HeartDisease'] == 1)]

# Select and display the desired columns
display_df = filtered_df[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]

print(display_df)
```",10,"The generated code is correct, concise, and addresses all requirements of the task. It properly loads the CSV, filters the data based on the specified conditions (`Age` > 50 AND `HeartDisease` == 1), and then correctly selects and displays the `Age`, `Sex`, `ChestPainType`, and `Cholesterol` columns. The imports are correct, and the code structure is clear. It provides exactly what was asked without unnecessary additions, which makes it good for a benchmark evaluation.",gemma-3-12b-it,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: winequality-red.csv not found in the current directory."")
    exit()

# Filter wines based on alcohol and pH
filtered_df = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]

# Identify the wine with the highest quality score
if not filtered_df.empty:
    best_wine = filtered_df.loc[filtered_df['quality'].idxmax()]

    # Display all features of the best wine
  ...",9,"The generated code correctly implements all specified requirements: filtering by compound conditions, finding the maximum quality in the filtered subset, and displaying the features of that wine. It also includes basic error handling for file not found and handles the case where no wines meet the criteria.

Strengths:
- **Correctness:** Accurately performs all filtering and selection logic.
- **Conciseness:** Achieves the task with minimal, clean code.
- **Error Handling:** Includes a check for `FileNotFoundError` and an `if not filtered_df.empty:` condition, which are good practices.

Areas for minor improvement (to achieve a 10):
- **Function Encapsulation:** Encapsulating the logic within a function (as in the ground truth) would improve reusability and modularity, especially in larger projects.
- **Output Clarity:** The `print(best_wine)` output isn't as explicitly formatted as `print(best_wine.to_string())` which can sometimes truncate output for wider dataframes, though for a single series it's usually fine. Adding descriptive print statements (e.g., ""Wine with the highest quality in the filtered subset:"") would enhance user experience.
- **Robustness (minor):** The `KeyError` handling for missing columns is not present, though it's less common for standard datasets.",gemma-3-12b-it,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
try:
    df = pd.read_csv(""heart.csv"")
except FileNotFoundError:
    print(""Error: heart.csv not found in the current directory."")
    exit()

# Generate histogram for Age
plt.figure(figsize=(8, 6))
plt.hist(df['Age'], bins=10, color='skyblue', edgecolor='black')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Generate bar chart for Sex
plt.figure(figsize=(6, 4))
sex_counts ...",8,"The generated code correctly addresses all the requirements of the prompt. It loads the dataset, generates a histogram for the `Age` column, and a bar chart for the `Sex` column. Both plots have titles and labeled axes. The code includes proper imports and handles a `FileNotFoundError`.

**Strong Points:**
- **Correctness:** The code correctly loads the data and generates the requested plots.
- **Completeness:** All aspects of the task, including specific plot types, labels, and titles, are covered.
- **Clarity:** The code is straightforward and easy to understand.
- **Error Handling:** Basic file not found error handling is implemented.

**Areas for Improvement (to achieve a higher score):**
- **Plotting Library Choice:** While `matplotlib` is used effectively, the prompt mentions ""matplotlib or seaborn"". The ground truth utilizes `seaborn` for enhanced aesthetics (`sns.histplot` with `kde` and `sns.countplot`) and additional features like `bins='auto'` for the histogram, and automatically mapping sex categories. The generated code directly uses `plt.hist` and `value_counts().plot(kind='bar')`, which are functional but less feature-rich and aesthetically refined compared to `seaborn` equivalents.
- **Sex Column Interpretation:** The common `heart.csv` dataset uses numerical values (0 and 1) for 'Sex'. The ground truth code intelligently maps these to 'Female' and 'Male' for better readability on the bar chart. The generated code plots 0 and 1 directly, which is less user-friendly. While not explicitly asked to map, it's a good practice in data visualization.
- **Robustness of Error Handling:** The ground truth includes more comprehensive error handling (e.g., `KeyError` for missing columns), which makes it more robust for varied input conditions.

Overall, it's a very good attempt that fulfills the core requirements. The suggested improvements primarily focus on best practices in data visualization and more robust error handling.",gemma-3-12b-it,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

try:
    df = pd.read_csv(""spotify_2023.csv"")
except FileNotFoundError:
    print(""Error: spotify_2023.csv not found in the current directory."")
    exit()

# 1. List column names and data types
print(""Column Names and Data Types:"")
for col in df.columns:
    print(f""- {col}: {df[col].dtype}"")

# 2. Convert 'streams' to numeric, handle errors, and report NaNs
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
print(""\nNumber of NaNs in 'streams' after co...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **CSV Loading:** Handles `FileNotFoundError` gracefully.
- **`streams` Conversion:** Correctly converts `streams` to numeric with `errors='coerce'` and reports NaNs.
- **Descriptive Statistics:** Calculates mean, median, std, min, and max for the specified numerical columns and checks for column existence.
- **Unique Artists:** Correctly counts unique artists.

**Areas for Improvement:**
- **Column Names and Data Types Reporting:** While functional, the ground truth's use of `df.info()` provides a more comprehensive and standard pandas summary (including non-null counts and memory usage) which is generally preferred for exploratory data analysis. The generated code iterates through columns, which is less concise for this specific task.
- **Descriptive Statistics Output Format:** The ground truth code uses `df.describe().loc[...]`, which outputs a clean table for all requested statistics across all columns at once. The generated code iterates through columns and prints statistics individually for each, making the output less readable and comparable.
- **Explicit NaN Reporting:** The ground truth provides a more detailed report about NaNs, including the original and new dtypes, and a sample of rows with NaNs if present. The generated code only reports the count.
- **Clarity of Output:** The print statements in the ground truth are generally more descriptive and provide clearer section headers.

Overall, the code is functionally correct for the primary analytical tasks but could be refined for better output presentation and utilize more idiomatic pandas methods where appropriate.",gemma-3-12b-it,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

try:
    df = pd.read_csv(""spotify_2023.csv"")
except FileNotFoundError:
    print(""Error: spotify_2023.csv not found in the current directory."")
    exit()

df_2023 = df[df['released_year'] == 2023]

df_2023['streams'] = pd.to_numeric(df_2023['streams'], errors='coerce')

selected_columns = df_2023[['track_name', 'artist(s)_name', 'streams']]

sorted_data = selected_columns.sort_values(by='streams', ascending=False)

print(sorted_data)
```",8,"The generated code correctly addresses most of the requirements. It loads the CSV, filters by 'released_year', converts 'streams' to numeric, selects the specified columns, and sorts the data in descending order by 'streams'.

Areas for improvement:
1.  **Order of Operations for `pd.to_numeric`**: The generated code applies `pd.to_numeric` *after* filtering for `released_year` == 2023. While this works for the current task, it's generally safer and more robust to convert the `streams` column to a numeric type *before* any filtering or manipulation if the full DataFrame might contain non-numeric `streams` values that could cause issues with other operations (like sorting or aggregation on the original `df`). The ground truth performs this earlier.
2.  **Handling Missing Values**: The `dropna(subset=['streams'])` step in the ground truth is a good practice if you want to ensure that only records with valid numeric stream counts are included in the final sorted list. The generated code implicitly ranks `NaN` values at the end (which is pandas' default behavior for `sort_values` when NaNs are present), but it doesn't explicitly drop them. Depending on the exact requirement, dropping NaNs might be preferred to get a cleaner ""top streams"" list. For this task, it's not a critical miss but an improvement.
3.  **`SettingWithCopyWarning`**: Applying `df_2023['streams'] = pd.to_numeric(df_2023['streams'], errors='coerce')` after filtering can sometimes trigger a `SettingWithCopyWarning`. While not always an error, it's best practice to avoid it, often by using `.loc` for chained indexing or by ensuring the filtered DataFrame is a true copy if modifications are intended. The ground truth avoided this by performing the numeric conversion on the original DataFrame or by creating a new `df_2023` using `.loc` then selecting columns. Again, not a critical failure here, but good to be aware of.

Overall, it's a good solution that delivers the requested output. The points above are mostly about robustness and best practices that would elevate it to a perfect score.",gemma-3-12b-it,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
try:
    df = pd.read_csv(""spotify_2023.csv"")
except FileNotFoundError:
    print(""Error: spotify_2023.csv not found in current directory."")
    exit()

# Generate histogram of bpm
plt.figure(figsize=(10, 6))
plt.hist(df['bpm'], bins=50, color='skyblue', edgecolor='black')
plt.title('Distribution of BPM')
plt.xlabel('BPM')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.show()

# Create scatter plot of d...",9,"The generated code successfully loads the CSV, generates both specified plots (histogram and scatter plot), and includes titles and labeled axes for each, fulfilling all core requirements of the task. The imports are correct, and the code structure is logical.

Improvements:
- The `plt.show()` call after the first plot immediately displays and closes that plot, preventing the next plot from appearing on the same overall matplotlib window. While functionally correct as separate plots, typically for multiple plots, `plt.show()` is called only once at the very end to display all generated figures. If the intent was two separate windows, this is fine, but if it was for a single output execution, it separates them.
- While the axis labels are good, adding '(%)' specifically to the `danceability_%` and `energy_%` columns in the scatter plot's x-label and y-label (as done in the ground truth) would be a slight improvement for clarity, consistent with the column names. The generated code does use 'Danceability (%)' and 'Energy (%)' which is good, but the ground truth was slightly more explicit to match the exact column names. This is minor.",gemma-3-12b-it,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd

try:
    df = pd.read_csv(""youtube_trendings.csv"")
except FileNotFoundError:
    print(""Error: youtube_trendings.csv not found in the current directory."")
    exit()

# 1. List column names and data types
print(""Column Names and Data Types:"")
print(df.dtypes)

# 2. Clean 'Video views', 'Likes', and 'Dislikes' columns
for col in ['Video views', 'Likes', 'Dislikes']:
    df[col] = df[col].astype(str).str.replace(',', '').astype(float)
    df[col] = df[col].fillna(0)
...",8,"The generated code correctly addresses most of the requirements.

**Strong Points:**
- **Correct Loading:** Handles `FileNotFoundError` gracefully.
- **Data Type Listing:** `df.dtypes` correctly lists column names and types.
- **Numeric Cleaning:** Correctly removes commas, converts to numeric, and fills NaNs with 0 for the specified columns.
- **Descriptive Statistics:** `df.describe()` is a concise and effective way to get the required statistics.
- **Category Filling:** Correctly fills missing 'Category' values with 'Unknown'.
- **Unique Category Count:** Correctly reports the count of unique categories using `nunique()`.

**Areas for Improvement:**
- **Explicit NaN Handling in Numeric Conversion:** The original prompt specified ""if conversion fails for any value, treat that value as NaN, then fill all resulting NaNs..."". The generated code applies `astype(float)` directly after `replace(',', '')`. If there are non-numeric strings *after* comma removal, `astype(float)` will raise a `ValueError` instead of coercing to NaN. The ground truth uses `pd.to_numeric(df[col], errors='coerce')` which is more robust for explicitly handling conversion failures by setting them to NaN. While `df.fillna(0)` would then catch these `NaN`s, the initial conversion step isn't as resilient as it could be.
- **Specific Statistics for Numeric Columns:** While `df.describe()` provides mean, std, min, max, it *also* provides count and 25/50/75 percentiles. The ground truth explicitly extracts and prints only mean, median, std, min, and max, directly aligning with the prompt's request for ""basic descriptive statistics (mean, median, std, min, max)"". The generated code's output is more generic but acceptable in its output.

Overall, it's a very good attempt that fulfills the core requirements. The minor issue with explicit `errors='coerce'` in numeric conversion and adherence to *only* the requested descriptive statistics are small points of refinement.",gemma-3-12b-it,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: youtube_trendings.csv not found."")
    exit()

# Clean 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '').astype(float, errors='coerce')
df['Video views'] = df['Video views'].fillna(0)

# Filter by 'Video views'
df_filtered = df[df['Video views'] > 50000000]

# Select and display 'Video' and 'Video views'
result_df = df_filtered[['Video', 'Vid...",9,"The generated code is highly correct and directly addresses all the requirements of the task.

**Strong points:**
*   **Correct CSV Loading:** Handles `FileNotFoundError` robustly.
*   **Efficient Cleaning:** The `str.replace(',', '')` followed by `astype(float, errors='coerce')` is a concise and effective way to clean the 'Video views' column, handling conversion failures correctly.
*   **Correct NaN Handling:** `fillna(0)` is applied correctly after conversion attempts.
*   **Accurate Filtering:** The filtering condition `df['Video views'] > 50000000` is spot on.
*   **Correct Column Selection:** `df_filtered[['Video', 'Video views']]` properly selects the required columns.
*   **Correct Sorting:** `sort_values(by='Video views', ascending=False)` correctly sorts the data.
*   **Readability:** The code is well-structured and easy to follow.

**Minor Area for Improvement (leading to a score of 9 instead of 10):**
*   **Chaining for `Video views` cleaning:** While the current approach is clear, `df['Video views'] = df['Video views'].str.replace(',', '').astype(float, errors='coerce').fillna(0)` could be chained into a single line for slightly more conciseness. However, the current two-line approach is perfectly readable and acceptable.
*   The ground truth explicitly used `pd.to_numeric` instead of `astype(float, errors='coerce')`. While `astype` is perfectly valid here, `pd.to_numeric` is often preferred for more control and explicit error handling, especially when mixed types might be present, though `errors='coerce'` in `astype` achieves a similar outcome in this specific case. This is a very minor difference in best practice, not a functional error.",gemma-3-12b-it,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: youtube_trendings.csv not found in the current directory."")
    exit()

# Clean 'Likes' column
df['Likes'] = df['Likes'].str.replace(',', '').astype(float, errors='coerce').fillna(0)

# Clean 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '').astype(float, errors='coerce').fillna(0)


# Create scatter plot
plt.f...",9,"The generated code is very good and largely achieves the task requirements.

**Strong Points:**
*   Successfully loads the CSV.
*   Correctly handles the cleaning of 'Likes' and 'Video views' columns:
    *   Removes commas using `str.replace(',', '')`.
    *   Converts to numeric using `astype(float, errors='coerce')` which correctly handles conversion errors.
    *   Fills NaNs with 0 using `fillna(0)`.
*   Generates a scatter plot with the correct columns `Video views` on x-axis and `Likes` on y-axis.
*   Sets the correct plot title ('Video Views vs. Likes').
*   Sets the correct x-axis label ('Video Views (cleaned)').
*   Sets the correct y-axis label ('Likes (cleaned)').
*   Includes `plt.grid(True)` for better readability.
*   Includes all necessary imports (`pandas`, `matplotlib.pyplot`).

**Areas for Minor Improvement (to reach a perfect 10):**
*   **Robustness of `str.replace`:** The generated code directly applies `str.replace` to `df['Likes']` and `df['Video views']`. This implicitly assumes these columns are already of string type or can be coerced to string without issues. The ground truth explicitly uses `.astype(str).str.replace(...)` which is more robust, as `df['Likes'].str` would raise an error if the column is already numeric. However, given the context of a dataset where these columns usually contain numbers with commas, `str.replace` often works directly if pandas infers them as objects/strings.
*   **Error Handling Detail:** The `FileNotFoundError` handling is good, but the ground truth includes a more general `except Exception as e:` which catches other potential loading errors, making it slightly more robust. This is a minor point for this specific problem, as the prompt mainly focuses on the data cleaning and plotting aspects.
*   **Plot Customization (Minor):** The ground truth includes `alpha=0.6` and `s=10` in the `scatter` plot for better visual appearance (transparency and marker size), and `plt.tight_layout()` for better layout. These are minor stylistic improvements, not core functional requirements.

Overall, the generated code is highly functional and fulfills the prompt's core requirements effectively and concisely. The slight difference in string conversion robustness and general error handling are minimal and often situational for a task like this.",gemma-3-12b-it,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd

try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: mobiles_2025.csv not found in the current directory."")
    exit()

# Clean Mobile Weight
df['Mobile Weight'] = df['Mobile Weight'].str.replace('g', '').astype(float, errors='coerce')

# Clean RAM
df['RAM'] = df['RAM'].str.replace('GB', '').astype(float, errors='coerce')

# Clean Battery Capacity
df['Battery Capacity'] = df['Battery Capacity'].str.replace('mAh', '').astype(floa...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Correct Cleaning Logic:** The `str.replace` and `pd.to_numeric` (or `astype(float, errors='coerce')`) approach for cleaning 'Mobile Weight', 'RAM', 'Battery Capacity', and 'Launched Price (USA)' is correct and handles conversion errors gracefully by setting them to NaN.
- **Correct Data Type Reporting:** `df.dtypes` is used correctly to display column names and their inferred data types.
- **Correct Descriptive Statistics:** Each of the specified columns has its descriptive statistics calculated using `.describe()`, which provides the requested mean, median (50%), std, min, and max.
- **Correct Unique Count:** `df['Company Name'].nunique()` correctly reports the count of unique values.
- **Proper Imports:** `pandas` is correctly imported.
- **Error Handling for File:** Includes a basic `try-except` block for `FileNotFoundError`.

**Areas for Improvement (to reach a higher score):**
- **Missing `numpy` Import:** The ground truth includes `import numpy as np`, which is often good practice for data science tasks, although not strictly needed for this specific problem (as `pd.to_numeric` and `astype` handle coercing to float/NaN without explicit NumPy calls). However, a more robust solution might use `np.nan` explicitly sometimes.
- **Lack of Defensive Programming:** While the code handles `FileNotFoundError`, it assumes the presence of all specified columns (`Mobile Weight`, `RAM`, `Battery Capacity`, `Launched Price (USA)`, `Company Name`). In a real-world scenario, checking `if 'Column Name' in df.columns:` before attempting operations would make the code more robust against variations in input data.
- **Output Clarity:** While functional, the output of the descriptive statistics is a bit verbose (`print(""\nMobile Weight:"") print(df['Mobile Weight'].describe())`). The ground truth's approach of using `df[existing_numeric_cols].describe().loc[['mean', '50%', 'std', 'min', 'max']]` presents this more compactly for the specific requested statistics. The prompt explicitly asked for *reporting* mean, median, std, min, max, which `.describe()` does, but combining them could be slightly better.
- **String Cleaning Details:** For 'Mobile Weight', 'RAM', 'Battery Capacity', and 'Launched Price (USA)', adding `.str.strip()` after `replace` can prevent leading/trailing whitespace from interfering with `astype(float)`. The generated code assumes no leading/trailing spaces exist after the unit or prefix is removed. For example, '10g ' (with a trailing space) would become '10 ' then fail `float()` without `.strip()`. `astype(float, errors='coerce')` would handle this by setting to NaN, but ideally, you'd want to clean it first.

Overall, it's a very solid attempt that fulfills almost all functional requirements. The minor points listed above are mostly about robustness, code style/efficiency, and handling edge cases/malformed data more gracefully.",gemma-3-12b-it,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean 'Launched Price (India)' column
df['Launched Price (India)'] = df['Launched Price (India)'].str.replace('INR ', '').str.replace(',', '').astype(float, errors='coerce')

# Clean 'RAM' column
df['RAM'] = df['RAM'].str.replace('GB', '').astype(float, errors='coerce')

# Filter the dataset
df_filtered = df[(df['Company Name'] == 'Apple') & (df['Launched Price (India)'] < 100000)]

# Select and display the...",9,"The generated code correctly addresses all the requirements of the task. It properly loads the CSV, cleans both 'Launched Price (India)' and 'RAM' columns (including handling errors with `coerce`), filters the data as specified, selects the correct columns, and sorts the final output. The use of chained `.str.replace()` and `.astype(float, errors='coerce')` is concise and effective.

One minor improvement for robustness could be to implicitly handle potential non-string types in the 'RAM' and 'Launched Price (India)' columns by adding `.astype(str)` before the `.str.replace()` operations, although `pd.to_numeric` (which `astype(float, errors='coerce')` uses under the hood for non-numeric types) can often handle direct conversion from other types like int/float to NaN without explicit `str` conversion if needed. However, given the nature of the asked cleaning (removing 'INR ', 'GB', commas), conversion to string first is generally safer. The ground truth code explicitly converts to string first for this reason. This is a minor point and doesn't detract significantly from the correctness of the code for the given task description.

Another good practice, though not explicitly required by the prompt, would be to include error handling for `pd.read_csv`, such as a `try-except` block for `FileNotFoundError`. However, this was not a specific requirement for the prompt's output, only the core data manipulation.",gemma-3-12b-it,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: mobiles_2025.csv not found."")
    exit()

# Clean RAM column
df['RAM'] = df['RAM'].str.replace('GB', '').astype(float, errors='coerce')

# Clean Battery Capacity column
df['Battery Capacity'] = df['Battery Capacity'].str.replace('mAh', '').astype(float, errors='coerce')

# Create scatter plot
plt.figure(figsize=(10, 6))
for company in df['Compan...",7,"The generated code correctly loads the dataset, cleans the 'RAM' and 'Battery Capacity' columns (including handling errors with `coerce`), and creates a scatter plot with appropriate labels. The core functionality is present.

However, there are a few areas for improvement:
1.  **Cleaning Robustness**: The `astype(float, errors='coerce')` for cleaning directly on the string series might fail if the values contain leading/trailing spaces or other non-numeric characters beyond 'GB'/'mAh'. The ground truth code uses `.str.strip()` which is more robust.
2.  **Plotting Method**: The plotting loop `for company in df['Company Name'].unique():` is less efficient and more verbose than using a library like Seaborn (as in the ground truth) or even just `df.plot.scatter(..., c='Company Name', colormap='viridis')` if plotting directly with pandas. Seaborn's `scatterplot` with the `hue` argument automates coloring and legend creation effectively. The current approach also cycles through default matplotlib colors, which might not be distinct enough for many companies, and the `plt.legend()` will simply show the company names without specifically linking them to the colors unless a custom colormap is used or more manual effort is put into mapping.
3.  **Completeness of Coloring**: While it attempts to color by company, the manual loop combined with `plt.scatter` without explicitly setting `c` to a numerical representation of `Company Name` (or using a built-in `hue` mechanism like Seaborn's) means the colors are just default matplotlib cycle colors for each new plot, not necessarily distinct or easily traceable back to the legend for many companies. The legend helps, but a more integrated approach like Seaborn's `hue` or a direct pandas plot with `c` argument is superior for this requirement.

Overall, it's a good attempt that gets most of the critical pieces right, but could benefit from more robust data cleaning and a more modern, efficient plotting strategy for coloring by category.",gemma-3-12b-it,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd

def titanic_eda():
    # Load the dataset
    titanic_data = pd.read_csv('titanic.csv')

    # Display the first 5 rows
    print(""First 5 rows of the dataset:"")
    print(titanic_data.head())
    print(""\n"")

    # Show the shape of the dataset
    print(""Shape of the dataset (rows, columns):"", titanic_data.shape)
    print(""\n"")

    # List all column names and their data types
    print(""Column names and their data types:"")
    print(titanic_data.info())
    pri...",8,"The generated code successfully completes almost all tasks correctly.

**Strong points:**
*   **Correctness:** It correctly loads the data, displays the head, shows the shape, calculates the survival rate, and counts missing values.
*   **`df.info()` usage:** Using `df.info()` is a clean way to display column names and data types.
*   **Concise survival rate calculation:** Using `.mean()` for the survival rate is a very elegant and Pythonic way to achieve this, as the 'Survived' column contains 0s and 1s.

**Areas for improvement (preventing a perfect score):**
*   **`df.info()` output:** The `print(titanic_data.info())` line will print `None` after the info output because `info()` returns `None`. It's better to just call `titanic_data.info()` directly, as the method itself handles the printing.
*   **Error Handling:** The ground truth code includes basic error handling for `FileNotFoundError`. While not explicitly requested in the prompt, it's good practice for robust code that interacts with files.
*   **Clarity of output separation:** The `print(""\n"")` lines sometimes add extra blank lines, making the output slightly less compact than the ground truth, which uses clear headings and single newlines effectively.
*   **Formatting of survival rate:** The survival rate is printed without specified decimal places, whereas the ground truth formats it to two decimal places, which is generally preferred for percentages.",gemma-3-12b-it,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
imdb_df = pd.read_csv(""imdb_top_250.csv"")

# 1. List column names and data types
print(""Column Names and Data Types:"")
print(imdb_df.info())

# 2. Calculate and report descriptive statistics
numerical_cols = [""Year"", ""Rating"", ""Runtime""]
descriptive_stats = imdb_df[numerical_cols].describe()
print(""\nDescriptive Statistics for Numerical Columns:"")
print(descriptive_stats)

# 3. Report the total number of movies
total_movies = len(imdb_df)
print(""...",6,"The generated code correctly loads the dataset, lists column names and data types (though `imdb_df.info()` also prints a summary which isn't explicitly requested as a direct output for just ""column names and data types""), and reports the total number of movies.

However, it falls short on several critical aspects:
1.  **Robustness**: It doesn't handle `FileNotFoundError`, which is a common and important consideration when loading external files.
2.  **Runtime Column Preprocessing**: The `Runtime` column in 'imdb_top_250.csv' is typically in a string format (e.g., '142 min'). The generated code attempts to calculate descriptive statistics directly on this column without converting it to a numeric type. This will likely result in an error or incorrect statistics (e.g., if `describe()` ignores non-numeric columns by default for such calculations, which it often does, then `Runtime` statistics will be missing without any indication of why). The ground truth specifically handles this conversion.
3.  **Specific Statistical Metrics**: The prompt asked for ""mean, median, standard deviation, min, max"". The `describe()` method outputs these, but also includes count, 25th percentile, and 75th percentile. While `describe()` is generally good, for a precise request, filtering to only the requested metrics is better practice and demonstrates more control. The ground truth precisely selects these.
4.  **Output Clarity for `df.info()`**: `imdb_df.info()` prints directly to stdout and also returns `None`. Printing `imdb_df.info()` will result in `None` being printed after the info block, which is not ideal. The ground truth just calls `df.info()` for its side effect of printing.
5.  **Error Handling for Missing Columns**: If any of `Year`, `Rating`, or `Runtime` were missing, the `describe()` call would fail or misbehave. The ground truth handles this by filtering `NUMERICAL_COLUMNS_FOR_STATS`.

To improve, the code needs to integrate robust error handling for file loading, pre-process the 'Runtime' column to a numeric type, and consider how to select *only* the requested descriptive statistics.",gemma-3-12b-it,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Filter the dataset
df_filtered = df[df['Rating'] >= 9.0]

# Select and display the desired columns
df_selected = df_filtered[['Title', 'Rating']]

# Sort the data by Rating in descending order
df_sorted = df_selected.sort_values(by='Rating', ascending=False)

# Display the sorted data
print(df_sorted)
```",9,"The generated code correctly performs all the required steps: loading the CSV, filtering by rating, selecting specific columns, and sorting the results. It demonstrates good understanding of pandas operations. The code is clean and readable.

The primary reason it didn't receive a perfect 10 is the lack of robust error handling, specifically for `FileNotFoundError`. The ground truth code includes a `try-except` block to gracefully handle cases where the CSV file might not be present, which is a good practice for real-world applications. While error handling wasn't explicitly mentioned as a requirement for *this specific task*, it's a crucial aspect of complete and production-ready code.

Despite this, the core logic for data manipulation is excellent and directly addresses all parts of the task.",gemma-3-12b-it,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
try:
    df = pd.read_csv(""imdb_top_250.csv"")
except FileNotFoundError:
    print(""Error: imdb_top_250.csv not found in the current directory."")
    exit()

# 1. Histogram of 'Rating'
plt.figure(figsize=(8, 6))
plt.hist(df['Rating'], bins=10, edgecolor='black')
plt.title('Distribution of Movie Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# 2. Scatter plot of 'Rating' vs. 'Runtime'
plt.figure(f...",7,"The generated code correctly addresses the primary requirements of the task: loading the CSV, plotting a histogram for 'Rating', and a scatter plot for 'Rating' vs. 'Runtime', with appropriate titles and labels.

**Strong Points:**
*   Correctly loads the CSV using pandas.
*   Successfully generates a histogram for the 'Rating' column with title and labels.
*   Successfully generates a scatter plot for 'Rating' vs. 'Runtime' with title and labels.
*   Includes basic error handling for `FileNotFoundError`.

**Areas for Improvement (Why not higher):**
1.  **Data Cleaning/Preprocessing for 'Runtime':** This is the most significant missing piece. The `Runtime` column in the `imdb_top_250.csv` dataset is typically in a string format (e.g., ""120 min""). The generated code attempts to plot it directly, which will likely result in an error or an incorrect plot if `Runtime` is not purely numeric. The ground truth demonstrates robust preprocessing for this column.
2.  **Missing Imports (Seaborn):** While `matplotlib.pyplot` is imported and used, `seaborn` is also a common library for such visualizations as mentioned in the prompt (""Use matplotlib or seaborn for plotting""). The ground truth uses `seaborn` for the histogram which often produces more aesthetically pleasing plots.
3.  **Robust Error Handling/Column Checks:** The ground truth includes checks to ensure columns like 'Rating' are present, and handles cases where 'Runtime' might not be appropriately formatted or present. The generated code assumes these columns exist and are in a plottable format.
4.  **Plot Aesthetics/Best Practices:** The `seaborn` library (used in the ground truth) often provides better default aesthetics, and the ground truth also includes `kde=True` for the histogram and `grid` for both plots, which enhance readability. `tight_layout()` is also a good practice to prevent overlapping elements.",gemma-3-12b-it,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
penguins = pd.read_csv(""penguins.csv"")

# Report missing values per column
missing_values = penguins.isnull().sum()
print(""Missing values per column:\n"", missing_values)

# Drop rows with missing values in specific columns
penguins = penguins.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])

# Calculate descriptive statistics
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'b...",7,"The generated code correctly loads the dataset, reports missing values per column, drops rows with missing values in the specified numerical columns, calculates descriptive statistics for these columns, and counts unique values for 'species' and 'island'.

However, there are a few areas for improvement where it deviates from the requirements or best practices:

1.  **Descriptive Statistics Detail**: The `describe()` method provides more statistics than specifically requested (mean, median, standard deviation, min, max). While it includes them, the prompt asked to ""calculate and report basic descriptive statistics (mean, median, standard deviation, min, max)"". Using `agg()` as in the ground truth is more precise to the prompt's request. `describe()` also includes count, 25%, 50% (median), and 75% quartiles.
2.  **Reporting Specific Statistics**: The prompt asked to ""calculate and report basic descriptive statistics (mean, median, standard deviation, min, max)"". The current output of `describe()` is a table, but it's not explicitly formatted to show *only* these specific statistics.
3.  **Error Handling**: The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth and good practice for robust code.
4.  **Clarity of Output**: The output messages are somewhat concise. The ground truth provides more verbose and clear print statements indicating what each section of output represents. For example, explicitly showing ""--- Missing Values Report ---"" or ""--- Descriptive Statistics for Filtered Numerical Columns ---"".
5.  **Implicit Filtering**: The prompt states ""first drop any rows where *any* of these four specific columns have a missing value. Then, calculate and report basic descriptive statistics for these four columns using the data that remains."" The generated code correctly performs this by overwriting `penguins` after dropping NAs. The unique count for `species` and `island` is then performed on this *filtered* DataFrame, which wasn't explicitly asked for, but is a reasonable interpretation. The ground truth performs this on the *original* DataFrame, which is also a valid interpretation as the `dropna` instruction only applied to the numerical statistics part.

Overall, it's a solid attempt that performs the core tasks, but could be refined for precision, robustness, and output clarity.",gemma-3-12b-it,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

try:
    penguins = pd.read_csv('penguins.csv')
    filtered_penguins = penguins[(penguins['species'] == 'Adelie') & (penguins['island'] == 'Torgersen')]
    culmen_data = filtered_penguins[['culmen_length_mm', 'culmen_depth_mm']]
    culmen_data = culmen_data.dropna()
    print(culmen_data.head())
except FileNotFoundError:
    print(""Error: penguins.csv not found. Please ensure the file is in the current directory."")
```",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, filters the data for 'Adelie' species on 'Torgersen' island, selects the specified culmen columns, drops rows with missing values in these columns, and displays the first 5 rows. The use of a `try-except` block for `FileNotFoundError` is also good practice.

One minor point for improvement, which is not critical for correctness but good for robustness, is that `dropna()` without the `subset` argument will drop rows with any NaN values in the `culmen_data` DataFrame. In this specific case, `culmen_data` only contains `culmen_length_mm` and `culmen_depth_mm`, so it achieves the same result as `dropna(subset=['culmen_length_mm', 'culmen_depth_mm'])`. However, explicitly stating the `subset` makes the intent clearer and is more robust if `culmen_data` were ever to contain other columns (which it does not in this problem). Given the current task, it's not a flaw, but worth noting for higher complexity scenarios. Additionally, the ground truth explicitly prints ""First 5 rows of the processed DataFrame:"", which the generated code does not. This is a very minor stylistic difference.",gemma-3-12b-it,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

penguins = pd.read_csv(""penguins.csv"")

# Filter out rows with missing values in relevant columns
penguins_filtered = penguins[
    penguins[""culmen_length_mm""].notna() &
    penguins[""culmen_depth_mm""].notna() &
    penguins[""species""].notna()
]

# Create the scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(penguins_filtered[""culmen_length_mm""], penguins_filtered[""culmen_depth_mm""], c=penguins_filtered[""species""], cmap=""viridis""...",6,"The generated code correctly loads the data, filters NA values for the specified columns, creates a scatter plot with the correct axes, and includes a title and labels. These are strong points.

However, it fails on one crucial aspect: coloring points by species. The generated code attempts to color by `c=penguins_filtered[""species""]` and `cmap=""viridis""`. While `species` is a categorical column (text/strings), `matplotlib.pyplot.scatter` expects numerical data for the `c` argument when using a colormap, or specific color values (e.g., 'red', '#FF0000', or RGB tuples) for each point. Passing string species names directly to `c` with a `cmap` will likely result in an error or an incorrect plot (e.g., all points being the same color or an arbitrary mapping). The `colorbar` also won't correctly represent categorical species.

To correctly color by species, one would typically use `seaborn.scatterplot` (as in the ground truth) which handles categorical `hue` mapping automatically, or manually map species strings to distinct colors if using `matplotlib.pyplot.scatter`. This is a significant functional flaw given the requirement to color by `species`.

Areas for improvement:
1.  **Correct Categorical Coloring:** Implement proper coloring by species. Using `seaborn.scatterplot` is the most straightforward and recommended approach for this, as demonstrated in the ground truth. If sticking to `matplotlib.pyplot.scatter`, a manual mapping of species to colors would be necessary, and a custom legend would replace the colorbar.
2.  **Robustness:** The ground truth includes error handling for `FileNotFoundError`. While not explicitly requested, it's good practice for data loading scripts.
3.  **Readability/Best Practices:** The ground truth encapsulates the logic within a function, which makes the code more reusable and testable. While not strictly evaluated here, it's a good practice.",gemma-3-12b-it,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

# 1. Read the dataset
try:
    penguins = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: penguins.csv not found in the current directory."")
    exit()

# 2. Fill NA values in culmen_length_mm and culmen_depth_mm with the mean
penguins['culmen_length_mm'].fillna(penguins['culmen_length_mm'].mean(), inplace=True)
penguins['culmen_depth_mm'].fillna(penguins['culmen_depth_mm'].mean(), inplace=True)

# 3. Calculate and print mean, median, and st...",8,"The generated code correctly addresses all parts of the task. It loads the dataset, correctly fills NA values for `culmen_length_mm` and `culmen_depth_mm` with their respective means, calculates and prints the mean, median, and standard deviation for these columns, and finally prints the total number of rows and columns. The `try-except` block for file loading is a good practice.

Areas for minor improvement:
1.  **Readability/Clarity**: Using f-strings for the statistics output is good, but the output for `culmen_depth_mm` has a typo (`culen_depth_mm` instead of `culmen_depth_mm`). Also, consistent formatting (e.g., rounding the printed statistics to two decimal places) would improve readability, similar to the ground truth.
2.  **Robustness**: Instead of hardcoding the column names multiple times for NA filling and statistics calculation, using a list of column names and iterating through it (as in the ground truth) would make the code more robust and easier to modify if more columns needed similar processing.
3.  **Variable Naming**: While `penguins` is a clear variable name for the DataFrame, defining constants for the filename (`DATASET_FILE`) and numerical columns (`NUMERICAL_COLUMNS`) can further enhance maintainability and readability, especially in larger scripts.

Overall, it's a solid solution that completes the task efficiently.",gemma-3-12b-it,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the dataset
try:
    penguins = pd.read_csv(""penguins.csv"")
except FileNotFoundError:
    print(""Error: penguins.csv not found. Make sure it's in the current directory."")
    exit()

# 2. Drop rows where flipper_length_mm is NA
penguins = penguins.dropna(subset=['flipper_length_mm'])

# 3. Create a histogram of flipper_length_mm
plt.figure(figsize=(8, 6))
sns.histplot(penguins['flipper_length_mm'], kde...",9,"The generated code successfully completes all tasks outlined in the prompt. It correctly loads the dataset, handles NA values for `flipper_length_mm`, and generates both a histogram and a bar chart with appropriate titles and axis labels.

Strengths:
- **Correctness**: All parts of the task are addressed and correctly implemented.
- **Clarity**: The code is straightforward and easy to understand.
- **Libraries**: Uses `pandas`, `matplotlib.pyplot`, and `seaborn` as requested.
- **NA Handling**: Correctly uses `dropna(subset=['flipper_length_mm'])`.
- **Plotting**: Generates both the histogram and bar chart with correct columns and labels.

Areas for minor improvement (not critical enough to deduct significant points):
- **Error Handling**: The `FileNotFoundError` handling is good, but the ground truth adds more comprehensive error handling (e.g., `except Exception as e`).
- **Plot Enhancements**: While the generated plots are correct, the ground truth adds more aesthetic and readability enhancements such as `sns.set_theme`, `kde=True` (for the histogram), font sizes, grids, and `plt.tight_layout()`. These are not explicitly required by the prompt but are good practices for professional visualizations. The y-axis label for the histogram could be ""Count"" instead of ""Frequency"" since `histplot` by default shows counts when `kde=False`.
- **Informative Prints**: The ground truth adds print statements to inform the user about the shape changes due to NA dropping, which is good practice.

Overall, it's a very good solution that fulfills the core requirements of the task.",gemma-3-12b-it,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# 1. Load the dataset
try:
    penguins = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: penguins.csv not found in the current directory."")
    exit()

# 2. Filter the data for 'Torgersen' island
torgersen_penguins = penguins[penguins['island'] == 'Torgersen']

# 3. Count penguins by species in the filtered subset
species_counts = torgersen_penguins['species'].value_counts()

# Print the counts
print(species_counts)
```",8,"The generated code correctly addresses all the core requirements of the task: it loads the dataset, filters it to include only penguins from 'Torgersen' island, and then counts and prints the number of penguins belonging to each species within that filtered subset. The use of `try-except` for file loading is good for robustness. The code is concise and readable.

Areas for improvement to achieve a higher score (more aligned with the robustness of the ground truth):
1.  **Error Handling for Columns**: The code assumes the 'island' and 'species' columns exist. If these columns were missing, it would raise a `KeyError`. The ground truth includes checks for column existence, which makes it more robust.
2.  **Informative Output**: While the species counts are printed, the ground truth provides more verbose and descriptive print statements indicating success at each step (e.g., ""Dataset loaded successfully,"" ""Filtered data for 'Torgersen' island,"" and clear headers for the final output). This enhances user experience.
3.  **Edge Case - No Torgersen Penguins**: If no penguins are found on 'Torgersen' island, the current code would print an empty `species_counts` Series. The ground truth explicitly handles this case with a message like ""No penguins found on 'Torgersen' island.""",gemma-3-12b-it,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

print(df.shape)
print(df.dtypes)
```",9,"The generated code correctly defines the DataFrame, prints its shape, and lists the data types of all columns, fulfilling all the requirements of the task. The code is concise and efficient. The only minor point for improvement would be to include descriptive print statements before `df.shape` and `df.dtypes` for better clarity, similar to the ground truth code. However, this is a stylistic choice and does not detract from the correctness or functionality of the code.",gemma-3-12b-it,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
titanic_df = pd.read_csv('titanic.csv')

# 1. Bar chart: Survivors vs Non-survivors
plt.figure(figsize=(6, 4))
survival_counts = titanic_df['Survived'].value_counts()
survival_counts.plot(kind='bar')
plt.title('Number of Survivors vs. Non-Survivors')
plt.xlabel('Survival (0 = No, 1 = Yes)')
plt.ylabel('Count')
plt.show()

# 2. Bar chart: Distribution of Passenger Classes
plt.figure(figsize=(6, 4))
pclass_counts = t...",7,"The generated code correctly creates four distinct plots as requested and includes proper imports. It also handles missing age values, although the method used (`dropna()`) is less robust than filling with a median or mean, which is often preferred for maintaining data distribution and avoiding loss of observations.

Areas for improvement:
- **Age handling:** Using `dropna()` for age discards rows with missing age, which might be a significant portion of the data. Filling with the median (`titanic_df['Age'].fillna(titanic_df['Age'].median(), inplace=True)`) or mean would be a more standard and often better approach.
- **Axis labels for `Survived` plot:** The x-axis labels `0` and `1` for 'Survived' are not as descriptive as 'Non-survivor' and 'Survivor', which the ground truth code used.
- **Plot aesthetics:** The plots are functional but lack some aesthetic enhancements like `tight_layout()`, `grid`, `xticks` rotation, or custom colors which make the plots more readable and professional. The ground truth code includes these details.
- **Error handling:** The ground truth includes error handling for `FileNotFoundError`, which is good practice for production-ready code. The generated code lacks this.",gemma-3-12b-it,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]}

df = pd.DataFrame(data)

# Calculate and print the average 'Midterm_Score' for all stude...",9,"The generated code correctly performs all the requested tasks: creating the DataFrame, calculating and printing the average midterm and final scores, and identifying the subject with the highest average final score. The code is well-structured and uses appropriate pandas functions. It correctly imports pandas.

Minor areas for improvement (which don't impact correctness or functionality significantly for this task, but align with good practice):
- The print statements could be more descriptive, e.g., ""Average Midterm Score **for all students**"" as in the ground truth.
- String formatting for numerical outputs (e.g., `:.2f` for two decimal places) could be applied for better readability, especially for averages. The ground truth includes this.
- The output for the subject with the highest final score could also include the actual average score value for that subject, as the ground truth does, which provides more context.",gemma-3-12b-it,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
        'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
        'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
        'Volume': [1000000, 120...",9,"The generated code correctly performs all the required tasks: it creates the DataFrame with 'Date' as the index, calculates the 'Daily_Range' column accurately, identifies the date with the highest 'Close' price, and prints the updated DataFrame as requested. The imports are correct, and the code structure is logical and easy to follow.

The only minor point of difference is that the prompt asks to ""print the DataFrame including the new 'Daily_Range' column"" (which it does), and then for the highest close price printout the Ground Truth includes a f-string with a newline and explicit formatting of the date, whereas the generated code's f-string does not format the date or add a newline, but it still accurately prints the date. This is a very minor stylistic difference and does not affect correctness or functionality significantly.",gemma-3-12b-it,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST0...",10,"The generated code is correct, complete, and well-structured. It accurately creates the DataFrame, calculates the 'Total_Revenue' column, correctly identifies the product sold the most by quantity, and prints the first 5 rows of the modified DataFrame. All necessary imports are included, and the code follows standard pandas practices. The output formatting for printing the head of the DataFrame is slightly different from the ground truth (`df.head()` vs `df.head(5)`), but `df.head()` by default prints the first 5 rows, so functionally it's identical.",gemma-3-12b-it,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

average_session_duration = df[""SessionDurationMinutes""].mean()
print(f""Average Session Duration (Minutes): {average_session_dur...",9,"The generated code correctly defines the DataFrame, calculates the average session duration, and counts users by device type, fulfilling all task requirements. The code is clean, concise, and uses appropriate pandas methods for the operations.

Areas of strength:
- Correct DataFrame creation.
- Accurate calculation of average session duration using `.mean()`.
- Correctly counts device types using `.value_counts()`.
- Output formatting is clear and readable.

Minor areas for improvement to reach a perfect score:
- The exact printing format for ""Average Session Duration"" differs slightly from the ground truth (e.g., ground truth added "".2f"" for decimal formatting, and ""minutes"" unit). While not explicitly required for this specific task, matching the format could be seen as a minor deviation.
- The ground truth included separating lines (print(""-"" * 50)) and number prefixes (e.g., ""1. Average Session Duration..."") which the generated code did not. Again, this is a stylistic difference and doesn't impact correctness or functionality.

Overall, the code is highly effective and correct.",gemma-3-12b-it,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

max_temperature = df[""TemperatureCelsius""].max()
print(f""Maximum TemperatureCel...",9,"The generated code correctly defines the DataFrame and accurately finds and prints the maximum temperature and minimum humidity. It adheres to all requirements of the task. The only minor difference from the ground truth is the absence of the units ""°C"" and ""%"" in the print statements for temperature and humidity, respectively, which were present in the ground truth's output format. While not explicitly stated as a critical requirement for the *output string format*, it's a small detail that could make the output more informative. Otherwise, the code is concise, correct, and demonstrates good understanding of pandas operations.",gemma-3-12b-it,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}

df = pd.DataFrame(data)

# Print the total number of ""Likes"" across all posts
total_likes = df[""Likes""].sum()
print(f""Total Likes: {total_likes}"")

# Fin...",10,"The generated code is correct, complete, and of good quality. It precisely defines the DataFrame as specified, correctly calculates and prints the total number of ""Likes"", and accurately identifies and prints the ""PostType"" with the highest average number of ""Shares"". The variable names are clear, and the code is well-structured and easy to understand. It directly addresses all requirements of the task.",gemma-3-12b-it,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",9,"The generated code is highly correct and complete, successfully addressing all requirements of the task. It accurately defines the DataFrame with the specified data types, calculates the average age, and correctly counts the patients by gender. The code adheres to good practices by importing pandas and using its functionalities effectively.

The only minor point of difference, and reason for not giving a perfect 10, is the output formatting for the average age. The ground truth code used `:.2f` to format the average age to two decimal places, which is often good practice for numerical outputs like averages, reflecting a slightly more polished presentation. The generated code prints the full float precision by default (`Average Age: 42.5`), which is not incorrect but less refined. However, this is a very minor stylistic difference and does not impact the correctness of the calculation or the overall solution. The addition of comments, as seen in the ground truth, would also slightly improve readability but isn't strictly required for functional correctness.",gemma-3-12b-it,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}
df = pd.DataFrame(data)

# 1) Print the ""Title"" of the movie with the highest ""Rating"".
highest_rated_movi...",9,"The generated code is highly correct and concise, fulfilling all requirements of the task.

**Strengths:**
- **Correctness:** The code accurately defines the DataFrame, correctly identifies the title of the movie with the highest rating using `idxmax()`, and correctly calculates the average year.
- **Conciseness:** Using `idxmax()` is a very efficient and clean way to find the index of the maximum value, leading to more compact code compared to filtering by `max()`.
- **Completeness:** Both required tasks (finding the highest-rated movie title and average year) are addressed.
- **Code Quality:** Proper `pandas` import is present, and variable names are clear.

**Areas for Minor Improvement (for a perfect 10):**
- **Output Clarity:** While the primary output required for the second task is the average year, the ground truth adds a descriptive string for better user understanding (""The average year of all movies is: ...""). The generated code simply prints the raw number. Adding a descriptive print string would enhance user experience.
- **Handling Edge Cases (Minor):** The `idxmax()` approach assumes there will be a single maximum rating or that the first one found is sufficient. In cases where multiple movies share the absolute highest rating, `idxmax()` will return the index of the first occurrence. The ground truth code explicitly handles this by filtering first, then taking `iloc[0]`, which is slightly more robust if the requirement were to show *all* such movies, even if the prompt implies one. However, for ""the title of the movie,"" `idxmax()` is generally accepted and efficient.

Overall, this is a very strong and effective solution.",gemma-3-12b-it,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

# Print the total number of cars in the dataset
total_cars = len(df)
...",9,"The generated code correctly defines the DataFrame, accurately calculates and prints the total number of cars, and computes and prints the average ""PriceUSD"". It addresses all requirements of the task. The use of `len(df)` for total cars and `.mean()` for average price is standard and efficient. The f-strings for printing are also good practice.

The only minor room for improvement, which is not a correctness issue but a style/completeness detail, is that the output strings are slightly less descriptive than the ground truth. For example, ""Total number of cars: {total_cars}"" vs ""Total number of cars in the dataset: {total_cars}"" and ""Average PriceUSD: {average_price}"" vs ""Average PriceUSD of all cars: ${average_price:.2f}"". The ground truth also formatted the currency to two decimal places, which is generally good practice for price values. These are minor stylistic points and do not detract from the functional correctness or quality.",gemma-3-12b-it,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}

df = pd.DataFrame(data)

highest_hp_pokemon = df.loc[df[""HP""].idxmax(), ""Name""]
...",9,"The generated code correctly defines the DataFrame and accurately identifies the Pokemon with the highest HP and lowest Defense. The code is concise and uses appropriate pandas functions (`idxmax`, `idxmin`, `loc`). However, it misses the explicit printing of descriptive labels, which were present in the ground truth for ""The Pokemon with the highest HP is:"" and ""The Pokemon with the lowest Defense is:"". While the task prompt asked to ""Print the name..."", the ground truth provided a more user-friendly output. This is a minor stylistic difference rather than a functional error.",gemma-3-12b-it,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd

# Load the dataset
titanic_df = pd.read_csv('titanic.csv')

# Identify columns with missing values
missing_values = titanic_df.isnull().sum()
print(""Missing Values:\n"", missing_values)

# Fill missing Age values with the median age
titanic_df['Age'].fillna(titanic_df['Age'].median(), inplace=True)

# Fill missing Embarked values with the most frequent port
titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0], inplace=True)

# Drop the Cabin column
titanic...",6,"The generated code addresses most of the requirements but has a critical flaw in the title extraction step, which significantly impacts its correctness and completeness.

**Strengths:**
*   Correctly loads the dataset.
*   Accurately identifies and fills missing 'Age' values with the median.
*   Accurately identifies and fills missing 'Embarked' values with the most frequent port.
*   Correctly drops the 'Cabin' column.
*   Displays the shape and head of the cleaned dataset as requested.
*   Uses `inplace=True` where appropriate, making the code more concise.

**Areas for Improvement / Critical Flaws:**

1.  **Title Extraction (Major Flaw):** The line `titanic_df['Name'] = titanic_df['Name'].str.extract(r'(Mr|Mrs|Miss|Master)')` overwrites the original 'Name' column with only these specific titles. This is problematic for two reasons:
    *   It loses the original full names, which may be needed for other analyses or if the task implicitly assumed the `Name` column would remain for other reasons (though the prompt doesn't explicitly state this). More critically, it *modifies* the column intended for general extraction, rather than just extracting.
    *   The prompt specifically asks for ""Mr., Mrs., Miss., Master., **etc.**"", implying a more general extraction of titles (like 'Dr.', 'Rev.', 'Capt.', etc.) as done in the ground truth. The current regex only extracts a very limited set of common titles and ignores all others, filling them with NaN, which are then filled with 'Unknown'. This misses the ""etc."" part of the requirement.
    *   The subsequent `titanic_df.drop('Name', axis=1, inplace=True)` is also odd given that `titanic_df['Name']` has already been *transformed* into `Title` and then copied to `Title`. The logical flow would be to extract `Title` from the *original* `Name` column and then drop the *original* `Name` if it's no longer needed, not to transform `Name` and then drop it.

2.  **Completeness of Title Extraction:** As mentioned above, the `str.extract(r'(Mr|Mrs|Miss|Master)')` is not comprehensive enough for ""etc."" as implied by the prompt. The ground truth's use of `re.search(' ([A-Za-z]+)\.', name)` is much more robust for extracting *any* title followed by a period.

3.  **Redundant Step in Title Handling:** The code first transforms the 'Name' column to just the extracted titles (or NaN) and *then* assigns this *transformed* 'Name' column to a new 'Title' column. A more direct approach would be `titanic_df['Title'] = titanic_df['Name'].str.extract(r'...')` and then `titanic_df.drop('Name', axis=1, inplace=True)` if the original `Name` column is no longer needed. The current approach overwrites the `Name` column unnecessarily.

To improve, the code should revise the title extraction logic to be more general and avoid overwriting the `Name` column unless explicitly intended. The `str.extract` can be powerful, but its usage here loses information and doesn't fully meet the ""etc."" requirement.",gemma-3-12b-it,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}

df = pd.DataFrame(data)

# 1) Print the ""...",10,"The generated code is correct, concise, and effectively addresses all requirements of the task. It correctly defines the DataFrame, identifies the title of the book with the highest price using `idxmax()`, and calculates the total stock using `sum()`. The code is clean and follows standard Python and pandas conventions. It successfully implements the logic for both sub-tasks. The print statements are direct but fulfill the requirement.",gemma-3-12b-it,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""water_potability.csv"")

# Display the first 10 rows
print(""First 10 rows:"")
print(df.head(10))

# Show the shape of the dataset
print(""\nShape of the dataset:"")
print(df.shape)

# List all column names and their data types
print(""\nColumn names and data types:"")
print(df.info())

# Calculate the percentage of potable vs non-potable water samples
potability_counts = df['Potability'].value_counts(normalize=True) * 100
print(""\nPer...",7,"The generated code correctly addresses most of the requirements. It successfully loads the data, displays the first 10 rows, shows the shape, lists column names and types, calculates potability percentages, counts missing values, and provides summary statistics.

However, there are several areas for improvement to meet the standard of the ground truth and be considered more robust:

1.  **Error Handling (Critical Missing):** The generated code lacks explicit error handling for `FileNotFoundError` when loading the CSV. The ground truth includes a `try-except` block, which is crucial for real-world applications to prevent crashes if the file is missing.
2.  **Output Clarity:** While the ground truth uses clear print statements with separators (`---`, `===`), the generated code's output is less structured and harder to read. For example, `df.info()` already prints information, so `print(df.info())` results in `None` being printed as well.
3.  **Specific Requirement Adherence:**
    *   For missing values, the ground truth specifically prints only columns with *missing values* (greater than 0), which is more informative than printing all columns when many might have zero missing values.
    *   The ground truth provides a small interpretation for the potability distribution, which adds value.
4.  **Redundancy in `df.info()` print:** `df.info()` itself prints the information to the console. Wrapping it in `print()` will cause `None` to be printed afterward, which is not ideal.

Overall, it's a good attempt that covers the core logic but falls short on robustness, user-friendliness of the output, and some nuanced aspects of the requirements.",gemma-3-12b-it,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd

def clean_water_potability_data(file_path='water_potability.csv'):
    """"""
    Performs basic data cleaning on the water_potability.csv dataset.

    Args:
        file_path (str): Path to the CSV file.

    Returns:
        pandas.DataFrame: The cleaned dataset.
    """"""

    # 1. Load the dataset
    df = pd.read_csv(file_path)

    # 2. Identify missing values
    missing_values = df.isnull().sum()
    print(""Missing Values Before Cleaning:"")
    print(missing_va...",6,"The generated code addresses most of the requirements but has several shortcomings that prevent it from achieving a higher score.

**Positives:**
*   Correctly loads the dataset and handles basic missing value imputation (mean for numerical columns).
*   Correctly identifies and removes duplicate rows.
*   Attempts to verify data ranges.
*   Provides some before/after summary statistics (missing values and shape).
*   Displays the first 5 rows of the cleaned data.
*   Encapsulates the cleaning logic in a function, which is good practice.

**Areas for Improvement/Deficiencies:**
1.  **Missing Value Identification and Reporting (Before Cleaning):** The generated code simply prints all `missing_values = df.isnull().sum()`, which includes columns with no missing values. The ground truth code specifically identifies and displays *only* columns that have missing values, which is more informative for the ""identify columns with missing values"" sub-task.
2.  **Range Validation - Incompleteness and Column Naming:**
    *   The `elif col == 'pH':` condition uses 'pH' (lowercase 'p', uppercase 'H') while the actual column name in the dataset is 'ph' (all lowercase). This means the pH range check will not be applied correctly unless the column name is 'pH'. *Self-correction: The ground truth code uses 'ph' and it should be consistent.* This is a critical logical error.
    *   It only checks `pH`, `Trihalomethanes`, and `Sulfate`. The task specifies checking *all* values for reasonable ranges, and the ground truth code implements a more comprehensive check for non-negative values for other relevant parameters (`Hardness`, `Solids`, `Chloramines`, `Conductivity`, `Organic_carbon`, `Turbidity`). The generated code misses most of these.
    *   The range validation removes rows *within the loop*, which means the `df` is modified multiple times. If a row satisfies the `pH` condition but then fails `Trihalomethanes`, it's removed. This is logically correct for simple range exclusion but can be less efficient than applying all conditions simultaneously (as often seen in single boolean masking operations, like `df = df[(df['ph'] >= 0) & (df['ph'] <= 14) & (df['Hardness'] >= 0) & ... ]`). However, a loop with `df = df[df[col] >= 0]` is also acceptable. The main issue here is the *missing* checks for other columns.
3.  **Summary Statistics:**
    *   The ""Missing Values After Cleaning"" is printed, but it doesn't clearly state ""No missing values remaining"" if that's the case.
    *   The ""Dataset Shape Before Cleaning"" and ""After Cleaning"" are printed using `initial_shape` and `final_shape` respectively. `initial_shape` captures the shape *after loading but before any cleaning*, whereas `final_shape` captures the shape *after duplicate removal but before range validation*. This means the `final_shape` printed is not truly the shape of the *final* cleaned dataset, as rows might be removed by range validation *after* this shape is recorded. The summary for the final shape should be after *all* cleaning steps are complete. The ground truth code prints the shape *after* all cleaning steps.
4.  **Error Handling:** The ground truth code includes basic `try-except` for `FileNotFoundError`, which is good practice for robust code. The generated code lacks this.
5.  **Output Clarity:** The ground truth code provides more descriptive print statements regarding which steps are being performed (e.g., ""Filling missing values:"", ""Removed X duplicate rows.""). The generated code's output is more concise but less informative about the process steps.

Overall, it's a good attempt, but the range validation is incomplete and the summary statistics are slightly misaligned with the final state of the dataframe.",gemma-3-12b-it,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('water_potability.csv')

# 1. Bar chart of potable vs non-potable water samples
plt.figure(figsize=(6, 4))
sns.countplot(x='Potability', data=df)
plt.title('Distribution of Potable vs Non-Potable Water Samples')
plt.xlabel('Potability')
plt.ylabel('Count')
plt.show()

# 2. Histograms for pH, Hardness, and Chloramines
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.histpl...",7,"The generated code successfully creates all four required visualizations. It demonstrates good understanding of using pandas, matplotlib, and seaborn for data visualization.

**Strengths:**
*   Correctly uses `countplot` for the bar chart, `histplot` for distributions, `boxplot` for comparison, and `heatmap` for correlations.
*   Handles subplot creation for histograms effectively.
*   Includes titles and labels for all plots, which is a key requirement.
*   Correctly imports necessary libraries.

**Areas for Improvement:**
*   **Handling Missing Values:** While seaborn's plotting functions often handle `NaN`s by ignoring them, explicitly calling `.dropna()` on the series for histograms (as in the ground truth) can make the intent clearer and control how missing data is handled. For `df.corr()`, `NaN`s are handled by default, but it's good to be aware.
*   **Plot Customization/Clarity:**
    *   The bar chart lacks labels for the `Potability` categories (0 and 1), making it less immediately understandable without referring to external context.
    *   The histograms could benefit from specifying `bins` for better distribution representation.
    *   The `figsize` for the bar chart is a bit small (`(6,4)` vs `(8,6)` in GT), making labels potentially cramped.
    *   Adding `plt.tight_layout()` to all independent plots (not just the subplots) can improve overall presentation by preventing labels from overlapping.
    *   The ground truth added `grid` lines, which can enhance readability.
*   **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is a good practice for robust code, though not explicitly required by the prompt, it's a mark of more polished code.
*   **Consistency/Best Practices:** The `sns.set_style(""whitegrid"")` from the ground truth is a good practice to set a consistent and visually appealing theme across all plots. Using `palette` arguments for `countplot` and `boxplot` can make them more visually appealing.",gemma-3-12b-it,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression
import numpy as np

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Feature Engineering
data['density_acidity'] = data['density'] * data['acidity']
data['volatile_acidity_ratio'] = data['volatile.acidity'] / data['acidity']
data['sulphates_residual_ratio'] = data['sulphates'] / da...",5,"The generated code attempts to address the task but falls short on several critical aspects, leading to a low score.

**Correctness Issues:**
*   **Feature Engineering:**
    *   `data['density_acidity'] = data['density'] * data['acidity']`: There is no 'acidity' column. The wine dataset has 'fixed acidity' and 'volatile acidity'. This line will raise a `KeyError`.
    *   `data['volatile_acidity_ratio'] = data['volatile.acidity'] / data['acidity']`: Same issue, 'volatile.acidity' is wrong, should be 'volatile acidity'. 'acidity' column does not exist. This will fail.
    *   `data['sulphates_residual_ratio'] = data['sulphates'] / data['residual.sugar']`: This looks plausible, but 'residual.sugar' should be 'residual sugar'.
    *   The code does not handle potential division by zero for the ratio features, which the ground truth gracefully handles with an epsilon. This is a common issue with creating ratio features.
*   **RFE Feature Importances:** The line `rfe_importances = rfe.feature_importances_` is incorrect. The `rfe` object itself does not have a `feature_importances_` attribute after fitting; its underlying estimator (RandomForestRegressor) does. Furthermore, `rfe.feature_importances_` would only work if the estimator had feature importances, but `rfe` is the selector, not the estimator. To get feature importances for the *selected* features, one would need to re-fit the `RandomForestRegressor` on only those selected features, as demonstrated in the ground truth. The current `zip` line trying to display `rfe_importances` will incorrectly produce `KeyError` or an attribute error, or just display junk if it somehow ran.
*   **SelectKBest Scoring:** The output for SelectKBest only shows the selected features, not their scores, which was explicitly asked for in the prompt's requirement ""Display the final selected features with their importance scores or rankings.""

**Completeness Issues:**
*   **Meaningful Features:** While it attempts three new features, the incorrect column names make them non-meaningful and incorrect, failing the ""meaningful"" aspect. The ground truth provides examples of well-justified features.
*   **Number of Features:** The prompt mentioned ""top 5-7 most important features."" The generated code arbitrarily uses `k=6` for both methods without explicitly stating why or making it a configurable parameter. The ground truth makes `N_FEATURES_TO_SELECT` explicit.
*   **Comparison and Contrast:** The code does not perform a proper comparison or contrast. It merely prints the two lists of selected features and then a combined list. It completely misses the qualitative analysis part of the prompt: ""Compare and contrast the feature sets selected by each method."" This includes identifying common features, unique features, and explaining *why* they might differ, which is crucial for demonstrating understanding of the techniques.
*   **Display Importance/Rankings:** As noted above, RFE importances are incorrectly handled, and SelectKBest scores are not displayed. The ground truth cleanly presents these alongside the features in dataframes.
*   **Scaling:** The ground truth includes `StandardScaler`, which is a good practice for many ML algorithms, even if RFE with RandomForest and SelectKBest are less sensitive. The generated code omits this.
*   **Error Handling:** No error handling for `FileNotFoundError` when loading the CSV.

**Code Quality, Structure, and Readability:**
*   **Variable Names:** Consistent.
*   **Comments:** Sparse. More comments would be helpful, especially given the logical errors.
*   **Output Formatting:** The output is very basic and not well-structured for comparison like the ground truth's use of dataframes.

In summary, the code demonstrates a basic understanding of the methods but fails significantly on execution, accuracy of feature engineering, and the detailed comparative requirements. The critical errors in feature engineering and RFE importance calculation make the code non-functional as intended.",gemma-3-12b-it,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor
from sklearn.experimental import enable_stacking
from sklearn.stacking import StackingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import ...",4,"The generated code fails to meet several key requirements of the task, particularly in data preprocessing, model evaluation, and comprehensive visualization.

**Primary reasons for failure:**

1.  **Incorrect Data Preprocessing:**
    *   **Missing Values:** Uses `df.dropna()` which is a catastrophic approach for a real-world dataset. The ground truth correctly uses `SimpleImputer` for numerical and categorical features to preserve data.
    *   **Categorical Encoding:** Uses `LabelEncoder` for all object columns, which is inappropriate for nominal categorical features (e.g., 'Platform', 'Genre', 'Publisher') as it introduces an artificial ordinal relationship. `OneHotEncoder` is the correct method for these in most cases, which the ground truth uses.
    *   **User_Score Handling:** Does not specifically handle the 'tbd' values (or similar non-numeric entries) in 'User_Score' which should be converted to NaN first then imputed, as done in the ground truth.

2.  **Incomplete Model Evaluation:**
    *   The `cross_val_score` is used, but only the negative mean squared error is stored. The task explicitly asks for MAE, MSE, RMSE, and R² for *each* model using cross-validation. The generated code calculates these metrics only *once* on a single `X_test` split, which doesn't fulfill the cross-validation evaluation requirement for all models.
    *   The results are stored in `cv_scores` but never properly displayed in a consolidated, readable format like the ground truth's `results` dictionary and `pd.DataFrame` summary.

3.  **Flawed Ensemble Implementation:**
    *   **XGBoost Placeholder:** The generated code puts `RandomForestRegressor` as a placeholder for XGBoost. This is a critical error as XGBoost is a distinct algorithm and a core requirement. It also suggests that `xgboost` library was not installed or used.
    *   **Voting/Stacking Evaluation:** The `evaluate_model` function uses the models *after* they've been potentially trained during `cross_val_score` (which refits them internally). However, the main evaluation loop for individual models is missing, and ensembles are evaluated just once on `X_test` not via proper cross-validation. The ground truth correctly evaluates voting and stacking ensembles through cross-validation as well.

4.  **Limited Visualization:**
    *   **Feature Importance:** Only provides feature importance for the *best Random Forest model*, not for ""ensemble models"" generally. It also uses `range(X_train.shape[1])` as x-axis labels, which becomes uninterpretable with Label Encoding and mixed feature types. The ground truth correctly extracts feature names after preprocessing, including OHE columns, and handles models that don't have direct `feature_importances_`.
    *   **Residual Plots:** Only provides a residual plot for the *Random Forest model*, not a more comprehensive analysis choosing the best performing model.

5.  **Lack of Pipeline for Preprocessing:** The generated code performs preprocessing steps (`dropna`, `LabelEncoder`) *before* splitting the data or defining a pipeline. This is problematic:
    *   `dropna()` prior to split can lead to data leakage if rows with NaNs are dropped from the training set but not the test set (though in this case it drops from everywhere).
    *   More importantly, without a `Pipeline` or `ColumnTransformer`, the preprocessing steps are applied to the entire dataset once. When `cross_val_score` or `GridSearchCV` are used, they internally split data. The preprocessing applied on the full `X` then means it's not correctly separated for each fold, mimicking a real-world scenario where a model is trained on a subset and applied to unseen data. The ground truth correctly uses `ColumnTransformer` and `Pipeline` to encapsulate preprocessing within the cross-validation loop.

**Overall:** The code demonstrates a basic understanding of some Sklearn components but critically misunderstands proper data preprocessing, cross-validation methodology for evaluation, and robust ensemble implementation. It fails to meet the ""expert code evaluator"" expectation for a data science benchmark.",gemma-3-12b-it,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import pymc3 as pm
import ruptures as rpt
from lifelines import KaplanMeierFitter
from statsmodels.stats.sandwi...",3,"The generated code attempts to address many of the requirements but falls short significantly in correctness, completeness, and code quality.

**Correctness & Completeness Issues:**

*   **Data Loading and Aggregation (Task 1):** The code attempts to read `df.groupby(""Year"")[""Global_Sales""]`, but the CSV uses `Year_of_Release`, not ""Year"". This is a fundamental error. Also, it doesn't correctly convert to a datetime index and set frequency, which is crucial for time series analysis in `statsmodels`.
*   **ARIMA Modeling (Task 2):**
    *   The `df_agg` is grouped by ""Year"", which doesn't exist.
    *   `ARIMA` model from `statsmodels.tsa.arima.model` requires the time series to be indexed by `DatetimeIndex`. The generated code does not handle this.
    *   The `ARIMA` model fitting and forecasting are very basic; it doesn't use `auto_arima` or provide confidence intervals, which were specified in the prompt.
*   **Seasonal Decomposition (Task 3):**
    *   The `period=4` for yearly data is incorrect. Yearly data typically has no inherent seasonality at a 4-year cycle unless specified (e.g., presidential election years which is not relevant here). The ground truth correctly identifies this and suggests a longer period if meaningful, or acknowledges its limitations.
    *   It doesn't handle plotting or showing the decomposition results as requested implicitly by ""perform seasonal decomposition"".
*   **Multivariate Regression (Task 4):**
    *   The interaction term creation (`interaction_cols`, `X_encoded[:, [col1, col2]]*X_encoded[:, [col1, col2]]`) is fundamentally flawed and syntactically incorrect. It also doesn't handle the interaction of numerical (`Year`) with categorical features, which is a key part of the ground truth (Year * Genre, Year * Platform, Year * Publisher).
    *   It uses `train_test_split` instead of `TimeSeriesSplit` for validation, which is inappropriate for time series data.
    *   It lacks proper variable handling (e.g., `X_encoded.columns` won't directly give meaningful column names from `OneHotEncoder`).
    *   No confidence or prediction intervals are generated for this model.
*   **Polynomial Regression with Regularization (Task 5):**
    *   The polynomial feature creation `np.column_stack((df[""Year""]**2, df[""Year""]**3))` is very manual and rigid. `PolynomialFeatures` from `sklearn` is the standard and more flexible approach.
    *   It again uses `train_test_split` instead of `TimeSeriesSplit`.
    *   No confidence or prediction intervals are generated.
*   **Bayesian Regression (Task 6):**
    *   The `PyMC3` model is extremely simplified and likely incorrect for the given task. It tries to model `mu = beta * X_poly.mean()`, which is a global mean, not a regression with multiple features.
    *   It doesn't use the actual features (`Year`, `Genre`, `Platform`, `Publisher`) as inputs to the Bayesian model.
    *   `observed = pm.Normal('observed', mu=mu, sigma=sigma)` is sampling a prior, not the likelihood over the observed data.
    *   Categorical variables are not handled within the `PyMC3` model.
    *   No uncertainty (prediction/confidence intervals) from the Bayesian model is extracted or printed.
*   **Change Point Detection (Task 7):**
    *   The `pen=10` is an arbitrary value; the ground truth code correctly derives a penalty based on `2 * log(n_samples_signal)` for better heuristic.
    *   It doesn't map indices back to years for interpretability, nor does it plot the change points.
*   **Survival Analysis (Task 8):**
    *   The `Years` column calculation `df['Years'] = 2023 - df['Year']` is a very crude and incorrect definition of ""longevity."" Game longevity should be about how long a game remained active/profitable, not just how old it is from the current year. The ground truth correctly defines this based on `max_year - release_year`.
    *   `kmf.fit(df['Years'], df['Global_Sales'])`: `Global_Sales` is incorrect as the `event_observed` parameter. It should be a binary event (1 if event occurred, 0 if censored), or if not specified in `lifelines`, it assumes event=1. This is a crucial mistake.
    *   It only implements Kaplan-Meier and completely misses the Cox Proportional Hazards model, which was specifically requested to model with covariates.
*   **Confidence and Prediction Intervals (Task 9):** The generated code simply adds a ""placeholder"" comment, indicating it clearly doesn't address this critical requirement for *all* models. The ground truth provides examples for ARIMA, Multivariate OLS, and approximates for Polynomial (acknowledging limitations) and Cox.
*   **Time Series Cross-Validation (Task 10):** The generated code provides a placeholder comment, failing to implement this requirement entirely. The ground truth implements this for ARIMA and regression models.

**Code Quality Issues:**

*   **Imports:** Missing `matplotlib.pyplot`, `seaborn`, `pmdarima` (for auto_arima), `statsmodels.formula.api` (useful for formulae), `TimeSeriesSplit`, `PolynomialFeatures`, `StandardScaler`, `CoxPHFitter`, `ruptures`.
*   **Data Preprocessing:** Many steps from the ground truth like handling `Year_of_Release` as string/NaN, converting to int, setting datetime index frequency for time series are missing.
*   **Readability/Structure:** The code is one large function, making it harder to test or run specific parts. The ground truth structures it into distinct sections with print statements, making it easier to follow.
*   **Reproducibility:** No random seed is set.
*   **Warning Suppression:** The prompt states ""without explanations,"" but also asks for proper imports, implying robust and production-ready code. Suppressing warnings is important for cleaner output.
*   **Plotting:** No plots are generated, despite being a strong component of statistical analysis and often requested implicitly in such tasks.

In summary, the generated code demonstrates a very superficial understanding of the requested advanced statistical modeling and time series analysis techniques. Many implementations are either incorrect, incomplete, or rely on placeholders.",gemma-3-12b-it,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture
from sklearn.decomposition import PCA, TSNE, UMAP
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, f_classif
from scipy.cluster...",6,"The generated code attempts to address most of the requirements but falls short in several key areas, impacting its correctness, quality, and completeness.

**Correctness:**
*   **Preprocessing:** The handling of 'Year_of_Release' is incorrect (it's dropped in the ground truth, but here all NaNs are dropped). Categorical encoding `df[col].cat.codes` is not ideal for one-hot encoding, which is generally preferred for clustering. `Market_Position` and `Publisher_Portfolio_Size` are calculated correctly but rely on `Total_Sales` which is simple sum, not `Global_Sales` as in requirement/ground truth. `Regional_Sales_Distribution` is also incorrectly calculated as a sum, not proportions.
*   **Optimal Cluster Determination:** The silhouette `silhouette_avg` function for DBSCAN incorrectly returns 0, indicating a lack of understanding of valid metrics for this algorithm (it doesn't inherently produce a fixed number of partitions like k-means). There's no elbow method or gap statistic implemented.
*   **Consensus Clustering:** Explicitly mentioned as a ""placeholder,"" which means it's not implemented, failing a core requirement.
*   **Anomaly Detection:** The `IsolationForest` implementation is correct, but there's no display or further analysis of the anomalies as requested by ""identify unusual games or market patterns.""
*   **Association Rule Mining:** The preprocessing for ARM is problematic. Converting numerical features to categorical by binning them (`pd.cut`) and then building a transaction DataFrame by grouping all listed categorical features to create a pivot table (that pivots on *itself*) is fundamentally incorrect and won't produce meaningful itemsets for market basket analysis. The `apriori` call is also on the wrong structure.
*   **Recommendation System:** The recommendation function is a placeholder (`df[df.index.isin(clusters[game_index])]` doesn't make sense as `clusters` would be labels, not indices). It doesn't actually use the clustering results in a practical way, nor does it properly filter out the input game itself from recommendations.
*   **Validation Metrics:** While internal validation for Silhouette and Davies-Bouldin is attempted, the implementation for DBSCAN is again incorrect (`silhouette_scores[model] = 0`), and for GMM, KMeans, and Agglomerative, it fixes `n_clusters=5` reducing its value for assessing general optimal clusters. External validation is commented out and relies on external ground truth (`y_test`) which was not provided or implied by the task.

**Code Quality:**
*   **Imports:** Mostly correct, but `LabelEncoder` is imported but not used, and `PCA` is imported and not used. `UMAP` is imported from `umap` but then `sklearn.manifold.UMAP` is imported (which doesn't exist, it should be `umap.UMAP`).
*   **Structure:** The code is sectioned well for the different tasks.
*   **Clarity:** Lack of comments for complex sections (e.g., ARM, recommendation system placeholder) makes understanding difficult.
*   **Robustness:** No proper error handling for missing data in the initial stages (just `dropna()` without strategic imputation or feature consideration). The `kmeans` and `gmm` objects are instantiated but not used directly for fitting in the main clustering section; instead, new instances are created later inside functions.

**Completeness:**
*   **Feature Engineering:** 'Regional_Sales_Distribution' is not proportion-based.
*   **Optimal Clusters:** Lacks elbow method and gap statistic.
*   **Consensus Clustering:** Not implemented.
*   **Cluster Profiling:** Limited in its description of statistical significance testing. `describe(include=['object'])` can be misleading if the original categorical codes remain numerical integers.
*   **Anomaly Detection:** Needs more detail in presenting outliers.
*   **Association Rules:** Severely flawed implementation.
*   **Recommendation System:** Incomplete and non-functional.
*   **Validation:** Lacks external validation (as per prompt for ""internal and external validation metrics""), and the internal validation is partially flawed.

Overall, the code demonstrates an understanding of the components required but struggles with accurate and complete implementation of most advanced features, leading to either non-functional sections or incorrect results.",gemma-3-12b-it,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import Decision...",4,"The generated code attempts to address several requirements but fails in implementation details and completeness for many of the advanced features.

**Primary reasons for failure and what was critically missing or incorrect:**

1.  **Missing/Incorrect Imports:** Crucially missing `OneHotEncoder` from `sklearn.preprocessing`, which is used in the Stacking section. `sklearn.voting` is not a standard module; `VotingClassifier` is from `sklearn.ensemble`. `LinearRegression` not used.
2.  **Basic Preprocessing:** The feature engineering portion is very basic (`StandardScaler`, `SimpleImputer`). It completely misses polynomial features, interaction terms, or custom features as required by the prompt's ""comprehensive feature engineering."" The ground truth includes `PolynomialFeatures` and custom ratios.
3.  **Missing Global Configuration:** No `RANDOM_SEED` or `N_SPLITS` configuration at the top, which impacts reproducibility and consistency.
4.  **Ensemble Methods - Incomplete/Incorrect Usage:**
    *   **Voting Classifier:** Correctly attempts hard and soft voting.
    *   **Bagging, Boosting:** Declares the models but doesn't *implement* their training or evaluation explicitly beyond declaration.
    *   **Stacking Ensemble:** This section is severely broken.
        *   `stacking = ColumnTransformer([('onehot', OneHotEncoder(handle_unknown='ignore'), [0,1,2,3,4,5,6])], remainder='passthrough')` makes no sense in the context of stacking. Stacking takes *trained models* (estimators) as its base learners (`level0_estimators`). The `ColumnTransformer` is for feature preprocessing, not for defining a stacking ensemble itself.
        *   There is no `StackingClassifier` class in the generated code at all. The prompt specifically asks for `StackingClassifier`.
        *   The ground truth shows a proper `StackingClassifier` implementation with base estimators and a meta-learner.
5.  **Bayesian Optimization (Optuna):** While Optuna is used, the optimization objective is simple and operates on a single `train_test_split` rather than cross-validation, which is less robust. The ground truth uses it within a pre-split validation set for efficiency during hyperparameter search, but the final model evaluation is done with nested CV.
6.  **Nested Cross-Validation:** The `nested_cv` function provided is conceptually attempting nested CV, but it calls `GridSearchCV.get_params()` which is incorrect syntax. `GridSearchCV` is initialized with the `model` directly. `cv=5` inside `GridSearchCV` seems to be the inner loop, but `model.get_params()` is invalid for a `GridSearchCV` constructor. Furthermore, it explicitly calculates `roc_auc_score` with `predict_proba` which is good, but the overall `nested_cv` function is not called/applied to *all* the ensemble methods, only defined. The ground truth explicitly loops through outer folds and trains/evaluates all specified models and ensembles within each fold.
7.  **Custom Ensemble Methods:** The `custom_ensemble` function calculates weights based on AUC but then applies these weights to `X` (the feature matrix) to compute `ensemble_preds`. This is incorrect. Ensemble predictions are combined from the *predictions* (probabilities) of the base models, not from the raw features. The ground truth correctly averages the *predictions* (probabilities) of base models, weighted by their performance.
8.  **CatBoost for Categorical Features:** The generated code applies `LabelEncoder` to *all* object columns at the very beginning and then uses `SimpleImputer` and `StandardScaler` on the entire dataset. This obliterates any categorical feature information needed for `CatBoost` to leverage. The prompt specifically asked for CatBoost to handle categorical features. The ground truth carefully noted that `cat_features=None` was used because preprocessing had already made all features numerical, acknowledging what *should* be done if true categorical features were present. The generated code's initial preprocessing makes all features numeric globally, defeating `CatBoost`'s specialized capability.
9.  **Model Calibration and Reliability Analysis:** A `CalibratedClassifierCV` is used, but there's no `reliability_curve` definition, and the plot for calibration (reliability diagram) is completely absent. The ground truth plots this.
10. **Ensemble Diversity Analysis/Model Combination Strategies:** These sections are completely missing. There's no correlation matrix of OOF predictions or any form of meta-learning on OOF predictions as in the ground truth.
11. **SHAP Values:** rudimentary implementation, but `explainer=shap.Explainer(best_xgb,X_train)` is potentially problematic for `TreeExplainer` which is more standard for tree-based models and should be `shap.TreeExplainer`. Also, no `summary_plot` type specification (like `bar`) or dependence plots.

In summary, the generated code only partially fulfills a few requirements and largely misses the ""advanced"" and ""comprehensive"" aspects specified, often implementing them incorrectly or in a very basic manner.",gemma-3-12b-it,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import pymc as pm
import scipy.stats as stats
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from lifelines import KaplanMeierFitter
from lifelines.utils import KFoldCrossValidator
import statsmodels.api as sm

# 1. Load and Preproce...",3,"The generated code attempts to address many aspects of the prompt but fails significantly in correctness, completeness, and understanding of the core concepts requested.

**Correctness:**
*   **PyMC3 vs. PyMC4:** The prompt explicitly asks for PyMC3 or PyMC4, but the code uses `pymc` which typically refers to PyMC v5+. This is a version mismatch that would cause errors if PyMC5 isn't installed. The ground truth correctly uses `pymc3`.
*   **Hierarchical Model Implementation:** The ""hierarchical model"" implemented is a simple logistic regression, not a hierarchical model. A hierarchical model requires grouping variables and group-specific parameters from a common hyperprior (e.g., `alpha_group[group_idx]` and `mu_alpha`, `sigma_alpha` as in the ground truth). The generated code completely misses this.
*   **Missing Value Handling:** The prompt specifically mentions ""identify columns where 0 might represent missing values (common in this dataset)"". The generated code uses `fillna()` on `isnull().any()` for numerical columns, which correctly handles actual `NaN` values but completely misses the convention of `0` in certain columns representing missing values in the `diabetes.csv` dataset. This is a critical omission for this specific dataset.
*   **Gaussian Process:** The code uses `sklearn.gaussian_process.GaussianProcessRegressor`. The prompt specifies ""using PyMC3 or PyMC4"" for *all* modeling, including Gaussian Processes. The ground truth correctly implements GP using `pm.gp`. This is a misinterpretation of the tool constraint.
*   **Survival Analysis:** The `KaplanMeierFitter` provides non-parametric survival curves but does not perform ""survival *analysis* techniques treating diabetes onset as time-to-event data"" as in a Cox Proportional Hazards model or parametric survival model, nor does it account for covariates. The prompt implies a regression-based approach. The simulation of `Time` is very simplistic and not tied to risk factors.
*   **Causal Inference:** The Propensity Score Matching implementation is incomplete. It calculates propensity scores but then uses `predict_proba` on `X_train` and matches based on `y_train` as the ""treatment."" In causal inference, the treatment is a *specific variable* (e.g., `High_BMI` in the ground truth), not the outcome variable `y`. This is a fundamental misunderstanding of PSM. It also lacks any form of Instrumental Variables, which was explicitly requested.
*   **Model Comparison:** `pm.model_diagnostics` is not for model comparison; it's for MCMC diagnostics. While `pm.waic` and `pm.loo` are used, there's no comparison between *multiple* models as requested by ""Bayesian model comparison."" The ground truth shows how to compare two models (hierarchical vs. non-hierarchical).
*   **PPC Plotting:** The PPC plot simply compares `y_test` to `ppc['likelihood']`, without mapping the probabilities to observed outcomes for proper comparison of distributions. `matplotlib.pyplot` is imported but not utilized properly for visualization compared to `arviz.plot_ppc`.
*   **Credible Intervals:** `pm.summary` provides summary statistics and credible intervals, but the prompt asks to ""Create credible intervals and posterior predictive checks for model validation,"" implying a visualization or clear demonstration of the intervals.

**Completeness:**
*   **Hierarchical Model:** Not implemented.
*   **Mixed-effects model:** Not implemented (this is essentially part of the hierarchical model request).
*   **Survival Analysis:** Only basic Kaplan-Meier, no Cox model or covariate-based analysis.
*   **Gaussian Processes:** Implemented with `sklearn` not `PyMC`.
*   **Causal Inference:** PSM is incorrectly implemented, and Instrumental Variables are completely missing.
*   **Model Comparison:** Only calculates WAIC/LOO for one model, not comparing.
*   **Informative Priors:** While `mu` and `sigma` values are set, they aren't explicitly described as ""informative based on medical knowledge"" as the ground truth does, which attempts to tie them to typical effect sizes (e.g., higher `mu` for glucose/BMI).
*   **Comprehensive Missing Value Handling:** Fails to address the ""0 as NaN"" issue specific to this dataset.
*   **FamilyHistory Column:** The code imports `diabetes_data` and tries to use `FamilyHistory` as a feature, which does not exist in the `diabetes.csv` dataset. This indicates the code was not tested with the actual dataset or was copied from a different context.

**Code Quality:**
*   **Imports:** Uses `pymc` instead of `pymc3`/`pymc4`. Includes many unused imports (e.g., `scipy.stats`, `KFoldCrossValidator`).
*   **Structure:** Code is structured sequentially but the lack of proper `with pm.Model()` blocks for some attempts (e.g., survival analysis, GP) shows a lack of understanding of PyMC's model context.
*   **Readability:** Variable names are okay but the overall logic flow is disjointed due to the incorrect implementation of core tasks.
*   **Warnings:** No warning suppression, which is crucial for PyMC3/ArviZ usage.
*   **Output:** Very minimal print statements, making it hard to follow progress or interpret results.
*   **Seed:** `np.random.seed(42)` is used only for survival simulation and not for PyMC sampling, which is crucial for reproducibility of Bayesian models.",gemma-3-12b-it,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture, SpectralClustering
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.manifold import TSNE, UMAP
from sklearn.neural_network import Autoencoder
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.cluster import OPTICS
from scipy.stats import gap_statistic
impo...",3,"The generated code attempts to address several requirements but falls significantly short in implementation and correctness.

**Primary Reasons for Failure & Critical Missing/Incorrect Aspects:**

1.  **Fundamental Library Misuses and Errors:**
    *   `from sklearn.neural_network import Autoencoder`: `sklearn` does not have a built-in `Autoencoder` class. This leads to an `ImportError`. The Ground Truth correctly uses `tensorflow.keras`.
    *   `gap_statistic` import: `scipy.stats` does not directly provide a `gap_statistic` function. This would lead to an `ImportError`.
    *   `apriori(data, ...)`: The `mlxtend.frequent_patterns.apriori` function expects a DataFrame where each row is a transaction and columns are items (binary values indicating presence). The generated code passes the raw `data` DataFrame, which is not suitable, and will likely result in an error or unexpected output due to data types and non-binary values. The Ground Truth correctly preprocesses data by discretizing and one-hot encoding.

2.  **Incomplete and Incorrect Feature Engineering:**
    *   `data['BMI'] = data['BodyMassIndex']`: The `diabetes.csv` typically has a column named 'BMI', not 'BodyMassIndex'. This line would create a new 'BMI' column from a non-existent one, propagating NaNs or errors.
    *   **Metabolic Syndrome Indicators:** The generated code assumes columns like `HighBP`, `HighChol`, `Stroke`, `HeartDisease` exist directly in `diabetes.csv`. These features are NOT present in the standard Pima Indian Diabetes dataset. The Ground Truth correctly engineers these indicators from `Glucose`, `BloodPressure`, `BMI` based on commonly understood definitions, albeit simplified. This makes the feature engineering entirely incorrect for the given `diabetes.csv`.
    *   **Missing Value Handling:** The generated code completely ignores handling missing values (zeros in 'Glucose', 'BMI', etc.), which is crucial for this dataset. The Ground Truth correctly addresses this with `SimpleImputer`.

3.  **Lack of Parameter Optimization Implementation:**
    *   The `optimize_parameters` function for `kmeans` only calculates silhouette scores but doesn't implement the elbow method or a robust way to *select* the optimal K, it simply returns `np.argmax(silhouette_scores) + 2`, which is a naive approach and doesn't consider the full range or the elbow point.
    *   For other algorithms like DBSCAN, Hierarchical, GMM, Spectral, and OPTICS, the `optimize_parameters` function is a placeholder (`best_n_clusters = 5 #Default value. Needs customization`). This completely fails the requirement for ""optimal clustering parameters using silhouette analysis, gap statistic, and elbow method with statistical validation."" The Ground Truth implements these specific methods (BIC/AIC for GMM, T-SNE/UMAP for visualization, and proper silhouette analysis for others).

4.  **Consensus Clustering & Anomaly Detection:**
    *   These crucial sections are explicitly marked as ""omitted for brevity"" or ""Requires running multiple clustering algorithms and averaging results, omitted for brevity."" This directly violates the completeness requirement. The Ground Truth provides a robust implementation for consensus clustering.

5.  **Time-Series Clustering:**
    *   Marked as ""omitted for brevity"" and correctly notes it's not applicable to the dataset, but the task asked for it if temporal patterns *existed*. The Ground Truth provides a detailed conceptual outline fulfilling the spirit of the requirement.

6.  **Patient Phenotyping and Risk Stratification:**
    *   `create_phenotype_profiles`: Only calculates mean values. It lacks statistical significance testing (ANOVA/Kruskal-Wallis, Chi-squared) between clusters, which is a core part of ""detailed patient phenotype profiles for each cluster with statistical significance testing.""
    *   Risk stratification is also marked as ""omitted for brevity.""

7.  **Code Quality Issues:**
    *   No proper plots or visualizations for the dimensionality reduction, clustering results, or anomaly detection, despite the task explicitly mentioning visualization (`for visualization`).
    *   Lack of clear reporting messages or print statements explaining progress, which the Ground Truth uses extensively.
    *   `UMAP` in `sklearn.manifold` is incorrect, it should be `umap.UMAP` from `umap-learn`.
    *   No error handling for `silhouette_score` if only one cluster is found (e.g., DBSCAN returning all noise).

In summary, the generated code has severe functional errors due to incorrect library usage, makes wrong assumptions about the input data, and omits significant portions of the required functionality, making it largely non-executable and incomplete for the given task.",gemma-3-12b-it,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import VotingRegressor, BaggingRegressor, RandomForestRegressor, ExtraTreesRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
f...",3,"The generated code attempts to address various requirements but falls short in correctness, completeness, and code quality significantly.

**Primary Reasons for Failure & Critical Missing/Incorrect Aspects:**

*   **Data Loading and Preprocessing:**
    *   **Incorrect Column Names:** The code assumes 'Country', 'Year', 'Happiness.Rank', 'Happiness.Score', and 'Life.Ladder' column names in the `preprocess_data` function. The actual CSV (`world-happiness-report-2021.csv`) has different column names like 'Country name', 'Ladder score', etc. This leads to immediate `KeyError` at runtime.
    *   **Missing Features:** It completely ignores categorical features like 'Regional indicator' which is crucial for comprehensive preprocessing and one-hot encoding.
    *   **Inadequate Preprocessing Pipeline:** It lacks a proper `ColumnTransformer` and `Pipeline` for handling numerical and categorical features consistently, which is a standard and robust approach in scikit-learn. The ground truth uses this effectively.
    *   **Poor Handling of Missing Values:** `data.fillna(data.mean())` is a simplistic approach and doesn't differentiate between numerical and categorical columns.

*   **Ensemble Methods - Stacking:**
    *   **Incorrect `StackingClassifier`:** The generated `StackingClassifier` is fundamentally flawed. It's named `StackingClassifier` but is built for regression. More importantly, it fits base estimators on the *entire* training data and uses their predictions on the *same* training data to train the meta-learner (`self.base_predictions[:, i] = estimator.fit(X, y).predict(X)`). This is a critical data leakage issue that negates the purpose of stacking, which requires out-of-fold predictions. `sklearn.ensemble.StackingRegressor` handles this correctly using CV. It also completely misses addressing `sklearn.ensemble.StackingRegressor`, which is the intended method.

*   **Bayesian Optimization (Optuna):**
    *   **Flawed Objective Function:** The `create_model_and_evaluate` function for Optuna tries to dynamically check for parameters like `if 'n_estimators' in trial.suggest_int:`, which is syntactically and logically incorrect. `trial.suggest_int` is a function call, not an object to be checked for members. It also tries to set parameters on a generic `model` object passed in, without knowing its type, which is prone to errors.
    *   **Hardcoded Metric:** The `target` argument in `optimize_hyperparameters` is unused in `create_model_and_evaluate` which re-hardcodes `neg_mean_squared_error`.
    *   **Trial Suggestion Errors:** `trial.suggest_int('n_estimators', 10, 500)` will be problematic within the `if 'n_estimators' in trial.suggest_int:` block.

*   **Nested Cross-Validation:**
    *   **Completely Missing:** The requirement was for *nested* cross-validation for robust evaluation. The generated code only shows `evaluate_model` which performs simple K-Fold cross-validation, but crucially, it does *not* do the outer/inner loop split and hyperparameter tuning within the inner loop, which is the core of nested CV.

*   **Custom Ensemble with Dynamic Weight Assignment:**
    *   **Theoretical Flaws:** The concept of dynamically assigning weights is attempted, but it assumes fixed weights are provided during initialization. A robust dynamic weighting scheme would typically involve calculating weights *based on performance* (e.g., inverse error) on validation or OOF predictions, then applying those. The current `create_custom_ensemble` takes predefined `weights`.
    *   **Missing OOF:** Crucially, for proper dynamic weighting, OOF predictions are needed to prevent leakage (as demonstrated in the ground truth). This is absent.

*   **Advanced Techniques (Blending, Multi-level Stacking):**
    *   **Not Implemented:** The code explicitly states ""Blending and Multi-Level Stacking (Simplified)"" but then provides no actual implementation, only a comment. This is a critical omission.

*   **Model Interpretation (SHAP, LIME):**
    *   **SHAP Implementation:** `shap.Explainer(model, X)` is generally used for tree models where `model` is the actual estimator. Here it's passed a *pipeline* or wrapper which might not work directly without extracting the underlying `xgb` model. The `explainer(X)` call is also incorrect; it should be `explainer.shap_values(X_sample)`.
    *   **LIME Implementation:** The `explain_model_lime` function has issues:
        *   `X_lime[0]` when `X_lime` is a DataFrame or Series is problematic without `iloc` or `loc`.
        *   The `predict` function passed to LIME must handle pre-processing. The current `model.predict` would likely fail because `model` is the raw estimator, not the pipeline that includes preprocessing.

*   **Model Calibration and Uncertainty Quantification:**
    *   **Misuse of `CalibratedRegressor`:** `CalibratedRegressor` is primarily for *classification* where it calibrates probability predictions. For regression, while conceptual calibration exists via techniques like isotonic regression on residuals or conformal prediction, `CalibratedRegressor` itself is not the standard tool for regression uncertainty. What's required is typically prediction intervals (e.g., via quantile regression or bootstrapping) and residual analysis for calibration.

*   **Ensemble Diversity Analysis and Model Combination Optimization:**
    *   **Incomplete Diversity Analysis:** `calculate_diversity(predictions)` simply returns variance, which is a very basic measure. Diversity analysis usually involves correlation of predictions or error consistency.
    *   **Missing Optimization:** The code provides no actual implementation for optimizing weights for model combination, only the `calculate_diversity` function.

*   **Code Quality and Structure:**
    *   **Imports:** `from sklearn.ensemble import ... XGBRegressor, LGBMRegressor, CatBoostRegressor` is an error in imports as XGBoost, LightGBM, and CatBoost are external libraries, not part of `sklearn.ensemble`.
    *   **Global Variables & Poor Class Design:** The preprocessing and ensemble functions often take `X`, `y` directly, losing the pipeline benefit. The `StackingClassifier` is an unnecessary and incorrect custom class instead of using `sklearn.ensemble.StackingRegressor`.
    *   **Lack of Pipelines:** Without proper `sklearn.pipeline.Pipeline` objects (especially enclosing the preprocessor), many subsequent steps (like Optuna, nested CV, SHAP/LIME, blending) become extremely difficult or impossible to implement correctly because the data needs to be preprocessed consistently before feeding to models. The groundbreaking `Pipeline` and `ColumnTransformer` from `sklearn` were not used.
    *   **Hardcoding `random_state`:** While using `random_state` is good for reproducibility, it's hardcoded many times, leading to messy code.
    *   **Unclear Flow:** The ""Example Usage"" section just calls functions and prints scores, but a complete, runnable workflow for evaluating and interpreting the models is missing.

In summary, the generated code demonstrates a superficial understanding of the requirements and struggles with fundamental concepts like data leakage, proper scikit-learn pipeline usage, and the specific implementations of advanced techniques (nested CV, correct stacking, regression calibration). Many parts of the code would fail to run or produce incorrect results due to conceptual errors and incorrect API usage.",gemma-3-12b-it,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import statsmodels.api as sm
import scipy.stats as stats
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
import scikit_posthocs as sp
import random
import math

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Preprocessing
df = df.rename(columns={'Country name': 'Country'})
numeric_cols = ['GDP per capita', 'Social support', 'Healthy life ex...",4,"The generated code attempts to address several aspects of the prompt but falls significantly short in correctness, completeness, and adherence to specified libraries and advanced statistical methods.

**Correctness:**
*   **PyMC3 Usage:** The Hierarchical Bayesian Model implementation is conceptually flawed with incorrect prior definitions (e.g., `region_effects` defined but not used in `mu`, then `region_prior` defined separately; `target_variable` is assumed to be `Happiness Score` but the `df` is loaded as `world-happiness-report-2021.csv` which doesn't have this column by default). The use of `pm.Normal('mu', mu=0, sigma=1)` and `pm.Normal('happiness', mu=beta_gdp * gdp + beta_social * social, ...)` in the informative priors model is confusing and seems to redefine `mu` incorrectly.
*   **SEM:** The SEM implementation is a simple OLS regression (`sm.OLS`). This is *not* a Structural Equation Model. SEM involves defining latent variables, direct and indirect effects, and typically uses libraries designed for SEM (like `lavaan` in R, or `semopy`/`statsmodels.formula.api` with custom path specification, but usually not just OLS). The prompt explicitly asked for *building* SEM, not just running linear regressions.
*   **Causal Inference (IV & PSM):** These sections are predominantly placeholders, explicitly stating that a full implementation is missing or not readily available. This directly fails a core requirement. The prompt specifies identifying causal effects of GDP on happiness, implying these methods should be implemented, not just noted.
*   **Gaussian Processes:** This section is also a placeholder. It imports `GaussianProcessRegressor` but then comments out the actual implementation, effectively failing to address this requirement. The prompt required *modeling* with GPs, not just importing or noting the class.
*   **Mediation Analysis:** This is likewise a placeholder, stating what is needed but not implementing it. The prompt specifically asked for mediation analysis *to understand how social factors mediate the GDP-happiness relationship*.
*   **Missing Value Handling:** The preprocessing only handles `fillna` with mean for numeric and 'Unknown' for categorical. The prompt asked for *comprehensive* missing value handling. The ground truth, for instance, uses `SimpleImputer` and `StandardScaler` within a `ColumnTransformer` for a more robust approach.
*   **Priors:** While `pm.Normal` is used for priors, the *informative* aspect based on happiness research literature is largely absent, with many priors set to `mu=0, sigma=1`, which are relatively uninformative. The prompt asked for ""informative priors based on happiness research literature"".
*   **Difference-in-Differences:** The simulation of the time component and treatment is very simplistic and doesn't clearly map to a robust DiD setup specified by academic literature (e.g., `treated` and `time` variables should typically be fixed for each ""country"" and ""time period"" within a panel, and their interaction capture the effect). The provided code uses a non-persistent `Year` and `Treatment` which may not create a true panel structure as intended.

**Completeness:**
*   **Comprehensive Missing Value Handling:** Not adequately done.
*   **Hierarchical Bayesian Models:** Attempted, but with correctness issues.
*   **Structural Equation Models:** Not implemented.
*   **Causal Inference (IV, PSM):** Not implemented.
*   **Bayesian Regression with Informative Priors:** Attempted, but informative aspect is weak.
*   **Gaussian Processes:** Not implemented.
*   **Mediation Analysis:** Not implemented.
*   **Difference-in-Differences:** Simplistic and likely incorrect implementation.
*   **Bayesian Model Comparison (WAIC, LOO-CV, Bayes Factors):** Placeholder. `pymc3` has direct support for WAIC/LOO, but not implemented.
*   **Credible Intervals, Posterior Predictive Checks, Sensitivity Analysis:** Credible intervals are partly shown by `pm.plot_posterior`, but comprehensive ArviZ `az.summary` with HDIs and `az.plot_ppc` (which the ground truth uses via `az.from_pymc3`) are missing. Sensitivity analysis is a placeholder.

**Code Quality:**
*   **Imports:** Includes unnecessary imports like `scikit_posthocs` and `random`, `math` that are not used comprehensively. Some crucial imports like `arviz` are missing.
*   **Structure:** The flow is somewhat logical based on the prompt's points, but many sections are incomplete placeholders.
*   **Variable Naming:** `target_variable` is defined as 'Happiness Score', but the CSV has 'Ladder score'. This indicates a potential runtime error if the code were run without modification.
*   **PyMC3 `pm.sample` usage:** Lacks `return_inferencedata=True` leading to older trace format, which is not ideal for `ArviZ` usage.
*   **Lack of Explanations:** The request was ""Provide only the Python code without explanations"" for the output, but the ground truth still includes print statements and comments for clarity within the code, which is good practice. The generated code has minimal comments.

Overall, the code demonstrates a superficial understanding of the requirements, implementing only the most basic components or leaving significant portions as placeholders. It fails to leverage advanced PyMC3 and ArviZ features, and misinterprets complex statistical methods like SEM and DiD.",gemma-3-12b-it,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture, SpectralClustering, HDBSCAN
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.manifold import TSNE, LocallyLinearEmbedding, Isomap
from sklearn.neighbors import NearestNeighbors
fr...",3,"The generated code attempts to address several requirements but fails significantly in correctness, completeness, and code quality, especially when compared to the ground truth.

**Correctness and Completeness Failures:**

1.  **Missing Imports and Incorrect Usage:**
    *   `umap` is used without importing it (only `import umap` not `from umap import UMAP`).
    *   `Autoencoder` is imported from `sklearn.autoencoders` which does not exist. The task specified `autoencoders` for dimensionality reduction, implying a neural network implementation (like in the ground truth using Keras/TensorFlow), which is completely missed in the implementation.
    *   `keras` is imported directly, but the autoencoder part is not correctly built using it.
    *   `sklearn.networkx` is imported, but `from_numpy_array` should be `nx.from_numpy_array` after `import networkx as nx`.

2.  **Preprocessing and Feature Engineering:**
    *   `data.dropna()` is too aggressive. The ground truth shows more robust imputation strategies (median for numerical, mode for categorical).
    *   The `Happiness_Profile` feature is defined as a sum of scaled current happiness factors, which is redundant and doesn't represent an ""advanced feature engineering"" of a profile. It should be a new perspective, not just a sum of scaled inputs. The ground truth correctly uses interaction terms and ratios.
    *   Regional characteristics are partially done by one-hot encoding, but the ground truth also includes numerical features derived from regions.

3.  **Dimensionality Reduction:**
    *   UMAP is missing a proper import.
    *   Autoencoder implementation is completely absent/incorrect. It's listed in imports but not used correctly. The ground truth correctly builds and trains an autoencoder using Keras.
    *   Manifold learning (Isomap) is missing.

4.  **Clustering Algorithms and Parameter Optimization:**
    *   The `perform_clustering` function's `model.labels_` assumes all models have this attribute. GMMs have `predict()`, DBSCAN/HDBSCAN sometimes have `labels_` but their parameters are critical. Spectral Clustering's `labels_` might not be directly `fit_predict`.
    *   Parameter optimization is very basic (only silhouette score) and doesn't implement the elbow method, gap statistic, or stability analysis asked for. The implemented `optimize_parameters` function for finding the best `k` is not actually called for any of the clustering algorithms. Instead, fixed `n_clusters=6` is used for K-Means, Agglomerative, GMM, Spectral.
    *   DBSCAN and HDBSCAN parameters are hardcoded and not optimized based on analysis.
    *   The notion of ""optimal clustering parameters"" is largely ignored, and the code lacks analytical justification (e.g., plots for Elbow/Silhouette).

5.  **Consensus Clustering:**
    *   The `consensus_clustering` function is fundamentally flawed. It tries to average cluster assignments directly, which is meaningless across different algorithms that might produce different numbers of clusters or entirely different groupings. It doesn't use a co-association matrix approach, which is the standard for robust consensus clustering and is implemented in the ground truth. The current `consensus_labels` becomes `np.array(consensus_labels)` which is 6 lists of 6 numbers after the loops. This is not assigning a consolidated label to each data point.

6.  **Country Profiles:**
    *   `country_profiles` function only prints means and sizes. It completely lacks ""statistical significance testing and effect size analysis"" which is a major requirement and is fully implemented in the ground truth using ANOVA and Eta-squared.

7.  **Anomaly Detection:**
    *   Only Isolation Forest is used. LOF and One-Class SVM are missing. The ground truth implements all three.

8.  **Network Analysis:**
    *   `np.corrcoef(data[numerical_features].T)` calculates correlations between features, not between countries based on happiness factors as required. The task asks for understanding relationships *between countries*. The ground truth correctly calculates `cosine_similarity(X)` between countries (rows), not features (columns).
    *   The generated network graph simply draws the correlation matrix of features, which is not what was asked.

9.  **Association Rule Mining:**
    *   The input `df` to `apriori` is `uniques = data[['Region', 'Happiness_Profile']].unique()`. This essentially creates a DataFrame of unique combinations of 'Region' and the (redundant) 'Happiness_Profile' for only 19 distinct rows if all combinations are unique. This is not performing ARM on the *characteristics* of countries. ARM should be applied to binned continuous features, similar to the ground truth's approach using `pd.qcut` and `get_dummies` on all relevant numerical features.

10. **Recommendation System and Policy Insights:**
    *   The recommendation system is highly oversimplified (`similar_countries = data[labels == country_cluster]`). It recommends based *only* on the consensus cluster, without any finer-grained similarity within the cluster based on features, which is crucial for a recommendation system. The ground truth offers a more advanced similarity-based recommendation within clusters.
    *   Policy insights are completely absent beyond placeholder comments. The ground truth provides detailed qualitative insights derived from cluster profiles.
    *   Geopolitical validation is missing.

**Code Quality Issues:**

*   Lack of modularity: The code is largely a top-down script, whereas the ground truth uses a class structure for better organization and reusability.
*   Hardcoded values: For clustering algorithms (e.g., `n_clusters=6`, `eps=0.5`), parameters are hardcoded without optimization or justification.
*   Clarity: Variables like `X` are reused without clear context (e.g., PCA, t-SNE variables are named `X_tsne`, but the original `X` is again used for clustering without specification if it's the raw or reduced `X`).
*   Error handling: No `try-except` blocks for resilience against data issues or algorithm failures.
*   Verbosity: Missing print statements and visual cues to signify progress or output, unlike the ground truth.
*   Comments: Minimal comments, making it harder to understand the intent compared to the ground truth.

In summary, the generated code falls short in nearly every advanced technique requested. It provides basic implementations that misinterpret the requirements or fail to execute correctly, leading to incorrect or meaningless results for consensus clustering, network analysis, and association rule mining. The lack of detailed parameter optimization, statistical testing, and meaningful policy insights further diminishes its value.",gemma-3-12b-it,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX
import pmdarima as pm
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Load data
data = pd.read_csv('synthetic_covid19_data.csv')

# Parse date and set as index
data['date'] = pd.to_datetime(data['date'])
data = data.set_index('date')

# Filter for Germany
germany_data = data[data['location'] == 'Germany']

# Target and exogenous variables
target = germany_data['new_cases']
exoge...",7,"The generated code correctly implements the core SARIMA forecasting task, including data loading, filtering, auto_arima for order selection, model training, forecasting, and RMSE evaluation, along with plotting. This covers most of the requirements.

However, there are several areas for improvement:

1.  **Missing Value Handling:** The generated code uses `fillna(target.mean())` and `fillna(exogenous.mean())`. While this handles NaNs, it's generally not the best approach for time series, especially for continuous variables like `stringency_index` and `people_fully_vaccinated`, or for `new_cases` where a mean might not be representative of a specific point in time. Forward-fill (`ffill`) and then backward-fill (`bfill`) are typically preferred for time series data to preserve the temporal order and information, as demonstrated in the ground truth. Also, applying `mean()` to the *entire* target/exogenous series before splitting can lead to data leakage (using future information for imputation in the training set).
2.  **Robustness and Error Handling:** The generated code lacks error handling for file not found, or checks for insufficient data length for the test set, which makes it less robust. The ground truth includes these important checks.
3.  **Data Splitting Logic:** The generated code performs `germany_data[:-30]` and `germany_data[-30:]`, then extracts `new_cases` and exogenous variables from these *sub-dataframes*. While functional, it's slightly less efficient and clean than splitting the pre-selected `y` and `X` variables directly. Also, the `model.fit(disp=False)` is used twice.
4.  **`enforce_stationarity` and `enforce_invertibility`:** The generated code sets `enforce_stationarity=True, enforce_invertibility=True`. For SARIMA models, especially when `d` or `D` are non-zero (as determined by `auto_arima`), these parameters often need to be set to `False` to prevent warnings or errors from `statsmodels` regarding stationarity/invertibility assumptions, as the differencing effectively handles stationarity. The ground truth correctly notes this potential issue.
5.  **Negative Forecasts:** The generated code does not handle negative forecasts for `new_cases`, which is illogical in a real-world scenario. The ground truth sets negative forecasts to 0.
6.  **Minor Code Quality/Clarity:** The `model` variable is reused for both `pm.auto_arima` and `SARIMAX` initialization, which might be slightly confusing, though it works. Also, `rmse = (mean_squared_error(test_target, forecast, squared=False))**0.5` is redundant as setting `squared=False` directly calculates RMSE.
7.  **Plotting:** The plot is basic and doesn't include the training data, which helps provide context for the forecast. The title needs to include the RMSE.",gemma-3-12b-it,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scikit_posthocs import posthoc_dunn

# Load the dataset
try:
    df = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: winequality-red.csv not found in the current directory."")
    exit()

# 1. Define Quality Categories
df['quality_group'] = pd.cut(df['quality'], bins=[0, 4, 6, d...",6,"The generated code attempts to address all aspects of the task, but has significant issues in its statistical methodology and robustness.

**Correctness:**
- **Quality Grouping:** Correctly categorizes wines into low, medium, and high quality based on the given score ranges using `pd.cut`.
- **Statistical Test Selection (Partial Failure):** The logic for choosing between ANOVA and Kruskal-Wallis is flawed. It only checks for normality and then directly applies ANOVA if normal, or Kruskal-Wallis if not. It **misses the crucial check for homogeneity of variances for ANOVA**. Even if data is normal, if variances are not equal, ANOVA assumptions are violated, and a non-parametric test or Welch's ANOVA (not implemented here) should be considered.
- **Post-hoc Tests (Major Failure):**
    - The post-hoc selection logic is incorrect. It re-runs `f_oneway` and then applies Tukey if the *p-value* of that re-run is less than 0.05. This doesn't correctly link the post-hoc test to the *initial* primary test performed (ANOVA or Kruskal-Wallis).
    - If the initial primary test was Kruskal-Wallis, it should unequivocally use Dunn's test. The code uses Dunn's test if `f_oneway`'s p-value is *not* less than 0.05, which is a faulty conditional.
    - `posthoc_dunn` requires `scikit_posthocs` which is a valid import but not a standard one like in the ground truth. This dependency should ideally be mentioned or justified.
    - `pairwise_tukeyhsd` needs the `groups` argument to be the actual group labels, not the entire DataFrame column. This would lead to incorrect results or errors.
- **Error Handling:** Has basic `FileNotFoundError`, but lacks robustness for empty data in groups or insufficient data for statistical tests (e.g., Shapiro-Wilk needing at least 3 points, not just `ValueError` catching).

**Code Quality:**
- **Imports:** Correctly imports necessary libraries. `scikit_posthocs` is an external dependency not part of `scipy.stats` or `statsmodels`.
- **Structure:** The code is logically structured into sections (quality grouping, statistical tests, post-hoc, visualization, summary), which is good.
- **Readability:** Generally readable. Variable names are appropriate.
- **Magic Numbers:** Uses `0.05` directly in conditions, which is acceptable but could be defined as a constant at the top for better maintainability (like `ALPHA` in ground truth).

**Completeness:**
- **Quality Grouping:** Achieved.
- **Statistical Tests:** Performed, but decision logic is incomplete (missing variance homogeneity check).
- **Post-hoc Tests:** Attempted, but the conditional logic for selection and the application for Tukey's HSD are incorrect.
- **Visualizations:** Generates box plots correctly for significant features.
- **Summary:** Provides a list of significant features. However, it does not summarize the post-hoc results in the summary as thoroughly as the ground truth.

**Specific improvements:**
1.  **Homogeneity of Variance:** Before performing ANOVA, add Levene's test to check for equality of variances. If assumptions for ANOVA (normality AND homogeneity) are not met, then proceed with Kruskal-Wallis.
2.  **Robust Statistical Test Selection:** The `if/else` block for normality and test selection needs to correctly store *which* test was applied (ANOVA or Kruskal-Wallis) so that the post-hoc steps can refer back to it, rather than re-running `f_oneway` to decide.
3.  **Correct Tukey's HSD Usage:** `pairwise_tukeyhsd` expects the data and group labels as separate arrays, not the entire DataFrame columns directly as done in the generated code.
4.  **Correct Post-hoc Logic:** The post-hoc loop should explicitly check if the primary test for `feature` was ANOVA (then use Tukey) or Kruskal-Wallis (then use Dunn). The current logic based on re-running `f_oneway` is severely flawed.
5.  **Handling Edge Cases:** Add checks for groups having insufficient data for statistical tests (e.g., Shapiro-Wilk requires >2 samples, Levene's and ANOVA similar). The `try-except ValueError` for Shapiro-Wilk is a start but not comprehensive.
6.  **Summary Detail:** Enhance the summary to clearly list the primary test and post-hoc results for each significant feature, rather than just listing the features.",gemma-3-12b-it,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import shap
import datetime

# Load data
df = pd.read_csv('synthetic_covid19_data.csv')

# Parse date
df['date'] = pd.to_datetime(df['date'])

# Define target variable
def create_target(df):
    df['new_deaths_sorted'] = df.groupby('location')['new_deaths'].apply(lambda x: x[x > 0].sort_values(asce...",4,"The generated code has several critical issues that make it largely incorrect or incomplete for the given task.

**Major Issues:**

1.  **Incorrect Target Variable Definition:** This is the most significant flaw.
    *   The prompt states: ""A 'high_daily_death_increase' is defined as a day where 'new_deaths' for a 'location' is in the top 10% of its own historical 'new_deaths' (excluding days with zero new deaths).""
    *   The generated code's `create_target` function is fundamentally flawed.
        *   `df.groupby('location')['new_deaths'].apply(lambda x: x[x > 0].sort_values(ascending=False))` attempts to sort `new_deaths` for each location _within the apply, creating a Series for each row's `new_deaths_sorted` column, which is an incorrect data structure and approach.
        *   `row['new_deaths_sorted'].iloc[-int(len(row['new_deaths_sorted']) * 0.1)]` tries to get the value at the 90th percentile, but `iloc[-int(len(...) * 0.1)]` for a sorted *descending* list of non-zero values would actually get the 90th *smallest* value among the non-zero deaths (i.e. the 10th percentile, or even incorrectly index an empty series if `new_deaths_sorted` is empty). It should be `quantile(0.90)`.
        *   The comparison `... <= row['new_deaths']` is also backward. It should be `current_new_deaths > 90th_percentile_threshold`.
    *   This incorrect target definition makes the entire subsequent modeling incorrect.

2.  **Missing Value Handling (Feature Engineering):**
    *   The prompt explicitly states: ""Handle missing values from feature engineering (e.g., fill with 0 or backfill)"".
    *   The generated code uses `.fillna(0)` within the rolling/lag calculations. While this fills NaNs caused by the rolling window not having enough data (e.g., first few days), it doesn't account for *actual* NaNs in the original `new_cases`, `hosp_patients`, `icu_patients` columns that are then used in the rolling calculations. The ground truth handles this more robustly by filling *after* transformation (e.g., ffill then fillna for numerical NaNs). The prompt also suggests backfill, which is not used by the generated code as an option.

3.  **Missing Value Handling (Original Features):**
    *   The `people_fully_vaccinated` is divided by `df['population'].mean()`, which is incorrect. It should be `df['people_fully_vaccinated'] / df['population']` (per row) and then handle potential NaNs or division by zero. The generated code calculates a single mean for the entire population and attempts to use it as a divisor for a DataFrame column, which will likely result in an error or incorrect broadcast.
    *   Other core features like `stringency_index`, `median_age`, etc., are not explicitly checked for NaNs and filled, which could lead to model errors if they contain NaNs later in the pipeline that are not handled by `StandardScaler` (though sometimes `fit_transform` can implicitly handle NaNs, it's not good practice).

4.  **Feature Selection Error:**
    *   The line `'people_fully_vaccinated' / df['population'].mean()` within the `features` list is syntactically incorrect for selecting a feature or creating a derived feature at that stage. This would cause an error. It should be a column name string, and the ratio should be computed earlier as a new column, as done in the ground truth.

5.  **Preprocessing (OHE and Scaling):**
    *   The code applies `pd.get_dummies` directly, which means the training and testing sets will be one-hot encoded *separately* instead of fitting the encoder on training and transforming both. This is a common data leakage mistake if not handled properly (e.g., using `ColumnTransformer` or manually fitting OHE), and can lead to inconsistent columns between train/test sets if categories differ.
    *   `StandardScaler` is applied to `X`, which contains both numerical and already one-hot encoded features (if `pd.get_dummies` was applied correctly before `X` was defined). One-hot encoded features should not be scaled. This requires `ColumnTransformer` for proper preprocessing.

6.  **SHAP Feature Importance:**
    *   `shap_values.summary_values(features)` and `shap.summary_plot(shap_values, X_train, features=top_features)` are incorrect ways to use SHAP with `explainer(X_train)` returning `Explanation` objects. The `features` argument in `summary_plot` expects an array-like or list of feature names, not the SHAP `Explanation` object. Also, accessing `summary_values` on the `Explanation` object directly isn't the standard way to get global importance for `summary_plot`. One would typically pass `shap_values` and `X_train` to `summary_plot` directly, and it infers features. If feature names are needed, they should be extracted from the preprocessed data, which is not done correctly here due to the lack of `ColumnTransformer`.
    *   `top_features` extraction `shap_summary_values.index.values[:10]` is also not how SHAP values or the summary plot are typically used. The plot automatically handles top features.

**Minor Issues / Areas for Improvement:**

1.  **Code Structure and Readability:** Functions like `create_target` and `feature_engineering` are good, but they modify the global `df` implicitly, which can lead to confusion. Passing `df` and returning `df` explicitly is better.
2.  **Imports:** `sklearn.metrics` is not imported, but `metrics.roc_auc_score` and `metrics.f1_score` are used, which will cause a `NameError`.
3.  **TimeSeriesSplit Usage:** While `TimeSeriesSplit` is used in `GridSearchCV`, a simple `next(tscv.split(X_scaled))` for the train/test split is not robust. A robust time-based split for the final evaluation involves sorting by date and taking the last N% of data, as done in the ground truth. `TimeSeriesSplit` is primarily for cross-validation on the training set.
4.  **XGBoost `eval_metric`:** Not set, but it's good practice to set it along with `use_label_encoder=False` for silent warnings and better performance/logging.
5.  **Lack of `Pipeline` and `ColumnTransformer`:** This is a major structural weakness leading to the preprocessing issues mentioned above. The prompt explicitly asks for scaling and one-hot encoding, and a `Pipeline` with `ColumnTransformer` is the standard and correct way to manage this.

In summary, the target variable definition is critically flawed, feature engineering is incomplete with respect to missing values, feature selection has a syntax error, preprocessing is incorrect, and the SHAP usage is not standard or complete.",gemma-3-12b-it,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import statsmodels.formula.api as smf
import statsmodels.api as sm
import numpy as np

# Load the data
try:
    data = pd.read_csv(""synthetic_covid19_data.csv"")
except FileNotFoundError:
    print(""Error: synthetic_covid19_data.csv not found. Please ensure the file is in the same directory as the script."")
    exit()


# Parse the date column
data['date'] = pd.to_datetime(data['date'])

# Calculate vaccination percentage
data['vaccination_percentage'] = data['people...",4,"The generated code attempts to address the task but fundamentally misunderstands key aspects of mixed-effects modeling and robust data handling.

**Primary Reasons for Failure/Critical Missing:**

1.  **Incorrect Mixed-Effects Model Specification (`groups` and `formula`):**
    *   **Random Effects Structure:** The prompt clearly asked for ""random intercepts for 'location' nested within 'continent'"". The generated code uses `groups=data[""continent""], random_effects=""~1""`. This only accounts for *random intercepts for continent*, not *location nested within continent*. To achieve nested random effects using `statsmodels.formula.api.mixedlm`, one typically creates a unique identifier for each `(continent, location)` combination and uses that as the `groups` argument, with `re_formula=""1""`. The ground truth correctly implements `df['continent_location_id']` for this purpose.
    *   **Fixed Effects (`C(location) + C(continent)`):** The formula `reproduction_rate ~ stringency_index + vaccination_percentage + stringency_index:vaccination_percentage + C(location) + C(continent)` includes `C(location)` and `C(continent)`. This treats 'location' and 'continent' as *fixed effects*, essentially adding dummy variables for every location and continent. This is incorrect for a mixed-effects model where variability **across** locations/continents is to be modeled as random effects, not by explicitly parameterizing each level as a fixed effect. This approach would also lead to ""many parameters"" issues if there are many locations. The core idea of random effects is to pool information and estimate variances of these random effects, not their individual intercepts.

2.  **Inadequate Missing Value Handling:**
    *   The `vaccination_percentage` is calculated **before** handling missing `population` and `people_fully_vaccinated`. If `population` or `people_fully_vaccinated` are NaN, `vaccination_percentage` will be NaN, potentially leading to immediate issues or inaccurate calculations. The ground truth first imputes these components before calculating the percentage.
    *   The imputation strategy is inconsistent and potentially problematic: ""Impute reproduction_rate using median grouped by location *before* interpolation"" and then ""Interpolate missing values for stringency_index and vaccination_percentage using linear interpolation"".
        *   For `reproduction_rate`, median imputation before any interpolation is not ideal for time-series data where linear interpolation (as used in the ground truth) is generally preferred to maintain temporal trends.
        *   The linear interpolation `limit_direction` and `limit_area` are not specified, meaning it might not fill NaNs at the beginning or end of groups, as noted in the ground truth.
    *   A blanket `data.dropna()` after imputation is a crude approach. While it might remove rows with remaining NaNs, robust imputation strategies (like the more comprehensive multi-stage imputation in the ground truth) aim to minimize such loss, especially for critical variables.

3.  **Incomplete Reporting of Results:**
    *   The prompt asked for coefficients, standard errors, *and p-values*. While `mdf.fe`, `mdf.se_fe`, and `mdf.pfe` provide these separately, the ground truth generates a concise table (from `model_results.summary().tables[1]`) which is standard practice and easier to read.
    *   The prompt also asked for ""Interpretation of the significance and direction of these effects,"" which is completely missing from the generated code.
    *   The random effects variance components are reported using `mdf.vc`, which is good, but there's no interpretation of what that value represents, which was explicitly requested.

4.  **Minor Issues:**
    *   The imports `statsmodels.api as sm` is present but `sm` is not used.
    *   The general flow and print statements for progress and checks are also missing, making it harder to track execution or diagnose issues. While not strictly part of the statistical model, it contributes to overall code quality and user experience.

In summary, the generated code misses the precise implementation of the mixed-effects structure, has a less robust and consistent approach to missing value handling for time-series data, and fails to provide the requested interpretation of the model results.",gemma-3-12b-it,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

# 1. Load the dataset
data = pd.read_csv(""clash_royale_cards.csv"")

# 2. Select features
features = ['elixirCost', 'Win Rate', 'Usage']
X = data[features]

# 3. Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. Determine the optimal...",9,"The generated code is very good and directly addresses almost all requirements of the task.

**Strong points:**
*   **Correctness:** Loads data, selects features, standardizes them, applies K-Means, performs PCA, and visualizes the results correctly.
*   **Feature Selection and Standardization:** These steps are correctly implemented as requested.
*   **K-Selection:** The code correctly implements *both* the Elbow method and the Silhouette method for determining K, which is a thorough approach, even though the task specified ""you can use WCSS or silhouette"". This adds value.
*   **K-Means Application:** K-Means is applied with the chosen K and `n_init=10` is used, which is good practice.
*   **PCA and Visualization:** PCA is correctly applied to reduce dimensionality, and the scatter plot visualizes the clusters with appropriate labels and a colorbar.
*   **Imports:** All necessary libraries are imported.

**Areas for improvement (minor):**
*   **Error Handling for File Loading:** The ground truth includes a `try-except FileNotFoundError` block, which is robust. The generated code does not include this, meaning it would crash if the file is missing.
*   **Dynamic K Selection:** While the elbow and silhouette plots are generated, `K = 3` is hardcoded. The task implies using the plots to *justify* the choice of K, which the code does by generating the plots, but a perfect solution might have a comment explaining *why* K=3 was chosen from the plots, or even a more programmatic (though often less reliable for visual methods like elbow) way to infer K. However, hardcoding K after visual inspection is common and acceptable.
*   **Plot Titles/Labels:** The PCA plot labels (`Principal Component 1`, `Principal Component 2`) are good, but adding the explained variance percentage to them would be more informative, as done in the ground truth. This isn't a strict requirement but improves clarity.
*   **Readability:** The `data` variable is used for the original dataframe and then mutated by adding the `cluster` column. While functional, explicitly creating a copy for `X` and then merging `cluster` back to just the `data` (or using `df` as in ground truth) can sometimes be clearer for more complex pipelines.

Overall, the generated code provides a highly functional and largely complete solution. The missing error handling is the primary reason it's not a perfect 10, as it makes the script less robust.",gemma-3-12b-it,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# Select features and target
features = ['elixirCost', 'Usage', 'rarity']
target = 'Win Rate'
X = data[features]
y = data[target]

# Preprocess rarity column
encoder = OneHotEncoder(handle_unknow...",7,"The generated code correctly loads the data, selects features and target, splits the data, trains a RandomForestRegressor, and evaluates it using R2 and MSE. This covers most of the requirements.

However, there's a significant issue with the one-hot encoding of the `rarity` column. The line `X[['rarity']] = encoder.fit_transform(X[['rarity']])` attempts to assign the encoded array back to a single column named `rarity`. This is incorrect because `OneHotEncoder` outputs multiple columns for the one-hot encoded representation. The subsequent line `X = X.drop('rarity', axis=1)` then drops this single, incorrectly assigned column, meaning the one-hot encoded features are effectively lost before training. The model will run, but `rarity` will not be correctly incorporated into the features.

A more robust way to handle this with `OneHotEncoder` would be to create new columns and then concatenate them with the original DataFrame (after dropping the original 'rarity' column). Alternatively, `pd.get_dummies` (as used in the ground truth) is often simpler for this specific task with DataFrames, as it directly creates new columns and integrates them.

Strong points:
- Proper imports.
- Correct data loading and feature/target selection.
- Correct data splitting.
- Correct model training and evaluation using specified metrics.
- Uses `random_state` for reproducibility.

Areas for improvement:
- The one-hot encoding implementation for `rarity` is flawed, leading to the `rarity` feature not being correctly used by the model. This is a critical logical error for feature engineering.
- The `try-except` block for file loading (as in the ground truth) would make the code more robust against missing files.
- The ground truth also included checks for missing columns, which adds robustness.
- Handling of potential missing values in the data is absent.",gemma-3-12b-it,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
try:
    df = pd.read_csv(""clash_royale_cards.csv"")
except FileNotFoundError:
    print(""Error: clash_royale_cards.csv not found in current directory."")
    exit()

# Identify unique rarity categories
rarity_categories = df['rarity'].unique()

# Perform ANOVA test
group_win_rates = {}
for rarity in rarity_categories:
    group_win_rates[rarity] = df[df['rarity'] == rarity]['Win Rate']

f_statistic, p_value = stats.f_oneway...",7,"The generated code correctly addresses most of the task requirements, including loading data, performing ANOVA, stating hypotheses, and interpreting results based on alpha.

However, there are two significant areas for improvement regarding robustness and data handling:

1.  **Missing Data Cleaning for 'Win Rate':** The code assumes 'Win Rate' is already numeric and doesn't handle potential non-numeric values or NaNs in that column. `pd.to_numeric` with `errors='coerce'` and subsequent `dropna` would make the code more robust, as in the ground truth. Without this, if 'Win Rate' contains non-numeric entries, `scipy.stats.f_oneway` might raise an error or produce incorrect results.
2.  **Lack of Robustness for Insufficient Data:** The ground truth includes checks for `df.empty` after cleaning, `len(rarity_categories) < 2`, and ensuring each group passed to `f_oneway` has `>= 2` observations. The generated code does not include these checks, which could lead to errors if the dataset is incomplete or has insufficient data for certain rarity categories. For example, if a `rarity` group has only one `Win Rate`, `f_oneway` could still run but might produce an incorrect result or a warning. A more robust solution would filter out such groups or warn the user.

These omissions affect the overall robustness and reliability of the code, especially when dealing with real-world, potentially messy datasets. The other aspects (imports, hypothesis statements, printing F-statistic/p-value, and interpretation) are well-handled.",gemma-3-12b-it,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
iris = pd.read_csv('iris.csv')

# Define features and target
X = iris[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm...",8,"The generated code correctly addresses almost all requirements of the task. It loads the data, defines X and y, encodes the target, splits the data, trains all three specified models with correct parameters, and evaluates them using the required metrics (accuracy, precision, recall, F1-score, all macro-averaged). It also correctly identifies the best model based on the F1-score.

Areas for improvement:
1.  **Missing `stratify` in `train_test_split`**: The ground truth uses `stratify=y_encoded` (or `stratify=y` from the generated code's perspective), which is crucial for maintaining class distribution in the train and test sets, especially with smaller or imbalanced datasets. This was a specific instruction from the ground truth provided (though not explicitly mentioned in the prompt's instructions, it's good practice for classification tasks). Given the prompt didn't *explicitly* specify `stratify`, the generated code isn't strictly incorrect on this point according to the prompt's instructions, but it's a best practice considered by the ground truth.
2.  **Logistic Regression `max_iter`**: The ground truth increased `max_iter` for Logistic Regression (`max_iter=200`) to prevent potential convergence warnings. While the default might work for this dataset, it's a common tweak. The generated code uses default, which is permissible by the prompt's ""default parameters"" rule.
3.  **Code Structure/Readability**: While functional, the ground truth's use of a dictionary for models and a loop for training and evaluation makes the code more concise, scalable, and less repetitive, improving overall code quality. The generated code repeats evaluation steps for each model.
4.  **Error Handling**: The ground truth included basic error handling for `FileNotFoundError` when loading the CSV, which is a good practice not present in the generated code.

Overall, the generated code fulfills the core requirements robustly. The missing `stratify` is the most notable functional difference which, while not explicitly asked for by the prompt, is a best practice often included in such tasks. The other points are more about software engineering best practices and minor parameter tuning, rather than outright correctness errors according to the prompt's explicit instructions.",gemma-3-12b-it,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
iris = pd.read_csv('iris.csv')

# Select the features
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
X = iris[features]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=3...",9,"The generated code is largely correct and fulfills almost all requirements of the task.

**Strong Points:**
*   Correctly loads the dataset and selects the specified features.
*   Correctly standardizes the features using `StandardScaler`.
*   Applies K-Means clustering with the correct parameters (`n_clusters=3`, `n_init='auto'`, `random_state=42`).
*   Applies PCA to reduce dimensions to 2 principal components.
*   Generates two scatter plots as requested, one colored by K-Means labels and one by true species labels.
*   Adds appropriate titles and axis labels to both plots.
*   Includes all necessary imports.

**Areas for Minor Improvement (Why not a 10):**
*   **Encoding Species**: While the generated code correctly encodes 'Species' using a manual dictionary mapping, the ground truth uses `LabelEncoder` from `sklearn.preprocessing`. `LabelEncoder` is generally preferred for this kind of task as it's more robust and scalable, especially if the set of unique labels is large or unknown beforehand. The task specifically mentioned ""after encoding Species to numerical values for coloring"", which `LabelEncoder` directly addresses in a generalizable way.
*   **Error Handling**: The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code lacks this.
*   **Plot Customization**: While the plots are correct, the ground truth's color bars are more informative (e.g., showing actual species names for the true species plot). This is a minor aesthetic and informational improvement.
*   **Variable Naming**: `labels` for K-Means cluster labels is acceptable, but `kmeans_labels` (as in GT) is slightly more descriptive in the context of the overall script.

Overall, the generated code successfully implements the requested data science pipeline and visualization. The deviation in `Species` encoding and minor plot refinements prevent a perfect score.",gemma-3-12b-it,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# Load the dataset
iris = pd.read_csv('iris.csv')

# Create new features
iris['PetalArea'] = iris['PetalLengthCm'] * iris['PetalWidthCm']
iris['SepalArea'] = iris['SepalLengthCm'] * iris['SepalWidthCm']

# Define features X
X = iris[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'PetalArea', '...",10,"The generated code is excellent. It correctly loads the dataset, creates the required new features, defines `X` and `y` using both original and new features, encodes the target variable, trains the Gradient Boosting Classifier with the specified parameters, performs 5-fold cross-validation, and reports the mean accuracy. All necessary imports are included, and the code structure is clean and efficient. It fully meets all the requirements of the task.",gemma-3-12b-it,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load the dataset
data = pd.read_csv('heart.csv')

# Define features (X) and target (y)
X = data[['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG', 'MaxHR', 'Exerci...",6,"The generated code attempts to address all requirements of the task. However, there are significant issues in the preprocessing steps that lead to incorrect feature handling and potential data leakage, lowering its score.

**Positive Aspects:**
*   Correctly loads the dataset.
*   Correctly identifies features and target variable.
*   Correctly uses `train_test_split` with the specified parameters.
*   Correctly initializes and trains the `RandomForestClassifier` with specified parameters.
*   Correctly calculates and prints all required evaluation metrics (accuracy, precision, recall, F1-score, ROC-AUC).
*   Imports necessary libraries.

**Areas for Improvement/Critical Issues:**

1.  **Incorrect Preprocessing Order & Feature Mixing:**
    *   **Data Leakage/Incorrect `pd.concat`:** The `OneHotEncoder` is applied directly to `X[categorical_cols]`, and then the result is `pd.concat` with `X.drop(categorical_cols, axis=1)`. This `pd.concat` operation is fundamentally flawed. `X.drop(categorical_cols, axis=1)` keeps the original *unscaled* numerical features, while the one-hot encoded features are *newly generated* and likely have a different index if `X` wasn't reset. This leads to a mismatched DataFrame where numerical and categorical features are not correctly aligned or may be duplicated/missing.
    *   **Order of Operations:** The code first performs one-hot encoding *then* scaling. While not strictly an error in itself, applying `fit_transform` on the entire `X` (or its numerical subset) *before* splitting the data causes data leakage from the test set into the training set's scaling parameters. This is a common mistake and violates good machine learning practices. Data preprocessing (especially `fit_transform`) should only occur on the training data. The ground truth correctly uses `ColumnTransformer` within a `Pipeline` to ensure preprocessing is applied after the split and only `fit` on training data.
    *   **Incorrect `X` assignment in `OneHotEncoder` block:**
        ```python
        X = pd.DataFrame(encoder.fit_transform(X[categorical_cols]), columns=encoder.get_feature_names_out(categorical_cols))
        X = pd.concat([X, X.drop(categorical_cols, axis=1)], axis=1)
        ```
        The first line overwrites `X` with *only* the one-hot encoded features, losing all numerical features. The second `concat` tries to bring back the original numerical features (`X.drop(categorical_cols, axis=1)`) but `X` then refers to the one-hot encoded DataFrame, not the original `X`. This logic is severely flawed and will lead to an incorrect `X` DataFrame being passed to the model.

2.  **Lack of `Pipeline` and `ColumnTransformer`:** The ground truth demonstrates best practices by using `ColumnTransformer` within a `Pipeline`. This correctly separates preprocessing for different feature types and ensures that `fit_transform` is only applied to the training data and `transform` to the test data, preventing data leakage. The generated code's manual preprocessing steps are prone to errors and data leakage.

3.  **Missing `handle_unknown` in `OneHotEncoder`:** While included, `handle_unknown='ignore'` is good practice for the encoder.

These issues related to preprocessing are critical. The model might still run and produce metrics, but the training data preparation is fundamentally incorrect, leading to unreliable results and non-reproducible benchmarks.",gemma-3-12b-it,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

# 1. Load the dataset
df = pd.read_csv('heart.csv')

# 2. Select features
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']
X = df[features].copy()

# 3. Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. Determine o...",8,"The generated code correctly performs all the requested steps, demonstrating a good understanding of the task.

**Strong points:**
*   **Correct Importing and Loading:** All necessary libraries are imported, and the dataset is correctly loaded.
*   **Feature Selection and Standardization:** The specified features are correctly selected and standardized.
*   **K-Means Parameter Tuning (Elbow & Silhouette):** Both the elbow method and silhouette scores are calculated and plotted for the specified range of K values.
*   **K-Means Application:** K-Means is applied with the correct `n_init='auto'` and `random_state=42` parameters.
*   **PCA Dimensionality Reduction:** PCA is correctly applied to reduce dimensions to 2.
*   **Visualization:** A scatter plot of the PCA components colored by cluster labels is generated, and includes a title and axis labels.

**Areas for improvement to achieve a higher score:**
*   **Choice of K:** While the prompt asks for the code to determine an *appropriate* K and use the elbow/silhouette plots to *justify* (meaning the choice should be informed by the plots), the generated code *hardcodes* `K = 2` without any clear justification from the plots or a mechanism to automatically select an optimal K (which would be a more advanced task, but the current implementation doesn't even visually suggest `K=2` is typically the best choice for this dataset/range, or explicitly state the visual justification for that choice). The prompt implies a more thoughtful selection of K. A simple comment stating *why* `K=2` was chosen based on the plots would improve this, or better yet, setting K to 3 as it's often the visual ""elbow"" and a good silhouette peak for this dataset.
*   **Robustness (File Not Found):** The generated code doesn't include error handling for `FileNotFoundError` if `heart.csv` is not present, which the ground truth code includes. While not explicitly asked for, it's good practice and the ground truth exemplified it.
*   **Clarity in Plots:** The plots are functional but lack specific markers or legends to clearly distinguish the cluster groups within the scatter plot, similar to the ground truth which iterates through clusters for clearer visualization and legend. Although the `cmap='viridis'` does color the points, a legend explicitly linking colors to cluster numbers would be beneficial for clarity.",gemma-3-12b-it,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
import numpy as np

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Create the binary target variable
data['quality_category'] = np.where(data['quality'] >= 7, 'good', 'bad')
data['quality_category'] = data['quality_category...",7,"The generated code largely addresses the requirements but has some notable flaws and missing practices.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly creates the binary target variable (though with an intermediate step of 'good'/'bad' strings, which is fine, but direct integer conversion is more efficient as in GT).
*   Correctly performs feature scaling using `StandardScaler`.
*   Correctly splits data into train/test sets.
*   Correctly trains a Logistic Regression model.
*   Correctly evaluates the model using accuracy, F1-score, and classification report.
*   Correctly identifies and reports important features based on coefficients.
*   Imports are appropriate.

**Areas for Improvement / Flaws:**

1.  **Binary Target Variable Creation:** The generated code renames the target column to `quality_category` and converts it to 'good'/'bad' strings, then to categorical codes. While this works, it's an unnecessary intermediate step. The ground truth directly converts `(df['quality'] >= 7).astype(int)` which is more direct and efficient.
2.  **Dropping Columns for X:** The generated code creates `X` by dropping only the original `quality` column. However, it *should also drop the newly created target column (`quality_category`)* from `X`. Currently, `quality_category` is still in the features `X`, which is a form of data leakage. This is a critical logical error. The `quality` column has already been replaced by `quality_category` as the target `y`. So `X` should be `data.drop(['quality', 'quality_category'], axis=1)`.
3.  **Missing Value Handling:** The prompt explicitly asks to ""handle missing values if any"". The generated code completely omits this step. Although this specific dataset is clean, robust code for a general data science task should include a check and strategy for missing values.
4.  **Scaling and Splitting Order:** The generated code scales `X` *before* splitting into train/test. This means `X_scaled` contains information from the test set, leading to data leakage during scaling. The `StandardScaler` should be `fit_transform` on the training data (`X_train`) and then only `transform` for the test data (`X_test`). The ground truth implements this correctly.
5.  **Interpretability of Feature Importance:** While the feature importance is reported, the output does not map feature names back to the coefficients themselves, only to the absolute values. The ground truth provides both the original coefficient and the absolute coefficient, which is more informative for interpreting the direction of the relationship. It also converts the transformed data back to a DataFrame with column names after scaling, which is good for interpretability.
6.  **Code Readability/Structure:** No comments are provided, and print statements are minimal, making it harder to follow the execution flow compared to the ground truth. The error handling for file loading is also missing.",gemma-3-12b-it,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

# 1. Load the dataset
try:
    df = pd.read_csv(""heart.csv"")
except FileNotFoundError:
    print(""Error: heart.csv not found in the current directory."")
    exit()

# 2. Create a box plot
plt.figure(figsize=(8, 6))
sns.boxplot(x=""HeartDisease"", y=""Oldpeak"", data=df)
plt.title(""Oldpeak Distribution by Heart Disease Status"")
plt.xlabel(""HeartDi...",7,"The generated code correctly performs most of the requested tasks.

**Strong Points:**
*   Correctly loads the dataset.
*   Successfully generates the box plot.
*   Performs logistic regression and extracts the coefficient.
*   Calculates and prints the odds ratio.
*   Correctly discretizes `Oldpeak` using `pd.qcut` into three bins.
*   Correctly calculates and prints the proportion of `HeartDisease=1` within each bin.

**Areas for Improvement (Why it's not higher):**

1.  **Interpretation of Odds Ratio:** The interpretation provided is a placeholder string (""Interpretation: For every one-unit increase in Oldpeak, the odds of having heart disease are multiplied by {odds_ratio}."") rather than a dynamic interpretation based on the actual calculated `odds_ratio`. It lacks the crucial part about whether the odds *increase* or *decrease* and by what percentage, which was a specific requirement for the interpretation. (Ground truth calculates `(odds_ratio - 1) * 100` for increase or `(1 - odds_ratio) * 100` for decrease.)
2.  **Visualization of Proportions:** The task implicitly asks for display (and the ground truth provides a bar plot visualization of the proportions). The generated code calculates and prints the proportions but does not visualize them, which would complete the analysis for that section.
3.  **Robustness/Error Handling:**
    *   No handling for `NaN` values in `Oldpeak` or `HeartDisease` columns, which could cause issues for plotting, regression, or `qcut`. (Ground truth includes a `dropna` check).
    *   No `duplicates='drop'` argument for `pd.qcut`, which can lead to `ValueError` if there are too many identical values in `Oldpeak` that prevent the creation of distinct quantiles. (Ground truth includes a `try-except` block for this).
4.  **Clarity/Verbosity in Output:** The ground truth provides more verbose and clear print statements indicating the completion of each step, which aids in understanding the execution flow.
5.  **Small Plotting Details:** The `xticks` and `grid` in the box plot are not as detailed as in the ground truth, making the plot slightly less informative (e.g., `(0: No, 1: Yes)` labels).

Overall, it's a solid attempt that gets most of the technical steps right, but it falls short on the completeness of interpretation and visualization, and some robustness aspects.",gemma-3-12b-it,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

# 1. Load data
try:
    data = pd.read_csv(""spotify_2023.csv"")
except FileNotFoundError:
    print(""Error: 'spotify_2023.csv' not found in the current directory."")
    exit()

# 2. Convert 'streams' to numeric and drop NaN valu...",7,"The generated code generally addresses most of the task requirements but has a significant issue in how it handles feature selection and one-hot encoding, which makes the `X` dataframe inconsistent and potentially misaligned.

**Strong Points:**
*   Correctly loads the data and handles `FileNotFoundError`.
*   Correctly converts `streams` to numeric and drops NaNs.
*   Correctly creates `is_popular` based on the 75th percentile.
*   Selects the specified numerical features.
*   Correctly splits the data using `random_state=42`.
*   Trains a Logistic Regression model with `max_iter=1000`.
*   Correctly calculates and prints F1-score (macro), ROC AUC score, and the confusion matrix.
*   Includes all necessary imports.

**Areas for Improvement:**
*   **Incorrect Feature Handling and One-Hot Encoding:** This is the most critical issue.
    *   Initially, `X` is created with only numerical features (`X = data[features]`).
    *   Then, `feature_list` is created including categorical features, and `X` is reassigned to `data[feature_list]`. This overwrites the previous `X`.
    *   The `OneHotEncoder` is called on `X[['key', 'mode']]` which is correct, but then `X_encoded` is created by concatenating `X[features]` (which contains the original numerical features *from the second `X` assignment*) with the `encoded_features`. This concatenation is done using `np.concatenate`, which creates a NumPy array.
    *   Finally, `X` is re-assigned to this raw NumPy array `X_encoded`. This means `X_train` and `X_test` lose their DataFrame structure and column names. While `LogisticRegression` can work with NumPy arrays, this approach makes debugging harder and is generally less robust than maintaining a DataFrame for `X` until the very last step if needed, or by using `pd.get_dummies` which directly returns a DataFrame with column names.
    *   A much cleaner and more standard way to handle one-hot encoding in this scenario is using `pd.get_dummies` directly on `data` or on a copy of `data` that includes both numerical and categorical features. The ground truth code handles this much better by building `X` with all features (numerical + categorical) and *then* applying `pd.get_dummies` to `X`, which keeps it as a DataFrame.

*   **Missing `stratify` in `train_test_split`:** The target variable `is_popular` is likely imbalanced (as popular songs are usually a smaller subset). `stratify=y` is important for `train_test_split` to ensure that the class distribution of `is_popular` is maintained in both training and testing sets, leading to more reliable model evaluation. The ground truth includes this.

*   **Missing `random_state` for `LogisticRegression`:** While not explicitly asked for, adding `random_state=42` to `LogisticRegression` instantiation is good practice for reproducibility of the model training process, especially initialization.

*   **Clarity and Structure:** The ground truth code has better comments explaining each step which aids readability and understanding. The step-by-step print statements in the ground truth also provide valuable feedback during execution.",gemma-3-12b-it,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. Load data
try:
    df = pd.read_csv(""spotify_2023.csv"")
except FileNotFoundError:
    print(""Error: spotify_2023.csv not found in the current directory."")
    exit()

# 2. Select audio features and drop NaNs
selected_features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%',
                    ...",9,"The generated code correctly performs all the required steps: loading data, selecting and handling NaNs in audio features, standardizing, applying K-Means clustering, performing PCA, visualizing the clusters, and reporting the mean original features per cluster. All parameters (K=4, random_state=42, n_init='auto', etc.) are correctly applied.

**Strengths:**
*   **Correctness:** All steps are implemented accurately and produce the expected output.
*   **Conciseness:** The code is more compact than the ground truth in some areas, which can be a plus for readability without sacrificing functionality.
*   **Completeness:** Addresses all specified requirements.

**Areas for Minor Improvement (to reach a perfect 10):**
*   **NaN Handling Output:** The ground truth includes a print statement showing how many rows were dropped due to NaNs. While not strictly required by the prompt, it's good practice for data cleaning stages to provide such diagnostic information. The generated code implicitly handles this by `df = df[selected_features].dropna()`, but doesn't report the impact.
*   **Original Data Preservation:** The ground truth explicitly creates `df_original_for_analysis = df_features.copy()` before scaling. The generated code applies `dropna()` directly to `df` and modifies `df` by adding the 'cluster' column, which means the `original_features = df[selected_features]` line might implicitly refer to the already modified `df` (which is fine because the `cluster` column is not in `selected_features`). However, `df` now contains NaNs *removed* and the cluster column. While it works for this problem, in more complex scenarios, explicitly separating the data for original value calculations versus scaled/transformed data can prevent subtle bugs. The current approach is *correct* for this problem because selecting `df[selected_features]` ensures only the original, unscaled values are taken for the mean calculation, and the NaNs were dropped from `df` at the beginning, so `df` at this point already represents the clean original data used for K-Means and PCA. So this is more of a style preference or a consideration for robustness in larger projects.

Overall, it's a very strong submission that meets all the core requirements.",gemma-3-12b-it,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. Load data
try:
    df = pd.read_csv('spotify_2023.csv')
except FileNotFoundError:
    print(""Error: spotify_2023.csv not found in current directory."")
    exit()

# 2. Convert 'streams' to numeric, coerce errors to NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# 3. Filter data
df = df[df['streams'] > 10000000]
df = df[(df['released_year'] >= 2010) & (df['released_year'] <= 2023)]
df = df.dropna(subset=['stre...",8,"The generated code correctly addresses almost all aspects of the task.

**Strong Points:**
- **Correctness:** The code correctly loads data, converts `streams` to numeric with `errors='coerce'`, filters data based on both `streams` and `released_year` conditions, groups by `released_year`, calculates the average `danceability_%`, `energy_%`, and `valence_%`, and generates line plots for each feature.
- **Completeness:** All the specified requirements (data loading, `streams` conversion, filtering, grouping, aggregation, plotting) are covered.
- **Code Quality:** Imports are proper, and the code structure is logical and easy to follow, mirroring the steps outlined in the task description. The use of `tight_layout()` for plots is also good practice.

**Areas for Improvement (Why not a 10):**
- **Order of filtering:** The ground truth code drops NaNs from `streams` *after* conversion but *before* the numerical filter `df_filtered[df_filtered['streams'] > 10_000_000]`. The generated code applies the numerical filter first (`df = df[df['streams'] > 10000000]`) and *then* drops NaNs (`df = df.dropna(subset=['streams'])`). While this doesn't change the final result for the specific `<= 10,000,000` case (as NaNs would fail the `>` comparison anyway), it's generally safer and clearer to drop NaNs immediately after coercion if they are not intended for further numerical operations, to prevent unexpected behavior with other operations further down the line that might implicitly handle/ignore NaNs differently. For this specific task, it has no impact on correctness but is a subtle point of robustness.
- **Plotting Aesthetics:** While the plots are correct, they lack some aesthetic refinements present in the ground truth, such as markers (`marker='o'`), specific colors, and rotation of x-axis ticks (`plt.xticks(yearly_avg_features.index, rotation=45)`). These improvements make the plots more readable and visually appealing, especially for years. Adding a grid (`plt.grid(True)`) would also enhance readability.
- **Robustness/User Feedback:** The ground truth includes `print` statements for tracking data shape changes and informing the user about the process, as well as a check for an empty DataFrame after filtering. While not strictly required by the prompt, these additions make the code more robust and user-friendly, especially important for a data analysis script. The generated code only handles `FileNotFoundError` for loading.",gemma-3-12b-it,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean Likes, Dislikes, and Video views
for col in ['Likes', 'Dislikes', 'Video views']:
    df[col] = df[col].astype(str).str.replace(',', '').astype(float)
    df[col] = df[col].fillna(0)

# Trea...",7,"The generated code addresses most of the requirements but has a few critical issues and areas for improvement.

**Strong Points:**
*   Correctly loads the dataset.
*   Correctly handles comma removal and conversion to numeric types for `Likes`, `Dislikes`, and `Video views`, and fills NaNs with 0.
*   Handles missing `Category` values by filling with 'Unknown' and performs one-hot encoding.
*   Correctly defines `X` and `y` for the most part.
*   Splits data with the correct test size and `random_state`.
*   Trains a Linear Regression model.
*   Evaluates and reports R-squared and MAE.
*   Includes all necessary imports.

**Areas for Improvement / Issues:**

1.  **`published` column handling (Critical Issue):** The prompt explicitly asks to treat the `published` column (year) as a numeric feature. The generated code `df['published'] = df['published'].astype(int)` directly casts the existing `published` column to `int`. The `youtube_trendings.csv` dataset's `published` column is a full date string (e.g., '2023-10-26 10:00:00 UTC'). Directly converting this to `int` will cause a `ValueError` or incorrect results if it somehow manages to convert. The ground truth correctly parses this as a datetime and then extracts the year. This is a significant functional bug.
2.  **One-Hot Encoding `X` construction (Minor Issue/Inefficiency):** The way `X` is constructed after one-hot encoding is inefficient and potentially buggy.
    ```python
    X = df[['Dislikes', 'Video views', 'published']]
    for col in category_encoded.columns:
        X = pd.concat([X, df[col]], axis=1) # This line assumes df[col] exists, which it doesn't after concat and drop.
    ```
    The `category_df` (which holds the encoded columns) is concatenated to `df`, but then `df` is used to try and retrieve `df[col]` for the `X` construction loop. The `category_df` itself should be used for concatenation to `X`, not `df`. This leads to a `KeyError` because `df[col]` will not exist for the one-hot encoded columns `Category_Music`, `Category_Gaming`, etc. The correct way is shown in the ground truth using `pd.concat` with the `category_df` directly.
3.  **`sparse_output=False` for `OneHotEncoder`:** While `sparse_output=False` is used, the subsequent manual `pd.DataFrame` conversion `category_df = pd.DataFrame(category_encoded, columns=encoder.get_feature_names_out(['Category']))` is redundant if `sparse_output=False` is already set. `fit_transform` would return a dense NumPy array directly. However, it's not a major bug, just slightly inefficient.
4.  **No error handling for file not found:** The code assumes the CSV file exists. While the prompt doesn't explicitly ask for it, robust code should include it (as in the ground truth).
5.  **Data type for `Likes`, `Dislikes`, `Video views`:** While conversion to `float` and then filling NaNs with 0 is done, the prompt implicitly suggests these should be integers (which is common for counts). The ground truth then casts them to `int` after cleaning. The generated code leaves them as `float`, which might be acceptable but less precise for counts.",gemma-3-12b-it,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '').astype(float)
df['Video views'] = df['Video views'].fillna(0)

# Ensure 'published' is numeric
df['published'] = df['published'].astype(int)

# Group by year and calculate metrics
grouped = df.groupby('published').agg(
    avg_views=('Video views', 'mean'),
    unique_videos=('Video', '...",6,"The generated code correctly addresses most of the requirements. It loads the dataset, cleans the 'Video views' column by removing commas, converting to float, and filling NaNs. It also correctly groups the data by the 'published' year and calculates the average video views and unique video counts. The plotting is generally correct, creating two subplots with appropriate titles and labels.

However, there are a couple of significant issues that lower the score:

1.  **Handling of 'published' column:** The prompt explicitly states, ""Ensure the `published` column (year) is treated as a numeric or integer type,"" and the ground truth uses `publishedAt` to extract the year. The generated code assumes a column named 'published' already exists and contains just the year, and directly tries `df['published'].astype(int)`. In the `youtube_trendings.csv` dataset, the year information is part of the `publishedAt` column (e.g., '2020-08-11T16:00:27Z'). Simply casting `published` to `int` would likely lead to an error if `published` doesn't exist or isn't in a clean year format. This is a critical omission in understanding the dataset structure implied by the task (which refers to 'published' year, typically extracted from a full datetime string).
2.  **Subplot orientation:** The task asks for two subplots where ""the first subplot should be a line graph... and the second subplot should be a line graph..."". This implies one above the other (`nrows=2, ncols=1`). The generated code creates them side-by-side (`1, 2`), which is a minor deviation but still a deviation from the implied layout.
3.  **Plot titles and labels:** While titles and labels are present, the ground truth code used more descriptive titles like 'Average Video Views Per Year' and included ticks for integer years, enhancing readability. The generated code's titles are a bit generic.

To improve, the code should:
*   Correctly parse the `publishedAt` column to extract the year, if `published` column isn't directly available with years.
*   Change the subplot layout to `2, 1` as per typical prompt interpretation (one above the other).",gemma-3-12b-it,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
import collections.Counter
from nltk.corpus import stopwords
import nltk

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)
    return text

def analyze_youtube_titles(csv_file=""youtube_trendings.csv""):
    df = pd.read_csv(csv_file)

    # Create title_word_count feature
    df['title_word_count'] = df[...",7,"The generated code correctly addresses most of the requirements.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly calculates `title_word_count`.
*   Effectively cleans the `Video views` column (removes commas, converts to numeric, fills NaNs).
*   Calculates and prints the Pearson correlation.
*   Filters the dataset for 2023/2024 videos.
*   Extracts and processes words (lowercase, remove punctuation, remove stop words).
*   Identifies and prints the 5 most common words.
*   Properly handles `nltk.download('stopwords')`.

**Areas for Improvement:**
*   **Import `collections.Counter`:** The generated code imports `collections.Counter` as `import collections.Counter` which is incorrect syntax. It should be `from collections import Counter`. While it might work in some environments due to internal Python mechanisms, it's not standard and can lead to issues. The ground truth correctly uses `from collections import Counter`.
*   **Case Sensitivity in Stop Word Removal:** The ground truth code correctly converts words to lowercase *before* checking against stop words (`word for word in title.split() if word not in stop_words`). The generated code processes `cleaned_title.lower().split()` for `words`, but then `filtered_words = [word for word in words if word not in stop_words]`. This implicitly handles lowercase comparison, so it's functionally correct here, but explicitly processing `title.lower()` and then using that for splitting and filtering is a clearer approach, as done in the ground truth.
*   **Punctuation Removal Redundancy:** The `clean_text` function removes punctuation. Then, `cleaned_title.lower().split()` is used. The `re.sub(r'[^\w\s]', '', text)` already handles non-alphanumeric characters. While functionally correct, it's good to be aware of the interaction. The ground truth combines `lower()` and `re.sub` within the loop, which is also effective.
*   **Error Handling for File Not Found:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is robust. The generated code does not include this, which means it would crash if `youtube_trendings.csv` is missing.
*   **Readability/Clarity:** The ground truth breaks down the process into clear, commented steps, making it easier to understand the intent of each section. The generated code uses a function, which is good practice for modularity, but could benefit from more internal commenting or clearer logical separation for the various steps. The `clean_text` function is a good modularization, however.
*   **Post-processing for common words:** The ground truth adds `and word.isalpha()` to the word filtering, which prevents numbers (e.g., ""iphone15"") or empty strings from being counted as valid words after punctuation removal. The generated code might include such tokens if they survive the initial cleaning and are not stop words. This is a subtle but important refinement for ""words.""",gemma-3-12b-it,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score

# 1. Load data
df = pd.read_csv('mobiles_2025.csv')

# 2. Clean Launched Price (USA) and create target
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '', regex=False)
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace(',', '', regex=False)
df['Launched Price (USA)']...",7,"The generated code correctly addresses most of the task requirements, but it has several issues that prevent it from being a top-tier solution.

**Strengths:**
*   Loads the data correctly.
*   Handles the `Launched Price (USA)` cleaning and `is_expensive_usa` target creation mostly correctly (with one minor nuance discussed below).
*   Correctly cleans `Mobile Weight`, `RAM`, `Battery Capacity`, and `Screen Size`.
*   Correctly one-hot encodes `Company Name` and `Processor`.
*   Identifies the correct features (`X`) and target (`y`).
*   Splits the data as requested.
*   Trains the RandomForestClassifier with the specified parameters.
*   Evaluates using F1-score (macro) and ROC AUC.

**Areas for Improvement / Issues:**

1.  **Target `is_expensive_usa` Creation:**
    *   **Issue:** The ground truth sets `is_expensive_usa` to `np.nan` if `Launched Price (USA)` is `NaN` *before* dropping rows. The generated code directly converts to `int` (`.astype(int)`). If `pd.to_numeric` results in `NaN`, then `(df['Launched Price (USA)'] > 1000)` would be `False`, and `astype(int)` would make it `0`. This means unparseable price values would be treated as ""not expensive"" (0) rather than being dropped explicitly due to an ambiguous price (which the `dropna(subset=['is_expensive_usa'])` is meant to do for actual NaNs). While `dropna(subset=['is_expensive_usa'])` is called, the effect is different because `astype(int)` converts `False` to `0` and `True` to `1`, and `NaN` to `0`, preventing actual `NaN` values *in the `is_expensive_usa` column itself* later. The prompt implicitly meant to drop rows where the *original price* could not be parsed into a numeric value, leading to an `is_expensive_usa` that is `NaN`. The ground truth does this by applying the `np.nan` only when `pd.notna(x)` is `False`.
    *   **Recommendation:** Use a similar logic to the ground truth where `is_expensive_usa` is explicitly set to `np.nan` if the cleaned price is NaN.

2.  **Camera Feature Cleaning (`extract_mp` function):**
    *   **Issue:** The `extract_mp` function
        ```python
        def extract_mp(camera_string):
            if isinstance(camera_string, str):
                try:
                    return float(camera_string.split(' ')[0])
                except:
                    return 0.0
            else:
                return 0.0
        ```
        assumes the MP value is always the *first* word before a space. For example, ""48 MP"" would correctly become 48.0. However, ""Dual 12MP + 5MP"" would yield 2.0 (from 'Dual') if `split(' ')[0]` is used, which is incorrect. The prompt specifically said ""extract the first numerical MP value"". The ground truth's regex `r'(\d+(\.\d+)?)\s*MP'` is much more robust.
    *   **Recommendation:** Implement a more robust regex-based extraction for camera MP values.

3.  **NaN Handling in Features/Target (`X`, `y`):**
    *   **Issue:** The code calls `df.dropna()`, `X.dropna()`, and `y.dropna()` separately and after `X` and `y` have been defined.
        *   `df.dropna()`: This operates on the *entire dataframe* and might drop rows based on *other* columns not used in `X` or `y`, and it also changes `df` *before* `X` and `y` are definitively selected.
        *   `X = X.dropna()` and `y = y.dropna()`: This will cause a mismatch in indices between `X` and `y` unless the exact same rows happen to have NaNs in both, which is often not the case. If `X` has a NaN in one row and `y` doesn't in that same row, `X.dropna()` will remove that row from `X`, but `y.dropna()` might not remove the corresponding row from `y`, leading to `X` and `y` having different numbers of rows or mismatched indices. This needs to be handled by dropping NaNs *consistently* across both `X` and `y` at the same time, usually by joining them or using the indices. The ground truth addresses this by taking the combined `X` and `y`, dropping NaNs there, and then re-assigning.
    *   **Recommendation:** Drop NaNs from `X` and `y` simultaneously to ensure alignment, typically by concatenating them, dropping NaNs, and then splitting, or by dropping NaNs from the original `df` based on the columns selected for `X` and `y` combined. Also, this should ideally happen *after* all feature engineering on `X` is complete.

4.  **`drop_first=True` in `pd.get_dummies`:**
    *   **Issue:** The prompt did not specify `drop_first=True` for one-hot encoding. While `drop_first=True` avoids multicollinearity, it's not always desired for interpretability or when the model implicitly handles multicollinearity. The ground truth correctly uses `drop_first=False` (or omits it, as `False` is default for `dummy_na`). Sticking to the exact requirements is important unless there's a strong reason to deviate.
    *   **Recommendation:** Remove `drop_first=True` as it wasn't specified.

5.  **Robustness (File Not Found):**
    *   **Issue:** The code doesn't include error handling for `FileNotFoundError` when loading the CSV, which could make it crash if the file isn't present.
    *   **Recommendation:** Add a `try-except FileNotFoundError` block for loading the CSV.

6.  **Readability/Clarity:**
    *   The repeated `regex=False` is good, but the overall chaining of operations on the same column in step 2 for launched price can sometimes be less readable for complex transformations than temporary columns (as in GT) or breaking it down.
    *   Using column names directly for assigning cleaned values (`df['Mobile Weight'] = ...`) modifies the original column, which is fine, but sometimes creating new cleaned columns (`df['Mobile Weight_cleaned'] = ...`) can preserve original data and make it clearer what's being used for features. The prompt only asked for cleaning, so overwriting is acceptable but worth noting.

In summary, the code provides a mostly correct solution but lacks robustness in error handling, has an imprecise method for camera MP extraction, and has a critical flaw in its NaN handling that could lead to misaligned `X` and `y` sets which would cause model training to fail or produce incorrect results.",gemma-3-12b-it,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

def clean_price(price_column):
    """"""
    Cleans price columns: removes currency symbols and commas, then converts to numeric.
    Handles errors by returning NaN.
    """"""
    if pd.isna(price_column):
        return np.nan
    try:
        price_str = str(price_column).replace('PKR', '').replace('INR', '').replace('CNY', '').replace('USD', '').replace('AED', '').replace(',', '')
        return float(price_str)
    except:
        return np.nan
...",9,"The generated code is largely correct and achieves the task goals.

**Strong Points:**
- Correctly loads the CSV file.
- The `clean_price` function correctly handles currency symbols, commas, and converts to float, returning `NaN` on errors. It uses a concise `.replace()` chain.
- Correctly applies the cleaning function to all specified price columns.
- Correctly calculates `Average Global Price` by taking the mean across the relevant columns.
- Correctly identifies the top 5 models by `Average Global Price` and sorts them.
- Displays the results in a readable format.
- Uses good function modularity (`clean_price`, `calculate_average_global_price`, `main`).

**Areas for Improvement (minor points):**
- **Handling `skipna` in `mean()`:** The `calculate_average_global_price` function uses `df[...].mean(axis=1)`. By default, `mean()` in pandas for DataFrames will skip NaN values (`skipna=True` is the default). This is correct for the prompt's requirement ""This average should only consider countries for which price data is available (not NaN) for that specific model."" However, explicitly stating `skipna=True` (like in the ground truth) would make the intent clearer and more robust against potential future pandas default changes, though it is not strictly necessary here.
- **Error Handling for File Not Found:** The generated code does not include a `try-except` block for `pd.read_csv`, which means it would crash if the file is not found. The ground truth handles this gracefully.
- **Redundancy in `calculate_average_global_price` function:** The `calculate_average_global_price` function performs both the cleaning and the averaging. While functional, it might be cleaner to apply the `clean_price` function in `main` or a separate helper function, and then calculate the average, to separate concerns more effectively, especially if only the cleaning or only the averaging were needed elsewhere.
- **Handling `NaN` in final display:** If a model's `Average Global Price` turned out to be `NaN` (e.g., all original price columns for that model were `NaN` or unparseable), it would still show up in the top 5 if sorted correctly. The `dropna(subset=['Average Global Price'])` in the ground truth explicitly filters these out before display, which might be a slightly more robust way to present ""meaningful"" top 5 results. In practice, for a top 5, `NaN` values typically sort to the end, but it's a detail worth noting.

Overall, the code is very good and fully addresses the core requirements. The few minor points are mostly about defensive programming and slight structural improvements rather than correctness issues.",gemma-3-12b-it,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: mobiles_2025.csv not found in the current directory."")
    exit()

# Clean RAM
df['RAM'] = df['RAM'].str.replace('GB', '', regex=False)
df['RAM'] = pd.to_numeric(df['RAM'], errors='coerce')

# Clean Battery Capacity
df['Battery Capacity'] = df['Battery Capacity'].str.replace('mAh', '', regex=False)
df['Battery Capacity'] = pd....",7,"The generated code correctly addresses most of the task requirements, including loading the data, cleaning the specified columns, handling NaNs, filtering for Apple products, grouping by year, and generating box plots for each year.

However, there are a few areas for improvement:
1.  **Robustness in Cleaning:** The generated code applies `str.replace` directly without first ensuring the column is of string type (`.astype(str)`). While it might work for columns that are already strings or can be implicitly converted, it's safer and less prone to errors to explicitly convert to string first, as done in the ground truth.
2.  **Missing `Launched Year` Cleaning:** The `Launched Year` column is used for grouping, but the code does not explicitly clean it (e.g., convert to numeric, handle potential NaNs and then to integer) as the ground truth does. This can lead to issues if `Launched Year` is not already in a clean numeric format, or if it contains non-numeric entries.
3.  **Plotting Library Usage:** While `matplotlib.pyplot` is used, the generated code uses `plt.plot(kind='box')`. While functional, `seaborn.boxplot` or directly using `ax.boxplot` from matplotlib offers more control and better aesthetics out-of-the-box, which is why seaborn was included in the ground truth's imports and usage. The ground truth also uses a more professional style for plots (`plt.style.use('seaborn-v0_8-darkgrid')`) which was a suggestion in the prompt.
4.  **Clarity of Subplot Titles/Labels:** The titles for the subplots are good, but the overall figure lacks a main title indicating the year, which the ground truth included using `fig.suptitle`. The y-axis labels for RAM and Battery Capacity are slightly different from the ground truth (e.g., 'RAM (GB)' vs 'RAM (GB)').
5.  **Efficiency of Grouping and Plotting:** Using `df_grouped.get_group(year)` multiple times within the loop is less efficient than filtering the DataFrame once for the `year` and then plotting from that filtered DataFrame, as shown in the ground truth (`year_data = apple_df[apple_df['Launched Year'] == year]`).
6.  **Missing Imports:** Although `seaborn` was not explicitly requested or used, its omission prevents the generation of more aesthetically pleasing plots.
7.  **No `seaborn` import despite being mentioned in the prompt**: The prompt mentions ""Include proper imports (pandas, matplotlib.pyplot or seaborn)"". The generated code only imports `matplotlib.pyplot`.

Overall, the code is a good attempt and produces the required output, but it could be more robust, efficient, and adhere more closely to best practices for data cleaning and visualization.",gemma-3-12b-it,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures
from sklearn.impute import KNNImputer, SimpleImputer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
fr...",4,"The generated code attempts to address many of the requirements but falls short in critical areas, particularly in advanced preprocessing, pipeline usage, comprehensive evaluation, and proper application of interpretability tools.

**Areas where it falls short:**

1.  **Data Preprocessing and Feature Engineering:**
    *   **Advanced Feature Engineering:** The `preprocess_titanic` function is very basic. It extracts a title (but hardcodes a mapping which is problematic for unseen titles), creates `FamilySize` and `IsAlone`, and then applies `fillna` for Age and Embarked. It attempts `PolynomialFeatures` but does it on a subset of features *before* imputation and proper encoding, creating new columns and then dropping originals, which is very messy and error-prone.
    *   **Text Mining (from Names):** It extracts titles but lacks the depth of the ground truth (e.g., grouping rare titles properly, handling missing values, `Name_Length`, `Is_Married`).
    *   **Missing Values (Multiple Imputation):** It uses `KNNImputer` (though not shown in the preprocessing function used) and `SimpleImputer` for Age and Embarked *before* scaling and encoding, which is incorrect. The ground truth uses `IterativeImputer` (MICE), which is a much more advanced multiple imputation strategy that works within pipelines.
    *   **Handling Cabin/Ticket:** The generated code completely ignores the `Cabin` and `Ticket` features, which the ground truth addresses with feature engineering (extracting `Deck`).

2.  **Pipeline Usage:** The code doesn't use `ColumnTransformer` or `Pipeline` effectively for bundling preprocessing steps *before* model training, especially when features are dynamically generated or scaled. The `preprocess_titanic` function modifies the DataFrame directly, which is less robust than using `TransformerMixin` and `Pipeline` for a clean workflow. This also makes the feature selection part quite convoluted.

3.  **Hyperparameter Tuning:** It uses `GridSearchCV` only. The task explicitly asked for `GridSearchCV` *or* `RandomizedSearchCV`. While `GridSearchCV` is one option, `RandomizedSearchCV` is often preferred for its efficiency in high-dimensional search spaces, and the task mentioned it as a primary option. The parameter grids are also quite limited.

4.  **Ensemble Methods:** Stacking and Voting are implemented, but they are built directly from the *trained* base models, not as part of a `Pipeline` themselves. This means the individual models in the ensemble haven't necessarily seen data preprocessed by a consistent transformer (especially since their training was done implicitly through `build_and_tune_model` without transparent preprocessing).

5.  **Feature Selection:**
    *   The `feature_selection_comparison` function operates on the raw `X_train` data, not the properly preprocessed data (scaled, encoded, polynomial features generated), making the feature selection less meaningful on the ""real"" features the models learn from.
    *   It only implements `RFE` and `SelectFromModel` and doesn't explicitly compare their *impact* within a proper pipeline context, which is a key requirement. The ground truth trains models *after* feature selection to demonstrate impact.

6.  **Extensive Model Evaluation:**
    *   **Stratified k-fold:** Used for GridSearchCV but not clearly for overall model evaluation of *all* models.
    *   **Learning Curves and Validation Curves:** Completely missing. This was a specific highly advanced evaluation requirement.
    *   **Comprehensive Performance Metrics:** While AUC, accuracy, and F1 are calculated, the presentation is unstructured.
    *   **Statistical Significance Tests:** It uses `ttest_ind` which is for independent samples. For comparing models on the *same data splits* (e.g., from cross-validation), a paired t-test (`scipy.stats.ttest_rel`) is preferred and more appropriate, as used in the ground truth. The generated code also doesn't perform cross-validation to get the scores for comparison, which renders the t-test incorrect here.

7.  **Model Interpretability (SHAP/LIME):**
    *   **SHAP:** The `shap.Explainer` is used but it's not clear whether it correctly handles the preprocessing chain for the `stacker`. For tree-based models (like XGBoost implicitly in the ground truth for SHAP `TreeExplainer`), there are more efficient explainers. The `explainer` is initialized on `stacker` (an ensemble of diverse models), which might not be the most straightforward for SHAP. The `explainer(X_test)` creates `shap_values` but the underlying model type used by `Explainer` isn't specified (e.g., `KernelExplainer`, `TreeExplainer`), which is critical for performance and correctness.
    *   **LIME:** There's an import for LIME, but no actual LIME implementation or usage in the final code. This is a critical omission.

8.  **Imports:** Some necessary imports are missing (e.g., `sklearn.ensemble.VotingClassifier` from top, it's imported later within the code), and `sklearn.compose.ColumnTransformer`, `sklearn.pipeline.Pipeline`, `sklearn.experimental.enable_iterative_imputer` are completely absent, reflecting the lack of advanced pipeline usage.

In summary, the generated code implements some basic elements but misses the core ""advanced"" aspects requested in the prompt, especially concerning robust preprocessing pipelines, advanced evaluation metrics, and the correct application of interpretability tools within a full modeling pipeline.",gemma-3-12b-it,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
data = pd.read_csv('imdb_top_250.csv')

# Define features (X) and target (y)
X = data[['Year', 'Runtime']]
y = data['Rating']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Linear Regres...",7,"The generated code correctly performs all the core steps of the task: loading data, defining features and target, splitting data, training a Linear Regression model, and evaluating it using R2 and MAE. All required imports are present, and the code is structured logically.

However, it lacks robustness and completeness in handling real-world data issues, which the ground truth addresses. Specifically:
1.  **Error Handling for File Loading:** The generated code does not include `try-except` blocks to handle `FileNotFoundError` or other exceptions when loading the CSV, which can lead to script termination if the file is missing.
2.  **Data Preprocessing/Cleaning:** The `Runtime` column in `imdb_top_250.csv` often contains ' min' suffix (e.g., ""120 min"") and is of object type. The generated code does not handle this, meaning `pd.to_numeric` on such a column would likely fail or produce `NaN` values, and these NaNs are not explicitly dropped. The `Year` and `Rating` columns also benefit from explicit `pd.to_numeric` with `errors='coerce'` as a safeguard.
3.  **Missing Value Handling:** The code implicitly relies on `train_test_split` or model training to handle `NaN` values (which `LinearRegression` cannot handle directly, raising an error), but it doesn't explicitly drop rows with `NaN`s in the feature or target columns after potential type conversions, which is crucial for model stability.

While functional for a perfectly clean dataset, these omissions make the code less production-ready and less resilient to typical data inconsistencies that often exist in real datasets. Addressing these points would significantly improve its quality and completeness.",gemma-3-12b-it,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 1. Load the dataset
df = pd.read_csv(""imdb_top_250.csv"")

# 2. Select the features
features = [""Rating"", ""Runtime""]
X = df[features]

# 3. Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. Apply K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
df[""Cluster""] = kmeans.fit_predic...",7,"The generated code correctly performs the K-Means clustering steps as required: loading data, selecting features, standardization, applying K-Means (with correct parameters), and creating a scatter plot colored by clusters. The calculation and printing of cluster means are also correct, although the ground truth's `groupby` approach is more idiomatic for pandas.

However, the major drawback is the lack of robust data cleaning or error handling, particularly for the 'Runtime' column. The task description implies using `imdb_top_250.csv`, which often has 'Runtime' in a string format (e.g., '142 min') and might have missing values. The generated code directly tries to use `df[""Runtime""]` for plotting and mean calculation without explicit conversion or handling of non-numeric values or ' min' suffixes. This will likely cause errors or incorrect behavior if the data is not pre-cleaned, which is a critical part of data science workflows. The ground truth explicitly addresses this common data format issue.

Improvements needed:
- Add robust data cleaning for the 'Runtime' column to convert it to a numeric type, handling the ' min' suffix and potential non-numeric entries (e.g., using `pd.to_numeric` with `errors='coerce'`).
- Handle potential `FileNotFoundError` when loading the CSV.
- Handle potential NaN values in the selected `Rating` and `Runtime` columns after any conversions, before scaling and clustering.
- The `groupby().mean()` method used in the ground truth for calculating cluster means is more efficient and conventional in pandas than iterating through clusters. While the generated code's loop works, the `groupby` approach is preferred for code quality.",gemma-3-12b-it,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter

try:
    stopwords.words('english')
except LookupError:
    nltk.download('stopwords')

try:
    word_tokenize('test')
except LookupError:
    nltk.download('punkt')

df = pd.read_csv('imdb_top_250.csv')

df['title_length'] = df['Title'].str.len()

correlation = df['title_length'].corr(df['Rating'])
print(f""Pearson correlation between ...",8,"The generated code largely fulfills the requirements of the task.

**Strong Points:**
*   Correctly loads the dataset.
*   Successfully calculates `title_length` and its correlation with `Rating`.
*   Includes necessary NLTK downloads for `stopwords` and `punkt`.
*   Applies lowercase conversion, removes punctuation, and tokenizes titles.
*   Removes stopwords effectively.
*   Identifies and prints the 10 most common words with their frequencies.
*   Uses `collections.Counter` efficiently.

**Areas for Improvement:**
*   **Punctuation Removal:** The `re` module (used in ground truth) is generally more robust for removing punctuation based on non-alphanumeric characters than `c.isalnum() or c.isspace()`. While `isalnum()` plus `isspace()` might seem to cover it, it doesn't always handle all edge cases or unicode punctuation as gracefully as `re.sub(r'[^a-z0-9\s]', '', text)`. For instance, if the original title could contain non-ASCII alphanumeric characters, `isalnum()` might strip them, whereas a more targeted regex could preserve them if needed. However, for a typical English movie title dataset, it's often sufficient.
*   **Correlation Calculation Precision:** The prompt asks for `pearson` correlation specifically, and while `corr()` defaults to Pearson, explicitly stating `method='pearson'` as in the ground truth is good practice for clarity. Also, the ground truth includes an important step of converting `Rating` to numeric with `errors='coerce'` and `dropna()` to handle potential non-numeric values or NaNs in the data which could impact correlation calculation. The generated code assumes `Rating` is clean enough, which might not always be true in real-world datasets. This is a subtle but important robustness difference.
*   **Code Structure/Readability:** Putting all preprocessing steps (lowercase, punctuation removal, tokenization, stopword removal) into a single `preprocess_title` function, while concise, can make debugging or modifying individual steps slightly less straightforward than breaking them into separate DataFrame operations as seen in the ground truth. The ground truth's approach of creating intermediate columns like `processed_title`, `tokenized_title`, `filtered_title` offers better visibility into each step of the pipeline.

Overall, the code is very close to perfect, with minor areas related to robustness and code structure that could be enhanced.",gemma-3-12b-it,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv(""penguins.csv"")

# Define features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g',...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being fully correct and robust.

**Positive Aspects:**
*   Correctly loads the dataset and defines features/target.
*   Uses `SimpleImputer` for both numerical (median) and categorical (most frequent) imputation, which is a good practice.
*   Correctly uses `StandardScaler` for numerical feature scaling.
*   Correctly uses `OneHotEncoder` for categorical features.
*   Correctly uses `LabelEncoder` for the target variable.
*   Trains a `RandomForestClassifier` with specified parameters.
*   Evaluates using 5-fold cross-validation and prints the mean accuracy.
*   Imports are mostly complete.

**Issues and Areas for Improvement:**

1.  **Imputation Order and Data Overwriting/Mismatch:**
    *   The most significant issue is in the imputation and encoding steps.
    *   `X[numerical_cols] = imputer_numerical.fit_transform(X[numerical_cols])` correctly imputes numerical columns *in place* within `X`.
    *   `X['sex'] = imputer_categorical.fit_transform(X[['sex']])` and `X['island'] = imputer_categorical.fit_transform(X[['island']])` impute the categorical columns.
    *   **CRITICAL FLAW:** `X = pd.DataFrame(encoder.fit_transform(X[['island', 'sex']]).toarray(), columns=encoder.get_feature_names_out(['island', 'sex']))` This line **overwrites the entire `X` DataFrame** with *only* the one-hot encoded categorical features. All the numerical features and their previous imputation/scaling would be lost.
    *   Subsequently, `X[numerical_cols] = scaler.fit_transform(X[numerical_cols])` tries to scale `numerical_cols` on the *new, reduced* `X` (which only contains the one-hot encoded features). This will lead to a `KeyError` because `numerical_cols` are no longer present in `X`.
    *   **Solution:** The preprocessing steps should be applied sequentially to the `X` DataFrame, preserving all columns. A common approach is to use `ColumnTransformer` (more robust) or manually handle numerical and categorical columns, performing operations, and then concatenating them.

2.  **Dropping NA in 'island' (after mode imputation):** The requirement states: ""(Note: if a row still has NA in `island` after mode imputation, e.g. if all were NA, it can be dropped, or a placeholder like 'Unknown' can be used if mode is NA itself. For this task, if mode imputation results in NA for island, drop those rows).""
    *   The generated code uses `imputer_categorical = SimpleImputer(strategy='most_frequent')` for `island`. If `island` column was entirely NA, `mode()` calculation itself might return `NaN` or `SimpleImputer` might raise an error/be undefined. The provided `SimpleImputer` for `most_frequent` generally works, but the ground truth handles the edge case of an *entirely* NA column explicitly, ensuring no `NA` remains.
    *   `X = X.dropna(subset=['island'])` is good as a safeguard *after* imputation, but it should ideally be integrated into the overall preprocessing logic, particularly concerning the `y` alignment. If rows are dropped from `X`, they must also be dropped from `y`. The current code misses this alignment.

3.  **Missing `y` alignment after dropping rows:** The `dropna(subset=['island'])` only affects `X`. If rows are dropped from `X`, the corresponding rows in `y` must also be dropped to maintain alignment for cross-validation. This is a common oversight that leads to silent errors or incorrect results.

4.  **No handling of `species` NA values:** The ground truth specifically includes handling of NA values in the target `species` (`y`). The generated code does not account for this, which could cause issues if the `species` column has NaNs.

Overall, the code structure and intent are mostly correct, but the execution of feature transformation in a way that preserves all features for the final model is fundamentally flawed, leading to incorrect input for the classifier. The lack of `y` alignment after dropping rows is also a significant error.",gemma-3-12b-it,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
penguins = pd.read_csv(""penguins.csv"")

# Select features
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']

# Drop rows with missing values
penguins = penguins.dropna(subset=features)

# Standardize the features
scaler = StandardScaler()
scaled_features = sc...",8,"The generated code correctly addresses most of the requirements. It loads the data, selects and cleans the specified features, standardizes them, applies K-Means with the correct parameters, and performs PCA. It also successfully generates the two required scatter plots.

Strong points:
- Correctly identifies and uses the specified features.
- Handles NA values correctly for the selected features.
- Applies StandardScaler as required.
- Implements K-Means with K=3, `n_init='auto'`, and `random_state=42`.
- Applies PCA to 2 components.
- Generates both scatter plots with appropriate titles and axis labels.

Areas for improvement:
- The handling of the `species` column for plotting could be more robust. The generated code implicitly relies on `penguins['species']` corresponding to the `principal_components` after NA removal. While it works in this specific case because the `dropna` operation modifies the DataFrame in place, explicitly aligning the `species` column with the processed data, as shown in the ground truth (`original_species_labels = df_cleaned['species']` and then `pca_df['Original_Species'] = original_species_labels.reset_index(drop=True)`), makes the code more resilient to different data processing approaches and clearer about data alignment.
- The plots could benefit from a legend to clarify what the colors represent (K-Means cluster IDs vs. species names). Matplotlib's `scatter` function often requires manual handling of legends when `c` is used for coloring, or one could use a library like Seaborn which automates this.
- While using `plt.scatter` is acceptable, Seaborn, as used in the ground truth, generally produces more aesthetically pleasing plots with less code for common data visualization tasks like scatter plots with hue, as it automatically handles legends and can apply default styles (`sns.set_style`).",gemma-3-12b-it,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.ensemble import GradientBoostingClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Impute missing values
df['culmen_length_mm'].fillna(df['culmen_length_mm'].mean(), inplace=True)
df['culmen_depth_mm'].fillna(df['culmen_depth_mm'].mean(), inplace=True)
df['flipper_length_mm'].fillna(df['flipper_length_mm'...",7,"The generated code correctly addresses most of the task requirements, including loading data, handling missing values for numerical columns and `sex` (with explicit `inplace=True` which is good practice), one-hot encoding, and training the GradientBoostingClassifier. It successfully extracts and prints feature importances.

However, there are a few issues that reduce the score:

1.  **Imputation Order and Dependency:** The imputation of `island` for NA values is done *after* the other imputations (`culmen_length_mm`, etc.). The task specified dropping rows for `island` NA *first*. While this particular order might not drastically change the results in this specific dataset (assuming few island NAs), it's a deviation from the requirement and can be critical if `island` NAs affect other calculations. Also, dropping rows for `island` NAs before imputing numerical columns might result in slightly different means for numerical columns as the sample size for mean calculation would be smaller.

2.  **Imputation of `sex`:** The imputation of `sex` using `df['sex'].fillna(df['sex'].mode()[0], inplace=True)` is correct. However, `island` is imputed by dropping rows `df.dropna(subset=['island'], inplace=True)`, which is also correct, but the order specified in the prompt was to drop `island` first.

3.  **One-Hot Encoding `drop_first=True`:** The `pd.get_dummies` call for both `island` and `sex` uses `drop_first=True`. While this is common practice for avoiding multicollinearity in modeling, it means one category for each feature is dropped. Consequently, the feature importance for the *dropped* category cannot be directly observed from the printed list. The ground truth code explicitly avoided `drop_first=True` to ensure all dummy variables (and thus their importances) are visible, which aligns better with the spirit of ""extract and print the feature importances associating them with their original feature names"" that includes dummy variables.

4.  **Box Plot Visualization:** The box plots are generated correctly *per species*, but the task requested ""For each `species`, create box plots to visualize the distribution of `flipper_length_mm` (using imputed data) *grouped by the imputed `sex` column*."" The generated code creates separate plots for each species, which is fine, but each of those plots shows `flipper_length_mm` vs `sex`. The ground truth provided a single plot that shows `flipper_length_mm` across all species, with `sex` as a hue, which is a more compact and direct way to visualize the interactions. The generated code's approach is acceptable but less efficient in terms of visualization space.

5.  **Code readability for feature importance printing:** While the output of `f'{feature_names[i]}: {importance:.4f}'` is functional, the ground truth's use of a Pandas DataFrame for `importance_df` provides a more structured and readable output, especially when dealing with many features. This is a minor point but contributes to code quality.",gemma-3-12b-it,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer

# Load the dataset
df = pd.read_csv('penguins.csv')

# Impute numerical columns using KNNImputer
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
imputer_numerical = KNNImputer(n_neighbors=3)
df[numerical_cols] = imputer_numerical.fit_transform(df[numerical_cols])

# Impute categorical column 'sex' with the mode
sex_mode = df['sex'].mode()[0]  # Get the first mode in case of multiple ...",8,"The generated code correctly implements all core requirements of the task: loading the data, performing KNN imputation for specified numerical columns, and mode imputation for the 'sex' column. It also verifies the remaining NAs.

Strong points:
- Correctly uses `KNNImputer` with `n_neighbors=3`.
- Correctly identifies and imputes numerical columns.
- Correctly identifies and imputes the 'sex' column with its mode.
- Prints the remaining NA values, addressing the verification step.
- Concise and readable.

Areas for improvement to achieve a higher score:
- **Error Handling (File Not Found):** The ground truth includes a `try-except FileNotFoundError` block for `pd.read_csv`, which is robust practice. The generated code assumes the file will always be present, which could lead to a crash if it isn't.
- **Verification Clarity:** While it prints all NA counts, the ground truth's specific print statements for the *imputed* columns make the verification more explicit and easier to confirm at a glance.
- **Comments/Explanation:** The ground truth includes more comments explaining each step (e.g., why `[0]` is used for mode), which enhances readability and understanding, especially for someone unfamiliar with the code.
- **Mode Calculation Robustness:** If `df['sex']` were entirely NA, `df['sex'].mode()[0]` would raise an `IndexError`. The ground truth checks for an empty mode result, making it more robust. (Although in typical datasets, a column wouldn't be *entirely* NA, it's a good practice for robustness).",gemma-3-12b-it,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
penguins = pd.read_csv('penguins.csv')

# Filter the dataset to include only 'Adelie' and 'Gentoo' species
penguins_filtered = penguins[penguins['species'].isin(['Adelie', 'Gentoo'])]

# Handle NA values in the 'culmen_length_mm' column by dropping rows with NAs
penguins_filtered = penguins_filtered.dropna(subset=['culmen_length_mm'])

# Perform an independent two-sample t-test
adelie_culmen = penguins_filtered[penguins_fi...",9,"The generated code correctly performs all the specified steps for the hypothesis test. It loads the data, filters species, handles NA values, performs the t-test, and interprets the p-value against the significance level. The variable names are clear and the code is straightforward.

**Strong points:**
*   **Correctness:** All core steps are implemented correctly.
*   **Clarity:** The code is easy to read and understand.
*   **Conciseness:** It gets straight to the point without unnecessary complexity.

**Areas for minor improvement (to reach a perfect 10, aligning with best practices for robust data science code):**
*   **Error Handling:** The ground truth includes `try-except` blocks for file loading, which is good practice for robust code that might be deployed or run in varied environments. The generated code assumes the file exists.
*   **Readability/Informative Output:** While the output for the conclusion is good, adding print statements for the shape of the DataFrame at different stages or the number of NAs dropped, as seen in the ground truth, can significantly improve debugging and user understanding, especially for a data science benchmark.
*   **Function Encapsulation:** Encapsulating the logic within a function (as in the ground truth) makes the code reusable and modular, which is considered good software engineering practice.
*   **Comments:** Adding comments, especially above the t-test step to briefly state the null and alternative hypotheses, would further enhance understanding for someone reviewing the code.
*   **`equal_var` parameter in `ttest_ind`:** While `True` is the default for `scipy.stats.ttest_ind`, explicitly setting `equal_var=True` or `False` (depending on assumption) is good practice to make the assumption clear to the reader, or to consider a variance test like Levene's if unsure.

Overall, it's a very good solution that successfully addresses the core requirements of the task. The suggested improvements are mainly for robustness, reusability, and enhanced diagnostic output, not for correctness of the statistical test itself.",gemma-3-12b-it,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# 1. Load the dataset
try:
    penguins = pd.read_csv(""penguins.csv"")
except FileNotFoundError:
    print(""Error: penguins.csv not found in the current directory."")
    exit()

# 2. Define featur...",6,"The generated code correctly performs most of the required steps, such as loading data, defining features/target, performing numerical imputation, training a RandomForest, making predictions, and printing accuracy and classification report.

However, there are significant issues with the handling of categorical features (`island` and `sex`) during preprocessing, particularly in combination with `species` label encoding and data splitting.

1.  **Incorrect One-Hot Encoding and DataFrame Concatenation:**
    *   The `OneHotEncoder` is applied directly to the `penguins` DataFrame *before* `X` and `y` are separated. This modifies the original `penguins` DataFrame in place.
    *   When new OHE columns are created, they are concatenated back to `penguins` *without ensuring index alignment*. `pd.DataFrame(island_encoded, ...)` creates a new DataFrame with a default 0-based index. If the original `penguins` DataFrame has any missing rows, or if the index is not a clean 0-`len-1` range (e.g., after some rows were dropped), this concatenation will be incorrect, leading to misaligned data. `penguins.dropna(inplace=True)` earlier (though not explicitly requested/shown) or an unhandled index issue could lead to this.
    *   The `features` list initially defined still contains `'island'` and `'sex'`, but these columns are dropped and replaced by OHE columns later. The `X = penguins.drop(target, axis=1)` then includes all the new OHE columns, which is correct in principle, but the way `features` is used might be misleading if it was meant to define the *original* columns.
    *   A much more robust way to handle this, as demonstrated in the ground truth, is to use `ColumnTransformer` and build a `Pipeline` for preprocessing `X` *before* the model is trained, or at least apply preprocessing *on X* after X and y are separated and `copy()` is used.

2.  **Order of Operations for Preprocessing and Data Splitting:**
    *   All preprocessing (numerical imputation, sex imputation, OHE, label encoding of target) is done on the *entire* `penguins` DataFrame *before* the `train_test_split`.
    *   While numerical and sex imputation on the whole dataset can be acceptable for SimpleImputer (as it's a ""fit and transform"" process, it uses the global median/mode), applying `OneHotEncoder.fit_transform` on the entire dataset *before* the split means that the encoder is fitted on the entire dataset. This can lead to minor data leakage if the categories in the test set are significantly different or if the structure of the data needs to be preserved for *future* unseen data processing.
    *   Crucially, the `LabelEncoder` for `species` is applied to the full `y` *after* the `X` and `y` distinction, which is fine, but the OHE issue remains.

3.  **Missing Handling of `NaN`s after One-Hot Encoding:**
    *   The `SimpleImputer` for numerical columns fills NaNs. The `SimpleImputer` for 'sex' fills NaNs.
    *   However, if `island` had NaNs (which it doesn't in the standard dataset, but good practice), `OneHotEncoder` with `handle_unknown='ignore'` would create `NaN`s in the encoded output if it encountered new categories *not seen in fit*, or it would drop the column if `handle_unknown='error'`. `SimpleImputer` on categorical columns (specifically for island) is missing. The ground truth pipeline approach handles this more generically.

4.  **Redundant `features` list:** The `features` list is defined but not actually used to select `X`'s columns after the extensive in-place modifications to `penguins`. `X = penguins.drop(target, axis=1)` implicitly selects everything *except* the target, relying on the previous modifications to `penguins`. While it works, it's less explicit.

Overall, the code works for this specific dataset and its NaN patterns, but the preprocessing pipeline is brittle and not ideal. Using `ColumnTransformer` and `Pipeline` as shown in the ground truth code is the robust and recommended approach for handling multiple preprocessing steps on different column types and ensuring proper execution before model training.",gemma-3-12b-it,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Calculate Pearson correlation coefficient between Perfo...",9,"The generated code correctly defines the DataFrame, calculates the specified Pearson correlation coefficients, prints them, and correctly identifies the feature with the stronger absolute correlation with ""PerformanceScore"".

**Strong Points:**
- **Correctness:** All calculations and logical steps are accurate and produce the expected output.
- **Clarity:** Variable names are descriptive and the code is easy to follow.
- **Completeness:** The code addresses all the requirements of the task.

**Areas for Minor Improvement:**
- **Explicit `method='pearson'`:** While `pandas.Series.corr()` defaults to Pearson, explicitly stating `method='pearson'` (as in the ground truth) can improve code clarity and robustness, especially for those less familiar with the default arguments. This makes the code more self-documenting.
- **Handling of Equal Absolute Correlations:** The generated code's `else` block for `stronger_feature` implies that if `abs(correlation_years_experience)` is not strictly greater than `abs(correlation_training_hours)`, then `TrainingHours` is always considered stronger. This doesn't explicitly handle the edge case where the absolute correlations are exactly equal. While a minor detail and often not critical for practical purposes, the ground truth code includes an `elif` and an `else` to specifically address this equality, leading to more precise output in that specific scenario.
- **Output Formatting:** The correlation coefficients are printed without specific formatting (e.g., limiting decimal places). Adding string formatting (e.g., `:.4f` as in the ground truth) can make the output more readable and consistent.

Overall, the generated code is very good and fully functional for the given task. The suggested improvements are minor and relate to best practices and handling edge cases more explicitly.",gemma-3-12b-it,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from lifelines import KaplanMeierFitter, CoxPHFitter
from scipy import stats
import statsmodels.api as sm
import pymc3 as pm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the data
df = pd.read_csv('titanic.csv')

# Data Cleaning and Preprocessing
df['Age'].fillna(df['Age'].median(), inplace=True)
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=T...",2,"The generated code fails significantly in almost all aspects of the complex task requirements.

**Primary Reasons for Failure:**

*   **Kaplan-Meier Survival Analysis (Incorrect Implementation):**
    *   The `KaplanMeierFitter` is applied incorrectly. `kmf.fit(df['Age'], df['Survived'])` is conceptually wrong for survival analysis. `lifelines` expects a `duration` and `event_observed` column. `Age` is a characteristic, not a time-to-event variable. Survival analysis on Titanic data typically uses a *fixed time duration* (e.g., the voyage itself) and `Survived` as the event indicator (0 for event/death, 1 for censored/survived). The prompt specifically asked for ""voyage as time-to-event data (with survival as event indicator)"". The ground truth correctly defines `T` as a fixed duration and `E` as `1 - Survived`.
    *   Plotting `kmf.survival_function_.plot()` for `Age` as duration makes no sense in this context.

*   **Cox Proportional Hazards Regression (Incorrect Implementation & Data):**
    *   Similar to KM, `cph.fit(df, 'Age', 'Survived')` is incorrect. `Age` cannot be the `duration_col` directly in this context for the Titanic dataset. It should be the fixed voyage duration, and `Survived` should be the `event_col`.
    *   The code then applies `cph_multi.fit(df, 'Age', 'Survived')` essentially the same as the base, not demonstrating multivariate analysis beyond what the base model does. There's no clear definition of covariates for the Cox model.
    *   Interaction effects are demonstrated using `cph_interaction.fit(df, 'Age_Sex', 'Survived')`, which again uses a derived variable as the `duration_col`, which is fundamentally wrong. The features should be passed as a DataFrame for a multivariate Cox model.

*   **Propensity Score Matching (Severely Flawed):**
    *   The prompt asked for PSM to ""analyze treatment effects (e.g., passenger class impact)"". The generated code uses `X = df[['Pclass', 'Sex', 'Age']]` and `y = df['Survived']` to predict `Survived`. This is *not* propensity score modeling. Propensity scores are typically the probability of receiving a *treatment* (e.g., being in Pclass 1 vs Pclass 3), *given covariates*. The `LogisticRegression` should predict the *treatment variable*, not the outcome variable (`Survived`).
    *   There is no matching performed *after* calculating propensity scores. It simply predicts scores and stops. The analysis of treatment effect from matched groups, balance checks, or any standard PSM output is entirely missing.

*   **Bootstrap Resampling (Misunderstood & Incomplete):**
    *   The task requested bootstrap resampling for ""confidence intervals and statistical inference"". The generated code attempts to use `statsmodels.stats.bootstrap`. However, the `cox_bootstrap` function's return `cph_boot.hazard_ratios_.to_dict()` indicates it's trying to bootstrap hazard ratios, but the call to `sm_bootstrap.bootstrap` is incorrect.
    *   `sm_bootstrap.bootstrap(data=df, func=cox_bootstrap, n_rep=1000, event_col='Survived', time_col='Age', control_col=['Pclass', 'Sex'])` passes non-standard arguments (`event_col`, `time_col`, `control_col`) to a generic bootstrap function. Its usage is completely wrong for bootstrapping a CoxPH model's coefficients or C-index.

*   **Bayesian Analysis (Conceptual Errors & Impractical Model):**
    *   The PyMC3 model defines `age`, `sex`, and `pclass` as `pm.Normal` and `pm.Bernoulli` *stochastic variables* with means and stds derived from the data. These should typically be *observed data* (the `X` variables) within the `linear_predictor`. The current setup attempts to *model* the distribution of features within the Bayesian framework, which is not how a typical Bayesian regression model (like logistic regression for survival probability) is built.
    *   The `linear_predictor` then uses these stochastic variables instead of the observed data.
    *   `survival_probability = pm.invlogit(linear_predictor)` is used to predict a `Bernoulli` `observed = pm.Bernoulli('observed', p=survival_probability, observed=df['Survived'])`. This structure is closer to a mixed model or hierarchical model for *generating* data, not a standard logistic regression for observed data.
    *   The model structure is not suitable for ""probabilistic modeling of survival factors"" in the way a Bayesian logistic regression or Cox model (if such a PyMC3 implementation were chosen) would be.

*   **Advanced Statistical Tests & Model Diagnostics (Largely Missing):**
    *   Multivariate analysis is claimed but not correctly implemented in Cox model.
    *   Interaction effects are attempted but incorrectly modeled.
    *   Model diagnostics are bare minimum (`print_summary` and `plot_partial_dependencies`, but the latter is conceptually ill-suited for the *incorrectly specified Cox model*). Proportional hazards assumption tests, comparison metrics (C-index), AIC/BIC are missing.

*   **Model Validation & Sensitivity Analysis (Completely Missing):**
    *   The prompt asked for validation of model assumptions and sensitivity analysis. The generated code mentions ""Residual plots, goodness-of-fit tests, and exploring effect of outliers"" but states ""Not detailed here for brevity."" This means this critical part of the task was not implemented at all.

*   **Code Quality & Completeness:**
    *   Imports are okay for the tools chosen but lack `arviz` which is common for PyMC3.
    *   The overall structure is a sequence of attempts, many of which are fundamentally incorrect, rather than a coherent, robust analysis.
    *   Missing `arviz` and `seaborn` from imports, which were in ground truth and generally useful.
    *   `df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)` is standard and missing.
    *   Feature engineering (FamilySize, IsAlone for better models) is missing.
    *   One-hot encoding of categorical variables like `Embarked` and `Pclass` *for modeling* is critical and missing from the generated code, which just uses raw `Pclass` as a continuous variable in some contexts.

In summary, the generated code demonstrates a superficial understanding of the requested techniques and implements them incorrectly for the given dataset and task. It fails to meet the core requirements of advanced statistical modeling and proper survival analysis.",gemma-3-12b-it,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89, 70, 81],...",8,"The generated code correctly addresses almost all aspects of the task.

**Strong points:**
*   Correctly creates the pandas DataFrame.
*   Accurately calculates and prints the Pearson correlation coefficient.
*   Successfully creates the scatter plot with correct x and y axes, title, and legend.
*   Correctly colors the points based on 'Subject' and includes a legend.
*   Accurately adds the 'Overall_Grade' column based on the specified criteria.
*   Correctly calculates and prints the count of students per grade category.
*   Includes all necessary imports (`pandas`, `matplotlib.pyplot`, `scipy.stats`).

**Areas for improvement:**
*   The plot generation could benefit from using `seaborn` as suggested in the prompt's `ground truth code` to simplify coloring by subject and potentially enhance aesthetics. While the current `matplotlib` loop approach works, `seaborn.scatterplot` with `hue` parameter is typically more concise and robust for this kind of task.
*   The `print` statements in the ground truth code for better readability and progress tracking could be added. The generated code only prints the final correlation and grade distribution, without confirming DataFrame creation or plot display.
*   The `default='N/A'` in `np.select` for grade assignment (as seen in ground truth) is a robust practice, although not strictly necessary for this specific problem (all scores fall into a category). `apply` with a function is perfectly fine, but `np.select` is often more performant for larger datasets.
*   The prompt mentioned ""seaborn for potentially better aesthetics but not required for coloring"". While the `matplotlib` loop did achieve the coloring, `seaborn` would have been a more direct and possibly cleaner way to do it.

Overall, it's a very good solution that fulfills the core requirements. The minor points are mostly about best practices or slightly more advanced/efficient approaches rather than critical errors.",gemma-3-12b-it,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']),
        'Ticker': ['TECHCO']*10,
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0],
        'Low': [149.5, 150.5, 1...",7,"The generated code correctly creates the DataFrame, calculates daily returns and both SMAs. It also generates a plot with the required title, labels, and legend, and displays it. However, the identification of bullish crossovers is incorrect. Instead of identifying the *dates* where a crossover occurs (i.e., SMA_3Day was below/equal SMA_5Day on the previous day and is now above), it simply filters for all dates where `SMA_3Day > SMA_5Day`. This is a crucial distinction for technical analysis and means the last part of the prompt is not fully met. To improve, the code needs to implement the logic for detecting a *crossover event* by comparing the current and previous relationships between the two SMAs, similar to the ground truth.",gemma-3-12b-it,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST00...",9,"The generated code correctly addresses all parts of the task. It successfully creates the DataFrame, calculates total sales revenue per product, generates and displays the horizontal bar chart as specified, calculates the average order value, and identifies customers with more than one purchase. The imports are correct, and the overall structure is logical and easy to follow.

Improvements for a perfect score (minor):
- The `Category` column in the initial DataFrame definition is somewhat redundant as all values are 'Electronics'. While not an error per se, it's good practice to ensure all data is relevant for the task. (This is a very minor point and not a direct issue with the code itself, but rather the initial data provided in the prompt).
- The `plt.tight_layout()` call was missing, which can sometimes improve the plot's appearance by preventing labels from overlapping.
- The output for repeat customers could be slightly more descriptive, similar to the ground truth (e.g., explicitly stating ""CustomerID | Number of Purchases""). However, the current output is still clear and correct.
- The ground truth also included print statements for the original DataFrame and product revenue, and added separators (`""========""`) which improve readability for the output, but these are aesthetic choices and not strictly required by the prompt's core tasks.",gemma-3-12b-it,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Create DeviceType_Numeric
df[""DeviceType_Numeric""] = df[""DeviceType""].map({""Desktop"": 0, ""Mobile"": 1})

# Create PagesPerMinu...",9,"The generated code is largely correct and complete. It successfully defines the DataFrame, creates the `DeviceType_Numeric` feature, and handles the division by zero for `PagesPerMinute`. It also selects the correct feature set and prints the head of the resulting DataFrame.

The only minor point of improvement relates to the handling of division by zero:
1.  Using `float('inf')` covers positive infinity, but if negative infinity were possible (not the case here with only positive session durations and pages), `float('-inf')` would also be needed. The ground truth uses `np.inf` and `np.nan` which are more robust and idiomatic for NumPy/Pandas operations.
2.  More importantly, `replace([float('inf'), float('nan')], 0)` specifically targets `inf` and `nan`. However, if `SessionDurationMinutes` could be 0 when `PagesViewed` is also 0, this would result in `NaN` (0/0). The `replace` method in pandas for `NaN` can sometimes be tricky; `fillna(0)` is generally the more robust way to handle `NaN` values that might arise from division, including those from `x/0` (which become `inf` or `-inf`) or `0/0` (which become `NaN`). The ground truth's approach of `replace([np.inf, -np.inf], np.nan, inplace=True)` followed by `fillna(0, inplace=True)` is a more comprehensive and robust way to handle all division by zero scenarios, including `0/0` (NaN) and positive/negative infinity.

Despite this minor nuance in NaN/Inf handling, the code produces the correct output for the given data and a common understanding of division by zero. The structure and clarity are excellent.",gemma-3-12b-it,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26, 45, 28]
}
df = pd.DataFrame(data)
...",9,"The generated code correctly defines the DataFrame, calculates the mean and standard deviation for ""TemperatureCelsius"", identifies outliers using the 1.5 standard deviation rule, and prints the required information for each outlier.

**Strong points:**
*   Correctly initializes the DataFrame as specified.
*   Accurately calculates the mean and standard deviation.
*   Correctly applies the outlier detection logic using the 1.5 standard deviation threshold.
*   Iterates through the identified outliers and prints the `SensorID`, `Timestamp`, and `TemperatureCelsius` as requested.
*   Includes a check for `if not outliers.empty:` which is good practice.

**Areas for minor improvement (to reach a perfect 10):**
*   The `Timestamp` column is printed directly from the DataFrame, which is a string. While this works per the task, converting it to datetime objects (`pd.to_datetime`) at the start, as done in the ground truth, is generally better practice for time-series data and allows for more flexible formatting (e.g., `strftime`). However, for the given task, it's not strictly required and didn't lead to incorrect output here.
*   The output format for the outliers could be slightly more descriptive (e.g., stating it's an ""outlier"" and possibly adding units, as in the ground truth). This is a minor stylistic point, but enhances readability.",gemma-3-12b-it,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [10, 5, 25, 15, 8, 30, 9,...",6,"The generated code correctly defines the DataFrame and calculates the `EngagementScore`. However, it fails on task 2 and 3 because it does not calculate the *average* `EngagementScore` per `PostType` for each `Platform`. Instead, it finds the single `PostID` (row) within each platform that has the absolute maximum `EngagementScore`, regardless of `PostType`, and then incorrectly labels that `PostType` as the ""best"" for the platform. This is a critical misinterpretation of the task requirement to find the `PostType` with the *highest average* `EngagementScore`.

Specifically:
- `df.groupby(""Platform"").apply(lambda x: x.loc[x[""EngagementScore""].idxmax()])` correctly identifies the *row* with the highest `EngagementScore` *within each platform*. However, the task asked for the `PostType` that has the highest *average* `EngagementScore` across all its occurrences on that platform.
- The output then prints the `PostType` and `EngagementScore` of that single highest-scoring post, not the average for a `PostType`. For example, for Instagram, it picks the Video post (SM06) with 450 Likes, which has `EngagementScore` 570. The goal was to compare the average `EngagementScore` of Instagram Images vs. Instagram Videos.

To improve, the code needs to:
1. Group by both 'Platform' AND 'PostType'.
2. Calculate the mean `EngagementScore` for these groups.
3. Then, for each 'Platform', find the 'PostType' within its group that has the maximum *average* `EngagementScore`.",gemma-3-12b-it,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu"", ""Injury"", ...",9,"The generated code correctly addresses all parts of the task. It accurately defines the DataFrame, calculates the ""LengthOfStay"" column, groups by ""Diagnosis"" to compute the average length of stay and patient count, prints this summary, and correctly identifies and prints the diagnosis with the longest average stay.

**Strong points:**
*   **Correctness:** All calculations and identifications are accurate.
*   **Conciseness:** The code is direct and to the point, avoiding unnecessary verbosity while still being clear.
*   **Proper pandas usage:** Utilizes `pd.DataFrame`, `to_datetime`, `.dt.days`, `groupby().agg()`, `idxmax()`, and `max()` effectively.

**Areas for minor improvement (to reach a perfect 10):**
*   **Formatting of printed output:** While `print(diagnosis_stats)` outputs the DataFrame nicely, the ground truth code demonstrates more explicit and formatted printing for each diagnosis, which can sometimes be preferred for readability, especially if the output needs to be presented to a non-technical audience.
*   **Explicit confirmation of requirements:** The ground truth code includes print statements indicating ""Step 1"", ""Step 2"", etc., which explicitly confirms each task requirement is being met. While not strictly necessary for functionality, it can be useful in a benchmark scenario to confirm adherence to all prompts.
*   **Wrapping in a function:** The ground truth wraps the code in a function, which is good practice for reusability and organization, especially in larger scripts. However, for a single script run, the generated code's flat structure is reasonable.",gemma-3-12b-it,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",10,"The generated code is correct, complete, and of good quality. It successfully defines the DataFrame, calculates the ""WeightedRating"" precisely as specified, adds it as a new column, and correctly identifies and prints the title and weighted rating of the movie with the highest weighted rating. All requirements of the task are met. The code is concise and leverages pandas functionalities effectively.",gemma-3-12b-it,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 10000, 8000]
}
df = pd...",9,"The generated code correctly defines the DataFrame, calculates the average ""PriceUSD"" and ""Mileage"" per ""Brand"", and prints the resulting grouped DataFrame, fulfilling all task requirements.

The code is concise and uses the `agg` method appropriately to calculate multiple aggregations.

One minor suggestion for improvement could be to use named aggregations (e.g., `Average_PriceUSD=('PriceUSD', 'mean')`) within the `.agg()` function for more explicit column names in the resulting `grouped_df`, similar to the ground truth. While not strictly necessary for correctness, it can improve readability of the output. However, the current approach is perfectly valid and common.",gemma-3-12b-it,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63, 83],
    ""De...",10,"The generated code is excellent. It correctly defines the DataFrame, adds the ""TotalStats"" column, accurately identifies the Pokemon with the highest ""TotalStats"" and prints its name, and correctly calculates and prints the average ""Speed"" for each ""Type1"". The code is clean, well-structured, and directly addresses all requirements of the task. The use of `df.loc[df[""TotalStats""].idxmax()]` to get the row and then access the 'Name' is an efficient and correct way to retrieve the specific Pokemon's name. The looping through `average_speed_by_type1.items()` for printing is also clear and effective.",gemma-3-12b-it,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score
from scipy.stats import gap_statistic
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
from sklearn.neighbors import NearestNeighbors
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the d...",3,"The generated code attempts to address the task but falls significantly short in several key areas.

**Correctness:**
*   **Missing Imports/Incorrect Imports**: `gap_statistic` is not a standard scikit-learn function; it usually comes from external libraries like `yellowbrick` or needs to be implemented manually. The import for `umap` is also generic (`import umap`) and doesn't explicitly import the UMAP reducer as is best practice (`import umap.umap_ as umap`).
*   **Data Cleaning Errors**: It drops rows with `NaN` in 'Age' and 'Fare' early, which is not ideal for comprehensive Titanic analysis as these NAs are often imputed. More importantly, it applies `LabelEncoder` to 'Embarked' and 'Sex' directly, but this is a categorical feature where one-hot encoding is generally preferred to avoid introducing ordinality. The ground truth uses `OneHotEncoder` and `StandardScaler` within a `ColumnTransformer` and `Pipeline` for robust preprocessing.
*   **Feature Engineering Weakness**: The feature engineering is very basic compared to the ground truth.
    *   Social network analysis is simplified to `FamilySize_Cat` based on `TicketPrefix` which is likely not what was intended by ""family groups"" and ""ticket sharing"" (the ground truth correctly identifies `TicketGroupSize` and `FarePerPerson`).
    *   ""Socioeconomic indicators"" are limited to `FarePerPerson`.
    *   ""Text-based features"" are limited to `NameLength` and `NameTitle` (which is just a categorical encoding of the title). It entirely misses robust `CabinDeck` and `HasCabin` engineering.
    *   It also misses interaction features like `Age_Class`, `Fare_Class`, etc.
*   **Clustering Algorithm Implementation**:
    *   The `GaussianMixture` implementation doesn't use `n_init` or `covariance_type`, which can be important.
    *   DBSCAN implementation is provided but not tested with a range of parameters or validated in the main execution, which is crucial for DBSCAN.
*   **Validation Metrics**:
    *   `gap_statistic` is incorrectly imported/used.
    *   `calinski_harabasz_index` is defined but not actually used in the ""optimal number of clusters"" section.
    *   The only metric used in the main execution for `best_n_clusters` is silhouette score.
*   **Dimensionality Reduction**: `reduce_dimensionality` only uses PCA. It introduces t-SNE and UMAP imports but doesn't use them in the `reduce_dimensionality` function or in the subsequent visualization, which was a core requirement.
*   **Cluster Stability Analysis**: The `cluster_stability` function is flawed. It shuffles the data and then attempts to compare the labels. Stability analysis typically involves running the same clustering algorithm multiple times with *different initializations* (for K-means) or *resampled data* (bootstrapping for other algorithms) and comparing the resulting cluster assignments using metrics like Adjusted Rand Index (ARI) or Jaccard index. The current implementation compares labels of data that has been re-indexed against the original labels based on a sum of exact matches, which is not a robust stability measure and ignores assignment permutations.
*   **Consensus Clustering**: It merely provides a placeholder, failing the requirement.
*   **Cluster Analysis/Survival Patterns**: `analyze_cluster_characteristics` attempts to group `Survived` by cluster labels (which are just a K-means output). This function is incorrect as it passes `labels` (which is a 1D array of cluster assignments) as a column name for groupby and then tries to take the mean of `Survived` within that fake column, leading to errors. It should be merging the cluster labels back to the DataFrame and then grouping by the *actual cluster column*.
*   **Visualization**: Visualizations are very limited – only count/bar plots are shown manually, missing the crucial scatter plots of clusters in reduced dimensions (PCA, t-SNE, UMAP) and detailed feature distributions (violin plots) or feature importance heatmaps for each segment. The ground truth has comprehensive visualizations.
*   **External Validation**: The ""validation"" section is incomplete, only suggesting `LabelEncoder` for validation and hinting at predicting survival, but doesn't actually implement any external validation or comparison of cluster assignments against survival outcomes systematically.

**Code Quality:**
*   **Lack of Structure**: The code is somewhat monolithic, without clear separation of concerns compared to the ground truth which uses pipelines and well-factored functions and sections.
*   **Redundant Imports**: Some imports are present but not used (e.g., `NearestNeighbors`).
*   **Missing Error Handling**: No `try-except` for file loading.
*   **Inconsistent Naming**: `titanic` DataFrame is modified in place, which can lead to confusion.
*   **Hardcoded Parameters**: `eps` and `min_samples` for DBSCAN are hardcoded, with no guidance on how to find them.

**Completeness:**
*   **Feature Engineering**: Significantly less advanced and less robust than required. Missing socioeconomic indicators beyond basic fare per person, advanced text features (e.g., CabinDeck), and interaction terms.
*   **Multiple Clustering Algorithms**: While functions are defined, only K-means is fully utilized in the main execution `if __name__ == '__main__':` block for finding optimal clusters and subsequent analysis. Hierarchical, DBSCAN, and GMM are largely unused in the main flow.
*   **Optimal Cluster Determination**: Only uses Elbow method and Silhouette score for K-means. Missing Calinski-Harabasz and Gap statistic (which is broken) for all algorithms, and a more structured visual analysis of these metrics.
*   **Dimensionality Reduction**: Only PCA is used for reduction. t-SNE and UMAP are imported but not applied or visualized.
*   **Cluster Characteristics Analysis**: The provided function `analyze_cluster_characteristics` is fundamentally flawed.
*   **Cluster Stability & Consensus Clustering**: Stability analysis is incorrect and incomplete. Consensus clustering is a placeholder.
*   **Comprehensive Visualizations**: Very limited, missing 2D scatter plots of clusters on reduced dimensions, violin plots for feature distributions, and heatmaps of scaled feature means.
*   **External Validation**: Very basic and incomplete.

Overall, the generated code provides a skeletal structure but fails to implement most of the requested advanced functionalities correctly or comprehensively. Many critical components are either missing, incorrect, or merely placeholders.",gemma-3-12b-it,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dystopian""],
    ""Stock"": [...",9,"The generated code correctly performs all requested tasks: creating the DataFrame, calculating book count and average price per author, and identifying the author with the most books in stock. The use of `.agg()` for multiple aggregations and `.idxmax()` for finding the author with the most stock is efficient and idiomatic pandas.

**Strong points:**
*   **Correctness:** All calculations and identifications are accurate.
*   **Conciseness:** The code is clean and to the point.
*   **Pandas Idioms:** Uses `groupby().agg()` and `idxmax()` effectively which are good pandas practices.

**Areas for minor improvement:**
*   **Printing Format for Author Stats:** While `print(author_stats)` outputs the correct data, the ground truth's formatted loop provides a more readable and aesthetically pleasing output for the ""Author Book Statistics"" section, especially for a user-facing report. The generated code's output is a standard DataFrame representation which is functional but less ""pretty"".
*   **Output Clarity:** Adding a title for the first printed section (e.g., ""Author Statistics:"") is good, but separating the sections with newlines or clear headers like in the ground truth can improve overall readability when the script is run.

Overall, a very good solution that successfully meets almost all requirements.",gemma-3-12b-it,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
fr...",5,"The generated code attempts to address many aspects of the prompt but falls short in implementation details, resulting in a less advanced and less robust system than required.

**Correctness:**
*   **Data Preprocessing:** The imputation strategy is incorrect. It applies `SimpleImputer` (mean) then `KNNImputer` sequentially on the **same data**, which is redundant and doesn't represent ""multiple imputation strategies"" as intended (e.g., MICE). The outlier treatment is basic Winsorization but not truly ""advanced"" by modern standards. Feature engineering is attempted but the concatenation with `df_poly` is problematic as it adds polynomial features back to the original `df` with `numeric_cols` already present, leading to potential multicollinearity and incorrect feature handling. The interaction features are also added directly to `df` without proper integration into a pipeline or consideration of scaling.
*   **Model Optimization:** `GridSearchCV` is used correctly for base models. However, the models are trained directly on `X_train` which has not been properly preprocessed by a consistent pipeline. The `PolynomialFeatures` and interaction terms are added to the *original* DataFrame (`df`) *before* the train-test split and scaling, which is a major data leakage issue and incorrect handling of features. The `SVC` model requires `probability=True` for ROC AUC, which is missing.
*   **Ensemble Models:**
    *   **Stacking:** The `StackingClassifier` is correctly instantiated with `list(models.values())`, but these models are fitted on raw `X_train` and the meta-learner `final_estimator` is also fitted on this raw data. It does not correctly handle the meta-features (predictions of base models) for the final estimator.
    *   **Voting:** `VotingClassifier` is used, but it's important that base estimators support `predict_proba` for 'soft' voting, and `SVC` by default does not without `probability=True`.
    *   **Blending:** The blending implementation is fundamentally flawed. It calculates `predictions` from `X_train` and then fits a `LogisticRegression` directly on `X_train` itself. This is not blending. Blending requires creating out-of-fold predictions from a validation set (`X_blend_val`) and using those as inputs to the meta-learner, and then making predictions on the final test set using base models trained on the *full* training set and feeding those to the meta-learner. The current implementation is essentially just training a logistic regression on `X_train`.
*   **Advanced Evaluation:**
    *   **Stratified K-Fold:** Used in `GridSearchCV` but the subsequent ""Advanced Evaluation"" section re-implements a manual cross-validation loop on `X_train` which is less robust than using `cross_val_score` or similar functions, and it seems to be evaluating models (including ensembles) on `X_train` itself, not in a proper nested CV or on the held-out test set.
    *   **Learning/Validation Curves:** The implementation is highly unusual and incorrect. It plots `[i, 100]` for `n_estimators` `[train_auc, test_auc]`, which is not how learning/validation curves are typically generated. They should plot score vs. training set size or hyperparameter value. The plots generated are not standard learning/validation curves.
    *   **Bootstrap Sampling:** Missing completely.
*   **Model Interpretability (SHAP):** The SHAP usage is barebones, only showing `summary_plot`. `dependence_plot` and other advanced SHAP features (e.g., for specific instances) are missing.
*   **Statistical Significance Testing:** `ttest_rel` is used, but the `scores[name]` are just single average ROC AUC scores from the custom CV loop, not the individual fold scores needed for a paired t-test. A paired t-test requires a set of scores for each model from the *same folds*. The ground truth correctly stores CV scores per fold.
*   **Model Comparison Dashboard:** Very minimal, just printing average scores. No comprehensive dashboard as described.
*   **Model Calibration:** `calibration_curve` is used, but the plotting is basic and doesn't include the Brier score or comparison with a perfectly calibrated line explicitly labeled.

**Code Quality:**
*   **Imports:** Generally good, but `from xgboost import XGBClassifier` assumes `xgboost` is installed, which is fine but robust handling in the ground truth is better.
*   **Structure:** The `water_potability_system()` function wraps everything, which is okay, but it lacks modularity. Large blocks of logic are not separated into distinct functions.
*   **Pipelines:** Crucially, the preprocessing steps are applied directly to the DataFrame and not integrated into `sklearn.pipeline.Pipeline` or `ColumnTransformer`. This is a major drawback for advanced systems, making the code less reproducible, prone to data leakage, and harder to scale. The ground truth uses `Pipeline` and `ColumnTransformer` effectively.
*   **Error Handling:** Lacks robust error handling (e.g., for `FileNotFoundError`).

**Completeness:**
*   **Multiple Imputation Strategies:** Not fully met (only mean and KNN sequentially, not truly distinct strategies like MICE or different methods).
*   **Outlier Detection and Treatment:** Basic Winsorization, lacks advanced methods.
*   **Advanced Feature Engineering:** Polynomial features are added without scaling consideration. Interaction features are hastily added. The naming convention for polynomial features (`poly_i`) is generic and unhelpful.
*   **Optimization:** Uses `GridSearchCV`, but Bayesian Optimization is not used.
*   **Ensemble Methods:** Stacking and Voting are present but flawed. Blending is fundamentally incorrect.
*   **Advanced Evaluation:** Stratified K-Fold is used, but learning/validation curves are incorrect. **Bootstrap sampling for confidence intervals is entirely missing.**
*   **Model Interpretability (SHAP):** Basic SHAP summary plot only, lacks depth. LIME is missing.
*   **Statistical Significance Testing:** Attempted but faulty due to incorrect data for t-test.
*   **Comprehensive Model Comparison Dashboard:** Largely missing. No performance metrics dashboard, feature importance (beyond SHAP plot), or prediction confidence intervals.
*   **Model Calibration:** Basic calibration plot, but lacks the brier score comparison and clarity of the ground truth.

In summary, the generated code provides a high-level outline of the requested features but struggles with accurate and advanced implementation details for almost all of them. The lack of proper pipelines for preprocessing and the incorrect blending implementation are significant flaws.",gemma-3-12b-it,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pymc3 as pm
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd....",6,"The generated code attempts to address all parts of the prompt but lacks completeness, correctness, and robustness in several key areas.

**Strengths:**
*   **Structure:** The code is generally well-structured with clear sections for each major task.
*   **Imports:** Most necessary libraries are imported.
*   **Anomaly Detection:** Implements the three requested anomaly detection algorithms (Isolation Forest, LOF, One-Class SVM).
*   **PCA and Factor Analysis:** Includes basic implementations of PCA and Factor Analysis.
*   **Basic Visualizations:** Includes a pairplot, anomaly distribution plot, and correlation heatmap.

**Weaknesses and Areas for Improvement:**

1.  **Missing Data Imputation:**
    *   **Incorrect Column Listing:** The code attempts to impute `['ph', 'hard','solids','classes']` but `hard` and `classes` are not column names in `water_potability.csv` (they are `Hardness` and there is no 'classes' column). This leads to `KeyError` or silently incorrect imputation.
    *   **""MICE"" Implementation:** `SimpleImputer(strategy='median')` is *not* MICE. MICE (Multiple Imputation by Chained Equations) is a sophisticated technique. While `IterativeImputer` from `sklearn.experimental` is a MICE-like approach, the prompt explicitly asked for MICE, KNN, and Iterative imputer *and to compare their effectiveness*. Only mentions the techniques but doesn't perform comparison.
    *   **`df_` usage:** Uses `df_` consistently instead of `df` in imputation sections, which would cause `NameError`.
    *   **Lack of Comparison:** No comparison of imputation effectiveness is performed, which was a specific requirement.

2.  **Multivariate Statistical Models:**
    *   **Logistic Regression with Regularization:** The prompt explicitly asked for *regularization* (L1, L2). The `statsmodels.formula.api.logistic` model does not directly support L1/L2 regularization readily. Scikit-learn's `LogisticRegression` (with `penalty` parameter) is the standard way to do this.
    *   **GAM (Generalized Additive Models):**
        *   Uses `pygam.GammaRF` which is a Random Forest model, not a Generalized Additive Model. The prompt requested **GAM** (e.g., using `pygam.LogisticGAM` or `statsmodels.GAM`). This is a critical misunderstanding of the model type.
        *   `X` for GAM: `sm.add_constant(X)` is typically for `statsmodels` linear models, not `pygam`. `pygam` uses `s()` for smoothing terms.
    *   **Bayesian Logistic Regression (PyMC3):**
        *   The model structure is incomplete for a logistic regression: `p` is defined as `pm.Binomial('p', n=1, p=pm.invlogit(mu))`. This defines `p` as the observed data (likelihood), which is incorrect for a binary outcome. It should be `pm.Bernoulli('Y_obs', p=pm.invlogit(mu), observed=y)`.
        *   The variables `df['ph']`, `df['hard']`, etc., are used directly in the `mu` calculation within the model, but `df` might have missing values. Data should be preprocessed (imputed and potentially scaled) *before* being passed to `pymc3`.
        *   No post-sampling analysis like `traceplot` or `summary` from `arviz` was included, which is standard for MCMC results. No prediction on test set.

3.  **Advanced Hypothesis Testing:**
    *   **Multivariate ANOVA:** The `sm.stats.anova_lm` function is for univariate ANOVA, not Multivariate ANOVA (MANOVA). The prompt specifically asked for MANOVA. `statsmodels.multivariate.manova.MANOVA` is needed.
    *   **Permutation Tests:**
        *   The `permutation_test` function calculates permutation for correlation between `x` and `y`, not for mean differences between groups as is common in permutation tests for hypothesis testing. The example using `df['ph']` and `df['Potability']` attempts to find correlation between a continuous variable and a categorical variable, which is not the standard application for mean difference testing.
        *   `scipy.stats.permute` is not a direct function for permutation testing in the observed manner.
    *   **Bootstrap Confidence Intervals:**
        *   Similar to permutation tests, the `bootstrap_ci` function calculates CI for *correlation* between `x` and `y`, not for a single statistic like the mean of a variable. This is incorrect for the implied task.

4.  **Robust Statistical Methods:**
    *   Only a conceptual example of using median is provided; no actual robust *statistical modeling* (like `statsmodels.robust.robust_linear_model.RLM`) is demonstrated, which was implied by the ""robust statistical methods"" requirement beyond simple descriptive statistics.

5.  **Model Diagnostics:**
    *   **Q-Q Plots, Leverage Plots:** These were explicitly mentioned in the prompt but not included in the generated code.
    *   **Goodness-of-Fit Tests:** No specific goodness-of-fit tests are implemented beyond printing the `summary` table (which of course contains AIC/BIC/deviance, but no explicit test like Hosmer-Lemeshow).
    *   **Residuals/Influence:** While it plots Pearson residuals and Cook's distance, it misses the leverage plot (`plot_leverage_resid2`) and a more comprehensive summary of influence points.

6.  **Advanced Visualizations:**
    *   While some visualizations are present (pairplot, heatmap, anomaly distribution), the prompt asked for ""diagnostic dashboards"" and specific plots like Q-Q plots, leverage plots, which were missed during model diagnostics.

7.  **General Correctness Issues:**
    *   Many variables are implicitly used (e.g., `df_` in imputation, incorrect column names in `df_mice[['ph', 'hard','solids','classes']]`) leading to runtime errors.
    *   Lack of `warnings.filterwarnings('ignore')` which the ground truth code used to suppress verbose outputs common in `statsmodels` etc.
    *   The code uses `df.corr()` after adding PCA/FA components and anomaly scores, which are new columns. This changes the correlation matrix interpretation.

The generated code provides a skeleton but needs significant debugging, conceptual correction, and completion to meet the prompt's advanced requirements.",gemma-3-12b-it,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, GaussianMixture, SpectralClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from scipy.stats import gap_statistic
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.manifold import TSNE, UMAP
import matplotlib.pyplot as plt
import seaborn as sns
impor...",4,"The generated code attempts to address many aspects of the task, but falls short in several critical areas, particularly in correctness, completeness, and expert-level implementation.

**Correctness:**
- **Feature Engineering:** The engineered features (`pH_alkalinity`, `hardness_temp_ratio`, `contaminant_risk`, `potability_score`) are overly simplistic and not aligned with standard water quality indices or chemical understanding. `potability_score` is directly derived from `Potability` which is then immediately dropped, rendering it useless for clustering `X`. The `contaminant_risk` adds 'Iron' which is not in the dataset. There's no proper handling of missing values before feature engineering or scaling.
- **`gap_statistic`:** The `gap_statistic` function used from `scipy.stats` does not exist. The standard gap statistic calculation is more complex and typically involves random reference datasets, which is not implemented. This is a critical mathematical error.
- **DBSCAN parameters:** The `perform_clustering` function's DBSCAN call `DBSCAN(eps=params['eps'], min_samples=params['min_samples'], **params)` is redundant and might lead to errors if `eps` or `min_samples` are also in `**params`. More importantly, DBSCAN requires careful parameter tuning (eps and min_samples), which is not performed, leading to potentially poor results or misinterpretation.
- **Cluster Stability Analysis:** The `cluster_stability_analysis` function only performs bootstrap resampling with KMeans and returns a list of labels but doesn't *analyze* stability (e.g., using ARI across resamples or co-occurrence matrices) as required by the prompt's ""consensus clustering"" and ""bootstrapped resampling"" for stability. It's a very basic re-run and not a stability analysis.
- **UMAP Import:** UMAP is imported from `sklearn.manifold` which is incorrect. UMAP is a separate library (`umap-learn`).
- **Missing Value Imputation:** No explicit missing value imputation is performed, which is crucial for real-world datasets and can significantly impact clustering results. The ground truth includes this.
- **`create_dashboard` Radar Chart:** The radar chart tries to use `df.columns` as theta, but `radar_data[i]` contains mean values of *all* columns, which won't map correctly to a meaningful radar chart without normalization or feature selection.

**Completeness:**
- **Advanced Feature Engineering:** While some features are engineered, they are not ""advanced"" water quality indices or chemical balance ratios as specified. Contamination risk score is just a sum, not considering actual health standards or non-linear effects. Temporal patterns are not addressed (understandably, as the dataset is static, but a note on this would be expert).
- **Optimal Number of Clusters:** While `validate_clustering` is defined, the overall code does not actively use it to *determine* the optimal number of clusters for *all* algorithms. It simply uses a hardcoded `n_clusters = 3`. The plotting for validation metrics (Elbow, Silhouette, etc.) is missing.
- **Stability Analysis:** The stability analysis is severely lacking, as noted under correctness. Consensus clustering is completely absent.
- **Cluster Profiling:** The code defines `validate_with_standards` and `anomaly_detection` functions but leaves them as empty placeholders with `pass`. This fails the requirement for comprehensive water quality profiling and cluster-based anomaly detection. The ground truth analyzes profiles and performs anomaly detection.
- **Visualization:** While 2D/3D plots and radar charts are attempted, they are not as comprehensive or well-implemented (e.g., proper hover data, titles, meaningful radar charts) as in the ground truth.
- **External Validation:** The external validation using domain knowledge is just a placeholder.
- **Anomaly Detection:** Anomaly detection is a placeholder function. The ground truth uses IsolationForest and distance from centroid.

**Code Quality:**
- **Structure:** The code is functionalized, which is good, but the main execution part is very brief and does not demonstrate the full workflow implied by the functions.
- **Clarity and Comments:** Lacks sufficient comments for complex sections.
- **Error Handling:** No robust error handling (e.g., for file not found).
- **Hardcoding:** Critical parameters like `n_clusters` are hardcoded without demonstrating how to derive them.

Overall, the generated code provides a basic framework that addresses some parts of the prompt but fails to deliver on the ""expert"" and ""comprehensive"" nature of the task. Many critical steps are either missing, incorrect, or implemented only as placeholders.",gemma-3-12b-it,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
import numpy as np

# Load the dataset
try:
    wine_data = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: winequality-red.csv not found in the current directory."")
    exit()

# Calculate the correlation matrix
correlation_matrix = wine_data.corr()

# Print the correlation with 'quality'
quality_correlation = correlation_matrix['qu...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, calculates correlations, identifies the top positive and negative correlations, generates a correlation heatmap, and creates scatter plots with linear regression lines for the most correlated features.

Strong points:
- Correctly uses pandas for data loading and correlation calculation.
- Effectively uses seaborn for heatmap and scatter plots.
- Correctly identifies and prints the top 3 positive and negative correlations.
- Successfully implements linear regression using `sklearn.linear_model.LinearRegression` and plots the regression line.
- The use of `nlargest` and `nsmallest` is a concise way to get the top/bottom correlations.

Areas for minor improvement (not critical errors, but good practices):
- The `top_positive` and `top_negative` correlation calculations include 'quality' itself (i.e., 'quality' vs 'quality' correlation is 1). While `nlargest(3)` will naturally pick the next best positive correlations if 'quality' is first, it's generally cleaner to drop the self-correlation of 'quality' from `quality_correlation` before finding top positive/negative, as done in the ground truth. This is a subtle point but leads to more accurate identification of actual feature correlations.
- For the scatter plots, the ground truth uses `scipy.stats.linregress` which also returns p-values and standard errors, providing more statistical insight, although `sklearn.linear_model.LinearRegression` is perfectly valid and sufficient for fitting the line and calculating R-squared.
- The plots could benefit from minor touches like `plt.tight_layout()` to prevent labels/titles from overlapping, and explicitly setting DPI for higher resolution plots if needed, though this is cosmetic.
- The error handling for `FileNotFoundError` is present, which is good. Additional general error handling for data loading could be considered (e.g., if the CSV is corrupted or malformed), but for this specific task, `FileNotFoundError` is the most common issue.",gemma-3-12b-it,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
try:
    df = pd.read_csv(""video_games_sales.csv"")
except FileNotFoundError:
    print(""Error: video_games_sales.csv not found. Make sure it's in the current directory."")
    exit()

# Clean missing values
df = df.dropna()

# 1. T-test: Nintendo vs. Other Publishers
nintendo_sales = df[df['Publisher'] == 'Nintendo']['Global_Sales']
other_sales = df[df['Publisher'] != 'Nintendo']['Global_Sales']

t_stat, p_value = stats.tte...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being production-ready or fully correct for a data science benchmark.

**Strengths:**
*   **Correct Imports:** Uses `pandas` and `scipy.stats` as required.
*   **Error Handling for File:** Includes a `try-except` block for `FileNotFoundError`.
*   **Basic Test Implementation:** Correctly identifies and uses `ttest_ind`, `chi2_contingency`, and `mannwhitneyu`.
*   **Interpretation:** Provides a basic interpretation based on the p-value for each test.
*   **Top 5 Filtering:** Correctly implements the logic to filter for top 5 genres and platforms for the chi-square test.

**Weaknesses and Areas for Improvement:**

1.  **Missing Values Handling (Major Issue):** The ground truth performs granular missing value cleaning (`dropna` on specific columns like `Global_Sales`, `Year_of_Release`, `Publisher`, `Genre`, `Platform`) which is crucial for the tests. The generated code uses a single `df.dropna()` which drops *any* row with *any* NaN. This is likely to remove many more rows than necessary, potentially skewing the results and losing valuable data. For example, if a 'User_Score' is missing, the row would be dropped even if 'Global_Sales' and other relevant columns are present. The ground truth also explicitly handles `Year_of_Release` conversion to `int` after dropping NaNs, which the generated code misses when dealing with `Year` (see point 5).

2.  **Missing `Year_of_Release` `int` Conversion:** The generated code attempts to use a column named `Year` (`df['Year'] = df['Year'].fillna(0).astype(int)`). The dataset has a `Year_of_Release` column. This indicates a potential misunderstanding of the dataset columns or a typo that would lead to an error or incorrect results in the Mann-Whitney U test. The ground truth correctly references `Year_of_Release` and converts it to `int` *after* dropping NaNs specific to that column.

3.  **T-test `alternative` (Minor):** For the Nintendo test, ""significantly higher"" implies a one-sided test (`alternative='greater'`). The generated code uses the default two-sided test, which is a conceptual mismatch with the specified hypothesis. The ground truth correctly uses `alternative='greater'`.

4.  **No Checks for Group Size (Robustness):** The generated code does not include checks for whether the groups created for t-tests or Mann-Whitney U tests have enough data points (e.g., `len(series) < 2`). This can cause errors or misleading results if a filter leads to an empty or single-element series. The ground truth includes these crucial checks.

5.  **`equal_var` in T-test (Robustness):** The ground truth correctly uses `equal_var=False` (Welch's t-test), which is generally recommended unless equal variances are strictly assumed, as it's more robust. The generated code uses the default `equal_var=True`, which is less robust.

6.  **Redundant Column Assignment (`df['Year']`):** The line `df['Year'] = df['Year'].fillna(0).astype(int)` is problematic in two ways:
    *   It assumes a 'Year' column exists. The actual column is 'Year_of_Release'.
    *   It `fillna(0)` which is incorrect if the `Year_of_Release` column has actual missing values. It should be dropped, then converted. The ground truth drops missing `Year_of_Release` values and then converts.

7.  **Interpretation Clarity:** While basic interpretation is provided, the ground truth's approach of explicitly stating the null and alternative hypotheses before the result is clearer and more aligned with formal hypothesis testing reporting.

To improve, the developer should focus on:
*   A more robust and precise data cleaning strategy, similar to the ground truth, particularly for `Year_of_Release` and specific `dropna` subsets.
*   Correctly referencing column names (`Year_of_Release` instead of `Year`).
*   Adding checks for group sizes before performing statistical tests.
*   Considering the `alternative` parameter for one-sided tests when implied by the hypothesis.
*   Using `equal_var=False` for t-tests for better robustness.",gemma-3-12b-it,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import numpy as np

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values
df = df.dropna()

# Create the target variable 'High_Sales'
df['High_Sales'] = np.where(df['Global_Sales'] > 1...",6,"The generated code correctly addresses most of the requirements but has critical issues in data handling and feature selection that affect its correctness and completeness.

**Correctness Issues:**
1.  **Missing Value Handling:** The generated code uses `df.dropna()` without specifying a subset. This will drop any row with *any* NaN value. The ground truth, correctly, specifically drops NaNs from crucial columns like `Global_Sales`, `Year`, `Publisher`, and regional sales, while other NaNs (e.g., 'Game' or 'Developer' if present) might be less critical or handled differently. Moreover, the prompt implicitly assumes 'Year' is clean enough to be used directly after `dropna()`, but in real datasets, 'Year' might also have NaNs or non-numeric values requiring specific handling before subtraction. The ground truth correctly converts 'Year' to `int` after dropping NaNs.
2.  **`High_Sales` Creation:** While `np.where` works, the `.astype(int)` used in the ground truth is a more direct and common pandas way to convert boolean series to 0/1 integers. This is a minor point, but worth noting.
3.  **Feature Selection for X:** The generated code `X = df.drop(['Game', 'High_Sales'], axis=1)` is problematic. It assumes that 'Game' is the only other non-feature column besides 'High_Sales'. More importantly, it *includes* original sales columns (`NA_Sales`, `EU_Sales`, etc.) in `X` for encoding, which are numerical and should not be label-encoded. The ground truth carefully defines `features` to ensure only correct columns are used for training after appropriate transformations. The generated code implicitly uses all remaining columns, which might include unintended ones or numerical columns that were already sales figures. This leads to potential data leakage if 'Global_Sales' (or a transformed version) is still present.
4.  **`Years_Since_Release`:** The ground truth includes a check `df['Years_Since_Release'] = df['Years_Since_Release'].apply(lambda x: max(0, x))` to handle potential negative values if `CURRENT_YEAR` were less than `Year`, which is good practice. The generated code omits this.
5.  **Train-Test Split Stratification:** The ground truth uses `stratify=y` in `train_test_split`, which is crucial for binary classification tasks, especially when one class might be imbalanced, to ensure that both training and testing sets have a similar proportion of the target classes. The generated code omits this, which can lead to less reliable model evaluation, especially if 'High_Sales' is imbalanced.

**Code Quality and Completeness:**
1.  **Imports:** All necessary imports are present.
2.  **Structure:** The code is logically structured, following the steps outlined in the prompt.
3.  **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError`, which is a good practice for robust code. The generated code does not.
4.  **Feature Importance Display:** The approach to sort and print feature importances in the generated code is correct, but iterating and printing each one separately is less concise than the ground truth's use of a Pandas Series and `sort_values`, which provides a cleaner output format. The ground truth's method is more idiomatic pandas.
5.  **Missing Comments:** The ground truth includes comments explaining each section and crucial decisions (like missing value handling strategy), which is good for readability and maintainability.

Overall, the generated code provides a functional approach but sacrifices robustness and best practices in data preprocessing, particularly regarding missing values, feature selection, and train-test split strategy, which are critical for model performance and reliability.",gemma-3-12b-it,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures
from sklearn.impute import SimpleImputer

# Load the dataset
try:
    df = pd.read_csv('video_games_sales.csv')
except FileNotFoundError:
    print(""Error: video_games_sales.csv not found in the current directory."")
    exit()

# 1. Handle missing values
im...",6,"The generated code addresses most of the task requirements but has several issues in implementation and completeness.

**Correctness:**
- **Missing Value Handling:** The generated code only imputes sales columns using `mean` strategy. The ground truth handles 'Year', 'Publisher', and 'Genre' (crucial for feature engineering) specifically using appropriate strategies (median for Year, mode for categorical). This is a significant miss.
- **`Platform_Popularity`:** The generated code calculates `value_counts()` which gives the count for *each unique platform*, but it then merges this back to the original `df`. This is correct in principle, but the Ground Truth uses `transform('count')` which correctly broadcasts the count to *every row* belonging to that platform, which is generally what's desired for a new feature.
- **`Genre_Market_Share`:** The generated code calculates `genre_sales` correctly but then applies `(genre_sales['Global_Sales'] / total_sales) * 100` without merging it back to the original `df` in a broadcasted way, leading to incorrect or incomplete feature assignment on the main DataFrame. The ground truth's use of `transform('sum')` is correct for this.
- **`Year` type for Sales Momentum:** The generated code converts 'Year' to `int` *after* the initial missing value handling, but the `SimpleImputer` wasn't applied to Year. If Year had NaNs and wasn't handled, `astype(int)` would fail.
- **PCA Input:** The generated code drops 'Global_Sales' from `numerical_features` for PCA. While it's common to exclude target variables, the prompt did not specify this, and the ground truth uses all relevant numerical features for PCA. The prompt asked for ""PCA to reduce dimensionality and explain variance"" generally for numerical features, not for a specific model building purpose.
- **Polynomial Features:** The generated code correctly applies polynomial features. However, it seems to re-index the `poly_df` implicitly during concatenation, which might introduce mismatches if the original `df` had a non-default or non-contiguous index *after* any `dropna` calls. The ground truth explicitly uses `index=df.index` for `poly_df` to ensure alignment.
- **Interaction Features:** The generated code creates a string concatenation feature `'Year_Genre' = df['Year'].astype(str) + '_' + df['Genre']`. This is a categorical interaction feature. The ground truth explicitly creates *numerical* interaction features by multiplying `Year` with one-hot encoded `Genre` columns, which is typically what's implied by ""interaction features"" in the context of advanced numerical data analysis (e.g., for regression models). This is a conceptual difference.
- **Outlier Handling:** The outlier handling is correct for the specified sales columns. However, it only applies to sales columns, while other engineered numerical features (like `Sales_Ratio_NA_EU`, `Publisher_Avg_Sales`, `Genre_Market_Share`, `Sales_Momentum`) could also benefit from outlier treatment. The ground truth includes these.

**Code Quality:**
- **Imports:** Imports are generally correct.
- **Structure:** The code is structured well with comments indicating the task sections.
- **Documentation:** Lacks print statements to show intermediate results or confirm completion of steps, which makes debugging and understanding the flow harder compared to the ground truth.
- **Hardcoding:** No explicit `head()` or `info()` calls after significant transformations (like feature engineering or outlier handling) to verify changes, which is good practice.

**Completeness:**
- **Missing Value Handling:** Incomplete as noted above.
- **Engineered Features:** `Publisher_Avg_Sales`, `Genre_Market_Share`, and `Sales_Momentum` implementations are flawed in how they are merged or assigned back, leading to incomplete/incorrect features on the main dataframe. `Platform_Popularity` could be more robustly added.
- **PCA:** Only plots explained variance of components, not the cumulative explained variance plot which is more informative for choosing the number of components.
- **Polynomial Features:** Correctly implemented.
- **Interaction Features:** Conceptually different from the numerical interaction features usually expected in such contexts.
- **Outlier Handling:** Incomplete in terms of which columns are processed.

Overall, the generated code attempts to address all points but often misses nuance or provides incomplete implementations, especially in the feature engineering and missing value handling sections. The ground truth provides more robust and precise solutions for these steps.",gemma-3-12b-it,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load the dataset
df = pd.read_csv(""diabetes.csv"")

# Clean zero values
df = df.replace(0, np.nan)

# Impute missing values with median
for col in df.columns:
    if df[col].isnull().any():
        df[col] = df[col].fillna(df[col].median())

# Test glucose levels between diabetic and non-diabetic
diabetic = df[df['Outcome'] == 1]['Glucose']
non_diabetic = df[df['Outcome'] == 0]['Glucose']
t_stat, p_value_glucose = stats.t...",7,"The generated code correctly performs most of the required statistical tests and data preparation steps. However, it lacks comprehensive interpretation of the results with respect to the significance level (alpha=0.05) for each test, which was explicitly requested. Additionally, the handling of zero values during the data cleaning step is too broad; some columns (like 'Pregnancies' and 'Outcome') can legitimately have zero values, but the code replaces all zeros with NaN, which is incorrect. The generated code also doesn't provide the structured print statements and detailed feedback on the interpretation of results as seen in the ground truth, which is a key part of the task.

Specific areas for improvement:
- **Data Cleaning:** The replacement of all zeros with NaN (`df = df.replace(0, np.nan)`) is an oversimplification. 'Pregnancies' and 'Outcome' can legitimately be 0 and should not be imputed. The ground truth correctly identifies specific columns for imputation.
- **Interpretation of Results:** The prompt explicitly asked to ""Display p-values and interpret results with significance level α=0.05"". The generated code only prints the p-values and does not provide interpretation (e.g., ""reject/fail to reject null hypothesis,"" ""statistically significant difference""). This is a significant omission.
- **Robustness:** The code assumes the file exists and doesn't include error handling for `FileNotFoundError`.
- **BMI Category Binning:** The `bins` used for BMI categories in the generated code are `[-float('inf'), 20, 25, 30, float('inf')]`. The ground truth uses `[0, 18.5, 25, 30, np.inf]` and specifies `right=False` for the correct ranges. The generated code's definition of ""underweight"" (up to 20) and ""normal"" (20-25) might not align with standard BMI categories (under 18.5, 18.5-24.9).
- **Readability:** The code would benefit from more descriptive print statements similar to the ground truth, indicating which test is being performed and summarizing the findings.",gemma-3-12b-it,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.model_se...",6,"The generated code attempts to address most aspects of the task, but significant issues in correctness and completeness prevent it from being a high-quality solution.

**Correctness Issues:**
*   **Missing Value Handling:** The prompt specified replacing zeros with the median in *specific* columns (`Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`). The generated code uses `SimpleImputer` on these columns. While `SimpleImputer` with strategy 'median' can handle NaNs, it won't *replace 0s with NaNs first* before imputing. The `0` values would be treated as valid data points by `SimpleImputer`, leading to incorrect imputation. The ground truth correctly replaces 0s with `np.nan` *then* fills these `np.nan`s with the median.
*   **Categorical Encoding:** The prompt requires proper encoding of categorical variables. The generated code uses `LabelEncoder` for `Age_group` and `Insulin_level`. `LabelEncoder` assigns arbitrary numerical values (0, 1, 2, ...) which can imply an ordinal relationship that doesn't exist, leading to incorrect model interpretations for nominal categories. `OneHotEncoder` (as used in the ground truth) is the appropriate method for nominal categorical features.
*   **SVC `probability_score`:** The `SVC` constructor parameter is `probability=True`, not `probability_score=True`. This is a syntax error that would cause the code to crash.
*   **ROC Curve Plotting:** The `sns.roc_curve` function does not exist in Seaborn. `plt.plot` should be used with `roc_curve` results from `sklearn.metrics`. The current implementation also attempts to plot `plt.xlim()` and `plt.ylim()` which is incorrect for an ROC curve.

**Completeness Issues:**
*   **Feature Scaling Order:** Although scaling is done, it's done *after* splitting, but *before* the `ColumnTransformer`. The ground truth uses `ColumnTransformer` with `StandardScaler` and `OneHotEncoder` *within* the preprocessor, which is a more robust and pipeline-friendly approach, correctly handling feature names and types. The generated code processes numerical and categorical features separately which can be harder to manage and link back to original feature names.
*   **Confusion Matrix Visualization:** The generated code computes `confusion_matrices[name] = confusion_matrix(y_test, y_pred)` but never visualizes them using matplotlib or seaborn. The ground truth explicitly plots confusion matrices for all models.
*   **Comprehensive Evaluation Metrics:** While accuracy, precision, recall, f1-score, and ROC-AUC are used, the output for them is not as neatly presented or consistently printed as in the ground truth (e.g., no classification report).
*   **Feature Importance Display:** The `feature_importances` dictionary is populated, and head values are printed, but there's no visual plot (like a bar chart) of feature importance, which the prompt explicitly requested (""Create confusion matrices and ROC curves for visualization"" and ""9) Display feature importance for tree-based models."").
*   **Model Pipeline Integration:** The ground truth uses `Pipeline` and `ColumnTransformer` which are best practices for streamlining data preprocessing and model training, making the code more robust and preventing data leakage. The generated code attempts to do preprocessing steps manually outside a pipeline, which is less efficient and prone to errors.

**Minor Issues:**
*   Lack of explicit warning suppression or random seed setting at the top, which are good practices for reproducibility and cleaner output.
*   The `SimpleImputer` should ideally be applied to the entire dataset *after* replacing 0s with `np.nan` (as in the ground truth) or within a pipeline for consistency.

Overall, while the code structure follows the requested steps, several critical implementation details are incorrect or incomplete, leading to a less reliable and less robust solution than the ground truth.",gemma-3-12b-it,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mutual_info_classif
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('...",5,"The generated code attempts to address all requirements but falls short in several key areas, impacting its correctness and completeness.

**Key Issues:**

1.  **Missing Value Imputation:** The code uses `df.dropna()`, which is an inappropriate strategy for this dataset, especially for columns like 'Glucose', 'BloodPressure', 'BMI', 'SkinThickness', 'Insulin'. These columns contain '0' values that represent missing data, not actual zeros, and simply dropping rows would lead to significant data loss or incorrect analysis. The ground truth correctly identifies and imputes these specific '0's.
2.  **Polynomial Features:** The generated code applies `PolynomialFeatures(degree=2)` but directly modifies existing columns by creating `col_sq` and `col_cube` manually. This is less robust than using `sklearn.preprocessing.PolynomialFeatures` as demonstrated in the ground truth, which generates interaction terms and avoids manual handling of all combinations. It also creates a `_cube` feature which was not requested (only degree 2 was requested).
3.  **Feature Scaling:**
    *   The code applies `StandardScaler` and then immediately `MinMaxScaler` to the *same* data, effectively overwriting the standard scaling. The task implied performing *both* but not necessarily sequentially on the same DataFrame. The ground truth shows how one might choose one for subsequent steps.
    *   The `numerical_cols` selection for scaling is overly complex and manually excludes polynomial features, which defeats the purpose if polynomial features were intended to be scaled. The ground truth scales *all* numerical features consistently after their creation.
4.  **PCA Application:** The code applies `PCA` multiple times (`pca.fit_transform(df[numerical_cols])[:, 0]`, etc.) to extract individual components, which is inefficient and incorrect. The `fit_transform` should be called once, then components selected from the resulting array. Also, directly adding PCA components as new columns after scaling existing full DataFrame numerical columns is problematic without proper handling of original vs. transformed features.
5.  **Feature Selection:**
    *   `SelectKBest` uses `mutual_info_classif`, which is fine, but the task asked for `f_classif` as implied by the target variable (classification). While `mutual_info_classif` is a valid score, `f_classif` was expected for comparison.
    *   RFE uses `RandomForestClassifier` as estimator. While valid, `LogisticRegression` (as in ground truth) is more typical for RFE due to its interpretability and stability with feature coefficients, although any classifier works.
    *   Neither SelectKBest nor RFE results (e.g., `selected_features_kbest`, `selected_features_rfe`) are used in subsequent steps, making their execution effectively pointless in the overall pipeline. The ground truth correctly identifies and prints the selected features.
6.  **Correlation Filtering Implementation:** The function `remove_correlated_features` is a custom implementation, which is acceptable, but the ground truth's vectorized NumPy approach is more concise and efficient. More importantly, it removes features from `df.drop('Outcome', axis=1)` *before* SMOTE and other critical steps, but then `X_smote` and `y_smote` use the *original* `X` and `y` (which include previously dropped features), making the filtering ineffective for the subsequent steps. This breaks the flow.
7.  **Smote Application:** SMOTE is applied to `X` and `y` *before* the train-test split, which leads to data leakage. SMOTE should only be applied to the training data, as correctly done in the ground truth.
8.  **Feature Importance Visualization:** The visualization code is mostly correct, but the feature names are taken from `X_smote.columns` (the resampled, possibly PCA-transformed, and potentially incompletely filtered data). This can lead to misleading or incorrect feature names if the pipeline wasn't executed consistently.

**Overall:** The code demonstrates an understanding of the individual steps involved but fails to integrate them correctly into a cohesive and robust data science pipeline. The errors in missing value handling, PCA application, feature selection usage, scaling, and SMOTE application are significant and would lead to incorrect or suboptimal model training.",gemma-3-12b-it,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Handle missing values
df = df.dropna()

# 1. Define Western European countries
western_europe = ['Austria', 'Belgium', 'Denmark', 'Finland', 'France', 'Germany', 'Iceland', 'Ireland',
                  'Luxembourg', 'Netherlands', 'Norway', 'Sweden', 'Switzerland', 'United Kingdom']

# 2. Filter data and perform t-test
western_europe_happiness = df[df['Country name'].i...",5,"The generated code attempts to address all requirements but falls short on correctness and completeness in several critical areas.

**Correctness Issues:**
*   **Data Handling:** The `df.dropna()` instruction without specifying a subset is problematic. The ground truth correctly identifies `critical_columns` and drops NaNs only from those, ensuring data integrity for specific tests. A simple `dropna()` might remove too many rows or not enough for specific columns.
*   **Column Names:** The generated code uses incorrect column names for `Ladder score` (uses 'Score'), `Logged GDP per capita` (uses 'GDP per capita'), `Freedom to make life choices` (uses 'Freedom'), and `Generosity` (uses 'Generosity', which is correct but inconsistent with other renamings). These discrepancies lead to `KeyError` if the CSV header doesn't match the assumptions, making the code non-functional as-is.
*   **Task 1 (Western Europe vs. Others):** The generated code defines `western_europe` by a hardcoded list of country names. The dataset explicitly provides a 'Regional indicator' column which should be used. The ground truth correctly uses `'Regional indicator' == 'Western Europe'`, which is more robust and accurate.
*   **Task 3 (Top/Bottom 25% GDP):** The requirement is `GDP per capita`, but the generated code incorrectly splits the `df` based on 'Score' **then extracts `GDP per capita` from these filtered groups**, which is correct. The issue is in the column name `GDP per capita` where it should be `Logged GDP per capita`.
*   **Task 4 (ANOVA):** The generated code completely misinterprets the ANOVA task. It redefines 'Region' based on whether a country is in 'Western Europe' or 'Other', then performs ANOVA on only 6 hardcoded `regional_indicators` which are not derived from the actual dataset's `Regional indicator` column. The ground truth correctly groups the entire DataFrame based on the `Regional indicator` column, which is dynamic and robust to changes in the data. The generated code's ANOVA will fail if any of the hardcoded `regional_indicators` are not present, or if there are other regional indicators.
*   **Task 6 (Chi-square):** Similar to ANOVA, the generated code creates a `Region` column that is just 'Western Europe' or 'Other' instead of using the existing and comprehensive `Regional indicator` column. This significantly simplifies the test, losing the full context of regional association. Also, the `bins` for `Happiness Category` are arbitrary and not based on quantiles, which the ground truth uses for a more balanced distribution.
*   **Interpretation:** While interpretation is present, it's very basic (e.g., ""Western Europe has significantly higher happiness scores.""). The ground truth provides more detailed and nuanced interpretations, including mean/median comparisons.
*   **Output Consistency:** The ground truth meticulously prints intermediate insights (means, medians, statistics) and clearly labels each test and its outcome with detailed interpretation. The generated code mostly just prints p-values.

**Completeness Issues:**
*   Lack of robustness in data loading (`try-except` for `FileNotFoundError`).
*   Missing `alpha` variable for consistent significance level across tests.
*   Poor quality of interpretations compared to the ground truth.
*   The use of hardcoded country names instead of 'Regional indicator' makes the code fragile.
*   The ANOVA is severely simplified and incorrect.
*   Chi-square setup is also simplified and incorrect.

**Code Quality:**
*   Missing `import numpy as np` (though not strictly necessary for the code as written, it's common for statistical analysis).
*   Column name inconsistencies make the code difficult to follow and prone to errors.
*   Lack of comments and clear structure for each test.
*   No robust handling of missing values (addressed above).

Overall, the generated code demonstrates an understanding of the types of tests required, but its implementation details, especially regarding data manipulation, column usage, and correct application of tests based on the prompt's nuances, are significantly flawed. It would likely produce KeyErrors or incorrect results.",gemma-3-12b-it,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('world-ha...",5,"The generated code attempts to address many aspects of the prompt but fails significantly in several key areas, particularly regarding data preprocessing, hyperparameter tuning logic, and feature column consistency.

**Critical Issues:**

1.  **Incorrect Data Preprocessing/Feature Columns:** The prompt specifies exact feature names from the dataset: 'Logged GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', and 'Perceptions of corruption'. The generated code uses simplified names like 'GDP per capita' and 'Freedom', which do not exist in the `world-happiness-report-2021.csv` dataset. This will cause a `KeyError` at `X = df[features]`, making the code non-executable without manual intervention or dataset name changes. The ground truth correctly identifies and uses the exact column names.
2.  **Incorrect Categorical Encoding:** The generated code attempts `LabelEncoder` on 'Country' and 'Region'. While `LabelEncoder` can be used for categorical features, the prompt explicitly states that the features to be used are numerical and lists them. 'Country' and 'Regional indicator' (used by ground truth) are not among the specified features for model training. Applying `LabelEncoder` indiscriminately without a clear reason or if these are not going to be used as features is unnecessary and potentially misleading. The ground truth correctly notes that the specified features are numerical and thus no encoding is needed for *them*.
3.  **Missing Value Handling:** The code uses `df.replace([np.inf, -np.inf], np.nan, inplace=True)` and then `df.dropna(inplace=True)`. While `dropna` is a valid approach, the `replace` for inf/ninf is often not necessary for this dataset, and `dropna` on the *entire* DataFrame (inplace=True) is aggressive if there are missing values in columns not relevant to the model, contrasting with the ground truth's more targeted `dropna(subset=FEATURE_COLUMNS + [TARGET_COLUMN])`.
4.  **Flawed Hyperparameter Tuning Logic:**
    *   The `grid` variable contains parameters for *both* Ridge/Lasso (`alpha`) and RandomForest/GradientBoosting (`n_estimators`, `max_depth`, etc.).
    *   The `GridSearchCV` loop is fundamentally flawed. It tries to apply the full `grid` to Ridge/Lasso, which will fail because those models don't have `n_estimators`, `max_depth`, etc. (and vice-versa for the second `GridSearchCV` call).
    *   It also determines `best_model` based on the *last* model processed in the loop, not the *overall* best model across all models. This means only one model (the last one for which tuning was attempted) will be evaluated at the end, not the best among the tested models.
    *   The prompt asked for tuning the *best* model, not all models. The generated code attempts to tune all but does it incorrectly.
5.  **New Hypothetical Countries Data:** The `new_countries` DataFrame uses the incorrect feature names ('GDP per capita', 'Freedom') which again do not match the actual names in the dataset and thus will not match the model's expected input features (`X`). This would lead to a `KeyError` or prediction issues.

**Minor Issues/Improvements:**

*   **No Random State for Linear Models:** `Ridge` and `Lasso` can benefit from `random_state` if they have internal randomness (though less common for basic versions, good practice in general). The ground truth added `random_state=42` to these for consistency.
*   **Evaluation Clarity:** The evaluation section simply prints the metrics. The ground truth provides a summary DataFrame, which is much clearer for comparison.
*   **Residual Plot labels:** The generated code labels the residual plot's axes as ""Actual Values"" and ""Predicted Values,"" which is usually for a scatter plot of actual vs. predicted, not a residual plot (where y-axis should be `residuals` or `error`). The ground truth correctly labels it as ""Residuals (Actual - Predicted)"".
*   **Cross-validation for Best Model:** The prompt asks for cross-validation on *all* models. The generated code only uses the `best_model` selected during the flawed tuning step.
*   **Limited Analysis of New Predictions:** The analysis of new predictions is very brief, while the ground truth offers a more structured interpretation.

Overall, the fundamental errors in data preprocessing (incorrect column names) and the hyperparameter tuning logic make this code largely non-functional and unreliable for the given task.",gemma-3-12b-it,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA, TSNE
from sklearn.feature_selection import SelectKBest, RFE
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
from sklearn.metrics import mutual_info_score

# Load the dataset
df = pd.read_csv(""world-happiness-report-2021.csv"")

# Preprocessing
df.dropna(inplace=True)
df = df.astype({'Ladder score': f...",5,"The generated code attempts to address many aspects of the prompt but falls short in correctness, completeness, and code quality in several key areas.

**Correctness Issues:**
*   **Data Preprocessing and Column Names:** The code assumes 'GDP per capita', 'Freedom', 'Corruption Perception', 'Region', 'Win', and 'Dystopia' as column names. The actual CSV uses 'Logged GDP per capita', 'Freedom to make life choices', 'Perceptions of corruption', and 'Regional indicator'. This leads to `KeyError` or incorrect calculations. The `astype` conversion also attempts to convert columns that don't exist (Win, Dystopia) or are not numeric by default.
*   **Missing Value Handling:** The ground truth specifically handles `Perceptions of corruption` NaNs by filling with mean, then drops remaining NaNs. The generated code just `dropna()` which might be overly aggressive or not handle specific cases as intended for 'Perceptions of corruption'.
*   **'Governance_Score' Calculation:** The ground truth calculates `Freedom + (1 - Perceptions of corruption)` to reflect low corruption contributing positively. The generated code uses `Freedom - Corruption Perception`, which would penalize for higher perceived corruption, but the interpretation is different and dependent on the sign of the original 'Corruption Perception' values.
*   **Polynomial Features:** The generated code applies polynomial features to only a few selected base columns and includes `Ladder score` itself, which is the target. The ground truth explicitly uses `PolynomialFeatures` from `sklearn.preprocessing`, which creates interaction terms and squared terms correctly, and applies it to *key happiness factors* but not the target. The generated code's implementation `df[f'{col}_poly{p}'] = df[col] ** p` is rudimentary and only creates squared terms, not interaction terms, and `poly = np.array([1, 2])` is confusing.
*   **Feature Scaling:** The generated code applies three different scalers to the *same* set of columns consecutively, overwriting the previous scaled values. This means only the `RobustScaler`'s effect will remain. For evaluation, separate scaled dataframes/arrays for each scaler should be maintained.
*   **Dimensionality Reduction:** Similar to scaling, PCA and t-SNE are applied only to a rigid, small subset of base columns, ignoring all the newly engineered and polynomial features. The ground truth correctly applies it to the `X_scaled` data, which includes all transformed features.
*   **Feature Selection:**
    *   `SelectKBest`: Uses `mutual_info_score` which is for classification, not regression. It should be `mutual_info_regression` or `f_regression`.
    *   RFE: The estimator `RandomForestRegressor()` is called without `n_estimators`, which is crucial. Also, RFE should use a `LinearRegression` or similar simpler model for stable feature selection unless the task specifically requires RF.
*   **Correlation Filtering:** The logic for `filtered_features` is incorrect. `lower_triangle[lower_triangle.abs() < 0.7].dropna().index` does not correctly identify features to drop due to high correlation. It filters for correlations *less than* 0.7, not *greater than* a threshold. The ground truth correctly identifies and flags features to drop if their correlation coefficient with any other feature in the upper triangle is above a threshold.
*   **Feature Importance Visualization:** The `features` variable for the bar plot includes the target `Ladder score` and engineered features which are sometimes also the target. The dropdown selection logic for features to exclude in the `model.fit` call is fragile and prone to errors. `df.drop(['Region', 'Country name', ...])` will error if these columns are not present after earlier operations or if the column names are incorrect.

**Completeness Issues:**
*   **Missing Features:**
    *   'GDP_vs_Regional_Average' is created but 'Regional_GDP_Mean' creation is not explicit, although it's implicitly handled by `transform`.
    *   'Regional_Happiness_Rank': Missing `ascending=False` for ranking highest score as rank 1.
    *   **Interaction Features:** Only two specific interaction features are created. The prompt asks for ""interaction features between economic and social factors"" which implies a more general approach or at least a few more diverse examples. The ground truth provides two examples.
*   **Missing Visualizations:** While PCA and t-SNE visualizations are present, they are basic scatter plots without proper labeling or color mapping (`c=df['Ladder score']` is good, but labels are missing). The ground truth provides more detailed plots with `hue` and `colorbar`.
*   **Mutual Information Scoring:** The prompt mentions ""mutual information scoring"" explicitly, not just its use in `SelectKBest`. The ground truth explicitly lists out the scores.

**Code Quality Issues:**
*   **Imports:** `TSNE` is imported from `sklearn.decomposition` (which is incorrect, it's from `sklearn.manifold`). `mutual_info_score` is imported (for classification) instead of `mutual_info_regression`. `LinearRegression` is omitted. `seaborn` is not imported or used for plotting.
*   **Variable Naming:** Inconsistent use of `data` vs `df` for the main DataFrame.
*   **Hardcoding Column Names:** Many operations hardcode column names without checking for their existence, making the code fragile to changes in the dataset schema. The ground truth often checks for column existence or assumes a common set of relevant columns from the start.
*   **Lack of Structure/Comments:** The code lacks comments explaining various steps, especially the complex parts like feature selection and correlation filtering. The ground truth provides extensive comments.
*   **Redundant Operations:** Repeatedly scaling the same columns without storing intermediate results.
*   **Output/Printing:** No print statements to show progress or results of various steps (e.g., shape after filtering, selected features) until the final plots, unlike the ground truth.
*   **Error Handling:** The `FileNotFoundError` handling for the CSV is missing.

In summary, the generated code has significant functional and logical errors that would prevent it from running correctly or producing the intended results for the given task. It also lacks robustness, proper structure, and detailed outputs compared to the ground truth.",gemma-3-12b-it,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('synthetic_covid19_data.csv')
except FileNotFoundError:
    print(""Error: synthetic_covid19_data.csv not found. Make sure the file is in the correct directory."")
    exit()

# Parse the 'date' column and filter for 'USA'
df['date'] = pd.to_datetime(df['date'])
usa_data ...",6,"The generated code correctly performs data loading, parsing, filtering, resampling, and time series decomposition. It also attempts to check for stationarity and apply differencing, and plot ACF/PACF. However, there are critical issues in the implementation of the ACF and PACF plots, which causes the code to fail. The `autocorrelation_function()` and `partial_autocorrelation_function()` methods do not exist directly on a pandas Series, leading to an `AttributeError`. The `statsmodels.graphics.tsaplots` module, particularly `plot_acf` and `plot_pacf`, should have been used for this purpose, which was a specific requirement in the prompt. Additionally, the decomposition plot doesn't correctly use the `decomposition.plot()` method from `statsmodels`.

**Areas for Improvement:**
1.  **ACF/PACF Plotting:** The most significant error is the incorrect method calls for ACF and PACF plots. Replace `stationary_data.autocorrelation_function().plot()` and `stationary_data.partial_autocorrelation_function().plot()` with `plot_acf(stationary_data, ax=ax1)` and `plot_pacf(stationary_data, ax=ax2)` respectively, after importing them from `statsmodels.graphics.tsaplots`.
2.  **Decomposition Plotting:** The decomposition plot should ideally use `decomposition.plot()` which generates a combined figure. The current manual subplot creation is less idiomatic and requires explicit plotting of each component.
3.  **Error Handling/Robustness:** The ground truth code includes more robust error handling (e.g., checking if `weekly_data` is empty after resampling, handling cases where decomposition might fail due to insufficient data, and checking for sufficient data for ADF test). The generated code could benefit from similar robustness checks.
4.  **Clarity for Decomposition Period:** The `seasonal_decompose` function typically requires a `period` parameter, especially for weekly data (e.g., 52 for yearly seasonality). The generated code omits this, which might lead to `statsmodels` inferring a period or potentially an error with older versions if the index frequency isn't set, or suboptimal decomposition results if the default period is incorrect for weekly data.
5.  **Imports:** `from statsmodels.graphics.tsaplots import plot_acf, plot_pacf` is missing. numpy is imported but not explicitly used.",gemma-3-12b-it,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
import numpy as np

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Categorize wines based on pH
def categorize_ph(pH):
    if pH < 3.2:
        return 'Low pH'
    elif 3.2 <= pH <= 3.4:
        return 'Medium pH'
    else:
        return 'High pH'

data['pH_Group'] = data['pH'].apply(categorize_ph)

# Calculate average quality for each pH group
avg_quality = data.groupby('pH_Group')['quali...",5,"The generated code addresses most parts of the task but has significant flaws in implementation, particularly regarding the confidence intervals and categorizing pH groups.

**Correctness:**
- **pH Categorization:** The `categorize_ph` function is correct for the specified bins. However, it misses a crucial `right=False` (or inclusive lower/exclusive upper) handling that `pd.cut` provides. Specifically, `3.2 <= pH <= 3.4` includes 3.4, while `pH > 3.4` starts *after* 3.4. If `pH` was exactly 3.4, it would fall into the 'Medium pH' group, which is consistent with one common interpretation for these types of tasks rather than typical bins. The ground truth uses `[3.2, 3.4)` for Medium, which is more standard for `pd.cut`. The `pH > 3.4` for High pH is inclusive in the ground truth's `[3.4, inf)` bin, which is inconsistent with the generated code's definition. The prompt's definition was `Low pH (< 3.2)`, `Medium pH (3.2 to 3.4)`, and `High pH (> 3.4)`. The generated code gets these `min/max` definitions correct for the `if/elif/else` statements.
- **Confidence Intervals:** This is the most critical error. The generated code calculates `np.percentile([2.5, 97.5])` as confidence intervals. This is **incorrect** for 95% confidence intervals of the mean. A 95% confidence interval for the mean is typically calculated using the t-distribution (or z-distribution for very large samples) and the standard error of the mean, not directly from percentiles of the data. The prompt specifically asked for ""error bars representing 95% confidence intervals,"" which implies intervals of the mean.
- **ANOVA Test:** The ANOVA test is correctly performed using `stats.f_oneway`.
- **Visualization:** The visualization attempts to show error bars, but since the CI calculation is wrong, the error bars themselves are wrong. The `yerr` calculation is also problematic due to iterating over `avg_quality.index` after `confidence_intervals` was populated with string keys. `avg_quality` is a Series indexed by correct pH groups. Accessing `confidence_intervals[group]` where `group` is, e.g., 'Low pH' is correct. However, `confidence_intervals[group][1] - avg_quality[group]` would calculate the distance from the average to the upper percentile, which is one dimension of an error bar. The error bar should be symmetric unless specified. The ground truth correctly calculates $mean \pm SEM \times t_{critical}$.

**Code Quality:**
- **Clarity:** The code is reasonably clear.
- **Modularity:** The `categorize_ph` function is a small, reusable piece.
- **Imports:** Correct imports are present.
- **Error Handling:** No robust error handling (e.g., for file not found).
- **Group Order:** The `pH_Group` column is created, but no specific ordering (like `pd.Categorical`) is applied, which can lead to inconsistent plotting or grouping order (e.g., alphabetical).

**Completeness:**
- **Categorization:** Yes, implemented using a function and `.apply()`.
- **Average Quality:** Yes, calculated correctly.
- **ANOVA Test:** Yes, performed correctly and results printed.
- **Visualization:** Yes, a bar plot with error bars is produced. However, the error bar calculation is flawed (as mentioned above).
- **Reporting:** Reports ANOVA results. Doesn't report specific average qualities with CIs as nicely as the ground truth.

**Specific Areas for Improvement:**
1.  **Correct Confidence Interval Calculation:** Implement the standard way of calculating confidence intervals for the mean, using standard error of the mean (SEM) and the t-distribution (e.g., `scipy.stats.t.interval`).
2.  **Ordered Categories:** Use `pd.Categorical` with a `categories` argument to ensure that the pH groups always appear in the desired order (Low, Medium, High) in plots and group-by operations.
3.  **Visualization Enhancement:**
    *   Add axis labels and a proper plot title.
    *   Set `ylim` if appropriate (e.g., to the known quality range).
    *   Consider adding specific average quality values on top of bars for readability.
    *   Ensure error bars are symmetric based on the correct CI calculation.
4.  **File Not Found Handling:** Add a `try-except FileNotFoundError` block for robust file loading.",gemma-3-12b-it,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import numpy as np

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Parse 'date' column
df['date'] = pd.to_datetime(df['date'])

# Select features
features = ['new_cases', 'hosp_patients', 'icu_patients', 'reproduction_rate',
     ...",7,"The generated code correctly addresses most of the requirements. It loads data, parses the date, selects features, handles missing values, performs one-hot encoding, scales features, splits data, trains both models, and evaluates them.

However, there are a few significant issues and areas for improvement:

1.  **Scaling before Splitting (Data Leakage):** The most critical issue is that numerical features are scaled *before* the data is split into training and testing sets: `df[numerical_features] = scaler.fit_transform(df[numerical_features])`. This leads to data leakage because the scaler learns parameters (mean and standard deviation) from the entire dataset, including the test set. The `fit_transform` should only be applied to the training data, and then `transform` should be applied to the test data. This is a common and important mistake in machine learning pipelines.

2.  **Incorrect R-squared Calculation:** The R-squared calculation `np.corrcoef(y_test, y_test_pred_linear)[0, 1]**2` is not the standard statistical R-squared. While it's related (it calculates the squared Pearson correlation coefficient), it is not the same as `sklearn.metrics.r2_score` which measures the proportion of variance in the dependent variable that can be predicted from the independent variables. For regression evaluation, `r2_score` from `sklearn.metrics` is the correct metric to use as it gives a more accurate representation of the model's predictive power, especially when predictions might not be perfectly correlated with the true values or when the model is worse than a simple mean prediction. The standard `r2_score` can be negative, which `np.corrcoef`squared cannot be.

3.  **Missing Imports for Evaluation Metrics:** The code uses `np.mean` and `np.abs` for MAE and MSE, which is acceptable, but it fails to import `mean_absolute_error`, `mean_squared_error`, and `r2_score` from `sklearn.metrics`, which were implicitly required by the prompt's evaluation criteria and are standard for model evaluation. The prompt specified ""Include proper imports (pandas, sklearn)."" and implicitly for the metrics used.

4.  **Inefficient Feature Selection and Handling:** The way `features` is defined and then `numerical_features` includes both original features and 'continent' one-hot encoded columns is a bit convoluted. It would be cleaner to build the `X` and `y` dataframes and then identify the numerical columns within `X` for scaling *after* one-hot encoding but *before* the train-test split for better pipeline design (or ideally, after splitting as mentioned in point 1).

5.  **Handling `continent` in Missing Values:** The `for col in features + [target]: df[col] = df[col].fillna(df[col].median())` loop attempts to impute `continent` with its median, which is nonsensical for a categorical column. While `fillna(df[col].median())` for a string column would likely result in no change (as `median()` on strings isn't usually defined or would raise an error if it was a numeric representation), it highlights a potential semantic issue. The ground truth correctly excludes 'continent' from median imputation.

To improve:
*   Refactor the scaling step to occur *after* the train-test split.
*   Use `sklearn.metrics.r2_score`, `mean_absolute_error`, and `mean_squared_error` for evaluation, and add necessary imports.
*   Refine the missing value handling to specifically address only numerical columns for median imputation.
*   Consider a more robust data pipeline for preprocessing steps, perhaps using `ColumnTransformer` for clarity and to prevent leakage for all steps.",gemma-3-12b-it,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
from scipy.stats import f, pearsonr, t
from statsmodels.formula.api import ols
import numpy as np

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Handle missing values by median imputation
df['reproduction_rate'].fillna(df['reproduction_rate'].median(), inplace=True)
df['stringency_index'].fillna(df['stringency_index'].median(), inplace=True)
df['median_age'].fillna(df['median_age'].median(), inplace=True)
df['total_deaths'].fillna(df['total_de...",6,"The generated code attempts to address all parts of the task but has several inaccuracies and omissions in its implementation, leading to incorrect results for some tests.

**Correctness Issues:**
1.  **Missing Value Imputation:** While median imputation is used, it's done for the entire DataFrame without first checking if the columns exist or if there are actual missing values. The ground truth code correctly iterates through specific columns and applies imputation only if necessary.
2.  **ANOVA Implementation:** The `statsmodels.formula.api.ols` and `scipy.stats.f.anova_lm` approach for ANOVA is correct conceptually. However, the task asked for `scipy.stats.f_oneway`, which is simpler and more direct for comparing means of multiple groups. Crucially, the `C(continent)` coding in `statsmodels` is correct for ANOVA, but it expects `reproduction_rate` to be available for all selected rows. The filtering `df[df['continent'].isin(continents)]` is appropriate. An important omission is the null hypothesis statement for each test as requested by the prompt.
3.  **Pearson Correlation:** The `pearsonr` function is applied directly to the full columns, which might still contain NaNs if the initial imputation didn't cover all cases (though the listed columns for imputation should cover it). The ground truth specifically uses `dropna()` on the relevant columns before correlation to ensure valid input.
4.  **High Death Rate Category Derivation (Critical Error):** The code incorrectly derives `high_death_rate_country`. The task specifies: ""high_death_rate_country' based on whether a country's 'total_deaths' / 'population' is above the 75th percentile of this ratio **across all countries (use latest entry per country)**"". The generated code calculates `df['death_rate'] = df['total_deaths'] / df['population']` and then `df['death_rate'].quantile(0.75)` on the entire DataFrame `df`. This mixes different dates and does not consider *the latest entry per country* as required, which is a significant deviation and will produce incorrect results for the t-test. The `t.ttest_ind` function is then called on potentially mis-categorized groups.
5.  **T-test Type:** `scipy.stats.t.ttest_ind` is a generic t-test. The ground truth uses `equal_var=False` which performs Welch's t-test, generally more robust as it doesn't assume equal variances, which is often a safer assumption in real-world data. While not strictly ""incorrect"" to default `equal_var=True`, it's less robust.
6.  **Missing Null Hypothesis Statements:** The prompt explicitly asks to ""state the null hypothesis"" for each test. The generated code prints conclusions but not the explicit null hypotheses.

**Code Quality and Completeness:**
*   **Imports:** Imports are generally correct (`pandas`, `scipy.stats.f`, `scipy.stats.pearsonr`, `scipy.stats.t`, `statsmodels.formula.api`, `numpy`).
*   **Structure:** The code is logically structured into sections for each test.
*   **Output:** Prints p-values and conclusions, but misses the explicit null hypothesis statements.
*   **Robustness:** Lacks error handling (e.g., FileNotFoundError, insufficient data for tests beyond `pearsonr`). The ground truth includes checks for `FileNotFoundError` and insufficient data for ANOVA and t-tests.

The main issue is the incorrect definition of `high_death_rate_country`, which renders the t-test result invalid in the context of the prompt's requirements. The use of `statsmodels` for ANOVA, while fine, wasn't the simplest `scipy.stats` path requested, and the lack of null hypothesis statements hurts completeness.",gemma-3-12b-it,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
try:
    df = pd.read_csv('clash_royale_cards.csv')
except FileNotFoundError:
    print(""Error: clash_royale_cards.csv not found. Make sure the file is in the current directory."")
    exit()

# Select numerical columns
numerical_cols = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
numerical_df = df[numerical_cols]

# Calculate pairwise correlation matrix
correlation_ma...",9,"The generated code correctly implements all the requirements of the task: it loads the CSV, selects the specified numerical columns, calculates the correlation matrix, and visualizes it as a heatmap using seaborn. The imports are correct, and the structure is clean and readable.

Minor areas for improvement include:
- The error handling for `FileNotFoundError` exits the script directly, which is acceptable for a simple script, but a more robust application might raise an exception or use a function return value.
- The ground truth includes checks for missing columns, which is good practice for more robust code, but not explicitly required by the prompt's simplicity.
- The ground truth also uses a function and `if __name__ == ""__main__"":` block, which is better for reusability and modularity, but for a simple script, the generated code's direct execution is fine.
- The `plt.figure` and specific `sns.heatmap` parameters (like `linewidths`) in the ground truth add slightly more polish to the visualization, but the generated code's visualization is perfectly adequate.

Overall, the generated code is an excellent solution for the given task.",gemma-3-12b-it,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

try:
    df = pd.read_csv(""clash_royale_cards.csv"")
except FileNotFoundError:
    print(""Error: clash_royale_cards.csv not found in the current directory."")
    exit()

grouped = df.groupby(""rarity"").agg(
    avg_elixirCost = (""elixirCost"", ""mean""),
    avg_WinRate = (""winRate"", ""mean""),
    avg_Usage = (""usage"", ""mean"")
)

print(grouped)
```",8,"The generated code correctly loads the CSV, groups by `rarity`, and calculates the average `elixirCost`, `Win Rate`, and `Usage` for each rarity. It also includes error handling for `FileNotFoundError`, which is a good practice.

However, there's a minor issue with the column names. The prompt and ground truth specify ""Win Rate"" and ""Usage"" (with capitalization), while the generated code uses ""winRate"" and ""usage"". This could lead to a `KeyError` if the actual column names in the CSV are capitalized as in the prompt. Assuming the task implies strict adherence to the provided column names, this is a minor deviation. If the column names in the CSV are indeed lowercase, then the generated code would be perfect.

Additionally, the generated code uses `agg` with named aggregations, which is a more modern and sometimes clearer way to perform multiple aggregations than simply calling `.mean()` on a subset of columns, as in the ground truth. This is a point in its favor for code quality in some contexts, but both approaches are valid.

To improve, ensure strict matching of column names as specified in the problem description.",gemma-3-12b-it,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
data = pd.read_csv(""clash_royale_cards.csv"")

# Create the binary target column
data['is_high_win_rate'] = (data['Win Rate'] > 55).astype(int)

# Select features
features = ['elixirCost', 'Usage', 'rarity']
X = data[features]

# Apply one-hot encoding to the 'rarity' column
encoder = OneHotEncoder(handle_unknown='ignore')
X = encoder.fit_transform...",6,"The generated code correctly performs most of the required steps: loading data, creating the target column, and splitting the data with the correct parameters. However, there's a significant issue in how one-hot encoding is applied. The `OneHotEncoder` is used only on the 'rarity' column, and then the original `X` (which contained 'elixirCost', 'Usage', and 'rarity') is completely replaced by *only* the one-hot encoded 'rarity' columns. This means the 'elixirCost' and 'Usage' features are dropped from `X` before the train-test split, which is incorrect as they were specified as features to be used (`elixirCost`, `Usage`, and `rarity`).

To improve, the one-hot encoding should be applied in a way that preserves or correctly merges the numerical features ('elixirCost', 'Usage') with the one-hot encoded 'rarity' features. Using `pd.get_dummies` on the entire feature DataFrame `X` (as shown in the ground truth) would be simpler and more robust for this specific task, or using `ColumnTransformer` with `OneHotEncoder` if dealing with a mix of numerical and categorical features in a more complex pipeline. The current implementation effectively drops two of the three specified features for the classification task.",gemma-3-12b-it,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
iris_data = pd.read_csv('iris.csv')

# Define features (X)
X = iris_data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Define target (y)
y = iris_data['Species']

# Encode the categorical target variable
label_encoder = LabelEncode...",9,"The generated code correctly implements all specified requirements: loading data, defining features and target, encoding the target, splitting data (70/30, random_state=42), training a KNN classifier with `n_neighbors=3`, and reporting the accuracy. The imports are correct and the code structure is logical.

One minor improvement would be to include `stratify=y` in the `train_test_split` function, which is good practice for classification tasks with imbalanced datasets to ensure both training and test sets have similar class distributions. While not strictly required by the prompt, it's a detail often considered in robust machine learning workflows. Also, the ground truth includes a `try-except` block for `FileNotFoundError`, which is a good robustness feature, but the prompt did not explicitly ask for error handling. The printing format `Accuracy: {accuracy}` is slightly less specific than `K-Nearest Neighbors Classifier Accuracy: {accuracy:.4f}` but still acceptable. Overall, a very good solution.",gemma-3-12b-it,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
iris_data = pd.read_csv('iris.csv')

# Filter the data for 'Iris-versicolor' and 'Iris-virginica'
versicolor_data = iris_data[iris_data['Species'] == 'Iris-versicolor']
virginica_data = iris_data[iris_data['Species'] == 'Iris-virginica']

# Define the null and alternative hypotheses
# Null hypothesis (H0): There is no significant difference in the mean PetalLengthCm between Iris-versicolor and Iris-virginica.
# Alternative...",8,"The generated code correctly performs all the core steps of the task: loading the data, filtering for the specified species, performing the t-test, printing the t-statistic and p-value, and interpreting the result based on alpha.

However, it missed the following requirements:
- **Clarity of Hypotheses:** While it stated the hypotheses as comments, the task specifically asked to ""Clearly state the null and alternative hypotheses for this test"" which in the ground truth was interpreted as printing them explicitly before the test results. The generated code did not print the hypotheses.
- **Formatting of Numeric Output:** The T-statistic and P-value were printed without formatting (e.g., to 4 decimal places), which was done in the ground truth for better readability.

The missing file handling (try-except for FileNotFoundError) is a good practice but wasn't explicitly mandated for this specific problem statement as a core requirement, so its absence doesn't significantly penalize the score given the other requirements were met. Similarly, the empty group check was an edge case added in the ground truth, not a strict requirement, but good practice.",gemma-3-12b-it,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
iris = pd.read_csv('iris.csv')

# Create box plots for each feature, grouped by species
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

for feature in features:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x='Species', y=feature, data=iris)
    plt.title(f'Box Plot of {feature} by Species')
    plt.xlabel('Species')
    plt.ylabel(feature)
    plt.show()
```",9,"The generated code successfully loads the dataset and creates the required box plots for each numerical feature, grouped by species, with appropriate titles. It uses `pandas` and `seaborn` correctly for data loading and plotting.

Strong points:
- Correctly loads the `iris.csv` file.
- Iterates through all four specified numerical features.
- Generates a separate box plot for each feature.
- Correctly groups the box plots by the `Species` column.
- Sets appropriate titles for each plot.
- Uses `matplotlib.pyplot` and `seaborn` as requested.

Areas for minor improvement (not critical for functionality but good practice):
- The ground truth includes error handling for `FileNotFoundError`, which is a good practice for robust code. The generated code lacks this.
- The ground truth defines constants for dataset path, numerical features, and grouping column, which makes the code more configurable and readable. The generated code directly uses string literals.
- The ground truth adds more aesthetic touches like `tight_layout`, `grid`, and specific font sizes, which can improve plot presentation.

Overall, the generated code is highly correct and complete based on the task requirements, with only minor best-practice omissions compared to the more comprehensive ground truth.",gemma-3-12b-it,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder

# 1. Load the dataset
data = pd.read_csv('heart.csv')

# 2. Select features and target variable
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']
target = 'HeartDisease'

X = data[features]
y = data[target]

# 3. Preprocess categorical features using one-hot encoding
encoder = OneHotEn...",6,"The generated code largely fulfills the requirements of the task. It correctly loads the data, selects features and target, performs one-hot encoding for the specified categorical features, splits the data, trains a Logistic Regression model, and reports accuracy.

However, there are a few issues and sub-optimal practices that prevent a higher score:
1.  **Hardcoded Column Names in `X_encoded = pd.concat`:** When concatenating the numerical and one-hot encoded features, the `pd.DataFrame(X_encoded)` lacks column names. While `sklearn` internally handles numerical arrays, for better readability and a more robust approach, it's better to assign column names or use `ColumnTransformer` from `sklearn.compose` which handles this automatically.
2.  **Incorrect Dropping of Columns:** The line `X_encoded = X_encoded.drop(['Sex_F', 'ChestPainType_ASY', 'ChestPainType_ATA', 'ChestPainType_NAP'], axis=1)` is problematic and causes errors.
    *   `OneHotEncoder` with `sparse_output=False` directly returns a NumPy array, not a DataFrame with column names like `Sex_F`.
    *   Even if it were a DataFrame, dropping specific columns is an ad-hoc and incorrect way to handle multicollinearity or `handle_unknown` scenarios where `OneHotEncoder` by default creates *all* categories. `handle_unknown='ignore'` is already a good step, and typically you don't manually drop columns unless you specifically want to remove one category (e.g., for `drop_first=True` in `OneHotEncoder`, which was not specified or required here). In its current form, this `drop` operation will likely fail as those column names won't exist.
3.  **Missing `ColumnTransformer` (or equivalent):** The manual separation of numerical and categorical features (`X_numerical` and `X_encoded`) and then concatenation can be error-prone and less maintainable. The `ColumnTransformer` (as used in the ground truth) is the standard and more robust way to apply different transformations to different columns.
4.  **Accuracy Calculation:** While `(y_pred == y_test).mean()` works for accuracy, using `sklearn.metrics.accuracy_score` is the standard and more explicit way, promoting consistency and clarity.

To improve, the code should:
*   Adopt `ColumnTransformer` to handle preprocessing of mixed data types more cleanly.
*   Remove the incorrect column dropping step.
*   Use `accuracy_score` from `sklearn.metrics`.
*   Ensure that the `OneHotEncoder` output is correctly integrated with the numerical features, ideally retaining column names for clarity if a DataFrame is desired after encoding.",gemma-3-12b-it,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
data = pd.read_csv(""heart.csv"")

# Create two groups based on the 'Sex' column
males = data[data['Sex'] == 'M']['Cholesterol']
females = data[data['Sex'] == 'F']['Cholesterol']

# Perform an independent two-sample t-test
t_statistic, p_value = stats.ttest_ind(males, females)

# Null hypothesis: There is no statistically significant difference in the mean Cholesterol levels between male and female patients.
# Alternative hy...",7,"The generated code correctly performs the core task of loading data, separating groups, and executing the t-test. It also correctly states the hypotheses and interprets the p-value.

However, it lacks several aspects of robustness and good practice present in the ground truth:
1.  **Error Handling:** The generated code does not include any error handling for `FileNotFoundError` when loading the CSV, nor does it check for the existence of required columns (`Sex`, `Cholesterol`). This makes it brittle if the file is missing or the dataset structure is unexpected.
2.  **`equal_var` in `ttest_ind`:** The generated code uses the default `equal_var=True` for `stats.ttest_ind`. The ground truth explicitly uses `equal_var=False` (Welch's t-test), which is generally a more robust choice when variances are not known to be equal, and often preferred unless there's a strong reason to assume equal variances.
3.  **Encapsulation/Modularity:** The generated code is a script, not encapsulated within a function. The ground truth wraps the logic in a function `perform_cholesterol_t_test`, which improves reusability and testability.
4.  **Handling Missing Data:** The generated code does not explicitly handle `NaN` values in the 'Cholesterol' column. While `ttest_ind` generally ignores NaNs, it's good practice to show this (e.g., using `dropna()`) as it makes the data preparation clear and explicit, as done in the ground truth.
5.  **Output Clarity/Formatting:** While it prints the results, the output is less detailed and formatted than the ground truth (e.g., specific statements for null/alternative hypotheses, clear headings), reducing readability for someone interpreting the results.

Overall, it's a functional solution but could be improved significantly by incorporating error handling, considering robust statistical choices, and adopting better code organization, as demonstrated in the ground truth.",gemma-3-12b-it,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv(""winequality-red.csv"")

# Identify outliers using IQR for 'volatile acidity'
Q1_va = df['volatile acidity'].quantile(0.25)
Q3_va = df['volatile acidity'].quantile(0.75)
IQR_va = Q3_va - Q1_va
lower_bound_va = Q1_va - 1.5 * IQR_va
upper_bound_va = Q3_va + 1.5 * IQR_va
outliers_va = df[(df['volatile acidity'] < lower_bound_va) | (df['volatile acidity'] > upper_bound_va)]
num_outliers_va = len(outliers_va)

# Iden...",9,"The generated code correctly implements all the requirements of the task: identifying outliers using IQR, reporting their numbers, winsorizing the outliers by capping them at the 1.5*IQR bounds, and comparing statistics before and after treatment. The code is well-structured and uses appropriate pandas and numpy functions.

Strong points:
- Correct application of the IQR method for outlier detection.
- Accurate implementation of winsorization using `np.where`.
- Correctly compares mean and standard deviation before and after treatment.
- Creates a copy of the DataFrame (`df_capped`) before modification, preserving the original data for comparison.

Areas for minor improvement (not critical enough to significantly reduce the score but could make the code more robust/cleaner for larger tasks):
- The processing for `volatile acidity` and `residual sugar` is duplicated. A loop over the columns to treat would make the code more concise and scalable if more columns needed similar treatment (as seen in the ground truth).
- No error handling for `FileNotFoundError` when loading the CSV. While the prompt implies the file exists, robust code would include it.
- The output formatting for statistical comparison is a bit less verbose than the ground truth, making it slightly less clear, but functionally it delivers the required information.",gemma-3-12b-it,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv(""heart.csv"")

# Create a grouped bar chart
plt.figure(figsize=(10, 6))
sns.countplot(x='ChestPainType', hue='HeartDisease', data=df)
plt.title('Heart Disease Count by Chest Pain Type')
plt.xlabel('Chest Pain Type')
plt.ylabel('Count')
plt.legend(title='Heart Disease')
plt.show()
```",9,"The generated code correctly loads the dataset and creates a grouped bar chart as requested. It uses `seaborn.countplot` effectively to display the counts of `HeartDisease` for each `ChestPainType`, differentiating between 0 and 1 with different colored bars. The plot includes a title, labeled axes, and a legend, fulfilling all the explicit requirements.

Strong points:
- Correctly uses `pandas.read_csv` for data loading.
- Effectively uses `seaborn.countplot` for creating the grouped bar chart.
- Includes all required plot elements: title, x-label, y-label, and legend.
- Code is concise and readable.

Areas for minor improvement (not critical enough to heavily penalize but would make the code more robust and user-friendly):
- The `HeartDisease` legend labels are ""0"" and ""1"". It would be more informative to map them to ""No Heart Disease"" and ""Heart Disease"" as demonstrated in the ground truth.
- Error handling for `FileNotFoundError` or other issues during CSV loading is missing.
- The `ChestPainType` order is not explicitly set, though `seaborn` might handle it alphabetically by default. For specific orders like TA, ATA, NAP, ASY, it's better to explicitly define categorical order.
- The code is not encapsulated within a function, which is generally good practice for reusability, especially when dealing with file paths.",gemma-3-12b-it,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv(""spotify_2023.csv"")
except FileNotFoundError:
    print(""Error: spotify_2023.csv not found in the current directory."")
    exit()

# Clean the in_shazam_charts column
df['in_shazam_charts'] = df['in_shazam_charts'].str.replace(',', '').astype(float, errors='coerce')
nan_count = df['in_shazam_charts'].isna().sum()
print(f""Number of NaNs in in_shazam_charts: {nan_count}"")


# Group by key and mode and calculate average dan...",9,"The generated code correctly loads the CSV, handles `FileNotFoundError`, cleans the `in_shazam_charts` column by removing commas and converting to numeric (coercing errors), reports the count of NaNs, and then correctly groups the data by `key` and `mode` to calculate the average `danceability_%` and `valence_%`. It addresses all requirements. The only minor difference from the ground truth is the phrasing of the NaN count print statement and the explicit `astype(str)` call in the ground truth for safety (though it works here as `replace` implicitly converts to string). The generated code's single-line cleaning and conversion is also concise and effective.",gemma-3-12b-it,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# 1. Load the dataset
df = pd.read_csv(""spotify_2023.csv"")

# 2. Ensure 'streams' column is numeric
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams', 'in_spotify_playlists', 'energy_%'])

# 3. Select features (X) and target (y)
X = df[['in_spotify_playlists', 'en...",9,"The generated code is very good, correctly implementing all the requirements. It loads the dataset, handles the 'streams' column numerics and NaNs, selects features and target, splits the data, trains the model, and evaluates it, reporting R2 and MSE. The only minor deviation from the ground truth is the lack of explicit error handling (`try-except` for `FileNotFoundError`) and the print statements for informing the user about the steps and progress. However, these are not strict requirements of the problem statement, which focused on the core data science task. The output format for R2 and MSE is also slightly different (less precision) but still correct. Overall, a highly functional and well-structured solution.",gemma-3-12b-it,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd
from datetime import datetime
import numpy as np

def feature_engineering(file_path='spotify_2023.csv'):
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        print(f""Error: File not found at {file_path}"")
        return None

    # Combine date columns into a single datetime column
    df['release_date'] = pd.to_datetime(df[['released_year', 'released_month', 'released_day']].astype(str).agg(' '.join, axis=1), errors='coerce')

    # Ca...",7,"The generated code correctly performs most of the required feature engineering steps.

**Strengths:**
- Correctly loads the dataset.
- Successfully combines `released_year`, `released_month`, and `released_day` into a `release_date` column using `pd.to_datetime` with `errors='coerce'`, handling invalid dates appropriately.
- Correctly calculates `days_since_release` using a fixed reference date.
- Displays the required columns for the first 5 tracks.
- Includes basic error handling for file not found.

**Areas for Improvement (Why not a higher score):**
- **Data Type of `days_since_release`:** The requirement states that `days_since_release` should be an **integer**. The generated code uses `.astype(int)` which will convert NaNs (resulting from NaT `release_date` values) to 0 or raise an error for later operations, depending on the pandas version and context. The preferred way to handle this in pandas for nullable integer columns is to use `astype('Int64')` (pandas' nullable integer type), which allows NaN values to persist while the column is still considered an integer type. The current approach with `.astype(int)` might lead to incorrect values (0 for NaNs) or type errors if NaNs are present after the initial calculation.
- **Minor Import:** `import numpy as np` is included but not used in the provided code snippet.
- **Implicit Conversion for Date Construction:** While `df[['released_year', 'released_month', 'released_day']].astype(str).agg(' '.join, axis=1)` works, it might be more robust or explicit to format the date components with zero-padding (e.g., `str.zfill(2)`) to ensure consistent ""YYYY-MM-DD"" format, especially if single-digit months/days lead to issues in `pd.to_datetime` in some edge cases (though `pd.to_datetime` is generally smart enough). The ground truth's approach of `YYYY-MM-DD` string construction is often safer. However, for well-formed numeric date components, the generated code's approach is often sufficient.
- **Code Clarity:** While the code is functional, the ground truth's use of comments and breaking down steps (e.g., intermediate date string, then conversion) can improve readability and maintainability for more complex scenarios. However, for this task, the generated code is reasonably clear.",gemma-3-12b-it,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: youtube_trendings.csv not found in the current directory."")
    exit()

# Clean 'Video views' and 'Likes' columns
def clean_numeric_column(df, column):
    df[column] = df[column].str.replace(',', '').astype(float, errors='coerce')
    df[column] = df[column].fillna(0)
    return df

df = clean_numeric_column(df, 'Video views')
df = clean_numeric_column(df, 'Likes')

# Fi...",7,"The generated code correctly addresses most of the requirements. It loads the CSV, cleans the 'Video views' and 'Likes' columns by removing commas, converting to numeric, and filling NaNs with 0. It also correctly fills missing 'Category' values with 'Unknown' and then groups the data to calculate the sum of views and likes per category.

However, there are a few areas for improvement:
1.  **Robustness of `clean_numeric_column` function**: The `astype(float, errors='coerce')` within the `clean_numeric_column` function might fail if the column is not of string type to begin with (e.g., if it's already numeric or another type with non-string values that pandas cannot directly use for `str.replace`). The ground truth wisely uses `astype(str)` first to ensure the `str.replace` method works.
2.  **Error Handling/Column Existence**: The code assumes the columns 'Video views', 'Likes', and 'Category' *always* exist. If one of these columns were missing from the CSV, the code would raise a `KeyError` or an `AttributeError` (e.g., trying to call `.str.replace` on a non-string column), instead of gracefully handling it or printing a warning as the ground truth does. This makes the code less robust for varying input datasets.
3.  **Display**: While `print(grouped_data)` displays the results, the ground truth adds a descriptive header (""Sum of Video Views and Likes per Category:""), which is a nice touch for user readability, especially in a production-like scenario.
4.  **Redundancy in `clean_numeric_column` return**: The `clean_numeric_column` function modifies the DataFrame in place, so returning `df` and reassigning `df = clean_numeric_column(df, ...) ` is slightly redundant, though not strictly incorrect. It's more idiomatic to just call the function for its side effect if the intent is in-place modification.",gemma-3-12b-it,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
try:
    df = pd.read_csv(""youtube_trendings.csv"")
except FileNotFoundError:
    print(""Error: youtube_trendings.csv not found in the current directory."")
    exit()

# Clean the Likes column
df['Likes'] = df['Likes'].str.replace(',', '').astype(float, errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

# Fill missing values in Category
df['Category'] = df['Category'].fillna('Unknown')

# Create groups for Musi...",8,"The generated code correctly performs the core tasks: loading the dataset, cleaning the `Likes` column, filling missing `Category` values, creating the two groups, and running the t-test. It prints the t-statistic and p-value and interprets the result based on the given alpha.

**Strong points:**
- Correctly handles `FileNotFoundError`.
- Correctly cleans the `Likes` column (removes commas, converts to numeric, handles errors by coercing to NaN, and fills NaNs with 0).
- Correctly fills missing `Category` values with 'Unknown'.
- Correctly performs the independent two-sample t-test.
- Correctly interprets the p-value against `alpha = 0.05`.

**Areas for Improvement (to reach a higher score):**
- **Hypotheses Statement:** The prompt explicitly asked to ""Clearly state the null and alternative hypotheses for this test."" While the generated code includes them as comments, they are not printed as part of the output, which is a requirement. Stating them clearly in the output makes the scientific process more transparent.
- **`equal_var` in `ttest_ind`:** The ground truth code uses `equal_var=False` (Welch's t-test), which is generally a safer default as it doesn't assume equal variances between the two groups, a common violation in real-world data. The generated code uses the default `equal_var=True`, which assumes equal variances. While not strictly incorrect if the assumption holds, it's good practice to consider Welch's test or explicitly state the assumption. For a benchmark, explicitly following the best statistical practices or matching the ground truth's nuances for robustness is important.
- **Clarity of Output:** The output could be slightly more verbose and user-friendly, similar to the ground truth, indicating what steps are being performed (e.g., ""Dataset loaded successfully,"" ""Likes column cleaned""). This enhances readability for a user running the script.",gemma-3-12b-it,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd

def feature_engineering(csv_file):
    """"""
    Loads the youtube_trendings.csv dataset, performs feature engineering,
    and displays selected columns.

    Args:
        csv_file (str): The path to the CSV file.

    Returns:
        pandas.DataFrame: The dataframe with the engineered features.
    """"""
    df = pd.read_csv(csv_file)

    # Clean Likes and Dislikes
    df['Likes'] = df['Likes'].str.replace(',', '').astype(float, errors='coerce')
    df['Dislikes']...",8,"The generated code successfully completes almost all aspects of the task. It correctly loads the dataset, cleans the `Likes` and `Dislikes` columns by removing commas, converting to numeric, and filling NaNs with zero. It also correctly calculates `TotalEngagement` and `LikeDislikeRatio` and displays the required columns.

**Strong points:**
- **Concise cleaning:** The `Likes` and `Dislikes` cleaning steps are very compact and efficient, chaining `str.replace`, `astype(float, errors='coerce')`, and `fillna(0)`. This is a clean and effective way to handle the requirement.
- **Correct calculations:** Both `TotalEngagement` and `LikeDislikeRatio` are calculated accurately according to the prompt (especially the `+1` for `LikeDislikeRatio`).
- **Proper display:** The required columns are printed correctly.
- **Function encapsulation:** Encapsulating the logic within a function `feature_engineering` with a clear docstring is good practice.
- **`if __name__ == '__main__':` block:** Ensures the function is called only when the script is executed directly.

**Areas for improvement to reach a higher score:**
- **Missing error handling for file loading:** The prompt implies the `youtube_trendings.csv` file should be handled gracefully if not found. The generated code uses `pd.read_csv`, which would raise a `FileNotFoundError` without a `try-except` block. The ground truth includes robust error handling for this.
- **Minor issue with `astype(str)`:** The generated code directly applies `str.replace`. If the 'Likes'/'Dislikes' columns initially contain actual numeric types (e.g., if some rows are already numbers without commas), applying `.str` directly might raise an AttributeError. The ground truth's `astype(str)` ensures this step is robust for mixed-type columns. While modern pandas often handles this gracefully, explicitly converting to string first is safer. In this specific task, it's a minor point as the problem implies they might contain commas which are string-like representations.

Overall, it's a very good solution that achieves most of the requirements and demonstrates good understanding of pandas operations. The main missing piece is the explicit file loading error handling.",gemma-3-12b-it,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd

def analyze_mobile_data():
    """"""
    Loads, cleans, groups, and aggregates mobile data from a CSV file.
    """"""
    try:
        df = pd.read_csv(""mobiles_2025.csv"")
    except FileNotFoundError:
        print(""Error: mobiles_2025.csv not found in the current directory."")
        return

    # Clean Launched Price (USA)
    df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '', regex=False)
    df['Launched Price (USA)'] = df['Launched Pr...",9,"The generated code correctly implements all specified requirements: loading the CSV, cleaning both the 'Launched Price (USA)' and 'Screen Size' columns to numeric (handling errors with `errors='coerce'`), grouping by 'Company Name' and 'Launched Year', and calculating the mean of the cleaned columns for each group. The aggregation syntax using named aggregations is also good practice. The output format `print(aggregated_data)` is also correct as it displays the required information.

Areas for minor improvement (not critical enough to significantly reduce score):
1.  **Readability/Clarity**: When overwriting the original columns with cleaned numeric values, it can sometimes be beneficial to create new columns (e.g., `Cleaned Price`, `Cleaned Screen Size`) to retain the original raw data for debugging or other purposes. The ground truth demonstrates this by creating `Cleaned Price (USD)` and `Cleaned Screen Size (inches)` columns. However, for the given task, overwriting is acceptable.
2.  **Robustness**: Error handling for `FileNotFoundError` is present, which is good. An additional `except Exception as e:` block could be added for more general CSV loading errors, similar to the ground truth, but this is a minor detail.
3.  **String Conversion Before Cleaning**: While pandas often handles mixed types gracefully, explicitly converting columns to string type (e.g., `df['Column'].astype(str)`) before applying string operations can prevent potential `AttributeError` if some values are not strings (e.g., numeric values that slipped past initial checks). The ground truth does this. In this specific case, it likely wasn't an issue for the test data, but it's a good defensive coding practice.",gemma-3-12b-it,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

def clean_camera(camera_string):
    """"""Extracts the MP value from a camera string.""""""
    if isinstance(camera_string, str):
        match = re.search(r'(\d+)MP', camera_string)
        if match:
            return int(match.group(1))
        else:
            return 0
    else:
        return 0

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean camera columns
df['Cleaned Front Camera'] = df['Front Camera'].apply(clean_camera)
df['Cleaned ...",8,"The generated code correctly addresses most of the requirements. It successfully loads the dataset, defines a function to clean camera specifications, applies this function to create cleaned numeric columns, creates the `Total Camera MP` feature, and displays the specified columns for the first 10 rows.

**Strong points:**
*   Correctly uses `pandas` and `re` for the task.
*   The `clean_camera` function is generally correct for extracting the first numerical MP value.
*   Handles non-string inputs by returning 0, which is good.
*   The overall structure is clean and readable.

**Areas for improvement (to achieve a higher score):**
*   **Regex robustness:** The regex `r'(\d+)MP'` specifically looks for digits *followed by ""MP""*. The ground truth's regex `r'\d+'` is more robust as it captures any sequence of digits, which handles cases like ""48MP + 12MP"" (it would still correctly extract 48) or if the specification was just ""12"". While the prompt gave examples like '12MP / 4K' and '48MP + 12MP', the ground truth's approach is slightly more general, ensuring it captures the first numeric sequence even if ""MP"" isn't immediately adjacent or present. This might lead to subtle differences depending on the exact variations in the `mobiles_2025.csv` data. For the given examples, both would likely work, but the ground truth's is more robust.
*   **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for real-world robustness. The generated code lacks this.
*   **Clarity of Display:** The ground truth's use of `.to_string()` for display provides better formatting in a console output, although `print(df.head(10))` is perfectly functional.",gemma-3-12b-it,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
data = pd.read_csv('mobiles_2025.csv')

# Clean RAM
data['RAM'] = data['RAM'].str.replace('GB', '', regex=False)
data['RAM'] = pd.to_numeric(data['RAM'], errors='coerce')

# Clean Battery Capacity
data['Battery Capacity'] = data['Battery Capacity'].str.replace('mAh', '', regex=False)
data...",9,"The generated code correctly performs all requested tasks, including loading the data, cleaning the specified columns, handling conversion errors by coercing to NaN and then dropping rows (for the specified subset), splitting the data, training a Linear Regression model, and evaluating it with R-squared and MAE. The code is well-structured and uses appropriate imports.

A minor improvement could be to include `strip()` after `replace()` to handle potential leading/trailing whitespace, which the ground truth does. Also, the ground truth includes robustness checks like `try-except` for file loading and checks for empty dataframe after cleaning, which were not explicitly asked for but are good practices. However, for the specific requirements outlined in the task, the generated code achieved all of them accurately.",gemma-3-12b-it,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
from scipy import stats
import numpy as np

# Load the dataset
titanic = pd.read_csv('titanic.csv')

# Data Cleaning and Preprocessing
titanic['Age'] = titanic['Age'].fillna(titanic['Age'].median())
titanic['Survived'] = titanic['Survived'].astype(int)
titanic['Pclass'] = titanic['Pclass'].astype(int)

# 1. Chi-Square Test: Pclass vs. Survival
contingency_table = pd.crosstab(titanic['Pclass'], titanic['Survived'])
chi2_statistic, p_value, dof, expected = stats.chi2_...",7,"The generated code correctly performs the chi-square test, t-test, and ANOVA, and calculates effect sizes. It also includes proper imports and addresses most requirements.

However, there are several areas for improvement:

1.  **T-test Cohen's d Calculation:** The formula used for Cohen's d is incorrect. It appears to be `(mean1 - mean2) / sqrt(var1 + var2) / sqrt(2)`. The correct formula for independent samples t-test using pooled standard deviation (or considering Welch's t-test which is typically the default for `scipy.stats.ttest_ind` if `equal_var=False`) should be based on `(mean1 - mean2) / pooled_std`. The ground truth provides a more robust calculation for Cohen's d.
2.  **ANOVA Eta-squared Calculation:** The `ss_between` calculation is incorrect and results in 0, leading to an eta-squared of 0. This is a significant flaw as the effect size for ANOVA is not calculated properly. The ground truth code correctly implements the calculation for `ss_between`.
3.  **Missing Value Handling:**
    *   For the t-test (comparing ages), the generated code imputes missing 'Age' values with the median of the entire 'Age' column *before* splitting into survivor/non-survivor groups. While this allows the t-test to run without NaNs, the `dropna()` approach used in the ground truth is often preferred for statistical tests to avoid artificially reducing variance or biasing means, especially if a large number of values were imputed. The ground truth also explicitly checks if groups are empty after dropping NaNs.
    *   For the ANOVA (fare prices), the generated code assumes 'Fare' has no missing values or handles them implicitly via `groupby().apply(list)`. The ground truth explicitly uses `dropna(subset=['Fare'])`.
4.  **Clarity and Structure:** The output formatting is less detailed than the ground truth, which includes clearer descriptions of hypotheses, explicit mention of the alpha level for each test, and more comprehensive conclusions. The ground truth also adds checks for empty groups for the t-test and ANOVA.
5.  **Robustness for T-test Assumptions:** The generated code does not perform Levene's test to check for equality of variances for the t-test, which is an important assumption check. The ground truth code includes this and uses the `equal_var` parameter in `ttest_ind` accordingly.
6.  **Cramer's V Divisor:** The Cramer's V calculation uses `min((len(contingency_table.columns)-1), (len(contingency_table.index)-1))`. While this is generally correct for a generic contingency table, for a 2xN table (like Survived vs Pclass), `min_dim` should be `min(rows-1, cols-1)`. In this specific case (2 categories for 'Survived', 3 for 'Pclass'), `min_dim` would be 1, which the formula correctly reduces to (since `min(2-1, 3-1) = min(1,2) = 1`). However, the `phi` variable usage is slightly different from the standard `chi2 / N` component. The ground truth's approach is more explicit and directly aligns with the common formula for Cramer's V.

To achieve a higher score, the generated code needs to correct the effect size calculations (especially Eta-squared), improve missing value handling and assumption checks, and enhance the clarity and robustness of its output and methodology.",gemma-3-12b-it,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

try:
    df = pd.read_csv(""imdb_top_250.csv"")
except FileNotFoundError:
    print(""Error: imdb_top_250.csv not found in the current directory."")
    exit()

grouped_data = df.groupby(""Year"").agg({""Rating"": ""mean"", ""Runtime"": ""mean""})

plt.figure(figsize=(12, 6))
plt.plot(grouped_data.index, grouped_data[""Rating""])
plt.title(""Average Rating Over Year"")
plt.xlabel(""Year"")
plt.ylabel(""Average Rating"")
plt.grid(True)
plt.show()
```",7,"The generated code correctly performs the core tasks: loading the CSV, grouping by year, calculating means, and plotting. It successfully creates a line plot with a title and labeled axes.

However, it lacks robustness and completeness in several areas compared to the ground truth:
1.  **Data Cleaning:** The most significant omission is the complete lack of data cleaning for 'Year' and 'Runtime' columns. The ground truth code correctly anticipates potential non-numeric formats (e.g., ""(2000)"" for Year, ""120 min"" for Runtime) and handles them robustly using regular expressions and type conversions. The generated code assumes these columns are already perfectly numeric, which is often not the case with real-world CSVs, especially 'Runtime' which frequently includes "" min"".
2.  **Error Handling (Partial):** While it has a `try-except` for `FileNotFoundError`, it doesn't handle other potential errors like `KeyError` if columns are missing or `ValueError` if the data types are not as expected during aggregation due to unclean data.
3.  **Encapsulation/Modularity:** The ground truth wraps the logic in a function `analyze_imdb_data`, making the code reusable and better organized. The generated code is a script-level execution block.
4.  **Plot Enhancements:** While functional, the plot in the generated code is basic. The ground truth includes additional plotting features like `marker='o'`, `linestyle='-'`, `figsize=(12, 7)`, `fontsize` for title/labels, `xticks(rotation=45)`, `tight_layout()`, etc., which enhance readability and presentation.
5.  **Readability/Clarity:** The ground truth uses a `DATASET_PATH` constant and provides `docstrings` in the function, improving code clarity and maintainability.

The code works for a perfectly clean dataset but would likely break or produce incorrect results with a real-world, slightly messy `imdb_top_250.csv` file, which is common for such datasets. The lack of data cleaning significantly impacts its robustness.",gemma-3-12b-it,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv(""imdb_top_250.csv"")

# Create groups based on Year
group1 = df[df[""Year""] < 1990]
group2 = df[df[""Year""] >= 1990]

# Define null and alternative hypotheses
# Null hypothesis: There is no statistically significant difference in the mean Rating between the two groups.
# Alternative hypothesis: There is a statistically significant difference in the mean Rating between the two groups.

# Perform independent tw...",8,"The generated code correctly performs the core task of loading the data, creating groups, conducting a t-test, and interpreting the results. It has proper imports and a good structure.

Areas for improvement to reach a higher score:
1.  **Robustness (Error Handling):** The given code assumes the CSV file exists and that the 'Year' and 'Rating' columns are immediately numeric. The ground truth includes `try-except` for `FileNotFoundError` and `pd.to_numeric` with `errors='coerce'` followed by `dropna` for better handling of potential data non-numeric types or missing values, making it more robust.
2.  **Explicit Hypotheses:** While the generated code includes comments for the hypotheses, the ground truth explicitly prints them out as requested in the prompt (""Clearly state the null and alternative hypotheses for this test."").
3.  **Clarity of Output:** The ground truth provides more detailed print statements for the t-statistic and p-value (e.g., f-strings with formatting) and gives a more comprehensive interpretation of the conclusion, reiterating the groups involved.
4.  **Edge Case for T-test Inputs:** The ground truth includes a check for `len(ratings_before_1990) < 2` or `len(ratings_after_1990) < 2`, which is good practice to prevent errors if one of the groups ends up with insufficient data for a t-test.",gemma-3-12b-it,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv(""imdb_top_250.csv"")
except FileNotFoundError:
    print(""Error: imdb_top_250.csv not found in the current directory."")
    exit()

# Create the 'Decade' column
def categorize_year(year):
    if 1990 <= year <= 1999:
        return ""1990s""
    elif 2000 <= year <= 2009:
        return ""2000s""
    elif 2010 <= year <= 2019:
        return ""2010s""
    elif 2020 <= year <= 2029:
        return ""2020s""
    else:
        retur...",7,"The generated code correctly loads the dataset, creates the `Decade` column, and displays the required information for the first 10 movies. This addresses most of the task requirements.

However, the method used to create the `Decade` column is not robust or scalable for all possible year ranges. The `categorize_year` function uses a series of `if/elif` statements, which is fine for the decades explicitly listed (1990s, 2000s, 2010s, 2020s), but the `else` condition simply appends 's' to the year (e.g., 1985 becomes ""1985s"" instead of ""1980s""). This is incorrect and doesn't align with the instruction to categorize ""1990-1999 should be '1990s'"".

The ground truth code uses a more robust and vectorized approach (`((df['Year'] // 10) * 10).astype(str) + 's'`) to calculate the decade, which works for any year and is more efficient.

Additionally, the output format for displaying the first 10 rows is not exactly as requested by the ground truth (which uses `to_string(index=False)` for cleaner output and explicit column display). While `print(df[['Title', 'Year', 'Decade']].head(10))` works, the ground truth's approach provides a slightly cleaner and more controlled display.

For a higher score, the code should implement a more generic and correct logic for calculating the decade, similar to the ground truth, and perhaps improve the output formatting slightly. Encapsulating the logic in functions, as the ground truth does, also improves code organization.",gemma-3-12b-it,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Load the dataset
penguins = pd.read_csv('penguins.csv')

# Impute missing values in numerical columns
penguins['culmen_length_mm'] = penguins['culmen_length_mm'].fillna(penguins['culmen_length_mm'].mean())
penguins['culmen_depth_mm'] = penguins['culmen_depth_mm'].fillna(penguins['culmen_depth_mm'].mean())
penguins['flipper_length_mm'] = penguins['flipper_length_mm'].fillna(penguins['flipper_length_mm'].mean())
penguins['body_mass_g'] = penguins['body_mass_g'].fil...",10,"The generated code correctly loads the dataset, performs mean imputation for all specified numerical columns and mode imputation for the 'sex' column. It then correctly groups the data by 'species' and 'sex' and calculates the average `body_mass_g` for each group, displaying the results. The code is concise and directly addresses all requirements of the task. It demonstrates a solid understanding of pandas operations for data cleaning and aggregation.",gemma-3-12b-it,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
penguins = pd.read_csv('penguins.csv')

# Filter the data
penguins_filtered = penguins[penguins['species'].isin(['Adelie', 'Gentoo'])]

# Remove rows with missing values
penguins_cleaned = penguins_filtered.dropna(subset=['flipper_length_mm', 'species'])

# Define null and alternative hypotheses
# Null hypothesis: There is no statistically significant difference in the mean flipper length between Adelie and Gentoo penguins...",9,"The generated code correctly performs all the required steps: loading the data, filtering, handling NAs for the specified columns, performing the t-test, and interpreting the results based on the alpha level. The code is well-structured and uses appropriate libraries.

Strengths:
- Correctly loads and filters the data.
- Accurately handles NA values for the specified columns.
- Correctly separates data for the two groups.
- Performs the t-test using `scipy.stats.ttest_ind`.
- Correctly interprets the p-value against the alpha level and states the conclusion.
- Implicitly states hypotheses through comments.

Areas for minor improvement (preventing a perfect 10):
- **Explicit Hypothesis Statement:** While the hypotheses are stated in comments, the prompt asked to ""Clearly state the null and alternative hypotheses for this test."" The ground truth code explicitly prints these. Making the hypotheses a part of the printed output would align more closely with this requirement.
- **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code does not include this.
- **`equal_var` parameter in `ttest_ind`**: The generated code uses the default `equal_var=True` for `ttest_ind`. While often acceptable, explicitly setting `equal_var=False` (Welch's t-test) is generally more robust when population variances are unknown or unequal, which is often the case in real-world data and a common assumption in statistical tests. The problem statement didn't specify which type of t-test (equal or unequal variances), so the default is technically correct given the lack of specific instruction, but it's a point of consideration for statistical rigor.
- **Reporting precision:** The output prints t-statistic and p-value without explicit formatting, which can lead to very long floating-point numbers. Formatting them to a reasonable number of decimal places (e.g., `:.4f` as in the ground truth) improves readability.

Overall, it's an excellent solution that fulfills the core requirements of the task. The areas for improvement are minor details that enhance robustness, clarity, or presentation rather than correctness of the core statistical analysis.",gemma-3-12b-it,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1. Load the dataset
data = pd.read_csv(""penguins.csv"")

# 2. Select features (X) and target (y)
X = data[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]
y = data['species']

# 3. Drop rows with missing values
data = data.dropna(subset=['specie...",9,"The generated code correctly implements all the requirements of the task: data loading, feature and target selection, handling missing values, target encoding, data splitting, model training, and accuracy reporting. The imports are proper, and the code structure is logical and easy to follow.

One minor area for improvement is in step 3 (dropping NAs). The generated code reassigns `X` and `y` after dropping NAs on the `data` DataFrame. While this works, it's slightly less efficient than directly separating `X` and `y` *after* `dropna` on the relevant columns, as done in the ground truth. The ground truth first creates `df_cleaned` which contains only the selected features and target after dropping NAs, then separates `X` and `y` from `df_cleaned`, which is conceptually cleaner and prevents potential desynchronization if the initial `X` and `y` were used later. However, for this specific task, the generated code's approach still produces the correct `X` and `y` for further steps.

Additionally, the ground truth includes more robust error handling (e.g., `try-except` for file loading) and informative print statements for better user experience and debugging, which are good practices but were not explicitly requested as part of the core functionality.

Overall, the generated code is highly correct and complete based on the specified task requirements.",gemma-3-12b-it,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the dataset
try:
    penguins = pd.read_csv(""penguins.csv"")
except FileNotFoundError:
    print(""Error: penguins.csv not found. Make sure the file is in the current directory."")
    exit()

# 2. Drop rows with NA values in 'body_mass_g' or 'species'
penguins = penguins.dropna(subset=['body_mass_g', 'species'])

# 3. Calculate the average 'body_mass_g' for each 'species'
avg_body_mass = penguins.groupby...",9,"The generated code successfully completes all tasks outlined in the prompt. It correctly loads the data, handles NA values for the specified columns, calculates the average body mass per species, and generates a well-labeled bar plot. The code is clean, concise, and uses appropriate libraries (pandas, matplotlib, seaborn).

Strong points:
- Correctly implements all required steps.
- Handles `FileNotFoundError` gracefully.
- Uses `dropna(subset=[...])` for targeted NA removal, which is good practice.
- Generates a bar plot with appropriate title and axis labels.
- Uses `plt.xticks(rotation=45)` and `plt.tight_layout()` for better plot readability.

Areas for minor improvement (not critical enough to deduct significant points):
- The ground truth uses `plt.title(f'Average {TARGET_COLUMN.replace(""_"", "" "").title()} per {GROUPING_COLUMN.title()}', fontsize=16)` which dynamically creates a more informative title from the column names, making it more robust if column names change. The generated code uses hardcoded `'Average Body Mass by Species'`. While acceptable for this specific task, dynamic title generation is a slightly better practice.
- The ground truth also adds `plt.grid(axis='y', linestyle='--', alpha=0.7)` which can improve readability, although not explicitly requested.

Overall, an excellent submission that fully meets the requirements.",gemma-3-12b-it,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load the dataset
try:
    penguins = pd.read_csv(""penguins.csv"")
except FileNotFoundError:
    print(""Error: penguins.csv not found in the current directory."")
    exit()

# 2. Drop rows with NA values in relevant columns
penguins = penguins.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])

# 3. Create the 'culmen_area' feature
penguins['culmen_area'] = penguins['cu...",9,"The generated code correctly addresses all requirements of the task. It successfully loads the dataset, drops NA values from the specified columns, creates the `culmen_area` feature, calculates the Pearson correlation matrix for the required columns, and visualizes it as an annotated heatmap with the correct title. The use of `try-except` for file loading is good practice. The code is clean, concise, and follows a logical flow.

One minor suggestion for improvement, though not critical for correctness, would be to add `plt.tight_layout()` before `plt.show()` in the heatmap visualization to prevent labels on the axes from overlapping with the plot edges, especially if the number of features increases. The `figsize` is also a bit arbitrary and might be slightly improved for some screen sizes, but `(10, 8)` is a reasonable default. The ground truth also explicitly adds `.copy()` after `dropna` which is good practice to avoid `SettingWithCopyWarning`, though not strictly necessary if no further modifications are made to the `penguins` DataFrame before calculating correlation. However, for a user-facing application, the warnings can be confusing. Overall, a very good solution.",gemma-3-12b-it,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
penguins = pd.read_csv('penguins.csv')

# Select features and target variable
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
target = 'species'
X = penguins[features]
y = penguins[target]

# Impute numerical features with the mean
for col in ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']:
    X[col] = X[col].fillna(X[col].mean())

# Impute 'se...",8,"The generated code correctly addresses most of the requirements of the task.

**Strong Points:**
- **Correct Data Loading and Feature Selection:** The code successfully loads the `penguins.csv` dataset and selects the specified features and target.
- **Correct Numerical Imputation:** It correctly imputes missing values in numerical columns with their respective means.
- **Correct Sex Imputation and Mapping:** It correctly imputes 'sex' NA values with 'Unknown' and then maps 'MALE', 'FEMALE', and 'Unknown' to 0, 1, and -1, respectively.
- **Correct One-Hot Encoding:** It correctly applies one-hot encoding to the 'island' column.
- **Correct Output:** It prints the shape of the resulting feature matrix and the first 5 rows, as requested.

**Areas for Improvement (to achieve a higher score):**
- **Clarity on Target Separation:** While `y = penguins[target]` is present, the final output (`X.shape` and `X.head()`) only relates to the features. The task implies preparing the data for classification, and often `X` and `y` are explicitly separated at the end. The ground truth correctly separates `X` and `y` before the final print statements. Although not explicitly asked to print `y`'s shape or head, making `X` explicitly the feature matrix and implicitly separating `y` for a classification task is good practice.
- **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code assumes the file will always be present.
- **Copying DataFrames:** Directly modifying `X[col]` after `X = penguins[features]` might lead to `SettingWithCopyWarning` in some pandas versions if `X` is considered a view rather than a copy. While it often works, `df.copy()` is safer, as demonstrated in the ground truth.
- **Clarity of Output:** The ground truth code provides more descriptive print statements throughout the process, indicating what step is being performed. This improves readability and debugging.",gemma-3-12b-it,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST0...",9,"The generated code correctly performs all the required operations: creating the DataFrame, calculating total spending for each customer, identifying and printing the top 3 customers by spending, and calculating the total quantity of items sold per category. The code is concise and uses appropriate pandas functions.

One minor area for improvement is in the printing of the top 3 customers. The ground truth code formats the output to be more readable with a loop and f-string formatting, including a dollar sign and two decimal places. The generated code simply prints the pandas Series, which is functional but less user-friendly in presentation. Also, the ground truth code used `LineTotal` for the per-item calculation, which is a good practice for clarity in operations, while `Total_Spending` was used in the generated code for the _per-item_ calculation, which can be slightly confusing since it's ultimately summed for the *customer's* total spending. However, the logic remains sound.",gemma-3-12b-it,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
titanic_df = pd.read_csv('titanic.csv')

# Data prepr...",6,"The generated code addresses most of the requirements but has significant issues in data preprocessing and visualization, which impact its correctness and completeness.

**Strengths:**
*   Correctly loads data and splits into train/test sets.
*   Trains a Logistic Regression model.
*   Calculates and prints the required evaluation metrics (accuracy, precision, recall, f1, ROC-AUC).
*   Attempts confusion matrix and ROC curve visualization.
*   Performs cross-validation.

**Areas for Improvement/Weaknesses:**

1.  **Data Preprocessing (Critical):**
    *   **Missing Values:** It drops rows with NaNs in 'Age' and 'Fare' (`dropna`). The ground truth imputes 'Age' with median and 'Embarked' with mode, which is a more robust approach for handling missing data, preserving more samples.
    *   **Categorical Encoding:** It uses `LabelEncoder` for 'Sex', 'Embarked', and 'Survived'.
        *   `LabelEncoder` is inappropriate for 'Sex' and 'Embarked' because it assigns arbitrary numerical order, implying a false ordinal relationship, which can negatively affect model performance for nominal categorical data. `OneHotEncoder` is the correct approach for 'Sex' and 'Embarked' as used in the ground truth.
        *   Encoding 'Survived' (the target variable) with `LabelEncoder` is technically okay for `LogisticRegression` as it expects 0/1, but it's redundant if the original column contains 0 and 1 directly (which `survival` typically does). This is less of an issue than for features.
    *   **Feature Selection/Engineering:** It only selects 'Age', 'Sex', 'Fare', 'Embarked'. It entirely misses the handling of 'Pclass' (which is treated as categorical in the ground truth and is a significant predictor), and misses the feature engineering of 'FamilySize' from 'SibSp' and 'Parch', and dropping 'Name', 'Ticket', 'Cabin', 'PassengerId'. This significantly limits the model's potential performance and deviates from a standard Titanic modeling approach.
    *   **Preprocessing Pipeline:** The generated code applies `StandardScaler` to `X` *before* splitting, which is a data leakage issue. The scaled data `X` then contains information from the test set (`scaler.fit_transform(X)`). The ground truth correctly uses `Pipeline` and `ColumnTransformer` to ensure scaling is applied only after the split (specifically, `fit` on training data and `transform` on both train/test), preventing leakage and applying different transformations to different columns correctly.

2.  **Visualization:**
    *   **Confusion Matrix:** While it creates a matrix and displays it, it does not use `ConfusionMatrixDisplay.from_estimator` which is the recommended `sklearn` way for cleaner, standardized plots. It manually creates the plot.
    *   **ROC Curve:**
        *   The `fpr` and `tpr` variables are implicitly assumed but not defined where the plot is called (`plt.plot(fpr, tpr, ...)`). They are defined *after* the plot call, leading to a `NameError`.
        *   It does not use `RocCurveDisplay.from_estimator`, similar to the confusion matrix, which is the standard and more robust approach.

3.  **Feature Coefficients Interpretation:**
    *   It prints `model.coef_`, but since `LabelEncoder` was used, the interpretation is difficult and less meaningful than with one-hot encoded features. It also doesn't provide the feature names associated with the coefficients, making the output `[[...]]` unclear. The ground truth correctly extracts feature names after one-hot encoding and displays them with their coefficients for clear interpretation. The generated code also offers no textual interpretation of the coefficients as requested.

4.  **Completeness:**
    *   Lacks robust error handling for `titanic.csv` not found.
    *   Missing the detailed interpretation of coefficients.
    *   Missing stratify for `cross_val_score` for consistency in cross-validation across folds, especially important for imbalanced datasets.
    *   Does not print the mean and standard deviation of cross-validation scores for all metrics, only accuracy and its mean.

The core issue lies in the preprocessing that makes the model less effective and the interpretation less clear, combined with visualization errors. The ground truth provides a much more robust, standard, and complete solution using `sklearn` pipelines for preprocessing, which is best practice.",gemma-3-12b-it,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']),
        'Ticker': ['TECHCO']*8,
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
        'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0, 157.0, 154...",10,"The generated code is correct, complete, and of good quality. It correctly creates the DataFrame, sets 'Date' as the index, calculates the 3-day Simple Moving Average for 'Close' prices, and computes the percentage change in 'Volume' from the previous day, both handling NaN values as specified. The final DataFrame with the new columns is printed accurately. The use of `df.set_index('Date')` instead of `df.set_index('Date', inplace=True)` is also perfectly fine as it reassigns the DataFrame, achieving the same result.",gemma-3-12b-it,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]}

df = pd.DataFrame(data)

...",10,"The generated code is excellent. It correctly creates the DataFrame, calculates the 'Score_Improvement' column, computes the average improvement per subject, and accurately identifies the student(s) meeting the specific final score and attendance criteria. The code is clean, concise, and uses appropriate pandas functions. All requirements of the task are met. The output format for printing the results is also clear.",gemma-3-12b-it,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Filter the DataFrame
filtered_df = df[(df[""Department""]...",9,"The generated code is highly correct and effectively solves all aspects of the task. It correctly defines the DataFrame, filters it accurately based on two conditions, and calculates the average `ProjectsCompleted` for the filtered subset. It also includes an `if not filtered_df.empty:` check, which is good practice for robustness. The code quality is excellent with proper imports and clear variable names.

The only minor area for improvement, which is not critical to correctness but good for user clarity, is the precision formatting of the output for the average. The ground truth formats it to two decimal places (`.2f`), which is often desirable for numerical averages. The generated code prints it without specific formatting, which might result in a long float for some calculations. This is a very minor aesthetic point and doesn't affect the core functionality.",gemma-3-12b-it,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate conversion rate for Desktop users
desktop_users = df[df[""DeviceType""] == ""Desktop""]
desktop_conversion_rate = deskt...",9,"The generated code correctly defines the DataFrame and accurately calculates and prints the conversion rates for Desktop and Mobile users with proper formatting. All requirements of the task are met.

The only minor difference from the ground truth is the lack of explicit handling for edge cases where no 'Desktop' or 'Mobile' users might exist (e.g., if the filtered DataFrame is empty, `.mean()` on an empty series of booleans would return NaN). While this specific dataset doesn't trigger that issue, robust code would include checks for empty filtered DataFrames, as seen in the ground truth. However, for this specific problem, it's not a critical flaw.

Strong points:
- Correct DataFrame creation.
- Accurate filtering for `DeviceType`.
- Correct use of `.mean()` on the boolean 'Converted' column to get the proportion, then multiplying by 100 for percentage.
- Clear and correct f-string formatting for output.",gemma-3-12b-it,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

for sensor_id in df[""SensorID""].unique():
    sensor_data = df[df[""SensorID""] =...",9,"The generated code correctly defines the DataFrame and accurately calculates and prints the average air quality index for each sensor. It successfully meets all requirements of the task.

**Strong points:**
*   **Correctness:** The code produces the exact required output for the average AirQualityIndex per SensorID.
*   **Completeness:** It addresses both parts of the task: DataFrame creation and per-sensor average calculation/printing.
*   **Readability:** The variable names are clear, and the f-string for printing makes the output easy to understand.
*   **Pythonic (mostly):** The use of `df[""SensorID""].unique()` to iterate through unique sensor IDs is good.

**Areas for minor improvement (why it's not a 10):**
*   **Efficiency for larger datasets:** The `groupby().mean()` method used in the ground truth is generally more efficient and idiomatic for this type of aggregation in pandas, especially with larger DataFrames. The generated code iterates through unique sensor IDs and filters the DataFrame in each iteration, which can be less efficient for very large datasets compared to a single `groupby` operation.
*   **Output formatting (minor):** The ground truth formats the output to two decimal places (`.2f`), which is good practice for numerical results like averages. The generated code does not explicitly format the float, which might lead to more decimal places than necessary.

Overall, it's a very good solution that fully solves the problem. The suggested improvements are primarily about efficiency and best practices rather than correctness.",gemma-3-12b-it,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

# Create ""EngagementScore"" feature
df[""EngagementScore""] = df[""Likes""] + (df[""Shares""]...",9,"The generated code correctly defined the DataFrame, calculated the `EngagementScore`, filtered for Instagram posts, and accurately calculated and printed their average `EngagementScore`. All task requirements are met. The code is clean and concise.

One minor point for improvement, which is not a correctness issue but a best practice, is to format the output string for readability, similar to the ground truth (e.g., `print(f""Average EngagementScore for Instagram posts: {average_engagement_score:.2f}"")`). Also, the ground truth included a safeguard `if not instagram_posts_df.empty:` which is good practice for robust code, though not strictly necessary for this specific dataset.",gemma-3-12b-it,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the ""LengthOfStay"" column, accurately filters for ""Flu"" diagnosis, and correctly calculates the average ""LengthOfStay"" for these patients. All requirements of the task are met, and the code is clean and efficient. The use of `.dt.days` for calculating length of stay is correct and idiomatic pandas. The output is exactly the numerical result requested.",gemma-3-12b-it,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}
df = pd.DataFrame(data)

# Filter for Sci-Fi movies
s...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately filters for Sci-Fi movies, and calculates their average rating. The code includes a robust check for the case where no Sci-Fi movies are found, which is a good practice for real-world data handling. The overall structure is clean and readable, and all task requirements are met.",gemma-3-12b-it,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

# Filter the DataFrame to include only Toyota cars
toyota_cars = df[d...",9,"The generated code correctly defines the DataFrame, filters for Toyota cars, and accurately calculates their average mileage. The code quality is good, with proper imports and a clear, concise structure. It directly addresses all requirements of the task.

The only minor point of distinction from the ground truth is the print formatting of the output, which the ground truth specifies as `f""The average Mileage for Toyota cars is: {average_mileage_toyota:.2f}""`. The generated code simply prints the raw float value, which is acceptable but less user-friendly in its presentation. This does not affect correctness or completeness of the core tasks.",gemma-3-12b-it,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 80...",9,"The generated code correctly defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat. It correctly handles the edge case where no Fire-type Pokemon are found. The code quality is good, with proper imports and clear variable names.

The only minor difference from the ground truth is the lack of explicit formatting for the average attack stat (e.g., `:.2f`). While not strictly required by the prompt, it's a common practice for numerical output. Given the prompt's focus on functionality, this is a very minor point.",gemma-3-12b-it,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Load the dataset
df = pd.read_csv('titanic.csv')

# Data Cleaning and Preprocessing
df['Age'] = df['Age'].fillna(df['Age'].median())
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
df['Title'] = df['Name'].str.extract('([A-Za-z]+)\.')
df['Title'] = df['Title'].replace(['Mr', 'Miss', 'Mrs'], 'Other')
df['Title'] = df['...",7,"The generated code addresses most of the task requirements but lacks some sophistication and attention to detail present in the ground truth.

**Strong Points:**
- All required libraries are imported.
- All five visualization types are attempted, including the interactive Plotly scatter plot.
- Basic data preprocessing (age imputation) is done.
- The use of `hue` in bar plots helps to show survival rates by different groups (e.g., `hue='Pclass'` for survival by class, though the ground truth plots the mean survival rate directly for cleaner comparison).

**Areas for Improvement (Specific Feedback):**

1.  **Multi-panel figure (Task 1):**
    *   The generated code plots `hue='Pclass'` when `x='Survived'`. This creates bars for counts of survivors/non-survivors broken down by class, gender, etc., which is not directly a ""survival rate by a demographic group."" The ground truth correctly calculates the mean survival rate for each group (`df.groupby([...])['Survived'].mean()`) and then plots this rate. This is a significant difference in interpretation of ""survival rates by different demographic groups.""
    *   The `AgeGroup` bins in the generated code are slightly less granular and descriptive than the ground truth (e.g., 'Adult' spans 18-60, which is very broad).

2.  **Heatmap (Task 2):**
    *   Correctly implemented using `groupby` and `unstack` to get survival rates by class and gender.
    *   Minor: The ground truth uses `Pclass_Cat` (categorical labels) which improves readability of the heatmap axes; the generated uses raw `Pclass` numbers.

3.  **Violin Plots (Task 3):**
    *   Correctly implemented using `hue='Sex'` and `split=True` to compare age distributions of survivors vs non-survivors by gender.

4.  **Interactive Scatter Plot (Task 4):**
    *   The generated code uses `color='Survived'` and `symbol='Pclass'`. The prompt asked for ""different colors for passenger classes"" and implied survival as a separate differentiator. The ground truth correctly uses color for passenger class and symbol for survival, which is a better representation for the asked task.
    *   `hover_data` is basic; ground truth includes more useful information and uses `hover_name`.

5.  **Dashboard-style Figure (Task 5):**
    *   The dashboard attempts to combine multiple visualizations, but it includes external visualizations like 'Distribution of Family Size' and 'Correlation Heatmap' which were not explicitly requested, while missing the key request of ""proper annotations and insights"" (the textual summary with findings in the ground truth).
    *   Subplots 1 and 2 in the dashboard repeat the same misinterpretation of ""survival rate"" as in Task 1 (plotting counts by hue instead of the actual mean survival rate).
    *   The aesthetic quality and storytelling aspect of the dashboard is not as strong as the ground truth, particularly lacking the textual insight panel.
    *   `fig_dashboard.add_subplot(3, 2, 6)` for correlation heatmap is decent but not explicitly a ""survival"" visualization in the context of the requested dashboard.

**Minor Issues / Code Quality:**
-   The generated code adds extra preprocessing for `Embarked`, `FamilySize`, `Title`, and `SibSp`/`Parch` which are not directly used in the *explicitly requested* visualizations (except `FamilySize` in the dashboard, which wasn't part of the core requirements). While good for general data exploration, it adds unnecessary complexity for the specific task.
-   Error handling for `titanic.csv` not found is missing.
-   The comments are less detailed and focused on the task's specific goals compared to the ground truth.
-   No explicit `plt.rcParams` adjustments for figure quality/size as done in the ground truth for better visual consistency.",gemma-3-12b-it,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, ...",9,"The generated code correctly defines the DataFrame, filters for the 'Dystopian' genre, and accurately calculates the average stock for that genre. It addresses all core requirements of the task. The code is concise and directly implements the solution.

One minor area for improvement, which is present in the ground truth but not strictly required by the prompt's output format, is the inclusion of a more descriptive print statement for the final result (e.g., `print(f""Average stock for Dystopian books: {average_stock:.2f}"")`). The ground truth also includes an edge case check for an empty resulting DataFrame, which is good practice but not explicitly asked for in the prompt. Given the simplicity of this task, the generated code is excellent.",gemma-3-12b-it,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
from scipy import stats
import numpy as np

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Define the significance level
alpha = 0.05

# Numerical features
numerical_features = ['pH', 'Hardness', 'Solids', 'Alkalinity', 'Sulfate', 'Chlorides']

# Potability as a categorical variable
df['Potability'] = df['Potability'].astype('category')

# 1. T-tests
print(""T-tests:"")
for feature in numerical_features:
    potable = df[df['Potability'] == 1][feature]...",6,"The generated code attempts to address all parts of the prompt but has several significant issues in implementation and statistical accuracy.

**Correctness & Completeness Issues:**

*   **Missing Value Handling:** The most significant omission is the complete lack of missing value handling. The `water_potability.csv` dataset is known to have missing values, especially in `pH`, `Sulfate`, and `Trihalomethanes`. Performing statistical tests directly on columns with NaNs will lead to `nan` results or errors, invalidating the analysis. The ground truth explicitly addresses this by imputing means.
*   **Cohen's d Calculation:** The formula for Cohen's d provided by the generated code (`abs(t_statistic) / np.sqrt(len(potable) + len(non_potable))`) is incorrect. Cohen's d is defined as the difference between means divided by the *pooled standard deviation*. The ground truth correctly implements Cohen's d calculation using the pooled standard deviation.
*   **T-test `equal_var` parameter:** The generated code uses the default `equal_var=True` for `stats.ttest_ind`. For real-world data, especially when comparing two independent groups, it's generally safer and more robust to assume unequal variances and use Welch's t-test (`equal_var=False`), as performed in the ground truth. This is a crucial statistical consideration.
*   **Chi-Square Goodness-of-Fit Test Logic:**
    *   The `expected_frequencies` calculation (`df['Potability'].sum() / len(df) * observed_frequencies.index`) is conceptually flawed. The prompt asks to check if potability follows *an* expected distribution, implying a predefined one (e.g., uniform 50/50). The generated code's calculation tries to derive expected frequencies from the *observed* mean proportion, which doesn't make sense for a goodness-of-fit test against a *hypothesized* distribution. For a binary variable, comparing against a 50/50 split is a standard and interpretable goodness-of-fit test. The ground truth correctly assumes a 50/50 expected distribution.
    *   The `observed_frequencies.index` will be `[0, 1]` if both categories exist, making `expected_frequencies` result in something like `[0, avg_potability_proportion]`. This is not a valid expected frequency distribution for a goodness-of-fit test.
*   **Numerical Features List:** The generated code hardcodes a limited list of numerical features (`'pH', 'Hardness', 'Solids', 'Alkalinity', 'Sulfate', 'Chlorides'`). The `water_potability.csv` dataset has more numerical features (e.g., `Chloramines`, `Organic_carbon`, `Trihalomethanes`, `Turbidity`). The ground truth correctly identifies all numerical features dynamically by dropping 'Potability'. This affects the completeness of the analysis.
*   **Interpretation Detail:** The interpretations are quite brief (""Significant difference"" vs. ""No significant difference"", ""Data is normally distributed"" vs. ""Data is not normally distributed""). The ground truth provides more detailed interpretations, explicitly mentioning rejecting/failing to reject the null hypothesis and tying it back to the significance level.
*   **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.

**Code Quality Issues:**

*   **Comments & Readability:** The ground truth code is much better commented, explaining the rationale behind choices (e.g., imputation strategy, why Welch's t-test is used, interpretation of Cohen's d ranges). The generated code has minimal comments.
*   **Structure:** While it follows the requested order, the ground truth structures the code with clear sections and print statements for better readability of the output.
*   **Redundant Type Conversion:** `df['Potability'] = df['Potability'].astype('category')` is unnecessary for the statistical tests performed and doesn't impact their correctness.

Overall, the generated code demonstrates a basic understanding of the tests but critically fails on data preprocessing (missing values), incorrect statistical formula (Cohen's d), and flawed chi-square logic. The lack of robust handling for missing values alone makes the direct execution on the specified dataset problematic.",gemma-3-12b-it,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_curve
import m...",6,"The generated code addresses most requirements but has a few critical flaws that prevent it from being a strong solution.

**Strengths:**
*   **Comprehensive Metric List:** The code calculates all required metrics (accuracy, precision, recall, F1-score, ROC-AUC).
*   **Multiple Models:** Logistic Regression, Random Forest, and SVM are all included.
*   **Train-Test Split and Scaling:** These are correctly applied.
*   **Visualization:** ROC curves and confusion matrices are plotted for each model, though the confusion matrix plotting could be improved.
*   **Feature Importance:** Attempts to show feature importance for LR and RF.
*   **Cross-Validation:** Implements k-fold cross-validation.

**Weaknesses and Areas for Improvement:**

1.  **Incorrect Missing Value Handling (Critical):** The generated code uses `data = data.dropna()`. This is **incorrect** and problematic. `dropna()` removes rows with *any* missing values, leading to significant data loss (roughly 20% of the dataset in this case, all rows where Sulfates, Conductivity, or pH are NaN). The task specifically asked for ""handling missing values"" which typically implies imputation, not deletion, especially when the missingness is not extensive. The ground truth correctly uses `SimpleImputer(strategy='median')` after the train-test split to avoid data leakage.

2.  **Missing `probability=True` for SVC:** The `SVC` initializer in `models` originally had `probability_score=True`, which is a typo; it should be `probability=True` to enable `predict_proba` for ROC-AUC calculation. While it was corrected in `GridSearchCV`, the original `models` dictionary definition still has the typo. This means the individual `SVC` model without GridSearchCV would fail. (Note: The `GridSearchCV` implicitly uses the correctly specified `SVC` in `param_grid_svm`, so the actual trained best `SVC` model does have probability enabled).

3.  **Untrained Models for Evaluation:** The `models` dictionary is defined initially with untrained models. Then, GridSearchCV is applied to *new* instances of these models, and the `best_lr`, `best_rf`, `best_svm` are assigned back to the `models` dictionary. This is fine. However, the `cross_val_score` is performed on these *trained* models within the `for` loop, but it should generally be performed on a *fresh* model instance or a pipeline to ensure proper cross-validation where training happens *within* each fold. The ground truth uses a pipeline which correctly encapsulates preprocessing and training within the CV folds. Using a model already trained on the training data for `cross_val_score` for *that same* training data is conceptually messy, though not strictly incorrect if the goal is only to get CV metrics on the training set.

4.  **Inefficient Cross-Validation:** The `cross_val_score` is called for 'accuracy' on the *training data* `X_train, y_train`, but the task required k-fold cross-validation (k=5) to assess stability, typically on the *full* dataset (or properly integrated into a pipeline on the full dataset) to get a robust estimate of model performance. The usage here is limited to the training set's accuracy, and not all metrics (precision, recall, f1, roc-auc) are calculated for CV as requested. The ground truth code correctly uses a `Pipeline` with the *full* dataset `X, y` for robust cross-validation of all specified metrics.

5.  **Feature Importance Plotting for LR:** For Logistic Regression, coefficients are plotted, but they are generally more interpretable as absolute values for importance. The current plot uses the coefficients directly, which can be negative, making the bar plot harder to interpret for magnitude. The ground truth uses `abs(best_model.coef_)` for the plot. Also, `feature_names = X.columns` is correct, but the generated code misses defining `X` as a DataFrame for this to work after `X = scaler.fit_transform(X)`.

6.  **Confusion Matrix Visualization:** While confusion matrices are generated, the plotting is quite basic. The ground truth uses `sns.heatmap` and `ConfusionMatrixDisplay.from_estimator` for better visualization. The current confusion matrix plot lacks proper labels for cells or `annot=True` for counts.

7.  **ROC Curve Plotting:** The ROC curves are individually plotted for each model, which is fine, but the task asked for ""ROC curves for each model"", and a combined plot (as shown in the ground truth) is often more informative for comparison.

8.  **Redundant GridSearchCV and Model Retraining:** The code trains models with GridSearchCV, then stores the *best estimators* back into the `models` dictionary. This is perfectly valid. However, the subsequent loop immediately calls `cross_val_score` on these, and then `model.predict` and `model.predict_proba` on the `X_test`. While the models are indeed trained (by GridSearchCV), the typical MLOps flow would be to train the *final* model on the full training set (and optionally the best parameters from CV) and then evaluate on the independent test set. The code's flow is acceptable, but the cross-validation part is weak.

Overall, the data preprocessing (missing value handling) is the most significant flaw, leading to incorrect input data. The cross-validation implementation also needs significant improvement.",gemma-3-12b-it,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.feature_selection import SelectKBest, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Display initial data information
print(""Initial Dataset Info:\n"", df.info())
print(""\nInitial Dataset Descript...",5,"The generated code attempts to address all parts of the task but suffers from several critical issues and omissions that significantly impact its correctness and completeness.

**Critical Issues/Omissions:**

1.  **Missing Value Imputation:** The ground truth code explicitly handles missing values using `fillna(df[col].mean())`. The generated code loads the data and then immediately proceeds with feature engineering without any explicit missing value handling. This is a critical oversight, as the `water_potability.csv` dataset is known to have NaNs. This will cause subsequent operations (like scaling and correlation) to fail or produce incorrect results if not handled.
2.  **Feature Engineering Insufficiency and Errors:**
    *   **Ratio Features:** The generated code creates a few ratio features but names them incorrectly (e.g., `pH_Chloramines` instead of `pH_Chloramines_Ratio`). More importantly, it uses non-existent columns like `'Total solids'` (should be `'Solids'`) and `'Chloride'` (should be `'Chloramines'` for some of the suggested ratios, or `'Sulfate'` for another, depending on intent), and `'Alkalinity'` (does not exist in the dataset). This makes the engineered features incorrect, or causes the code to fail.
    *   **Polynomial Features:** The generated code only creates simple squared terms (`pH_squared`, `Hardness_squared`). The prompt specifically asks for `PolynomialFeatures` from `sklearn.preprocessing` which can generate interaction terms (e.g., `pH*Hardness`) and higher-order terms in a structured way. The generated code's approach for polynomial features is very limited and doesn't utilize the specified method.
    *   **Interaction Terms:** The generated code's interaction terms also suffer from incorrect column names (`'Alkalinity'`).
3.  **Data Transformation Order:** The generated code performs `StandardScaler` on `X` *before* adding engineered features to `X`. This means the engineered features are not scaled, or more accurately, the base DataFrame `df` is never correctly updated with the engineered features *before* scaling. It essentially scales the *original* features, then the engineered features are added on top of a ""standardized"" `df_transformed` in a way that doesn't make sense for analysis. The scaling should happen *after* all features (original + engineered) are created and combined.
4.  **Correlation Analysis:**
    *   The `df` used for correlation analysis will be a mix of scaled original features and unscaled engineered features (due to the scaling order error), leading to incorrect correlation values.
    *   The `if len(top_features) > 0:` condition in pairwise plots is mostly redundant; the problem is whether the calculated `top_5_correlated` list contains *valid* features found in `df`.
5.  **Visualization - Feature Importance:** The feature importance plot uses `rf_model.feature_importances_` where `rf_model` was trained on the *scaled* data for RFE, but the plot uses `feature_names = df.drop('Potability', axis=1).columns` which points to the original, non-engineered, or incorrectly mixed, `df`'s columns. This disconnect can lead to misinterpretation. Furthermore, the prompt asked for ""feature importance plots"" generically and ""pairwise scatter plots for top correlated features."" The generated code's pairplot implementation is problematic (see point 6).
6.  **Visualization - Pairwise Scatter Plots:** The nested loop for pairwise scatter plots (`for i in range(len(top_features)): for j in range(i + 1, len(top_features)):`) creates *separate plots and windows for each pair*, which is highly inefficient and not what `sns.pairplot` is designed for. `sns.pairplot` creates a single, consolidated grid of scatter plots (and histograms/KDEs) for multiple variables, which is what the ground truth does and is generally preferred for this type of visualization. The generated code fails to meet the specified visualization requirement in a meaningful way.
7.  **Comparison of Feature Sets:** The comparison is very superficial (`list(set(selected_features_selectkbest) & set(selected_features_rfe))`). The prompt asked for comparison *and analysis of overlap*, which typically includes intersection and differences.

**Minor Improvements:**

*   Missing value handling should be done explicitly (e.g., imputation).
*   Using `PolynomialFeatures` from `sklearn` for polynomial features would be more robust and complete.
*   The `StandardScaler` application needs to be after all feature engineering.
*   The `SelectKBest` score function should be `f_classif` explicitly to avoid potential warnings or misinterpretations, though it's the default for classification.
*   The RFE `n_features_to_select` should be specified as an integer, not a variable not defined.
*   `sns.set_style(""whitegrid"")` is good practice for cleaner plots.
*   Error handling for file loading is a nice touch.",gemma-3-12b-it,LLM_judge_config
