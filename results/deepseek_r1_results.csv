task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd

df = pd.read_csv('winequality-red.csv', sep=';')

print(""Column names and data types:"")
for col in df.columns:
    dtype = 'categorical' if df[col].dtype == 'object' else 'numerical'
    print(f""{col}: {dtype}"")

numerical_cols = df.select_dtypes(include='number').columns
desc_stats = df[numerical_cols].describe().transpose()
print(""\nDescriptive statistics for numerical columns:"")
print(desc_stats[['mean', '50%', 'std', 'min', 'max', '25%', '75%']])

print(f""\nTot...",6,"The generated code correctly addresses most of the task requirements but has some notable issues and areas for improvement compared to the ground truth.

**Positives:**
*   **Correct Loading:** Loads the CSV correctly.
*   **Descriptive Statistics:** Calculates and prints the descriptive statistics for numerical columns using `describe()`, which is efficient. It correctly transposes the output and selects the requested statistics.
*   **Dimensions:** Correctly reports the total number of rows and columns.
*   **Imports:** Includes essential imports.

**Areas for Improvement / Issues:**

1.  **Data Type Identification (Completeness and Accuracy):**
    *   The `df = pd.read_csv('winequality-red.csv', sep=';')` line in the generated code uses `sep=';'`. While this is correct for some versions/sources of the Wine Quality dataset, the problem statement implies `winequality-red.csv` and typical `read_csv` usage assumes comma-separated if not specified. The ground truth assumes comma separation, which is the default for `pd.read_csv`. If the provided `winequality-red.csv` is indeed semicolon-separated, this line is correct. However, for a generic `winequality-red.csv`, a comma separator is more common. This might lead to incorrect data loading based on the file's actual delimiter.
    *   The initial data type identification (`dtype = 'categorical' if df[col].dtype == 'object' else 'numerical'`) is too simplistic. It only differentiates between `object` and everything else. It fails to explicitly identify other non-numerical dtypes (like boolean, datetime) or specialized numerical dtypes (like integer vs. float). The ground truth uses `pd.api.types.is_numeric_dtype` and `pd.api.types.is_object_dtype` which are more robust.
    *   For the wine quality dataset, *all* columns are numerical initially (though `quality` is often treated as ordinal). The generated code's initial `dtype` check will incorrectly report all initially numerical columns as 'numerical', failing to distinguish the `quality` column in the initial printout.

2.  **Categorical/Ordinal Identification Logic:**
    *   The logic for identifying categorical/ordinal columns at the end is a heuristic (`elif pd.api.types.is_integer_dtype(df[col]): if df[col].nunique() < 10:`). While it works for the `quality` column (which is integer and has few unique values), it's not a definitive way to identify categorical/ordinal columns. Many numerical columns can have few unique values. The ground truth points out that for *this specific dataset*, all columns are numerical by default, and `quality` is *interpreted* as ordinal. The ground truth separates the ""strict"" identification based on pandas dtypes from the *interpretation* of the `quality` column. The generated code mixes this up.
    *   The ground truth also proactively identifies `categorical_ordinal_columns` during the initial loop for consistency and clarity.

3.  **Error Handling:** The ground truth includes basic file existence error handling (`try-except FileNotFoundError`), which is good practice for robust scripts. The generated code lacks this.

4.  **Output Clarity/Formatting:** The ground truth's output is more clearly segmented with headers (`--- Column Names and Identified Data Types ---`), making it easier to read and understand which requirement is being addressed.

Overall, the code performs the core statistical calculations well but falls short on the granularity and robustness of data type identification and column classification, especially for a benchmark expert. The hardcoded `sep=';'` is also a point of concern depending on the actual dataset file being used.",deepseek_r1,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

df = pd.read_csv('video_games_sales.csv')

print(""First 10 rows:"")
print(df.head(10))

print(""\nDataset shape:"")
print(df.shape)

print(""\nColumns and data types:"")
print(df.dtypes)

print(""\nTop 10 games by Global_Sales:"")
print(df.sort_values('Global_Sales', ascending=False).head(10))

print(""\nUnique Platforms:"")
print(df['Platform'].unique())

print(""\nUnique Genres:"")
print(df['Genre'].unique())

sales_cols = ['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales',...",7,"The generated code correctly performs all the requested tasks and produces the required output. It loads the dataset, displays the first 10 rows, shows the shape, lists column types, displays top games by global sales, prints unique platforms and genres, calculates sales statistics, and identifies missing values.

However, the code lacks robustness and user-friendliness that is present in the ground truth:
1.  **Error Handling:** It doesn't include any `try-except` block for `FileNotFoundError` when loading the CSV, which could cause a program crash if the file is missing.
2.  **Clarity of Output:** The ground truth includes more descriptive print statements (e.g., ""--- 1. Loading Dataset and Displaying First 10 Rows ---"", ""The dataset has X rows and Y columns.""), making the output much easier and more informative for the user.
3.  **Column Existence Checks:** The generated code assumes all specified columns (e.g., `Global_Sales`, `Platform`, `Genre`, `sales_cols`) exist. While this is likely true for this specific dataset, robust code would include checks (like `if 'Global_Sales' in df.columns:`) to prevent errors if the dataset structure changes.
4.  **Specific `df.info()` vs `df.dtypes`:** While `df.dtypes` technically shows data types, `df.info()` from the ground truth provides a more comprehensive overview including non-null counts and memory usage, which is generally preferred for EDA.
5.  **Targeted Missing Value Output:** The ground truth filters `df.isnull().sum()` to show only columns with missing values (`missing_values[missing_values > 0]`), which is more insightful than showing all columns when most might have zero missing values.

Overall, it's a functional solution but could be improved in terms of robustness, user experience, and comprehensiveness of some EDA outputs.",deepseek_r1,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd

df = pd.read_csv('video_games_sales.csv')

median_year = df['Year'].median()
df['Year'] = df['Year'].fillna(median_year).astype(int)

df = df.dropna(subset=['Publisher'])

df['Years_Since_Release'] = 2023 - df['Year']

df = df.drop_duplicates(subset=['Name', 'Platform'])

df = df[df['Global_Sales'] >= 0.1]

print(""Cleaned dataset shape:"", df.shape)
print(df.head(5))
```",7,"The generated code correctly addresses most of the requirements but lacks robustness and completeness in some areas compared to the ground truth.

**Strengths:**
- Loads the dataset correctly.
- Handles missing 'Year' values by filling with the median and converting to int.
- Removes rows with missing 'Publisher' information.
- Creates the 'Years_Since_Release' column correctly.
- Removes duplicate games based on 'Name' and 'Platform'.
- Filters out low-selling games based on 'Global_Sales'.
- Displays the final shape and head of the DataFrame as requested.

**Areas for Improvement:**
- **Missing value handling for 'Year' robustness:** The ground truth converts 'Year' to numeric with `errors='coerce'` before calculating the median and filling. This handles cases where 'Year' might contain non-numeric strings, which the generated code does not. If 'Year' has non-numeric values, `median()` would throw an error or produce an incorrect median.
- **Robustness for 'Global_Sales' filtering:** Similar to 'Year', the 'Global_Sales' column should ideally be converted to numeric using `pd.to_numeric(errors='coerce')` before filtering to handle potential non-numeric entries. The generated code directly filters, which could lead to errors if 'Global_Sales' isn't purely numeric.
- **`Years_Since_Release` negative values:** The ground truth explicitly handles cases where `CURRENT_YEAR - df['Year']` might result in negative values (e.g., if a future year is present in the dataset by mistake), setting them to 0. While less critical, it's a good practice for robustness.
- **Clarity and Step-by-Step Reporting:** The ground truth provides print statements at each significant step, showing the shape changes, which is very helpful for debugging and understanding the pipeline's progress. The generated code only shows the final shape.
- **Magic Number:** The ground truth defines `2023` as `CURRENT_YEAR`, making it easier to change or understand.
- **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions, which makes the code more robust against common issues. The generated code lacks this.",deepseek_r1,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('video_games_sales.csv')

plt.figure(figsize=(12,6))
platform_sales = df.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False).head(10)
platform_sales.plot(kind='bar', color='skyblue')
plt.title('Top 10 Platforms by Global Sales')
plt.xlabel('Platform')
plt.ylabel('Total Global Sales (millions)')
plt.grid(axis='y', linestyle='--')
plt.show()

plt.figure(figsize=(10,10))
genre_dist = df['Genre'].value...",7,"The generated code successfully creates all the requested visualizations, which is good. It correctly imports `pandas` and `matplotlib.pyplot`. However, there are significant areas for improvement, particularly in robustness and data handling, that prevent it from scoring higher.

**Strengths:**
*   All required plot types (bar, pie, histogram, scatter, line) are generated.
*   Basic titles and labels are included for most plots.
*   Uses `matplotlib.pyplot` as requested.

**Areas for Improvement:**
*   **Data Loading Robustness:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is crucial for
    user-friendly code. The generated code lacks this.
*   **Data Cleaning/Preprocessing:** The generated code completely
    misses essential data cleaning steps:
    *   **`Year_of_Release`:** This column often contains non-numeric values
        or NaNs in real-world datasets. The ground truth correctly converts it to
        numeric and handles NaNs. The generated code assumes clean data, which
        is a common pitfall. If `Year_of_Release` isn't numeric, the line plot
        will fail.
    *   **Sales Columns:** Similar to `Year_of_Release`, sales columns should be
        ensured to be numeric. The ground truth explicitly handles this.
*   **Plot Labeling:**
    *   **Bar Chart:** The Y-axis label ""Total Global Sales (millions)"" is good, but ""Millions"" is more specific. The ground truth rotates x-axis labels which improves readability for longer platform names.
    *   **Pie Chart:** The `plt.ylabel('')` is okay, but `plt.axis('equal')` is critical for ensuring the pie chart is drawn as a perfect circle, which the ground truth includes and the generated code omits. Also, the ground truth intelligently handles `Other` categories for readability, which is a good practice for pie charts with many categories.
    *   **Histogram:** The ground truth limits the x-axis range to the 99th percentile, which is a very common and effective technique to make histograms of skewed data more readable. The generated code just uses default bins, which might produce a less informative plot for sales data.
    *   **Line Plot:** The ground truth handles potential `Year_of_Release` outliers by filtering them (`>= 1980` and `<= 2020`), which is good for data quality. The generated code plots all years assuming they are valid. The ground truth also uses `xticks(rotation=45)` for better readability of year labels, which is missing in the generated code.
*   **Consistency in Naming/Styling:** Minor point, but slightly more consistent styling (e.g., all `plt.grid` calls using the same arguments, specific colors or alpha values) could improve the code. The ground truth uses `plt.tight_layout()` for every plot, which automatically adjusts subplot parameters for a tight layout, preventing labels from overlapping. This is a crucial detail for good visualization practice that the generated code misses.

In summary, while the generated code produces the requested plots, it lacks robustness, proper data preprocessing, and some best practices for visualization formatting and readability that are present in the ground truth.",deepseek_r1,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd

df = pd.read_csv('diabetes.csv')

print(""First 10 rows:"")
print(df.head(10))

print(""\nDataset shape:"")
print(df.shape)

print(""\nColumn names and data types:"")
for col, dtype in df.dtypes.items():
    print(f""{col}: {dtype}"")

print(""\nPercentage of diabetic vs non-diabetic patients:"")
outcome_perc = df['Outcome'].value_counts(normalize=True) * 100
print(outcome_perc.round(2))

print(""\nMissing values per column:"")
print(df.isnull().sum())

print(""\nSummary statis...",7,"The generated code addresses most of the requirements correctly.

**Strengths:**
*   Correctly loads the dataset and displays the first 10 rows.
*   Shows the shape of the dataset accurately.
*   Calculates and displays the percentage of diabetic vs non-diabetic patients correctly.
*   Displays missing values per column.
*   Shows basic summary statistics for numerical columns.

**Areas for Improvement:**
*   **Column Names and Data Types:** While it prints `col: dtype`, `df.info()` from the ground truth is a more standard and comprehensive way to display this information, including non-null counts, which is valuable for EDA.
*   **Identifying Zero Values:** The prompt specifically asked to ""Identify any columns with zero values that might represent missing data."" The generated code identifies columns with zero values but doesn't distinguish between columns where zero is a valid value (like 'Pregnancies' or 'Outcome') and those where it might represent missing data (like 'Glucose', 'BloodPressure', etc.). The ground truth correctly defines a list of `COLUMNS_TO_CHECK_FOR_ZERO_AS_MISSING`, which is a more robust approach for this specific dataset and a better practice for identifying truly suspicious zeros. The current implementation in the generated code just prints any column with a zero value (excluding Outcome), which might be misleading.
*   **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is good practice for robustness.
*   **Formatting/Readability:** The ground truth uses more explicit section headers and separators (`--- 1. First 10 rows...`, `print(""=""*50)`) which improves readability and navigability when outputs are long.
*   **Completeness (Zero Values):** The prompt specifically asked to ""Identify any columns with zero values that might represent missing data"". The generated code checks for columns with *any* zero values (excluding Outcome) but doesn't explicitly filter or label them as 'potential missing data' based on domain knowledge, which the ground truth does by defining `COLUMNS_TO_CHECK_FOR_ZERO_AS_MISSING`. This distinction is important for the specific context of the diabetes dataset.",deepseek_r1,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

df = pd.read_csv('diabetes.csv')

zero_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[zero_cols] = df[zero_cols].replace(0, np.nan)

for col in zero_cols:
    df[col].fillna(df[col].median(), inplace=True)

df.drop_duplicates(inplace=True)

df['BMI_Category'] = pd.cut(df['BMI'], bins=[0, 18.5, 25, 30, np.inf], labels=['Underweight', 'Normal', 'Overweight', 'Obese'], right...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being production-ready or fully correct.

**Correctness Issues:**
1.  **BMI_Category Bins:** The `bins` for `BMI_Category` are incorrect. The prompt specified 'Normal' (18.5-24.9), 'Overweight' (25-29.9), and 'Obese' (>=30). The generated code uses `[0, 18.5, 25, 30, np.inf]` which makes the ranges slightly off for 'Normal' (18.5-24.9) and 'Overweight' (25-29.9). Specifically, `right=False` combined with `25` as an upper bound means `25` would be in the next bin for 'Overweight', not 'Normal'. The `right=True` in the ground truth is crucial for inclusive upper bounds.
2.  **Scaling Numerical Columns:** The `numerical_cols` definition is hardcoded and incorrect. It includes an empty string `''` which will cause a `KeyError`. More importantly, it does not dynamically determine numerical columns, which is less robust. It also includes `Pregnancies` and `Age` which were implicitly left out of the zero-replacement, but are crucial for scaling. The ground truth correctly identifies all numerical columns and then excludes the target variable (`Outcome`).
3.  **Error Handling:** There is no error handling for loading the CSV file (e.g., `FileNotFoundError`), which is a basic robustness measure.
4.  **Implicit Assumptions:** It assumes all `zero_cols` exist without checking, which could lead to `KeyError` if a column is misspelled or missing.

**Code Quality and Completeness Issues:**
1.  **Lack of Comments/Explanation:** The generated code lacks comments, which are essential for understanding complex data preprocessing steps.
2.  **No Dynamic Column Checks:** The code doesn't check if columns exist before operating on them (e.g., `df[zero_cols]`), which can lead to errors if the dataset structure changes slightly.
3.  **Missing Output Details:** The prompt asks to display the cleaned dataset's shape AND first 5 rows, which it does. However, it doesn't indicate the original shape or any information about duplicate removal or scaling success, which the ground truth code provides. This makes the output less informative for a user trying to understand the preprocessing steps.
4.  **No Printing of Original Shape:** The prompt asked to display the cleaned dataset's shape (which is done) but also implicitly for comparison, which usually means original vs. cleaned, which the ground truth provides.

While the core steps are present (loading, zero replacement, median imputation, duplicate removal, BMI category, scaling, display), the implementation details for BMI categorization and especially numerical column selection for scaling are incorrect or fragile. The lack of robustness and informativeness compared to the ground truth makes it a less desirable solution.",deepseek_r1,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('diabetes.csv')

plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt="".2f"")
plt.title('Correlation Heatmap of Numerical Variables')
plt.tight_layout()
plt.show()

plt.figure(figsize=(15, 12))
for i, col in enumerate(df.columns):
    plt.subplot(3, 3, i+1)
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
plt.suptitle('Histograms of Numeri...",8,"The generated code successfully creates all the requested visualizations.

**Strong points:**
- All visualization types (heatmap, histograms, box plots, pie chart, scatter plots, pair plot) are present.
- Proper libraries (pandas, matplotlib, seaborn) are imported.
- Most plots include relevant titles and labels.
- The use of `tight_layout()` is good for preventing plot overlaps.
- The scatter plots correctly use `hue` for coloring by outcome.
- The pair plot correctly selects the specified key features and uses `hue`.

**Areas for improvement:**
- **Loading the dataset:** The generated code directly calls `pd.read_csv('diabetes.csv')` without any error handling (e.g., `try-except FileNotFoundError`). The ground truth includes this important safety measure.
- **Histograms:** The histogram loop `for i, col in enumerate(df.columns):` iterates through *all* columns, including 'Outcome'. While `sns.histplot` can handle the 'Outcome' column (as it's numerical), it's generally better practice to explicitly select numerical features, especially when `Outcome` is a categorical target. The subplot grid calculation (3,3) might also lead to an empty subplot if there are fewer than 9 columns, which is less robust than the ground truth's dynamic calculation.
- **Box plots:** Similar to histograms, the `df.columns[:-1]` approach to select features is reasonable but less explicit than defining `numerical_cols` and using it consistently. The subplot grid calculation (4,2) is hardcoded and might not be optimal for all datasets.
- **Labels for pie chart:** The ground truth provides more descriptive labels for the pie chart like 'Non-Diabetic (0)' and 'Diabetic (1)', which adds clarity.
- **Consistency in plotting:** While the code works, the ground truth demonstrates more careful handling of `numerical_cols` to ensure consistency across different plots (heatmap, histograms, box plots).
- **Initial dataset inspection:** The ground truth includes printing `df.head()`, `df.info()`, and `df.describe()`, which are good practices for understanding the dataset immediately after loading.",deepseek_r1,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

df = pd.read_csv('world-happiness-report-2021.csv')

print(""First 10 rows:"")
print(df.head(10))

print(""\nDataset shape:"", df.shape)

print(""\nColumn names and data types:"")
print(df.dtypes)

print(""\nUnique Regional Indicators and count of countries per region:"")
print(df['Regional indicator'].value_counts())

print(""\nTop 10 happiest countries:"")
print(df[['Country name', 'Ladder score']].sort_values('Ladder score', ascending=False).head(10))

key_factors = ['Lad...",7,"The generated code correctly addresses most of the tasks. However, it lacks robust error handling for file not found scenarios, which is crucial for real-world applications. The print statements are concise but could benefit from more structured output separators for better readability, similar to the ground truth. Additionally, the handling of potential missing columns for operations like summary statistics and top countries could be more robust to prevent runtime errors if columns are renamed or missing in a future dataset. The `GDP per capita` column name is incorrect compared to the dataset's `Logged GDP per capita` which will raise a KeyError.",deepseek_r1,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder

df = pd.read_csv('world-happiness-report-2021.csv')

numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
categorical_cols = df.select_dtypes(include=['object']).columns

df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())
for col in categorical_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

df['Happiness_Category'] = pd.cut(df['Ladder ...",8,"The generated code correctly implements most of the requirements.

**Strong points:**
*   Correctly loads the dataset.
*   Handles missing numerical values using the mean and categorical values using the mode. The numerical filling is more concise than the ground truth, applying `fillna` to all columns at once, which is generally acceptable for this scenario.
*   Creates 'Happiness\_Category', 'GDP\_Category', and 'Well\_being\_Score' accurately. Using `pd.cut` for `Happiness_Category` is a good alternative and more compact than `np.select`.
*   Correctly removes outliers for 'Ladder score' using the IQR method.
*   Applies `StandardScaler` to numerical features and `LabelEncoder` to 'Regional indicator'.
*   Displays the cleaned dataset's shape and first 5 rows.

**Areas for improvement (to reach a higher score):**
*   **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.
*   **Imputation for Categorical Columns:** While the provided code for categorical imputation `df[col] = df[col].fillna(df[col].mode()[0])` works, it iterates through each column individually. For a large number of categorical columns, a more vectorized approach could be slightly more efficient, though less readable for some. The current approach is not inherently wrong but could be optimized.
*   **Clarity of Numerical Feature Selection for Scaling:** The generated code re-selects `numerical_features = df.select_dtypes(include=['float64', 'int64']).columns` *after* creating new features and encoding. This is generally fine, but if specific numerical original columns (like 'Country name' if it were numeric) were meant to be *excluded* from scaling, then a more precise selection would be needed. In this specific dataset, all numerical columns are indeed intended for scaling, so it works, but the ground truth explicitly mentions excluding identifiers.
*   **Docstrings/Comments:** While the prompt asked for ""only the Python code without explanations,"" in a real-world scenario, adding comments for each step would improve readability and maintainability. This is not penalized here per the instruction, but worth noting for general best practices.",deepseek_r1,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from sklearn.preprocessing import MinMaxScaler

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# 1. World Map Visualization
fig = px.choropleth(df, locations=""Country name"", locationmode='country names',
                    color=""Ladder score"", 
                    title=""World Happiness Score 2021"",
       ...",8,"The generated code successfully creates most of the requested visualizations with good quality.

**Strong Points:**
*   **Correctness and Completeness:** All seven types of visualizations (world map, horizontal bar chart, heatmap, scatter plots, box plots, radar chart) are implemented.
*   **Visualization Libraries:** Uses `plotly.express` for the map, `seaborn` and `matplotlib.pyplot` for most other charts, and `sklearn.preprocessing.MinMaxScaler` for the radar chart, fulfilling the requirements.
*   **Plotting Components:** Most plots have appropriate titles and labels.
*   **Radar Chart Implementation:** The radar chart is correctly implemented, including normalization of features and plotting for the top 5 countries.

**Areas for Improvement:**
*   **Dataset Handling/Robustness:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes.
*   **Column Renaming:** The ground truth code renames columns for better readability and consistency across plots. The generated code uses the original column names, which are less user-friendly (e.g., 'Ladder score' vs 'Happiness Score', 'Logged GDP per capita' vs 'GDP per Capita'). This impacts title and label clarity slightly.
*   **Scatter Plot Legends:** The scatter plots do not include a `hue` parameter with 'Regional indicator' like the ground truth, which provides richer insights by distinguishing data points by region. They also lack a legend.
*   **Title Clarity/Consistency:** While most titles are present, they could be more descriptive (e.g., adding ""2021"" to all titles for consistency).
*   **Readability of Box Plot Labels:** The box plot's x-axis labels (`Regional indicator`) are not rotated, which can lead to overlapping if there are many regions with long names, like in the ground truth.
*   **Radar Chart Labels:** The `set_thetagrids` for the radar chart places the labels at the angles, which could be more clearly linked to the specific factor if the labels were adjusted slightly relative to the grid lines. The ground truth version uses `ax.set_xticklabels` which can be more direct.
*   **Code Structure/Comments:** The generated code has limited comments explaining sections, whereas the ground truth explicitly labels each task. This enhances readability.
*   **Consistent Styling:** The ground truth sets default `figure.dpi` and `figure.figsize` for Matplotlib plots, ensuring more consistent output across all `matplotlib`/`seaborn` figures. The generated code sets `figsize` for individual plots.",deepseek_r1,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd

df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

print(""First 10 rows:"")
print(df.head(10))

print(""\nDataset shape:"", df.shape)

print(""\nColumns and data types:"")
print(df.dtypes)

print(""\nTotal cases and deaths by continent:"")
continent_totals = df.groupby('continent')[['total_cases', 'total_deaths']].sum()
print(continent_totals)

print(""\nTop 5 locations by total cases on latest date:"")
latest_indices = df.groupby('location')['date'].idxm...",7,"The generated code addresses most of the requirements correctly and concisely.

**Strengths:**
*   Correctly loads the CSV and parses dates.
*   Displays the first 10 rows, shape, and column data types.
*   Correctly calculates total cases and deaths by continent.
*   Identifies top 5 locations by total cases on the latest date (with a minor nuance - see ""Areas for Improvement"").
*   Calculates summary statistics for `new_cases` and `new_deaths`.
*   Counts missing values correctly.
*   Uses `pandas` functions efficiently.

**Areas for Improvement:**
1.  **Robustness (File Handling):** The ground truth includes error handling for `FileNotFoundError`. The generated code does not, which is a good practice for real-world applications.
2.  **Display for Column Names and Types:** The prompt asked to ""List all column names and their data types."" While `df.dtypes` works, `df.info()` as used in the ground truth is often preferred because it also shows non-null counts and memory usage, providing a more comprehensive overview, especially relevant for EDA.
3.  **Top 5 Locations - Columns Displayed:** The prompt asks to display 'location', 'date', and 'total_cases' for the top 5 locations, but the generated code only displays `['location', 'total_cases']`. The ground truth correctly includes the 'date' column in the output.
4.  **Clarity of Output:** The ground truth includes clear print statements (e.g., ""--- First 10 rows of the dataset ---"") making the output more readable and understandable, especially in a script performing multiple analyses. The generated code's print statements are a bit more terse.
5.  **Handling Non-Numeric/Missing Values Robustness:** The ground truth's handling of `total_cases` for the top locations (using `pd.to_numeric` with `errors='coerce'`) and ensuring `new_cases`/`new_deaths` are numeric before statistics, is more robust. While the synthetic data might be clean, real-world data often isn't, and this protects against errors if those columns contain non-numeric data or NaNs that prevent calculation. The generated code assumes these columns are clean and numeric.
6.  **`df.dtypes` vs `df.info()`:** As mentioned in point 2, `df.dtypes` only shows the data types. `df.info()` provides more comprehensive information like non-null counts and memory usage which is more useful for EDA and aligns better with ""listing all column names and their data types"" in a practical sense by also showing nullability.",deepseek_r1,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('winequality-red.csv')
print(f""Average quality: {df['quality'].mean():.2f}"")

avg_alcohol = df.groupby('quality')['alcohol'].mean().reset_index()
print(avg_alcohol)

plt.bar(avg_alcohol['quality'], avg_alcohol['alcohol'])
plt.xlabel('Quality')
plt.ylabel('Average Alcohol Content')
plt.title('Average Alcohol Content by Wine Quality')
plt.show()
```",7,"The generated code correctly addresses the three core requirements: calculating mean quality, grouping by quality to get average alcohol, and creating a bar chart. The imports are proper and the structure is logical.

However, there are a few areas for improvement that prevent a higher score:
1.  **Error Handling:** The generated code lacks robust error handling for `FileNotFoundError` when reading the CSV, which the ground truth includes and is a good practice for real-world scripts.
2.  **Visualization Enhancement:** While it generates a bar chart, it uses `matplotlib.pyplot` directly for the bar chart without leveraging `seaborn` as suggested in the problem description (implicitly by ""matplotlib/seaborn"") and used in the ground truth. This results in a less aesthetically pleasing and less informative default plot compared to what Seaborn can offer with minimal configuration. The titles and labels are present, but there are no improvements like figure sizing, custom palettes, grid styles, or data labels on bars, which the ground truth demonstrates for better clarity and presentation.
3.  **Completeness of Output:** The ground truth provides more detailed and user-friendly print statements, separating the tasks and clearly labeling the output, which is generally better for a data analysis script.",deepseek_r1,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

df['hosp_patients'] = df['hosp_patients'].fillna(0)
df['icu_patients'] = df['icu_patients'].fillna(0)

numerical_cols = df.select_dtypes(include='number').columns
other_num_cols = numerical_cols.difference(['hosp_patients', 'icu_patients'])
for col in other_num_cols:
    df[col] = df[col].fillna(df[col].mean())

df['mortality_rate'] = (df['total_deaths'] / df['total_cases']) * ...",8,"The generated code is largely correct and addresses most of the requirements.

**Strong points:**
- Correctly loads the data and parses the 'date' column.
- Correctly handles missing values for 'hosp_patients' and 'icu_patients' with 0.
- Implements a more generalized approach for filling other numerical columns with their mean, which is robust.
- Correctly calculates 'mortality_rate' and 'vaccination_rate_per_capita', including handling of division by zero and resulting NaNs/infs (although the handling for NaNs could be slightly different as detailed below).
- Correctly filters the data for the year 2021.
- Displays the shape and head of the DataFrame as requested.

**Areas for improvement (preventing a higher score):**
1.  **Missing `try-except` for file loading:** The ground truth includes a `try-except` block for `FileNotFoundError`. While not explicitly requested in the prompt, it's good practice for robustness in a real-world scenario, but the prompt didn't ask for error handling, so it's not a major deduction.
2.  **Implicit type conversion for calculating rates:** The ground truth explicitly uses `pd.to_numeric(..., errors='coerce')` for columns involved in rate calculations (`total_deaths`, `total_cases`, `people_fully_vaccinated`, `population`). The generated code assumes these columns are already numeric, which might not always be true in raw CSVs and could lead to errors if the data contains non-numeric strings in these columns. This is a subtle but important robustness point.
3.  **Handling of NaN/inf for rates:**
    *   For `mortality_rate` and `vaccination_rate_per_capita`, the prompt asks to ""handle potential division by zero by replacing resulting NaNs or infs with 0"".
    *   The generated code uses `.replace([np.inf, -np.inf], np.nan).fillna(0)`. This handles `inf` and `-inf` by first converting them to `NaN`, and then `fillna(0)` handles both the original `NaN`s (e.g., from 0/0) and the newly created `NaN`s from inf. This is functionally correct for the specific requirement but slightly less direct than the ground truth's `replace().fillna()`.
    *   The ground truth's approach of `replace([np.inf, -np.inf], 0, inplace=True)` followed by `fillna(0, inplace=True)` is slightly more explicit in handling `inf` first, then `NaN`. Both approaches result in the correct outcome based on the prompt.

Overall, a very good attempt that fulfills the core requirements. The minor points are about robustness and explicit type handling.",deepseek_r1,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date')

plt.figure(figsize=(12, 6))
india_data = df[df['location'] == 'India']
plt.plot(india_data['date'], india_data['new_cases'])
plt.title('Daily New Cases in India')
plt.xlabel('Date')
plt.ylabel('New Cases')
plt.grid(True)

plt.figure(figsize=(12, 6))
continent_deaths = df.groupby('continent')['total_deaths'].sum().reset_index()
sns.barplot(x='co...",6,"The generated code attempts to address all requirements but falls short in several areas of correctness and detail.

**Strengths:**
*   **Correct structure:** Creates multiple plots as requested.
*   **Uses specified libraries:** `pandas`, `matplotlib.pyplot`, and `seaborn` are used.
*   **Basic plots created:** Line, bar, scatter, histogram, and box plots are attempted.

**Areas for Improvement (Why it didn't get a higher score):**

1.  **Parsing 'date' column:** The `parse_dates=['date')` argument in `pd.read_csv` has a syntax error (missing closing bracket). This would cause the code to fail immediately unless manually corrected. The ground truth uses `pd.to_datetime` after loading, which is a safer approach for general robustness.
2.  **Missing `plt.show()` for individual plots:** The generated code only has one `plt.show()` at the very end after all plotting commands. This will result in all plots overlapping in a single window, rather than displaying them individually or in separate windows/tabs, which is typically desired for distinct visualizations. Each plot should ideally have its own `plt.show()` (or be part of subplots shown once). The ground truth correctly uses `plt.show()` after each plot.
3.  **Bar Chart (Total Deaths by Continent):** The generated code simply sums `total_deaths` for each continent. This is conceptually flawed for cumulative metrics like 'total_deaths'. 'total_deaths' is cumulative per location per day. To get the 'total deaths by continent', one should typically take the *latest* `total_deaths` value for *each country* within that continent, and then sum those latest country values. The generated code sums up all daily 'total\_deaths' for all countries within a continent, which will lead to a massively inflated and incorrect figure. The ground truth correctly addresses this by taking `drop_duplicates(subset='location', keep='last')` before grouping.
4.  **Scatter Plot (GDP vs. Vaccinations):** While `drop_duplicates('location', keep='last')` is used, the plot itself could offer more insight (e.g., coloring by continent, sizing by population, log scales) as shown in the ground truth for better interpretability. The generated code also lacks error handling for missing values (e.g., `dropna`).
5.  **Lack of `tight_layout()` for every plot:** Although `plt.tight_layout()` is called once at the end, it's generally good practice to call it before `plt.show()` for each figure to ensure proper spacing of elements, especially when dealing with multiple distinct plots with different layout requirements.
6.  **Missing `sns.set_theme`:** The generated code doesn't set a professional style, which is good practice and was implied by the ""proper titles and labels"" requirement.
7.  **Minor formatting/readability:** While basic, the ground truth's use of comments for each step and more detailed plot titles/labels improves clarity.",deepseek_r1,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

df = pd.read_csv('clash_royale_cards.csv')

print(""Column names and data types:"")
print(df.dtypes)

numerical_cols = ['elixirCost', 'Win Rate', 'Usage']
stats = df[numerical_cols].agg(['mean', 'median', 'std', 'min', 'max'])

print(""\nDescriptive statistics:"")
print(stats)

print(""\nTotal number of cards:"", len(df))
```",7,"The generated code correctly addresses the core requirements of loading the data, identifying column data types, calculating descriptive statistics for the specified columns, and reporting the total number of rows.

**Strengths:**
- Correctly uses pandas to load the CSV.
- Accurately identifies and prints column data types using `df.dtypes`.
- Correctly calculates and displays the requested descriptive statistics (mean, median, std, min, max) for `elixirCost`, `Win Rate`, and `Usage` using the `agg` method, which is efficient.
- Correctly reports the total number of rows using `len(df)`.

**Areas for Improvement to achieve a higher score:**
- **Robustness in file loading:** The ground truth includes error handling for `FileNotFoundError` and other potential loading exceptions. The generated code lacks this, which could lead to a script crash if the file is missing.
- **Clarity in column listing:** While `df.dtypes` shows the data types for all columns, the request explicitly asked to ""List all column names and identify the data type of each column."" The ground truth explicitly iterates and prints column names separately before `df.dtypes` for better readability, which aligns slightly more with ""list all column names"" as distinct from just showing them in `dtypes`.
- **Readability of output messages:** The ground truth provides more descriptive print statements (e.g., ""--- Column Names and Data Types ---"", ""--- Descriptive Statistics for Numerical Columns ---"") which makes the output clearer and more user-friendly. The generated code's print statements are more concise but less informative when reading the output.
- **Column existence check:** The ground truth includes a check (`existing_numerical_cols = [col for col in numerical_cols_for_stats if col in df.columns]`) to ensure the numerical columns specified actually exist in the DataFrame before attempting to calculate statistics. This prevents a `KeyError` if one of the expected columns is misspelled or missing from the dataset. While the current dataset is assumed to have these columns, robust code handles such contingencies.",deepseek_r1,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

df = pd.read_csv('clash_royale_cards.csv')
filtered_df = df[df['elixirCost'] <= 3]
result = filtered_df[['Card', 'Win Rate']].sort_values('Win Rate', ascending=False)
print(result)
```",9,"The generated code correctly achieves all the stated requirements:
1. It loads the `clash_royale_cards.csv` dataset.
2. It filters cards with an `elixirCost` less than or equal to 3.
3. It selects and displays the `Card` name and `Win Rate` columns.
4. It sorts the results by `Win Rate` in descending order.
5. It includes the proper import (`pandas`).

The code is concise and efficient. The only minor point of difference from the ground truth is the lack of error handling for `FileNotFoundError` or missing columns, and the use of hardcoded string literals instead of constants. While these are good practices for robust production code, for the specific task of ""write Python code to load and manipulate data"", the generated code directly solves the problem presented. The ground truth code is more ""production-ready"" with its error handling and constants, but the generated code is perfectly functional and correct for the immediate task.",deepseek_r1,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('clash_royale_cards.csv')
plt.hist(df['elixirCost'], bins=range(0, 10))
plt.title('Distribution of Elixir Cost')
plt.xlabel('Elixir Cost')
plt.ylabel('Frequency')
plt.show()
```",7,"The generated code correctly loads the CSV and generates a histogram of the `elixirCost` column with a title and labeled axes, addressing the core requirements.

However, there are several areas for improvement:
- **Robustness:** The code lacks error handling for `FileNotFoundError`, which is crucial for real-world scenarios.
- **Imports:** While `matplotlib.pyplot` is imported, `seaborn` is commonly used for statistical plots due to its aesthetic appeal and simplified syntax, and the ground truth code used it. Although `matplotlib` alone suffices, including `seaborn` (as used in the ground truth) would be a good practice.
- **Histogram Binning:** `bins=range(0, 10)` might not perfectly align with discrete integer elixir costs. `range(1, 12)` or similar (to include 1-11 given the nature of elixir costs) as in the ground truth is more appropriate for clear visualization of discrete values. The `histplot` from seaborn handles this more elegantly.
- **Clarity of X-axis ticks:** The default `plt.hist` does not guarantee integer ticks on the x-axis, which is important for discrete elixir costs. The ground truth explicitly sets `xticks` for better readability.
- **Readability/Completeness (Minor):** The ground truth includes comments and addresses potential missing columns, which improves code clarity and robustness. The `figsize` is also a good addition for plot presentation.

Overall, it's a functional solution but could be improved for robustness, clarity, and presentation.",deepseek_r1,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

df = pd.read_csv('iris.csv')

print(""Columns:"")
print(df.columns.tolist())
print(""\nData types:"")
print(df.dtypes)

numerical_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
stats = df[numerical_columns].agg(['mean', 'median', 'std', 'min', 'max'])
print(""\nDescriptive statistics:"")
print(stats)

unique_species = df['Species'].nunique()
species_counts = df['Species'].value_counts()
print(""\nNumber of unique species:"", unique_species)
pr...",7,"The generated code successfully completes most of the requirements but lacks robustness and output clarity compared to the ground truth.

**Strong Points:**
*   Correctly loads the CSV file.
*   Accurately identifies and prints column names and data types.
*   Correctly calculates and prints the descriptive statistics (mean, median, std, min, max) for the specified numerical columns using `agg`.
*   Successfully identifies the number of unique species and their respective counts.
*   Uses `pandas` effectively for data manipulation.

**Areas for Improvement (leading to a score of 7 instead of higher):**

*   **Error Handling (Critical Missing Feature):** The most significant drawback is the complete lack of error handling. The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions when loading the CSV, which is crucial for robust code. The generated code will crash if `iris.csv` is not found, or if there's an issue with the file, and it doesn't handle cases where specified columns might be missing.
*   **Robustness for Column Existence:** The generated code assumes all `numerical_columns` and the `Species` column will always exist. If a column name is misspelled or missing from the CSV, the code will raise a `KeyError`. The ground truth gracefully handles missing numerical columns by checking their existence and reporting warnings, and it explicitly checks for the `Species` column.
*   **Output Formatting/Clarity:** While the output is functional, the ground truth uses more descriptive print statements and separators (`---...---` and `print(""-"" * 40 + ""\n"")`) to clearly delineate different sections of the output, making it more user-friendly and readable.
*   **Documentation/Comments:** The generated code is very minimal, lacking any comments to explain different sections. While the prompt asked for only the code, in a real-world scenario, proper comments improve readability and maintainability.

To achieve a higher score (8 or above), the generated code would need to incorporate robust error handling for file loading, checks for column existence before performing operations, and improved output formatting for better user experience.",deepseek_r1,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

df = pd.read_csv('iris.csv')
setosa_df = df[df['Species'] == 'Iris-setosa']
result = setosa_df[['SepalLengthCm', 'SepalWidthCm']]
print(result)
```",9,"The generated code correctly solves all aspects of the task: it loads the CSV, filters for 'Iris-setosa' species, and selects and displays the specified columns. Imports are correct and the code is concise and readable. The only minor improvement would be to include error handling for file not found or missing columns, which is present in the ground truth but not strictly required by the prompt's ""Provide only the Python code without explanations"" for the core solution.",deepseek_r1,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('iris.csv')

species_groups = df.groupby('Species')
for species, group in species_groups:
    plt.scatter(group['SepalLengthCm'], group['SepalWidthCm'], label=species)

plt.title('Sepal Length vs Sepal Width by Species')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.legend()
plt.show()
```",9,"The generated code correctly addresses all the core requirements of the task: loading the CSV, creating a scatter plot of `SepalLengthCm` vs `SepalWidthCm`, coloring points by `Species`, and including a title and axis labels. The use of `groupby('Species')` and iterating through groups is an elegant and efficient way to achieve the coloring by species.

Minor improvements could be made for a perfect score, such as:
1.  **X-axis and Y-axis labels:** While present, the ground truth uses the exact column names (`SepalLengthCm`, `SepalWidthCm`) as labels, which is often good practice for scientific plots unless a more descriptive label is clearly better. The generated code uses ""Sepal Length (cm)"" and ""Sepal Width (cm)"", which is also perfectly acceptable and arguably more user-friendly. I'll consider this a stylistic choice rather than a deficiency.
2.  **No error handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and `KeyError`, which enhances robustness. The generated code does not include this, which is fine for a benchmark focusing on the plotting logic but would be crucial for production code.
3.  **No `main` function:** The ground truth wraps the logic in a `main` function and uses `if __name__ == ""__main__"":`, which is good practice for larger scripts but not strictly necessary for this small task.

Overall, the generated code performs exactly as requested for the core plotting task.",deepseek_r1,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

df = pd.read_csv('heart.csv')

print(""Columns and Data Types:"")
print(df.dtypes)

numerical_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']
stats = df[numerical_cols].agg(['mean', 'median', 'std', 'min', 'max'])

print(""\nDescriptive Statistics:"")
print(stats)

print(""\nHeartDisease Counts:"")
print(df['HeartDisease'].value_counts())
```",8,"The generated code correctly addresses all primary requirements of the task. It loads the CSV, identifies column data types, calculates descriptive statistics for the specified numerical columns, and reports value counts for `HeartDisease`.

**Strong Points:**
*   Correctly uses `pandas.read_csv` to load the dataset.
*   Correctly uses `df.dtypes` to list column names and data types.
*   Correctly uses `df[numerical_cols].agg(['mean', 'median', 'std', 'min', 'max'])` to calculate the required descriptive statistics. This is an efficient way to get specific statistics compared to `df.describe()` if only a subset is needed.
*   Correctly uses `df['HeartDisease'].value_counts()` for the `HeartDisease` column.
*   Includes proper imports.

**Areas for Improvement (minor):**
*   **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and other general exceptions during file loading. The generated code lacks this, which is crucial for real-world applications where the file might not exist.
*   **Clarity of Output:** While the information is present, the output formatting could be improved for better readability (e.g., adding more descriptive print statements similar to the ground truth to clearly label each section). The ground truth's print statements make the output much clearer.
*   **Column Existence Check:** The ground truth checks if `numerical_cols` and `HeartDisease` column actually exist in the DataFrame before attempting operations. The generated code assumes their presence, which could lead to `KeyError` if a column name is misspelled or missing from the CSV.
*   **Descriptive Statistics Output Format:** The `agg` method outputs columns as rows and stats as columns, which might be less intuitive to read than the `describe` output (where columns are columns and stats are rows). However, it directly provides the requested stats efficiently. The ground truth adjusted `describe` output for better readability.

Overall, the code is functional and correct for the primary task requirements, but lacks the robustness and detailed output formatting of a production-ready script.",deepseek_r1,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

df = pd.read_csv('heart.csv')
filtered_df = df[(df['Age'] > 50) & (df['HeartDisease'] == 1)]
selected_columns = filtered_df[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]
print(selected_columns)
```",8,"The generated code correctly implements all the core requirements of the task: it loads the CSV, filters based on `Age` > 50 and `HeartDisease` == 1, and then selects and displays the specified columns (`Age`, `Sex`, `ChestPainType`, `Cholesterol`). The imports are correct and the logic is sound.

However, the code lacks the robustness and best practices seen in the ground truth. Specifically:
- It doesn't encapsulate the logic within a function, making it less reusable.
- It lacks error handling for `FileNotFoundError`, which is crucial for real-world scripts dealing with external files.
- It doesn't include any comments, which would improve readability for more complex tasks.
- It implicitly assumes the file exists and all columns are present, which could lead to runtime errors in different environments or with malformed data.

Despite these omissions, the core task is completed accurately and concisely. For a quick script, it's highly effective. For production-ready code or more complex scenarios, the ground truth's approach is superior.",deepseek_r1,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

df = pd.read_csv('winequality-red.csv', delimiter=';')
filtered_wines = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]
best_wine = filtered_wines.loc[filtered_wines['quality'].idxmax()]
pd.set_option('display.max_columns', None)
print(best_wine)
```",7,"The generated code correctly performs the filtering and identifies the wine with the highest quality in the subset. It also includes proper imports and correctly displays all features using `pd.set_option('display.max_columns', None)`.

However, it has a critical error in assuming the delimiter is a semicolon (`;`). The `winequality-red.csv` dataset typically uses a comma (`,`) as a delimiter. This assumption could lead to a `FileNotFoundError` if the CSV file isn't found, or more likely, it will load the entire row as a single column, leading to `KeyError` when accessing ""alcohol"", ""pH"", or ""quality"".

Additionally, while functional, the code lacks robustness and best practices compared to the ground truth. It doesn't include error handling for file not found or missing columns, nor does it provide informative print statements about the steps or the results (e.g., number of filtered wines). The ground truth code also encapsulates the logic within a function, making it more reusable and testable. The `delimiter=';'` argument is the main point of failure that prevents a higher score.",deepseek_r1,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('heart.csv')

plt.figure()
df['Age'].plot.hist(color='skyblue', edgecolor='black')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')

plt.figure()
sex_counts = df['Sex'].value_counts()
sex_counts.plot(kind='bar', color=['blue', 'pink'])
plt.title('Distribution of Sex')
plt.xlabel('Sex')
plt.ylabel('Count')
plt.xticks(rotation=0)

plt.show()
```",7,"The generated code correctly loads the CSV and generates a histogram for 'Age' and a bar chart for 'Sex', meeting the core requirements. Both plots have titles and labeled axes.

However, there are several areas for improvement to reach a higher score:

1.  **Plotting Library Usage:** The ground truth code leverages `seaborn` for plotting, which generally produces more aesthetically pleasing and informative plots (e.g., `sns.histplot` with `kde=True` and `bins='auto'`). The generated code uses `pandas.plot` internally `matplotlib.pyplot`, which is functional but less refined.
2.  **Robustness:** The generated code lacks error handling for `FileNotFoundError` or `KeyError`, which are crucial for real-world applications where the file might be missing or columns might be misnamed. The ground truth includes comprehensive `try-except` blocks.
3.  **Sex Column Interpretation:** The ground truth code explicitly handles the common scenario where 'Sex' is represented as 0s and 1s, mapping them to 'Female' and 'Male' for clearer visualization. The generated code directly plots 0 and 1, which requires external knowledge to interpret. This is a common data science practice on this dataset.
4.  **Plot Enhancements:** The ground truth adds visual enhancements like `figsize`, `grid`, `tight_layout`, and specific `fontsize` for titles and labels, making the plots more readable and professional. The generated code is more basic in this regard.
5.  **`plt.show()` placement:** The generated code calls `plt.show()` only once at the end, which combines all plots into a single window if interactive, but typically to display separate plots, `plt.show()` is called after each plot is fully defined which the ground truth does.

To improve, the code should:
*   Incorporate `seaborn` for plotting `Age` and `Sex` distributions.
*   Add error handling for file not found and missing columns.
*   Improve the 'Sex' plot by mapping 0/1 to 'Female'/'Male' labels.
*   Add visual enhancements like figure size, grid, and `tight_layout`.",deepseek_r1,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

df = pd.read_csv('spotify_2023.csv')

print(""Columns and Data Types:"")
for col in df.columns:
    print(f""{col}: {df[col].dtype}"")

df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
print(f""\nNaN values in 'streams': {df['streams'].isna().sum()}"")

numerical_cols = ['streams', 'bpm', 'danceability_%', 'valence_%', 'energy_%']
print(""\nDescriptive Statistics:"")
print(df[numerical_cols].agg(['mean', 'median', 'std', 'min', 'max']))

unique_artists = df['a...",8,"The generated code correctly addresses most of the task requirements.

Strong points:
- Correctly loads the CSV.
- Converts `streams` to numeric with `errors='coerce'` and reports NaNs.
- Calculates correct descriptive statistics for the specified numerical columns using `agg`, which is a concise way to do it.
- Correctly calculates the total number of unique artists, even handling cases where multiple artists might be listed in a single cell using `str.split(',').explode().str.strip()`. This is a very robust approach.

Areas for Improvement:
- **Column and Data Type Listing:** While it iterates through columns and prints them, it doesn't provide the same comprehensive output as `df.info()`, which also includes non-null counts and memory usage, and is generally preferred for a quick overview. The prompt asked to ""list all column names and identify the data type of each,"" which `df.info()` does more effectively.
- **Reporting NaNs:** It reports the count, but doesn't mention if any NaNs were *introduced* by the conversion or if they were already present. It also doesn't show sample rows with NaNs, which the ground truth included and can be helpful for debugging.
- **Error Handling for File Loading:** The ground truth included a `try-except` block for `FileNotFoundError`. While not explicitly requested, robust code should include this, especially when dealing with external files.
- **Output Clarity/Formatting:** The output isn't as clearly sectioned with headers as the ground truth, which makes it slightly less readable when multiple tasks are performed. For example, ""Columns and Data Types:"" is good, but adding more explicit headers like ""Descriptive Statistics for Numerical Columns"" helps.

Overall, it's a very good attempt that nails the core data transformations and calculations. The primary areas for improvement are in output formatting/completeness and minor robustness (file loading).",deepseek_r1,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

df = pd.read_csv('spotify_2023.csv')
df_2023 = df[df['released_year'] == 2023].copy()
df_2023['streams'] = pd.to_numeric(df_2023['streams'], errors='coerce')
selected = df_2023[['track_name', 'artist(s)_name', 'streams']]
sorted_selected = selected.sort_values('streams', ascending=False)
print(sorted_selected)
```",9,"The generated code correctly addresses all requirements of the task. It loads the CSV, filters by `released_year` 2023, coercizes the `streams` column to numeric, selects the specified columns, and sorts the result in descending order by `streams`. The use of `.copy()` after filtering is good practice to prevent `SettingWithCopyWarning`. The only minor point is the lack of explicit error handling for `FileNotFoundError` or other `pd.read_csv` issues, which is present in the ground truth but not strictly required by the prompt's explicit instructions (though good for robustness). However, given the prompt only asked for ""Python code without explanations"" and focused on the data manipulation steps, this is a very strong submission.",deepseek_r1,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('spotify_2023.csv')

plt.figure()
plt.hist(df['bpm'], bins=20)
plt.title('Distribution of BPM')
plt.xlabel('BPM')
plt.ylabel('Frequency')

plt.figure()
plt.scatter(df['danceability_%'], df['energy_%'])
plt.title('Danceability vs Energy')
plt.xlabel('Danceability (%)')
plt.ylabel('Energy (%)')

plt.show()
```",9,"The generated code correctly addresses all requirements of the task. It loads the CSV, generates a histogram for `bpm`, and a scatter plot for `danceability_%` vs `energy_%`. Both plots have appropriate titles and axis labels, and proper imports are included.

Areas for minor improvement (not critical to the task success but good practice):
1.  **Error Handling**: The ground truth includes `try-except` blocks for file loading and column access, which makes the code more robust. The generated code lacks this, meaning it would crash if the file is missing or a column name is incorrect.
2.  **Figure Sizing**: The ground truth specifies `figsize` for better plot aesthetics and readability. While not explicitly requested, it's a good practice.
3.  **Clarity of Labels**: While the labels are correct, the ground truth uses slightly more descriptive labels (e.g., 'BPM (Beats Per Minute)', 'Danceability (%)' instead of just 'Danceability'). This is a minor stylistic point.

Overall, it's a very good solution that successfully delivers on all core requirements.",deepseek_r1,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd

df = pd.read_csv('youtube_trendings.csv')

print(""Columns and data types:"")
print(df.dtypes)

cols = ['Video views', 'Likes', 'Dislikes']
for col in cols:
    df[col] = df[col].astype(str).str.replace(',', '', regex=False)
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

print(""\nDescriptive statistics:"")
print(df[cols].agg(['mean', 'median', 'std', 'min', 'max']))

df['Category'].fillna('Unknown', inplace=True)

print(f""\nUnique categories count: {...",9,"The generated code is largely correct and addresses most of the requirements.

**Strong points:**
- Correctly loads the CSV.
- Successfully handles the cleaning of 'Video views', 'Likes', and 'Dislikes' by removing commas, coercing to numeric, and filling NaNs with 0. This is done efficiently in a loop.
- Calculates and prints the descriptive statistics (mean, median, std, min, max) for the cleaned columns correctly, using `agg` which is a good pandas practice.
- Fills missing 'Category' values with 'Unknown' using `fillna` and `inplace=True`.
- Reports the count of unique values in 'Category'.

**Areas for minor improvement:**
- **File Handling/Error Handling:** The ground truth includes a `try-except FileNotFoundError` block, which is good practice for robust code, especially when dealing with external files. The generated code assumes the file will always be present, which could lead to a `FileNotFoundError`.
- **Output Verbosity/Clarity:** While the code produces the required outputs, the ground truth provides more descriptive print statements (e.g., ""Successfully loaded..."", ""Processing column: 'Video views'"", ""Statistics for 'Video views':"") which makes the output much clearer and more user-friendly. The task specifically asked to ""List all column names and their inferred data types"", and `print(df.dtypes)` fulfils this, but `df.info()` as used in the ground truth is often preferred as it also shows non-null counts and memory usage.
- **Completeness of descriptive statistics:** The `agg` method for statistics is concise, but it prints them grouped by statistic rather than by column. The ground truth prints statistics per column, which can be easier to read sometimes, though `agg` is perfectly acceptable.

Overall, it's a very good solution that correctly implements the core logic. The minor deductions are for robustness and output presentation, which are important aspects of a complete and user-friendly solution.",deepseek_r1,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

df = pd.read_csv('youtube_trendings.csv')

df['Video views'] = df['Video views'].str.replace(',', '')
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce').fillna(0)

filtered_df = df[df['Video views'] > 50_000_000]
result = filtered_df[['Video', 'Video views']].sort_values('Video views', ascending=False)

print(result)
```",9,"The generated code correctly performs all the requested operations: loading the data, cleaning the 'Video views' column (removing commas, converting to numeric, filling NaNs with 0), filtering by views, selecting the specified columns, and sorting. The code is concise and efficient.

One minor point is the missing `try-except` block for `FileNotFoundError` when loading the CSV, which was present in the ground truth. While not strictly required by the prompt's output format (""Provide only the Python code without explanations""), it's a good practice for robust code. However, the core logic for data manipulation is perfectly implemented.",deepseek_r1,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('youtube_trendings.csv')

df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', '', regex=False), errors='coerce').fillna(0)
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', '', regex=False), errors='coerce').fillna(0)

plt.scatter(df['Video views'], df['Likes'])
plt.title('Video Views vs. Likes')
plt.xlabel('Video Views (cleaned)')
plt.ylabel('Likes (cleaned)')
plt.show()
```",9,"The generated code correctly addresses all parts of the task. It properly loads the CSV, cleans the 'Likes' and 'Video views' columns by removing commas, converting to numeric, and filling NaNs with 0. It also generates the scatter plot with the specified title and axis labels. The code is concise and efficient for this particular task. The only minor improvement would be to add error handling for `pd.read_csv`, though the prompt did not explicitly require it, it's good practice. Also, it might be slightly more robust to cast to string *before* calling `.str.replace()` in case `Likes` or `Video views` contain non-string types, although for a CSV load, they are typically strings initially if they contain commas. However, `str.replace` on a Series will raise a warning if there are non-string types, but `errors='coerce'` in `pd.to_numeric` will handle non-numeric strings effectively.",deepseek_r1,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd

df = pd.read_csv('mobiles_2025.csv')

df['Mobile Weight'] = pd.to_numeric(df['Mobile Weight'].str.replace('g', '', regex=False), errors='coerce')
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', '', regex=False), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', '', regex=False), errors='coerce')
df['Launched Price (USA)'] = pd.to_numeric(
    df['Launched Price (USA)'].str.replace('USD ', '', regex=False).str.re...",9,"The generated code correctly performs all the requested tasks: loading the CSV, cleaning the specified columns (`Mobile Weight`, `RAM`, `Battery Capacity`, `Launched Price (USA)`) by removing specific strings and converting to numeric with error handling, listing column names and data types, calculating descriptive statistics (mean, median, std, min, max) for the cleaned columns, and counting unique values in the `Company Name` column.

**Strong Points:**
- **Concise and Correct Cleaning:** The cleaning steps are very compact and effective, directly applying `str.replace` and `pd.to_numeric` in one line per column.
- **Correct Error Handling:** Uses `errors='coerce'` as required, which handles conversion errors by setting to NaN.
- **Accurate Statistics Calculation:** Uses `agg` effectively to calculate all required descriptive statistics for the specified columns.
- **Proper Unique Count:** Correctly uses `nunique()` for the `Company Name` column.

**Minor Areas for Improvement (why not a 10):**
- **Missing `numpy` import:** While not strictly used in the final version, `numpy` is often imported with pandas, and the ground truth included it. However, it's not essential for this specific solution.
- **Implicit Error Handling for `read_csv`:** The code doesn't include explicit error handling for `pd.read_csv` (e.g., `FileNotFoundError`), which can make a script more robust in real-world scenarios. The task description didn't explicitly ask for this, but it's good practice.
- **Output clarity:** The output for column names and data types is presented in a loop, which is functional. However, `df.dtypes` directly (as in the ground truth) is often a more idiomatic pandas way to display this information and can be slightly cleaner for large numbers of columns.
- **No `astype(str)` before `str.replace`:** While `str.replace` often works on mixed-type columns, explicitly converting to string with `.astype(str)` before string operations can prevent potential `AttributeError` if a column happens to contain non-string types early on. In this specific case, for columns like 'Mobile Weight', they are likely to be strings initially, so it's not a critical issue but a robustness point.

Overall, it's an excellent and efficient solution that fully addresses the prompt's requirements.",deepseek_r1,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd

df = pd.read_csv('mobiles_2025.csv')

df['Launched Price (India)'] = pd.to_numeric(
    df['Launched Price (India)'].str.replace('INR |,', '', regex=True),
    errors='coerce'
)
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', '', regex=False), errors='coerce')

filtered_df = df[(df['Company Name'] == 'Apple') & (df['Launched Price (India)'] < 100000)]
result = filtered_df[['Model Name', 'RAM', 'Launched Price (India)']].sort_values('Launched Price (India)', a...",9,"The generated code correctly performs all requested operations: loading the CSV, cleaning both specified columns (handling errors), filtering the data, selecting the required columns, and sorting the results.

Strong points:
- Concise and efficient code for the cleaning and filtering steps.
- Correctly uses `pd.to_numeric` with `errors='coerce'` for robust cleaning.
- Accurate filtering conditions.
- Proper column selection and sorting.

Areas for minor improvement (not critical for functionality but good practice):
- The `Launched Price (India)` cleaning could be slightly more robust by ensuring the column is string type before applying `str.replace`, though pandas generally handles this well when applying `.str` accessors. The regex `INR |,` is clever but might be less readable than two separate `replace` calls for some.
- No explicit `try-except` block for file loading, which is good practice in production code to handle `FileNotFoundError`.
- No print statements or detailed output (e.g., initial DataFrame shape, filtered DataFrame shape) that the ground truth code includes for better user feedback during execution. However, the task only asks to **display** the final result, which it does.",deepseek_r1,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('mobiles_2025.csv')

df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', '', regex=False), errors='coerce')

sns.scatterplot(data=df, x='RAM', y='Battery Capacity', hue='Company Name')
plt.title('Battery Capacity vs RAM by Company')
plt.xlabel('RAM (GB)')
plt.ylabel('Battery Capacity (mAh)')
...",8,"The generated code successfully completes the core tasks. It loads the CSV, correctly cleans both 'RAM' and 'Battery Capacity' columns by removing units and converting to numeric, handling errors by coercing to NaN as requested. It also correctly generates a scatter plot with 'Battery Capacity' on the y-axis and 'RAM' on the x-axis, colors the points by 'Company Name', and includes a title and axis labels.

Areas for improvement (minor):
- The `regex=False` argument in `str.replace` for `Battery Capacity` is good practice, but it's missing for the `RAM` column cleaning. While 'GB' isn't a special regex character, consistency is ideal.
- The generated code directly overwrites the original 'RAM' and 'Battery Capacity' columns. While this works for this specific task, creating new cleaned columns (as in the ground truth) is often safer and more flexible for subsequent data analysis or debugging, preserving the raw data. This is a matter of preference and coding style in this context, but worth noting.
- The generated code uses `seaborn.scatterplot` directly which is good, but it doesn't set a figure size or add a grid, which were present in the ground truth for better visualization (though not explicitly requested in the prompt, they contribute to plotting quality).
- Error handling for `FileNotFoundError` when loading the CSV is not included, which is present in the ground truth and makes the code more robust. However, the prompt implicitly assumes the file exists, so this wasn't a strict requirement.",deepseek_r1,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd

df = pd.read_csv('titanic.csv')

print(""First 5 rows:"")
print(df.head())

print(""\nDataset shape:"")
print(df.shape)

print(""\nColumn names and data types:"")
print(df.dtypes)

survival_rate = df['Survived'].mean() * 100
print(f""\nSurvival rate: {survival_rate:.2f}%"")

print(""\nMissing values per column:"")
print(df.isnull().sum())
```",9,"The generated code is very good and correctly addresses almost all requirements.

**Strong Points:**
*   **Correctness:** All tasks are performed correctly.
*   **Conciseness:** The generated code is more concise than the ground truth in some areas (e.g., calculating survival rate using `.mean()`).
*   **Clarity:** The output is clearly labeled, making it easy to understand.
*   **Proper Imports:** `pandas` is correctly imported.

**Areas for Improvement (minor):**
*   **Error Handling:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is robust practice. While not explicitly asked for, adding this improves the code's resilience.
*   **Data Type Listing:** The ground truth uses `df.info()` which provides more comprehensive information (non-null counts, memory usage) about data types and columns compared to `df.dtypes` which only lists data types. For ""listing all column names and their data types,"" `df.info()` is generally preferred in EDA as it gives a fuller picture, although `df.dtypes` technically answers the direct question.

These are minor points. The generated code is highly functional and effective for the given task.",deepseek_r1,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

df = pd.read_csv('imdb_top_250.csv')

print(""Column names and data types:"")
print(df.dtypes)

print(""\nDescriptive statistics:"")
print(df[['Year', 'Rating', 'Runtime']].agg(['mean', 'median', 'std', 'min', 'max']))

print(""\nTotal number of movies:"", len(df))
```",6,"The generated code loads the data, lists column types, calculates descriptive statistics, and reports the total number of movies, thus addressing the main requirements. However, it lacks robustness and completeness in several areas.

**Areas for Improvement:**
1.  **Error Handling (Critical Missing Feature):** The ground truth includes `try-except` for `FileNotFoundError`, which is crucial for real-world robustness. The generated code will crash if the file is not found.
2.  **Runtime Column Preprocessing (Critical Missing Feature):** The `Runtime` column in 'imdb_top_250.csv' is typically a string (e.g., '142 min'). The generated code will fail when trying to calculate statistics on this column because `agg` will not work on non-numeric data types. The ground truth correctly handles this conversion. This is a critical omission for the given dataset.
3.  **Clarity of Output for Data Types:** `df.dtypes` is concise but `df.info()` as used in the ground truth provides more comprehensive information (non-null counts, memory usage) which is generally more useful for initial data exploration.
4.  **Specific Statistics Reporting:** While `agg` gets the job done, `describe().loc` provides a more direct way to select specific statistics and is often cleaner for complex statistical reporting. Also, `agg` doesn't handle non-numeric columns gracefully if they are part of the selection.

Due to the lack of error handling and, more critically, the absence of `Runtime` column preprocessing, which would cause the code to fail or produce incorrect results for the specified dataset, the score is lowered. If the `Runtime` column were already numeric, the score would be higher.",deepseek_r1,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

df = pd.read_csv('imdb_top_250.csv')
result = df[df['Rating'] >= 9.0][['Title', 'Rating']].sort_values(by='Rating', ascending=False)
print(result)
```",8,"The generated code correctly implements all the core requirements of the task:
1. Loads the CSV using pandas.
2. Filters movies with a `Rating` >= 9.0.
3. Selects `Title` and `Rating` columns.
4. Sorts by `Rating` in descending order.

The code is concise and directly addresses the problem.

Areas for improvement (and why it's not a 10):
1.  **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError`, `KeyError`, and general exceptions, which is crucial for production-ready code. The generated code lacks this, making it less resilient to issues like a missing file or incorrectly named columns.
2.  **Clarity/Readability:** While concise, chaining all operations can sometimes reduce readability for complex pipelines. Breaking it down into intermediate steps (as in the ground truth) can improve clarity, especially for others maintaining the code. For this specific task, it's arguably acceptable, but it's a general good practice consideration.
3.  **Path Definition:** The ground truth defines `DATASET_PATH` as a constant, which is a good practice for maintainability. The generated code hardcodes the string directly.",deepseek_r1,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('imdb_top_250.csv')

plt.figure()
plt.hist(df['Rating'], bins=15, edgecolor='black')
plt.title('Distribution of Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')

plt.figure()
plt.scatter(df['Runtime'], df['Rating'], alpha=0.6)
plt.title('Rating vs Runtime')
plt.xlabel('Runtime (minutes)')
plt.ylabel('Rating')

plt.show()
```",7,"The generated code correctly addresses the core requirements of the task: loading the CSV, plotting a histogram for 'Rating', and a scatter plot for 'Rating' vs. 'Runtime', both with titles and labels. The code is concise and functional.

However, it scores a 7 due to a few key areas for improvement that are present in the ground truth:

1.  **Robustness (Error Handling and Data Cleaning):** The generated code assumes the CSV file exists and that the 'Runtime' column is immediately in a numeric format. The ground truth includes robust error handling for `FileNotFoundError` and, more importantly, data cleaning for the `Runtime` column (e.g., extracting numeric values from strings like '120 min' and handling potential `NaN` values). This makes the ground truth significantly more robust for real-world datasets where data types might not be perfectly clean.
2.  **Plotting Libraries:** While `matplotlib` is used, `seaborn` is also mentioned in the prompt. The ground truth wisely uses `seaborn.histplot` which provides a nicer histogram with KDE by default and is generally aesthetically more pleasing, aligning with modern data visualization practices.
3.  **Visualization Enhancements:** The ground truth includes minor but effective enhancements for better visualization, such as `sns.histplot` for the histogram, `alpha` for scatter plot points, and `plt.tight_layout()` for better plot spacing.
4.  **Clarity for Runtime:** The ground truth preprocesses `Runtime` into `Runtime_minutes`, making it clear that numerical conversion has occurred, which is a good practice if `Runtime` indeed contains string data.

For a score of 8 or higher, the generated code would need to incorporate similar levels of robustness for data loading and cleaning, and potentially leverage `seaborn` for enhanced plot aesthetics.",deepseek_r1,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

df = pd.read_csv('penguins.csv')

print(""Missing values per column:"")
print(df.isna().sum())

numerical_columns = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_clean = df.dropna(subset=numerical_columns)

print(""\nDescriptive statistics:"")
print(df_clean[numerical_columns].agg(['mean', 'median', 'std', 'min', 'max']))

print(""\nUnique values:"")
print(f""Species: {df['species'].nunique()}"")
print(f""Island: {df['island'].nunique()}"")
`...",8,"The generated code correctly addresses all the core requirements of the task. It loads the dataset, reports NA counts per column, correctly filters rows based on NAs in the specified numerical columns, calculates and reports descriptive statistics for these columns on the filtered data, and reports unique counts for 'species' and 'island'.

Strong points:
- Proper use of pandas for data loading and manipulation.
- Correct application of `dropna(subset=...)` for filtering.
- Efficient use of `.agg()` for calculating multiple descriptive statistics.
- Correct use of `.nunique()` for unique value counts.

Areas for improvement to achieve a higher score (9 or 10):
- **Error Handling:** The ground truth includes robust `try-except` blocks for `FileNotFoundError` and general exceptions when loading the CSV, which is crucial for real-world applications. The generated code assumes the file will always be present, which is less robust.
- **Output Clarity:** While the output is correct, the ground truth uses more descriptive print statements and headers (e.g., ""--- Missing Values Report ---"", ""Original number of rows:"", ""Number of rows after dropping NAs..."") which significantly improves readability and understanding of the output for the user.
- **Filtering Report:** The ground truth reports the number of rows before and after filtering, which provides valuable context for the `dropna` operation.
- **Conditional Printing for NAs:** The ground truth specifically filters to show only columns with *missing values* and reports ""No missing values found in any column"" if none exist. The generated code will print all columns with 0 missing values, which is less precise.",deepseek_r1,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

df = pd.read_csv('penguins.csv')
result = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')][['culmen_length_mm', 'culmen_depth_mm']].dropna()
print(result.head(5))
```",9,"The generated code correctly solves the task. It loads the dataset, filters by species and island, selects the specified columns, drops rows with missing values, and displays the first 5 rows. The code is concise and efficient.

However, there is one minor subtlety: `dropna()` without a `subset` argument will drop rows if *any* column in the DataFrame has a missing value. In this specific case, since only `culmen_length_mm` and `culmen_depth_mm` are selected, `dropna()` implicitly operates on these two columns only, making the result functionally identical to the ground truth. But for robustness and clarity, explicitly stating `subset=['culmen_length_mm', 'culmen_depth_mm']` in `dropna()` would be slightly better practice, especially if the chain of operations was different or more columns were present temporarily.

The ground truth also includes error handling for `FileNotFoundError`, which is good practice for production code but was not explicitly required by the prompt's instructions for the core task, so its absence in the generated code is not a deduction for this benchmark.

Overall, it's a very good solution.",deepseek_r1,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('penguins.csv')
df_clean = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])
sns.scatterplot(x='culmen_length_mm', y='culmen_depth_mm', hue='species', data=df_clean)
plt.title('Culmen Length vs Depth by Species')
plt.xlabel('Culmen Length (mm)')
plt.ylabel('Culmen Depth (mm)')
plt.show()
```",9,"The generated code correctly addresses almost all requirements. It loads the CSV, filters out NAs from the specified columns, creates the scatter plot with correct axes and `hue` mapping, and includes a title and axis labels. The imports are proper.

The only minor point of improvement is that it lacks the error handling for `FileNotFoundError` that the Ground Truth includes, which makes the Ground Truth slightly more robust for a general-purpose function. However, assuming the file is present as per the problem description, the core plotting logic is sound and complete. The ground truth also uses a function and `if __name__ == '__main__':` block, which is good practice for reusability, but not explicitly requested in the prompt beyond ""Provide only the Python code"".",deepseek_r1,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Fill missing values with column means
df['culmen_length_mm'].fillna(df['culmen_length_mm'].mean(), inplace=True)
df['culmen_depth_mm'].fillna(df['culmen_depth_mm'].mean(), inplace=True)

# Calculate and print statistics for cleaned columns
print(""Culmen Length (mm):"")
print(f""Mean: {df['culmen_length_mm'].mean():.2f}"")
print(f""Median: {df['culmen_length_mm'].median():.2f}"")
print(f""Standard Deviation: {df['culm...",8,"The generated code correctly performs all the specified tasks: loading the dataset, filling NA values in the specified columns with their respective means, calculating and printing the mean, median, and standard deviation for the cleaned columns, and printing the total number of rows and columns.

**Strengths:**
*   **Correctness:** All calculations and data manipulations are accurate.
*   **Conciseness:** The `fillna` operations are compact and efficient.
*   **Readability:** The code is easy to understand.

**Areas for Improvement:**
*   **Error Handling:** The generated code lacks error handling for `FileNotFoundError` if `penguins.csv` does not exist. The ground truth includes a `try-except` block for this.
*   **Code Reusability/Maintainability:** The statistical calculations for `culmen_length_mm` and `culmen_depth_mm` are repeated. While it works for two columns, for a larger number of numerical columns, looping through them (as in the ground truth) would be more maintainable and less repetitive.
*   **Clarity in Output:** The ground truth provides more descriptive headers and separation for each section (e.g., ""--- NA Imputation Summary ---"", ""--- Statistics for Cleaned Numerical Columns ---""), making the output easier to parse. The ground truth also explicitly shows the initial NA count and the mean used for imputation, which is useful for debugging and understanding the imputation process.
*   **Column Existence Check:** The ground truth code explicitly checks if a column exists in the DataFrame before attempting operations on it. While unlikely for this specific problem (as the columns are predefined), it's good practice for more robust code when column names might be external inputs or prone to typos.

Overall, it's a very good solution that meets the core requirements. The improvements would mainly focus on robustness, maintainability, and providing more informative output.",deepseek_r1,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows with missing flipper length
df = df.dropna(subset=['flipper_length_mm'])

# Create histogram for flipper length
plt.figure()
sns.histplot(data=df, x='flipper_length_mm')
plt.title('Distribution of Flipper Length (mm)')
plt.xlabel('Flipper Length (mm)')
plt.show()

# Create bar chart for penguin counts by island
plt.figure()
sns.countplot(data=df, x...",8,"The generated code correctly addresses all core requirements of the task. It loads the dataset, handles NA values in `flipper_length_mm`, and generates both the specified histogram and bar chart with appropriate titles and x-axis labels.

Strong points:
- Correctly uses `pd.read_csv`, `df.dropna(subset=['flipper_length_mm'])` for data loading and NA handling.
- Effectively uses `seaborn.histplot` and `seaborn.countplot` for the visualizations.
- Correctly applies `plt.title` and `plt.xlabel` for the histogram.
- Correctly applies `plt.title`, `plt.xlabel`, and `plt.ylabel` for the bar chart.
- Each plot is displayed correctly using `plt.show()`.

Areas for improvement to achieve a higher score (9-10):
- **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions when loading the CSV, which makes the code more robust. The generated code lacks this.
- **Axis Label for Histogram:** The histogram is missing a y-axis label (`plt.ylabel`). The ground truth includes this, typically labeled 'Count'.
- **Clarity in `plt.figure()`:** While `plt.figure()` works, explicitly setting a `figsize` can improve readability and presentation, especially for reports.
- **Styling and Enhancements:** The ground truth includes `sns.set_theme`, `kde=True` for the histogram, and `order` for the bar chart, along with `plt.xticks`, `plt.yticks`, `plt.grid`, and `plt.tight_layout`. These are good practices for professional-looking and readable plots, but were not explicitly requested.

Overall, it's a solid solution that meets the specified criteria directly. The missing error handling and minor plot refinements are the main points for enhancement.",deepseek_r1,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter to include only 'Torgersen' island
torgersen_penguins = df[df['island'] == 'Torgersen']

# Count penguins per species and print
species_counts = torgersen_penguins['species'].value_counts()
print(species_counts)
```",8,"The generated code correctly solves the task by loading the dataset, filtering for 'Torgersen' island, and then counting and printing the species counts. It is concise and directly addresses all the core requirements.

Strong points:
- **Correctness:** The code correctly performs all the requested operations.
- **Conciseness:** It achieves the goal with minimal, readable code.
- **Usability:** If the file is present and columns exist, it will produce the desired output.

Areas for improvement (to reach a perfect score and match the robustness of the ground truth):
- **Error Handling:** The generated code lacks error handling for `FileNotFoundError` (if 'penguins.csv' doesn't exist) or `KeyError` (if 'island' or 'species' columns are missing). This makes it less robust than the ground truth.
- **Edge Cases:** It does not explicitly handle the case where no penguins are found on 'Torgersen' island, although `value_counts()` would return an empty Series if `torgersen_penguins` is empty, which is a reasonable implicit handling.
- **Informative Output:** The ground truth provides more user-friendly output messages (e.g., ""Dataset loaded successfully"", ""No penguins found on 'Torgersen' island""). While not strictly part of the core task, it improves user experience.",deepseek_r1,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

print(""Shape of the DataFrame:"", df.shape)
print(""\nData ...",10,"The generated code is entirely correct and effectively fulfills all the requirements of the task. It accurately defines the DataFrame as specified, correctly prints its shape, and lists the data types of each column. The code quality is good, with proper imports and a clean structure. It directly addresses all parts of the prompt, making it complete and efficient. The output format for printing the shape and dtypes is concise and clear.",deepseek_r1,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('titanic.csv')

plt.figure()
df['Survived'].value_counts().sort_index().plot(kind='bar')
plt.title('Survivors vs Non-Survivors')
plt.xlabel('Survived')
plt.ylabel('Count')
plt.xticks(ticks=[0, 1], labels=['No', 'Yes'], rotation=0)
plt.show()

plt.figure()
df['Pclass'].value_counts().sort_index().plot(kind='bar')
plt.title('Passenger Class Distribution')
plt.xlabel('Class')
plt.ylabel('Count')
plt.xticks(rotation=0)
p...",7,"The generated code correctly creates four distinct plots with proper titles and labels for Survived, Pclass, and Sex. However, for the Age histogram, it uses `dropna()` which discards missing values instead of handling them (e.g., by imputation), which was specified in the task as ""handle missing values appropriately."" While `dropna()` is a form of handling, the ground truth's approach of filling with the median is generally more appropriate for preserving data and better fulfills the ""appropriately"" clause in a data science context. Additionally, some plot aesthetic choices (like grid, colors, `tight_layout`, and informative x-axis labels for ""Survived"") present in the ground truth are missing, which subtly impact the overall quality and readability of the visualizations compared to the ideal. The generated code is concise but loses some robustness (e.g., file existence check) and visual polish found in the ground truth.",deepseek_r1,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]
}

df = pd.DataFrame(data)

avg_midterm = df['Midterm_Score'].mean()
print(f""Average Midterm Score: {...",9,"The generated code correctly performs all requested operations: creates the DataFrame, calculates and prints the average midterm and final scores, and identifies the subject with the highest average final score. The imports are correct, and the code structure is clear and concise.

One minor suggestion for improvement, which is not critical for correctness but good practice, would be to include the specific average score of the highest subject when printing, similar to the ground truth. This adds more informative output. However, the current output still correctly identifies the subject. Another tiny detail is the phrasing of the print statements (""Average Midterm Score for all students"" vs. ""Average Midterm Score""), but this is purely cosmetic and does not affect functionality. Overall, it's a very good solution.",deepseek_r1,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
    'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
    'Volume': [1000000, 1200000, 1100000, 1300...",9,"The generated code is highly correct and effectively solves all aspects of the task. It properly creates the DataFrame, sets the index, calculates the 'Daily_Range', identifies the date with the highest 'Close' price, and prints the updated DataFrame as requested. The code quality is good, with correct imports and a logical flow.

One small area for improvement, making it slightly less complete than the ground truth in terms of output formatting, is that the `print` statement for the date with the highest close price does not explicitly format the date as `YYYY-MM-DD`. While the default output of a pandas Timestamp is often close, specifying it explicitly with `strftime('%Y-%m-%d')` ensures consistent output format as seen in the ground truth and makes it more robust. Also, the ground truth uses a more descriptive print statement for the final DataFrame, indicating what is being printed, which enhances clarity. These are minor cosmetic differences and do not affect the functionality or correctness.",deepseek_r1,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
importdata =data =data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Web 'Mouse 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST001']
}

df = pd....",6,"The generated code addresses most of the requirements but has significant syntax errors and minor output discrepancies.

**Correctness:**
- The code correctly calculates 'Total_Revenue'.
- It correctly identifies the best-selling product by quantity using `groupby().sum().idxmax()`.
- It attempts to print the first 5 rows of the DataFrame.

**Code Quality:**
- The `import` statement is malformed (`importdata =data =data = {`). This is a critical syntax error that would prevent the code from running. It should be just `import pandas as pd`.
- The product data for `Webcam` is truncated and syntactically incorrect: `'Web 'Mouse 'Mouse'` instead of `'Webcam'`. This will cause an error when trying to create the DataFrame.

**Completeness:**
- All functional requirements regarding DataFrame creation, 'Total_Revenue' calculation, identification of the best-selling product, and printing the head are present.
- The print statement for the best-selling product has a slight wording difference compared to the ground truth, but it conveys the correct information.
- `df.head()` by default prints the first 5 rows, so `df.head(5)` is not strictly necessary but clearer.

**Areas for Improvement:**
- **Critical Fix:** Correct the `import` statement to `import pandas as pd`.
- **Critical Fix:** Correct the `Product` list data, specifically for 'Webcam', to `'Webcam'`.
- Ensure the print messages match the ground truth exactly if strict adherence is required, though the current messages are sufficient.",deepseek_r1,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

print(df['SessionDurationMinutes'].mean())
print(df['DeviceType'].value_counts())
```",9,"The generated code correctly performs all requested operations: defining the DataFrame, calculating the average session duration, and counting users by device type. This demonstrates a solid understanding of basic pandas operations.

The only minor points for improvement, preventing a perfect score, are related to code quality and presentation:
1.  **Readability of Output:** The output is a bit terse. Adding descriptive print statements (as in the ground truth) would make the output more user-friendly and easier to interpret for anyone running the script. For instance, clearly labeling ""Average Session Duration:"" and ""Number of users per Device Type:"" before printing the results would be beneficial.
2.  **Formatting:** The average session duration isn't formatted (e.g., to two decimal places), which can sometimes be desirable for numerical outputs.

Overall, the code is highly functional and correct, but could benefit from minor enhancements for better user experience and robust output formatting.",deepseek_r1,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

print(""Maximum TemperatureCelsius:"", df[""TemperatureCelsius""].max())
print(""Min...",10,"The generated code is excellent. It correctly defines the DataFrame exactly as specified, and then accurately finds and prints both the maximum ""TemperatureCelsius"" and the minimum ""HumidityPercent"" using the `max()` and `min()` pandas methods. The output format is clear and directly addresses the prompt's requirements. The code is concise, efficient, and demonstrates a solid understanding of pandas basics.",deepseek_r1,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# Calculate and print total likes
print(df['Likes'].sum())

# Calculate average shares per PostType and find the maximum
posttype...",9,"The generated code is highly correct and concise, fulfilling all the requirements of the task. It correctly defines the DataFrame, computes the total likes, and identifies the post type with the highest average shares. The code structure is clean and uses appropriate pandas functions.

Suggestions for improvement (to reach a perfect 10):
- The print statements could be more descriptive, similar to the ground truth (e.g., ""Total number of Likes across all posts: ...""). This makes the output clearer and more user-friendly. The current output just prints raw numbers/strings without context.",deepseek_r1,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It successfully defines the DataFrame, calculates the average age, and counts patients by gender. The code quality is good, with proper imports and a clear structure.

The only minor difference, which is a matter of stylistic preference rather than correctness, is the exact wording of the print statements. The ground truth provides slightly more descriptive print statements (e.g., ""Average Age of all patients:"" vs. ""Average Age:""). However, this does not affect the correctness of the calculated values or the core functionality.

Strong points:
- Correctly defines the DataFrame with `pd.to_datetime` for date columns.
- Accurately calculates the average age using `df['Age'].mean()`.
- Correctly counts patients by gender using `df['Gender'].value_counts()`.
- Uses f-strings for formatted output, which is good practice.
- The code is concise and efficient.",deepseek_r1,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}

df = pd.DataFrame(data)

print(df.loc[df['Rating'].idxmax(), 'Title'])
print(df['Year'].mean())
```",10,"The generated code is excellent.
1.  **Correctness:** It correctly defines the DataFrame, identifies the title of the movie with the highest rating using `idxmax()`, and calculates the average year. All operations are accurate.
2.  **Code Quality:** The code is concise and uses idiomatic pandas operations (`idxmax()`, `.mean()`). It's clean, readable, and directly addresses the requirements without unnecessary complexity.
3.  **Completeness:** It fully addresses all parts of the task: DataFrame creation, printing the title of the highest-rated movie, and printing the average year.

The use of `df.loc[df['Rating'].idxmax(), 'Title']` is a very efficient and clean way to get the title of the movie with the maximum rating, which is often preferred in pandas for its conciseness. The result for the average year is also spot on.",deepseek_r1,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

print(""Total number of cars:"", len(df))
print(""Average PriceUSD:"", df...",9,"The generated code correctly defines the DataFrame, accurately calculates and prints the total number of cars, and the average ""PriceUSD"". All requirements of the task are met. The code is concise and functional.

One minor suggestion for improvement, which is not critical for correctness but improves readability and user-friendliness, would be to add more descriptive f-strings to the print statements, similar to the ground truth code. For example, `print(f""Total number of cars in the dataset: {len(df)}"")` and `print(f""Average PriceUSD of all cars: ${df['PriceUSD'].mean():.2f}"")` would make the output clearer, especially the formatting for currency. However, as the current output is understandable, it doesn't detract significantly from the solution.",deepseek_r1,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}
df = pd.DataFrame(data)

print(df.loc[df['HP'].idxmax(), 'Name'])
print(df.loc[df...",10,"The generated code correctly defines the DataFrame with the exact data provided. It then accurately identifies and prints the names of the Pokemon with the highest ""HP"" and lowest ""Defense"" using concise and correct pandas `idxmax()` and `idxmin()` functions coupled with `loc[]`. The code is clean, efficient, and directly addresses all parts of the task requirement. It is a perfect solution.",deepseek_r1,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd

df = pd.read_csv('titanic.csv')

df['Age'].fillna(df['Age'].median(), inplace=True)
most_frequent_embarked = df['Embarked'].mode()[0]
df['Embarked'].fillna(most_frequent_embarked, inplace=True)
df.drop('Cabin', axis=1, inplace=True)
df['Title'] = df['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0].str.strip()

print(""Cleaned dataset shape:"", df.shape)
print(df.head())
```",7,"The generated code correctly performs most of the required data cleaning steps. It handles missing Age and Embarked values, drops the Cabin column, and extracts titles. However, it lacks the proper `re` import for robust title extraction (though its current regex-like string splitting works for most cases), and it doesn't standardize less common titles as seen in the ground truth, which is a good practice for this task. The output format for printing the shape and head is slightly different from the ground truth. It also misses the initial step of identifying columns with missing values.",deepseek_r1,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}
df = pd.DataFrame(data)

print(df.loc[df['...",10,"The generated code is correct, complete, and adheres to good code quality. It successfully defines the DataFrame as specified, identifies the title of the book with the highest price, and calculates the total stock. The use of `idxmax()` is efficient for finding the row with the maximum price, and `sum()` correctly calculates the total stock. The code is concise and directly addresses all parts of the task.",deepseek_r1,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd

df = pd.read_csv('water_potability.csv')

print(""1. First 10 rows:"")
print(df.head(10))

print(""\n2. Dataset shape:"")
print(df.shape)

print(""\n3. Columns and data types:"")
print(df.dtypes)

print(""\n4. Potable vs Non-potable water samples (%):"")
print(df['Potability'].value_counts(normalize=True).mul(100).round(2))

print(""\n5. Missing values per column:"")
print(df.isnull().sum())

print(""\n6. Summary statistics:"")
print(df.describe().T)
```",8,"The generated code correctly performs all the requested tasks. It loads the dataset, displays the first 10 rows, shows the shape, lists column names and data types, calculates potability percentages, counts missing values, and provides summary statistics.

However, there are a few minor areas for improvement compared to the ground truth:

1.  **Robustness (File Loading):** The generated code lacks error handling for `FileNotFoundError` or other exceptions when loading the CSV. The ground truth includes a `try-except` block, which is good practice for real-world applications.
2.  **Clarity of Output (`df.dtypes` vs `df.info()`):** While `df.dtypes` correctly lists column names and their data types, `df.info()` (used in the ground truth) provides a more comprehensive summary, including the non-null count for each column, which can be very informative during EDA.
3.  **Missing Value Presentation:** The ground truth code specifically filters to `print(missing_values[missing_values > 0])` and adds a message if no missing values are found. This makes the output cleaner by only showing columns with missing values and provides a clear statement when there are none. The generated code prints all columns, even those with zero missing values.
4.  **Summary Statistics (`.T`):** Using `.describe().T` is a good choice for readability when there are many features, as it transposes the output. This is a stylistic preference, and both are valid, but the ground truth simply uses `.describe()`.
5.  **Output Readability/Separation:** The ground truth uses `print(""\n"" + ""=""*50 + ""\n"")` to visually separate the different output sections, which improves readability. The generated code uses only ""\n"" which is less pronounced.

Despite these minor points, the generated code fulfills the core requirements of the task effectively and efficiently.",deepseek_r1,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd
import numpy as np

df = pd.read_csv('water_potability.csv')
original_missing = df.isnull().sum()
original_shape = df.shape

numerical_cols = df.select_dtypes(include=np.number).columns
df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())
df.drop_duplicates(inplace=True)

df['ph'] = df['ph'].clip(0, 14)
cols_to_clip = ['Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']
for col in col...",6,"The generated code addresses most of the requirements but has several critical areas for improvement.

**Strengths:**
*   **Correct Imports:** Uses `pandas` and `numpy` as required.
*   **Missing Value Handling:** Correctly identifies numerical columns and fills NaNs with the mean.
*   **Duplicate Removal:** Correctly uses `df.drop_duplicates()`.
*   **Before/After Comparison:** Attempts to show missing values and shape before and after cleaning.
*   **Display Cleaned Data:** Shows `df.head()`.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Identification and Reporting (Before):** The ground truth explicitly identifies and *prints only the columns that have missing values* before cleaning. The generated code prints `original_missing` which will show all columns, even those with no missing values, making it less clear which columns were affected. This is a minor point for correctness but affects completeness of detail.

2.  **Range Validation Strategy:** This is the most significant deviation and a critical area for improvement.
    *   **Ground Truth:** The ground truth *removes rows* where values are outside reasonable physical ranges (`df = df[(df['ph'] >= 0) & (df['ph'] <= 14)]` and `df = df[df[col] >= 0`). This is a more robust data quality check, as physically impossible values often indicate corrupted data points that should be removed. It also reports how many rows were removed.
    *   **Generated Code:** The generated code uses `.clip(lower=0)` and `.clip(0, 14)`. While clipping can be useful, it *modifies* the data point to fit the range rather than removing the potentially erroneous observation. For data quality validation, removing out-of-range values is often preferred as they are treated as invalid. This significantly impacts data integrity. The task explicitly said ""verify that all values are within reasonable ranges for water quality parameters,"" which implies checking validity, not just forcing values into a range. No reporting on how many values/rows were affected by this ""correction"" is provided.

3.  **Error Handling:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, making the code more robust. The generated code lacks this.

4.  **Clarity and Detail in Summary:**
    *   The ground truth provides more detailed print statements for each cleaning step (e.g., ""Removed X duplicate rows"", ""Filled missing values in 'X' with its mean"", ""No rows removed due to out-of-reasonable-range values""). The generated code is much more concise in its output, showing only the final before/after summary. While functional, it hides the process details.
    *   The ground truth explicitly verifies ""No missing values remaining"" after cleaning, which is a good verification step. The generated code just prints `new_missing` which might still contain non-zero values if something went wrong (though in this case for numerical columns, it will be zero).

5.  **`Potability` Column Handling:** The `Potability` column is a target variable, typically binary (0 or 1). Clipping it `(0, 1).astype(int)` is fine, but it's not explicitly part of ""water quality parameters"" from the perspective of continuous physical ranges like pH or hardness. Its inclusion as a ""value range check"" could be debated, but it doesn't hurt. However, the core range validation was for physical parameters.

To improve, the generated code should:
*   Switch from `clip` to filtering/dropping rows for out-of-range values.
*   Add informative print statements for each cleaning step.
*   Include basic file loading error handling.",deepseek_r1,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('water_potability.csv')
df.dropna(inplace=True)

plt.figure(figsize=(8, 5))
sns.countplot(x='Potability', data=df)
plt.title('Distribution of Potable vs Non-potable Water Samples')
plt.xlabel('Potability (0 = Non-potable, 1 = Potable)')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
sns.histplot(df['ph'], kde=True)
plt.title('Distribution of pH')
plt.xlabel('pH'...",7,"The generated code correctly addresses most of the visualization requirements. It creates all four distinct plots (bar chart, histograms, boxplot, heatmap) with proper imports and generally good structure.

However, there are a few areas for improvement, primarily concerning the handling of missing values and plot aesthetics/completeness:

1.  **Missing Value Handling:** The generated code uses `df.dropna(inplace=True)` at the beginning. While this handles missing values, it removes *all* rows with *any* missing data across the entire dataset. In many real-world scenarios, it's better to handle missing values on a per-plot basis (e.g., `dropna()` on specific columns for histograms or letting seaborn's plotting functions handle NaNs default behavior for boxplots/heatmaps), especially if different columns have missing values, to preserve as much data as possible for other plots. The ground truth demonstrates this by using `.dropna()` selectively for histograms and letting seaborn handle it for others, which is a more robust approach.
2.  **Plot formatting/Completeness:**
    *   **Bar Chart:** The ground truth code has more detailed `xticks` labels for clarity (e.g., 'Not Potable' vs '0'). The generated code's labels are acceptable but less user-friendly. The ground truth also adds `grid(axis='y')` for better readability.
    *   **Histograms:** The generated code uses `plt.subplot`, which is functional but less flexible than `plt.subplots` (as used in the ground truth) for managing axes and titles. The ground truth also adds `grid(axis='y')` and specific colors.
    *   **Boxplot:** Similar to the bar chart, the `xticks` could be more descriptive.
    *   **Overall Aesthetics:** The ground truth sets `sns.set_style(""whitegrid"")` for a consistent and clean look, and uses `plt.tight_layout()` more consistently which helps prevent labels from overlapping and generally improves plot presentation.
3.  **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness.

Overall, the generated code fulfills the core task but lacks some of the finer details and best practices for robust data visualization and missing data handling compared to the ground truth.",deepseek_r1,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor

# Load data
df = pd.read_csv('winequality-red.csv', sep=';')

# Create new features
df['total_acidity'] = df['fixed acidity'] + df['volatile acidity']
df['sulfur_dioxide_ratio'] = df['free sulfur dioxide'] / (df['total sulfur dioxide'] + 1e-6)
df['alcohol_density_ratio'] = df['alcohol'] / df['density']

# Separate features and target
X = df.drop('...",7,"The generated code creates three new features and applies two different feature selection techniques as requested. It correctly uses RFE with RandomForestRegressor and SelectKBest with f_regression. It also attempts to compare the selected features.

However, there are several areas for improvement:
1.  **Data Loading:** The `winequality-red.csv` file is typically comma-separated, but the generated code assumes `sep=';'`, which is incorrect for this dataset and might lead to errors or incorrect parsing if a different file is provided. The ground truth correctly assumes a default comma separator or handles potential `FileNotFoundError`.
2.  **Feature Engineering Meaningfulness:** While three features are created, their 'meaningfulness' could be more explicitly stated or justified (e.g., in comments, even if not part of the final output). ""total_acidity"" is a simple sum and less insightful than ""acidity_ratio"". ""sulfur_dioxide_ratio"" is good. ""alcohol_density_ratio"" is okay, but ""alcohol-sulphates_interaction"" from the ground truth is often a stronger predictor in wine quality.
3.  **Handling of `n_features_to_select` in RFE:** The generated code initializes `RFE(rfe_estimator, n_features_to_select=7)`. While it correctly selects 7 features, it does not extract the *importance scores* of these selected features in the same way the ground truth does. The `rfe_selector.estimator_.feature_importances_` will return importances for the *entire* dataset if `fit` was done on the entire dataset, or only the top features *if* the estimator itself is refit on the reduced set by `RFE`. The ground truth explicitly refits the `RandomForestRegressor` on the *selected* features to get their importances, which is a more robust way to present feature importance for RFE's chosen set.
4.  **Comparison and Contrast:** The comparison is very minimal (`print` statements showing sets). The prompt specifically asked to ""Compare and contrast the feature sets selected by each method"" in a more analytical way, including explaining *why* they might differ (e.g., filter vs. wrapper methods). The ground truth provides a detailed explanation of the differences in method philosophies and potential reasons for differing results.
5.  **Display of results:** While the dataframes show the results, there's no overall summary that reiterates the final selected features with their importance or rankings, as the ground truth does by printing detailed dataframes at the end.
6.  **Readability/Best Practices:** The ground truth includes an `EPSILON` for division by zero and handles potential `NaN`/`inf` values from feature engineering. While not strictly required by the prompt, these are good practices for robust code. No `StandardScaler` is used, which is generally good practice before RFE with `RandomForestRegressor` (though less critical than for linear models).
7.  **Minor Bug:** The `rfe_importances = rfe_selector.estimator_.feature_importances_` line in the generated code will contain feature importances for all features *if* `rfe_selector.fit(X, y)` trains the estimator on all features initially. To get importances *for the selected features only*, you'd typically retrain the estimator on `X[rfe_features]` as done in the ground truth. As implemented, `rfe_importances` will contain importances for *all* features, even if the feature column in `rfe_df` only shows the selected ones, potentially creating a mismatch. The ground truth explicitly refits the estimator on the reduced set for accurate importance scores of only the selected features.

Overall, it's a functional solution that hits the main points but lacks the depth, robustness, and detailed explanation found in a more expert solution.",deepseek_r1,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import cross_validate, cross_val_predict, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor, StackingRegressor
from sk...",7,"The generated code attempts to address all requirements but has several issues that prevent it from being a strong solution.

**Strengths:**
- Correctly loads and preprocesses the dataset, handling numerical and categorical features.
- Implements and evaluates all specified individual ensemble models (RF, GB, XGBoost, AdaBoost).
- Implements both Voting and Stacking ensembles.
- Uses cross-validation for model evaluation.
- Attempts hyperparameter tuning with GridSearchCV.
- Includes functions for feature importance plotting and residual analysis.
- Imports are mostly correct and present.

**Areas for Improvement / Issues:**

1.  **Preprocessing and Pipeline Integration:**
    *   The `num_cols = X.select_dtypes(include=np.number).columns.tolist()` and `cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()` lines for initial separation are fine, but the ground truth handles 'Name' and 'Rank' explicitly first, and also coercing 'User_Score' to numeric which is crucial for correct numerical/categorical split. The generated code misses this, leading to 'User_Score' being treated as categorical (if it contains non-numeric values like 'tbd') or 'Name'/'Rank' remaining if not dropped.
    *   The `SimpleImputer` for numerical features in the preprocessor is correct, but for categorical, `strategy='constant', fill_value='Unknown'` might not be the best general strategy; `most_frequent` is often preferred as it aligns with the distribution.
    *   The way individual models are defined (`rf = Pipeline(...)`) and then `voting = VotingRegressor([('rf', rf.named_steps['regressor']), ...])` is problematic. When passing `rf.named_steps['regressor']` to `VotingRegressor`, you're only passing the *estimator*, not the entire pipeline with its preprocessor. The `VotingRegressor` and `StackingRegressor` themselves typically expect base estimators, *not* pipelines. Their `n_jobs` argument, if used, would apply to parallelizing the *fitting of the base estimators*. The ground truth correctly uses `VotingRegressor` and `StackingRegressor` with bare estimators (`RandomForestRegressor`, etc.) that are then wrapped in a single pipeline for evaluation. The generated code attempts to put the `voting` and `stacking` estimators into `voting_pipe` and `stacking_pipe` *after* they've been defined using the already preprocessed estimators, which effectively means the preprocessor is applied *twice* (once within each individual model's named_steps['regressor'] if they were full pipelines, and then again by the `voting_pipe`'s preprocessor). This conceptual error can lead to incorrect preprocessing or errors.

2.  **Cross-Validation and `n_jobs`:**
    *   `cross_validate` for individual models is a good approach. However, for `VotingRegressor` and `StackingRegressor`, passing the entire `voting_pipe` and `stacking_pipe` to `cross_validate` is the correct way, but the internal structure of these pipelines needs to be fixed as per point 1.
    *   The use of `n_jobs=-1` within `cross_validate` is good, but `XGBRegressor` also needs `n_jobs=-1` directly in its constructor to utilize all cores effectively for its internal operations.

3.  **Hyperparameter Tuning:**
    *   The code only checks for `XGBoost` for tuning (`if best_model_name == 'XGBoost':`). This means tuning will only happen if XGBoost is the best model, regardless of which model actually performed best. The requirement was to perform tuning for `the best individual model`. The ground truth dynamically identifies the best model and applies tuning specific to that model.
    *   The generated tuning only considers `neg_root_mean_squared_error`, while the overall evaluation uses MAE, MSE, RMSE, R2. It's better to align the tuning metric with the primary evaluation metric if possible, or articulate why a different one is chosen.

4.  **Feature Importance:**
    *   The feature importance plot for `AdaBoost` will likely fail or be misleading unless the base estimator `AdaBoost` uses (e.g., a `DecisionTreeRegressor`) has a `feature_importances_` attribute. Adaboost does not have it directly. The ground truth code correctly points this out.
    *   The way `all_features` is constructed (`preprocessor.fit(X)`, then `get_feature_names_out`) is correct for getting feature names. However, `feature_names[indices][::-1]` assumes `feature_names` is an array that maps directly to the indices. The output of `get_feature_names_out` is correct, but the plotting logic needs to be robust (e.g., `all_features[indices]` rather than just `feature_names[indices]`).

5.  **Residual Plots:**
    *   Generating residual plots for *every* model using `cross_val_predict` is memory-intensive and may not be the most efficient or required. The prompt asked for ""Analyze prediction errors and create residual plots"" implying one or a few key models, not all. The ground truth focuses on one well-performing (or the best) model.

6.  **Code Structure and Readability:**
    *   Missing clear comments or print statements to denote sections of the code as per the task requirements (e.g., ""--- 1. Load and Preprocess Data ---""). This makes it harder to follow the flow and verify against requirements.
    *   Error handling for file not found is missing.

To improve, the code needs to:
*   Refine the data preprocessing to handle 'Name', 'Rank' columns and 'User_Score' data type conversion as in the ground truth.
*   Correctly set up the `VotingRegressor` and `StackingRegressor` to work with the preprocessor within a single pipeline. The `VotingRegressor` and `StackingRegressor` should be the ""regressor"" step in their respective pipelines, and take *bare* estimators as inputs, not pipelines.
*   Implement dynamic hyperparameter tuning for the actually best-performing model, not just hardcoding for XGBoost.
*   Address the feature importance calculation and plotting for models like AdaBoost which might not have it directly.
*   Add more comprehensive print statements to track progress and results.",deepseek_r1,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_square...",6,"The generated code attempts to address all requirements of the task, which is a significant positive. However, it falls short in several key areas of correctness, completeness, and robustness, particularly for advanced statistical modeling and time series analysis.

**Areas for Improvement/Incorrectness:**

1.  **Data Loading and Aggregation:**
    *   The `Year` column in the CSV is likely `Year_of_Release`. The generated code assumes `Year` directly. This leads to a `KeyError` if `Year_of_Release` is the correct column name.
    *   `annual_sales = annual_sales.asfreq('A').fillna(0)`: Using `fillna(0)` for missing years might be an oversimplification and can distort the time series. It's often better to interpolate or acknowledge missing periods explicitly. The ground truth uses `asfreq('YS')` for consistency and lets `pd.to_datetime` handle the conversion from int year.
    *   The ground truth also correctly handles `Year_of_Release` and `Global_Sales` NaNs.

2.  **ARIMA Modeling:**
    *   `train, test = train_test_split(annual_sales, test_size=0.2, shuffle=False)`: While `shuffle=False` is critical for time series, splitting `annual_sales` which sums `Global_Sales` by `Year` means the actual `df` (which regression models use) is not split consistently over time. The ground truth uses `pm.auto_arima` for better model selection and then performs a rolling forecast for CV, which is more appropriate for time series.
    *   The generated code uses a fixed `order=(2,1,2)` for ARIMA. Real-world scenarios almost always require `auto_arima` or manual ACF/PACF analysis to determine the order, especially for a benchmark.
    *   The forecasted `conf_int` is shown, but not explicitly used or plotted to demonstrate the prediction interval visually, which is a subtle but important part of model interpretation.

3.  **Seasonal Decomposition:**
    *   `seasonal_decompose(annual_sales, period=5)`: For yearly data, a `period` of 5 implies a 5-year cycle, which might be plausible, but for sales and many time series, seasonal decomposition is typically for monthly/quarterly data. The ground truth correctly notes this and proceeds cautiously. The general implementation is okay, but context is missing.

4.  **Multivariate Regression with Interaction Terms:**
    *   `PolynomialFeatures(interaction_only=True, include_bias=False)` creates interaction terms between *all* features, which can lead to a massive number of features (e.g., interaction between all levels of `Genre` and `Platform`) making the model unstable or impossible to fit due to memory/computation. The prompt asks for interaction terms, implying *specific* or *meaningful* ones, not all possible ones. The ground truth carefully selects interaction terms with key variables and limits the number of categories for categorical features.
    *   The interaction term logic is applied on the one-hot encoded features, which will generate interactions between dummy variables (e.g., Genre\_Action \* Genre\_Sports), which are usually not desired or meaningful in this context. Interactions are usually between continuous variables or between a continuous variable and a categorical variable (e.g., Year * Genre_Action). The ground truth correctly handles this.
    *   Confidence/prediction intervals for the multivariate regression are requested, but only `lr_ci = sm.OLS(y_pred_lr, sm.add_constant(X_test)).fit().get_prediction().conf_int()` is mentioned, which is an incorrect way to get intervals for a `Pipeline` from `sklearn`. `statsmodels` provides this directly with `get_prediction`, as shown in the ground truth.

5.  **Polynomial Regression with Regularization:**
    *   `X_poly = poly.fit_transform(preprocessor.fit_transform(X))`: This applies polynomial features to *all* transformed features (including one-hot encoded categorical variables), which is unconventional and often not desired. Polynomial regression is typically applied to continuous numerical features. The ground truth applies polynomial features only to `Year_of_Release`.
    *   No confidence/prediction intervals are provided for these models, despite being explicitly asked. The ground truth attempts to approximate them due to `sklearn`'s limitations.

6.  **Bayesian Regression:**
    *   `pymc3.glm.GLM.from_formula('Global_Sales ~ Year + Genre + Platform + Publisher', df, family=pm.glm.families.Normal())`: While syntactically correct, this model is very simplistic. It doesn't scale categorical variables well and might lead to issues with many unique genres/platforms/publishers. The ground truth correctly handles categorical variables with code and scales continuous variables for better MCMC sampling, and performs posterior predictive checks to show prediction intervals. The generated code also doesn't perform any posterior predictive checks or show credible intervals for predictions.

7.  **Change Point Detection:**
    *   `change_points = algo.predict(pen=10)`: A fixed penalty of `pen=10` is used without justification. The ground truth uses a more principled approach (2 * log(n)) or explains that penalty value selection is critical.

8.  **Survival Analysis:**
    *   `df['Event'] = (df['Global_Sales'] > 0).astype(int)`: This definition of ""event"" is questionable for survival analysis. In survival analysis, `Event` typically signifies the occurrence of the event of interest (e.g., game becoming unprofitable). Here, `Global_Sales > 0` is almost always true for an active game entry, meaning most games would be considered to have ""event observed"" very quickly, leading to a misleading survival function. The `Year` column as duration is also problematic, as `Year` is the release year, not the duration of survival. The ground truth defines 'longevity' more meaningfully as `Year_of_Release_last_observed - Year_of_Release_release + 1`, and uses `Observed_Event = 1` for all, acknowledging the simplification.
    *   CoxPHFitter is imported but not used or demonstrated.
    *   Confidence intervals/prediction intervals for survival models are not discussed.

9.  **Confidence/Prediction Intervals (General):**
    *   The prompt asks for CIs/PIs for *all models*. The generated code only superficially mentions `conf_int` for ARIMA and a questionable `lr_ci` for the linear model, and none for polynomial, Bayesian, or survival models. This is a significant omission.

10. **Time Series Cross-Validation:**
    *   `tscv.split(annual_sales)`: Only the `annual_sales` (yearly total sales) is used for TS-CV, and only for the ARIMA model as implemented. The prompt explicitly asks for models to be validated, implying the regression models as well. The ground truth implements TS-CV for both ARIMA and the regression models.
    *   The `PolynomialFeatures` `fit_transform` and `StandardScaler` `fit_transform` for each fold in the generated polynomial regression section is incorrect for time-series cross-validation. `fit_transform` should only be called on `X_train_poly` and `transform` on `X_test_poly` to prevent data leakage. The ground truth does this correctly.

**Overall:**

The generated code attempts to cover all tasks but lacks the statistical rigor, practical considerations, and thoroughness required for an ""expert code evaluator."" The implementations often use default or simplified approaches that are not robust or fully correct for the problem constraints. Significant effort is needed to ensure the correctness of the statistical modeling applied to the data. The prompt specifically states ""advanced statistical modeling,"" and many of the generated implementations are basic or flawed.",deepseek_r1,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE
import umap
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.ensemble import IsolationForest
from mlxtend.frequent_patterns import apriori, assoc...",6,"The generated code attempts to address most of the requirements but falls short in several areas regarding completeness, robustness, and some aspects of correctness.

**Strengths:**
*   **Preprocessing:** Includes some good feature engineering steps ('Regional\_Sales\_Distribution', 'Market\_Position', 'Publisher\_Portfolio\_Size').
*   **Multiple Clustering Algorithms:** Correctly implements K-Means, DBSCAN, Agglomerative (Hierarchical), and GMM.
*   **Dimensionality Reduction:** Uses t-SNE and UMAP, and attempts visualization.
*   **Anomaly Detection:** Correctly uses IsolationForest.
*   **Association Rule Mining:** Implements Apriori and association rules generation.
*   **Recommendation System:** Provides a basic nearest-neighbor-based recommendation.
*   **Internal Validation:** Calculates Silhouette, Davies-Bouldin, and Calinski-Harabasz scores.

**Areas for Improvement / Missing Aspects:**

1.  **Optimal Cluster Determination:**
    *   The `optimal_clusters` function only plots the elbow and silhouette scores but doesn't actually determine or use an optimal `n_clusters` programmatically for the clustering models. It hardcodes `n_clusters = 4`. The problem statement specifically asks for the determination of optimal clusters for *multiple* algorithms using gap statistic (missing) and using results from silhouette and elbow.
    *   The gap statistic is entirely missing.
    *   For DBSCAN, parameter tuning (eps, min\_samples) is critical and not properly addressed, just fixed values of 0.5 and 5 are given without context, which will likely not yield meaningful results for an arbitrary dataset.

2.  **Robust Preprocessing & Feature Engineering:**
    *   The `Regional_Sales` normalization is done *after* filtering for `Global_Sales > 0`, which is good, but if `Global_Sales` is 0 then division by zero will occur (though less likely with the filter). The ground truth handles this more robustly.
    *   OHE for categorical features is missing; `df = pd.get_dummies(df, columns=['Rating'], drop_first=True)` only applies to 'Rating', while 'Genre', 'Platform', 'Publisher' are critical categorical features that are not encoded and are not included in `X_scaled`. This is a major correctness issue as the clustering is done on numerical features only (and Rating).
    *   The `fillna(0)` on `X` after extracting features could hide larger issues if other numerical features have missing values beyond the imputed ones.

3.  **Consensus Clustering:**
    *   The implementation for consensus clustering is a pairwise co-occurrence count. While this is a valid approach, it's not truly a ""consensus clustering"" method that produces a single best clustering. It effectively creates a new similarity matrix and then clusters *that* using Agglomerative Clustering. This is a reasonable interpretation, but the overall presentation lacks clarity on the ""stability"" aspect mentioned in the prompt.
    *   The code only considers `KMeans`, `DBSCAN`, `Agglomerative`, and `GMM` labels for consensus. If any of these failed or returned noisy results (like DBSCAN), the consensus calculation would be affected.

4.  **Cluster Profiling and Statistical Significance:**
    *   The code calculates ANOVA p-values for features based on the consensus labels. This is a good start. However, it only prints the p-value without clear interpretation or detailed cluster profiles (e.g., mean/mode of features per cluster).
    *   Statistical significance for categorical features (e.g., Chi-squared test) is missing.
    *   The requirement for ""detailed cluster profiles"" is not fully met.

5.  **Recommendation System:**
    *   The current recommendation system is very rudimentary and only recommends the first 5 nearest neighbors of the *first* data point `X_scaled[0:1]`. It doesn't allow for dynamic recommendations for a given game ID or name, which the ground truth enables. This is a significant functional limitation.

6.  **External Validation Metrics:**
    *   The prompt asked for external validation metrics, which are completely missing. This would typically involve comparison against known labels if available (e.g., Adjusted Rand Index, Normalized Mutual Information). While often not possible with real-world, unlabelled data, its explicit omission is a drawback given the prompt.

7.  **Code Structure and Readability:**
    *   The code is somewhat monolithic, without clear functions or classes to encapsulate different phases (preprocessing, clustering, profiling, etc.). The ground truth follows a better, more modular structure with clear print statements indicating progress.
    *   Imports are generally fine.
    *   Lack of comments and meaningful print statements makes it harder to follow the execution flow.

8.  **DBSCAN:** No proper strategy for `eps` and `min_samples` determination, which makes DBSCAN effectively useless without manual tuning.

The generated code provides a functional prototype for several components but lacks the depth, robustness, and comprehensive implementation required by the prompt, especially in the areas of optimal cluster determination, comprehensive profiling, and external validation.

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder # LabelEncoder is not strictly needed for OHE but sometimes used before OHE
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE
import umap # Assuming umap-learn package is installed
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.ensemble import IsolationForest
from mlxtend.frequent_patterns import apriori, association_rules
from sklearn.neighbors import NearestNeighbors
from scipy.cluster.hierarchy import dendrogram, linkage # Linkage and dendrogram used for hierarchical visualization, not directly for clustering results
from scipy.stats import f_oneway
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.compose import ColumnTransformer # Needed for proper categorical handling
from sklearn.pipeline import Pipeline # Useful for streamlined preprocessing

# Suppress warnings
import warnings
warnings.filterwarnings('ignore')

# --- 1. Load and Preprocess the Dataset ---
print(""--- 1. Loading and Preprocessing Data ---"")
try:
    df = pd.read_csv('video_games_sales.csv')
except FileNotFoundError:
    print(""Error: video_games_sales.csv not found. Please ensure the file is in the current directory."")
    exit()

# Initial data cleaning and handling missing values
df.dropna(subset=['Name', 'Genre', 'Platform', 'Publisher'], inplace=True)

# Correct handling of Year_of_Release
df['Year_of_Release'] = pd.to_numeric(df['Year_of_Release'], errors='coerce')
df.dropna(subset=['Year_of_Release'], inplace=True) # Drop if Year still missing after coerce
df['Year_of_Release'] = df['Year_of_Release'].astype(int)

# Use SimpleImputer for various numerical columns
# Ensure these columns exist before imputation
num_cols_to_impute = ['Critic_Score', 'User_Score', 'Critic_Count', 'User_Count']
for col in num_cols_to_impute:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce') # Ensure numeric
        df[col] = SimpleImputer(strategy='median').fit_transform(df[[col]]) # Impute column by column
    else:
        print(f""Warning: Column '{col}' not found for imputation."")

# Fill 'Rating' missing values
df['Rating'] = df['Rating'].fillna('Unknown')

# Fill missing sales values with 0, assuming NaN means no sales in that region
sales_cols = ['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales']
for col in sales_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0) # Ensure numeric, then fill NaN

# Filter out rows where Global_Sales is 0 after imputation, to avoid division by zero for distribution
df = df[df['Global_Sales'] > 0].copy() # Ensure we are operating on a copy to avoid SettingWithCopyWarning

# Create new features below
# 'Regional_Sales_Distribution'
# Handle cases where Global_Sales is 0 to avoid division by zero
df['NA_Sales_Prop'] = df.apply(lambda row: row['NA_Sales'] / row['Global_Sales'] if row['Global_Sales'] > 0 else 0, axis=1)
df['EU_Sales_Prop'] = df.apply(lambda row: row['EU_Sales'] / row['Global_Sales'] if row['Global_Sales'] > 0 else 0, axis=1)
df['JP_Sales_Prop'] = df.apply(lambda row: row['JP_Sales'] / row['Global_Sales'] if row['Global_Sales'] > 0 else 0, axis=1)
df['Other_Sales_Prop'] = df.apply(lambda row: row['Other_Sales'] / row['Global_Sales'] if row['Global_Sales'] > 0 else 0, axis=1)

# 'Market_Position' (rank within genre by Global_Sales)
df['Market_Position'] = df.groupby('Genre')['Global_Sales'].rank(method='dense', ascending=False)

# 'Publisher_Portfolio_Size'
publisher_portfolio = df.groupby('Publisher')['Name'].nunique().reset_index()
publisher_portfolio.rename(columns={'Name': 'Publisher_Portfolio_Size'}, inplace=True)
df = pd.merge(df, publisher_portfolio, on='Publisher', how='left')

# Drop unused columns 'Developer', 'Platform_Encoded' etc. to avoid issues
df_original = df.copy() # Keep a copy of the original (cleaned) dataframe for profiling

# Select features for clustering
numerical_features = [
    'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales',
    'NA_Sales_Prop', 'EU_Sales_Prop', 'JP_Sales_Prop', 'Other_Sales_Prop',
    'Market_Position', 'Publisher_Portfolio_Size', 'Year_of_Release',
    'Critic_Score', 'User_Score', 'Critic_Count', 'User_Count'
]
categorical_features = ['Platform', 'Genre', 'Publisher', 'Rating'] # Include all categorical

numerical_cols = [col for col in numerical_features if col in df.columns]
categorical_cols = [col for col in categorical_features if col in df.columns]

# Create a preprocessor using ColumnTransformer for proper scaling and OHE
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)
    ],
    remainder='drop' # Drop columns not specified
)

X_processed = preprocessor.fit_transform(df)

# Get feature names after one-hot encoding
ohe_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)
all_feature_names = numerical_cols + list(ohe_feature_names)

X_processed_df = pd.DataFrame(X_processed, columns=all_feature_names, index=df.index)
print(f""Processed data shape: {X_processed_df.shape}"")
X_scaled = X_processed_df # rename to X_scaled for consistency with generated code

# --- 2. Implement Multiple Clustering Algorithms & 4. Optimal Cluster Determination ---
# Determine optimal number of clusters (k) for K-Means, GMM, Hierarchical
# Using Elbow Method and Silhouette Score

max_clusters = 10 # Define a reasonable maximum number of clusters to test
inertias = []
silhouette_scores_kmeans = []
silhouette_scores_gmm = []
silhouette_scores_agg = []
k_range = range(2, max_clusters + 1)

print(""\n--- 4. Determining Optimal Number of Clusters ---"")
print(""Running Elbow Method and Silhouette Analysis..."")

for k in k_range:
    # K-Means
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans_labels = kmeans.fit_predict(X_scaled)
    inertias.append(kmeans.inertia_)
    silhouette_scores_kmeans.append(silhouette_score(X_scaled, kmeans_labels))

    # Gaussian Mixture Models
    gmm = GaussianMixture(n_components=k, random_state=42)
    gmm_labels = gmm.fit_predict(X_scaled)
    silhouette_scores_gmm.append(silhouette_score(X_scaled, gmm_labels))

    # Hierarchical Clustering (Agglomerative)
    # Ensure n_clusters is not greater than the number of samples
    n_clusters_agg = min(k, X_scaled.shape[0] - 1 if X_scaled.shape[0] > 1 else 1) # Ensure at least 1 remaining point
    agg = AgglomerativeClustering(n_clusters=n_clusters_agg)
    agg_labels = agg.fit_predict(X_scaled)
    silhouette_scores_agg.append(silhouette_score(X_scaled, agg_labels))

# Plot Elbow Method
plt.figure(figsize=(10, 6))
plt.plot(k_range, inertias, marker='o')
plt.title('Elbow Method for K-Means')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (WCSS)')
plt.grid(True)
plt.show()

# Optimal k based on highest silhouette score
optimal_k_kmeans_sil = k_range[np.argmax(silhouette_scores_kmeans)]
optimal_k_gmm_sil = k_range[np.argmax(silhouette_scores_gmm)]
optimal_k_agg_sil = k_range[np.argmax(silhouette_scores_agg)]

# Plot Silhouette Scores
plt.figure(figsize=(12, 7))
plt.plot(k_range, silhouette_scores_kmeans, marker='o', label='K-Means Silhouette')
plt.plot(k_range, silhouette_scores_gmm, marker='o', label='GMM Silhouette')
plt.plot(k_range, silhouette_scores_agg, marker='o', label='Hierarchical Silhouette')
plt.title('Silhouette Scores for Different Clustering Algorithms')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.legend()
plt.grid(True)
plt.show()

print(f""Optimal k (K-Means Silhouette): {optimal_k_kmeans_sil}"")
print(f""Optimal k (GMM Silhouette): {optimal_k_gmm_sil}"")
print(f""Optimal k (Hierarchical Silhouette): {optimal_k_agg_sil}"")

# The generated code chooses n_clusters = 4. Let's use that as the primary k for consistency with the generated code.
n_clusters = 4 # Use the hardcoded value from the generated code for direct comparison logic

# Apply models
results = {}
df_with_clusters = df_original.copy() # Use the original dataframe to add cluster labels

# K-Means
kmeans_model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
df_with_clusters['KMeans_Cluster'] = kmeans_model.fit_predict(X_scaled)
results['KMeans'] = df_with_clusters['KMeans_Cluster'].values

# Gaussian Mixture Models
gmm_model = GaussianMixture(n_components=n_clusters, random_state=42)
df_with_clusters['GMM_Cluster'] = gmm_model.fit_predict(X_scaled)
results['GMM'] = df_with_clusters['GMM_Cluster'].values

# Hierarchical Clustering
agg_model = AgglomerativeClustering(n_clusters=n_clusters)
df_with_clusters['Agglomerative_Cluster'] = agg_model.fit_predict(X_scaled)
results['Agglomerative'] = df_with_clusters['Agglomerative_Cluster'].values

# DBSCAN - Needs proper parameter estimation
print(""\nAttempting DBSCAN (parameter tuning is critical and often iterative)..."")
try:
    # Use NearestNeighbors to estimate eps for DBSCAN
    neighbors = NearestNeighbors(n_neighbors=2 * X_scaled.shape[1]) # k = 2 * num_features
    neighbors_fit = neighbors.fit(X_scaled)
    distances, _ = neighbors_fit.kneighbors(X_scaled)
    distances = np.sort(distances[:, -1], axis=0) # Sort distances to the k-th nearest neighbor

    # Plot k-distance graph to visually identify elbow
    plt.figure(figsize=(10, 6))
    plt.plot(np.arange(len(distances)), distances)
    plt.title('K-distance Graph for DBSCAN (Heuristic for Eps)')
    plt.xlabel('Points sorted by distance')
    plt.ylabel(f'{2 * X_scaled.shape[1]}-th Nearest Neighbor Distance')
    plt.grid(True)
    plt.show()

    # Heuristic for eps: find the ""elbow""
    # For robust production use, this often involves manual inspection or more advanced methods.
    # Here, we can pick a heuristic point or a percentile, for example
    # dbscan_eps = distances[np.argmin(np.diff(distances, 2))]
    dbscan_eps = np.percentile(distances, 1) # Example: use 1st percentile as a starting point
    dbscan_min_samples = 2 * X_scaled.shape[1] # A common heuristic for min_samples

    print(f""DBSCAN parameters: eps={dbscan_eps:.4f}, min_samples={dbscan_min_samples}"")

    dbscan_model = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples)
    df_with_clusters['DBSCAN_Cluster'] = dbscan_model.fit_predict(X_scaled)
    results['DBSCAN'] = df_with_clusters['DBSCAN_Cluster'].values
    print(f""DBSCAN clustering completed. Number of clusters found: {len(np.unique(df_with_clusters['DBSCAN_Cluster'])) - (1 if -1 in df_with_clusters['DBSCAN_Cluster'].unique() else 0)}"")
    print(f""Number of noise points (-1 label): {np.sum(df_with_clusters['DBSCAN_Cluster'] == -1)}"")
except Exception as e:
    print(f""DBSCAN failed or encountered an issue: {e}. Skipping DBSCAN for now and setting a default column."")
    df_with_clusters['DBSCAN_Cluster'] = -2 # Indicate it was skipped, so it's not used in consensus

# --- 3. Dimensionality Reduction and Visualization ---
print(""\n--- 3. Dimensionality Reduction and Visualization ---"")

# t-SNE
print(""Applying t-SNE for dimensionality reduction..."")
tsne_model = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
tsne_results = tsne_model.fit_transform(X_scaled)
df_with_clusters['TSNE_1'] = tsne_results[:, 0]
df_with_clusters['TSNE_2'] = tsne_results[:, 1]

# UMAP
print(""Applying UMAP for dimensionality reduction..."")
umap_reducer = umap.UMAP(n_components=2, random_state=42)
umap_results = umap_reducer.fit_transform(X_scaled)
df_with_clusters['UMAP_1'] = umap_results[:, 0]
df_with_clusters['UMAP_2'] = umap_results[:, 1]

# Visualize clusters in reduced dimensions
cluster_cols_for_viz = ['KMeans_Cluster', 'GMM_Cluster', 'Agglomerative_Cluster']
if 'DBSCAN_Cluster' in df_with_clusters.columns and not (df_with_clusters['DBSCAN_Cluster'] == -2).all(): # Check if DBSCAN was actually run
    cluster_cols_for_viz.append('DBSCAN_Cluster')


# Plotting for visualization
fig, axes = plt.subplots(len(cluster_cols_for_viz), 2, figsize=(15, 5 * len(cluster_cols_for_viz)))
if len(cluster_cols_for_viz) == 1: # Handle case with a single row of plots
    axes = np.expand_dims(axes, axis=0) # Make it 2D array of axes

for i, cluster_type in enumerate(cluster_cols_for_viz):
    # t-SNE plot
    sns.scatterplot(x='TSNE_1', y='TSNE_2', hue=cluster_type, data=df_with_clusters, palette='viridis', s=20, alpha=0.7, ax=axes[i, 0])
    axes[i, 0].set_title(f't-SNE Visualization of {cluster_type}')
    axes[i, 0].set_xlabel('TSNE_1')
    axes[i, 0].set_ylabel('TSNE_2')
    axes[i, 0].legend(title='Cluster', loc='upper left', bbox_to_anchor=(1, 1))

    # UMAP plot
    sns.scatterplot(x='UMAP_1', y='UMAP_2', hue=cluster_type, data=df_with_clusters, palette='viridis', s=20, alpha=0.7, ax=axes[i, 1])
    axes[i, 1].set_title(f'UMAP Visualization of {cluster_type}')
    axes[i, 1].set_xlabel('UMAP_1')
    axes[i, 1].set_ylabel('UMAP_2')
    axes[i, 1].legend(title='Cluster', loc='upper left', bbox_to_anchor=(1, 1))

plt.tight_layout()
plt.show()


# --- 5. Perform Consensus Clustering ---
print(""\n--- 5. Performing Consensus Clustering ---"")
# Use the results dictionary, which stores actual cluster labels
active_cluster_labels = {}
for algo_name, labels_arr in results.items():
    if algo_name == 'DBSCAN' and (labels_arr == -2).all(): # If DBSCAN was skipped
        continue
    active_cluster_labels[algo_name] = labels_arr

num_algorithms = len(active_cluster_labels)
n_samples = X_scaled.shape[0]

if num_algorithms > 1:
    co_occurrence_matrix = np.zeros((n_samples, n_samples))
    for i in range(n_samples):
        for j in range(i + 1, n_samples):
            match_count = 0
            for algo_name, labels in active_cluster_labels.items():
                if algo_name == 'DBSCAN' and (labels[i] == -1 or labels[j] == -1):
                    # Do not count noise points as matched or mismatched
                    continue
                if labels[i] == labels[j]:
                    match_count += 1
            co_occurrence_matrix[i, j] = match_count
            co_occurrence_matrix[j, i] = match_count # Symmetric

    # Normalize by the number of algorithms
    co_occurrence_matrix /= num_algorithms
    np.fill_diagonal(co_occurrence_matrix, 1.0) # A point is always in the same cluster as itself

    # Convert similarity matrix to distance matrix
    distance_matrix = 1 - co_occurrence_matrix

    # Apply Agglomerative Clustering on the distance matrix
    # Using 'precomputed' affinity
    consensus_agg_model = AgglomerativeClustering(n_clusters=n_clusters, linkage='average', affinity='precomputed')
    df_with_clusters['Consensus_Cluster'] = consensus_agg_model.fit_predict(distance_matrix)
    print(f""Consensus clustering completed with {n_clusters} clusters."")
else:
    print(""Not enough clustering algorithms ran successfully for consensus clustering. Defaulting to KMeans as consensus."")
    df_with_clusters['Consensus_Cluster'] = df_with_clusters['KMeans_Cluster'] # Fallback

# --- 6. Create Detailed Cluster Profiles with Statistical Significance Testing ---
print(""\n--- 6. Creating Detailed Cluster Profiles ---"")

# Define features for profiling (original, unscaled features)
profile_numerical_features = numerical_features # Re-use the list of selected numerical features
profile_categorical_features = categorical_features # Re-use the list of selected categorical features

# Profile each cluster
cluster_groups = df_with_clusters.groupby('Consensus_Cluster')

print(""\n--- Cluster Profiles (Mean/Mode) ---"")
for cluster_id in sorted(df_with_clusters['Consensus_Cluster'].unique()):
    print(f""\n--- Cluster {cluster_id} ---"")
    cluster_data = cluster_groups.get_group(cluster_id)

    print(""Numerical Features (Mean):"")
    for col in profile_numerical_features:
        if col in cluster_data.columns:
            print(f""  {col}: {cluster_data[col].mean():.2f}"")

    print(""\nCategorical Features (Top 3 Modes):"")
    for col in profile_categorical_features:
        if col in cluster_data.columns:
            top_modes = cluster_data[col].value_counts(normalize=True).head(3)
            print(f""  {col}:"")
            for val, prop in top_modes.items():
                print(f""    - {val} ({prop:.2%})"")

# Statistical Significance Testing
print(""\n--- Statistical Significance Testing (Consensus Clusters vs. Overall) ---"")

# Numerical features (ANOVA)
print(""\nANOVA for Numerical Features:"")
for col in profile_numerical_features:
    if col in df_with_clusters.columns:
        # Create a list of arrays, one for each cluster's data for the current column
        cluster_data_for_anova = [cluster_groups.get_group(c_id)[col].dropna() for c_id in sorted(df_with_clusters['Consensus_Cluster'].unique())]
        # Filter out empty arrays or arrays with single element
        cluster_data_for_anova = [arr for arr in cluster_data_for_anova if len(arr) > 1]
        if len(cluster_data_for_anova) > 1 : # Need at least two groups to compare
            f_stat, p_val = f_oneway(*cluster_data_for_anova)
            print(f""  {col}: F-statistic={f_stat:.2f}, p-value={p_val:.3f} {'(Significant)' if p_val < 0.05 else '(Not Significant)'}"")
        else:
            print(f""  {col}: Not enough data in all clusters for ANOVA."")

# Categorical features (Chi-squared)
print(""\nChi-squared Test for Categorical Features:"")
from scipy.stats import chi2_contingency
for col in profile_categorical_features:
    if col in df_with_clusters.columns:
        # Create a contingency table
        contingency_table = pd.crosstab(df_with_clusters['Consensus_Cluster'], df_with_clusters[col])
        # Ensure that the table has at least 2 rows and 2 columns for chi2_contingency
        if contingency_table.shape[0] > 1 and contingency_table.shape[1] > 1:
            chi2, p_val, _, _ = chi2_contingency(contingency_table)
            print(f""  {col}: Chi2-statistic={chi2:.2f}, p-value={p_val:.3f} {'(Significant)' if p_val < 0.05 else '(Not Significant)'}"")
        else:
            print(f""  {col}: Not enough variation for Chi-squared test using current clusters and categorical feature."")


# --- 7. Implement Anomaly Detection ---
print(""\n--- 7. Implementing Anomaly Detection ---"")
iso = IsolationForest(random_state=42, contamination=0.01) # contamination is the expected proportion of outliers
df_with_clusters['Anomaly'] = iso.fit_predict(X_scaled)
anomalies = (df_with_clusters['Anomaly'] == -1).sum()
print(f""Number of detected anomalies: {anomalies}"")

print(""\nTop 5 Anomalous Games (by Isolation Forest score):"")
# IsolationForest returns -1 for anomaly, 1 for inlier. Lower scores mean more anomalous.
# The 'score_samples' method gives the actual anomaly score (lower is more anomalous)
df_with_clusters['Anomaly_Score_cont'] = iso.score_samples(X_scaled)
anomalous_games_df = df_with_clusters[df_with_clusters['Anomaly'] == -1].sort_values(by='Anomaly_Score_cont')
print(anomalous_games_df[['Name', 'Platform', 'Genre', 'Global_Sales', 'Anomaly_Score_cont']].head(5))


# --- 8. Use Association Rule Mining ---
print(""\n--- 8. Performing Association Rule Mining ---"")

# Prepare data for association rule mining
# We'll use 'Genre', 'Platform', 'Publisher', and the 'Consensus_Cluster' as items
# Convert these columns into a transactional format (one-hot encoded for mlxtend)
arm_features = ['Genre', 'Platform', 'Publisher'] # Use the original categorical data
# Add Consensus_Cluster as a feature item
df_arm = df_with_clusters.copy()
df_arm['Consensus_Cluster_Str'] = df_arm['Consensus_Cluster'].astype(str).apply(lambda x: f'Cluster_{x}')
arm_features.append('Consensus_Cluster_Str')

# One-hot encode selected features for ARM
# This handles the case where original 'Rating' was directly one-hot encoded earlier,
# so we need to select specific columns for ARM, not just pass through X_scaled.
arm_data_ohe = pd.get_dummies(df_arm[arm_features])

# Apply Apriori algorithm
min_support_val = 0.01 # Adjust as needed
frequent_itemsets = apriori(arm_data_ohe, min_support=min_support_val, use_colnames=True)
print(f""Found {len(frequent_itemsets)} frequent itemsets with min_support={min_support_val}"")


# Generate association rules
rules = association_rules(frequent_itemsets, metric=""lift"", min_threshold=1.0) # min_threshold for lift
rules = rules.sort_values(by=['lift', 'confidence'], ascending=[False, False])

print(""\nTop 10 Association Rules (by Lift):"")
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))

# --- 9. Build a Recommendation System based on Clustering Results ---
print(""\n--- 9. Building a Recommendation System ---"")

# Simple content-based recommendation: recommend games from the same cluster
def recommend_games_by_cluster(game_name, df_data, num_recommendations=5, cluster_col='Consensus_Cluster'):
    game_info = df_data[df_data['Name'] == game_name]
    if game_info.empty:
        print(f""Game '{game_name}' not found in the dataset."")
        return pd.DataFrame()

    game_cluster = game_info.iloc[0][cluster_col] # Take the first if multiple entries

    # Find other games in the same cluster, excluding the input game itself given its name
    recommendations = df_data[
        (df_data[cluster_col] == game_cluster) &
        (df_data['Name'] != game_name)
    ]
    # Sort by Global_Sales to recommend popular games within the cluster
    recommendations = recommendations.sort_values(by='Global_Sales', ascending=False)

    return recommendations[['Name', 'Platform', 'Genre', 'Publisher', 'Global_Sales']].head(num_recommendations)

# Example usage of the recommendation system
example_game = ""Super Mario Bros.""
print(f""\nRecommendations for '{example_game}':"")
recs = recommend_games_by_cluster(example_game, df_with_clusters)
if not recs.empty:
    print(recs)
else:
    print(""No recommendations found."")

example_game_2 = ""Grand Theft Auto V""
print(f""\nRecommendations for '{example_game_2}':"")
recs_2 = recommend_games_by_cluster(example_game_2, df_with_clusters)
if not recs_2.empty:
    print(recs_2)
else:
    print(""No recommendations found."")

# --- 10. Validate Clustering Results ---
print(""\n--- 10. Validating Clustering Results (Internal Metrics) ---"")

validation_results = {}
cluster_algorithms_to_validate = {
    'KMeans': df_with_clusters['KMeans_Cluster'],
    'GMM': df_with_clusters['GMM_Cluster'],
    'Agglomerative': df_with_clusters['Agglomerative_Cluster'],
    'Consensus': df_with_clusters['Consensus_Cluster']
}
if 'Consensus_Cluster' in df_with_clusters.columns and not (df_with_clusters['Consensus_Cluster'] == -2).all():
    cluster_algorithms_to_validate['DBSCAN'] = df_with_clusters['DBSCAN_Cluster']

for algo_name, labels in cluster_algorithms_to_validate.items():
    # Filter out noise points for DBSCAN if present, as they are not part of a cluster relevant for these metrics.
    if algo_name == 'DBSCAN':
        valid_indices = labels != -1
        X_valid = X_scaled[valid_indices]
        labels_valid = labels[valid_indices]
    else:
        X_valid = X_scaled
        labels_valid = labels

    n_clusters_found = len(np.unique(labels_valid))
    # Ensure there are at least 2 clusters and more samples than clusters for metrics to be calculable
    if n_clusters_found > 1 and len(labels_valid) > n_clusters_found:
        try:
            sil_score = silhouette_score(X_valid, labels_valid)
            db_index = davies_bouldin_score(X_valid, labels_valid)
            ch_index = calinski_harabasz_score(X_valid, labels_valid)
            validation_results[algo_name] = {
                'Silhouette Score': sil_score,
                'Davies-Bouldin Index': db_index,
                'Calinski-Harabasz Index': ch_index
            }
        except Exception as e:
            validation_results[algo_name] = f""Error calculating metrics: {e}""
    else:
        validation_results[algo_name] = ""Not enough clusters or samples (>1 sample per cluster) for validation metrics.""

for algo, metrics in validation_results.items():
    print(f""\n--- {algo} Validation ---"")
    if isinstance(metrics, dict):
        for metric_name, value in metrics.items():
            print(f""  {metric_name}: {value:.4f}"")
    else:
        print(f"" {metrics}"")

print(""\n--- Analysis Complete ---"")

```",deepseek_r1,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
import shap
import optuna
from optuna.samplers import TPESampler
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_m...",6,"The generated code attempts to address most of the requirements but falls short in several aspects, particularly in correctness, completeness, and adherence to the detailed instructions regarding nested cross-validation and comprehensive feature engineering.

**Correctness:**
- **Feature Engineering:** The ground truth significantly expands on feature engineering, handling missing values (zeros) by imputing with median and then applying PolynomialFeatures to the *imputed* data. The generated code applies `PolynomialFeatures` *before* handling zeros and does not perform any imputation, which is a critical flaw for the given dataset (diabetes.csv often has 0s that represent missing data in columns like Glucose, BloodPressure, etc.). The generated code also sets `interaction_only=True` for `PolynomialFeatures`, which is less comprehensive than the full `degree=2` in the ground truth.
- **Nested Cross-Validation:** The generated code performs `cross_val_score` with `XGBClassifier(**study.best_params)` using `outer_cv`. While this is a form of CV, it only evaluates the tuned XGBoost, not all models and ensembles within the nested CV setup as required. The ground truth explicitly uses an outer loop for unbiased evaluation of *all* models and ensembles, including the stacking and custom ensembles, and separates data appropriately within each fold. The generated code also does not store results for each model from the nested CV.
- **Model Calibration:** The `CalibratedClassifierCV` is initialized directly with `XGBClassifier()` without pre-fitting the base estimator or ensuring it's trained on the *same data* as the model being calibrated. The `cv='prefit'` method used in the ground truth is more appropriate for out-of-fold predictions or when the base estimator is already trained.
- **SHAP:** While SHAP is used, the `shap_values` calculation directly uses `X_test` from the initial `train_test_split`. In a robust nested CV setup, SHAP analysis should be done on the test fold of the *last* iteration or on a global model trained on the full dataset, consistent with the evaluation context, which is not clearly done here.
- **Dynamic Weight Assignment:** The `DynamicWeightedEnsemble` class has a potential data leakage. It fits its models and calculates weights based on `roc_auc_score(y, model.predict_proba(X)[:, 1])` where `X` and `y` are the `X_train` and `y_train` from the initial split. This means models predict on data they were trained on to get weights, leading to overly optimistic weights and violating the spirit of honest evaluation. The ground truth handles this by using a separate *inner* validation set for weight calculation.
- **Ensemble Diversity Analysis:** The ""disagreement"" calculation (using `entropy(np.mean(preds[i] != preds[j]))`) is conceptually interesting but not a standard way to measure diversity for classification ensembles (like pairwise disagreement or Q-statistic). The correlation matrix of *out-of-fold probabilities* is a much more standard and interpretable way as shown in the ground truth. The generated code calculates correlation of `preds` which are the *predicted classes*, not probabilities, reducing its utility for diversity analysis.

**Completeness:**
- **Comprehensive Feature Engineering:** Missing handling of zero values, and less comprehensive polynomial features.
- **Advanced Ensemble Methods:**
    - Only a few base estimators for VotingClassifier.
    - Bagging only for DT and SVC (not explicitly with different base estimators as broadly implied by the prompt).
    - Boosting algorithms are listed but not integrated into the main evaluation loop to compare against other models/ensembles.
- **Nested Cross-Validation:** While the structure is present, its application to *all* models and ensembles as a consistent evaluation framework is missing. Results are not collected across folds for aggregated metrics.
- **Custom Ensemble:** The dynamic weighting has significant issues as mentioned above. There's no ""model combination strategies"" beyond the basic stacking and this problematic custom ensemble.
- **CatBoost:** Not used for categorical features as instructed. The dataset is purely numerical after preprocessing (even if initially it might have columns that *could* be treated as categorical, the preprocessing transforms them into numerical values). The ground truth notes `cat_features=None` for this reason, highlighting awareness. The generated code just adds `CatBoostClassifier(silent=True)` without any specific handling of categorical features.
- **Model Calibration:** Implemented, but its integration and evaluation within the nested CV framework are missing. Only a single calibration example is shown.
- **Model Interpretation:** SHAP is used, but only the summary plot is shown. Dependence plots and force plots are missing.
- **Ensemble Diversity Analysis:** The correlation matrix is generated but for predicted classes, not probabilities, and the ""disagreement"" metric is non-standard.

**Code Quality:**
- **Imports:** Good, correctly imports necessary libraries.
- **Structure:** The code is somewhat sequential, but lacks the clear function definitions and modularity seen in the ground truth, especially for data preprocessing and the main CV loop, making it harder to read and maintain.
- **Readability:** Variable names are generally okay.
- **Assumptions:** Like the ground truth, assumes `diabetes.csv` is present.
- **Comments and Print Statements:** Lack sufficient explanations and progress indicators compared to the ground truth, which makes understanding the execution flow and performance challenging. For instance, the metrics for each model are not printed during the nested CV.

In summary, the generated code provides a skeleton for the requested tasks but often implements them partially, incorrectly, or without fully leveraging the power of techniques like nested CV or robust ensemble combinations as detailed in the ground truth. The data preprocessing is a major missed opportunity affecting downstream model performance.",deepseek_r1,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import theano.tensor as tt
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import arviz as az
import matplotlib.pyplot as plt
from statsmodels.regression.linear_model import OLS
from statsmodels.sandbox.regression.gmm import IV2SLS
from sklearn.neighbors import NearestNeighbo...",6,"The generated code attempts to address all parts of the prompt but often falls short in correctness, robustness, or full implementation as specified.

**Strengths:**
*   Correctly uses PyMC3 for Bayesian modeling.
*   Attempts hierarchical modeling, mixed-effects, Bayesian logistic regression, and survival analysis using PyMC3.
*   Includes MCMC sampling implicitly through `pm.sample()`.
*   Attempts Gaussian Process modeling.
*   Includes a basic attempt at Propensity Score Matching.
*   Attempts model comparison with WAIC and basic validation.
*   Handles missing values and scaling.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling (Pre-processing):**
    *   The generated code uses `IterativeImputer` which is a good choice. However, the ground truth explicitly states to ""Identify columns where 0 might represent missing values...replace 0 with np.nan"", which the generated code does not fully implement for these specific columns *before* imputation. It only replaces 0s in the specified columns *after* `df` is loaded. The `IterativeImputer` will then impute these NaNs. The ground truth correctly identifies domain-specific NaNs (0 values for physiological measurements) which is a crucial pre-processing step for this dataset.

2.  **Hierarchical Bayesian Model:**
    *   The ground truth provides a more robust and complete hierarchical logistic regression model, with explicit informative priors for *all* relevant features, group-specific intercepts (random effects), and a global intercept.
    *   The generated code's hierarchical model attempts random intercepts (`a[group_idx]`) but the `β` (coefficients for features) are global, not hierarchical. The prompt asks for ""hierarchical Bayesian models...to model diabetes risk with uncertainty quantification"" and ""mixed-effects models to account for individual variability and group effects"". The ground truth model achieves this by having group-specific intercepts within `pm.Model`, representing mixed effects. The generated code's `hierarchical_model` is missing a proper `β` assignment for the `tt.dot(X, β)` part – it implies `β` applies to all features but doesn't explicitly link them to feature names from `X`. This makes it harder to interpret and less specific.
    *   The `age_bins` creation in the generated code is good but later the X in the model directly uses `X.shape[1]` for `β` coefficients without explicitly naming features. This makes it challenging to apply informative priors linked to specific medical features.

3.  **Mixed-Effects Model:**
    *   The generated code tries to use `pm.GLM.from_formula`. While `GLM` can handle mixed effects, the formula `'Outcome ~ Glucose + BMI + BP + Age + (1|age_group)'` is syntactically correct for R-style formulas. However, `pm.GLM` in PyMC3 has certain limitations compared to building the model manually for complex mixed-effects structures or when finer control Bayesian priors are needed. The ground truth's hierarchical model *is* effectively a mixed-effects model by having group-level random effects (`alpha_group`). The generated code separates this into two distinct sections, but the `pm.GLM` approach might simplify the interpretation of the model. However, 'BP' is not a column name in the dataset, it should be 'BloodPressure'. This is a minor but significant bug.

4.  **Bayesian Logistic Regression with Informative Priors:**
    *   The generated code extracts an arbitrary `X` from `df_imputed.drop('Outcome', axis=1)` and standardizes it but doesn't map the column names to `X`'s indices when applying informative priors. This means `β_Glucose` will map to the first column of the `X` array, `β_BMI` to the second, etc., which is not guaranteed to be 'Glucose', 'BMI' unless `df_imputed.drop('Outcome', axis=1)` preserves order. The ground truth explicitly uses `df_scaled['Glucose']`, `df_scaled['BMI']`, etc., ensuring the link between features and their priors is correct and robust.
    *   The generated code also drops 'time' and 'event' columns created for survival analysis, which would alter `X` and affect the coefficient mapping.

5.  **Markov Chain Monte Carlo (MCMC) Sampling:**
    *   This was correctly implicitly handled by `pm.sample()` in all models.

6.  **Survival Analysis:**
    *   The generated code simulates time-to-event data with `time` and `event` columns.
    *   It then attempts to build a PyMC3 Weibull AFT model. While this is a valid Bayesian approach to survival analysis, the prompt specifically asked for ""survival analysis techniques"" and the ground truth demonstrated a Cox Proportional Hazards model using `lifelines`, which is a common and robust frequentist approach. The generated code's Weibull model is missing the `event_observed` in its likelihood which is critical for survival analysis. The Weibull likelihood in PyMC3 typically takes parameters for shape and scale; just using `pm.Weibull` and `observed=df_imputed['time']` without incorporating the 'event' (censoring information) is incorrect. The `pm.Weibull` expects parameters `alpha` (shape) and `beta` (scale), and does not directly support censored data in this simple form. `pymc3.distributions.dist_math.logcdf` or a custom likelihood incorporating censoring is usually needed. This is a critical failing.

7.  **Gaussian Processes:**
    *   The generated code tries to model `y_train` (the diabetes outcome) as a function of `X_train[:,1:2]` (which would be 'Glucose' assuming proper indexing and feature order). This is a GP *classification* rather than ""non-parametric modeling of relationships between variables"" as a regression task (e.g., how Glucose changes with Age). The ground truth correctly models a continuous variable (`Glucose`) as a function of another (`Age`) using GP regression, which is a more typical use case for demonstrating GPs for ""relationships between variables"".
    *   The `f = gp.prior('f', X=X_train[:,1:2])` implies `X` has just one feature for GP, but it should be `X_train[:, 1:2]` if `X_train` was correctly prepared, but still, the GP is being used for classification. The prompt asked for ""non-parametric modeling of relationships *between variables*"", which usually implies regression, not directly classification. The ground truth correctly implements GP regression for `Glucose` vs `Age`.

8.  **Causal Inference:**
    *   **Propensity Score Matching (PSM):** The generated code identifies treatment and calculates propensity scores. The matching step `nn.kneighbors` and `matched = control[indices.flatten()]` correctly identifies matched controls. However, it *does not* proceed to calculate the Average Treatment Effect (ATT) by comparing outcomes in the matched sample, which is the core purpose of PSM. The ground truth correctly calculates and prints the ATT.
    *   **Instrumental Variables (IV):** The generated code uses `statsmodels.sandbox.regression.gmm.IV2SLS` but sets `df_imputed[['SkinThickness']]` as the instrumental variable and `df_imputed[['BMI']]` as the endogenous variable. While `SkinThickness` *could* be an IV for `BMI` (it's related to body fat, thus BMI), it's a very weak and debatable choice for the purpose of identifying the causal effect of `BMI` on `Outcome` without further justification or validation. The ground truth explicitly simulates a *hypothetical* `Genetic_Predisposition_for_BMI` as an IV and then correctly implements the two-stage least squares (2SLS) logic (even if it's a frequentist approach in a Bayesian context, it serves as a clear demonstration of the *method*). The generated code's IV implementation is essentially a one-liner without showing the underlying 2SLS stages, and its interpretation is not provided. It also needs exogenous variables.

9.  **Bayesian Model Comparison:**
    *   The generated code correctly uses `pm.compare` with WAIC.
    *   It correctly compares three models: hierarchical, logistic, and GP. However, the GP model is misconfigured for the stated task, and the logistic model has the indexing issue for priors.

10. **Credible Intervals and Posterior Predictive Checks:**
    *   The generated code uses `az.plot_hdi` for `β` (though `β` is an array and plotting it directly requires selecting certain components, or it might plot all of them if `arviz` supports it) and `az.plot_ppc`. This is correct.

Overall, the generated code demonstrates some knowledge of PyMC3 and various statistical techniques but often lacks the specific detailed implementation required by the prompt, especially concerning the *robustness* and *correctness* of the models (e.g., proper linking of features to priors, correct survival model likelihood, the missing outcome calculation in PSM, and the specific use case for GPs) and the clear explanation of what each step achieves. The ground truth provides a much more complete and correctly implemented set of solutions for each task.",deepseek_r1,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, KBinsDiscretizer
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering, OPTICS
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import umap.umap_ as umap
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn....",6,"The generated code attempts to address all parts of the prompt but often falls short in terms of robustness, complete implementation, and adherence to specific nuances requested in the prompt.

**Strengths:**
*   **Imports:** Includes most necessary libraries.
*   **Structure:** Follows a general structure suggested by the task, with sections for different components.
*   **Basic Implementations:** Correctly applies and imports various clustering algorithms, dimensionality reduction techniques (t-SNE, UMAP, Autoencoder), and anomaly detection.
*   **Feature Engineering Attempt:** Includes `Metabolic_Syndrome` and `High_Glucose`/`High_BMI` basic features.
*   **Missing Value Handling:** Correctly identifies 0s as NaNs and uses `SimpleImputer`.

**Weaknesses and Areas for Improvement:**

1.  **Correctness & Robustness (Major Issue):**
    *   **Feature Engineering:** The ground truth clearly states ""advanced feature engineering including metabolic syndrome indicators."" The generated code creates a `Metabolic_Syndrome` binary feature based on a very strict AND condition of three thresholds, which is a simplification compared to the ground truth's more nuanced ""score"" based on summing multiple indicators, which is better.
    *   **Autoencoder:** The autoencoder used in the generated code is a simple `Sequential` model that maps `input_dim` to `2` directly, then tries to predict `X_scaled` from this 2-dim latent space. This is not a proper autoencoder setup for reconstruction (encoder -> latent -> decoder) and thus `encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[-2].output)` is incorrect. It should be `encoder = Model(inputs=input_layer, outputs=encoder_output)` defined separately within a properly constructed autoencoder. The generated autoencoder only has an encoder part and struggles to reconstruct.
    *   **DBSCAN/OPTICS Parameter Tuning:** The generated code uses hardcoded `eps` and `min_samples` for DBSCAN and fixed `min_samples` for OPTICS, ignoring the detailed parameter determination techniques (k-distance graph) specified in the ground truth. This is a critical omission for these density-based algorithms.
    *   **Consensus Clustering:** The approach to consensus clustering using `pd.get_dummies(clusterings).corr()` and then `linkage` on the correlation matrix is highly unconventional and incorrect for building a co-occurrence matrix. Consensus clustering typically involves counting how many times pairs of samples appear in the same cluster across *different* runs/algorithms, then performing hierarchical clustering on the derived similarity/distance matrix, as demonstrated in the ground truth. The current implementation won't yield meaningful consensus clusters.
    *   **Phenotype Profiling:** The code only calculates ANOVA p-values. It misses detailed descriptive statistics (mean, std, count per cluster) and specifically mentioned statistical tests like Kruskal-Wallis for continuous data or Chi-squared for categorical data across clusters. The `df['Cluster']` column is assigned `consensus_labels` but these labels are from a flawed consensus step.
    *   **Association Rule Mining:** Uses `KBinsDiscretizer` on `features` (which are scaled numerical values), but for ARM, values need to be human-interpretable (e.g., BloodPressure_High, Glucose_Q1). The current output from `KBinsDiscretizer` (ordinal, 0, 1, 2) is not ideal for `use_colnames=True` in Apriori unless a mapping is handled. The task also asked for ""patterns between patient characteristics and diabetes risk factors"" and filtering rules for 'Diabetes', which is only partially addressed in the provided rules as `df['Outcome_bin']` is not used in the ARM inputs implicitly.
    *   **Risk Stratification:** The risk stratification is based solely on `Glucose` quantiles within clusters, whereas the ground truth uses overall cluster characteristics (diabetes prevalence, avg glucose, BMI, etc.) and provides a more comprehensive clinical interpretation.
    *   **Target column:** The `features = df.drop('Outcome', axis=1)` is correct for scaling, but the engineered features (`Metabolic_Syndrome`, `High_Glucose`, `High_BMI`) are *not* automatically included in `features` or `X_scaled`. They need to be explicitly added to the feature set used for clustering and dimensionality reduction.

2.  **Completeness:**
    *   **Optimal Parameter Determination:** While K-Means optimization is attempted using silhouette score, it lacks the elbow method and gap statistic. More importantly, DBSCAN, GMM, HC, Spectral, and OPTICS lack proper parameter determination or optimization as detailed in the prompt/ground truth (e.g., BIC/AIC for GMM, k-distance plot for DBSCAN/OPTICS, silhouette for HC/Spectral). Hardcoding `n_clusters=3` for multiple algorithms is a significant omission.
    *   **Consensus Clustering Validation:** The prompt explicitly asks for ""consensus clustering to identify stable patient subgroups across different algorithms."" The generated code's implementation of consensus clustering is not robust or standard. It also lacks any visual output for consensus (e.g., dendrogram or silhouette as in GT).
    *   **Time-Series Clustering:** The generated code correctly notes the dataset is cross-sectional and lacks temporal data but gives only a very brief (and commented out) conceptual outline. The ground truth provides a more detailed discussion.
    *   **Clinical Interpretation:** The prompt specifically asks for ""clinical interpretation"" for risk stratification. The generated code provides a very brief and unhelpful comment, which is incomplete compared to the comprehensive interpretation in the ground truth.

3.  **Code Quality:**
    *   **No Plotting for Opt. Params:** Missing plots for elbow method, silhouette scores for GMM, Hierarchical, and Spectral, and k-distance plot for DBSCAN, which were explicitly mentioned or implied by ""statistical validation.""
    *   **Clarity:** Variable naming could sometimes be more descriptive (e.g., `X_ae` without comment on what `encoder` is).
    *   **Self-Correction/Defaults:** The ground truth includes robust checks (e.g., for `len(np.unique(labels)) > 1` before calculating silhouette, or fallbacks if consensus clustering fails) which are absent in the generated code.

The generated code attempts the different sections, but the implementation often relies on default parameters, simplified approaches, or incorrect algorithms for advanced steps like consensus clustering and autoencoder. This makes it less robust and less accurate per the task's requirements.",deepseek_r1,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import VotingRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.svm import SVR
from sklearn.tree import Deci...",4,"The generated code attempts to address many requirements but falls significantly short in implementation details, correctness, and completeness, especially for a data science benchmark expecting ""advanced"" techniques.

**Primary Reasons for Low Score:**

*   **Incorrect/Missing Advanced Ensemble Implementations:**
    *   **Voting Regressor (Hard/Soft):** Implemented, but the concept of ""hard"" vs ""soft"" voting for regression (simple vs. weighted average) is not explicitly demonstrated or differentiated beyond a basic `VotingRegressor` initialization. It doesn't show how to assign custom weights for ""soft"" voting.
    *   **Bagging:** Implemented with a `DecisionTreeRegressor` but lacks exploration of other base estimators as requested.
    *   **Boosting:** Implemented various boosting models, but they are just listed, not integrated into a structured ensemble approach or used in subsequent steps.
    *   **Stacking:** Only a single-level `StackingRegressor` is defined. The ""multi-level stacking"" requirement is not fulfilled. The `StackingRegressor` setup within the generated code has an immediate bug: `base_models.items()` returns (key, value) pairs, but `StackingRegressor` expects a list of (name, estimator) tuples, meaning the `base_models['rf']`, etc., are needed instead.
    *   **Custom Ensemble with Dynamic Weight Assignment:** The `DynamicWeightedEnsemble` class is fundamentally flawed. It performs a `train_test_split` *within* the `fit` method on the entire `X, y`, which is a severe form of data leakage if `X_val, y_val` is used to calculate weights. This is incorrect for a proper OOF-based weighting scheme. It also does not explicitly apply these weights to a combined prediction of *trained* models, only to OOF predictions.
    *   **Blending:** The blending implementation is also flawed. It trains base models on `X_train` then predicts on `X_blend`. This is conceptually okay, but the `base_models` dictionary is modified in place (`base_models = [model.fit(...) for model in base_models.values()]`), which changes `base_models` from a dict of *untrained* models to a list of *trained* models, making subsequent use difficult or incorrect (e.g., `base_models['xgb']` will fail). More importantly, the 'advanced techniques' section is outside a pipeline/cross-validation framework, which makes its evaluation unreliable.

*   **Nested Cross-Validation:** The code defines `outer_cv` and `inner_cv` KFold objects but *does not implement the nested CV logic itself*. It states ""Nested Cross-Validation"" as a heading but the code block underneath is empty or merely reassigns variables. This is a critical failure of a core requirement.

*   **Hyperparameter Tuning (Optuna):** The Optuna objective is defined for `XGBRegressor`, which is good. However, the `scoring` is `neg_mean_squared_error`, and `direction='maximize'` is used. For `neg_mean_squared_error`, maximizing means trying to get closer to 0, which is incorrect for RMSE minimization; it should be `direction='minimize'` if RMSE is desired, or correctly interpreted if NMSE is the goal. The use of `TPESampler` is commendable, but the overall integration into the subsequent model training (e.g., using the best params) is not explicitly shown.

*   **Model Interpretation (SHAP & LIME):**
    *   **SHAP:** The SHAP explainer is created using `shap.Explainer(base_models['xgb'])`. This model (`base_models['xgb']`) was *not* trained on the full preprocessed data or explicitly integrated into a pipeline for SHAP. The line `base_models = [model.fit(X_train, y_train) for model in base_models.values()]` trains some base models, but it's not clear which `base_models['xgb']` is being used for SHAP (the initial untrained one, or the one trained within the problematic blending section). The argument `X` to `shap.summary_plot` should be the *preprocessed* data, not the raw `X`. The generated `X` in `shap_values` is `shap_values = explainer(preprocessor.transform(X))`, but then `shap.summary_plot(shap_values, X)` passes the *raw* `X` to the `features` argument, which is wrong.
    *   **LIME:** The LIME explanation is linked to `base_models['xgb'].predict`, suffering from the same issue as SHAP regarding which model is being interpreted. The `get_feature_names_out()` is the correct approach for `ColumnTransformer`, but `explain_instance` requires the *raw* instance, while `predict` needs the preprocessed data, leading to a mismatch if `X.iloc[0]` is raw and `base_models['xgb']` expects preprocessed. The `verbose=True` for `LimeTabularExplainer` is not a common parameter and might cause an error.

*   **Model Calibration and Uncertainty Quantification:**
    *   Model calibration is completely missed.
    *   Uncertainty quantification (`std_pred = np.std(preds, axis=0)`) is a very superficial attempt. It simply calculates the standard deviation of predictions from *different* base models, which is only a proxy for uncertainty and not a formal uncertainty quantification method (e.g., prediction intervals, conformal prediction, Bayesian methods like dropout in neural networks, or quantile regression).

*   **Ensemble Diversity Analysis/Combination Optimization:**
    *   **Diversity Analysis:** `corr_matrix = np.corrcoef(preds)` where `preds` comes from *different* base models predicting on `X_processed`. This is a valid approach for diversity, but the interpretation `diversity = 1 - np.mean(np.abs(corr_matrix))` is overly simplistic and non-standard. More formal diversity metrics or visual analysis (like heatmap) would be expected.
    *   **Combination Optimization:** Completely missing. The ground truth provides detailed `scipy.optimize.minimize` usage for this.

*   **Code Quality & Structure:**
    *   Lack of proper comments explaining complex sections.
    *   Many parts are directly calling `.fit()` without pipelines, making the preprocessing inconsistent for different models. The overall structure is fragmented.
    *   The use of `X_processed` in blending, SHAP, and uncertainty quantification is inconsistent and can lead to errors because the preprocessing step sometimes applies to the entire `X` and sometimes it's expected to be part of a pipeline.
    *   Hardcoding specific feature names for `numerical_features` and `categorical_features` might break if the dataset changes.

The generated code attempts to touch upon many facets of the prompt but fails to deliver robust, correctly implemented, and integrated solutions for the ""advanced"" aspects requested in the prompt. Many sections are placeholders or contain critical logical errors.",deepseek_r1,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import semopy
import arviz as az
from semopy import Model
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import StandardScaler
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from statsmodels.stats.mediation import Mediation
from statsmodels.formula.api import ols
from statsmodels.api i...",4,"The generated code attempts to address all parts of the task but falls short on correctness, comprehensiveness, and the quality of implementation of advanced statistical methods.

**Primary reasons for low score:**

*   **Missing Value Handling:** The generated code uses `IterativeImputer` but then immediately overwrites `df[numeric_cols]` with scaled values without handling categorical columns or the target variable. The ground truth uses `ColumnTransformer` and `Pipeline` for a more robust and comprehensive preprocessing strategy, including handling missing values in categorical columns and the target variable.
*   **Hierarchical Bayesian Model:**
    *   It uses `sigma=10` or `sigma=1` for priors, which are very wide and less informative than the ground truth's choices, which are guided by the task's requirement for ""informative priors based on happiness research literature.""
    *   The `mu` calculation for happiness in the hierarchical model only includes GDP and Social Support, missing other predictors specified in the task (Healthy life expectancy, Freedom, Generosity, Corruption), making it incomplete.
    *   Priors like `mu=0.7, sigma=0.2` and `mu=0.5, sigma=0.1` are applied to raw (though scaled) data. The ground truth meticulously scales all numerical features *before* modeling and uses priors appropriate for standardized data, which is crucial for interpretability and model stability.
*   **Structural Equation Models (SEM):** The generated code uses `semopy`, which is a good choice for SEM. However, the task specifically asked for SEM ""using PyMC3 or PyMC4"" to analyze causal pathways *between* economic, social, and psychological factors. The ground truth demonstrates how to approximate SEM using interconnected Bayesian regressions in PyMC3, adhering to the PyMC3/4 requirement. The `semopy` implementation is outside the requested framework for SEM.
*   **Causal Inference:**
    *   **Instrumental Variables:** The IV implementation is critically flawed. `statsmodels.api.IV2SLS` expects `exog`, `endog`, and `instrument` as arguments for its 2SLS formulation. The generated code passes `df['Ladder score']` (outcome), `df['Logged GDP per capita']` (endogenous regressor), and `df[['Healthy life expectancy']]` (instrument). `Healthy life expectancy` is a *predictor* in the task description, not a valid instrument. A true IV should affect the endogenous variable but not the outcome directly. The ground truth correctly identifies the need for *simulating* a plausible instrument due to the cross-sectional nature of the data and provides a more rigorous 2SLS implementation with explicit stages.
    *   **Propensity Score Matching (PSM):** The generated code uses `causalinference.CausalModel`, which is acceptable, but the ground truth provides a more transparent and comprehensive PSM implementation, including explicitly calculating propensity scores, matching, and performing a t-test on the matched samples for significance. The generated code implicitly handles matching but doesn't show details or statistical tests after matching.
*   **Gaussian Processes:** The generated code uses `sklearn.gaussian_process.GaussianProcessRegressor`, which is not Bayesian. The task explicitly requests PyMC3/PyMC4 for Bayesian methods. The ground truth correctly implements GP in PyMC3, which is a key requirement for the ""Bayesian modeling"" aspect.
*   **Mediation Analysis:** The generated code uses `statsmodels.stats.mediation.Mediation`, which is a frequentist approach. The task's request for mediation analysis implies a Bayesian approach given the overall context of PyMC3/4. The ground truth performs Bayesian mediation analysis by using the posteriors from the approximated SEM models.
*   **Difference-in-Differences:** The simulation of `year` and `treated` is arbitrary and not connected to any inherent data properties. While simulation is necessary for a cross-sectional dataset, the ground truth's simulation is more structured and explains the rationale (e.g., top GDP countries as treated).
*   **Posterior Predictive Checks and Credible Intervals:** While implemented (PPC is plotted, and forest plot is mentioned), the depth of analysis and sensitivity testing is less than the ground truth.
*   **General Code Quality:**
    *   Lack of comments and print statements to explain steps, as seen in the ground truth.
    *   No explicit handling of warnings or setting random seeds for all models, which the ground truth does for reproducibility.
    *   The import of `pymc4` is not used.
    *   `pm.set_data_buffer_size` is not used which could be an issue for larger models/datasets.

In summary, while the generated code tries to touch upon each requirement, its implementations are often either incorrect, non-Bayesian where Bayesian is required, or significantly less comprehensive and robust compared to the ground truth. The fundamental misunderstanding of core requirements like ""PyMC3/4 for SEM"" and ""Bayesian GP"" and the incorrect IV implementation are major drawbacks.",deepseek_r1,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE, Isomap
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, pairwise_distances
from sklearn.neighbors import NearestNeighbors
from sklea...",6,"The generated code attempts to address all aspects of the prompt but lacks the depth and correctness required for an ""expert"" level analysis.

**Correctness (3/5):**
*   **Preprocessing:** The original `df.dropna()` removes rows without imputation, which is a less robust approach than the ground truth's median/mode imputation. The simple `np.hstack([X, scaler.fit_transform(df[['Regional Happiness']])])` for adding engineered features is not ideal, as `Regional Happiness` is calculated after dropping NaNs and before scaling, leading to potential inconsistencies. The `Regional Happiness` feature engineering is quite basic compared to the ground truth.
*   **Clustering:** The choice of fixed `n_clusters=5` for most algorithms, without any pre-analysis (like the provided silhouette/elbow methods), is a major flaw. DBSCAN and HDBSCAN parameters are also fixed without tuning, and the `HDBSCAN` implementation doesn't use `prediction_data=True` which can be useful for later tasks. The `AgglomerativeClustering` and `SpectralClustering` in the generated code do not use the `_find_optimal_k_silhouette` function from the ground truth.
*   **Dimensionality Reduction:** While implementations are there, the use of PCA (not explicitly requested but useful) is fine, but the autoencoder's `activation='linear'` for the final decoder layer when `X` is not scaled to [0,1] might be problematic depending on the data distribution. The ground truth scales X to [0,1] for `sigmoid` output.
*   **Parameter Optimization:** Only a basic silhouette loop for KMeans and a plot for DBSCAN `eps` are shown, which barely scratches the surface of ""optimal clustering parameters using silhouette analysis, gap statistic, elbow method, and stability analysis."" The gap statistic and stability analysis are completely missing.
*   **Consensus Clustering:** The `consensus_matrix` calculation is correct but its subsequent use with `AgglomerativeClustering` directly on `consensus_matrix` (which represents similarity, not distance) without converting it to a distance matrix (e.g., `1 - consensus_matrix`) is incorrect for clustering.
*   **Country Profiles:** `f_oneway` is applied to individual features of a *single* cluster, which makes no statistical sense and will always return a p_value of NaN or 1 (cannot compare a group to itself). ANOVA should compare feature means *across* clusters, as done in the ground truth. Effect size analysis is completely missing.
*   **Anomaly Detection:** Implementation is correct, but only Isolation Forest is shown.
*   **Network Analysis:** The `community.community_louvain` is good, but the visualization simply draws the graph, without specific emphasis on centrality measures or interpretation as requested.
*   **Association Rule Mining:** Discretization uses `pd.qcut` which is good, but the `df_discretized` is based on `X` (the scaled version), which makes the 'low','med','high' labels less intuitive compared to original feature values, and the columns remain named `0, 1, 2...` instead of original feature names before passing to apriori (the `columns=features` applies to `pd.DataFrame(X, columns=features)` but `pd.qcut` output renames columns to `0,1,2...`).
*   **Recommendation System and Policy Insights:** The recommendation system is basic (k-neighbors on raw data) and lacks the sophistication of looking at clustered data or geopolitical validation. Policy insights are mostly qualitative.

**Code Quality (3/5):**
*   **No Class Structure:** The biggest departure from the prompt's implied structure and the ground truth is the complete lack of a class (`AdvancedHappinessAnalysis`). All functions are global, leading to a sprawling script with no clear organization or modularity. This makes it hard to maintain, extend, and understand the flow.
*   **Missing Imports:** `Scipy.stats.mahalanobis`, `statsmodels.stats.multicomp.pairwise_tukeyhsd` and `sklearn.neural_network.MLPRegressor` are imported but not used, suggesting leftover code or incomplete thoughts. `community` (for Louvain algorithm) requires `python-louvain` library, which is not a standard networkx sub-module.
*   **Redundant Code:** The `_find_optimal_k_silhouette` helper and associated plotting are completely missing from the generated code, meaning the basic plotting of silhouette/elbow for K-Means is done manually and only for K-Means.
*   **Lack of Comments/Docstrings:** The code has minimal comments, which is critical for complex scripts.
*   **Variable Scope and Reusability:** Because there's no class, intermediate results (like `df_processed`, `clustering_results`, `dr_embeddings`) are not stored in a structured way for easy access by subsequent steps, often leading to recalculations or passing global variables.

**Completeness (2/5):**
*   **Feature Engineering:** ""Advanced feature engineering including happiness profiles and regional characteristics"" is partially met with `Regional Happiness` but misses the interaction terms and comprehensive regional dummy variable handling from the ground truth.
*   **Clustering Optimal Parameters:** Largely incomplete. Only basic silhouette for KMeans and a single plot for DBSCAN `eps` are done. Gap statistic and stability analysis are completely absent both in implementation and conceptual discussion.
*   **Consensus Clustering:** The implementation is flawed, as noted above.
*   **Detailed Country Profiles:** Fails significantly on statistical significance testing (incorrect application of ANOVA) and completely misses effect size analysis.
*   **Policy Insights:** Lacks the depth and ""geopolitical validation"" aspect, which was outlined as a crucial part of the request.
*   **General:** Many aspects are implemented at a surface level, without the depth or complexity described in the prompt or present in the ground truth. For instance, the prompt asks for ""advanced dimensionality reduction techniques,"" and while t-SNE, UMAP, and autoencoders are included, the visualizations are not as comprehensive as in the ground truth (e.g., integrating cluster labels onto DR plots). The autoencoder training `verbose=0` is good, but many other sections print no output.

In summary, the generated code provides a functional but rudimentary implementation of some requested tasks. It significantly lacks the robust statistical methods, proper code structure, and detailed analysis that were requested. The errors in statistical hypothesis testing and consensus clustering are significant.",deepseek_r1,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
from pmdarima import auto_arima
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Load and preprocess data
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
df = df[df'] =='] == 'Germany'].set_index('date')

# Handle missing values
df[['new_cases', 'stringency_index', 'people_fully_vaccinated']] = df[['new_cases', 'stringency_index', 'people_fully_vaccinated']]....",6,"The generated code provides a reasonable attempt at solving the time series forecasting problem using SARIMA. It correctly implements the core steps: data loading, filtering, splitting, SARIMA order determination with `auto_arima`, model training, forecasting, and RMSE evaluation, followed by plotting.

However, there are several areas where it falls short compared to the ground truth and the specified requirements, leading to a score of 6:

1.  **Correctness - Data Filtering Issue:** There's a syntax error in the data filtering line: `df = df[df'] =='] == 'Germany'].set_index('date')`. This would prevent the code from running as is. It should likely be `df = df[df['location'] == 'Germany'].set_index('date')`. This is a critical bug.
2.  **Completeness - Missing Value Handling:** While it uses `ffill` for missing values, it's less robust than the ground truth's approach (ffill then bfill, with interpolation as a fallback). The ground truth also explicitly checks for remaining NaNs, which is good practice. The generated code only applies `ffill` once. For time series, `bfill` is often necessary if initial entries are NaN.
3.  **Completeness - Exogenous Variable Selection:** The generated code implicitly assumes that `stringency_index` and `people_fully_vaccinated` are the only columns needed besides `new_cases` after filtering and setting the index. While this might often be the case in this specific example, the ground truth explicitly selects these columns from the filtered DataFrame (`data = germany_df[[target_variable] + exog_variables]`), making it more robust and explicit.
4.  **Code Quality - Robustness and Error Handling:** The ground truth includes robust error handling, such as checking for `FileNotFoundError`, insufficient data for splitting (`len(data) < test_days`), and detailed checks for NaN handling. The generated code lacks these safeguards, making it less robust in real-world scenarios.
5.  **Code Quality - Clarity of Variables:** The ground truth explicitly defines `target_variable` and `exog_variables` at the beginning, making the code more readable and easier to modify. The generated code hardcodes these column names in various places.
6.  **Completeness - Plotting Detail:** The plot generated is basic. The ground truth provides a more detailed plot, showing training data, actual test data, and forecasts, along with a more informative title and axis labels, which is better for presentation and analysis. The ground truth also prevents negative forecasts, which is relevant for 'new_cases'.
7.  **Minor - Imports:** The ground truth explicitly imports `numpy` and `warnings`, both of which are used (though `numpy` is only for `np.sqrt` in RMSE, which could be avoided by just using `mean_squared_error(..., squared=False)` directly, which the generated code correctly does for RMSE). The `warnings` import and usage for suppressing output is also a good practice for cleaner output.

In summary, the generated code outlines the correct analytical steps but has a critical bug in data filtering, is less robust in data preprocessing (especially missing values), and lacks some of the robustness and clarity of the ground truth solution.",deepseek_r1,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import scikit_posthocs as sp

df = pd.read_csv('winequality-red.csv')
df['quality_group'] = pd.cut(df['quality'], bins=[-1, 4, 6, 10], labels=['low', 'medium', 'high'])
features = df.columns.drop(['quality', 'quality_group']).tolist()
significant_features = []

for feature in features:
    low = df[df['quality_group'] == 'low'][...",5,"The generated code attempts to address the requirements but falls short in several critical areas, impacting its correctness and completeness.

**Correctness Issues:**
*   **Normality Check:** The normality check is flawed. It only checks if *all* groups are normal. If even one group is not normal, it immediately defaults to Kruskal-Wallis. This isn't entirely correct, as ANOVA also requires homogeneity of variance (Levene's test), which is completely missing. A robust solution would check normality for each group and then perform Levene's test if all are normal. The current implementation might incorrectly choose Kruskal-Wallis when ANOVA with Welch's might have been appropriate if homogeneity of variance fails.
*   **Post-hoc Analysis:**
    *   **Tukey's HSD:** The `pairwise_tukeyhsd` call `tukey = pairwise_tukeyhsd(df[feature], df['quality_group'])` is problematic. `pairwise_tukeyhsd` expects the `endog` (dependent variable) and `groups` as separate 1D arrays or series, not as a DataFrame-like construct where the order of `df[feature]` and `df['quality_group']` might not match the `statsmodels` expectation, especially if there are NaNs or different indices. This usually leads to an error or incorrect results. It should be used with combined data and group labels as shown in the ground truth.
    *   **Dunn's Test:** While `scikit_posthocs` is a valid library for Dunn's test, the result `dunn = sp.posthoc_dunn(...)` is calculated but *never stored or printed*. This means the code *performs* the post-hoc test but *does not present its results*, failing a key requirement.
*   **Statistical Results Storage:** The code does not store or print the p-values or test types from the initial ANOVA/Kruskal-Wallis tests, only using `significant_features` to indicate if a feature passed the 0.05 threshold. This makes it difficult to summarize the findings as required.
*   **Quality Group Definition:** While `pd.cut` can be used, the bins `[-1, 4, 6, 10]` for quality scores create `low` (0-4), `medium` (5-6), `high` (7-10). The ground truth specifically defines high quality as `>= 7`. This is a minor difference but highlights a lack of precision in defining the bins as strictly as the prompt implies.

**Completeness Issues:**
*   **Output of Statistical Tests:** The code lacks output for the primary statistical tests (ANOVA/Kruskal-Wallis) results (F/H stats, p-values).
*   **Output of Post-hoc Tests:** Critically, the post-hoc test results (both Tukey's HSD and Dunn's) are not printed or summarized. This is a major omission as the prompt explicitly asks for post-hoc analysis.
*   **Summarization:** The final summary only lists ""Significant features"" without any details about *why* they were significant based on the tests, or *how* they differ based on post-hoc results.

**Code Quality Issues:**
*   **Error Handling:** No `try-except` block for file loading, which is crucial for robustness.
*   **Constants:** `ALPHA` (0.05) is hardcoded repeatedly rather than being defined as a constant, which is a good practice for maintainability.
*   **Readability:** The in-line `if test_type == 'ANOVA': ... else: ...` block for post-hoc tests is a good start, but the missing output makes it less useful.
*   **Plotting:** The plotting loop `plt.subplot(len(significant_features)//3 +1, 3, i)` implies some subplots, but the `plt.show()` is called only once at the very end, resulting in a single figure with all subplots. This might be desired, but if `significant_features` is empty, it will produce an empty figure. The ground truth produces individual plots for each significant feature which is often more readable, especially if there are many.

In summary, the code attempts to cover the requirements but misses crucial steps in statistical validity (normality/homogeneity checks, proper `tukeyhsd` usage) and, more importantly, fails to present the detailed results of the statistical and post-hoc analyses, which are central to the task.",deepseek_r1,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, f1_score
import xgboost as xgb
import shap

# 1. Load and prepare data
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2. Create target variable
df['high_daily_death_inc...",7,"The generated code addresses most of the requirements of the task, demonstrating a good understanding of the problem and common data science practices. However, there are several key areas where it deviates from the instructions or implements them in a suboptimal way, preventing a higher score.

**Strong Points:**
*   **Data Loading and Parsing:** Correctly loads data and parses dates.
*   **Feature Engineering:** Implements rolling averages/stds, lag features, and days since first case, and vaccination rate effectively.
*   **Missing Value Handling:** Generally handles missing values, though the approach for FE-induced NaNs could be more nuanced.
*   **Feature Selection:** Selects appropriate features as requested.
*   **Preprocessing Pipeline:** Uses `ColumnTransformer` and `Pipeline` for preprocessing and model training, which is good practice.
*   **Time-based Split:** Correctly implements a time-based split for training and testing.
*   **XGBoost and Hyperparameter Tuning:** Uses XGBoost and `GridSearchCV` for hyperparameter tuning.
*   **Evaluation:** Evaluates using ROC-AUC and F1-score.
*   **SHAP Explanation:** Implements SHAP for feature importance.

**Areas for Improvement / Deviations from instructions:**

1.  **Target Variable Definition (Critical Deviation):** The most significant issue is in defining ""high_daily_death_increase"".
    *   **Instruction:** ""A 'high_daily_death_increase' is defined as a day where 'new_deaths' for a 'location' is in the top 10% of its own historical 'new_deaths' (excluding days with zero new deaths).""
    *   **Generated Code:** `df.groupby('location')['new_deaths'].transform(lambda x: (x >= x[x > 0].quantile(0.9)).astype(int) if (x > 0).any() else 0)`
        *   This implementation **does not exclude days with zero new deaths** from the comparison. It calculates the 90th percentile *only* on `new_deaths > 0`, but then compares *all* `new_deaths` (including zeros or negatives) against that percentile. This means a day with `new_deaths = 0` could be incorrectly labeled as `high_daily_death_increase` if the 90th percentile from positive deaths is 0 (unlikely but possible with very sparse data or if only positive deaths are considered as ""top 10%"").
        *   More critically, `(x >= x[x > 0].quantile(0.9))` means that if `new_deaths` is `0` and the 90th percentile (derived from positive deaths) is also `0`, it would mark `0` as a high increase, which is incorrect.
        *   The ground truth code correctly applies `(df['new_deaths'] > df['location_90th_percentile_deaths']) & (df['new_deaths'] > 0)).astype(int)`. This ensures that only days with *positive* new deaths that *exceed* the percentile are marked as 1.

2.  **Missing Value Handling for Engineered Features:**
    *   **Instruction:** ""Handle missing values from feature engineering (e.g., fill with 0 or backfill).""
    *   **Generated Code:** `df.fillna(0, inplace=True)` at the end. This is a blanket fillna. While it might fill NaNs from rolling/lag features, it does so indiscriminately across all columns, potentially losing information for other columns where 0 might not be the best imputation (e.g., `median_age`, `population` related values, though less likely for this dataset).
    *   For time-series features like rolling means/stds and lags, a `ffill` within `groupby('location')` followed by a `fillna(0)` (or other appropriate initial value for series start) is generally preferred to preserve time-series logic and avoid data leakage from the future. The generated code's `fillna(0)` for `.std()` after rolling is a specific fix but a more general approach for all engineered features is missing.

3.  **Feature Selection:**
    *   The `first_case_dates` merge and subsequent `days_since_first_case` calculation is good, but `first_case_date` and `population` are left in the DataFrame and will be passed to `fillna(0)` which is a bit messy, although `population` is used for `vaccination_rate`.
    *   The initial `numerical_features = X.columns[:-1].tolist()` is brittle. If `continent` is not the last column in `X`, this will fail. It's better to explicitly list numerical and categorical features or derive them based on `dtype`.

4.  **SHAP Analysis (Minor Issues):**
    *   The `shap.summary_plot` is called, but `plt.show()` is missing, meaning the plot won't display if run non-interactively.
    *   `feature_names` generation: `numerical_features` from `X.columns[:-1].tolist()` might not be entirely correct depending on the order after `ColumnTransformer`. Using `get_feature_names_out()` from the numerical scaler would be more robust to ensure correct names after numerical transformation, similar to how it's done for `OneHotEncoder`. The ground truth code successfully gets accurate feature names post-transformation.

5.  **Imports and Warnings:**
    *   Missing `matplotlib.pyplot` import, necessary for displaying SHAP plots.
    *   Does not include `warnings` suppression, which is good practice for cleaner output from libraries like `sklearn` and `xgboost`.

6.  **Code Structure/Robustness:**
    *   No error handling for `pd.read_csv`, which is a minor point but good practice.
    *   Minor issues like using `iloc` on `X` and `y` directly without sorting `X` and `y` first after `df.sort_values('date', inplace=True)`. While `X` and `y` are derived from `df`, it's safer to operate on the sorted `df` to extract `X_train`, `X_test`, `y_train`, `y_test`. The current code implicitly relies on `X` and `y` maintaining the order of `df`.",deepseek_r1,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.formula.api as smf

# Load data and parse dates
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Calculate vaccination percentage
df['vaccination_percentage'] = (df['people_fully_vaccinated'] / df['population']) * 100

# Sort data by location and date for interpolation
df = df.sort_values(['location', 'date'])

# Handle missing values with group-wise interpolation then median imputation
for var in ['reprod...",6,"The generated code attempts to address all aspects of the task, but significantly deviates from the requirements in the crucial areas of missing value handling and, more importantly, the implementation of nested random effects with `statsmodels.MixedLM`.

Here's a breakdown:

**Correctness (3/10):**
*   **Missing Value Handling:** The imputation strategy is deeply flawed. `x.interpolate(limit_direction='both').fillna(x.median())` will first interpolate within groups, and then `fillna(x.median())` will fill any remaining NaNs *within that group* with that group's median. This is problematic if a group is entirely NaN or if the median is calculated *before* interpolation fills some values. Crucially, it doesn't handle `population` or `people_fully_vaccinated` missing values before calculating `vaccination_percentage`, which leads to NaNs propagating. The ground truth correctly imputed `population` and `people_fully_vaccinated` first, then calculated `vaccination_percentage`, and then interpolated/imputed the final key variables.
*   **Mixed-Effects Model (Nested Random Effects):** This is the most critical error. The prompt explicitly asks for ""random intercepts for 'location' nested within 'continent'"". The generated code uses `groups=df['continent'], vc_formula={'location': '0 + C(location)'}`.
    *   `groups=df['continent']` implies random effects at the continent level.
    *   `vc_formula={'location': '0 + C(location)'}` attempts to specify variance components for location, but when combined with `groups=df['continent']`, `statsmodels.MixedLM` will interpret this as random slopes (or effects) for location *within* each continent group. This does *not* achieve ""location nested within continent"" in the sense of random intercepts for each *unique (continent, location)* combination. Instead, it seems to be modeling the variance of location as a random factor *within* continents, which is a different model. The correct way to implement ""location nested within continent"" random intercepts in `statsmodels` (as shown in the ground truth) is to create a unique identifier for each `(continent, location)` pair and use that as the `groups` argument. The prompt asks for *random intercepts for location nested within continent*, not random slopes or a separate variance component for the location factor within the continent group.
*   **Interpretation of Random Effects:** Due to the incorrect model specification, the interpretation of `vc` components is also incorrect. It prints `location: X.XX` and `continent: Y.YY`, suggesting separate variance components, which is not what a ""location nested within continent"" random intercept model typically yields for the problem statement. The task specifically asked for random intercepts *for* location nested within continent, meaning one variance component for the `(continent, location)` mixed effect group.

**Code Quality (7/10):**
*   Imports are correct.
*   Uses `parse_dates` in `read_csv`, which is good.
*   The structure is logical (data loading, imputation, model, results).
*   Output formatting with `print` statements is clear.
*   Lacks error handling for file not found, which is a minor but good practice.
*   The `group_keys=False` is a good pandas practice, but not strictly necessary here.

**Completeness (6/10):**
*   **Missing Value Handling:** Incomplete and incorrect as described above. Doesn't handle `population` correctly before vaccination calculation. Doesn't cap `vaccination_percentage` at 100%. Doesn't handle the fallback global median imputation for remaining NaNs after groupby interpolation, which is crucial for robustness.
*   **Model Specification:** Fails to correctly implement ""location nested within continent"" random effects for random intercepts. This is a major gap.
*   **Reporting:** It reports fixed effects coefficients, SEs, and p-values. It attempts to report variance components but, due to the incorrect model, the values printed are not the single random intercept variance requested for the nested groups.
*   **Interpretation:** Provides basic interpretation of fixed effects, but due to incorrect model, the interpretation of variance components is misleading.
*   **Assumptions:** Discusses assumptions, but doesn't elaborate as much as the ground truth on how to check them.

Overall, the core problem of correctly building the specified mixed-effects model and robustly handling missing data undermines its score. While it attempts to fulfill all requirements, the implementation for two critical steps is incorrect or significantly flawed.",deepseek_r1,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

# Load dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select features
X = df[['elixirCost', 'Win Rate', 'Usage']]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Determine optimal K using silhouette scores
silhouette_scores...",7,"The generated code correctly addresses most of the task requirements: loading data, selecting and standardizing features, applying K-Means, performing PCA, and visualizing the clusters.

However, there are a few issues that prevent it from getting a higher score:
1.  **Typo in K-Means loop**: There's a significant typo in the loop for determining K: `k KMe KMeans(n_clusters=k, random_state=42)` should be `kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)`. This would cause a `SyntaxError` and prevent the code from running successfully.
2.  **`n_init` parameter for KMeans**: The `n_init` parameter is not set for `KMeans` which will lead to a FutureWarning and might not produce the best results, as `n_init` defaults to 10 in future versions.
3.  **Choosing `best_k`**: The logic `best_k = silhouette_scores.index(max(silhouette_scores)) + 2` correctly finds the index of the max score and adds 2 (since K starts from 2). This is a valid way to select K using silhouette scores.
4.  **No WCSS plot for K-selection**: The prompt explicitly mentions ""You can use the elbow method (plotting WCSS) or silhouette scores to justify your choice of K."" While using silhouette scores is fine, the ground truth explicitly uses and plots WCSS, which is a common practice for the ""elbow method."" The generated code uses silhouette scores, which is a valid alternative, but it doesn't *plot* them, which would be ideal for justification. It calculates them internally and picks the best.
5.  **Lack of error handling**: The generated code assumes the CSV file exists. The ground truth includes `try-except` blocks for `FileNotFoundError` and checks for missing columns, which improves robustness.
6.  **Minor plot details**: The plot lacks detailed labels (e.g., explained variance for PC1/PC2) and a color bar, which improves interpretability.

To improve, the generated code should fix the syntax error, add `n_init=10` to KMeans, consider plotting the silhouette scores (or WCSS if using that method) to justify K visually, and add basic file/column existence checks.",deepseek_r1,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error

df = pd.read_csv('clash_royale_cards.csv')

X = df[['elixirCost', 'Usage', 'rarity']]
y = df['Win Rate']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

preprocessor = Co...",9,"The generated code is highly correct and addresses almost all requirements of the task. It successfully loads the data, selects the correct features and target, preprocesses the `rarity` column using one-hot encoding (specifically `OneHotEncoder` with `ColumnTransformer` which is a robust and modern approach), splits the data appropriately, trains a `RandomForestRegressor`, and evaluates it using R-squared and MSE. The `random_state` is correctly used for reproducibility.

Minor points for improvement (preventing a perfect 10):
- **Missing File Handling/Error Messages:** The ground truth includes a `try-except` block for `FileNotFoundError` and checks for missing columns. The generated code assumes the CSV file exists and all columns are present, which is a common oversight in a direct code generation scenario but important for robust production-ready code.
- **No Handling of Missing Values:** Neither `X` nor `y` are explicitly checked or handled for missing values before processing. While the task didn't explicitly state to handle them, it's good practice for real-world datasets and the ground truth included a basic `dropna`.
- **Output Format:** The prompt implicitly asks for ""proper imports"" and to ""provide only the Python code without explanations,"" and the output in the generated code printing the evaluation results is `print(f""R2 Score: {r2:.4f}"")` instead of the more descriptive `print(""\n--- Model Evaluation Results ---"")` etc., as shown in the ground truth. This is a minor stylistic point, but the prompt implies adherence to the spirit of comprehensive code.

Overall, it's an excellent solution that demonstrates a strong understanding of the required ML pipeline steps.",deepseek_r1,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy.stats import f_oneway

df = pd.read_csv('clash_royale_cards.csv')

rarities = df['rarity'].unique()
groups = [df[df['rarity'] == rarity]['Win Rate'] for rarity in rarities]

f_stat, p_value = f_oneway(*groups)

print(""Null Hypothesis: The mean Win Rate is equal across all rarity categories."")
print(""Alternative Hypothesis: At least one rarity category has a different mean Win Rate."")
print(f""F-statistic: {f_stat}"")
print(f""P-value: {p_value}"")

alpha = 0....",7,"The generated code correctly addresses most of the requirements but has some limitations and areas for improvement.

**Strong Points:**
*   Correctly loads the data.
*   Correctly identifies unique rarity categories.
*   Correctly performs the ANOVA test using `scipy.stats.f_oneway`.
*   Correctly states the null and alternative hypotheses.
*   Correctly prints the F-statistic and p-value.
*   Correctly interprets the p-value against the alpha level.
*   Includes necessary imports (pandas, scipy.stats).

**Areas for Improvement:**
*   **Robustness in Data Loading/Cleaning:** The ground truth includes error handling for `FileNotFoundError` and `pd.to_numeric` with `errors='coerce'` to handle non-numeric `Win Rate` values, followed by `dropna`. The generated code assumes perfect data and file existence, which can lead to crashes if the CSV is missing or contains dirty data.
*   **Handling Empty or Insufficient Data Groups:** The generated code directly creates `groups` from `df['Win Rate']` without checking if a rarity group might be empty or have fewer than 2 data points, which can cause issues with `f_oneway` if not all groups have sufficient data. The ground truth explicitly checks for this and excludes groups with less than 2 observations, making it more robust.
*   **Print Formatting:** The F-statistic and p-value are printed without specific formatting (e.g., to a few decimal places), which can make the output less readable compared to the ground truth.
*   **Explicitness on Hypotheses:** While the hypotheses are stated, the ground truth provides a slightly more formal and detailed phrasing (""There is no significant difference..."" vs. ""The mean Win Rate is equal...""). Both are acceptable, but the ground truth is a bit more complete in its wording.
*   **Clarity in Output:** The ground truth separates different sections of the output (Hypotheses, ANOVA results, Interpretation) with headers and separators, enhancing readability.

Overall, the core logic for performing the ANOVA and interpreting the p-value is sound, but it lacks the robustness and detailed output considerations present in a more production-ready or expert-level code.",deepseek_r1,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

df = pd.read_csv('iris.csv')
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = df['Species']

le = LabelEncoder()
y = le.fit...",9,"The generated code successfully completes almost all aspects of the task, demonstrating strong correctness and good code quality.

**Strong points:**
*   **Correctness:** It correctly loads the data, defines X and y, encodes the target, splits the data, trains all three specified models with the correct parameters, calculates and prints all required metrics (accuracy, precision, recall, F1-score with macro average), and identifies the best model based on F1-score.
*   **Code Quality:** Imports are correct and organized. The structure is logical, iterating through models to train and evaluate.
*   **Completeness (mostly):** It addresses most of the requirements.

**Areas for minor improvement (why it's not a 10):**
*   **Missing `stratify` in `train_test_split`:** The ground truth includes `stratify=y_encoded` (or `stratify=y` in the generated code's context) to ensure the class distribution is maintained across training and testing sets, which is good practice for classification tasks, especially with potentially imbalanced datasets (though Iris is fairly balanced). The generated code omits `stratify`. While it might still produce acceptable results for Iris due to its balance, it's a minor deviation from best practice and the implied thoroughness of the ground truth.
*   **Logistic Regression `max_iter`:** The ground truth explicitly sets `max_iter=200` for `LogisticRegression` to prevent potential `ConvergenceWarning` messages (which often occur with default `max_iter=100` on some datasets). The generated code uses default parameters, which might lead to this warning. While not a functional error, it's a small detail that improves robustness.
*   **Output Formatting:** The output of the generated code for individual model performance is on one line. While functionally correct, the ground truth's multi-line, clearly labeled output for each metric makes it slightly more readable and user-friendly for comparison at a glance.

Overall, it's a very good solution that fulfills the core requirements effectively. The missing `stratify` is the most significant functional omission, but the others are minor refinements.",deepseek_r1,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

data = pd.read_csv('iris.csv')
features = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
clusters = kmeans.fit_predict(scaled_features)
...",9,"The generated code is largely correct and addresses most of the requirements.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly selects the specified features.
*   Correctly applies `StandardScaler`.
*   Correctly applies `KMeans` with the specified parameters (`n_clusters=3`, `n_init='auto'`, `random_state=42`).
*   Correctly applies `PCA` to 2 components.
*   Correctly generates two scatter plots using the PCA components.
*   Correctly colors the first plot by K-Means labels and the second by true species labels (after encoding).
*   Includes all necessary imports.
*   Uses `plt.subplot` to display both plots side-by-side, which is a good practice for comparison.

**Areas for improvement (leading to a score of 9 instead of 10):**
*   **Minor Title Discrepancy:** The task explicitly asks for titles ""PCA of Iris Dataset: Points Colored by K-Means Cluster Labels"" and ""PCA of Iris Dataset: Points Colored by True Species Labels"". The generated code uses ""Clusters from K-Means"" and ""Clusters by True Species"". While functional, it doesn't precisely match the requested phrasing.
*   **Colorbar Labels:** The true species plot's colorbar, while present, doesn't map the encoded numerical values back to their original species names (e.g., 'setosa', 'versicolor', 'virginica'). The ground truth code demonstrates how to achieve this using `set_ticklabels` and `inverse_transform`, which significantly improves the interpretability of the `Species` plot. Without this, the colorbar shows generic numbers (0, 1, 2), which aren't as informative as the actual species names.
*   **No `try-except` for file loading:** The ground truth includes a `try-except FileNotFoundError`, which is good practice for robustness, although not explicitly required by the problem description.

Overall, the code is highly functional and demonstrates a strong understanding of the task. The minor issues are primarily related to exact phrasing in titles and the advanced labeling of the colorbar, which makes the plot less informative compared to the ground truth.",deepseek_r1,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

df = pd.read_csv('iris.csv')
df['PetalArea'] = df['PetalLengthCm'] * df['PetalWidthCm']
df['SepalArea'] = df['SepalLengthCm'] * df['SepalWidthCm']
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'PetalArea', 'SepalArea']]
le = LabelEncoder()
y = le.fit_transform(df['Species'])
model = Gr...",9,"The generated code correctly implements all the requirements of the task:
1. It loads the dataset.
2. It creates both `PetalArea` and `SepalArea` features.
3. It correctly defines `X` to include the original four numerical features and the two new features.
4. It defines and encodes the target `y` using `LabelEncoder`.
5. It initializes and uses a `GradientBoostingClassifier` with the specified parameters.
6. It performs 5-fold cross-validation.
7. It reports the mean cross-validated accuracy score.

The code is concise and efficient. The only minor point for improvement, which doesn't detract significantly from its correctness or completeness, is the lack of print statements to explicitly show the cross-validation scores per fold or a more descriptive output for the final mean accuracy, similar to the ground truth. However, for a coding task focused on functionality, it is excellent. It also lacks the `try-except` block for file loading, but the prompt implies the file is present.",deepseek_r1,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load dataset
df = pd.read_csv('heart.csv')

# Define features and target
X = df[['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', ...",8,"The generated code correctly addresses most of the requirements. It loads the data, defines X and y, correctly identifies categorical and numerical features for preprocessing, performs one-hot encoding and scaling, splits the data, trains a RandomForestClassifier, and evaluates using the specified metrics.

Strong points:
- Correctly identifies and separates categorical and numerical features for preprocessing.
- Uses `ColumnTransformer` for preprocessing multiple column types.
- Splits data with the correct test size and random state.
- Trains `RandomForestClassifier` with specified `n_estimators` and `random_state`.
- Calculates all required evaluation metrics (accuracy, macro precision, macro recall, macro F1, ROC-AUC).

Areas for improvement to achieve a higher score:
- **Preprocessing Pipeline:** The ground truth uses a `Pipeline` which is best practice for machine learning workflows. This prevents data leakage (by ensuring `fit_transform` and `transform` are applied consistently) and makes the model deployment easier. The generated code applies `preprocessor.fit_transform(X)` before the train-test split, which is a potential data leakage issue. The preprocessor should be `fit` only on the training data. While `ColumnTransformer` does some fit-transform internally, the `fit_transform` on the entire `X` before splitting can lead to information leak from the test set into the preprocessing steps. A `Pipeline` correctly handles this by fitting the preprocessor only on `X_train`.
- **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is a good practice for robust code. The generated code omits this.
- **Clarity and Comments:** While simple, adding a few comments can improve readability, similar to the ground truth.
- **Print Statements:** The ground truth includes more verbose print statements for clarity on execution flow.

Overall, it's a very good attempt that almost perfectly meets the requirements, with the primary improvement being the implementation of the preprocessing pipeline to prevent data leakage and follow best practices.",deepseek_r1,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('heart.csv')

# Select features
features = df[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(features)

# Determine optimal K
wcss = []
sil_scores...",9,"The generated code is largely correct and addresses most of the requirements.

**Strong points:**
- Correctly loads the dataset.
- Correctly selects and standardizes the specified numerical features.
- Correctly implements the elbow method (WCSS) and calculates silhouette scores for K from 2 to 5.
- Plots both WCSS and silhouette scores as requested.
- Applies K-Means clustering with `n_init='auto'` and `random_state=42`.
- Performs PCA to reduce dimensionality to 2 components.
- Creates a scatter plot of the two principal components, colored by cluster labels, with a title.
- Imports are handled correctly.

**Areas for minor improvement:**
- **Choice of K:** The ground truth code makes a more nuanced point about visually inspecting the plots to `choose` K, often considering the elbow and a peak in silhouette. The generated code *automatically* selects `optimal_k` based *solely* on the maximum silhouette score. While often a good heuristic, the prompt asked to ""justify your choice of K"" using both, implying a more visual/manual interpretation step which the ground truth code exemplified by setting `chosen_k = 3` after the plots. For this specific task, a hardcoded choice (after observing the plots) or a print statement saying ""chosen K is X based on max silhouette"" would have been slightly closer to the spirit of ""justifying"" K, rather than just picking the maximum without explanation.
- **Plotting Aesthetics:** The PCA plot in the generated code uses default scatter plot markers and colors, which can make it slightly harder to distinguish individual clusters compared to the ground truth which explicitly iterates through clusters to assign distinct colors and potentially adds a legend. While not explicitly asked for in detail, better aesthetics make the visualization clearer.
- **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` for the CSV, which is a good practice, though not explicitly requested by the prompt.

Overall, the generated code fulfills the core requirements well, with the choice of K being the only minor deviation in interpretation from the ground truth's approach to determining the ""best"" K.",deepseek_r1,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, f1_score, classification_report

df = pd.read_csv('winequality-red.csv', delimiter=';')
df['target'] = df['quality'].apply(lambda x: 1 if x >= 7 else 0)
X = df.drop(['quality', 'target'], axis=1)
y = df['target']...",8,"The generated code correctly addresses most of the requirements. It successfully creates the binary target, preprocesses the data (though with a minor issue for this specific dataset), splits the data, trains a logistic regression model, evaluates performance, and identifies important features.

**Strong points:**
*   Correct setup of imports.
*   Correct logic for creating the binary target variable.
*   Proper use of `train_test_split`.
*   Correct application of `StandardScaler` for feature scaling.
*   Correctly trains a `LogisticRegression` model.
*   Correctly calculates and prints accuracy, F1-score, and classification report.
*   Correctly extracts and sorts feature importance based on coefficients.

**Areas for improvement:**
*   **Delimiter for `pd.read_csv`**: The dataset `winequality-red.csv` typically uses a semicolon (`;`) as a delimiter, which the generated code correctly handles (`delimiter=';'`). However, the task instruction mentioned `winequality-red.csv` (which usually implies comma-separated values unless otherwise specified). The ground truth implicitly assumes a well-formed CSV (comma-separated if extension is `.csv`). If the task implied a standard CSV extension, then `delimiter=';'` would be incorrect. Given the context, it's a good practical fix, but technically deviates from what a pure `.csv` might imply.
*   **Missing values handling**: While `SimpleImputer(strategy='mean')` is used, the `winequality-red.csv` dataset is known to not have missing values. The ground truth includes a check for missing values and only imputes if they exist, which is more robust. The generated code always imputes, which is harmless but less efficient/conditional.
*   **`stratify=y` in `train_test_split`**: The target variable `is_good_wine` (or `target`) is highly imbalanced in this dataset. Including `stratify=y` in `train_test_split` (as done in the ground truth) is crucial to ensure that the training and testing sets have a similar proportion of 'good' and 'bad' wines, leading to more reliable model evaluation. This was missed.
*   **Reproducibility**: `random_state` is used for `train_test_split` and `LogisticRegression` for reproducibility, but it's good practice to also include it in the `LogisticRegression` constructor (`random_state=42`) for reproducibility of the model training process itself.
*   **Clarity of output**: The ground truth provides more descriptive print statements for each step, which enhances readability and understanding of the code's execution flow. The generated code is more concise but could benefit from more descriptive output.
*   **`feature_names` after scaling**: While `feature_names` correctly captures the original column names, the `X_train` and `X_test` arrays become NumPy arrays after scaling, losing their column names. The feature importance is correctly mapped back using the original `feature_names`, but sometimes maintaining DataFrame structure through scaling can be beneficial for debugging (though not strictly necessary here).

Overall, the code is very functional and gets the job done. The missing `stratify=y` is the most significant omission affecting the scientific rigor of the evaluation, followed by the lack of `random_state` for the model.",deepseek_r1,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

df = pd.read_csv('heart.csv')

plt.figure()
sns.boxplot(x='HeartDisease', y='Oldpeak', data=df)
plt.show()

X = df[['Oldpeak']]
y = df['HeartDisease']
model = LogisticRegression().fit(X, y)
coef = model.coef_[0][0]
print(f""Coefficient: {coef}"")
print(f""Odds Ratio: {np.exp(coef)}"")

df['OldpeakBin'] = pd.qcut(df['Oldpeak'], q=3, labels=['low',...",7,"The generated code correctly addresses most of the requirements but lacks some crucial details and robustness present in the ground truth.

**Strong Points:**
*   Correctly loads the dataset.
*   Successfully generates the box plot.
*   Performs logistic regression and extracts the coefficient and odds ratio.
*   Correctly discretizes `Oldpeak` using `pd.qcut`.
*   Calculates the proportion of HeartDisease=1 within each bin.

**Areas for Improvement:**
*   **Error Handling (Missing File):** The code lacks the `try-except FileNotFoundError` block for loading the CSV, which is good practice for robust code.
*   **Missing Value Handling:** There's no explicit check for missing values in `Oldpeak` or `HeartDisease` before plotting or modeling, which could lead to errors or incorrect results if dirty data is present.
*   **Box Plot Enhancements:** The generated box plot is very basic. It misses descriptive `title`, `xlabel`, `ylabel`, and `xticks` labels, which significantly improve readability and understanding for others.
*   **Interpretation of Coefficient/Odds Ratio:** While the code prints the coefficient and odds ratio, it completely omits the *interpretation* of the odds ratio, which was an explicit part of the prompt: ""Report the coefficient for Oldpeak and interpret its meaning in the context of odds ratios."" This is a significant miss.
*   **Visualization of Binned Proportions:** The prompt implies or at least heavily suggests a visualization for the binned proportions (like the ground truth's bar plot) since it was done for the box plot. While not explicitly stated ""plot the proportions,"" it's standard practice in data analysis to visualize such results. The generated code only prints the proportions.
*   **Robustness of `pd.qcut`:** The ground truth includes a `try-except` block for `pd.qcut` to handle potential `ValueError` if there are too many identical values in `Oldpeak` preventing unique quantiles, and a fallback to `pd.cut`. This makes the code more robust.
*   **Readability/Clarity:** Adding print statements to indicate which step is being performed (as in the ground truth) improves the user experience and understanding of the code's flow.",deepseek_r1,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

# Load data
df = pd.read_csv('spotify_2023.csv')

# Convert streams to numeric and drop NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams'])

# Create binary target variable
thres...",7,"The generated code correctly implements most of the requirements. It loads data, handles `streams` conversion and NaN dropping, creates the `is_popular` target, selects features, performs one-hot encoding, splits data, trains a Logistic Regression model, and evaluates it using F1-score (macro), ROC AUC score, and confusion matrix.

However, there are a few minor issues and areas for improvement:
1.  **Missing `random_state` for `LogisticRegression`**: While `max_iter=1000` is set, `random_state` is not specified for the Logistic Regression model, which can affect reproducibility if the solver uses stochastic elements (though for `lbfgs` often default, it's good practice for consistency). The ground truth code includes `random_state=42` for the model.
2.  **No `stratify` in `train_test_split`**: The code splits the data without stratifying the target variable (`is_popular`). Given that `is_popular` is created based on a percentile, it will likely be imbalanced. Stratification (`stratify=y`) ensures that the proportion of classes in the training and testing sets is similar to the original dataset, which is crucial for imbalanced datasets. The ground truth code correctly uses `stratify=y`.
3.  **One-Hot Encoding `reset_index` issue**: When `X = pd.concat([X_numerical, X_categorical], axis=1)`, if `df` was not reset before dropping `NaN`s, `X_numerical` and `X_categorical` might have misaligned indices leading to incorrect feature combinations. The generated code uses `X_numerical = df[selected_features].reset_index(drop=True)` which is a sensible approach. However, `X_categorical` needs to ensure its index aligns as well when created from `df[categorical_features]`. `pd.get_dummies` handles this by default as it operates directly on the DataFrame subset in its standard use, but `OneHotEncoder` needs careful handling of indices if not passed the entire DataFrame. In this specific case, `df[categorical_features]` means the indices of `df` are carried over to the subset, so `encoder.fit_transform` and then `pd.DataFrame` creation effectively loses these original indices in `X_categorical`. `pd.concat` then joins based on default integer indices. This happens to work correctly here because `pd.to_numeric` and `dropna` maintain the original index, and then `reset_index` is applied *only* to `X_numerical`. If `X_categorical` also had its index reset from `df[categorical_features]` before encoding, it would be safer. A simpler approach, like in the ground truth using `pd.get_dummies` directly on `X`, avoids these index management concerns.
4.  **Clarity of Print Statements**: While the metrics are printed, the ground truth provides more descriptive print statements for each step, which improves readability and debugging. The task did not explicitly ask for verbose logging, but it's good practice.

Overall, the code is functional and produces the expected output. The issues are mainly about best practices for robustness and reproducibility (stratification, random state) and a slightly less robust approach to one-hot encoding than `pd.get_dummies` when not operating on the whole `X` upfront.",deepseek_r1,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load data and select features
df = pd.read_csv('spotify_2023.csv')
selected_features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%',
                     'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm']
df_selected = df[selected_features].dropna()

# Standardize features
scaler = Stand...",9,"The generated code is very good and largely fulfills all the requirements of the task.

**Strong Points:**
- **Correctness:** All core steps (data loading, feature selection, NaN handling, standardization, K-Means, PCA, plotting, mean calculation) are performed correctly.
- **Conciseness:** The code is more concise than the ground truth while achieving the same results, which is often a good sign of clean code.
- **Completeness:** All specified requirements are met: correct K-Means parameters, PCA components and random state, plotting specifications, and mean calculation for original features.
- **Code Quality:** Proper imports are present.

**Minor Area for Improvement (leading to a score of 9 instead of 10):**
- **Robustness in Data Loading:** The code assumes 'spotify_2023.csv' exists directly without any `try-except` block for `FileNotFoundError`. While the prompt stated to ""assume CSV is in current directory"", robust code in real-world scenarios would include error handling as shown in the ground truth. This is a best practice for programs that rely on external files.

Overall, an excellent solution that demonstrates a strong understanding of the task and relevant libraries.",deepseek_r1,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('spotify_2023.csv')

# Convert 'streams' to numeric and handle errors
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# Drop rows with NaN in 'streams' and apply filters
df = df.dropna(subset=['streams'])
df = df[(df['streams'] > 10000000) & (df['released_year'].between(2010, 2023))]

# Group by year and calculate averages
grouped = df.groupby('released_year').agg({
    'danceability_%': 'm...",8,"The generated code correctly addresses most of the task requirements.

**Strong points:**
*   **Correct Filtering:** It correctly filters `streams` greater than 10,000,000 and `released_year` between 2010 and 2023.
*   **Correct Data Type Conversion:** `streams` is correctly converted to numeric, with errors coerced to `NaN`.
*   **Correct NaN Handling:** Rows with `NaN` streams are dropped before filtering.
*   **Correct Aggregation:** It correctly groups by `released_year` and calculates the mean for `danceability_%`, `energy_%`, and `valence_%`.
*   **Imports:** Proper imports for `pandas` and `matplotlib.pyplot` are included.
*   **Plotting:** It generates line plots for each feature with appropriate titles and labels.

**Areas for Improvement:**
*   **Plot Display:** The plots are displayed individually using `plt.show()` after each plot. While this works, the requirement was to ""Display all three plots."" The ground truth uses `plt.subplot` to display all plots within a single figure, which is generally preferred for presenting related trends together for easy comparison. This is a minor stylistic and presentation point but improves usability.
*   **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust. The generated code lacks this. This wasn't explicitly asked in the prompt's requirements but demonstrates better practice for production-ready code.
*   **Clarity of Filtering Steps:** While functionally correct, the filtering step `df = df.dropna(subset=['streams']); df = df[(df['streams'] > 10000000) & (df['released_year'].between(2010, 2023))]` combines the `dropna` and the main filters on a single line for the second filter. Breaking these filters down into separate lines or intermediate dataframes (as done in the ground truth with `df_filtered`) can sometimes improve readability and debuggability for complex filtering logic.
*   **Consistent Variable Naming:** The ground truth used `yearly_avg_features` for the grouped dataframe, which is quite descriptive. The generated code uses `grouped`, which is also acceptable but less specific.

Overall, the generated code fulfills the core analytical and plotting requirements very well. The suggestions above are primarily for improving robustness, presentation, and code readability.",deepseek_r1,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

df = pd.read_csv('youtube_trendings.csv')

for col in ['Likes', 'Dislikes', 'Video views']:
    df[col] = df[col].str.replace(',', '', regex=True)
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

df['published'] = pd.to_numeric(df['published'], errors='coerce')

df['Category'] = df['Category...",7,"The generated code correctly performs most of the required steps.

**Strengths:**
*   Loads the dataset.
*   Correctly cleans `Likes`, `Dislikes`, and `Video views` by removing commas, converting to numeric, and filling NaNs with 0.
*   Fills missing `Category` values with 'Unknown' and applies one-hot encoding using `pd.get_dummies`.
*   Defines `X` and `y` correctly for the specified features and target.
*   Splits data into training and testing sets with the correct `test_size` and `random_state`.
*   Trains a Linear Regression model.
*   Evaluates the model and reports R2 and MAE.

**Areas for Improvement:**
*   **Published year:** The core task required treating the `published` column (year) as a numeric feature. The generated code attempts `pd.to_numeric(df['published'], errors='coerce')`, but the `published` column in the `youtube_trendings.csv` dataset is typically a datetime string (e.g., '2020-08-11T12:00:00Z'). Simply converting it to numeric will result in NaNs or incorrect values for most entries. It should first be converted to datetime and then the year extracted, as done in the ground truth. This is a significant functional error based on the dataset's nature.
*   **Imports:** The generated code is missing `sklearn.preprocessing` which, while not explicitly used (as it uses `pd.get_dummies`), was specified in the task prompt for completeness when defining imports necessary for the task description.
*   **Warning messages/Robustness:** The ground truth includes checks for column existence and prints warnings if columns are not found, which adds robustness. This is not strictly required by the prompt but improves code quality.
*   **Output format:** While minor, the prompt specified printing R2 as `R-squared (R2) Score: {r2:.4f}` and MAE as `Mean Absolute Error (MAE): {mae:.2f}` for consistency. The generated code uses simpler print statements.",deepseek_r1,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('youtube_trendings.csv')

df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)
df['published'] = pd.to_numeric(df['published'], errors='coerce')

grouped = df.groupby('published').agg(
    views_mean=('Video views', 'mean'),
    unique_videos=('Video', 'nunique')
).reset_index()

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
ax1.plot(grouped['published'], groupe...",7,"The generated code addresses most of the requirements but has a critical issue with the ""published"" column.

**Strengths:**
*   **Data Loading:** Correctly loads the CSV.
*   **Video Views Cleaning:** Accurately removes commas, converts to numeric, and fills NaNs with 0.
*   **Grouping and Aggregation:** Correctly groups by `published` year and calculates both the mean views and unique video count.
*   **Plotting:** Creates two subplots with appropriate titles and labels for each metric as requested. Uses `plt.tight_layout()` for better plot presentation.

**Areas for Improvement/Issues:**
*   **""published"" column handling:**
    *   The prompt specifically asks to ensure the `published` column (year) is treated as a numeric or integer type. The dataset typically has a `publishedAt` column that contains the full date/time string, not just a year. The ground truth correctly extracts the year from `publishedAt`. The generated code assumes a column *named `published` exists and contains only the year*. If `youtube_trendings.csv` contains `publishedAt`, this line `df['published'] = pd.to_numeric(df['published'], errors='coerce')` will either fail or produce NaNs if `published` doesn't exist or is not directly convertible to numeric. This is a significant point of failure if the CSV structure is as implied by ""publishedAt"" in similar datasets and the ground truth.
    *   Filling NaNs for the `published` column is also missing, though `errors='coerce'` will introduce NaNs which then might cause issues in `groupby`.
*   **Robustness:** No error handling for file not found, which is good practice for data loading in real-world scenarios.
*   **Clarity/Comments:** While not explicitly asked for in the prompt to *include* comments, adding comments, especially for the cleaning steps, would improve readability for a complex task.
*   **Plot Enhancements:** The plots are basic (no markers, custom colors, grid lines, or improved axis ticks/rotations as seen in the ground truth). These are minor but contribute to a more polished and readable visualization.",deepseek_r1,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
from nltk.corpus import stopwords

df = pd.read_csv('youtube_trendings.csv')

df['title_word_count'] = df['Video'].apply(lambda x: len(str(x).split()))
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)

correlation = df['title_word_count'].corr(df['Video views'])
print(f""Pearson correlation: {correlation:.4f}"")

df['published'] = pd.to_datetime(df['published'])
filtered_df =...",8,"The generated code successfully completes most of the tasks, demonstrating a good understanding of pandas operations and text processing.

**Strong Points:**
- Correctly loads the dataset.
- Accurately calculates `title_word_count`.
- Effectively cleans and converts `Video views` to numeric, handling missing values.
- Correctly calculates and prints the Pearson correlation.
- Filters the dataset based on publication year as required.
- Processes video titles to remove punctuation and converts to lowercase.
- Correctly identifies and prints the 5 most common words, utilizing `collections.Counter` and `nltk.corpus.stopwords`.
- The code is concise and generally well-structured.

**Areas for Improvement / Minor Issues:**
- **NLTK Download:** The code assumes `nltk.corpus.stopwords` is already downloaded. The ground truth code includes a `try-except` block to ensure this, which is a good practice for robust code. While not explicitly asked to *handle* the download, the Ground Truth sets a good example of handling prerequisites.
- **Output Clarity:** The output for the correlation and most common words is less explicit than the ground truth. For example, `print(f""Pearson correlation: {correlation:.4f}"")` is fine, but the ground truth adds more context (""Pearson correlation between 'title_word_count' and 'Video views'""). Similarly, the `most_common_words` output could be formatted more readably for each word and its count.
- **Stop Word Handling Detail:** In the `words.extend` list comprehension, `word not in stop_words` is correctly applied. However, the ground truth also adds `.isalpha()` to filter out numeric words or empty strings that might result from `re.sub` and `split()`, which is a slightly more robust cleaning step. This is a minor subtlety but contributes to more accurate word analysis.
- **Error Handling for File:** No `try-except` block for `pd.read_csv`, which means a `FileNotFoundError` would crash the script. The ground truth handles this gracefully.
- **Word processing `str(title).lower()`:** While `str(title)` is used, the initial conversion of the 'Video' column to string type in the ground truth (`df['Video'] = df['Video'].astype(str)`) is a more explicit and robust way to ensure all entries are strings before processing, preventing potential errors with non-string title types. The generated code implicitly handles this during `str(x).split()` for `title_word_count` and `str(title).lower()` for common words.

Overall, it's a very good solution that meets almost all requirements effectively. The suggested improvements are primarily about robustness, clarity, and minor edge-case handling.",deepseek_r1,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score

# Load data
df = pd.read_csv('mobiles_2025.csv')

# Clean target and create binary label
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '', regex=False).str.replace(',', '')
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'], errors='coerce')
df...",7,"The generated code correctly addresses most of the task requirements, demonstrating a good understanding of data cleaning, feature engineering, and model training in Python.

**Strong Points:**
*   Correctly loads the data.
*   Successfully cleans `Launched Price (USA)` and creates `is_expensive_usa`.
*   Correctly cleans `Mobile Weight`, `RAM`, `Battery Capacity`, and `Screen Size`.
*   Successfully performs one-hot encoding for `Company Name` and `Processor`.
*   Correctly splits data and trains a `RandomForestClassifier`.
*   Evaluates with F1-score (macro) and ROC AUC score.

**Areas for Improvement:**
1.  **`is_expensive_usa` NaN Handling:** The prompt specified a nuanced NaN handling for `is_expensive_usa`: ""1 if cleaned `Launched Price (USA)` > 1000, else 0. Drop rows where `is_expensive_usa` is NaN."" The generated code `(df['Launched Price (USA)'] > 1000).astype(int)` will convert `NaN > 1000` to `False`, which then becomes `0`. This means rows with unparseable prices (that became NaN) are treated as `0` (not expensive) rather than being dropped based on the `is_expensive_usa` column itself being NaN. The `dropna(subset=['is_expensive_usa'])` is therefore ineffective for filtering out unparseable price rows. While the ground truth also uses `np.nan` for the target first, the key is the `apply` function with `pd.notna(x)` inside, which correctly ensures that `NaN` in `Launched Price (USA)_cleaned` *propagates* to `is_expensive_usa` as `np.nan` for later dropping. The generated code's approach will incorrectly classify these as 0.
2.  **Camera Feature Extraction:** The regex `r'(\d+)\s*MP'` for camera features only extracts integer MP values. The ground truth uses `r'(\d+(\.\d+)?)\s*MP'`, which correctly handles floating-point MP values (e.g., 48.0 MP). This will lead to data loss or incorrect values for cameras with non-integer MP values.
3.  **NaN Handling for X and y:** The prompt states ""Drop rows with any NaNs in features or target."" The generated code `X.join(y).dropna()` followed by reassigning `X` and `y` is a good approach. However, it's worth noting that the cleaning steps for numerical features simply use `errors='coerce'`, which introduces NaNs. These NaNs are correctly handled by the final `dropna` step, but the prompt's requirement specifically asks to ""Drop rows with any NaNs in features or target *after all transformations*"", which is what the code effectively does. The ground truth code has a more explicit flow for this, ensuring `Launched Year` as well is included in the final `dropna` check.
4.  **No `Launched Year` Cleaning/Inclusion:** The task specifies `Launched Year` as one of the numerical features in `X`. The generated code implicitly relies on `Launched Year` being clean and numeric, but it doesn't explicitly clean it (though it's unlikely to need much cleaning beyond `pd.to_numeric` if it contains errors). More critically, in the line `df = df[features + ['is_expensive_usa']]`, `Launched Year` is correctly included, which is good. However, if there are NaNs in `Launched Year`, they would also be covered by the final `dropna`.
5.  **Robustness (File Not Found):** The ground truth includes a robust `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code assumes the file exists.
6.  **Redundant `astype(int)`:** For `is_expensive_usa`, the `.astype(int)` converts `False` to `0` and `True` to `1`. This is simpler than the ground truth's `apply` function, but it fails to correctly propagate NaNs as described in point 1.

Overall, the code is functional and largely correct, but minor errors in NaN handling and regex for numerical extraction prevent it from being perfect.",deepseek_r1,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

def clean_price(price):
    cleaned = str(price)
    currencies = ['PKR', 'INR', 'CNY', 'USD', 'AED']
    for currency in currencies:
        cleaned = cleaned.replace(currency, '')
    cleaned = cleaned.replace(',', '').strip()
    return pd.to_numeric(cleaned, errors='coerce')

df = pd.read_csv('mobiles_2025.csv')

price_columns = [
    'Launched Price (Pakistan)', 
    'Launched Price (India)', 
    'Launched Price (China)', 
    'Launched Pri...",7,"The generated code correctly addresses most of the task requirements, but it has some minor issues that prevent it from achieving a higher score.

**Strengths:**
*   **Correct Cleaning Logic:** The `clean_price` function correctly removes currency symbols and commas and uses `pd.to_numeric(errors='coerce')` which is an efficient way to handle conversion errors by returning NaN.
*   **Correct Column Application:** The cleaning function is correctly applied to all specified price columns.
*   **Correct Average Calculation:** The `df[price_columns].mean(axis=1)` correctly calculates the average global price. Importantly, `pd.mean()` on a DataFrame/Series by default already handles NaNs by skipping them (`skipna=True` is the default behavior), so this aspect is correctly implemented.
*   **Correct Top Models Identification:** Using `df.nlargest(5, 'Average Global Price')` is an efficient and correct way to get the top 5 models.
*   **Proper Display:** The code correctly selects and prints `Model Name` and `Average Global Price`.

**Areas for Improvement (Why not higher score):**

*   **Error Handling for File Loading:** The generated code lacks a `try-except` block for `pd.read_csv`, which means if the CSV file is not found, the script will crash with a `FileNotFoundError` instead of gracefully handling it. The ground truth includes this, which is good practice.
*   **Handling `clean_price` Input:** The `clean_price` function immediately converts `price` to `str(price)`. While this works for most cases, if an entry is `np.nan` from the start, `str(np.nan)` becomes the string 'nan', which then `pd.to_numeric` will correctly convert to `NaN`. The ground truth explicitly checks for `pd.isna(price_str)` at the beginning of the function, which is a slightly more robust or clearer way to handle pre-existing `NaN` values, though the generated code's approach effectively achieves the same for `np.nan`.
*   **Documentation and Comments:** While not explicitly asked for in ""provide only the Python code without explanations,"" generally, good quality code includes comments or docstrings, especially for functions like `clean_price`. The ground truth includes a good docstring for the function.
*   **Warning for Missing Columns:** The ground truth includes a check and warning if any of the `price_columns` are not found in the DataFrame. This makes the code more robust against malformed or updated input data. The generated code assumes all columns will exist.
*   **Clarity of Display Output:** The ground truth adds a print statement ""Top 5 Mobile Phone Models by Average Global Price:"" which makes the output more user-friendly. The generated code just prints the DataFrame directly.",deepseek_r1,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('mobiles_2025.csv')

df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', '', regex=False), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', '', regex=False), errors='coerce')
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'].str.replace('USD ', '', regex=False).str.replace(',', '', regex=False), errors='coerce')

df_clean = df.dropna(subset=['RAM'...",8,"The generated code correctly addresses most of the requirements. It successfully loads the data, cleans the specified columns (`RAM`, `Battery Capacity`, `Launched Price (USA)`) by removing characters and converting to numeric, and handles errors by coercing to `NaN`. It then correctly drops rows with `NaN`s in these cleaned columns. The filtering for 'Apple' products and grouping by ""Launched Year"" are also correctly implemented. Finally, it generates separate box plots for `RAM`, `Battery Capacity`, and `Launched Price (USA)` for each year, with appropriate titles and y-axis labels.

Areas for improvement to achieve a higher score include:
- **Robustness in data loading:** The ground truth code includes a `try-except` block for `FileNotFoundError`, which is good practice for robust code.
- **Explicit `str.strip()`:** While the current cleaning often works, adding `.str.strip()` after removing units (e.g., `'GB'`) is good practice to handle potential leading/trailing spaces.
- **Handling `Launched Year` NaNs:** The ground truth also explicitly checks and cleans `Launched Year` by converting it to numeric and then integer, and dropping NaNs, which is a good robustness measure ensuring the grouping works as expected. The generated code implicitly assumes `Launched Year` is clean enough for `groupby`.
- **Plotting style/enhancements:** The ground truth utilizes `seaborn` and includes `plt.style.use`, making plots potentially more visually appealing and professional. It also adds a grid and proper x-axis label removal.
- **Informative print statements:** While not explicitly requested for evaluation, the ground truth's print statements provide useful feedback on the steps being performed and data counts, which can be helpful for debugging and understanding flow.",deepseek_r1,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import (train_test_split, StratifiedKFold, GridSearchCV, 
                                     RandomizedSearchCV, learning_curve, valida...",6,"The generated code attempts to address many aspects of the prompt but falls short in several key areas compared to the ground truth, leading to reduced correctness, completeness, and overall quality.

**Correctness & Completeness Issues:**

*   **Advanced Feature Engineering:** The prompt explicitly asks for ""advanced feature engineering (polynomial features, interaction terms, text mining from names)"".
    *   **Text Mining:** The generated code does some basic title extraction and `HasCabin` (which is good), but misses the depth of the ground truth's `FeatureEngineerTransformer` which extracts 'Is_Married', 'Name_Length', and extracts Deck from 'Cabin'. This is a significant miss on ""text mining from names"" beyond simple title extraction.
    *   **Polynomial Features/Interaction Terms:** The generated code applies `PolynomialFeatures(degree=2, interaction_only=True)` which is only interaction terms. The ground truth uses `interaction_only=False` which means it generates polynomial features *and* interaction terms, which is more comprehensive and aligns better with ""polynomial features, interaction terms"".
*   **Missing Values (Multiple Imputation):** The prompt specifically requests ""multiple imputation strategies"". While `IterativeImputer` is used for numerical features, the ground truth explicitly uses `SimpleImputer` for categorical ones (`Embarked`). The generated code only shows `IterativeImputer` for numerical and `SimpleImputer` for categorical, but doesn't explicitly refer to them as ""multiple imputation strategies"" for *different types of features or different approaches on the same feature*, just standard preprocessing. The ground truth *defines* IterativeImputer as the ""multiple imputation strategy"" and handles others simply. The prompt was a bit ambiguous on ""multiple imputation strategies,"" but the ground truth's approach covered this better.
*   **Hyperparameter Tuning:** The generated code uses `GridSearchCV` (`param_grids`), while the prompt explicitly mentions ""GridSearchCV *or* RandomizedSearchCV"". The ground truth correctly uses `RandomizedSearchCV`, which is generally more efficient for larger search spaces. This isn't a critical error but a point of differentiation.
*   **Model Coverage:** The generated code misses XGBoost, which the ground truth includes and tunes. This is a common and important model in data science.
*   **Feature Selection:**
    *   The prompt asks to ""Implement feature selection using multiple techniques and compare their impact"". The generated code attempts this with `Lasso`, `RF_importance`, and `MutualInfo`, but its `MutualInfo` implementation `SelectFromModel(SelectFromModel(estimator=mutual_info_classif, threshold='median'))` is syntactically incorrect and will throw an error or not work as intended. `mutual_info_classif` directly operates on features and target; it doesn't need to be wrapped in another `SelectFromModel`. The ground truth provides correct implementations for `SelectKBest`, `RFE`, and `SelectFromModel`.
    *   The generated code compares impact only via a generic `RandomForestClassifier` and doesn't visualize the results as well as the ground truth.
*   **Extensive Model Evaluation:**
    *   **Learning Curves:** While a learning curve is plotted, the ground truth plots for a more representative model (StackingClassifier) and provides better visual clarity with fill-between for std deviation.
    *   **Validation Curves:** The generated code completely misses validation curves.
    *   **Performance Metrics:** The output logs only `accuracy_score` during CV for comparing feature selection, and an overall `metrics` list with accuracy for models. The prompt asks for ""comprehensive performance metrics"" (e.g., precision, recall, F1, ROC AUC). The ground truth covers ROC AUC extensively.
    *   **Statistical Significance Tests:** The generated code only performs `ttest_rel` between ""model_scores"" (base models), missing the comparison against the best ensemble (StackingClassifier) which the ground truth correctly implements.
*   **Model Interpretability (LIME):** The generated code misses LIME entirely. It only implements SHAP.
*   **SHAP Implementation:** The SHAP analysis in the generated code uses `shap.Explainer` directly on the preprocessed data and the base model, which is okay for tree-based models like RF, but the ground truth demonstrates how to properly integrate SHAP with pipelines and uses `TreeExplainer` for XGBoost specifically while also demonstrating `initjs` and `force_plot` for deeper interpretability. The ground truth also explicitly shows SHAP for an *ensemble* (XGBoost from the ensemble), not just a base model.

**Code Quality Issues:**

*   **Pipeline Structure:** The ground truth builds full pipelines including the `FeatureEngineerTransformer` which preprocesses the *raw* data `X_train`, which is a cleaner and more robust way to handle the entire workflow. The generated code performs initial feature engineering *before* the `train_test_split`, which is generally a bad practice as it can lead to data leakage, especially with features like `Title` or `HasCabin` if they were derived from a larger dataset before splitting. Also, it performs a separate `X_train_processed = feature_engineering.fit_transform(X_train)` and then passes `X_train_processed` to models, failing to integrate the `feature_engineering` step *into* the model pipelines (`GridSearchCV` or `cross_val_score`). This makes the evaluation process much less robust and potentially problematic for real-world deployment. The ground truth integrates *all* preprocessing into the pipeline directly.
*   **`get_feature_names_out`:** The ground truth correctly implements a helper function to get feature names after complex preprocessing (including `PolynomialFeatures` and `OneHotEncoder`) which is crucial for interpretability tools like SHAP. The generated code calls `feature_engineering.get_feature_names_out()` directly, but it's not clear if this would correctly reconstruct all feature names as the ground truth's `get_feature_names_after_preprocessing` does, especially with `PolynomialFeatures` interaction terms.
*   **Plotting Functions:** The `plot_learning_curve` function is called but not defined in the generated code, leading to an error.

Overall, while the generated code attempts to cover the requirements, its implementation details, particularly around robust pipeline construction, advanced feature engineering, specific evaluation metrics, and comprehensive interpretability tools, fall short of the ground truth.",deepseek_r1,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

df = pd.read_csv('imdb_top_250.csv')
X = df[['Year', 'Runtime']]
y = df['Rating']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(f""R2: {r2_score(y_test, y_pred):.4f}"")
print...",6,"The generated code correctly performs the core tasks: loading data, selecting features/target, splitting data, training a Linear Regression model, and evaluating with R2 and MAE. The imports are correct and the structure is logical.

However, it lacks essential robustness and completeness compared to the ground truth. Key missing elements include:
- **Error Handling:** No `try-except` block for `FileNotFoundError` or other exceptions when loading the CSV, which is crucial for real-world applications.
- **Data Preprocessing/Cleaning:** The ground truth includes robust data cleaning for `Runtime` (removing ' min' and converting to numeric) and ensures all relevant columns are numeric, also handling potential `NaN` values. The generated code assumes the CSV file is perfectly clean and numeric, which is often not the case for real datasets (e.g., '100 min' in `Runtime` would cause an error or silent failure if not handled).
- **Output:** The output is minimal (`R2: ...` and `MAE: ...`). While it fulfills the direct evaluation requirement, the ground truth provides more informative print statements about data loading, splitting, and model training success.

For a higher rating, robust error handling and basic data cleaning/preprocessing steps (especially for non-numeric columns like 'Runtime' that are specified as features) would be necessary.",deepseek_r1,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

df = pd.read_csv('imdb_top_250.csv')
X = df[['Rating', 'Runtime']]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42).fit(X_scaled)
labels = kmeans.labels_
plt.scatter(df['Runtime'], df['Rating'], c=labels, cmap='viridis')
plt.title('K-Means Clustering (Rating vs Runtime)')
plt.xl...",6,"The generated code correctly performs the core steps: loading data, selecting features, scaling, applying K-Means (with correct parameters), and calculating cluster means. However, it misses a crucial data cleaning step for the 'Runtime' column, which is often imported as a string (e.g., '142 min') and needs conversion to numeric type. This omission would likely lead to an error or incorrect clustering if the 'Runtime' column is not already purely numeric in the CSV. The plotting also lacks some details present in the ground truth for better visualization (e.g., `s` and `alpha` parameters for scatter, `colorbar`, `grid`). The error handling for `FileNotFoundError` and missing columns is also absent, which is good practice for robustness.",deepseek_r1,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import re

nltk.download('stopwords')
nltk.download('punkt')

df = pd.read_csv('imdb_top_250.csv')

df['title_length'] = df['Title'].str.len()
correlation = df['title_length'].corr(df['Rating'])
print(f""Pearson correlation coefficient: {correlation:.3f}"")

stop_words = set(stopwords.words('english'))
df['processed_tokens'] = df['Title'].apply(
    la...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from achieving a higher score.

**Correctness:**
- The correlation calculation is incorrect. It directly calculates `df['title_length'].corr(df['Rating'])` without ensuring `Rating` is numeric or handling potential NaNs. The prompt specifically asked for ""Pearson correlation coefficient"" and the ground truth properly converts `Rating` to numeric and handles missing values.
- The preprocessing of titles is done in a single lambda function, which is less readable and harder to debug than the step-by-step approach in the ground truth. More importantly, the regex `[^a-z0-9\s]` does not properly handle all non-alphanumeric characters or ensure consistency for lowercase before `word_tokenize`. For example, `re.sub(r'[^a-z0-9\s]', '', x.lower())` is problematic because `word_tokenize` will treat ""Movie Title"" as two tokens, even after lowercasing `movie title`, if punctuation is kept. The instruction was to ""remove punctuation (characters that are not alphanumeric or whitespace)"", which the `re.sub` within the lambda doesn't fully capture correctly because the tokenization happens *after* this sub, but the regex itself is not handling all punctuation. It's close but the order of operations and the specific regex for punctuation could lead to subtle errors compared to the ground truth.

**Code Quality:**
- Imports are mostly correct (though `string` was not imported as per prompt's optional hint of its utility within `string.punctuation`, even if `re` is ultimately better).
- NLTK downloads are hardcoded, while the ground truth gracefully handles if they are already present (though for a benchmark, hardcoding might be acceptable, but it's less robust).
- The use of a single lambda function for all preprocessing makes the code less modular and readable for complex text processing tasks compared to separating concerns into different steps or functions as demonstrated in the ground truth.
- The print statements are concise but could be slightly more descriptive.

**Completeness:**
- All major steps are addressed (load data, `title_length`, correlation, preprocessing, stopwords, common words).
- However, the output for correlation is just ""Pearson correlation coefficient"" without clarifying which columns it's between, which was specified in the prompt (""correlation coefficient between `title_length` and the `Rating` column"").
- The most significant omission is the flawed correlation calculation. The prompt implicitly expects a robust calculation. The ground truth explicitly shows how to handle the `Rating` column's data type and NaNs, which is crucial for a reliable correlation.

**Areas for Improvement:**
- **Robust Correlation Calculation:** Implement proper handling for the 'Rating' column to ensure it's numeric and handle NaN values before calculating correlation, as shown in the ground truth.
- **Modular Preprocessing:** Break down the text preprocessing steps (lowercase, remove punctuation, tokenize, remove stopwords) into separate, more readable operations (e.g., separate `.apply()` calls for each step or a dedicated function), instead of a single complex lambda. This aligns better with the sequence described in the prompt's steps 4 and 5.
- **Output Clarity:** Make the correlation print statement more explicit about the variables involved, e.g., ""Pearson correlation between title_length and Rating: ..."".",deepseek_r1,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score
import numpy as np

# Load data and handle missing target
df = pd.read_csv('penguins.csv').dropna(subset=['species'])
X = df[['culmen_length_mm', 'culmen_depth_mm'...",9,"The generated code correctly addresses most of the requirements using a well-structured pipeline approach, which is generally good practice for data preprocessing.

**Strong Points:**
- **Pipeline usage:** Effectively uses `Pipeline` and `ColumnTransformer` for clean and organized preprocessing steps (imputation, scaling, one-hot encoding). This makes the code modular and easier to understand.
- **Correct Imputation Strategies:** Uses `SimpleImputer` with 'median' for numerical and 'most_frequent' for categorical features, as requested.
- **Correct Scaling and Encoding:** `StandardScaler` and `OneHotEncoder` are used appropriately.
- **Target Encoding:** `LabelEncoder` is used for the target variable `species`.
- **Model Training and Evaluation:** `RandomForestClassifier` and `cross_val_score` are used correctly with the specified parameters.
- **Handling missing target:** The `dropna(subset=['species'])` at the beginning is a good way to handle missing target values.

**Areas for Improvement (Why not a 10):**
- **Specific `island` NA handling:** The prompt explicitly stated: ""if a row still has NA in `island` after mode imputation, e.g. if all were NA, it can be dropped, or a placeholder like 'Unknown' can be used if mode is NA itself. For this task, if mode imputation results in NA for island, drop those rows.""
    - The ground truth code handled this by explicitly checking if `island` was entirely NA before mode imputation, and then dropping rows if any `island` NAs persisted post-imputation (which can happen if the mode itself was NA, or if the column was all NA and couldn't be imputed).
    - The generated code attempts to address this with `handle_unknown='ignore'` in `OneHotEncoder` and then by checking `X_processed_df[island_columns].sum(axis=1) == 0`. While this approach correctly identifies rows where the 'island' category might have resulted in all zeros (e.g., if the original `island` value was `np.nan` and not handled by `SimpleImputer` or `handle_unknown` created an all-zero vector for it), it's not directly adhering to the ""if mode imputation results in NA for island, drop those rows"" phrasing. `SimpleImputer(strategy='most_frequent')` *should* handle NAs by filling them, so logically no NAs should remain after the imputer step itself. However, the exact edge case of ""if mode itself is NA"" (which is highly unlikely for strings like island names unless the column is entirely `NaN`) or if a categorical imputer somehow failed to fill an NA, might not be perfectly caught by a standard `SimpleImputer`. The ground truth directly checked for NAs *after* imputation, which is more robust for that specific requirement. The generated code's check `X_processed_df[island_columns].sum(axis=1) == 0` is a good proxy for ""unknown/missing"" categories but doesn't specifically target ""NA after *mode imputation*"".
    - In practice, `SimpleImputer(strategy='most_frequent')` should ensure no NAs remain in the categorical columns. The added `island_mask` logic handles a slightly different case (where one-hot encoding might result in all zeros for a category, potentially if the original `NaN` was passed through, which shouldn't happen with `most_frequent` strategy unless the input column was entirely `NaN` and `SimpleImputer` could not find a mode, or the output of `get_feature_names_out` was not perfectly aligned, but this is less likely).
- **Redundant conversion to DataFrame:** Although creating `X_processed_df` is useful for verifying `island` NAs, it's not strictly necessary for the `cross_val_score` if the preceding `Pipeline` correctly produced numerical data. The `y = y[~island_mask]` also assumes `y` aligns with `X_processed_df`'s index, which it does because of `index=X.index`, which is good.

Overall, the generated code is very good, demonstrating advanced Sklearn usage. The only minor deviation is the very specific corner case handling for 'island' NAs.",deepseek_r1,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Load and preprocess data
df = pd.read_csv('penguins.csv')
selected_features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_clean = df.dropna(subset=selected_features)
X = df_clean[selected_features]
species = df_clean['species']

# Standardize features
scaler = StandardScaler()
...",7,"The generated code correctly performs data loading, feature selection, NA handling, standardization, K-Means clustering, and PCA. However, there are a few areas for improvement, particularly regarding plotting and robustness.

**Strong Points:**
*   Correctly loads data and selects specified features.
*   Handles NAs using `dropna` on the specified subset.
*   Correctly applies `StandardScaler`.
*   Correctly implements `KMeans` with `n_clusters=3`, `n_init='auto'`, and `random_state=42`.
*   Correctly applies `PCA` to 2 components.
*   Generates two plots with PCA components, one for clusters and one for species.

**Areas for Improvement:**
*   **Plotting Fidelity:** The coloring for the species plot uses `species.astype('category').cat.codes`, which assigns an arbitrary numerical map to the species. While it technically colors them, it doesn't use distinct categorical colors or a legend that maps the colors to the actual species names, making the plot less interpretable compared to the ground truth which uses explicit legend titles and categorical palettes.
*   **Plotting Clarity:** Both plots use `cmap='viridis'`. While `viridis` is good for continuous data or ordered categories, for distinct categorical clusters and species, different categorical colormaps (like `tab10`, `Paired`, etc.) would be more appropriate for distinguishing groups.
*   **Titles:** The titles are present but could be more descriptive as per the prompt's `for comparison` request for the second plot.
*   **`species` handling:** The ground truth explicitly ensures that `species` aligns with the cleaned data by including it when dropping NAs. The generated code also does this correctly by taking `species = df_clean['species']`.
*   **Missing `species` `hue` logic:** The generated code directly converts `species` to numeric codes for plotting color, which works but isn't as clear or standard as using a `hue` argument in a plotting library like Seaborn, where mapping of colors to labels is automatic in the legend. While Matplotlib `scatter` doesn't have a `hue` argument like Seaborn, one would typically use `plt.legend()` with label arguments for discrete categories.
*   **Robustness:** The code lacks error handling for the `penguins.csv` file not being found, which the ground truth includes.
*   **Aesthetics:** The ground truth includes `seaborn.set_style` which improves plot aesthetics. The generated code does not. The lack of explicit `xlabel`, `ylabel`, and `legend` setup (especially for the species plot) makes the plots less informative.",deepseek_r1,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# Load dataset
df = pd.read_csv('penguins.csv')

# Impute missing values
numeric_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())
df['sex'] = df['sex'].fillna(df['sex'].mode()[0])
df.dropna(s...",7,"The generated code correctly addresses most of the requirements.

**Strong points:**
- Correctly loads the dataset.
- Imputes numerical missing values with the mean and string `sex` with the mode.
- Correctly one-hot encodes `island` and `sex`.
- Correctly drops rows where `island` is NA. (Order of operations for imputation: it imputes all numericals and sex first, then drops island NAs, which is fine since dropping island NAs won't affect the numerical means or sex mode, and it will clean up those rows before `X` is formed.)
- Defines `X` and `y` correctly, with `species` label encoded.
- Trains the `GradientBoostingClassifier` with specified parameters.
- Extracts and prints feature importances associated with correct feature names.
- Uses `seaborn` for plotting.

**Areas for improvement to achieve a higher score:**
- **Plotting:** The plotting section only creates box plots grouped by `sex` for each `species` but does not group by `sex` within the `species` as requested by the prompt (i.e., `sns.boxplot(x='species', y='flipper_length_mm', hue='sex')`). The current plot is a series of subplots, each showing flipper length by sex for a single species, which is not what was asked. The prompt precisely asks for ""box plots to visualize the distribution of `flipper_length_mm` (using imputed data) grouped by the imputed `sex` column *for each species*"". This implies a single plot with species on X, flipper length on Y, and `sex` as a hue.
- **Robustness:** No `try-except` block for file loading, which can lead to a `FileNotFoundError` crashing the script if the CSV is not present.
- **Clarity of Imputation Order:** While functionally correct, the ground truth's approach of dropping `island` NAs *before* calculating means/modes for other columns might be slightly more robust, as mean/mode calculations would then only be based on rows that will actually be used. In this case, it doesn't make a practical difference since `island` NA rows only contain `island` NA.
- **`get_dummies` `dtype`:** Not critical, but `pd.get_dummies` now recommends `dtype=int` for integer output.

Overall, it's a very good attempt, with the only significant deviation being the interpretation and execution of the final plotting requirement, which is key for a data science benchmark.",deepseek_r1,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer

# Load the dataset
df = pd.read_csv('penguins.csv')

# Define numerical columns and impute using KNN
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
imputer = KNNImputer(n_neighbors=3)
df[numerical_cols] = imputer.fit_transform(df[numerical_cols])

# Impute 'sex' column with mode
sex_mode = df['sex'].mode()[0]
df['sex'] = df['sex'].fillna(sex_mode)

# Print remaining NA counts
print(...",9,"The generated code correctly performs all the required data cleaning steps: loading the data, applying KNN imputation to the specified numerical columns with `n_neighbors=3`, and imputing the `sex` column with its mode. It also correctly prints the remaining NA values to verify.

Strengths:
- Correctly identifies and uses `KNNImputer` for numerical columns.
- Correctly calculates and applies mode imputation for the categorical `sex` column.
- The `fillna` method for the 'sex' column is properly used.
- Verifies the remaining NA values.

Areas for minor improvement:
- Error handling for `FileNotFoundError` when loading the CSV is missing, which is present in the ground truth. While the prompt assumes the CSV is in the current directory, robust code would include this.
- The `inplace=True` in `fillna` is often considered less clear than direct assignment (`df['sex'] = df['sex'].fillna(sex_mode)`), but the generated code uses direct assignment which is good. The ground truth uses `inplace=True` which is a minor point of divergence from modern pandas recommendations.
- The generated code uses `isna().sum()` which is equivalent and perfectly fine, and arguably more modern than `isnull().sum()`.
- No print statements to confirm the imputation steps have been performed which were in the ground truth, but this is not a core requirement.

Overall, it's a very solid and functional solution that meets all core requirements.",deepseek_r1,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter to include only Adelie and Gentoo species
df_filtered = df[df['species'].isin(['Adelie', 'Gentoo'])]

# Drop rows with NA in culmen_length_mm
df_filtered = df_filtered.dropna(subset=['culmen_length_mm'])

# Split into two samples
adelie = df_filtered[df_filtered['species'] == 'Adelie']['culmen_length_mm']
gentoo = df_filtered[df_filtered['species'] == 'Gentoo']['culmen_l...",9,"The generated code correctly performs all the requested steps for the hypothesis test. It loads the data, filters species, handles NAs, performs the t-test, and interprets the results correctly. The imports are proper, and the code is straightforward.

**Strong Points:**
*   **Correctness:** All logical steps (loading, filtering, NA handling, t-test, interpretation) are correctly implemented.
*   **Completeness:** Addresses all six requirements outlined in the task.
*   **Readability:** The code is clear and easy to follow.

**Areas for Minor Improvement (for a perfect 10):**
*   **Encapsulation/Modularity:** The ground truth code wraps the logic in a function, which is good practice for reusability and managing larger scripts. The generated code is a top-level script, which is acceptable for a small task but less robust.
*   **Error Handling:** The generated code lacks explicit error handling for `FileNotFoundError` or other issues that might occur during CSV loading, which the ground truth includes.
*   **Clarity of Interpretation:** While the conclusion is correct, the ground truth's conclusion is slightly more detailed and informative, stating ""statistically significant evidence to suggest a difference..."" rather than just ""significant difference.""

Overall, it's a very solid solution that directly answers the prompt's requirements. The minor points are more about production-readiness and robustness rather than fundamental correctness for this specific task.",deepseek_r1,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report

# Load dataset
df = pd.read_csv('penguins.csv')

# Define features and target
X = df[['culmen_length_mm', 'culm...",9,"The generated code is very good and meets almost all the requirements.

**Strong points:**
- **Correctness:** The code correctly loads the data, defines features and target, performs the specified preprocessing steps (median imputation for numerical, mode for sex, one-hot encoding for `island` and `sex`), label encodes the target, splits data with stratification, trains a `RandomForestClassifier`, and evaluates the model using accuracy and a classification report.
- **Code Quality:** Uses `ColumnTransformer` and `Pipeline` effectively for streamlined preprocessing, which is a good practice in scikit-learn. Imports are correct and the code is structured logically.
- **Completeness:** Addresses most of the requirements.

**Areas for minor improvement:**
- **Preprocessing for `island`:** The ground truth code uses a `SimpleImputer(strategy='most_frequent')` for all categorical features within the `categorical_transformer` pipeline before `OneHotEncoder`. While `island` might not have missing values in this specific dataset, best practice for robust pipelines often includes an imputer for categorical features even if immediate necessity isn't apparent, to handle potential future data variations. The generated code applies `OneHotEncoder` directly to `island` without an imputer in the `ColumnTransformer` tuple, which is fine if `island` is guaranteed to have no NAs, but less robust than the pipeline approach for `sex` and the ground truth's combined categorical handling. However, given the prompt only specified ""for `sex`, impute NAs with the mode"" specifically, not for `island`, this isn't a critical error for *this* task.
- **`copy()` for X and y:** The ground truth code uses `X = df[features].copy()` and `y = df[target].copy()`. While not strictly necessary in this exact scenario (as `X` and `y` are not modified in place before splitting), using `.copy()` can prevent `SettingWithCopyWarning` in more complex scenarios where one might modify `X` or `y` after selection. This is a minor stylistic/robustness point.

Overall, the generated code is highly functional and well-structured, demonstrating strong understanding of the requirements. The minor points are about robust pipeline design rather than critical failures for the given task.",deepseek_r1,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

corr_years = df['PerformanceScore'].corr(df['YearsExperie...",8,"The generated code correctly defines the DataFrame, calculates the specified Pearson correlation coefficients, and prints them. It also correctly identifies which feature has a stronger absolute correlation with ""PerformanceScore"".

**Strong Points:**
*   **Correct DataFrame Creation:** The DataFrame is created accurately as per the task description.
*   **Correct Correlation Calculation:** The `corr()` method is used correctly to calculate Pearson correlations.
*   **Correct Absolute Comparison:** The `abs()` function is appropriately used to compare the strength of correlations, regardless of their sign.
*   **Clear Output:** The print statements are clear and informative.

**Areas for Improvement:**
*   **Handling Equal Absolute Correlations:** The `else` block in the generated code implicitly assigns ""TrainingHours"" as stronger if the absolute correlations are equal (or if `corr_training` is stronger). The ground truth handles this edge case explicitly with an `elif` for `abs_corr_training > abs_corr_years` and a final `else` for equality, which is more robust and accurate.
*   **Formatting of Printed Correlations:** The generated code uses `:.2f` for printing correlations, while the ground truth uses `:.4f`. While `:.2f` is acceptable, `:.4f` provides more precision, which is generally preferred when displaying correlation coefficients.
*   **Optional Method Parameter:** Although `pearson` is the default method for `corr()`, explicitly stating `method='pearson'` as in the ground truth can improve code readability and explicitness, making it clear what type of correlation is being calculated without relying on default behavior.
*   **Completeness of Output for Comparison:** The ground truth includes the actual absolute correlation values when printing which feature is stronger, which provides more context and verification. The generated code only states which one is stronger without showing the values.",deepseek_r1,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import multivariate_logrank_test
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import logit
from pymc3 import Model, Normal, Binomial, sample, plot_trace
import arviz as az

# Data loading and preprocessing
df = pd.read_csv('titanic.csv')
df['Age'].fillna(df['Age'].median(), inplace=True)
df['Embarked']...",6,"The generated code attempts to address all requirements of the prompt, which is commendable. However, several critical issues and inaccuracies prevent it from achieving a higher score.

**Correctness:**
*   **Kaplan-Meier and Cox Model `T` definition:** Both the ground truth and the generated code correctly identify the problem that `time-to-event` isn't readily available for the Titanic dataset. The ground truth clearly states it assumes a fixed T=1. However, the generated code uses `T = np.ones(len(df))` for KM and `df_cox['time'] = 1` for Cox, which is essentially the same as the ground truth's interpretation of a 'fixed voyage duration'. While conceptually it's a limitation of the dataset, it follows the ground truth's logical workaround for ""voyage as time-to-event"".
*   **Cox Model Formula:** The generated code uses a direct `formula` string for the `CoxPHFitter`. While this is syntactically correct for `lifelines`, it makes dynamic variable selection harder and less robust than the ground truth's approach of creating a list of features. More importantly, it lists `Pclass_2` and `Pclass_3` when typically for one-hot encoding with `drop_first=True` (which the generated code uses), one or more (e.g., `Pclass_1`) would be the reference. This could lead to perfect multicollinearity issues if `Pclass_1` is implicitly treated as the reference. The ground truth explicitly handles this (`exclude 'Pclass_3' as it's the reference category`).
*   **Propensity Score Matching:**
    *   The definition of `treatment` is problematic (`(df_ps['Pclass_2'] == 0) & (df_ps['Pclass_3'] == 0)`), which effectively means Pclass 1. This is a reasonable choice for a comparison group.
    *   The matching logic `control_neighbors = np.abs(control['propensity'].values - treated['propensity'].values[:, None]).argmin(axis=1)` performs matching, but it's a very basic nearest-neighbor without replacement and doesn't handle caliper or common support, which are standard in PSM.
    *   Crucially, it uses `iloc` to get matched controls but doesn't remove the matched control from the pool of controls, which can lead to controls being matched multiple times (matching with replacement implicitly, but without explicitly doing so). This is less robust than using `sklearn.neighbors.NearestNeighbors` as in the ground truth, which allows for more controlled matching.
    *   It also doesn't perform covariate balance checks (e.g., SMD), which is a crucial step in PSM to validate the matching effectiveness. The ground truth includes a manual SMD calculation.
*   **Bootstrap Resampling:** The bootstrap loop itself is correctly implemented for `CoxPHFitter`. However, it hardcodes the formula `Sex + Age` for the bootstrapped models, which significantly limits its scope compared to the full model. The ground truth bootstraps the coefficients of the *base Cox model* identified earlier, which is more comprehensive.
*   **Bayesian Analysis:**
    *   The Bayes model only includes 'Sex' and 'Age', while the problem asks for modeling ""survival factors"" which would imply a broader set of features.
    *   The standardization of `X_data` is done *inside* the `with Model()` block, which is unusual and should typically be done beforehand.
    *   The `Binomial` likelihood has `n=1`, meaning it's a Bernoulli distribution, which is correct for binary outcomes.
    *   It uses `pymc3` correctly for sampling and summary.
*   **Sensitivity Analysis/Model Validation:** The generated code only provides Bayesian prior sensitivity analysis, which is one aspect. It completely misses imputation strategy sensitivity and conceptual model specification sensitivity as done in the ground truth. The prompt generally asks for model validation and sensitivity analysis, which implies a broader scope than just prior sensitivity.

**Code Quality:**
*   **Imports:** Mostly correct, but `multivariate_logrank_test` is imported but not used. `pymc3` is used as `Model`, `Normal`, etc., which implies `from pymc3 import *`, generally discouraged. The ground truth uses `import pymc3 as pm` and then `pm.Model`, which is standard.
*   **Variable Naming:** `df_cox` and `df_ps` are used correctly for creating specific dataframes for different sections.
*   **Structure:** Code is structured by the numbered requirements, which is good.
*   **Missing `plt.show()`:** Several plots (`plot_trace`, `plot_posterior`) are generated in the ground truth and displayed using `plt.show()`, which is good practice to ensure plots appear. The generated code misses this for `plot_trace` in the Bayesian section, meaning the plot might not be shown unless the environment auto-renders.
*   **Warning handling:** No explicit warning suppression like the ground truth. This is not critical for functionality but improves readability during execution.

**Completeness:**
*   **Advanced Statistical Tests:** The prompt asks for multivariate analysis (partially covered by Cox), interaction effects (covered), and model diagnostics. The Cox model diagnostics (like proportional hazards test) are not fully shown or interpreted as thoroughly as in the ground truth (especially considering the constant `T` issue).
*   **Propensity Score Matching:** Misses caliper, common support, and most importantly, covariate balance checks.
*   **Bootstrap:** The bootstrap focuses only on `Sex` and `Age` rather than the full model's coefficients.
*   **Bayesian Analysis:** Implements a simpler model than expected (only two features).
*   **Model Validation/Sensitivity:** It's incomplete, only focusing on prior sensitivity for Bayesian. It completely misses imputation strategy sensitivity and overall model specification sensitivity/robustness checks which are crucial for ""comprehensive model validation"".

In summary, the generated code provides a functional starting point but lacks the robustness, thoroughness, and statistical rigor of an expert-level solution. The details of implementation for PSM and the limited scope of bootstrap/Bayesian models are significant weaknesses.",deepseek_r1,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89, 70...",9,"The generated code is very good and correctly addresses almost all aspects of the task.

**Strong Points:**
*   **Correct DataFrame Creation:** The DataFrame is created accurately.
*   **Correct Correlation Calculation:** The Pearson correlation coefficient is calculated and printed correctly.
*   **Effective Scatter Plot:** The scatter plot correctly uses `Attendance_Rate` on the x-axis and `Final_Score` on the y-axis. It correctly colors points by 'Subject' and includes a title, axis labels, and a legend. Using `sns.set()` and `plt.scatter` in a loop achieves the desired visual outcome.
*   **Correct Grade Assignment:** The use of `pd.cut` for assigning `Overall_Grade` is an elegant and correct solution for categorical binning based on scores. The `reindex` and `fill_value=0` properly handles cases where a grade category might have no students, ensuring a consistent output.
*   **Correct Grade Distribution:** The count of students in each grade category is correctly calculated and printed.

**Areas for Minor Improvement (Why not a 10):**
*   **Correlation Printing Format:** The task asked for specific precision in printing the correlation ('4f' in the ground truth, '3f' in generated). While `0.3f` is perfectly reasonable, `0.4f` was specified in the ground truth.
*   **Seaborn Usage for Plotting:** While the loop with `plt.scatter` works, using `sns.scatterplot` with the `hue` parameter directly (as in the ground truth) is the more idiomatic and often preferred way to handle this type of categorical coloring in Seaborn, as it often produces slightly cleaner legends and color differentiations automatically. However, the current implementation achieves the required visual output, so this is a minor stylistic/efficiency point.
*   **Imports:** `from scipy import stats` imports the entire `stats` module, whereas `from scipy.stats import pearsonr` directly imports the needed function, which is slightly more precise, though either is fine.

Overall, the code is highly functional and fulfills the requirements effectively.",deepseek_r1,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create DataFrame and set 'Date' as index
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', 
                            '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']),
    'Ticker': ['TECHCO']*10,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155....",10,"The generated code is excellent and correctly addresses all requirements of the task.

**Strong points:**
- **Correctness:** All calculations (Daily Return, SMA_3Day, SMA_5Day) are computed accurately.
- **DataFrame Creation:** The DataFrame is set up correctly with the 'Date' column as the index in a concise manner.
- **Plotting:** The plot is generated as requested, including the 'Close' price, both SMAs, a descriptive title, labeled axes, and a legend.
- **Bullish Crossover Identification:** The logic for identifying bullish crossovers (SMA_3Day crossing above SMA_5Day) is precise and correctly implemented using `shift()` and boolean indexing.
- **Code Quality:** The code is well-structured, readable, and includes necessary imports. The use of `.round(2)` on `Daily_Return` and SMAs is a thoughtful touch for presentation, although not strictly required by the prompt, it doesn't detract from correctness.
- **Completeness:** Every aspect of the prompt, from data creation to calculations, plotting, and identification of specific market signals, is covered comprehensively.
- **Output:** The bullish crossover dates are printed clearly and in the expected format.",deepseek_r1,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', ...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong points:**
- **Correct DataFrame creation:** The DataFrame is created accurately with the given data.
- **Correct total sales revenue calculation:** It correctly calculates `Revenue` for each transaction and then aggregates it by `Product`.
- **Correct horizontal bar chart generation:** The chart is generated as requested, with appropriate labels and title.
- **Correct average order value calculation:** It correctly calculates `total_revenue_all` and divides by the number of unique `OrderID`s.
- **Correct identification of repeat customers:** It accurately uses `value_counts()` and filtering to find customers with more than one purchase and prints their details.
- **Necessary Imports:** All required libraries (`pandas`, `matplotlib.pyplot`) are correctly imported.

**Areas for minor improvement:**
- **Output formatting for repeat customers:** While correct, the output for repeat customers could be slightly more aligned with the ground truth's tabular format for better readability (e.g., using `print` with f-strings and aligning columns or just printing the series directly and letting pandas format it).
- **Chart size:** The `plt.figure()` call in the generated code doesn't specify a `figsize`, which can sometimes lead to less optimal plot dimensions depending on the default settings. The ground truth includes `figsize=(10, 6)` for better readability. This is a minor aesthetic point.
- **No prints for intermediate steps:** The ground truth includes print statements for the original DataFrame and product revenue, which can be helpful for debugging or understanding the flow. The generated code strictly focuses on the final requested outputs, which is fine, but adding these could improve user experience during development.

Overall, the generated code is very good and effectively solves the problem. The minor suggestions are mostly about presentation and slight enhancements rather than functional correctness.",deepseek_r1,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd
import numpy as np

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Create DeviceType_Numeric
df['DeviceType_Numeric'] = df['DeviceType'].map({'Desktop': 0, 'Mobile': 1})

# ...",10,"The generated code is excellent. It correctly defines the initial DataFrame, creates the ""DeviceType_Numeric"" feature with the specified mapping, and accurately calculates ""PagesPerMinute"" while robustly handling potential division by zero using `replace` and `fillna`. The final feature selection is also correct, and the head of the resulting DataFrame is printed as requested. The code is concise, clean, and directly addresses all requirements of the task. Proper imports are included, and the structure is logical.",deepseek_r1,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26, 45, 28]
}

df = pd.DataFrame(data)...",9,"The generated code correctly defines the DataFrame and performs the outlier detection as requested. It calculates the mean and standard deviation, identifies outliers based on the 1.5 standard deviation threshold, and iterates through the identified outliers to print the required information.

Strengths:
- Correct DataFrame creation.
- Accurate calculation of mean and standard deviation.
- Correct application of the 1.5 standard deviation rule for outlier identification.
- Correctly prints the SensorID, Timestamp, and TemperatureCelsius for each outlier.

Areas for minor improvement:
- The output format for outliers could be slightly more descriptive, similar to the ground truth (e.g., adding labels like ""SensorID:"", ""Timestamp:"", ""Temperature:"").
- While not explicitly required and doesn't affect correctness for this specific task, converting the 'Timestamp' column to datetime objects using `pd.to_datetime` is good practice for time-series data and improves flexibility for future operations, as shown in the ground truth.
- The ground truth includes informative print statements for the calculated mean, std, and bounds, which makes the output more user-friendly for understanding the thresholds used. The generated code only prints the outliers.",deepseek_r1,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [10, 5, 25, 15, 8, 30, 9,...",10,"The generated code is correct, complete, and of good quality. It precisely defines the DataFrame, calculates the `EngagementScore` as specified, correctly identifies the `PostType` with the highest average `EngagementScore` for each `Platform`, and prints the results in a clear format.

Strong points:
- **Correctness:** All calculations and logical steps are accurate.
- **Efficiency:** The use of `groupby().idxmax()` is an efficient way to find the row with the maximum value within each group.
- **Readability:** The code is concise and easy to understand.
- **Completeness:** It addresses all parts of the task requirement.
- **Output Format:** The printing format is clear and uses f-strings for better readability, and formats the score to two decimal places as implied by typical benchmark requirements (though not explicitly stated for formatting in the prompt).",deepseek_r1,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu"", ""Injury"", ...",9,"The generated code is highly correct, complete, and demonstrates good quality. It precisely defines the DataFrame, accurately calculates 'LengthOfStay', correctly aggregates average length of stay and patient count per diagnosis, prints this summary, and identifies the diagnosis with the longest average stay.

Strong points:
- **Correctness**: All calculations (LengthOfStay, average stay, patient count) are performed accurately.
- **Completeness**: All four requirements of the task (DF creation, LengthOfStay, grouped stats, printing, longest stay identification) are fully met.
- **Conciseness**: The code is very direct and uses efficient pandas operations (e.g., `groupby().agg()`, `idxmax()`).
- **Readability**: Variable names are clear and descriptive.

Areas for minor improvement (prevented a 10):
- **Output Formatting**: While the `to_string(index=False)` provides a clean tabular output, it doesn't precisely match the explicit, formatted printing requested by the ground truth's `print(f""{row['Diagnosis']:<15} ..."")` for ""printing this information (Diagnosis, Average LengthOfStay, Patient Count)"". The generated's output is acceptable for displaying the data but lacks the exact custom formatting.
- **Function Encapsulation**: The ground truth uses a function (`analyze_patient_admissions`) to encapsulate the logic, which is generally good practice for larger scripts as it aids reusability and organization. The generated code is direct-run script-style. For this specific task, it's not a major flaw, but in a production environment, functions are often preferred.
- **Comments/Explanations**: The ground truth includes more verbose comments, explaining each step. While the generated code is clear enough without them for this problem, adding comments for complex steps is a good habit.",deepseek_r1,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",9,"The generated code correctly performs all the required tasks: defining the DataFrame, calculating the 'WeightedRating' as a new column, and identifying and printing the title and weighted rating of the movie with the highest weighted rating. The code is concise and leverages pandas vectorized operations effectively.

One minor area for improvement, which is not a functional error but a good practice, is handling the edge case where `total_votes` might be zero to prevent a `ZeroDivisionError`. While not strictly necessary for this specific dataset, robust code would include such a check. The printing format for the final output could also be more descriptive, similar to the ground truth (e.g., ""Title: X, WeightedRating: Y""). However, the current output still provides the requested information.",deepseek_r1,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 10000, 8000]
}
df = pd...",9,"The generated code correctly defines the DataFrame, calculates the average ""PriceUSD"" and ""Mileage"" per ""Brand"", and prints the resulting grouped DataFrame. It achieves all the requirements of the task.

Strengths:
- **Correctness:** The `groupby().agg()` method is used effectively to calculate the mean of both 'PriceUSD' and 'Mileage' for each brand.
- **Conciseness:** The solution is quite concise, combining the grouping, aggregation, and resetting of the index into a single line.
- **Completeness:** All parts of the task (DataFrame creation, calculations, new DataFrame creation, printing) are addressed.

Areas for minor improvement:
- **Output Clarity:** While `reset_index()` makes 'Brand' a column, the ground truth code used named aggregations (`Average_PriceUSD`, `Average_Mileage`) which results in clearer column names in the output DataFrame. The generated code's column names remain `PriceUSD` and `Mileage`, which might be slightly ambiguous when displayed as averages. This is a minor stylistic choice and doesn't impact correctness.
- **Printed Output:** The ground truth included a descriptive header before printing the DataFrame, which can improve user understanding. This is a small detail but good practice.",deepseek_r1,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63, 83],
    ""De...",9,"The generated code correctly performs all the requested tasks: defining the DataFrame, adding the ""TotalStats"" column, identifying the Pokemon with the highest ""TotalStats"", and calculating the average ""Speed"" for each ""Type1"". The code is concise and uses appropriate pandas functions.

One minor improvement for completeness could be to explicitly print the labels for the outputs, particularly for ""Pokemon with highest TotalStats"" and the average speeds, similar to the ground truth. While the code correctly prints the values, adding descriptive labels makes the output clearer for the user. For instance, `print(f""Pokemon with highest TotalStats: {df.loc[df['TotalStats'].idxmax(), 'Name']}"")`. Also, the loop for printing average speed is functional, but printing the `average_speed` Series directly is often cleaner and idiomatic for pandas output unless specific formatting (like the :.2f) is strictly required per item. However, the current output is still correct.",deepseek_r1,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.neighbors import NearestN...",7,"The generated code addresses most of the requirements but has several areas for improvement in terms of robustness, completeness, and adherence to advanced methodologies.

**Strong Points:**
*   **Comprehensive Feature Engineering Attempted:** Included family size, isolated status, title extraction, ticket group size, and fare per person.
*   **Multiple Clustering Algorithms:** Implemented K-means, Hierarchical, DBSCAN, and GMM.
*   **Dimensionality Reduction:** Utilized PCA, t-SNE, and UMAP for visualization.
*   **Visualization:** Included scatter plots for clusters in reduced dimensions and bar plots for cluster characteristics.
*   **Cluster Stability Analysis:** Included a basic stability check using ARI for K-means.
*   **External Validation:** Used Adjusted Mutual Information score.

**Areas for Improvement/Missing Aspects:**

1.  **Feature Engineering Robustness:**
    *   The `Cabin` feature processing (`df['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'Unknown')`) can lead to issues if there are entries like 'U123'. The ground truth (using `fillna('U').apply(lambda x: x[0])` and `HasCabin`) is more robust.
    *   Missing NameLength and TicketPrefix from the ground truth feature engineering, and interactions like Age\_Class, Fare\_Class, FarePerPerson\_Class. These were explicitly requested advanced features.

2.  **Preprocessing Pipeline:** The generated code performs preprocessing steps somewhat manually (`SimpleImputer`, `OneHotEncoder`, `StandardScaler`). While functional for this script, it lacks the robustness and maintainability of `sklearn.pipeline.Pipeline` and `ColumnTransformer` from the ground truth, which is best practice for handling mixed data types. The task explicitly requests ""proper preprocessing and scaling.""

3.  **Optimal Number of Clusters:**
    *   Only `gap_statistic` was used, which is good, but the task asked for ""multiple validation metrics (silhouette score, elbow method, gap statistic, calinski-harabasz index)"". The generated code calculates silhouette score implicitly by using gap statistic but doesn't explicitly plot or evaluate other metrics (Elbow, Calinski-Harabasz, Davies-Bouldin) for comparison across algorithms, as the ground truth does. This makes the determination of `optimal_k` less comprehensive.
    *   The optimal-k is applied to all clustering algorithms, which might not be truly optimal for hierarchical or GMM, although it's a common simplification.

4.  **DBSCAN Parameter Tuning:** The `epsilon` for DBSCAN is set based on the 95th percentile of distances. While a heuristic, it's not the robust k-distance graph approach often used to determine `eps`. This makes DBSCAN less reliable and potentially producing poor results without a clear understanding of its density landscape. The ground truth points this out as a consideration even when using arbitrary values.

5.  **Cluster Analysis Completeness:**
    *   Visualizations of cluster characteristics (e.g., violin plots for numerical features, count plots for categorical features) are missing, which are crucial for interpreting segments beyond just mean/median values. The generated code uses a heatmap, which is good but not as informative as detailed distributions.
    *   `df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, errors='ignore', inplace=True)` removes original features that would be useful for interpreting clusters against original values, even if they were used to create new features. The ground truth maintains a separate `df_original` for this purpose.

6.  **""Feature Importance"" in Clustering Context:** The use of `RandomForestClassifier` for ""feature importance"" here is somewhat misplaced. Feature importance is for supervised learning (predicting `Survived`). While interesting, it doesn't directly describe *which features define the clusters themselves* in an unsupervised context. The heatmap of scaled feature means is a better representation for cluster characterization.

7.  **Consensus Clustering:** The prompt asked for ""consensus clustering,"" which typically involves combining results from multiple clustering algorithms or multiple runs of a single algorithm to derive a more stable and robust partitioning. The generated code included stability analysis, but not explicit consensus clustering. The ground truth only did a simplified stability analysis.

8.  **Code Structure and Imports:** Imports are mostly correct, but the execution flow is logical and follows a good analytical path. Warnings suppression and plot style setting are absent, which contributes to a slightly less polished output.",deepseek_r1,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dystopian""],
    ""Stock"": [...",9,"The generated code correctly performs all the required tasks: it defines the DataFrame, calculates the count of books and average price per author, and identifies the author with the most books in stock.

**Strong points:**
*   **Correct DataFrame Creation:** The DataFrame is accurately defined as requested.
*   **Correct Aggregation:** The `groupby` and `agg` functions are used effectively to calculate the count of books (`BookID`, 'count') and average price (`Price`, 'mean').
*   **Accurate Stock Calculation:** The sum of stock per author and identification of the author with the maximum total stock is correctly implemented using `groupby` and `idxmax()`.
*   **Clear Output:** The output for author statistics and the author with the most stock is clear and easy to understand.
*   **Concise Code:** The solution is quite concise, especially the way `max_stock` is identified.

**Areas for Improvement (minor):**
*   **Average Price Rounding:** The ground truth does the rounding at the printing stage `:.2f` which is generally preferred for calculations, whereas the generated code rounds at the aggregation stage using `lambda x: round(x.mean(), 2)`. While not strictly wrong here, it can sometimes lead to loss of precision if subsequent calculations were to be performed on `Average_Price`. It's often better to keep numerical precision until the final display.
*   **Output Formatting for Author Statistics:** The ground truth code used a loop and f-strings for more flexible and formatted output for author statistics, including left-padding for alignment and a dollar sign for price. The generated code uses `to_string(index=False)`, which is functional but less customisable in terms of individual column formatting (e.g., currency symbols, specific decimal places per column). For this particular task, both are acceptable, but the ground truth's approach offers more control over presentation.
*   **Separators:** The ground truth adds separators (`---`, `===`) to improve readability and visually distinguish different sections of the output, which is a good practice for user experience.

Overall, the generated code performs very well and completely fulfills the requirements of the task. The minor points are mostly stylistic or related to best practices for numerical precision/display rather than correctness of the core logic.",deepseek_r1,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, learning_curve, validation_curve, cross_val_score
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,...",6,"The generated code attempts to address most of the requirements but falls short in several areas, particularly concerning the `OutlierRemover`, the comprehensiveness of the ensemble methods, the evaluation metrics, and the display of results.

**Strengths:**
*   **Basic Structure:** Follows a logical flow of data loading, preprocessing, model building, and evaluation.
*   **Multiple Models:** Includes Random Forest, Gradient Boosting, XGBoost, SVM, and Neural Network.
*   **Ensemble Methods:** Implements Voting and Stacking.
*   **Hyperparameter Tuning:** Uses GridSearchCV.
*   **Data Preprocessing Pipeline:** Integrates imputation, outlier handling, polynomial features, and scaling.
*   **Model Interpretability:** Attempts SHAP.
*   **Calibration:** Includes calibration curves.
*   **Statistical Testing:** Performs paired t-tests.

**Weaknesses and Areas for Improvement:**

1.  **Preprocessing - Outlier Treatment:** The `OutlierRemover` custom transformer is implemented using IQR. While it applies clipping, it re-calculates `q1`, `q3`, `iqr`, and `bounds` for *each column within the `transform` method*, which means it's deriving parameters from the *transformed* data in a pipeline rather than from the *fitted* data. It should be `fit` on `X` (training data) to calculate the bounds once, then `transform` should use those stored bounds. The current implementation essentially re-calculates bounds on subsets of data in the pipeline, which is incorrect. The Ground Truth uses `OutlierCapper` which correctly fits bounds and then clips.
2.  **Ensemble Methods - Blending:** Blending is a distinct requirement specified in the prompt. The generated code does not include any implementation for blending.
3.  **Advanced Evaluation:**
    *   **Evaluation Metrics:** The `GridSearchCV` and `cross_val_score` for base models primarily use 'accuracy'. The prompt specifically asked for 'roc_auc' as a key metric (and mentioned multiple metrics).
    *   **Learning Curves & Validation Curves:** The generated code attempts to plot learning curves and validation curves, but the implementation is incorrect. `%matplotlib inline` is not proper Python code for plotting inside a script, and the `shap.initjs()` call before plotting seems misplaced and unnecessary for static plots. Also, the current learning/validation curve plots are not well-formatted or informative compared to the ground truth. They are also missing axis labels and legends.
    *   **Bootstrap Sampling:** The code mentions and implies using bootstrap sampling for confidence intervals but does not explicitly implement the bootstrapping logic to calculate these intervals for metrics. The heatmap of p-values is a good addition for statistical testing, but the bootstrap for CIs is missing.
    *   **Prediction Confidence Intervals:** There's no explicit calculation or display of prediction confidence intervals for individual predictions. The prompt asked for this in the dashboard.
4.  **Model Interpretability (SHAP):** The SHAP usage is problematic. `explainer = shap.Explainer(best_models['rf'].named_steps['model'], masker=X_train)` is an attempt, but `X_train` should be the reference dataset for the explainer, not passed directly as a masker for `X_test` in the call `explainer(X_test)`. More importantly, `shap_values.values[:,:,1]` is typically for multi-output models where `shap_values` is actually `shap_values(X_test)`. For classification, `shap_values` object has correct attributes. The indexing for binary classification should typically be `shap_values[:, 1]` after getting the raw values from `explainer.shap_values()`. The ground truth example correctly handles `shap_values[1]` for binary classification. The `masker` argument in `shap.Explainer` is correctly used in the general sense, but the specific usage may need adjustment based on data types (e.g., `shap.maskers.Independent(X_train)` for non-tree models, or simple data frame for tree models).
5.  **Dashboard:** The prompt asks for a ""comprehensive model comparison dashboard with performance metrics, feature importance, and prediction confidence intervals."" The generated code outputs a series of independent plots (CV accuracy boxplot, p-value heatmap, calibration curve) but no consolidated dashboard or summary table beyond the individual plots. Feature importance from SHAP is plotted separately, not integrated into a dashboard alongside performance metrics.
6.  **Code Quality:**
    *   **Imports:** `enable_iterative_imputer` is unnecessary as `IterativeImputer` is part of `sklearn.impute`.
    *   **Redundant Imputers:** Mentions `SimpleImputer`, `KNNImputer`, but only `IterativeImputer` is used in the pipeline.
    *   **Plotting:** The plots are basic (no labels, titles are simple). `plt.figure()` is called multiple times without `plt.show()`, leading to overlapping or suppressed plots. The `shap.initjs()` should be called once, usually at the beginning when intending to display interactive plots outside a script context (e.g., in notebooks). For static plots, it's not directly doing anything impactful in a script.
    *   **Error Handling:** Lacks robust error handling (e.g., for missing `water_potability.csv` or issues with `xgboost` import).
    *   **Warning Suppression:** The prompt mentioned suppressing warnings for cleaner output, which the generated code doesn't do.

Overall, the code demonstrates an understanding of many requirements but lacks the depth and correctness for several advanced features, particularly in the evaluation and presentation aspects, and has minor implementation issues in preprocessing and SHAP.",deepseek_r1,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, KNNImputer
from sklearn.linear_model import BayesianRidge, LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error
from sklearn.ensemb...",6,"The generated code attempts to address most of the requirements but falls short in implementation depth and correctness for several advanced statistical modeling and diagnostic aspects.

**Strengths:**
*   **Missing Data Imputation:** Correctly implements and compares KNN, IterativeImputer (MICE-like), and provides basic comparison.
*   **Dimensionality Reduction:** Includes PCA and Factor Analysis.
*   **Anomaly Detection:** Correctly uses Isolation Forest, Local Outlier Factor, and One-Class SVM.
*   **Hypothesis Testing:** Includes MANOVA, permutation test, and bootstrap confidence intervals, though some implementations are simplistic.
*   **Basic Model Building:** Logistic Regression (L1) is implemented.

**Weaknesses and Areas for Improvement:**

1.  **Missing Data Imputation Comparison:** While showing means is a start, a true comparison of effectiveness would involve evaluating model performance on data imputed by different methods (e.g., using a simple logistic regression and comparing metrics). The prompt specifically asked to compare their *effectiveness*.
2.  **Multivariate Statistical Models (GAM):** The `statsmodels.gam.api` is used for GLMGam, which is a specific type of GAM (Generalized Linear Model with a smooth term). However, the implementation for `BSplines` is very generic (`df=[5]*X_train_scaled.shape[1]`) and doesn't explicitly link to the input features properly, potentially leading to errors or sub-optimal models. The output summary is printed, but there's no prediction or evaluation of the GAM. The requested `PyGAM` library was not used.
3.  **Bayesian Logistic Regression (PyMC3):**
    *   `pm.glm.GLM(X_train_scaled, y_train, family=pm.glm.families.Binomial())` is used directly, which simplifies GLMs but doesn't allow for custom prior specification, which is a key aspect of Bayesian modeling often demonstrated.
    *   The `trace` is sampled, and `az.summary(trace)` is printed, but there's **no prediction** on the test set or evaluation of the Bayesian model's performance (e.g., ROC AUC, classification report), which was explicitly required.
4.  **Robust Statistical Methods:** The prompt asked to apply robust statistical methods to handle outliers and non-normal distributions. While `RobustScaler` is used, the core of this requirement was to *apply* a robust statistical *model* (e.g., RLM in statsmodels, as in the ground truth) if applicable. Simply scaling data isn't a robust *method* in the modeling sense for the task.
5.  **Model Diagnostics:**
    *   For Logistic Regression, only `logit_model.summary()`, `qqplot(residuals)`, and scatterplot of `fittedvalues` vs `residuals` are shown. This is basic.
    *   It misses **influence measures** (Cook's distance, leverage plots), which are critical for model diagnostics, especially for identifying influential observations. `influence_plot` was not used.
    *   **Goodness-of-fit tests** for logistic regression (e.g., Hosmer-Lemeshow, or simply interpretation of deviance/AIC/BIC from the GLM summary, along with ROC curves as done in the ground truth) are largely missing in terms of explicit discussion or implementation, beyond printing the logit summary.
6.  **Advanced Hypothesis Testing (Permutation Test, Bootstrap CI):** While implemented, the example for `permutation_test` uses `scipy.stats.permutation_test`, which is relatively new and less common for manually crafted permutation tests for specific statistics (like mean difference) as asked in the ground truth, particularly for demonstrating the concept. The bootstrap CI also uses `scipy.stats.bootstrap` directly, which is correct, but simpler examples often show the manual loop to illustrate the process. More importantly, the use of `dropna()` in the ground truth for permutation/bootstrap helps ensure data integrity for these statistical tests, which is missing in the generated code and could cause issues if NaNs were present.
7.  **Visualizations:** The generated code provides some basic scatter plots and a heatmap. However, it explicitly misses:
    *   **Q-Q plots of residuals** (done, but could be made more prominent).
    *   **Leverage plots** (not included).
    *   **Diagnostic dashboards:** The requested ""diagnostic dashboards"" imply a more structured, comprehensive set of plots to evaluate the models, which is partially addressed by the 2x2 subplot but could be more tailored to the specific model (e.g., influence plots for regression, ROC curves, calibration plots for classification).
    *   More context-specific visualizations like PCA explained variance plot or GAM partial dependence plots are absent.
    *   The visualization part is less ""advanced statistical visualizations"" and more ""standard EDA plots"".

8.  **Imports:** Some imports like `patsy` might be needed for GAM in `statsmodels` (though the code tries to avoid via direct array input). `multipletests` is imported but not used.

9.  **Data Loading:** The `df.dropna(subset=['Potability'])` is dangerous as 'Potability' is the target variable. Dropping rows with missing target values is fine, but the original task refers to missing values in the *features*, which the imputers then handle. The ground truth loads `water_potability.csv` as is and then separates `X` and `y`, allowing imputers to handle NaNs in `X`. The generated code assumes `X` might have NaNs, but `df.dropna` could prematurely remove rows relevant for imputation.

10. **Code Structure and Readability:** The ground truth code provided much clearer section headers and print statements, making it easier to follow the execution flow and understand which part addresses which requirement. The generated code is more monolithic.",deepseek_r1,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.manifold import TSNE
import umap.umap_ as umap
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from scipy.cluster.hier...",6,"The generated code attempts to address most requirements but falls short on implementation details and comprehensive approach compared to the ground truth.

**Strengths:**
- **Feature Engineering:** Implements some advanced features like ratios and risk scores (`Sulfate_Cl_ratio`, `Hardness_TDS_ratio`).
- **Multiple Clustering Algorithms:** Uses KMeans, Agglomerative, DBSCAN, GMM, but Spectral Clustering is missing.
- **Dimensionality Reduction:** Includes PCA, t-SNE, UMAP, and Factor Analysis.
- **Cluster Validation:** Computes Silhouette, Davies-Bouldin, and Calinski-Harabasz scores.
- **Anomaly Detection:** Uses Isolation Forest.
- **Visualization:** Attempts 3D plots and a radar chart using Plotly.

**Weaknesses and Areas for Improvement:**
1.  **Completeness (Task Requirements):**
    *   **Optimal Number of Clusters:** While the code calculates metrics for a range of K, it doesn't explicitly determine and *set* an `optimal_k` based on these metrics for subsequent clustering. The ground truth visually plots these metrics for interpretation. The generated code implicitly uses `n_clusters=3` for consensus and KMeans, but the derivation is not shown or justified from the calculated metrics.
    *   **DBSCAN Parameter Tuning:** The generated code calculates `dbscan_clusters` for varying `eps` but doesn't use this information to determine an optimal `eps` and `min_samples` (e.g., using a k-distance graph) for actual DBSCAN execution. The ground truth provides a plot to guide this.
    *   **Spectral Clustering:** This algorithm is listed in the prompt but missing from the generated code entirely.
    *   **Cluster Stability Analysis:** The bootstrap resampling for consensus clustering is initiated, but the dendrogram for interpreting stability is not actually plotted. The consensus matrix is computed, but its use for stability assessment (e.g., visual inspection via dendrogram) is incomplete. The ground truth provides detailed steps for consensus clustering including the dendrogram.
    *   **Water Quality Profiles:** While `cluster_profiles` are generated, the analysis and interpretation are not explicitly printed or detailed enough to constitute ""analyze water quality profiles for each cluster including chemical composition patterns, contamination risks, and potability rates"" as comprehensively as the ground truth.
    *   **Dashboard:** The Plotly figure generation is incomplete. The `make_subplots` setup is incorrect and leads to an empty or malformed plot. The radar chart setup seems off. The ground truth generates multiple distinct interactive plots for each DR technique and a proper radar chart.
    *   **External Validation/Domain Knowledge:** This section is completely missing.
    *   **Anomaly Detection:** While Isolation Forest is used, the interpretation of results (e.g., printing indices of anomalies) is less structured. The ground truth also explicitly implemented a ""distance from centroid"" method, which was a good and common approach to include.

2.  **Code Quality & Correctness:**
    *   **Imputation:** Only 'ph', 'Sulfate', 'Trihalomethanes' are imputed. The original dataset has missing 'Chloramines', 'Organic_carbon', 'Solids', 'Turbidity' in some rows. The ground truth imputes all numeric columns.
    *   **Feature Engineering:** The engineered features in the generated code are simpler than in the ground truth. The ground truth included more sophisticated indices like 'pH_Deviation', 'pH_Quality_Score', 'Hardness_Quality_Score', 'Contamination_Risk_Score', and a more comprehensive 'WQI_Overall'. The generated code's ratios might suffer from division by zero if 'Chloramines' or 'Solids' are zero (though `df[Chloramines]` values in this dataset are usually non-zero, it's a general robustness point).
    *   **Plotly Subplots:** The `make_subplots` call in Plotly is incorrect for the intended visualizations. `go.Scatter3d` needs 'scene' type for the subplot, which is only assigned to (1,1). The last trace is also added to a non-existent subplot (2,2) with a 'scatter3d' type. The `name=polar_df['index']` for `go.Scatterpolar` will not work as intended for multiple traces. The ground truth generates individual, working Plotly figures for each visualization.
    *   **Consensus Clustering:** The `consensus` matrix creation needs to be careful about what is stored and how it's used. `linkage(consensus, 'average')` might be problematic if `consensus` is directly a co-occurrence matrix; usually, it's done on a distance matrix derived from co-occurrence or if `consensus` matrix is already symmetric and represents similarities. The ground truth converts to a distance matrix (`1 - similarity_matrix`) before hierarchical clustering for the dendrogram, which is the standard approach.

In summary, the generated code outlines the steps but often provides a less robust, less complete, or incorrect implementation of those steps compared to the detailed requirements and the ground truth. A significant portion of the analysis and visualization is missing or faulty.",deepseek_r1,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression

df = pd.read_csv('winequality-red.csv', delimiter=';')

corr_matrix = df.corr()
quality_corr = corr_matrix['quality'].drop('quality').sort_values(ascending=False)

top3_pos = quality_corr.head(3)
top3_neg = quality_corr.tail(3)[::-1]

print(""Top 3 Positively Correlated Features:"")
print(top3_pos)
print(""\nTop 3 Negatively Correlated Features:"")
print(top3_neg)

p...",8,"The generated code correctly addresses most of the task requirements.

**Strong Points:**
- Correctly loads the dataset.
- Accurately calculates the correlation matrix.
- Identifies and prints the top 3 positively and negatively correlated features with 'quality'.
- Effectively generates a correlation matrix heatmap with annotations and a suitable colormap.
- Creates scatter plots for the most positively and negatively correlated features against 'quality' and correctly includes linear regression lines using `sns.regplot`, which simplifies the process compared to manual `linregress` and plotting.
- Uses proper imports (`pandas`, `matplotlib.pyplot`, `seaborn`, `sklearn.linear_model`).

**Areas for Improvement (Why not a 10):**
- **Minor Issue with Negative Correlation Sorting:** When selecting `top3_neg = quality_corr.tail(3)[::-1]`, `[::-1]` is used to reverse the order, but `tail()` already returns the smallest (most negative) values. If the goal is to get the top 3 *most negative* correlations (i.e., those closest to -1), then `quality_corr.nsmallest(3)` would be more direct and robust, or simply `quality_corr.tail(3)` if `quality_corr` was sorted `ascending=True` initially for negative values. Given the current `sort_values(ascending=False)`, `tail(3)` gets the smallest (most negative) values, but the `[::-1]` reverses their display order, which might be confusing. For identifying the ""most negative"" value for plotting, `top3_neg.index[0]` will still point to the correct one (the one with the largest negative coefficient).
- **Hardcoded Delimiter:** The `delimiter=';'` is included in `pd.read_csv`. While this might be correct for a specific version of `winequality-red.csv`, the problem description simply states ""assume the CSV file is in the current directory"" and typically `pd.read_csv` handles comma-separated files by default. If the CSV is truly comma-separated, this could cause an error or misinterpretation. It's safer to either omit it if the default works, or confirm the delimiter from the dataset source. For this dataset, the delimiter *is* indeed ';', so it works, but it's a detail worth noting for general robustness.
- **R-squared Value on Plots:** The ground truth includes the R-squared value on the regression line labels, which is a good practice for interpreting the strength of the linear relationship. The generated code does not include this on the `regplot` labels. While `regplot` shows the confidence interval, R-squared is a direct measure of fit.

Overall, the code is very good and fulfills the core requirements effectively and concisely using `seaborn`'s `regplot`. The issues are minor refinements rather than fundamental correctness problems.",deepseek_r1,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency, mannwhitneyu

# Load and clean data
df = pd.read_csv('video_games_sales.csv')
df.dropna(inplace=True)

# Test Nintendo vs other publishers
nintendo = df[df['Publisher'] == 'Nintendo']['Global_Sales']
others = df[df['Publisher'] != 'Nintendo']['Global_Sales']
t_stat1, p1 = ttest_ind(nintendo, others, alternative='greater', equal_var=False)

# Test Action vs Sports genres
action = df[df['Genre'] == 'Action']['Global...",6,"The generated code attempts to address all parts of the task but falls short on data cleaning robustness, missing error handling, and lacks the required interpretation depth.

**Strengths:**
- Correctly identifies and uses the `ttest_ind`, `chi2_contingency`, and `mannwhitneyu` functions.
- Performs all requested statistical tests.
- Uses `alternative='greater'` for the Nintendo test and `alternative='two-sided'` for Mann-Whitney, as specified.
- Correctly identifies top 5 genres and platforms for the chi-square test.
- Provides a basic interpretation of results (Reject H0/Fail to reject H0).

**Weaknesses and Areas for Improvement:**

1.  **Data Cleaning (Critical):**
    *   **Over-aggressive `dropna()`:** `df.dropna(inplace=True)` at the beginning drops *any* row with *any* NaN. This is problematic because `User_Score` or other non-critical columns might have NaNs, leading to significant data loss for columns that are actually used in tests (e.g., `Global_Sales`, `Publisher`, `Genre`, `Platform`, `Year`).
    *   **`Year_of_Release` vs `Year`:** The task specifies `Year_of_Release`. The generated code uses `df['Year'] = pd.to_numeric(df['Year'], errors='coerce')`, which assumes a column named 'Year' and tries to convert it. The correct column name from the ground truth is `Year_of_Release`. This is a bug that would cause a `KeyError` or fail to convert the correct column if it exists.
    *   **Lack of targeted cleaning:** The ground truth shows a much more robust cleaning process, ensuring that key columns for tests (like `Global_Sales`, `Year_of_Release`, `Publisher`, `Genre`, `Platform`) are cleaned specifically, and `Year_of_Release` is converted to `int` early.

2.  **Robustness and Error Handling:**
    *   **Missing `try-except` for CSV:** No error handling if 'video_games_sales.csv' is not found.
    *   **No checks for empty groups:** The ground truth code includes checks like `if len(nintendo_sales) < 2 or len(other_publishers_sales) < 2:` before performing t-tests to prevent errors if a group has insufficient data. The generated code lacks these critical checks, which could lead to runtime errors if the data is sparse.
    *   **Contingency Table Validation:** The ground truth also checks if the `contingency_table` is empty or all zeros before performing the chi-square test, which is important for robustness.

3.  **Completeness of Interpretation:**
    *   The task requested displaying p-values and *interpreting results with significance level α=0.05*. While `Reject H0` or `Fail to reject H0` is a start, the ground truth provides much more detailed, human-readable interpretations (e.g., ""Nintendo games have significantly higher average global sales compared to all other publishers combined""). The generated code's interpretations are minimal.
    *   The `print` statements don't explicitly state the null vs. alternative hypotheses for each test, which is good practice for clarity.

4.  **Implicit Assumptions about Data Types:**
    *   The ground truth explicitly converts `Year_of_Release` to `int` and handles `User_Score` (though not directly used in the tests, it shows good cleaning practice). The generated code's `df['Year'] = pd.to_numeric(...)` attempts conversion but might miss other types or specific column names.

5.  **Code Structure and Readability:**
    *   The ground truth uses comments to clearly delineate each step and hypothesis test, making the code much easier to follow. The generated code is more compact but less descriptive.
    *   `ALPHA` constant is missing, making the significance level (0.05) magical numbers in `if p1 < 0.05` lines.

**Overall:** The code works *functionally* but lacks the robust data handling, error checks, and detailed interpretation expected from ""expert"" level code for a benchmark, leading to potential failures in real-world scenarios or with slightly varied datasets.",deepseek_r1,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

df = pd.read_csv('video_games_sales.csv')

df['Year'] = df['Year'].fillna(df['Year'].median())
df['Publisher'] = df['Publisher'].fillna('Unknown')
df['Platform'] = df['Platform'].fillna('Unknown')...",7,"The generated code addresses most of the requirements but has several areas for improvement in terms of correctness, completeness, and robustness.

**Strengths:**
*   Correctly loads the dataset.
*   Successfully creates the 'High_Sales' target variable.
*   Applies label encoding for categorical features.
*   Creates 'Total_Regional_Sales' and 'Years_Since_Release' features.
*   Splits the data into training and testing sets.
*   Trains a Random Forest classifier.
*   Evaluates the model using accuracy, precision, recall, F1-score, and confusion matrix.
*   Displays feature importance.
*   Includes necessary imports.

**Areas for Improvement:**

1.  **Missing Value Handling:**
    *   The generated code imputes `Year` with the median, which is reasonable.
    *   It imputes categorical columns (Platform, Genre, Publisher) with 'Unknown' and sales columns with 0. While this prevents errors, it's not always the best strategy for `Global_Sales` or `NA_Sales`, etc., as `Global_Sales` is used for the target variable and regional sales are features. The ground truth's approach of dropping rows where `Global_Sales` or regional sales are missing is often more robust for model training as these are critical numerical values.
    *   The prompt specifies ""Handle missing values."" The generated code's imputation strategy is a form of handling, but less ideal for sales figures than dropping for this specific task where sales are core features/target.

2.  **Feature Selection for Model Training:**
    *   The ground truth explicitly includes all original sales columns (`NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`) *in addition to* `Total_Regional_Sales` as features. The generated code *only* uses `Total_Regional_Sales` and drops the individual regional sales. This is a significant difference. While `Total_Regional_Sales` captures the sum, individual regional sales can provide nuanced information (e.g., a game with high NA sales but low EU/JP sales might behave differently than one with balanced sales). The prompt implies using all relevant features where appropriate.

3.  **Label Encoding:**
    *   There's a typo in the generated code: `Publisher_Publisher_encoded` which should likely be `Publisher_encoded`. This would lead to a `KeyError` when trying to select `Publisher_encoded` for `X`. (Corrected in the review, but as written, it's an error).
    *   The ground truth stores the `LabelEncoder` instances in a dictionary, which is good practice if one ever needs to inverse transform or use the same encoder on new data. The generated code creates new, unnamed `LabelEncoder` instances for each column.

4.  **Clarity and Robustness:**
    *   No error handling for file not found.
    *   No `random_state` for `RandomForestClassifier` (the generated code *does* include it, which is good, but the ground truth comment mentions it for general practice).
    *   The ground truth provides comments and print statements for clarity and debugging, which while not strictly required by the prompt's output format, contribute to good code quality.

5.  **Output Format (Minor):**
    *   The generated code simply prints the metrics and feature importance. The ground truth provides more descriptive headers (e.g., ""--- Model Evaluation ---"", ""--- Feature Importance ---"") and clearer formatting (e.g., f-strings for floats, explicit labels for confusion matrix axes).

To score higher, the code would need to:
*   Refine the missing value handling, especially for numerical sales figures, to align more closely with typical robust ML practices (e.g., dropping critical target/feature NaNs).
*   Correct the typo in the Publisher encoding.
*   Ensure all relevant features (including individual regional sales) are used if they provide distinct information beyond their sum.
*   Add basic file loading error handling.",deepseek_r1,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import OneHotEncoder
import warnings
warnings.filterwarnings('ignore')

# 1) Load and handle missing values
df = pd.read_csv('video_games_sales.csv')
numerical_cols = df.select_dtypes(include=np.number).columns
categorical_cols = ...",7,"The generated code addresses most of the requirements but has some notable issues and areas for improvement.

**Strong points:**
- All 7 main tasks are attempted.
- Correct use of `SimpleImputer` or `median`/`mode` for missing values.
- Correct implementation of all 5 engineered features.
- Correct correlation heatmap generation.
- Correct PCA implementation and visualization of explained variance.
- Correct polynomial features generation.
- Attempted interaction features between Year and Genre.
- Correct outlier handling using IQR and capping.

**Areas for improvement/Issues:**
1.  **Missing Value Handling:**
    *   **Ground Truth:** Uses `SimpleImputer` for `Year`, `Publisher`, and `Genre`. It specifically converts `Year` to `int` after imputation. It also fills 'Name'.
    *   **Generated Code:** Uses `median()` for all numerical columns and `'Unknown'` for all categorical columns. This is generally acceptable but `SimpleImputer` is often preferred for consistency and better handling of potential future data where median/mode might change. More importantly, it doesn't convert 'Year' to integer, which might be critical for later tasks like 'Sales_Momentum' (though Python might handle it implicitly). The `Name` column isn't handled specifically, which is minor.

2.  **`Sales_Ratio_NA_EU`:**
    *   **Ground Truth:** Adds a small epsilon (`1e-6`) to the denominator to prevent division by zero, which is robust.
    *   **Generated Code:** Uses `replace([np.inf, -np.inf], np.nan, inplace=True)` and then fills NaNs with the median. While functional, adding epsilon is a more direct way to avoid the infinite values in the first place and can lead to cleaner results if not all values are 0.

3.  **Interaction Feature (Year x Genre):**
    *   **Ground Truth:** Uses `get_dummies` for genres and then multiplies `Year` by each dummy column. This correctly creates interaction terms.
    *   **Generated Code:** Uses `OneHotEncoder` which is fine, but then directly multiplies the `genre_encoded_df` by `df['Year']`. This broadcasting works but the column naming derived (`Year_Genre_<col_split_0>`) is not intuitive and could be messy depending on the original one-hot encoded names. The Ground Truth's approach of `Year_x_{col}` where `col` is the genre dummy is clearer. Furthermore, the generated code seems to drop the original one-hot encoded columns when creating the interaction terms, which might not be desired.

4.  **PCA - `numerical_df` selection:**
    *   **Ground Truth:** Explicitly removes 'Rank' from numerical features before PCA, as 'Rank' is often an identifier and not a true numerical feature for variance analysis.
    *   **Generated Code:** Uses `df.select_dtypes(include=np.number)` for `numerical_df` without explicitly checking for or removing 'Rank'. This might include 'Rank' in PCA, which could skew results if 'Rank' is an ordinal or ID column.

5.  **Output and Explanation:**
    *   **Ground Truth:** Includes print statements and descriptive comments at each step, explaining what's being done, which is good for understanding and debugging.
    *   **Generated Code:** Lacks descriptive print statements or comments, making it harder to follow the execution flow and outcomes from the console output. While the task asked for ""only the Python code without explanations,"" in a real-world scenario or a benchmark, informative print statements about the state of data would be beneficial.

Overall, the generated code accomplishes most of the core tasks successfully, demonstrating a good understanding of the required techniques. The issues are mostly related to robustness, clarity, and minor logical oversights rather than fundamental errors.",deepseek_r1,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, mannwhitneyu, chi2_contingency, pearsonr, f_oneway

df = pd.read_csv('diabetes.csv')

columns_to_clean = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_to_clean] = df[columns_to_clean].replace(0, np.nan)
for col in columns_to_clean:
    df[col].fillna(df[col].median(), inplace=True)

glucose_diabetic = df[df['Outcome'] == 1]['Glucose']
glucose_non_diabetic = df[df['Outcome'] == 0]['Glu...",6,"The generated code correctly performs most of the requested statistical tests and handles the data cleaning as specified. However, it falls short in terms of completeness and interpretation.

**Strengths:**
*   Correctly implements median imputation for zero values in specified columns.
*   Correctly performs t-test, Mann-Whitney U test, Chi-square test, Pearson correlation, and ANOVA.
*   Uses `equal_var=False` for the t-test, which is good practice.

**Areas for Improvement / Missing Features:**

1.  **Robust Error Handling (Task 1: Load Data):** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code assumes the file will always be present.
2.  **Explicit Significance Level:** While the code uses `0.05` directly, defining `ALPHA = 0.05` at the beginning (as in the ground truth) is more Pythonic and makes the code cleaner and easier to modify if the significance level needs to change.
3.  **Detailed Interpretation of Results (Major Weakness):** The most significant missing piece is the detailed interpretation of the results. The task explicitly asks to ""Display p-values and interpret results with significance level α=0.05."" The generated code only prints ""Significant"" or ""Not significant,"" which is a minimal interpretation. The ground truth provides comprehensive explanations for each test, linking the p-value to the rejection or failure to reject the null hypothesis, and explaining what that means in the context of the variables being tested (e.g., ""diabetic patients have significantly higher glucose levels""). This is a critical component of a data science task.
4.  **Clarity of Age Groups:** The task specified age groups as ""young: <30, middle: 30-50, older: >50"". The `pd.cut` default `right=True` includes the rightmost bin edge, so `[0, 30, 50, 100]` with labels `['young', 'middle', 'older']` would technically mean `(0, 30]`, `(30, 50]`, `(50, 100]`. The ground truth explicitly uses `right=True` and `[0, 29, 50, np.inf]`, which is slightly more precise for the `<30` definition (making it `(0, 29]` which implies `age <= 29`), and `>50` (`(50, inf]`). While the output logic for `AgeGroup` is almost the same for practical purposes, the ground truth's definition aligns more strictly with `age < 30` (by using 29 as the breakpoint).
5.  **BMI Category Definition (Minor Issue):** The task defined BMI categories as ""Underweight: <18.5, Normal: 18.5-24.9, Overweight: 25-29.9, Obese: >=30"". `pd.cut` with `bins=[0, 18.5, 25, 30, 100]` and default `right=True` means `(0, 18.5]`, `(18.5, 25]`, `(25, 30]`, `(30, 100]`. This doesn't perfectly align with the `[x, y)` or `>x` criteria. The ground truth uses `right=False` to create `[0, 18.5)`, `[18.5, 25)`, `[25, 30)`, `[30, inf)` which is much closer to the requested ranges, though still needs careful handling for `>=30` vs `[30, inf)`.
6.  **Readability/Comments:** The ground truth includes comments and print statements that clearly delineate each step and its purpose, enhancing readability and understanding. The generated code is more concise but sacrifices some clarity.",deepseek_r1,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClas...",8,"The generated code provides a comprehensive solution to the problem, covering most of the requirements.

**Strong Points:**
- **Correctness:** The code correctly implements data loading, preprocessing (handling zeros with median, feature engineering), categorical encoding, data splitting, model training, cross-validation, hyperparameter tuning, and comprehensive evaluation.
- **Feature Engineering:** Correctly creates 'Glucose_BMI_ratio' and 'Insulin_level' new features.
- **Categorical Feature Encoding:** Uses `pd.cut` for 'Age_group' and `pd.qcut` for 'Insulin_level', and correctly integrates `ColumnTransformer` with `OneHotEncoder` and `StandardScaler` within pipelines for preprocessing.
- **Multiple Models & Evaluation:** Trains and evaluates Logistic Regression, Random Forest, SVM, and Gradient Boosting, using accuracy, precision, recall, F1-score, and ROC-AUC.
- **Hyperparameter Tuning:** Implements `GridSearchCV` for the model that performed best in cross-validation, which is a good approach.
- **Visualization:** Generates confusion matrices and ROC curves for all models.
- **Feature Importance:** Attempts to display feature importance for tree-based models.

**Areas for Improvement:**
- **'Age_group' definition:** The `Age_group` bins (`[0, 30, 40, 50, 100]`) and labels (`['<30', '30-40', '40-50', '50+']`) are less granular than the ground truth (`[0, 20, 30, 40, 50, 60, np.inf]`). While not strictly incorrect, it impacts the new feature.
- **X variable definition:** The `X = df.drop(['Outcome', 'Age', 'Insulin'], axis=1)` line is problematic.
    - It explicitly drops 'Age' and 'Insulin' from `X` even though `Age_group` and `Insulin_level` are derived from them. While the new features are added, dropping the original can remove potentially useful information unless the derived features fully capture it or the original features are highly correlated with the derived ones and we aim for dimensionality reduction. The ground truth keeps them.
    - It's critical why 'Insulin' is dropped, as 'Insulin_level' is derived from it. The original 'Insulin' numerical feature might still be useful alongside its categorical representation.
- **Feature Importance Logic:** The feature importance extraction for models not explicitly part of the `tuned_model` (i.e., non-tuned tree-based models) is a bit convoluted. It retrains the pipeline on `X_train` within the loop (`pipeline.fit(X_train, y_train)`), which is inefficient and not strictly necessary if they were already part of the initial `models` dictionary within a pipeline setup for cross-validation and evaluation. The ground truth preprocesses X_train once and then uses `X_train_processed_df` for training.
- **Consistency in Model Handling:** The way `models` dictionary is updated with `tuned_model` and then iterated over for evaluation and plotting can lead to slightly less clean code than having a master `models` dictionary that includes both original and tuned versions from the start, or handling the tuned model as a special case for final comparison.
- **Clarity of `numeric_features`:** `numeric_features = X.columns.difference(categorical_features)` can be brittle if X contains non-numerical columns that are not in `categorical_features` (e.g., if any object/string columns slipped through without being explicitly handled or dropped). The ground truth's `X.select_dtypes(include=np.number).columns.tolist()` is more robust for identifying numerical columns.
- **Output and Readability:** The ground truth includes more verbose print statements for progress and results, which enhances readability and understanding for a benchmark. However, this was not a strict requirement for the generated code.",deepseek_r1,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, RFE, f_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegr...",6,"The generated code attempts to address all requirements of the task, which is a good start.

**Strengths:**
*   **Imports:** Most necessary imports are present.
*   **Preprocessing:** Handles imputation of zeros to NaNs and then imputes them (though with mean instead of median as in ground truth, but that's a reasonable choice).
*   **Engineered Features:** Creates a good number of the requested engineered features.
*   **Polynomial Features:** Applies polynomial features correctly.
*   **Scaling:** Performs both StandardScaler and MinMaxScaler.
*   **PCA:** Applies PCA and prints explained variance.
*   **Feature Selection:** Uses SelectKBest and RFE.
*   **Correlation Filtering:** Implements correlation-based feature filtering.
*   **SMOTE:** Attempts to apply SMOTE.
*   **Feature Importance Visualization:** Random Forest for feature importance and visualization is present.

**Areas for Improvement / Missing/Incorrect Aspects:**

1.  **Imputation Strategy:** The ground truth used `median` for imputation, which is generally more robust to outliers for skewed data compared to `mean`. The generated code uses `mean`. (Minor point, but noted)
2.  **Risk Factor Count:** The generated code defines `Risk_Factor_Count` differently (by thresholding specific individual features and summing binary indicators), while the ground truth used slightly different, more medically informed thresholds for glucose, BP, BMI, and age. The generated code also calculates `High_Insulin`, which wasn't in the ground truth's definition of `Risk_Factor_Count`. The ground truth's approach was more aligned with the ""count of high-risk factors"" description.
3.  **Polynomial Features Column Selection:** The ground truth applied polynomial features to a more specific set of ""core numerical features"" (`Pregnancies`, `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`, `Age`) and correctly `concat`ed them after dropping the originals. The generated code uses `selected_cols = ['Glucose', 'BMI', 'Insulin', 'BloodPressure', 'Age']` which is a subset and doesn't handle the concatenation or dropping of original columns as cleanly, potentially leading to redundant features or issues if the `df` is used later. The ground truth method correctly removes the original columns before concatenating the expanded polynomial features.
4.  **Consistency in `X` for subsequent steps:**
    *   PCA is applied to `X_std`.
    *   SelectKBest and RFE are applied to `X_std`.
    *   Correlation filtering is applied to `X_std_df` (which is `X_std` wrapped in a DataFrame).
    *   SMOTE is *then* applied to `X_filtered` (which is the result of correlation filtering).
    *   Feature importance visualization uses `X_filtered.columns` but `rf.fit` was on `X_train` from the SMOTE output. This means the feature names for plotting might not perfectly align with the features the RF model was trained on if SMOTE introduced new dimensions or if features were dropped in PCA/KBest/RFE. The ground truth ensures that the final `X_final` being fed into SMOTE and then feature importance is consistent after all filtering steps.
5.  **PCA Usage and Explanation:** The ground truth explicitly used `n_components=0.95` to retain components explaining 95% variance and printed the number of components selected and cumulative variance. The generated code just runs `pca = PCA()` (which uses `None` meaning all components) and prints `explained_variance_ratio_` but doesn't explain the decision or the number of components chosen for dimensionality reduction. The ground truth shows a more practical application for 'dimensionality reduction'.
6.  **SMOTE Variable Naming & Split:** The line `X_res, y SMOTE SMOTE().fit_resample(X_filtered, y)` has a syntax error (`y SMOTE SMOTE()`). It should be `X_res, y_res = SMOTE().fit_resample(X_filtered, y)`. Also, the `train_test_split` is done *after* SMOTE on the *entire* dataset, which leads to data leakage (training on data that was oversampled from test data). SMOTE should be applied *only* to the training data. The ground truth correctly splits training and test data *before* applying SMOTE to only the training set. This is a critical error.
7.  **Feature Importance Plotting:** While the importances are calculated, the `plt.xticks` part directly uses `features[i]` where `features` is `X_filtered.columns`. If `X_filtered` (before SMOTE and split) has a different number of columns or names than the `X_train` used for `rf.fit` after the split and SMOTE, the x-axis labels might be incorrect. The ground truth handles this by getting `X_train_smote.columns` directly.
8.  **Missing Output/Print Statements:** The ground truth code includes extensive `print` statements to indicate what step is being performed and the results of certain operations (e.g., number of features after engineering, after correlation filtering, PCA components). This makes the code much more understandable and provides necessary insights into the pipeline's behavior. The generated code lacks these.
9.  **No Warnings Suppression:** Ground truth included `warnings.filterwarnings('ignore')` which is good practice for cleaner output in a benchmark.

The most critical issues are the incorrect SMOTE application (data leakage) and the less robust handling of polynomial features and consistent data flow after various transformations.",deepseek_r1,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, mannwhitneyu, f_oneway, pearsonr, chi2_contingency

# Load and clean data
df = pd.read_csv('world-happiness-report-2021.csv')
df.dropna(inplace=True)

# 2) Western Europe vs others t-test
western_eu = df[df['Regional indicator'] == 'Western Europe']['Ladder score']
other_regions = df[df['Regional indicator'] != 'Western Europe']['Ladder score']
t_stat, p_two_tailed = ttest_ind(western_eu, other_regions, equal_var...",6,"The generated code attempts to address all parts of the task, but it falls short in several areas, particularly regarding data handling, the depth of statistical interpretation, and specific test parameters.

**Strengths:**
*   Correctly identifies and uses the required statistical tests (`ttest_ind`, `mannwhitneyu`, `f_oneway`, `pearsonr`, `chi2_contingency`).
*   Handles the grouping for most tests appropriately (e.g., Western Europe vs. others, top 25% vs. bottom 25%).
*   Calculates and prints p-values, and provides a basic interpretation of significance.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling:** The code uses `df.dropna(inplace=True)`, which drops *any* row with *any* NaN. The prompt specified handling missing values, and the ground truth demonstrated a more robust approach by identifying `critical_columns` and only dropping NaNs from those. Dropping all NaNs might remove too much data or data that isn't relevant to the statistical tests, potentially biasing results or reducing statistical power. The ground truth also explicitly printed how many rows were dropped, which is good practice.

2.  **Interpretation of Results:**
    *   The prompt specifically asked for interpretation with `α=0.05`. While the code checks `p < 0.05`, it only states ""Significant"" or ""Not significant."" The ground truth goes further to interpret the *direction* of the significant difference (e.g., ""significantly HIGHER happiness scores,"" ""significantly POSITIVE correlation""). This is crucial for drawing meaningful conclusions from the statistical tests.
    *   For the Mann-Whitney U test, median comparison is often more appropriate for interpretation than mean, especially given it's a non-parametric test. The generated code doesn't print the medians.
    *   For ANOVA, the ground truth correctly states that post-hoc tests would be needed to identify *which* specific regions differ, which is an important nuance. The generated code misses this.

3.  **One-tailed vs. Two-tailed t-test (Question 2):**
    *   The prompt implicitly asks for a test of ""significantly *higher* happiness scores,"" which suggests a one-tailed test. The `ttest_ind` function by default performs a two-tailed test. The generated code attempts to convert a two-tailed p-value to a one-tailed by `p_two_tailed / 2 if mean_we > mean_other else 1 - (p_two_tailed / 2)`. While this is a common approach, it's prone to error if not handled carefully, and `scipy.stats.ttest_ind` now supports the `alternative` parameter (though not in all versions, depends on scipy version). The ground truth's use of `equal_var=False` (Welch's t-test) is good, and it performs a two-tailed test, then interprets the direction based on means. The interpretation in the generated code for *p_we* is a bit confusing and doesn't explicitly state ""one-tailed"" or ""two-tailed"" result.
    *   The prompt asked to ""Test whether Western European countries have significantly higher happiness scores..."", which *implies* a one-tailed test. The ground truth appropriately used a two-tailed test and then interpreted the direction of the significant difference, which is often a more robust and less assumption-prone approach unless a *very specific* directional hypothesis is pre-defined. The generated code's ad-hoc p-value adjustment for one-sided interpretation without explicit mention is less clear.

4.  **Clarity and Readability:**
    *   Variable names are a bit short (`p_we`, `p_gdp`, `p_anova`).
    *   The output format is concise but lacks the detailed breakdown (mean/median values, t/F/chi2 statistics) that aids in understanding the results, which the ground truth provides.
    *   Lack of introductory/concluding print statements for each test, making the output less organized than the ground truth.

5.  **Robustness (File Handling):** The generated code doesn't include robust file handling (e.g., `try-except` for `FileNotFoundError`), which is generally good practice for data loading.

To improve, the generated code should:
*   Refine missing data handling.
*   Provide more comprehensive interpretations of results, including direction and relevant descriptive statistics (means, medians).
*   Add more informative print statements to structure the output.
*   Consider using the `alternative` parameter for t-tests and Mann-Whitney U tests if applicable, or clearly stating if it's a two-tailed test being interpreted directionally.",deepseek_r1,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.p...",6,"The generated code addresses many aspects of the prompt but has notable omissions and areas for improvement, particularly concerning data preprocessing, comprehensive model evaluation, and cross-validation.

**Strengths:**
*   **Pipeline Usage:** Effectively uses `Pipeline` for preprocessing and model integration, which is good practice.
*   **Multiple Models:** Implements and compares multiple regression models as requested.
*   **Hyperparameter Tuning:** Attempts hyperparameter tuning for the best model using `GridSearchCV`.
*   **Evaluation Metrics:** Calculates MAE, MSE, RMSE, and R2, and stores them in a DataFrame.
*   **Visualizations:** Includes residual plot and feature importance visualization for the best model.
*   **Cross-validation:** Implements `cross_val_score`.
*   **New Predictions:** Makes predictions for hypothetical new data.

**Weaknesses and Areas for Improvement:**

1.  **Data Preprocessing (Handling Missing Values):**
    *   The prompt specifically asks to ""handle missing values."" The generated code uses `SimpleImputer(strategy='mean')` within pipelines. While this is a form of handling, the ground truth explicitly drops rows with missing values in the target or feature columns. More importantly, the generated code *does not check* for missing values before applying the imputer or specify which columns might have them. The ground truth explicitly verifies missing values and handles them by dropping rows as the first step for all relevant specified columns, which is a safer approach for this dataset if missing data is minimal, ensuring data integrity.
    *   **Categorical Encoding:** The prompt mentions ""encoding categorical variables"". While the specified features (`Logged GDP per capita`, etc.) are numerical, the dataset *does* contain categorical variables like 'Regional indicator' and 'Country name'. The ground truth explicitly notes that no categorical features are *used* in the specified model input, thus no encoding is applied (which is correct given the specified features). The generated code completely omits any consideration or mention of categorical variables, implying it might have overlooked this aspect of preprocessing in the broader dataset context.

2.  **Feature and Target Definition Consistency:**
    *   The prompt says ""Create target variable using Ladder score and features using GDP per capita, Social support, Healthy life expectancy, Freedom, Generosity, and Perceptions of corruption"". The generated code correctly uses `Ladder score` as target and the exact variable names from the dataset for features. This is good.

3.  **Comprehensive Model Evaluation:**
    *   The generated code evaluates models and puts them into a `results_df`. However, it doesn't clearly print or display the performance of *all* initial models in a summary table before tuning, which the ground truth does. The `results_df` is created but not explicitly printed to console, making it harder to immediately see the initial comparison.
    *   The tuning block for `GridSearchCV` evaluates only for R2 (`scoring='r2'`). The ground truth uses `neg_mean_squared_error` during grid search (optimizing for MSE), which is often more common for regression models, and then re-evaluates the best tuned model with all metrics (MAE, MSE, RMSE, R2) on the test set. The generated code applies tuning and then only makes `y_pred` with the best model, without explicitly re-evaluating and printing the metrics of the *tuned* best model on the test set, creating a gap in comparison.

4.  **Hyperparameter Tuning Scope:**
    *   The generated code applies `GridSearchCV` only if `param_grid` is not empty, implying that tuning *might not happen* if the best initial model didn't fall into 'Gradient Boosting' or 'Random Forest'. This is a logic flaw: the prompt asks for hyperparameter tuning for *the best model*. While it's reasonable to *expect* a tree-based model to be best here, the code's current structure doesn't guarantee tuning will occur for the actual `best_model_name` if it turns out to be, say, Ridge. The ground truth hardcodes the 'Gradient Boosting Regressor' for tuning, assuming it will likely be a strong candidate, which is a pragmatic choice, but it also clearly declares this choice.

5.  **Visualizations:**
    *   Residual plot: The generated code creates a residual plot, but it's very basic. The ground truth adds `plt.grid`, `plt.axhline`, and better labels.
    *   Feature importance: Implemented, but the `best_model.named_steps['model']` approach is specific to pipelines, which is correct, but doesn't handle cases where `feature_importances_` might not exist (e.g., if a linear model happened to be chosen, though not common for feature importance plots). The ground truth handles this with an `if hasattr(best_tuned_model, 'feature_importances_'):` check, which is more robust.

6.  **Cross-Validation:**
    *   The prompt asks to ""Implement cross-validation to assess model stability."" The generated code uses `cross_val_score(best_model, X, y, cv=5, scoring='r2')` which is a valid start. However, the ground truth performs CV using *both* R2 and RMSE (by converting negative MSE scores), and also prints the mean and standard deviation of these scores, providing a more comprehensive view of model stability and performance spread across folds.

7.  **New Predictions & Analysis:**
    *   The generated code makes predictions for new data but only prints the raw predictions. The ground truth provides a brief, but critical, *analysis* of these predictions, explaining how they align with expectations based on the input features. This adds significant interpretability.

8.  **Imports and Best Practices:**
    *   The generated code misses the `KFold` import for cross-validation structure, although `cross_val_score` simplifies its direct use. `seaborn` is also missing, which the ground truth uses for better plotting.
    *   Missing `np.random.seed(42)` at the beginning for overall reproducibility, although `random_state=42` is used in specific sklearn calls.
    *   The ground truth includes error handling for `pd.read_csv`, which is a good practice for robustness.",deepseek_r1,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, mutual_info_regression, RFE
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression

df = pd.read_csv('world-happiness-report-2021.c...",6,"The generated code attempts to address all requirements but falls short in several aspects regarding correctness, completeness, and code quality.

**Strengths:**
*   Successfully loads the data.
*   Implements the five requested engineered features correctly.
*   Applies polynomial features.
*   Attempts all three scaling methods.
*   Attempts PCA and t-SNE for dimensionality reduction/visualization.
*   Attempts SelectKBest and RFE for feature selection.
*   Attempts correlation-based filtering and mutual information scoring.
*   Creates interaction features.
*   Uses Random Forest for feature importance and visualization.

**Areas for Improvement (leading to a score of 6):**

1.  **Preprocessing and Missing Values:** The ground truth meticulously handles missing values by filling `Perceptions of corruption` and then dropping any remaining NaNs. The generated code uses `.dropna()` immediately after loading, which is a very aggressive approach and might remove too much data or relevant data. This could lead to a significantly smaller dataset than intended and potentially impact the representativeness of the analysis. A more robust approach like the ground truth is preferred.

2.  **Polynomial Features Column Indexing:** The concatenation for polynomial features (`pd.concat([df, pd.DataFrame(poly_features, columns=poly_cols)], axis=1)`) is missing `index=df.index`. This means the polynomial features DataFrame will have a new default integer index (0, 1, 2...), which might not align with the `df` if rows were dropped or reordered earlier. The ground truth correctly uses `index=df_processed.index` to ensure alignment.

3.  **Feature Scaling Application:**
    *   The generated code applies scaling *multiple times* within a loop and then re-assigns the scaled data back to the original `df` with new column names (`f'{col}_{name}'`). This significantly duplicates features and inflates the DataFrame unnecessarily. The requirement was to *perform* scaling using multiple methods, implying that distinct scaled versions of `X` should be available, not that all scaled versions should be added back to the main DataFrame for subsequent steps.
    *   For subsequent steps (PCA, t-SNE, Feature Selection), the ground truth explicitly chooses *one* scaled version (`X_scaled = df_scaled_standard.copy()`). The generated code implicitly uses `X = df[numeric_cols].fillna(0)` which means it's running PCA/t-SNE/Feature Selection on the *original* numeric columns, *not* the scaled ones that were created (or the most recent scaled versions). This is a critical error in the workflow as scaling is typically a prerequisite for PCA and certain feature selection methods.

4.  **Dimensionality Reduction Visualization:**
    *   The PCA and t-SNE visualizations are incorrectly implemented. They are put inside the *same loop* that iterates through scalers. This means the plots are generated for each scaler *within the loop*, and only the final set of plots (likely for RobustScaler or the last one) will be shown, confusing the user and potentially leading to overlapping plots or incorrect subplots.
    *   More importantly, the plots are generated using `plt.subplot(2,3,i)` and `plt.subplot(2,3,i+3)`. With `i` starting at 1, this tries to plot into `subplot(2,3,1), (2,3,4), (2,3,2), (2,3,5), (2,3,3), (2,3,6)`. This means PCA for StandardScaler gets (2,3,1), t-SNE for StandardScaler gets (2,3,4). Then PCA for MinMaxScaler overwrites (2,3,2), etc. This will definitely lead to non-intuitive or incorrect visualizations.
    *   The `X` used for plotting `X_scaled = scaler.fit_transform(X)` inside the loop is the *unscaled* `X` from `X = df[numeric_cols].fillna(0)`. This means it scales `X` every time, but it's not maintaining consistent data for the subsequent steps. The `X` used for `SelectKBest`, `RFE`, and `RandomForestRegressor` will be the original unscaled `numeric_cols` with NaNs filled, not the scaled data. This defeats the purpose of the scaling step.

5.  **Feature Selection Target Variable:** The `f_regression` from SelectKBest was omitted from the requirements but was correctly used in the ground truth as it's more appropriate for regression than `mutual_info_regression` alone. The prompt asked for `f_regression` but the derived code uses `mutual_info_regression`. (Correction: The prompt originally asked for `f_regression` then `mutual_info_regression` was added to the ground truth later. The generated code did use `mutual_info_regression` for `SelectKBest`, which is fine, but it also used `f_regression` in the ground truth which is usually a better choice for linear relationships. The prompt's wording was a bit ambiguous on this specific detail in its evolution.)

6.  **Correlation-based Feature Filtering:** The generated code calculates `corr = X.corrwith(y)` and identifies `high_corr = corr[abs(corr) > 0.5].index`. This is *correlation with the target variable*, not *inter-feature correlation* (highly correlated features among themselves) which is what correlation-based feature *filtering* usually aims to address for multicollinearity. The ground truth correctly implements removal of highly *inter-correlated* features, which is a common and important step in feature filtering.

7.  **Interaction Features:** The selection of `econ_factors` and `social_factors` is incomplete as `GDP_vs_Regional_Average` and `Governance_Score` are newly engineered features, not initial columns from the data. The ground truth explicitly uses key original factors like `Logged GDP per capita` and `Social support` for this step. The generated code uses `GDP_vs_Regional_Average`, `Happiness_Efficiency`, `Social_Wellness_Index`, and `Governance_Score` in its interaction factors, which is an acceptable interpretation, but the ground truth stuck to more fundamental economic and social drivers.

8.  **Code Quality:**
    *   Missing imports like `warnings` for cleaner output.
    *   Lack of print statements to indicate the progress or results of steps (e.g., shape after scaling, lists of selected features). The ground truth provides excellent logging.
    *   The use of `fillna(0)` on `X` before scaling and other steps is generally not a good imputation strategy for feature engineering; it's better to use mean/median/mode or more sophisticated methods. The ground truth uses `fillna(X.mean())` which is a more reasonable default.
    *   The use of `astype(int)` for `Regional_Happiness_Rank` is not strictly necessary and could lose precision if there were non-integer ranks in other scenarios (though not likely here).

In summary, while the generated code covers most of the required operations, the implementation details, particularly regarding data flow (how scaled data is used downstream, how each scaler result is treated), visualization execution, and the specific interpretation of correlation-based filtering, contain significant flaws that would lead to incorrect results or misleading analyses.",deepseek_r1,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Load and preprocess data
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
df = df[df['location'] == 'USA'].set_index('date')

# Resample to weekly frequency
weekly_data = df['new_cases'].resample('W').sum().to_frame()

# Time series decomposition
dec...",8,"The generated code correctly addresses most of the task requirements.

**Strengths:**
*   **Correct Filtering and Resampling:** Successfully loads the data, filters for 'USA', sets the date as index, and resamples 'new\_cases' to weekly sum.
*   **Decomposition (Additive Model):** Correctly performs seasonal decomposition with an additive model and plots the result.
*   **ADF Test and Differencing:** Implements the ADF test for stationarity, checks the p-value, and correctly applies first-order differencing if the original series is non-stationary. It then re-tests the differenced series.
*   **ACF and PACF Plots:** Generates ACF and PACF plots for the (potentially differenced) stationary series.
*   **Proper Imports:** All necessary libraries are imported.

**Areas for Improvement:**
*   **Robustness/Error Handling:** The code lacks error handling for file not found, empty data after filtering, or cases where `seasonal_decompose` might fail due to insufficient data for the specified `period`. The ground truth includes robust `try-except` blocks and checks for `len(weekly_cases)`.
*   **Informative Output:** While it prints ADF statistics, more descriptive print statements (like critical values, and explicit conclusions about stationarity) would improve clarity, as seen in the ground truth.
*   **Explicit Plot Titles/Labels:** While the decomposition plot implicitly gets titles, individual plots for ACF/PACF lack explicit titles beyond the `plot_acf` and `plot_pacf` default, which can be improved for clarity.
*   **Edge Cases in ADF Test/Differencing:** The ground truth also addresses the `dropna()` more carefully and checks if the differenced series has enough data points to be meaningful, which is a good practice.
*   **ADF Test Output Detail:** The ADF test output only shows the statistic and p-value. The critical values (`result[4]`) are crucial for a complete interpretation, and including them as in the ground truth would be beneficial.
*   **Implicitly Handles `period` for Decomposition:** The code assumes a fixed `period=52`. The ground truth includes logic to warn or adjust if the data length is insufficient for this period, making it more flexible.

Overall, the generated code provides a functional and mostly correct solution to the problem. The main areas for improvement are in making the code more robust and providing more comprehensive output, which are crucial for reliable data science pipelines.",deepseek_r1,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
import numpy as np

df = pd.read_csv('winequality-red.csv')
df['pH_group'] = pd.cut(df['pH'], bins=[-np.inf, 3.2, 3.4, np.inf], labels=['Low', 'Medium', 'High'])

print(""Average quality per pH group:"")
print(df.groupby('pH_group')['quality'].mean().reset_index().to_string(index=False))

low = df[df['pH_group'] == 'Low']['quality']
medium = df[df['pH_group'] == 'Medium']['quality']
high = df[df['pH_group'] =...",7,"The generated code successfully addresses most of the requirements but has some areas for improvement.

**Correctness:**
*   **pH Grouping:** The pH grouping is correctly implemented using `pd.cut`.
*   **Average Quality Calculation:** Correctly calculates the mean quality for each group.
*   **ANOVA Test:** Correctly performs the ANOVA test.
*   **Confidence Intervals:** The calculation of `yerr` for confidence intervals is correct for the standard error of the mean * t-score. However, it's missing the actual `ci_lower` and `ci_upper` in the plot which should be calculated using `stats.t.interval` for better representation of error bars as two-sided. The current calculation provides the half-width of the CI, making symmetric error bars, which is acceptable but could be more robust.
*   **Visualization:** The bar plot is generated correctly with error bars.

**Code Quality:**
*   **Imports:** All necessary imports are present.
*   **Readability:** The code is reasonably readable, but could benefit from more descriptive variable names for the individual `low`, `medium`, `high` quality lists that feed into ANOVA.
*   **Structure:** It's a linear script, which is fine for a single task.

**Completeness:**
*   **Categorization:** Implemented. However, the labels `Low`, `Medium`, `High` are less descriptive than the ground truth's `Low pH (< 3.2)`, etc., which might be important for a clearer interpretation of the plot and printed output.
*   **Average Quality Reporting:** Implemented, but only the mean is printed, not the full confidence interval range, which the ground truth does.
*   **ANOVA Test Reporting:** Implemented and reported.
*   **Visualization:** Implemented.
    *   The task explicitly asked for ""error bars representing 95% confidence intervals"". While the calculation (`yerr`) is derived from the CI formula, it only provides the *half-width* of the interval from the mean. The ground truth explicitly calculates the `ci_lower` and `ci_upper` and uses `group_stats['mean_quality'] - group_stats['ci_lower']` as the `yerr` to ensure the error bar accurately reflects the interval, which is a more precise approach than simply calculating the half-width.
    *   The plot title is good, but the Ground Truth adds average quality values on top of the bars, which is a nice touch for immediate data interpretation.
    *   No handling for `FileNotFoundError`.
    *   The categories for `pH_group` are not ordered, which can lead to inconsistencies in plotting and group order if not handled explicitly. The Ground Truth makes `pH_group` a `Categorical` type with a defined order, which is a best practice.
    *   No specific handling for cases where a pH group might be empty, which could cause issues for `sem` or `len-1` operations if not handled carefully (though unlikely with this dataset).

To achieve a higher score, the generated code could:
1.  Use more descriptive pH group labels as mentioned in the ground truth.
2.  Print the confidence interval bounds along with the mean quality for each group.
3.  Explicitly order the categorical `pH_group` column to ensure consistent plotting.
4.  Optionally, add error handling for `FileNotFoundError`.
5.  Refine the error bar calculation to be more robust, potentially by using `stats.t.interval` directly to get the bounds and then deriving the error bar length.",deepseek_r1,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date')

features = ['new_cases', 'hosp_patients', 'icu_patients', 'repr...",7,"The generated code correctly addresses most of the requirements but has a few issues that reduce its score:

**Correctness:**
- **Missing `date` parsing parenthesis:** The line `df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date')` has a syntax error with missing closing parenthesis for `parse_dates`. This would cause a `SyntaxError` and prevent the code from running.
- **Handling of `continent` for `SimpleImputer`**: The `continent` column is imputed using `strategy='most_frequent'` which is generally correct for categorical data. However, the task states to handle missing values ""in selected features and target by imputing with the median"". While `continent` is a selected feature, it's categorical and median imputation isn't applicable. The ground truth correctly excludes it from median imputation. The generated code also separates it for a different imputation strategy which is good, but the task phrasing could lead to confusion.
- **Data Leakage in Scaling/Imputation:** The code performs scaling *before* splitting the data (`X_scaled = scaler.fit_transform(X_combined)` and `imputer_X.fit_transform(X)`). This is a significant issue as it introduces data leakage from the test set into the training process, leading to an over-optimistic evaluation of the model's performance on unseen data. Imputation and scaling should be fitted only on the training data and then applied to both training and test sets.

**Code Quality:**
- **Imports:** Imports are generally good and necessary.
- **Readability:** The code is mostly readable, but breaking down the imputation/encoding steps into smaller, more clearly labeled blocks (as in the ground truth) would improve clarity, especially around the order of operations.
- **Error Handling:** Lacks basic error handling for `FileNotFoundError` as seen in the ground truth, which is good practice for robust code.
- **Redundant `DataFrame` creation:** Creating `X = pd.DataFrame(X_imputed, columns=features)` and `X_final = pd.DataFrame(X_scaled, columns=X_combined.columns)` right after `fit_transform` is often not strictly necessary if you can directly work with the numpy arrays or ensure column names are maintained more efficiently.

**Completeness:**
- All major requirements (loading, feature selection, target, missing values, encoding, scaling, splitting, model training, evaluation metrics, printing) are attempted.
- The print statements for evaluation metrics are less detailed than the ground truth (e.g., missing model names and clearer formatting).

Overall, the code demonstrates an understanding of the steps required but fails on a critical data preprocessing best practice (data leakage) and has a minor syntax error. If the data leakage and syntax error were fixed, it would be a much stronger solution.",deepseek_r1,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Load and preprocess data
df = pd.read_csv('synthetic_covid19_data.csv')
cols = ['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']
for col in cols:
    df[col] = df[col].fillna(df[col].median())

# ANOVA test for reproduction_rate across continents
filtered_df = df[df['continent'].isin(['Asia', 'Europe', 'North America'])]
mo...",7,"The generated code correctly implements the three statistical tests required by the task and handles missing values via median imputation. It also correctly states the null hypotheses and interprets the p-values for conclusions.

However, there are several areas for improvement:

1.  **Robustness in Data Loading/Imputation:** The generated code does not include error handling for `FileNotFoundError` when loading the CSV, which the ground truth does. While it handles imputation, it doesn't explicitly confirm the state of missing values after imputation, nor does it handle cases where a column might be missing.
2.  **ANOVA Implementation Detail:** While using `statsmodels` for ANOVA is valid, the direct `scipy.stats.f_oneway` approach used in the ground truth is often simpler for a basic one-way ANOVA as requested, especially when the groups are already separated. The `statsmodels` output requires an extra step to extract the p-value.
3.  **T-test Edge Cases:** In the t-test section, `high_age.dropna()` and `other_age.dropna()` are used within `ttest_ind`. While `median_age` is imputed at the beginning, ensuring there won't be NaNs from that source, further operations (like `total_deaths / population` creating NaNs) could lead to an empty or single-element `median_age` series during the t-test, which `ttest_ind` might not handle gracefully without explicit checks (e.g., if one group ends up empty after filtering for `high_death_rate_country`). The ground truth specifically checks for empty groups and provides more informative messages.
4.  **Implicit Assumptions/Missing Context:** The generated code assumes 'date' column is not critical for the ""latest entry per country"" step beyond taking the `last()` entry after `groupby('country')`. Without sorting by date beforehand, `last()` in `groupby` might not reliably give the *latest* entry. The ground truth explicitly sorts by `['country', 'date']` before `groupby().last()`, which is crucial for correctness.
5.  **Output Clarity:** The output is concise but lacks some informative print statements (e.g., ""Data Loading and Imputation Complete"", Pearson correlation coefficient value) that improve readability and debugging, present in the ground truth.
6.  **`total_deaths / population` handling:** If `population` is zero, it would result in `inf`, or `NaN` if either is `NaN`. The ground truth explicitly handles `inf` and `-inf` values arising from this division, which is a good practice for robustness. The generated code implicitly relies on `dropna()` later to clean these.
7.  **Alpha definition:** The ground truth defines `ALPHA` as a constant, which promotes good practice for configurable parameters. The generated code hardcodes `0.05` repeatedly.",deepseek_r1,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('clash_royale_cards.csv')
selected_columns = df[['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']]
corr_matrix = selected_columns.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt="".2f"")
plt.title('Correlation Matrix Heatmap')
plt.show()
```",9,"The generated code correctly performs all steps of the task: loading the CSV, selecting the specified numerical columns, calculating the correlation matrix, and visualizing it with a heatmap using seaborn. The imports are correct, and the heatmap parameters (`annot`, `cmap`, `fmt`) are well-chosen for clarity.

Areas for minor improvement (not critical for this task):
- **Error Handling**: The ground truth includes robust error handling (e.g., `FileNotFoundError`, `KeyError`, `ValueError` for missing columns). The generated code lacks explicit error handling, which could make it less robust in a real-world scenario where the file might not exist or columns might be misspelled.
- **Modularity**: The ground truth wraps the logic in a function and uses `if __name__ == ""__main__"":` which is good practice for reusability and script execution. The generated code is a simple script, which is acceptable for a direct task but less modular.
- **Clarity of `linewidths`**: The ground truth's heatmap includes `linewidths=.5` which marginally improves visual separation of cells. This is a very minor aesthetic point.

Overall, the generated code is highly effective for the given task and directly addresses all prompt requirements. The missing error handling is the primary reason it's not a perfect 10, as robustness is a key aspect of good code, but for a direct ""solve the task"" benchmark, it's very strong.",deepseek_r1,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

df = pd.read_csv('clash_royale_cards.csv')
grouped = df.groupby('rarity').agg({'elixirCost': 'mean', 'Win Rate': 'mean', 'Usage': 'mean'})
print(grouped)
```",10,"The generated code is correct, complete, and of good quality. It properly loads the CSV, groups by `rarity`, calculates the average `elixirCost`, `Win Rate`, and `Usage`, and displays the results. The use of `.agg()` is an equally valid and often more explicit way to perform multiple aggregations than selecting columns before `.mean()`. The code fully addresses all requirements of the task.",deepseek_r1,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

df = pd.read_csv('clash_royale_cards.csv')
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)
X = df[['elix', '', 'Usage', 'rarity']]
y = df['is_high_win_rate']

ct = ColumnTransformer(
    [('encoder', OneHotEncoder(), ['rarity'])],
    remainder='passthrough'
)
X_processed = ct.fit_transform(X)

X_train, X_test...",5,"The generated code attempts to address the task but contains critical errors that prevent it from functioning correctly as specified by the requirements.

**Major Issues:**

1.  **Feature Selection Error:** The line `X = df[['elix', '', 'Usage', 'rarity']]` contains incorrect feature names (`'elix'` instead of `'elixirCost'`) and an empty string (`''`) which will cause a `KeyError` when trying to access these columns from the DataFrame. This directly violates requirement 3: ""Select the following features: `elixirCost`, `Usage`, and `rarity`.""
2.  **`ColumnTransformer` Output:** While `ColumnTransformer` is a valid way to perform encoding, its output (`X_processed`) is a NumPy array, not a pandas DataFrame. This changes the data structure and removes column names, which can complicate subsequent steps (e.g., if one needed to inspect features by name). The ground truth's `pd.get_dummies` maintains a DataFrame structure with meaningful column names.
3.  **Missing Imports/Error Handling:** While `sklearn.compose.ColumnTransformer` is imported, it's not explicitly listed as a required import in the prompt. More importantly, it lacks any error handling for `FileNotFoundError` when loading the CSV, which the ground truth included.

**Minor Issues/Improvements:**

*   The ground truth used `pd.get_dummies` which is often simpler and more idiomatic for one-hot encoding a few columns in pandas DataFrames compared to `ColumnTransformer` when not dealing with complex pipelines involving multiple preprocessing steps. However, `ColumnTransformer` is technically correct for the encoding itself, provided the input features are correct.

**Overall:**

The critical error in feature selection makes the code non-functional for the specified task. While the intent to use `astype(int)` for the binary target and `train_test_split` is correct, the fundamental mistake in feature selection prevents it from being a correct solution.",deepseek_r1,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

data = pd.read_csv('iris.csv')
X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = data['Species']

le = LabelEncoder()
y_encoded = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state...",9,"The generated code correctly implements all specified requirements of the task:
1. Loads the dataset.
2. Defines `X` with the correct feature columns.
3. Defines `y` with the correct target column.
4. Encodes the `Species` target variable using `LabelEncoder`.
5. Splits the data into training (70%) and testing (30%) sets using `random_state=42`.
6. Trains a KNN classifier with `n_neighbors=3`.
7. Makes predictions and reports the accuracy score.
8. Includes all necessary imports.

The code is concise and efficient.

Minor improvements could include:
- Adding `try-except` block for `pd.read_csv` to handle `FileNotFoundError`, as shown in the ground truth code. This makes the code more robust.
- The `print` statement for accuracy in the ground truth uses a more descriptive label (""K-Nearest Neighbors Classifier Accuracy"") and `:.4f` for formatting, which is slightly better than `:.2f` for accuracy scores. While not strictly a correctness issue, it's a small polish.
- The ground truth includes `stratify=y_encoded` in `train_test_split`, which is good practice for classification tasks with potentially imbalanced classes, although it wasn't explicitly requested in the problem description. Its absence isn't a critical error here but is a best practice.",deepseek_r1,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy import stats

df = pd.read_csv('iris.csv')
versicolor = df[df['Species'] == 'Iris-versicolor']['PetalLengthCm']
virginica = df[df['Species'] == 'Iris-virginica']['PetalLengthCm']

t_stat, p_val = stats.ttest_ind(versicolor, virginica)

print(""H0: The mean PetalLengthCm of Iris-versicolor and Iris-virginica are equal."")
print(""H1: The mean PetalLengthCm of Iris-versicolor and Iris-virginica are not equal."")
print(f""T-statistic: {t_stat:.4f}"")
print(f""P-val...",8,"The generated code correctly performs the t-test and interprets the results. It addresses the main requirements, including loading data, filtering, performing the test, stating hypotheses, and printing results with interpretation.

Areas for improvement to achieve a higher score (closer to 10):
- **Error Handling**: The ground truth includes `try-except` for `FileNotFoundError` and checks for empty groups, which are good practices for robust code. The generated code lacks these.
- **Clarity of Output**: While stated, the ground truth's output for hypotheses and conclusions is more verbose and explanatory, making it clearer for someone reviewing the output. For example, explicitly stating ""Conclusion: There is a statistically significant difference..."" vs. just ""Reject the null hypothesis.""
- **Formatting**: The ground truth uses `---` separators which improves readability of the output.

Overall, it's a solid solution that completes the core task effectively.",deepseek_r1,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('iris.csv')

features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

for feature in features:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='Species', y=feature, data=df)
    plt.title(f'Distribution of {feature} by Species')
    plt.xlabel('Species')
    plt.ylabel(feature)
    plt.show()
```",9,"The generated code successfully loads the iris.csv dataset and generates the required box plots. It correctly creates four individual box plots, one for each numerical feature, and each plot groups the data by the `Species` column. The plots also have appropriate titles.

**Strong points:**
- Correctly loads the data.
- Correctly identifies and iterates through the numerical features.
- Uses `seaborn.boxplot` with the correct `x` (Species) and `y` (feature) arguments.
- Generates separate figures for each plot using `plt.figure()`.
- Sets appropriate plot titles.
- Includes all necessary imports.

**Minor areas for improvement (to reach a 10):**
- **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions when loading the CSV, which makes the code more robust. The generated code assumes the file will always be present.
- **Readability/Maintainability:** The ground truth defines constants for `DATASET_PATH`, `NUMERICAL_FEATURES`, and `GROUPING_COLUMN`. While the generated code uses a list `features` which is good, defining 'Species' as a constant would offer a minor improvement in maintainability if the column name ever changed.
- **Plot Customization:** The ground truth includes `plt.xlabel`, `plt.ylabel`, `plt.grid`, and `plt.tight_layout()` for enhanced plot readability and aesthetics. While the core requirements are met without these, they improve the user experience of the plots. `plt.xlabel` and `plt.ylabel` were present in the generated code, which is good, but grid and tight_layout were missing.

Overall, the generated code is highly effective and meets almost all requirements. The missing error handling and minor plot enhancements are negligible enough not to detract significantly from its practical functionality.",deepseek_r1,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score

data = pd.read_csv('heart.csv')
X = data[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']]
y = data['HeartDisease']

preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), ['Se...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
- Correctly loads the data.
- Correctly selects the specified features and target variable.
- Successfully performs one-hot encoding on the specified categorical features using `ColumnTransformer`.
- Correctly splits the data into training and testing sets with the specified `test_size` and `random_state`.
- Trains a `LogisticRegression` model.
- Calculates and prints the accuracy score.
- Includes all necessary imports.

**Areas for improvement (why not a 10):**
1.  **Pipeline Usage:** The ground truth code uses a `Pipeline` which is best practice when preprocessing and modeling. This ensures that preprocessing steps (like `OneHotEncoder`) are applied consistently to new data (e.g., test data or future unseen data) without needing to `fit_transform` the entire `X` beforehand and then split. The generated code `fit_transform`s `X` *before* the split, which means the `ColumnTransformer` is fitted on the entire dataset (`X`). While it works for this specific task, it's generally not ideal for robust machine learning workflows where the preprocessor should only learn from the training data to prevent data leakage.
2.  **`max_iter` for `LogisticRegression`:** The ground truth code increased `max_iter` to 1000 for `LogisticRegression` (`LogisticRegression(max_iter=1000)`). While not strictly required by the prompt, `LogisticRegression` often triggers convergence warnings with default `max_iter` (100) on some datasets, and increasing it is a common fix. The generated code uses default parameters, which might lead to a warning.
3.  **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` for `heart.csv`, which is good practice for robust code. The generated code assumes the file exists.

Overall, the generated code produces the correct result for the given task and demonstrates a good understanding of the required ML steps. The primary area for a perfect score would be the adoption of `Pipeline` for better workflow management and preventing potential data leakage in larger projects, as well as minor robustness improvements.",deepseek_r1,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

data = pd.read_csv('heart.csv')
male_chol = data[data['Sex'] == 'M']['Cholesterol']
female_chol = data[data['Sex'] == 'F']['Cholesterol']

print(""H0: The mean Cholesterol levels of males and females are equal (μ1 = μ2)."")
print(""H1: The mean Cholesterol levels of males and females are not equal (μ1 ≠ μ2)."")

t_stat, p_value = stats.ttest_ind(male_chol, female_chol, equal_var=False)
print(f""T-statistic: {t_stat:.4f}\nP-value: {p_value:.4f}"")
...",8,"The generated code correctly performs the requested independent two-sample t-test, including data loading, group separation, hypothesis statement, t-statistic and p-value calculation, and interpretation based on alpha=0.05. It correctly uses `equal_var=False` for Welch's t-test which is a good practice.

Strong points:
- Correct library imports.
- Correct loading and filtering of data.
- Accurate calculation of t-statistic and p-value using `scipy.stats.ttest_ind`.
- Clear statement of null and alternative hypotheses.
- Correct interpretation of the p-value against the alpha level.

Areas for improvement to reach a higher score (9 or 10):
- **Robustness/Error Handling**: The generated code lacks error handling for `FileNotFoundError` or if expected columns (`Sex`, `Cholesterol`) are missing. The ground truth includes `try-except` blocks for file loading and checks for column existence.
- **Data Preprocessing**: While `dropna()` was not explicitly requested, ensuring that missing values in 'Cholesterol' don't affect the test is a good practice, especially in real-world scenarios. The generated code implicitly assumes no NaNs or that `ttest_ind` handles them gracefully (which it often does by skipping them, but explicit handling is clearer). The ground truth explicitly uses `.dropna()`.
- **Modularity**: The ground truth wraps the logic in a function, `perform_cholesterol_t_test`, making it reusable and easier to manage, especially if the task were part of a larger application. The generated code is a script-like execution.
- **Clarity of Output**: While the output is correct, the ground truth's use of f-strings for more formatted output values, specific ""--- Hypotheses ---"", ""--- T-test Results ---"", and ""--- Interpretation ---"" headers, and more detailed interpretation, makes the output more user-friendly and readable.",deepseek_r1,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

df = pd.read_csv('winequality-red.csv')
cols = ['volatile acidity', 'residual sugar']
outliers_count = {}

for col in cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = (df[col] < lower_bound) | (df[col] > upper_bound)
    outliers_count[col] = outliers.sum()

winsorized_df = df.copy()
for col in cols:
    Q1 = winsorized_df[col]....",7,"The generated code correctly performs the core tasks of identifying outliers using IQR, winsorizing the data, and comparing statistics. It uses `np.clip` for efficient winsorization, which is a good practice.

However, there are several areas for improvement:
1.  **Reporting Outliers (Requirement 2):** While `outliers_count` is calculated, it's only printed at the end, not explicitly ""reported"" column by column as part of the identification process, which was a specific requirement. The prompt asked to ""Report the number of outliers found for each column"", ideally immediately after finding them for that column.
2.  **Redundant IQR Calculation:** The IQR calculation (Q1, Q3, IQR, bounds) is performed twice for the same columns (once for counting outliers and once for winsorization). Although this doesn't lead to incorrect results, it's less efficient and readable. The bounds calculated in the first loop could be stored and reused in the second loop.
3.  **Output Clarity/Completeness (Requirement 4):** The output of the statistics comparison is very concise (`original_stats`, `winsorized_stats`). The prompt specifically asked to ""Compare the mean and standard deviation of these two columns before and after outlier treatment."" The ground truth provides a more readable, column-by-column comparison with clear labels, which aligns better with the ""compare"" aspect. It also implicitly handles the display.
4.  **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError`, which is good practice for loading datasets. The generated code lacks this.
5.  **Readability/Comments:** While the code is functional, it lacks comments and clear print statements to guide the user through the steps as the ground truth does, which was implied by the task description (""Provide only the Python code without explanations"" implies the code itself should convey its steps not external text). The ground truth's verbose print statements fulfill this implicitly.

Overall, it's a good attempt that gets the core logic right but falls short on meeting all the explicit output and structural requirements, and could be more efficient/robust.",deepseek_r1,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('heart.csv')

plt.figure(figsize=(10, 6))
sns.countplot(x='ChestPainType', hue='HeartDisease', data=df, order=['TA', 'ATA', 'NAP', 'ASY'])
plt.title('Heart Disease Count by Chest Pain Type')
plt.xlabel('Chest Pain Type')
plt.ylabel('Count')
plt.legend(title='Heart Disease')
plt.show()
```",7,"The generated code correctly loads the dataset and creates a grouped bar chart as requested. It uses `seaborn.countplot` effectively to display the counts of `HeartDisease` for each `ChestPainType`, which is a good choice for this task. It also includes the required title, labels, and legend.

However, there are a few areas for improvement to reach a higher score:
1.  **Error Handling**: The code lacks error handling for `FileNotFoundError` or other exceptions that might occur during CSV loading. The ground truth includes a robust `try-except` block.
2.  **Modularity**: The generated code is not encapsulated within a function, making it less reusable. The ground truth provides a function (`create_heart_disease_chest_pain_grouped_bar_chart`) with an argument for the file path, improving modularity and reusability.
3.  **Legend Clarity**: While the legend is present, `hue='HeartDisease'` directly uses the numerical values (0 and 1) in the legend. The ground truth improves this by mapping `HeartDisease` to more descriptive labels ('No Heart Disease' and 'Heart Disease'), which significantly enhances readability.
4.  **`ChestPainType` Order**: Although `order=['TA', 'ATA', 'NAP', 'ASY']` is explicitly provided in the `countplot`, the ground truth takes a slightly more robust approach by making `ChestPainType` a `Categorical` type with a defined order, which can be beneficial for consistent ordering in other plots as well.
5.  **Docstrings/Comments**: The ground truth includes a docstring explaining the function's purpose, which is good practice for code readability and maintainability. The generated code lacks any comments or docstrings.
6.  **`if __name__ == '__main__':`**: The ground truth wraps the execution in a `if __name__ == '__main__':` block, which is standard practice for Python scripts to ensure code only runs when the script is executed directly.",deepseek_r1,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

df = pd.read_csv('spotify_2023.csv')
df['in_shazam_charts'] = df['in_shazam_charts'].str.replace(',', '', regex=False)
df['in_shazam_charts'] = pd.to_numeric(df['in_shazam_charts'], errors='coerce')
print(f""NaNs in in_shazam_charts: {df['in_shazam_charts'].isna().sum()}"")
result = df.groupby(['key', 'mode'])[['danceability_%', 'valence_%']].mean().reset_index()
print(result)
```",9,"The generated code correctly addresses all requirements in the task. It loads the CSV, cleans `in_shazam_charts` by removing commas and converting to numeric with error coercion, reports the count of NaNs, groups by `key` and `mode`, and calculates the average `danceability_%` and `valence_%`.

Strengths:
- Correctly loads the CSV and handles the specified columns.
- `str.replace(',', '', regex=False)` is a good practice for literal string replacement.
- `pd.to_numeric(..., errors='coerce')` is correctly used.
- `isna().sum()` correctly reports NaNs.
- `groupby().mean()` and selecting the correct columns is precise.
- `reset_index()` provides a clean output format.

Minor improvements for a perfect score (though not explicitly requested/mandated):
- The print statement for NaNs could be slightly more descriptive matching the ground truth.
- The `reset_index()` is not strictly necessary as `groupby().mean()` already returns a Series or DataFrame that can be printed. However, it doesn't harm correctness and can make the output more readable depending on preference.
- The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice. While not explicitly asked in the prompt, it would make the code more robust.",deepseek_r1,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

df = pd.read_csv('spotify_2023.csv')
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df.dropna(subset=['streams', 'in_spotify_playlists', 'energy_%'], inplace=True)

X = df[['in_spotify_playlists', 'energy_%']]
y = df['streams']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_si...",9,"The generated code correctly implements all the requested steps for the task. It loads the dataset, handles the 'streams' column coercion and NaN dropping correctly, selects features and target, splits the data, trains a Linear Regression model, and reports R2 and MSE. The imports are correct and the structure is logical.

The only minor area for improvement is the lack of robust error handling for `FileNotFoundError` when loading the CSV, which was present in the ground truth. While not explicitly asked for, it's good practice. Also, the output messages are more concise than the ground truth. However, for a coding task focused on functional correctness, this is a very strong submission.",deepseek_r1,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

df = pd.read_csv('spotify_2023.csv')

df['release_date'] = pd.to_datetime(
    df['released_year'].astype(str) + '-' +
    df['released_month'].astype(str).str.zfill(2) + '-' +
    df['released_day'].astype(str).str.zfill(2),
    errors='coerce'
)

fixed_date = pd.to_datetime('2023-12-31')
df['days_since_release'] = (fixed_date - df['release_date']).dt.days.astype('Int64')

print(df[['track_name', 'release_date', 'days_since_release']].head(5))
```",9,"The generated code is highly correct and directly addresses almost all requirements of the task.

**Strong points:**
*   Correctly loads the CSV.
*   Successfully combines `released_year`, `released_month`, and `released_day` into `release_date`.
*   Correctly uses `errors='coerce'` to handle invalid dates, setting them to NaT.
*   Accurately calculates `days_since_release` as the difference between `fixed_date` and `release_date`.
*   Converts `days_since_release` to an integer type (`Int64`), which handles NaNs correctly.
*   Displays the requested columns (`track_name`, `release_date`, `days_since_release`) for the first 5 tracks using `.head(5)`.
*   Includes the necessary `pandas` import.

**Minor areas for improvement (prevented a perfect 10):**
*   **Lack of File Not Found Error Handling:** The ground truth includes `try-except` blocks for file loading, which is good practice for robust code. The generated code assumes the file will always be present, which might lead to a `FileNotFoundError` if the file doesn't exist, crashing the script. This wasn't explicitly asked for in the prompt's requirements but is generally part of robust code quality.
*   **No output descriptions:** The ground truth includes `print` statements to indicate success or describe output sections (e.g., ""--- First 5 tracks with new features ---""). While not strictly a functional requirement for the output, it improves user experience and clarity in a real-world script.

Overall, the generated code is very good and functionally complete for the core requirements. The minor points are related to code robustness and user-friendliness rather than core functionality.",deepseek_r1,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

df = pd.read_csv('youtube_trendings.csv')

for col in ['Video views', 'Likes']:
    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '', regex=False), errors='coerce').fillna(0)

df['Category'] = df['Category'].fillna('Unknown')

grouped = df.groupby('Category')[['Video views', 'Likes']].sum()

print(grouped)
```",9,"The generated code is highly correct and concise, achieving almost all task requirements.

**Strong Points:**
- **Correctness:** It correctly loads the dataset, cleans 'Video views' and 'Likes' by removing commas, converting to numeric (treating errors as NaN), and filling NaNs with 0. It also correctly fills missing 'Category' values with 'Unknown'. Finally, it groups by 'Category' and calculates the sum of cleaned 'Video views' and 'Likes'.
- **Conciseness:** The loop for processing 'Video views' and 'Likes' is elegant and efficient.
- **Completeness:** It addresses all the core requirements of the task.

**Areas for Minor Improvement:**
- **Error Handling (Minor):** The generated code lacks explicit error handling for `FileNotFoundError` if `youtube_trendings.csv` doesn't exist. The ground truth includes a `try-except` block for this, which is good practice in real-world scenarios. However, the task prompt implies the file will be present, so this is a minor omission for a benchmark.
- **Column Existence Checks (Minor):** The ground truth explicitly checks for the existence of columns ('Video views', 'Likes', 'Category') before attempting operations on them. While not strictly required by the prompt (which assumes these columns exist), it makes the code more robust to unexpected dataset schemas. For this specific task, it's not a critical flaw.
- **Display Output (Minor):** The ground truth includes a print statement explaining the output (`""Sum of Video Views and Likes per Category:""`), which enhances readability. The generated code just prints the DataFrame directly, which is acceptable but slightly less user-friendly.

Overall, it's a very good solution that perfectly accomplishes the core logic. The minor points are mostly about robustness and presentation, which are often considered good practices but not always explicitly required by a task description.",deepseek_r1,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Likes' column
df['Likes'] = df['Likes'].astype(str).str.replace(',', '').apply(pd.to_numeric, errors='coerce').fillna(0)

# Fill missing 'Category' values
df['Category'] = df['Category'].fillna('Unknown')

# Create groups
music_likes = df[df['Category'] == 'Music']['Likes']
sports_likes = df[df['Category'] == 'Sports']['Likes']

# Perform t-test
t_stat, p_val = stats.ttes...",7,"The generated code correctly performs the core data loading, cleaning, and t-test operations. It handles the `Likes` column cleaning (commas, numeric conversion with errors='coerce', fillna(0)) and the `Category` column missing values. The t-test is correctly applied using `equal_var=False`.

However, it misses several explicit requirements from the prompt:
1.  **Stating hypotheses clearly**: The hypotheses are commented out and not printed in a user-friendly format as requested. The prompt specifically asked to ""Clearly state the null and alternative hypotheses for this test.""
2.  **Detailed interpretation**: While it correctly states ""Reject the null hypothesis"" or ""Fail to reject the null hypothesis"", it lacks the more comprehensive interpretation and conclusion about statistical significance as seen in the ground truth. The prompt asked to ""interpret the p-value and state whether to reject the null hypothesis.""
3.  **Robustness/Error Handling**: It does not include error handling for `FileNotFoundError` when loading the CSV, nor does it check if there are sufficient samples in the 'Music' and 'Sports' categories to perform a meaningful t-test, which could lead to errors or misleading results if either group is empty or too small.
4.  **Clarity of output**: The output for t-statistic and p-value is concise but lacks descriptive text (e.g., ""Calculated t-statistic:"", ""Calculated p-value:"").
5.  **New column for cleaned Likes**: While the cleaning is correct, it overwrites the original 'Likes' column instead of creating a new one (e.g., `Likes_cleaned`) as implicitly suggested by the ground truth's approach using `Likes_cleaned` to keep the original if needed and for better separation of concerns. This isn't a critical error but a minor point for good practice.

Improvements would involve adding the robust error handling, explicitly printing the hypotheses and the full interpretation based on the p-value and alpha, and more verbose output.",deepseek_r1,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd

df = pd.read_csv('youtube_trendings.csv')

for col in ['Likes', 'Dislikes']:
    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '', regex=False), errors='coerce').fillna(0)

df['TotalEngagement'] = df['Likes'] + df['Dislikes']
df['LikeDislikeRatio'] = df['Likes'] / (df['Dislikes'] + 1)

print(df[['Video', 'TotalEngagement', 'LikeDislikeRatio']])
```",10,"The generated code correctly addresses all requirements of the task. It loads the dataset, efficiently cleans 'Likes' and 'Dislikes' columns by removing commas, converting to numeric, and filling NaNs with 0 in a concise loop. It then correctly calculates `TotalEngagement` and `LikeDislikeRatio` as specified, including the important `+1` to prevent division by zero. Finally, it displays the required columns. The use of a loop for cleaning `Likes` and `Dislikes` is a good practice for conciseness and avoiding code duplication. The `regex=False` in `str.replace` is also a good, albeit minor, optimization for simple string replacements.",deepseek_r1,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd

df = pd.read_csv('mobiles_2025.csv')
df['Launched Price (USA)'] = pd.to_numeric(
    df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''), errors='coerce'
)
df['Screen Size'] = pd.to_numeric(df['Screen Size'].str.replace(' inches', ''), errors='coerce')
aggregated = df.groupby(['Company Name', 'Launched Year']).agg(
    Average_Launched_Price=('Launched Price (USA)', 'mean'),
    Average_Screen_Size=('Screen Size', 'mean')
).reset_index()
print(a...",9,"The generated code correctly addresses all requirements of the task. It loads the CSV, cleans both 'Launched Price (USA)' and 'Screen Size' columns to numeric types, handles conversion errors by coercing to NaN, groups by the specified columns, and calculates the correct averages. The output format is also as expected.

Strong points:
- Concise and efficient cleaning steps for both columns.
- Correct use of `errors='coerce'` for handling tricky conversions.
- Accurate grouping and aggregation using `.agg()`.
- Uses `reset_index()` to get the desired output format.

Minor areas for improvement (not critical for correctness, hence the high score):
- The `Launched Price (USA)` cleaning could benefit from `regex=False` in `str.replace()` for slightly better performance and clarity when not using regex patterns, though it works fine without it here.
- The task asked for ""Display the resulting aggregated data."" which the code does, but `print()` is explicitly used. This is generally good practice to ensure output even in environments that might not auto-print the last variable.
- Error handling for `FileNotFoundError` is missing for the CSV loading, though not strictly required by the prompt's `Assume the CSV file is in the current directory` statement.",deepseek_r1,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

df = pd.read_csv('mobiles_2025.csv')

def clean_camera_spec(spec):
    if pd.isna(spec):
        return 0
    spec_str = str(spec)
    match = re.search(r'\d+', spec_str)
    return int(match.group()) if match else 0

df['Cleaned Front Camera'] = df['Front Camera'].apply(clean_camera_spec)
df['Cleaned Back Camera'] = df['Back Camera'].apply(clean_camera_spec)

df['Total Camera MP'] = df['Cleaned Front Camera'] + df['Cleaned Back Camera']

print(df[['Model...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task. It successfully loads the dataset, defines and applies the cleaning function to extract MP values, handles non-numeric cases by returning 0, creates the `Total Camera MP` feature, and displays the required columns for the first 10 rows.

**Strong points:**
*   Correctly uses `pandas` for data loading and manipulation.
*   The `clean_camera_spec` function is well-implemented, correctly extracting the first digit sequence and handling `NaN` values.
*   Properly uses `re` for regular expression matching.
*   Calculates `Total Camera MP` correctly.
*   Displays the requested columns for the first 10 rows as required.

**Areas for minor improvement (preventing a perfect 10):**
*   The prompt asked for ""proper imports (pandas, re)"" which are present, but the generated code doesn't include the `try-except` block for `FileNotFoundError` when loading the CSV, which was present in the ground truth for robustness. While not strictly part of the feature engineering task, it's good practice for loading data and makes the script more production-ready.
*   The output display for the first 10 rows could benefit from `to_string()` for better console formatting, as seen in the ground truth, but `print(df.head(10))` is perfectly acceptable for displaying DataFrames.
*   The ground truth included docstrings and type hints in the `clean_camera_mp` function, which contribute to better code quality and maintainability, though not explicitly asked for in the prompt.

Overall, it's a very strong submission that fulfills the core task perfectly.",deepseek_r1,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

df = pd.read_csv('mobiles_2025.csv')

df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
df['Screen Size'] = pd.to_numeric(df['Screen Size'].str.replace(' inches', ''), errors='coe...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, cleans the specified columns by removing units and converting them to numeric types (handling errors by coercing to NaN and then dropping rows), defines features and target, splits the data, trains a Linear Regression model, and reports the R-squared and MAE.

Strengths:
- Correctly implements all steps.
- Uses appropriate pandas and scikit-learn functions.
- Handles cleaning of both 'USD ' and commas for 'Launched Price (USA)'.
- Correctly uses `errors='coerce'` and `dropna(subset=...)` for robust cleaning.
- Uses `random_state=42` for reproducible splitting.

Areas for minor improvement (not critical enough to reduce score significantly):
- The `print` statements for R2 and MAE are a bit terse (`R2: ...`, `MAE: ...`). The ground truth provides more descriptive output, which is generally better for readability and understanding of the results.
- The generated code doesn't include error handling for `FileNotFoundError` when loading the CSV, which is a good practice as shown in the ground truth. While not explicitly asked for in the prompt, it makes the code more robust.
- The `strip()` operation after `replace()` isn't explicitly included in the generated code, which `replace` sometimes handles (especially with `regex=False`), but it's a good defensive coding practice that the ground truth implements. For this specific task and patterns, it might not be strictly necessary as the patterns are at the ends of strings.",deepseek_r1,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency, ttest_ind, f_oneway

titanic = pd.read_csv('titanic.csv')

# Chi-square test for Pclass and Survival
contingency_table = pd.crosstab(titanic['Pclass'], titanic['Survived'])
chi2, p_chi, dof, expected = chi2_contingency(contingency_table)
n = contingency_table.sum().sum()
min_dim = min(contingency_table.shape) - 1
cramer_v = np.sqrt(chi2 / (n * min_dim))

# T-test for Age between Survivors and Non-Survivors...",7,"The generated code correctly performs the chi-square test, t-test, and ANOVA, and calculates the specified effect sizes. It also prints the p-values, test statistics, and conclusions for each test, addressing the core requirements.

However, there are areas for improvement regarding robustness, clarity, and adherence to statistical best practices:

1.  **Robustness of T-test (Cohen's d and `equal_var`):**
    *   The calculation of `pooled_std` for Cohen's d is too simplistic (averaging squares). The correct formula for pooled standard deviation for Cohen's d is `sqrt(((n1-1)*s1^2 + (n2-1)*s2^2) / (n1+n2-2))`. The generated code assumes equal sample sizes and simply averages the variances, which is incorrect.
    *   The `ttest_ind` function by default assumes equal variances (`equal_var=True`). It's best practice to formally test for equality of variances (e.g., using Levene's test) and set `equal_var=False` if variances are significantly different (Welch's t-test), as seen in the ground truth. The generated code does not do this, potentially leading to incorrect p-values if the variance assumption is violated.

2.  **Missing Error Handling:**
    *   The ground truth includes error handling for `FileNotFoundError` when loading the CSV, which is a good practice. The generated code assumes the file exists.
    *   It also lacks checks for empty dataframes or groups after `dropna()` operations before attempting calculations, which could lead to errors (e.g., division by zero or `nan` results that are not handled gracefully).

3.  **Clarity and Interpretation:**
    *   While it states ""Reject H0"" or ""Fail to reject H0"", the ground truth provides a more verbose and interpreted conclusion (e.g., ""There is a statistically significant association...""). This is important for clarity.
    *   The ground truth also provides initial printed statements and section headers which improve readability for the user, especially when this is used as a standalone script for analysis.

4.  **Assumptions/Warnings for ANOVA:**
    *   The ground truth explicitly checks for homogeneity of variances for ANOVA using Levene's test and prints a warning if violated. This is crucial for interpreting ANOVA results accurately. The generated code omits this.

5.  **Small issue with Cohen's d calculation:** If `n1 + n2 - 2` results in 0 or less (e.g., very small groups, or only one data point per group), the ground truth correctly handles preventing division by zero for pooled standard deviation. The generated code does not explicitly handle this for `pooled_std`.

Overall, it's a solid attempt that gets most of the statistical tests right, but falls short in robustness, detailed interpretation, and adhering to more rigorous statistical methodologies, especially regarding assumptions for parametric tests.",deepseek_r1,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('imdb_top_250.csv')
grouped = df.groupby('Year')[['Rating', 'Runtime']].mean()
plt.plot(grouped.index, grouped['Rating'])
plt.title('Average Rating Trend Over Years')
plt.xlabel('Year')
plt.ylabel('Average Rating')
plt.show()
```",6,"The generated code correctly performs the core tasks of loading data, grouping by `Year`, calculating average `Rating` and `Runtime`, and plotting the average `Rating`. However, it has significant shortcomings in robustness and completeness when compared to the ground truth.

**Strengths:**
*   Correctly loads the CSV using pandas.
*   Successfully groups by `Year` and calculates the mean for `Rating` and `Runtime`.
*   Generates a line plot of `Average Rating` over `Year`.
*   Includes a title and axis labels for the plot.

**Weaknesses and Areas for Improvement:**
*   **No Data Cleaning/Preprocessing:** The most critical missing piece is the lack of data cleaning for 'Year', 'Runtime', and 'Rating' columns. The ground truth code handles potential non-numeric values in 'Year' (e.g., extracting 4-digit year), cleans 'Runtime' (removes ' min' and converts to int), and ensures 'Rating' is numeric. The generated code assumes clean data, which is rarely the case in real-world scenarios, and would likely fail or produce incorrect results if the 'imdb_top_250.csv' contains even slight variations in formatting (e.g., '2000s' or '120 min').
*   **Missing Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError`, `KeyError`, and general `Exception`. The generated code has no error handling, making it fragile if the file doesn't exist or columns are missing.
*   **No Function Encapsulation:** The ground truth encapsulates the logic within a function (`analyze_imdb_data`), which is good practice for reusability and organization. The generated code is a flat script.
*   **Plotting Enhancements:** While it meets the basic plotting requirements, the ground truth's plot is more robust (e.g., `figsize`, `marker`, `linestyle`, `grid`, `xticks` rotation, `tight_layout`) for better readability and presentation.
*   **`reset_index()` for Grouped Data:** Although `grouped.index` works for plotting, explicitly calling `reset_index()` after `groupby().agg()` as in the ground truth makes the grouped data (e.g., `yearly_stats`) an ordinary DataFrame, which is generally more flexible and readable for subsequent operations beyond just plotting.",deepseek_r1,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

df = pd.read_csv('imdb_top_250.csv')

group1 = df[df['Year'] < 1990]['Rating']
group2 = df[df['Year'] >= 1990]['Rating']

t_stat, p_value = stats.ttest_ind(group1, group2)

print(""Null hypothesis (H0): The mean ratings of both groups are equal (μ1 = μ2)"")
print(""Alternative hypothesis (H1): The mean ratings of both groups are not equal (μ1 ≠ μ2)"")
print(f""T-statistic: {t_stat:.4f}"")
print(f""P-value: {p_value:.4f}"")

alpha = 0.05
if p_value <...",8,"The generated code correctly performs the core task: loading the data, creating the two groups, running the independent two-sample t-test, and interpreting the results based on the significance level. It also properly states the hypotheses and prints the required values.

Areas for improvement (to reach a 10):
1.  **Robustness in Data Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` and explicit `pd.to_numeric` conversions with `errors='coerce'` followed by `dropna`. This makes the code more robust to common data issues (missing file, non-numeric data in 'Year' or 'Rating'). The generated code assumes the file exists and 'Year'/'Rating' columns are already clean.
2.  **Hypotheses Detail:** While the generated code states the null and alternative hypotheses, the ground truth provides a more detailed and context-specific formulation, directly referencing ""movies released before 1990"" and ""after 1990"" which aligns better with the problem description.
3.  **Clarity of Interpretation:** The interpretation in the generated code is concise but could be slightly more detailed, similar to the ground truth which reiterates the specific conclusion about the ""mean 'Rating' between movies released before 1990 and movies released in or after 1990.""
4.  **Edge Cases for T-test:** The ground truth includes a check for group sizes (`if len(ratings_before_1990) < 2...`) which is good practice to prevent errors if one of the groups ends up being too small for a t-test.
5.  **`equal_var` parameter**: The ground truth explicitly states `equal_var=True` and comments on its implication, which is good practice for statistical tests where this assumption is critical. The default is `True`, so it's not strictly incorrect, but explicit is better for clarity.

Overall, it's a very good attempt that fulfills most of the requirements. The points above are about making it more robust, self-documenting, and statistically solid.",deepseek_r1,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

df = pd.read_csv('imdb_top_250.csv')
df['Decade'] = (df['Year'] // 10 * 10).astype(str) + 's'
print(df[['Title', 'Year', 'Decade']].head(10))
```",7,"The generated code correctly performs the core task of loading the data, creating the 'Decade' column, and displaying the requested information for the first 10 movies. The logic for creating the `Decade` column `(df['Year'] // 10 * 10).astype(str) + 's'` is concise and correct.

However, the code lacks some elements present in the ground truth that contribute to robustness and good practice:
1.  **Error Handling**: The generated code doesn't include any error handling for `FileNotFoundError` or `KeyError` if the CSV file or expected columns are missing, unlike the ground truth which uses `try-except` blocks.
2.  **Modularity/Functions**: The ground truth encapsulates the logic into functions (`perform_feature_engineering`, `main`), which makes the code more organized, reusable, and testable. The generated code is a simple script.
3.  **Comments/Docstrings**: There are no comments or docstrings explaining the code's purpose or specific steps.
4.  **Clarity/Readability (minor)**: While correct, directly chaining the operations without breaking them down might be slightly less readable for very complex transformations, though for this specific task it's acceptable.
5.  **Robustness for 'Year' column**: The ground truth explicitly handles the case where the 'Year' column might not be numeric by using `pd.to_numeric` with `errors='coerce'` and then dropping NaNs. While the task implies the data is clean, a more robust solution considers data quality issues.

Overall, it's a functional solution that nails the core logic but could be improved in terms of production readiness, robustness, and software engineering best practices.",deepseek_r1,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

df = pd.read_csv('penguins.csv')

numerical_columns = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numerical_columns:
    df[col] = df[col].fillna(df[col].mean())

mode_sex = df['sex'].mode()[0]
df['sex'] = df['sex'].fillna(mode_sex)

result = df.groupby(['species', 'sex'])['body_mass_g'].mean()
print(result)
```",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, imputes missing numerical values with the column mean, imputes missing categorical values in 'sex' with the mode, groups the data by 'species' and 'sex', and calculates the average 'body_mass_g'. The code is concise and directly implements the stated steps.

One minor area for improvement, present in the Ground Truth but not the generated code, is the inclusion of robust error handling (e.g., `try-except` blocks for `FileNotFoundError`). While not explicitly asked for in the prompt's requirements, it is good practice for real-world scenarios. Also, the Ground Truth explicitly checks `if df[col].isnull().any():` before calculating the mean and filling, which is a minor optimization to avoid unnecessary operations (though the `fillna(mean())` in pandas handles this gracefully). However, for the given task, the generated code meets all specific requirements effectively.",deepseek_r1,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

penguins = pd.read_csv('penguins.csv')
filtered = penguins[(penguins['species'].isin(['Adelie', 'Gentoo']))]
filtered_clean = filtered.dropna(subset=['flipper_length_mm', 'species'])

adelie = filtered_clean[filtered_clean['species'] == 'Adelie']['flipper_length_mm']
gentoo = filtered_clean[filtered_clean['species'] == 'Gentoo']['flipper_length_mm']

t_stat, p_value = stats.ttest_ind(adelie, gentoo)

print(""Null hypothesis (H0): The mean fli...",8,"The generated code correctly performs the data loading, filtering, NA removal, and statistical test as required. It also correctly states the hypotheses, prints the t-statistic and p-value, and interprets the result based on the significance level.

Strengths:
- Correctly loads the CSV.
- Accurately filters for 'Adelie' and 'Gentoo' species.
- Correctly handles NA values in the specified columns.
- Properly separates data for the two groups.
- Performs the `ttest_ind` correctly.
- States the null and alternative hypotheses.
- Prints the t-statistic and p-value.
- Correctly interprets the p-value against `alpha=0.05`.

Areas for improvement (preventing a perfect score):
- Error handling for `FileNotFoundError` when loading the CSV is missing. The ground truth includes this, which is good practice for robust code.
- The `ttest_ind` function has an `equal_var` parameter which defaults to `True`. While not explicitly specified in the prompt, in scientific contexts, it's often better to consider Welch's t-test (`equal_var=False`) if variances are not assumed to be equal. The ground truth code explicitly sets `equal_var=False`, which adds robustness.
- The output for the t-statistic and p-value could be formatted for better readability (e.g., to a specific number of decimal places).
- The prompt asked ""Clearly state the null and alternative hypotheses for this test."" While the code prints them, they could be slightly more formal or detailed, explicitly mentioning the means (e.g., ""The mean flipper length of Adelie is equal to the mean flipper length of Gentoo""). The current phrasing ""The mean flipper length of Adelie and Gentoo penguins is equal"" is acceptable but could be more precise.
- The prompt asked for ""proper imports"". While `pandas` and `scipy.stats` are imported, explicit self-documenting steps (like in the ground truth) make the code easier to follow for someone new to it. However, the prompt only asked for code, so this is a minor stylistic point.",deepseek_r1,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

data = pd.read_csv('penguins.csv')
data = data[['species', 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']].dropna()
X = data.drop('species', axis=1)
y = data['species']

le = LabelEncoder()
y = le.fit_transform(y)

X_train, X_test, y_train, y_tes...",9,"The generated code correctly implements all the requested steps for data preparation and model training. It loads the data, selects the specified features and target, handles missing values (though implicitly through column selection before `dropna`), encodes the target, splits the data, trains a Logistic Regression model, and reports the accuracy.

Strong points:
- All required libraries are imported.
- Data loading is correct.
- Feature and target selection are correct.
- Missing values are handled for the relevant columns.
- `LabelEncoder` is used correctly for the target variable.
- `train_test_split` is used with the correct `test_size` and `random_state`.
- `LogisticRegression` is initialized with `max_iter=1000`.
- The model is trained and evaluated using `accuracy_score`.

Areas for minor improvement (not critical for functionality but improve robustness and clarity):
- The `dropna()` is applied after selecting columns. While this achieves the desired outcome of dropping NAs in the specified columns, it could be made more explicit that only *those* columns are checked for NAs. The ground truth code `df_selected = df[feature_columns + [target_column]].copy()` followed by `df_cleaned = df_selected.dropna()` is a more robust way to ensure NAs are only dropped based on the critical columns, rather than potentially dropping rows due to NAs in *other* columns not used in the model. However, in this specific case, selecting the columns then dropping NAs on that subset works the same.
- No explicit `print` statements to show intermediate steps or confirmations (e.g., data loaded, sizes of splits, accuracy). This is part of the prompt's `report` requirement and adds clarity for a user. While the accuracy is printed, other intermediate steps are not.",deepseek_r1,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows with missing body mass or species
df_clean = df.dropna(subset=['body_mass_g', 'species'])

# Calculate average body mass per species
avg_mass = df_clean.groupby('species')['body_mass_g'].mean().reset_index()

# Create bar plot
plt.figure(figsize=(10, 6))
sns.barplot(x='species', y='body_mass_g', data=avg_mass)
plt.title('Average Body Mass by Pengui...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, correctly drops rows with NA values in the specified columns (`body_mass_g` and `species`), calculates the average body mass per species, and generates a bar plot with appropriate titles and labels.

**Strong Points:**
*   **Correctness:** All steps are implemented correctly, leading to the desired output.
*   **Conciseness:** The code is clean and to the point, avoiding unnecessary verbosity.
*   **Readability:** Variable names are intuitive, and the flow of operations is easy to follow.
*   **Plotting:** Uses Seaborn for a visually appealing plot and correctly sets title and axis labels.

**Minor Area for Improvement (Not critical for this task, but good practice):**
*   **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is robustness. While not strictly required by the prompt, it's a good practice for production-ready code. However, for a data science benchmark focusing on analysis steps, its omission doesn't detract from the core task completion.
*   **Plot Customization:** The ground truth code has more extensive plot customization (e.g., `palette`, `xticks rotation`, `grid`, `tight_layout`), which makes the plot more professional. The generated code's plot is functional and meets the basic labeling requirements, but could be enhanced for presentation.",deepseek_r1,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
penguins = pd.read_csv('penguins.csv')

# Drop rows with missing values in specified columns
columns_to_clean = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
penguins_clean = penguins.dropna(subset=columns_to_clean)

# Create new feature 'culmen_area'
penguins_clean['culmen_area'] = penguins_clean['culmen_length_mm'] * penguins_clean['culmen_depth_mm']

# Calculat...",9,"The generated code successfully completes all aspects of the task. It correctly loads the dataset, handles NA values in the specified columns, creates the `culmen_area` feature, calculates the Pearson correlation matrix for the required columns, and visualizes it as an annotated heatmap with the correct title. The use of `dropna(subset=...)` is correct for the NA handling. The code is clean and well-structured, making it easy to understand.

Minor points for improvement (not affecting the score significantly):
- Error handling for `FileNotFoundError` when loading the CSV is not included, which is present in the ground truth. While not explicitly asked for, it's good practice.
- The `plt.xticks` and `plt.yticks` rotations for better label readability are omitted, which were in the ground truth but are often a stylistic choice.
- No print statements to confirm steps or display intermediate results (like the correlation matrix itself or row counts), which makes debugging or understanding execution flow harder. However, the task only asked for code, not extensive logging.",deepseek_r1,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd

# Load the dataset
data = pd.read_csv('penguins.csv')

# Select features and target
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
X = data[features]
y = data['species']

# Impute numerical features with mean
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
X[numerical_cols] = X[numerical_cols].fillna(X[numerical_cols].mean())

# Impute and encode 'sex' column
X['se...",9,"The generated code correctly addresses almost all requirements of the task. It loads the data, selects the specified features and target, correctly imputes numerical NA values with the mean, imputes and encodes the `sex` column as required, and applies one-hot encoding to the `island` column. It also prints the shape and the first 5 rows of the processed feature matrix as requested.

Strong points:
- Concise and efficient code.
- Correct use of `fillna` for numerical and categorical imputation.
- Correct application of `map` for `sex` encoding.
- Correct use of `get_dummies` for one-hot encoding.

Areas for minor improvement (preventing a perfect 10):
- **SettingWithCopyWarning:** The line `X[numerical_cols] = X[numerical_cols].fillna(X[numerical_cols].mean())` and `X['sex'] = X['sex'].fillna('Unknown').map(...)` could potentially raise a `SettingWithCopyWarning` in some pandas versions or if `X` is a view rather than a copy. While it often works, explicitly using `.loc` or `.copy()` at an earlier stage (like `X = data[features].copy()`) is generally safer practice to avoid this warning and ensure modifications are made on the intended DataFrame.
- **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code lacks this.
- **Clarity of `NaN` after mapping:** While the `sex` mapping is correct, it doesn't explicitly check or handle cases where there might be other unexpected values in the 'sex' column that would then become `NaN` after mapping. The ground truth code includes a fallback `fillna` for such cases, adding robustness. However, for the given problem statement, where 'MALE', 'FEMALE', and 'Unknown' (after imputation) are implicitly the only expected values, this is a very minor point.",deepseek_r1,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST001']
}

df = pd...",9,"The generated code correctly creates the DataFrame, calculates total spending per customer, identifies and prints the top 3 customers by spending, and calculates total quantity sold per category. The code is well-structured and uses appropriate pandas operations. The `to_string(index=False)` for printing the category quantities is a nice touch for cleaner output.

A minor point for improvement, which keeps it from a perfect 10, is the use of `df['TotalSpending'] = df['Quantity'] * df['Price_USD']`. While this works for the current task, it's generally better practice to calculate the line item total (`df['LineTotal']` as in the ground truth) first, and then group by `CustomerID` to sum these line totals. This is because `TotalSpending` as a new column in `df` implies a per-row total which is then summed, but the column name could be confusing if you were to interpret it as a column representing *each customer's total spending* before aggregation. The ground truth's `LineTotal` is more accurate for what it represents at the row level. However, for the given problem's specific calculations, both approaches yield the correct final sums. Another minor detail is the lack of formatting for the currency output (e.g., `$,.2f`) which was present in the ground truth for better readability. Nonetheless, the core logic and results are correct and complete as per the task requirements.",deepseek_r1,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, roc_auc_score, classification_report,
                             confusion_matrix, Confu...",6,"The generated code correctly addresses most of the requirements but has significant shortcomings in data preprocessing and the display of feature coefficients.

**Strong points:**
- Correctly loads data and drops irrelevant columns.
- Handles missing values for 'Age' and 'Embarked'.
- Implements train-test split.
- Trains a logistic regression model using a pipeline.
- Evaluates the model using multiple metrics (accuracy, precision, recall, F1, ROC-AUC).
- Visualizes confusion matrix and ROC curve.
- Performs cross-validation.
- Includes necessary imports.

**Areas for improvement:**

1.  **Data Preprocessing (Categorical Variables and Scaling):**
    *   The `titanic['Sex'] = titanic['Sex'].map({'male':0, 'female':1})` and `titanic = pd.get_dummies(titanic, columns=['Embarked'], drop_first=False)` steps are performed *before* the `StandardScaler` in the pipeline. This means:
        *   `Sex` is treated as a numerical feature by `StandardScaler`, which is incorrect for a binary categorical variable especially when it's ordinal encoded (0 or 1). It should ideally be one-hot encoded or handled appropriately without scaling.
        *   `Embarked` is one-hot encoded before the pipeline, meaning `StandardScaler` will attempt to scale the one-hot encoded columns, which is inappropriate.
    *   The `StandardScaler` is applied to *all* features in `X` because it's the first step in `make_pipeline`. This incorrectly scales one-hot encoded `Embarked` and the mapped `Sex` column, which should not be scaled in the same way as continuous numerical features like `Age` and `Fare`.
    *   The ground truth uses `ColumnTransformer` and separate pipelines for numerical scaling and categorical one-hot encoding, which is the correct and robust way to handle mixed data types in a scikit-learn pipeline.

2.  **Feature Coefficients Interpretation:**
    *   The `coef_df` displays coefficients for `X.columns` directly. However, after one-hot encoding `Embarked` and mapping `Sex` directly on the DataFrame, the columns of `X` are correct *for the input to the model*. The issue is that `Sex` was transformed to 0/1 and then scaled incorrectly, and `Embarked` was one-hot encoded and then scaled incorrectly *before* being fed to the model. Also, it doesn't clearly distinguish between the original features and their transformed versions (e.g., `Embarked_S`, `Embarked_C`).
    *   The generated code also *lacks the interpretation* of coefficients, which was explicitly requested in the prompt (""Display feature coefficients and their interpretation"").

3.  **Cross-Validation:**
    *   Only `accuracy` is used for cross-validation (`scoring='accuracy'`). The prompt asked for evaluation with multiple metrics (accuracy, precision, recall, F1-score, ROC-AUC) and the ground truth demonstrated how to do this using `cross_validate` with a dictionary of scoring metrics. `cross_val_score` is simpler but less flexible for multiple metrics.
    *   Cross-validation is performed only on `X_train`, `y_train`, but it should ideally be performed on the full dataset (`X`, `y`) for a robust estimate of overall model performance, or at least on the training set without prior splitting if the goal is hyperparameter tuning within the training phase. The ground truth performs it on the full dataset.

4.  **Minor Issues:**
    *   `classification_report` is printed, which is good, but the prompt specifically asked for Accuracy, Precision, Recall, F1, ROC-AUC to be displayed, which are already done. This isn't a major flaw but an additional output.
    *   No handling of `FamilySize` feature engineering, as seen in the ground truth. This is a common and useful step for the Titanic dataset.",deepseek_r1,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']),
    'Ticker': ['TECHCO']*8,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0, 157.0, 154.0],
    'Volum...",9,"The generated code correctly addresses all parts of the task. It properly creates the DataFrame, sets the index, calculates the 3-day SMA for 'Close' (handling NaNs for initial days correctly), and calculates the percentage change in 'Volume' (handling the first day with NaN correctly). The only minor issue is the `* 100` in the `Volume_Pct_Change` calculation. While it's common to express percentage changes as numbers out of 100, `pct_change()` returns values as fractions (e.g., 0.20 for 20%), which is typically standard in data analysis unless explicitly stated to be scaled. The prompt asked for ""percentage change,"" which usually implies the fractional value. If it had asked for ""percentage change in percent,"" then multiplying by 100 would be appropriate. This is a very minor interpretation difference and doesn't detract significantly from the correctness. The code is well-structured and uses appropriate pandas methods.",deepseek_r1,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]
}

df = pd.DataFrame(data)

df['Score_...",9,"The generated code correctly performs all the required tasks: it creates the DataFrame, calculates the 'Score_Improvement' column, computes and prints the average improvement per subject, and identifies students meeting the specified score and attendance criteria.

The code quality is good, using standard pandas operations efficiently. Imports are correct.

The only minor point of difference, not a full correctness issue, is the print format for the identified students. The ground truth code iterates and prints each `StudentID` on a new line and handles the case where no students meet the criteria explicitly. The generated code uses `.to_string(index=False)` which prints the Series directly but might not be ideal if you strictly wanted individual student IDs on separate lines. However, for a data science benchmark, this is usually acceptable, and it does display the correct IDs.

One minor improvement would be to add a check for an empty result in the identification step, similar to the ground truth, to provide a more descriptive message if no students meet the stringent criteria. However, for the given data, there is a student who meets the criteria.",deepseek_r1,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

tech_high_perf = df[(df['Department'] == 'Tech') & (df['...",9,"The generated code is highly correct and concise. It successfully defines the DataFrame, filters it based on the specified conditions (""Tech"" department and ""PerformanceScore"" greater than 3), and accurately calculates the average ""ProjectsCompleted"" for the filtered subset.

**Strong points:**
*   **Correctness:** The code produces the correct output for the given task.
*   **Conciseness:** It achieves the task with minimal and efficient lines of code.
*   **Readability:** Variable names are clear and self-explanatory (`tech_high_perf`, `average_projects`).

**Areas for minor improvement (to reach a perfect 10):**
*   **Output Formatting:** The ground truth code formats the output using an f-string with a more descriptive message (`The average number of 'ProjectsCompleted' for Tech department employees with PerformanceScore > 3 is: {average_projects_completed:.2f}`). The generated code just prints the raw number, which is functional but less user-friendly for a benchmark.
*   **Edge Case Handling:** The ground truth code includes an `if not filtered_df.empty:` check to gracefully handle scenarios where no employees match the filtering criteria. While not strictly required by the prompt's output specification, this is a good practice for robust code. For this specific dataset, there will always be a result, but in a more general scenario, such a check is valuable.

Overall, it's a very strong submission that fulfills the core requirements effectively.",deepseek_r1,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

conversion_rates = df.groupby('DeviceType')['Converted'].mean() * 100

print(f""Desktop Conversion Rate: {conversion_rates['Desk...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the conversion rates for both Desktop and Mobile users using a very concise and efficient `groupby()` operation, and prints the results clearly labeled. It demonstrates a strong understanding of pandas for this type of aggregation. The code is clean, readable, and directly addresses all parts of the prompt. No improvements are needed.",deepseek_r1,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

averages = df.groupby('SensorID')['AirQualityIndex'].mean()
for sensor_id, avg ...",10,"The generated code is excellent. It correctly defines the DataFrame as specified, accurately calculates the average ""AirQualityIndex"" for each unique ""SensorID"", and prints the results clearly. The use of `groupby()` followed by `mean()` is the idiomatic pandas way to solve this problem. The output formatting using an f-string and `:.2f` for two decimal places istambién clear and user-friendly. All requirements of the task are met perfectly.",deepseek_r1,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

# Create EngagementScore
df['EngagementScore'] = df['Likes'] + (df['Shares'] * 2) + (d...",9,"The generated code correctly defines the DataFrame, creates the ""EngagementScore"" feature, filters for ""Instagram"" posts, and calculates the average ""EngagementScore"" for them. All requirements of the task are met. The code is concise and readable.

One minor point for improvement, which doesn't affect correctness but could be considered for robustness: the ground truth includes a check `if not instagram_posts_df.empty:` before calculating the mean. While not strictly necessary for this specific dataset (as Instagram posts exist), it's good practice for generalizable code to prevent errors if the filtered DataFrame could potentially be empty. However, given the prompt's context of having specific data, the generated code is perfectly acceptable and achieves the desired outcome.",deepseek_r1,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",9,"The generated code correctly defines the DataFrame, calculates the ""LengthOfStay"" column, filters for ""Flu"" patients, and accurately calculates and prints their average ""LengthOfStay"". The code is concise and directly addresses all requirements of the task.

**Strong points:**
*   **Correctness:** All calculations and filtering operations are accurate.
*   **Completeness:** All specified steps (DataFrame creation, LengthOfStay calculation, filtering, average calculation, and printing) are covered.
*   **Conciseness:** The code is efficient and to the point.

**Areas for minor improvement:**
*   **Readability/Clarity of Output:** The ground truth code includes print statements for intermediate DataFrames and a more descriptive f-string for the final output, which enhances clarity when running the script. The generated code simply prints the raw float value of the mean. While technically correct as per ""print their average 'LengthOfStay'"", a more descriptive output would be better for a user.
*   **Robustness (`.copy()`):** While not strictly necessary here as `flu_patients` is a new DataFrame resulting from a filter, in more complex scenarios where one might be modifying a slice of a DataFrame, using `.copy()` after filtering (`df[df['Diagnosis'] == 'Flu'].copy()`) is good practice to avoid `SettingWithCopyWarning` and ensure modifications are made on a distinct copy rather than a view of the original DataFrame. This is a minor point for this specific problem as no further modifications are made to `flu_patients`.

Overall, a very good solution that successfully accomplishes the task.",deepseek_r1,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}
df = pd.DataFrame(data)

sci_fi_movies = df[df['Genre...",9,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates their average rating, addressing all requirements of the task. The code is concise and directly implements the requested operations. The primary reason it's not a 10 is the lack of explicit handling for the edge case where no Sci-Fi movies might be present in the DataFrame (though not relevant for this specific dataset). The ground truth includes a check for an empty DataFrame, which enhances robustness. However, for the given problem constraints and data, the generated code performs perfectly.",deepseek_r1,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

toyota_cars = df[df['Brand'] == 'Toyota']
print(toyota_cars['Mileage'...",10,"The generated code correctly defines the DataFrame, filters for Toyota cars, and accurately calculates their average mileage. It is concise and directly addresses all requirements of the task. The output format for the average mileage also matches the ground truth's numerical output (though the ground truth provides formatting that isn't strictly requested). This is a perfect solution for the given prompt.",deepseek_r1,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 80...",9,"The generated code correctly defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat. It is concise and directly addresses all parts of the prompt. The only minor point for improvement, which keeps it from a perfect 10, is the lack of a descriptive print statement for the output. The ground truth code includes a formatted print statement with a label, which is better for readability and user understanding. However, the core logic is perfectly sound.",deepseek_r1,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

titanic = pd.read_csv('titanic.csv')
titanic['AgeGroup'] = pd.cut(titanic['Age'], bins=[0, 18, 30, 50, 80], labels=['0-18', '19-30', '31-50', '51-80'])

plt.figure(figsize=(18, 5))
plt.subplot(1, 3, 1)
sns.barplot(x='Pclass', y='Survived', data=titanic)
plt.title('Survival Rate by Class')
plt.subplot(1, 3, 2)
sns.barplot(x='Sex', y='Survived', data=titanic)
plt.title('Survival Rate by...",6,"The generated code addresses most of the task requirements but lacks in robustness, data preprocessing, and the sophistication of the dashboard design compared to the ground truth.

**Strengths:**
*   **Correctness:** It correctly generates all the requested plot types (multi-panel bar charts, heatmap, violin plot, interactive scatter plot).
*   **Imports:** All necessary libraries are imported.
*   **Basic Functionality:** The core visualization components are present and generally work as expected.
*   **Interactive Plot:** The Plotly scatter plot is correctly implemented and interactive.

**Areas for Improvement:**

1.  **Data Preprocessing and Robustness:**
    *   **Missing Values:** The generated code does not handle missing 'Age' or 'Fare' values, which is crucial for real-world datasets and can lead to errors or inaccurate visualizations. The ground truth fills NaNs with median values.
    *   **Categorical Conversion:** `Pclass` is used directly as numeric, but converting it to categorical with descriptive labels (e.g., '1st Class', '2nd Class') like in the ground truth significantly improves readability and professionalism of the plots.
    *   **AgeGroup Definition:** The `AgeGroup` bins are very simplistic (`[0, 18, 30, 50, 80]`). While it works, a more nuanced age grouping (like 'Child', 'Teenager', 'Young Adult', etc.) often provides better insights and is used in the ground truth.

2.  **Visualization Sophistication and Details:**
    *   **Dashboard Design:** The dashboard (Requirement 5) is implemented using `subplot2grid`, but it combines existing plots rather than creating new, appropriate ones for a dashboard context.
        *   The placement of ""Key Insights"" as text is incorrect and overlaps the `suptitle` and actual plots. It's meant to be in a dedicated section (like an `ax`) for better presentation.
        *   The ground truth uses additional plot types (KDE plot for age/fare distribution) and a dedicated text box for insights, which creates a more comprehensive and well-designed dashboard.
    *   **Split Violin Plot:** While `split=True` is used for the violin plot, the ground truth uses `hue='Survived_Cat'` after converting `Survived` to 'Yes'/'No' for more readable legend labels.
    *   **Plotly Customization:** The Plotly plot could benefit from `hover_data` displaying more relevant information (like passenger name, sex), as done in the ground truth.
    *   **Consistency:** The use of `Pclass` directly as `1, 2, 3` in plots instead of descriptive labels affects readability.
    *   **Annotation/Storytelling:** The annotations and insights are very basic and poorly placed on the dashboard. The ground truth provides a structured text box with richer insights.
    *   **Plot Aesthetics:** Setting `set_style`, `figure.dpi`, `figure.figsize` for better plot aesthetics and resolution is missing in the generated code.

3.  **Code Structure and Readability:**
    *   The generated code jumps directly into plotting without clear sections for data loading, preprocessing, and then each visualization task. The ground truth separates these logically, improving readability and maintainability.
    *   Repetitive calculations (e.g., `heatmap_data` calculation for the dashboard) could be optimized.

In summary, the generated code provides a functional solution to the visualization task but lacks the robust preprocessing, advanced visualization design, and comprehensive storytelling elements present in the expertly crafted ground truth.",deepseek_r1,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, ...",9,"The generated code is highly correct and directly addresses the core requirements of the task. It successfully defines the DataFrame, filters for Dystopian genre books, and accurately calculates the average stock for that genre. The code is concise and efficient.

The only minor area for improvement, preventing a perfect 10, is the lack of robust error handling for an empty filtered DataFrame, as seen in the ground truth's `if not dystopian_books_df.empty:` check. While in this specific problem scenario, it's guaranteed that Dystopian books exist, in a more general production context, an empty `dystopian_books` DataFrame would result in `NaN` for `average_stock`, which might not always be desired without explicit handling or a more informative message. Additionally, the ground truth includes more verbose printing for intermediate steps and formatting, which is good practice for clarity but not strictly required by the prompt's output specification. However, for the given task's scope, the generated code is excellent.",deepseek_r1,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind, shapiro, chisquare
import numpy as np

# Load and clean data
df = pd.read_csv('water_potability.csv')
df.dropna(inplace=True)

# Split into potable and non-potable groups
potable = df[df['Potability'] == 1]
non_potable = df[df['Potability'] == 0]
features = df.columns.drop('Potability')
alpha = 0.05

# T-tests and Cohen's d for significant results
print(""T-TEST RESULTS:"")
for feature in features:
    group1 = potable[feature]
    g...",6,"The generated code attempts to address all requirements of the task, but several critical aspects are not handled correctly or robustly, leading to potential misinterpretations of statistical results.

**Strengths:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Basic Structure:** The code is logically structured by test type.
*   **Attempts all tasks:** It tries to perform t-tests, normality tests, chi-square, and Cohen's d.
*   **Basic Interpretation:** Provides a simple interpretation for each test based on p-value.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling (Critical Flaw):** The generated code uses `df.dropna(inplace=True)`. This is a severe issue for statistical analysis, as it removes potentially significant chunks of data (rows with *any* missing value). The Ground Truth uses mean imputation, which is a much more appropriate **simple** strategy for this type of problem where missing values are likely MCAR/MAR and you don't want to lose data for specific tests. The task explicitly mentioned ""handle missing values"". `dropna` without further consideration is a poor approach here. This also impacts the statistical power and representativeness of the remaining data.

2.  **T-test `equal_var` Parameter:** The generated code uses `ttest_ind(group1, group2)`, which by default assumes equal variances (`equal_var=True`). However, the Ground Truth correctly uses `equal_var=False` (Welch's t-test), which is more robust and generally recommended when population variances are unknown or unequal, a common scenario in real-world data science. Not considering this affects the validity of the t-test results.

3.  **Cohen's d Calculation (Potential Error):** The generated Cohen's d calculation uses the standard pooled standard deviation formula assuming equal variances. If `equal_var=False` is used for the t-test (as it should be), then Cohen's d should ideally be calculated using a different pooled standard deviation formula (e.g., using only the standard deviations, or using a method that doesn't assume equal variances). While the provided formula *can* be used, it's inconsistent with a Welch's t-test result, if the variances indeed differ. The Ground Truth provides a more robust calculation for Cohen's d that considers different sample sizes and standard deviations, better reflecting a more general applicability. Also, the interpretation of Cohen's d only for significant results is good, but the Ground Truth includes standard interpretations for the magnitude of the effect size.

4.  **Normality Test Interpretation:** The Shapiro-Wilk test is performed on potable and non-potable groups separately. While this is valid, the task asked for normality tests ""on key numerical features."" The Ground Truth performs it on the entire feature, which is also a common approach *before* splitting for t-tests, to understand the overall distribution. Also, the Shapiro-Wilk test has limitations for very large datasets (e.g., N > 5000), which the Ground Truth correctly mentions and suggests alternatives. The generated code doesn't account for this.

5.  **Output Clarity and Verbosity:** The output for the generated code is concise but lacks some of the detailed interpretation and context provided by the Ground Truth (e.g., explicitly stating significance levels, reasons for rejecting/failing to reject null hypothesis, details like mean values in t-tests, and the precise interpretation of p-values).

6.  **Error Handling:** The Ground Truth includes basic error handling for file not found, which is good practice. The generated code lacks this.

7.  **Redundant Imports:** While not a critical flaw, `from scipy.stats import ttest_ind, shapiro, chisquare` is slightly less clean than `from scipy import stats` if you're using multiple functions from the same submodule, as it avoids listing many functions.

Overall, the biggest issue is the `dropna()` which fundamentally alters the dataset for analysis. If the dataset has many missing values, this would invalidate the entire analysis. The lack of consideration for `equal_var` in the t-test is another significant statistical oversight. These two points significantly reduce the score.",deepseek_r1,LLM_judge_config
M8,medium,Classification,,,Error,Failed to generate code,deepseek_r1,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier

# Load and preprocess data
df = pd.read_csv('water_potability.csv')
df.dropna(subset=['Potability'], inplace=True)
features = df.columns.drop('Potability')
df[features] = df[features].fillna(df[features].mean())

# Feature en...",6,"The generated code addresses most of the task requirements but has several issues that reduce its quality and correctness.

**Correctness Issues:**
*   **Missing Value Handling:** The prompt specified handling missing values by imputing with the mean. The generated code uses `df.dropna(subset=['Potability'], inplace=True)`, which drops rows where 'Potability' is NaN (though 'Potability' is generally a binary target, so this might not be an issue for that specific column). Crucially, for feature columns, it uses `df[features] = df[features].fillna(df[features].mean())`. While this fills NaNs, it does so *after* creating new features and uses `df[features]` which might include potentially problematic columns for mean imputation without specific checks. The ground truth explicitly handles `ph`, `Sulfate`, and `Trihalomethanes`, which are known to have NaNs.
*   **Division by Zero in Ratios:** The ground truth code includes `epsilon = 1e-6` when calculating ratios to prevent division by zero. The generated code does not, which can lead to `inf` or `NaN` values if a denominator is zero. This will then affect downstream calculations like correlation and feature selection.
*   **Polynomial Feature Implementation:** The generated code's implementation of polynomial features, specifically `df_poly = df_poly.drop(original_features, axis=1); df = pd.concat([df, df_poly], axis=1)`, can lead to issues. If an `original_feature` was already present in `df_poly` (which `PolynomialFeatures` does by default unless `include_bias=False` and `degree=1` features are filtered), dropping them from `df_poly` is correct, but then concatenating might still lead to duplicate columns if not handled carefully, or issues with index alignment if the original `df` was modified. The `index` is passed, but the ground truth's method of iterating and adding only new columns is more robust.
*   **Feature Selection `k` Value:** The generated code sets `k=5` for `SelectKBest` and `n_features_to_select=5` for `RFE`. While not strictly ""incorrect"" as the prompt didn't specify the exact `k`, selecting only 5 is very restrictive given the task's context (e.g., getting top 5 correlated features, but for selection, usually more are sought). The ground truth uses 15, which provides a more reasonable set for comparison.
*   **RFE `step` Parameter:** The generated code uses `step=0.1` for RFE. While this is a valid parameter, it means dropping 10% of features at each iteration. Given a relatively small number of features, this might lead to faster convergence but potentially miss optimal stepping. The ground truth uses `step=1`, which removes one feature at a time, providing a more granular search, albeit slower.
*   **Feature Importance Plot:** While a feature importance plot is created, it's just `SelectKBest` scores and RandomForest importances. The prompt asked for ""feature importance plots"" (plural) and the ground truth derived it from a trained RandomForest on *all* features, which is a better interpretation of the task than just plotting the scores from `SelectKBest`.
*   **Pairplot:** The `sns.pairplot(df, vars=top_5_corr, hue='Potability')` call uses `df` directly, which likely contains many more columns than needed for the pairplot, potentially leading to slow execution or errors if `top_5_corr` contains features not in `df` due to prior issues (e.g., division by zero leading to NaNs/infs). The ground truth creates a specific DataFrame subset.

**Code Quality and Completeness Issues:**
*   **Missing Scale Standardization:** A significant omission is the lack of feature scaling (e.g., `StandardScaler`) before applying `SelectKBest` and `RFE`. This is crucial for methods sensitive to feature scales, like `SelectKBest` (with `f_classif`, which is ANOVA-like and sensitive to variance) and RFE with a tree-based estimator if intermediate calculations benefit from it (though RandomForest itself is generally scale-invariant, the RFE process's stability can be affected). The ground truth correctly applies `StandardScaler`.
*   **Redundant `df.dropna(subset=['Potability'], inplace=True)`:** The problem description implies that `Potability` is the target, which is typically well-defined without NaNs in classification tasks. Dropping based on the target column here seems redundant or potentially problematic if there were indeed NaNs in the target.
*   **Visualization Details:** The visualization code is functional but lacks the polish of the ground truth. Things like `annot=False`, `fmt="".2f""`, `linewidths=.5` for heatmap, specific figure sizes, rotation of labels, and clear titles for plots (e.g., the pairplot's title positioning) are missing, making the plots less informative or harder to read.
*   **Comparison of Feature Sets:** The comparison is basic (`print` statements and a single `overlap` check). The ground truth provides a much more detailed breakdown of overlaps and unique features between *all* selected sets, which fulfills the ""analyze their overlap"" requirement more comprehensively.
*   **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError`. The generated code does not.",deepseek_r1,LLM_judge_config
