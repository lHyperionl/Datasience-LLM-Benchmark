task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd

# Load dataset
df = pd.read_csv('winequality-red.csv')

# Display column names and data types
print(""Column Names and Data Types:"")
print(df.dtypes)

# Calculate and report descriptive statistics for numerical columns
print(""\nDescriptive Statistics:"")
print(df.describe())

# Number of rows and columns
print(""\nNumber of Rows and Columns:"")
print(f""Rows: {df.shape[0]}\nColumns: {df.shape[1]}"")

# Identify categorical or ordinal columns
categorical_cols = df.select_...",7,"The generated code correctly performs most of the required tasks using appropriate pandas functions. It successfully loads the dataset, identifies data types, calculates descriptive statistics for numerical columns implicitly through `df.describe()`, and reports dataset dimensions.

However, there are a few areas for improvement:
1.  **Categorical Column Identification:** While `df.select_dtypes(include=['object', 'category'])` is the correct method for explicitly defined categorical or object types, the `winequality-red.csv` dataset contains *only* numerical columns. The ground truth correctly points out that 'quality', though numerical, is often treated as ordinal. The generated code's current approach would report ""No categorical columns found"", which is technically correct but misses the nuance of the `quality` column, which was hinted at in the prompt's requirement to ""Identify any columns that appear to be categorical or ordinal."" A more robust solution might involve checking for low cardinality integer columns if *no* explicit categorical types are found, or at least adding a comment about the nature of the quality column.
2.  **Explicit Data Type Identification:** The prompt asked to ""identify the data type of each column (numerical, categorical, etc.)"" and then later to ""Identify any columns that appear to be categorical or ordinal."" The generated code uses `df.dtypes` which lists pandas dtypes, but it doesn't explicitly categorize them as ""Numerical,"" ""Categorical,"" or ""Ordinal"" as the ground truth does. While `df.describe()` covers numbers automatically, the explicit categorisation for *all* columns is a distinct requirement.
3.  **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError`, which is good practice for data loading scripts. The generated code lacks this.
4.  **Descriptive Statistics Specificity:** `df.describe()` provides the required statistics (mean, median/50%, std, min, max, quartiles). This is handled well, but the prompt specifically asked ""For all numerical columns"", and while `df.describe()` defaults to numerical columns unless specified, explicitly calling it on `df.select_dtypes(include=np.number)` could be slightly clearer if the dataframe contained mixed types, though for this specific dataset it's fine.

Overall, it's a good attempt that covers the main requirements, but could be more precise and robust.",mistral-nemo,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

# Load the dataset
data = pd.read_csv('video_games_sales.csv')

# Display the first 10 rows
print(data.head(10))

# Show the shape of the dataset
print(""Shape:"", data.shape)

# List all column names and their data types
print(""\nColumn names and data types:"")
print(data.dtypes)

# Display the top 10 games by Global_Sales
print(""\nTop 10 games by Global_Sales:"")
print(data.sort_values('Global_Sales', ascending=False).head(10))

# Show the unique values in the Platfo...",7,"The generated code correctly performs all the requested tasks. It loads the data, displays the head, shape, dtypes, top 10 games, unique platforms/genres, summary statistics for sales columns, and missing values.

However, there are several areas for improvement as compared to the ground truth:
1.  **Readability/Clarity**: The ground truth code uses `print` statements with clear headers (e.g., ""--- 1. Loading Dataset..."") which significantly improves the readability and understanding of each step's output. The generated code lacks these descriptive headers, making the output less structured.
2.  **Robustness (Error Handling)**: The generated code does not include any error handling for `FileNotFoundError` when attempting to load the CSV, which the ground truth gracefully handles. This is a crucial aspect for production-ready or robust scripts.
3.  **Specific Output Requests**: The ground truth explicitly states ""The dataset has {df.shape[0]} rows and {df.shape[1]} columns."" which helps clarify the shape output. The generated code just prints `data.shape`.
4.  **Displaying Top 10 Games (`Name`, `Platform`, etc.)**: The ground truth specifically selects relevant columns (`[['Name', 'Platform', 'Genre', 'Publisher', 'Global_Sales']]`) for displaying the top 10 games, making the output more focused and readable. The generated code just prints the entire row for the top 10, which can be verbose.
5.  **Handling Missing or Non-existent Columns**: The ground truth includes checks (`if 'Global_Sales' in df.columns:`, `if 'Platform' in df.columns:`, etc.) to ensure columns exist before trying to access them. It also handles the case where expected sales columns might not all be present. This makes the code more robust against variations in schema. The generated code assumes all columns will exist.
6.  **Missing Value Clarity**: The ground truth filters `missing_values[missing_values > 0]` to only show columns with actual missing values, which is often more useful than showing all columns if most have 0 missing values. It also prints ""Total missing values per column shown above."" for clarity.

Overall, the generated code is functional and produces the expected results, but it lacks the robustness, clarity, and attention to detail that make the ground truth code more professional and user-friendly for EDA.",mistral-nemo,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values in the Year column by filling them with the median year
df['Year'].fillna(df['Year'].median(), inplace=True)

# Remove games with missing Publisher information
df.dropna(subset=['Publisher'], inplace=True)

# Create a new column 'Years_Since_Release'
df['Years_Since_Release'] = 2023 - df['Year']

# Remove duplicate games
df.drop_duplicates(subset=['Name', 'Platform'], inp...",7,"The generated code largely achieves the task requirements. However, there are a few areas for improvement that prevent it from getting a higher score.

**Strong Points:**
- Correctly loads the dataset.
- Handles missing 'Year' values with the median.
- Correctly removes missing 'Publisher' values.
- Creates 'Years_Since_Release' correctly.
- Correctly removes duplicates based on 'Name' and 'Platform'.
- Correctly filters out low-selling games.
- Displays the final shape and head of the DataFrame.

**Areas for Improvement:**
- **Robustness for 'Year' column:** The ground truth includes `pd.to_numeric(df['Year'], errors='coerce')` and `astype(int)` which makes the code more robust if the 'Year' column contains non-numeric strings or floats after median imputation. The generated code assumes 'Year' is already numeric or can be directly processed by `median()`.
- **Robustness for 'Global_Sales' column:** Similar to 'Year', `df['Global_Sales'] = pd.to_numeric(df['Global_Sales'], errors='coerce')` and subsequent `dropna` ensure that sales data is numerical before filtering. The generated code assumes 'Global_Sales' is already numeric.
- **Negative 'Years_Since_Release':** The ground truth explicitly handles cases where `CURRENT_YEAR - df['Year']` might result in a negative number (e.g., if a future year was somehow entered as a release year) by setting negative values to 0. The generated code does not account for this.
- **Explicit current year definition:** While implied by `2023`, using a `CURRENT_YEAR` constant like in the ground truth makes the code more readable and easier to maintain.
- **Informative print statements:** The ground truth includes print statements at various stages, showing the shape changes, which is very helpful for debugging and understanding the data transformation process. The generated code only provides the final shape and head, which is sufficient for the prompt but less informative for real-world development.
- **Error handling:** The lack of a `try-except` block for file loading or other potential errors makes the generated code less robust.",mistral-nemo,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Create a bar chart showing the top 10 gaming platforms by total global sales
platform_sales = df.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False)[:10]
platform_sales.plot(kind='bar', title='Top 10 Gaming Platforms by Global Sales')
plt.xlabel('Platform')
plt.ylabel('Total Global Sales')
plt.show()

# Generate a pie chart displaying the distribution ...",6,"The generated code attempts to address all requirements of the task, creating various visualizations. However, there are significant issues that prevent it from being fully correct and robust.

**Correctness Issues:**
1.  **Data Loading Error Handling:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is crucial for robust code.
2.  **Data Cleaning/Type Conversion:** The generated code assumes `Year_of_Release` column exists and is directly convertible to a numeric type for the line plot. The ground truth demonstrates robust handling of `Year_of_Release` (coercing errors, dropping NaNs, and casting to int) and explicitly ensuring sales columns are numeric. The generated code's last plot attempts to create a 'Year' column from a non-existent ' Release Date' column, indicating a misinterpretation or lack of data schema understanding. The ground truth correctly uses `Year_of_Release`.
3.  **Plot Customization/Readability:** While plots are generated, they lack the detailed customization present in the ground truth (e.g., `figsize`, `alpha`, `s` for scatter, explicit `plt.show()` after each plot, `plt.grid`, `plt.tight_layout`, specific colors). The pie chart's `plt.show()` is missing after the `plot` command. The histogram lacks a `range` parameter, which is useful for focusing on the bulk of the data, as shown in the ground truth.
4.  **Pie Chart Readability:** The generated pie chart directly plots all genres, which can be unreadable if there are many small categories. The ground truth includes logic to group small categories into an ""Other"" slice for better readability.
5.  **Line Plot X-axis:** The line plot for yearly releases does not filter or handle potential outliers or malformed years effectively, relying solely on the column name directly from `pd.DatetimeIndex`, which is incorrect for `Year_of_Release` in the dataset.

**Completeness & Code Quality Issues:**
1.  **Imports:** It's missing `numpy` from imports, which might not be strictly necessary for this specific solution but is often a common import in data science scripts and is in the ground truth.
2.  **`plt.figure` and `plt.tight_layout`:** The generated code doesn't use `plt.figure(figsize=...)` for explicit figure sizing or `plt.tight_layout()` for better spacing, which are important for professional-looking plots and preventing overlap.
3.  **Redundant `plt.show()`:** The generated code implicitly relies on `plot` methods of pandas dataframes/series to call `plt.show()` automatically (or for the last plot in a cell in an interactive environment), which isn't best practice for explicitly controlled plot display. Specifically, the pie chart will likely not show up immediately as `plt.show()` is missing after its plot command.
4.  **Clarity of Labels:** While titles and basic labels are present, the ground truth provides more descriptive labels (e.g., ""Total Global Sales (Millions)"", ""North American Sales (Millions)""), which adds to clarity.

Overall, the generated code provides a basic attempt at the visualizations but lacks the robustness, thoroughness in data handling, and detailed plot customization that an ""expert"" solution would entail. The major flaw is the incorrect handling of the 'Year_of_Release' for the line plot.",mistral-nemo,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset and display the first 10 rows
df = pd.read_csv('diabetes.csv')
print(""First 10 rows:"")
print(df.head(10))

# Show the shape of the dataset
print(""\nData shape:"")
print(df.shape)

# List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# Calculate percentage of diabetic vs non-diabetic patients (Outcome column)
print(""\nPercentage of diabetic and non-diabetic patients:"")
print(df['O...",6,"The generated code addresses most of the requirements but has several critical areas for improvement:

**Strengths:**
*   Correctly loads the CSV and displays the first 10 rows.
*   Correctly shows the shape of the dataset.
*   Correctly calculates and displays the percentage of diabetic vs non-diabetic patients.
*   Correctly displays the count of missing values.
*   Correctly displays summary statistics for numerical columns using `select_dtypes`.

**Areas for Improvement / Missed Requirements:**
1.  **Column Names and Data Types (Requirement 3):** The code uses `df.dtypes`, which technically lists data types but doesn't provide the more comprehensive info (non-null counts) that `df.info()` would, as requested in the ground truth's hint (""df.info() provides column names, non-null counts, and dtypes"").
2.  **Identification of Zero Values as Missing (Requirement 7):** This is the most significant deficit. The generated code `print(df.columns[df.sum() == 0])` is fundamentally incorrect for this purpose.
    *   `df.sum() == 0` checks if the *sum of all values* in a column is zero, not if individual values within the column are zero. For example, a column with `[1, 0, -1]` would sum to zero but contains non-zero entries.
    *   It doesn't specifically target columns where zero *might* represent missing data (e.g., 'Glucose', 'BloodPressure', 'BMI'). 'Pregnancies' can legitimately be zero, and 'Outcome' is a binary class. The ground truth correctly identifies specific columns to check.
    *   It doesn't provide counts or percentages of these zero values, only the column names.
3.  **Error Handling:** The generated code lacks basic error handling for `FileNotFoundError` if `diabetes.csv` is missing, unlike the ground truth.
4.  **Clarity and Formatting:** The output of the generated code is less descriptive and harder to read than the ground truth. The ground truth uses headers (`--- 1. First 10 rows... ---`) and separators (`===`) to clearly delineate output sections, which improves readability, a good practice for exploratory analysis reports.
5.  **Robustness for Target Column:** The ground truth checks if `TARGET_COLUMN` exists before attempting calculations, which is more robust.

To achieve a higher score, the generated code needs to implement a correct and targeted approach for identifying zero values that might represent missing data, provide specific counts for these, and improve the overall output presentation and robustness.",mistral-nemo,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler

# 1) Load the dataset
df = pd.read_csv('diabetes.csv')

# 2) Identify and replace zero values with NaN in specific columns
columns_to_replace = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_to_replace] = df[columns_to_replace].replace(0, pd.np.nan)

# 4) Handle missing values by filling them with the median of each respective column
df[columns_to_replace] = df[columns_to_replace].fillna(d...",7,"The generated code addresses most of the requirements but has a few issues that prevent it from being a perfect solution.

**Strong Points:**
*   **Correctness in core steps:** It correctly loads the dataset, identifies the specific columns for zero replacement, replaces zeros with NaN, fills NaNs with the median, removes duplicates, creates the `BMI_Category`, and scales numerical features.
*   **Concise handling of missing values:** The `df[columns_to_replace] = df[columns_to_replace].fillna(df[columns_to_replace].median())` line is a very efficient way to fill missing values for multiple columns, compared to the loop in the ground truth.
*   **Appropriate use of `pd.cut`:** The `BMI_Category` creation is done correctly using `pd.cut`.

**Areas for Improvement:**
*   **Missing `numpy` import:** The generated code uses `np.inf` but does not import `numpy`, which would lead to a `NameError`. It also uses `pd.np.nan` which is deprecated; `np.nan` is the standard.
*   **Error Handling for File Not Found:** The generated code lacks robust error handling for `FileNotFoundError` when loading the CSV, which the ground truth gracefully handles with a `try-except` block.
*   **Verbosity/Display:** The generated code displays the shape and head as a tuple at the very end `df_shape, df_head`, which isn't a standard or clean way to display output in a script; it should use `print()` statements for better readability during execution. The ground truth provides more informative print statements throughout the process.
*   **Robustness in Column Selection for Scaling:** The generated code hardcodes `num_features` for scaling. While correct for this specific dataset, the ground truth dynamically selects numerical columns and explicitly excludes 'Outcome', making it more robust to variations in the dataset structure. This becomes important if the target variable changes or other non-feature numerical columns are present.
*   **Missing documentation/comments:** While the prompt didn't explicitly ask for extensive comments, the ground truth's detailed comments for each step enhance readability and understanding, which is good practice. The generated code has minimal comments.",mistral-nemo,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('diabetes.csv')

# 1. Correlation Heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, fmt="".2f"")
plt.title('Correlation Heatmap')
plt.show()

# 2. Histograms for all numerical features
df.hist(figsize=(10, 10))
plt.suptitle('Histograms of Numerical Features', y=1.05)
plt.show()

# 3. Box plots comparing each feature between diabetic and non-diabetic patients...",7,"The generated code creates most of the requested visualizations and demonstrates a good understanding of matplotlib and seaborn. However, there are several areas where it falls short compared to the ground truth in terms of completeness, robustness, and presentation.

**Strengths:**
*   Successfully generates correlation heatmap, histograms, box plots, pie chart, scatter plots, and a pair plot.
*   Uses `hue='Outcome'` correctly for scatter plots and pair plot to color by diabetes outcome.
*   Properly imports necessary libraries.

**Areas for Improvement:**
1.  **Correlation Heatmap:** The generated code computes `df.corr()` which includes `Outcome` in the correlation matrix, whereas the ground truth explicitly excludes `Outcome` to correlate only numerical features. While not strictly ""wrong,"" the ground truth's approach is often preferred for visualizing relationships *between features*. Also, the ground truth adds more formatting like `linewidths` and better axis rotation.
2.  **Histograms:** The generated code uses `df.hist()`, which is a quick way but doesn't offer the same level of customization as using `sns.histplot` in a loop, as done in the ground truth. Specifically, the ground truth adds `kde=True` for density estimation, customizes titles and labels for individual subplots, and arranges them in a more controlled grid. The generated code also lacks individual subplot titles.
3.  **Box Plots:** The generated code creates a new figure for *each* box plot, leading to many individual plot windows. The ground truth combines them into a single figure with subplots, which is more efficient for presentation and comparison. Also, the title and label formatting is less descriptive in the generated code.
4.  **Scatter Plots:** While the plots themselves are correct, the generated code combines them into one figure but doesn't set individual titles for each subplot, making them less clear at a glance. It also doesn't explicitly handle the `Outcome` label mapping in the legend as the ground truth does (e.g., 'Non-Diabetic' vs 'Diabetic'). The ground truth also explicitly sets `s` (size) and `alpha` for better visualization.
5.  **Pair Plot:** The generated code uses the default `diag_kind` for the diagonal plots, which is a histogram. The ground truth specifies `diag_kind='kde'` for a kernel density estimate, which is often more informative for continuous distributions on diagonal plots.
6.  **Load Dataset Robustness:** The ground truth includes `try-except` block for `FileNotFoundError`, which is good practice for real-world scenarios.
7.  **General Formatting and Readability:** The ground truth includes more comprehensive titles, labels, font sizes, and uses `plt.tight_layout()` more consistently, which improves the overall aesthetic and readability of the plots. It also defines `numerical_cols` once and reuses it, which is more robust if the dataset structure changes.
8.  **Information Output:** The ground truth includes `df.head()`, `df.info()`, and `df.describe()` calls upon loading the dataset, which is useful for initial data exploration but not strictly a visualization requirement.",mistral-nemo,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"import pandas as pd

# 1) Load the dataset and display the first 10 rows
df = pd.read_csv('world-happiness-report-2021.csv')
print(df.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(""\nShape of the dataset:"")
print(df.shape)

# 3) List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# 4) Display the unique regional indicators and count of countries per region
print(""\nUnique regional indicators and count of countries per regi...",7,"The generated code correctly addresses most of the tasks and produces the expected output. It demonstrates correct usage of pandas for loading, inspecting, and summarizing data.

However, there are several areas for improvement to reach a higher score:

1.  **Robustness (Missing file handling):** The ground truth includes error handling for `FileNotFoundError` (Task 1). The generated code assumes the file exists, which is a common point of failure in real-world scenarios.
2.  **Clarity of Output:** The ground truth adds separators (`print(""\n"" + ""=""*70 + ""\n"")`) and clear headings (`print(""1. First 10 rows of the dataset:"")`) for each task, making the output much more readable and organized. The generated code's output, while correct, is less clearly delineated between tasks.
3.  **Data Type Display (Task 3):** The generated code uses `df.dtypes` which is acceptable, but `df.info()` used in the ground truth provides a more comprehensive overview including non-null counts, which is generally preferred for initial EDA.
4.  **Specific Column Names (Task 6):** The generated code assumes 'GDP per capita' exists. The correct column name in the dataset is 'Logged GDP per capita'. The ground truth handles this more robustly by filtering for existing columns. This would cause a `KeyError` if 'GDP per capita' was not present.
5.  **Output for Top 10 Happiest Countries (Task 5):** The generated code outputs all columns for the top 10 countries (`df.nlargest(10, 'Ladder score')`). The ground truth specifically selects `'Country name', 'Ladder score'`, which is often more relevant for this particular display.
6.  **Readability/Comments:** While the generated code has comments for each task, the overall structure and print statements could be improved for better readability (as seen in the ground truth).
7.  **Defensive Programming (Column Existence):** The ground truth checks for the existence of columns (e.g., 'Regional indicator', 'Ladder score', 'Country name', 'Logged GDP per capita') before trying to access them. The generated code directly accesses them, which could lead to `KeyError` if the column names were slightly different or missing.",mistral-nemo,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from scipy import stats

# 1) Load the dataset
data = pd.read_csv('world-happiness-report-2021.csv')

# 2) Handle missing values
num_cols = data.select_dtypes(include=np.number).columns
cat_cols = data.select_dtypes(include=['object', 'category']).columns

data[num_cols] = data[num_cols].fillna(data[num_cols].mean())
data[cat_cols] = data[cat_cols].fillna(data[cat_cols].mode().iloc[0])...",7,"The generated code is largely correct and addresses most requirements, demonstrating a good understanding of data preprocessing tasks.

**Strengths:**
*   Correctly loads the dataset.
*   Handles missing values for both numerical and categorical columns.
*   Correctly implements `Happiness_Category` using `pd.cut`, which is a clean way to define bins.
*   Correctly implements `Well_being_Score` by averaging the specified columns.
*   Successfully removes outliers for 'Ladder score' using the IQR method.
*   Correctly applies `StandardScaler` to numerical features.
*   Correctly applies `LabelEncoder` to 'Regional indicator'.
*   Displays the shape and head of the cleaned data.

**Areas for Improvement (Why not a higher score):**

1.  **`GDP_Category` Implementation:** The requirement was 'based on quartiles', and `pd.qcut` is the correct function for this. However, the generated code uses `labels=False`, which returns integer codes for the bins (0, 1, 2, 3), not the common quartile labels (Q1, Q2, Q3, Q4) or a more descriptive representation. While technically quartile-based, it's not as user-friendly or informative as ""Q1"" labels. The ground truth explicitly uses `labels=['Q1', 'Q2', 'Q3', 'Q4']`.
2.  **Handling Missing Categorical Values (Mode):** The generated code uses `data[cat_cols].fillna(data[cat_cols].mode().iloc[0])`. This approach takes the *first mode of the entire DataFrame's categorical columns*, not the mode for each individual column. If different categorical columns have different modes, this would incorrectly fill them. The ground truth iterates through each categorical column and fills with that column's specific mode, which is the correct and robust way.
3.  **Numerical Features for Standard Scaling:** The generated code re-identifies `num_cols` *after* creating `GDP_Category` and `Well_being_Score`. While `Well_being_Score` *should* be scaled, `GDP_Category` (even if it's integer labels 0-3) *should not* be scaled by `StandardScaler` as it represents a categorical grouping, not a continuous numerical feature. Scaling it can imply a relationship that doesn't exist, and it could lead to issues in downstream modeling if treated as continuous after scaling. The generated code incorrectly scales `GDP_Category`.
4.  **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code lacks this.
5.  **Small Detail: `Freedom` vs `Freedom to make life choices` for `Well_being_Score`:** The ground truth uses `Freedom to make life choices` which is the full column name specified in the dataset and problem description context. The generated code simplifies to `Freedom`. While it might work if pandas' fuzzy matching allows or if the column name is precisely 'Freedom', it's better to be explicit with the correct full column name for robustness. (Self-correction: Looking at the CSV, the column is indeed `Freedom to make life choices`. So the generated code has an issue here.)",mistral-nemo,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import rcParams

# Load the dataset
data = pd.read_csv('world-happiness-report-2021.csv')

# 2) World map visualization
fig = px.choropleth(data, locations='Country code', locationmode='ISO-3', color='Score', hover_name='Country name',
                    animation_frame='Year', range_color=(0, 10), projection=""natural earth"", title='Wo...",6,"The generated code attempts to address all requirements of the task. However, there are significant issues in its correctness, completeness, and adherence to the specified tools for certain visualizations.

**Strengths:**
*   Successfully loads the dataset.
*   Attempts to create all required visualization types (map, bar, heatmap, scatter, box, radar).
*   Uses a mix of Plotly, Seaborn, and Matplotlib as requested.

**Weaknesses and Areas for Improvement:**

1.  **Map Visualization (Task 2):**
    *   **Incorrect `locationmode` and `locations`:** The generated code uses `'Country code'` and `'ISO-3'`, neither of which are present in the `world-happiness-report-2021.csv` dataset. The ground truth correctly identifies `Country name` and `country names`. This causes the map to be empty or incorrect.
    *   **""Year"" column:** The `animation_frame='Year'` is incorrect as the 2021 dataset is typically a single year's data and doesn't have a 'Year' column for animation. This will likely cause an error or an empty animation.
    *   **'Score' column:** The generated code uses 'Score', but the dataset has 'Ladder score'. This would need renaming or correct column usage.

2.  **Bar Chart (Task 3):**
    *   **'Region' column:** Similar to the map, the dataset typically has 'Regional indicator', not 'Region'. This will cause a `KeyError`.

3.  **Correlation Heatmap (Task 4):**
    *   **Plotly `px.imshow` for heatmap:** While `px.imshow` can create ""heatmaps"" for raw 2D arrays, it's not the conventional way to make a correlation heatmap with annotations and clear labels directly from a DataFrame's `.corr()` output. Seaborn's `sns.heatmap` is much more suitable for this purpose, providing `annot`, `fmt`, `linewidths`, etc., which are missing from the Plotly version here. The current Plotly heatmap lacks annotations, which are crucial for interpreting correlations.

4.  **Scatter Plots (Task 5):**
    *   **Column names:** Similar to previous points, 'GDP per capita' should be 'Logged GDP per capita', and 'Ladder score' is correct but needs to be consistent if renamed.
    *   **`size='Region'`:** Using a categorical column like 'Region' for `size` can lead to confusing or uninformative plots (Plotly might try to map categories to distinct sizes, which isn't ideal here). It's better to use `color` or `symbol` for categorical differentiation.

5.  **Box Plots (Task 6):**
    *   **Column names:** 'Region' and 'Score' are incorrect. Should be 'Regional indicator' and 'Ladder score'.
    *   **Figure size:** The `sns.set(rc={'figure.figsize':(11.7, 8.27)})` is a valid way to set figure size, but it's a global setting for all subsequent seaborn plots. A more explicit `plt.figure(figsize=...)` for a single plot is often preferred for more granular control.

6.  **Radar Chart (Task 7):**
    *   **`data.head(5)`:** This assumes the dataset is already sorted by happiness score. The task explicitly asks for the ""top 5 happiest countries,"" which requires sorting by the 'Ladder score' column first.
    *   **`top_5_happiest.iloc[i, 2:].values`:** This directly uses slice indexing, which is highly fragile. If the column order changes, or if new columns are added/removed, this will break or produce incorrect results. It's much safer to explicitly select the factor columns as done in the ground truth.
    *   **`theta=data.columns[2:]`:** Same fragility issue for `theta`. It directly assumes column names from index 2 onwards are the correct features for the radar chart, which is a bad assumption.
    *   **Normalization:** A critical omission is the normalization of the radar chart features. Without normalization (scaling values to a common range, e.g., 0-1), a radar chart is misleading because factors with naturally larger numerical ranges (like GDP per capita) will dominate the visual representation, even if their relative importance or contribution to happiness is comparable to factors with smaller ranges (like generosity). The ground truth correctly implements `MinMaxScaler`.
    *   **Title placement:** `polar_radialaxis_angularun√§chst` is not a standard Plotly parameter and looks like a typo.

7.  **General Issues:**
    *   **Column Renaming:** The ground truth renames columns for easier access and clarity, which is good practice. The generated code uses original column names, which are less readable for plotting.
    *   **Error Handling:** No error handling for `FileNotFoundError`.
    *   **Imports:** `rcParams` from `matplotlib` is imported but not explicitly used in a significant way (though the `sns.set` call indirectly uses it). `go` is used for the radar chart, which is appropriate.

Overall, the generated code demonstrates an understanding of the types of visualizations requested but struggles with specific implementation details, especially regarding correct column usage, robust data selection, and crucial visualization best practices like data normalization for radar charts and proper annotation for heatmaps. The errors in column names for almost all plots render the code largely non-functional without manual correction.",mistral-nemo,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"import pandas as pd
import datetime

# 1) Load the dataset and parse the 'date' column as datetime objects.
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

# 2) Display the first 10 rows.
print(df.head(10))

# 3) Show the shape of the dataset (rows and columns).
print(f""Dataset shape: {df.shape}"")

# 4) List all column names and their data types.
print(""Column names and data types:"")
print(df.dtypes)

# 5) Calculate and display the total sum of 'total_case...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being production-ready or fully correct for the given task.

**Correctness Issues:**
*   **Loading the dataset (Task 1):** The generated code attempts to parse the 'date' column *after* loading the CSV, which is less efficient and prone to errors if the column isn't already a string type. The `parse_dates` argument in `pd.read_csv` is the idiomatic and robust way to handle this.
*   **Top 5 Locations (Task 6):** The logic for finding the top 5 locations is incorrect. `df[df['date'].isin(latest_date)]` will filter the dataframe to *only* include rows where the date is one of the maximum dates found for *any* location. This doesn't correctly select the *single latest entry for each location*. For example, if Location A's latest date is 2023-10-01 and Location B's latest date is 2023-11-01, the generated code *might* include earlier entries for Location A if the date matches *any* other location's `max()` date. The correct approach is to sort by location and date, then `drop_duplicates(subset=['location'], keep='last')` to get the latest record for each location before sorting by `total_cases`.
*   **Summary Statistics (Task 7):** While `describe()` provides summary statistics, it includes count, 25%, 50%, 75% percentiles by default, not specifically median. The prompt asked for `mean, median, min, max, std`. `describe()` gives these, but `agg()` with explicit functions is more precise if specific statistics are requested. Also, it prints `\\nSummary statistics` instead of `\nSummary statistics` due to an extra backslash.
*   **Error Handling:** The generated code lacks any robust error handling (e.g., `try-except` for `FileNotFoundError`) which is crucial for a real-world script.

**Code Quality and Completeness Issues:**
*   **Verbosity/Formatting:** Some print statements are less descriptive than the ground truth (e.g., just `df.head(10)` without a header). The output of `df.dtypes` is also less comprehensive than `df.info()`, which also provides non-null counts.
*   **Clarity in Top Locations:** The generated code for Task 6 is not immediately clear in its intent and is fundamentally flawed in its implementation.
*   **Redundant Import:** `import datetime` is not used directly.

**Strong Points:**
*   It correctly addresses the loading and initial parsing, though not optimally.
*   The `head()`, `shape`, `dtypes`, `groupby().sum()`, and `isnull().sum()` parts are correctly implemented for their specific purposes.
*   It covers all requested tasks, even if some implementations are flawed.",mistral-nemo,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('winequality-red.csv', sep=';')

# Calculate average 'quality'
average_quality = df['quality'].mean()
print(f'Average quality: {average_quality}')

# Group by 'quality' and calculate average 'alcohol'
grouped_average_alcohol = df.groupby('quality')['alcohol'].mean()
print(grouped_average_alcohol)

# Create bar chart
sns.barplot(x='quality', y='alcohol', data=grouped_average_alcoho...",8,"The generated code correctly addresses all three main requirements of the task: calculating the mean quality, grouping by quality to find average alcohol content, and creating a bar chart visualization.

Strengths:
- Correctly imports necessary libraries (pandas, seaborn, matplotlib.pyplot).
- Accurately calculates the average 'quality'.
- Correctly groups by 'quality' and calculates the mean 'alcohol' for each group.
- Successfully creates a bar chart using `seaborn.barplot` with appropriate axes and title.
- Uses `reset_index()` for plotting the grouped data, which is a good practice.

Areas for Improvement:
- **File Loading:** The generated code assumes a semicolon separator (`sep=';'`) which is not explicitly stated in the prompt, though it might be a common format for this specific dataset. The ground truth handles a potential `FileNotFoundError`, which is good practice for robust code.
- **Output Formatting:** The print statements for average quality and grouped alcohol content are functional but could be slightly more descriptive (e.g., adding ""1. Average 'quality' of red wines:"" for the first output and a more descriptive header for the grouped data).
- **Plot Enhancements:** While the plot is functional, the ground truth includes more advanced plotting features like:
    - `plt.figure(figsize=...)` for better sizing.
    - Setting `sns.set_style(""whitegrid"")`.
    - Adding `xticks` and `yticks` font sizes.
    - Adding data labels on top of the bars, which significantly enhances readability.
    - Using `plt.tight_layout()` for better layout.
    - Choosing a specific `palette` for the bar chart.
    These are not critical for functionality but improve the quality and professionalism of the visualization.
- **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError`, which makes the script more robust. This was not explicitly required by the prompt but is a good practice in production-ready code.

Overall, it's a very good attempt that fulfills the core requirements. The minor improvements are mostly related to robustness and visualization aesthetics rather than core functionality.",mistral-nemo,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# 1) Load the dataset and parse the 'date' column as datetime objects.
data = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2) Handle missing values in 'hosp_patients' and 'icu_patients' by filling them with 0. For other numerical columns, fill missing values with the mean of their respective column.
numerical_cols = ['positive_rate', 'reproduction_rate']
data['hosp_patients'] = data['hosp_patients'].fillna(0)
data['icu_patie...",8,"The generated code correctly addresses most of the task requirements, demonstrating a good understanding of data cleaning and preprocessing in pandas.

**Strong points:**
*   **Correct data loading and datetime parsing:** `parse_dates=['date']` is an efficient way to load and parse the date column directly.
*   **Correct handling of `hosp_patients` and `icu_patients` missing values:** Fills with 0 as specified.
*   **Correct handling of `positive_rate` and `reproduction_rate` missing values:** Fills with the mean, and the syntax `data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].mean())` is concise and correct.
*   **Correct `mortality_rate` calculation and division by zero handling:** Uses `replace` and `fillna(0)` which effectively handles `inf` and `NaN` cases.
*   **Correct `vaccination_rate_per_capita` calculation:** Uses `np.where` for robust division by zero handling where `population` is 0, which is good.
*   **Correct filtering for the year 2021.**
*   **Correct display of shape and head.**
*   **Proper imports.**

**Areas for improvement (prevented a higher score):**
*   **Robustness in filling NaN for `mortality_rate` and `vaccination_rate_per_capita`:** While `data['mortality_rate'] = data['mortality_rate'].replace([np.inf, -np.inf], np.nan).fillna(0)` works, it's slightly less direct than the ground truth's `replace` then `fillna`. The `np.where` for `vaccination_rate_per_capita` is good for *division by zero* but doesn't explicitly guarantee handling NaNs that might arise from `people_fully_vaccinated` or `population` being NaN initially. If the initial columns used for calculation contain NaNs, the result will be NaN, and the `np.where` condition `data['population'] == 0` won't catch this. The task specifically asked to handle NaNs or infs from division by zero, which `np.where` does well for the zero divisor, but a more comprehensive `fillna(0)` or `replace([np.inf, -np.inf], 0).fillna(0)` would ensure all cases of resulting NaNs are handled.
*   **Error handling for `FileNotFoundError`:** The ground truth includes a `try-except` block for file loading, which is good practice for robustness. The generated code does not. While not explicitly asked for, it's a good practice for real-world scenarios.

Overall, the generated code is highly functional and meets nearly all requirements effectively. The minor points are mostly about robustness and defensive programming, which distinguish an excellent solution from a very good one.",mistral-nemo,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset and parse the 'date' column.
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

# 2) Create a time series line plot of 'new_cases' for the location 'India' over the entire period.
India_data = df[df['location'] == 'India']
plt.figure(figsize=(12,6))
sns.lineplot(x='date', y='new_cases', data=India_data)
plt.title('New COVID-19 Cases in India Over Time')
...",6,"The generated code addresses most of the requirements but has several key issues that reduce its correctness and quality.

**Correctness Issues:**
*   **Item 3 (Bar Chart for Total Deaths by Continent):** The logic `df['total_deaths'] = df.groupby('continent')['total_deaths'].cumsum()` is fundamentally incorrect for this task. It calculates a cumulative sum for each continent *over time*, not the total deaths by continent. To get the sum of total deaths for each continent, one should first get the latest `total_deaths` for each *country* and then sum those up by continent. The current approach will produce vastly inflated and illogical numbers. The use of `df.drop_duplicates(subset='continent')` after the incorrect `cumsum` also doesn't fix the issue; it just picks one (arbitrary) cumulative sum value per continent.
*   **Item 4 (Scatter Plot with Latest Entries):** While `latest_entries = df.loc[df.groupby('location')['date'].idxmax()]` is a correct way to get the latest entry per location, the plot itself doesn't handle `NaN` values for `gdp_per_capita` or `total_vaccinations`. This can lead to an empty plot if there are many NaNs. The ground truth explicitly addresses this with `dropna`.
*   **General Data Handling:** The ground truth includes error handling for file not found and general exceptions, which is good practice for robustness. The generated code lacks this.

**Completeness/Quality Issues:**
*   **Formatting/Presentation:** The plots generated by the code are quite basic. They lack advanced styling (like `sns.set_theme`), and some useful features that improve readability (e.g., `plt.grid`, `plt.tight_layout`, `plt.xticks(rotation=...)`, `log` scales where appropriate as seen in the scatter plot in GT).
*   **Titles and Labels:** While titles and basic labels are present, some could be more descriptive.
*   **No `kde` for histogram:** The ground truth includes `kde=True` for the histogram, which often provides a useful density estimate alongside the bars. The generated code sets `kde=False`.
*   **Legend for Scatter Plot:** The ground truth includes a `hue` argument for the scatter plot, coloring points by continent and providing a legend, which significantly enhances the plot's interpretability. The generated code omits this.
*   **No `print` statements:** The ground truth includes print statements to indicate progress, which can be helpful if the script takes a long time or for debugging. Not a critical flaw, but good practice.

**Strong Points:**
*   Correctly loads data and parses the 'date' column.
*   Correctly implements the time series line plot of 'new_cases' for India.
*   Correctly implements the histogram for 'reproduction_rate'.
*   Correctly implements the box plot for 'stringency_index' by continent.
*   Uses the correct libraries (`pandas`, `matplotlib.pyplot`, `seaborn`).",mistral-nemo,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# 1) Load the dataset and list column names with data types
df = pd.read_csv('clash_royale_cards.csv')
print(df.info())

# 2) Calculate descriptive statistics for numerical columns
print(""\nDescriptive Statistics:"")
print(df[['elixirCost', 'Win Rate', 'Usage']].describe())

# 3) Provide the total number of cards (rows)
print(""\nTotal number of cards: "", len(df))
```",8,"The generated code correctly addresses all parts of the task. It loads the data, uses `df.info()` to display column names and data types, calculates descriptive statistics for the specified columns using `df.describe()`, and reports the total number of rows with `len(df)`.

Strong points:
- **Conciseness:** It achieves the required output with very compact and readable code.
- **Correctness:** All calculations and reports are accurate.
- **Appropriate Pandas functions:** `df.info()`, `df.describe()`, and `len(df)` are efficient and standard ways to get the requested information.

Areas for improvement (to achieve a higher score, closer to the ground truth):
- **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and general exceptions during CSV loading, which is crucial for production-ready code. The generated code lacks this, which could cause a script to fail ungracefully if the file isn't present.
- **Explicit Output Formatting:** While `df.info()` and `df.describe()` provide the requested data, the ground truth explicitly formats the output with descriptive print statements (e.g., ""--- Column Names and Data Types ---"", ""Column Names:"", ""Data Types of Each Column:""). This makes the output more user-friendly and easier to parse for someone reading the program's output. For example, `df.info()` prints ""None"" at the end, which isn't ideal for a clean output.
- **Specific Statistics:** The task asked for ""mean, median, standard deviation, min, max"". While `df.describe()` provides these, it also provides count, 25th, 50th (median), and 75th percentiles. The ground truth focuses on precisely the requested statistics using `.agg()`, which is more targeted. However, for a quick benchmark, `describe()` is often acceptable.",mistral-nemo,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

df = pd.read_csv('clash_royale_cards.csv')
filtered_df = df[df['elixirCost'] <= 3]
result = filtered_df[['Card', 'Win Rate']].sort_values('Win Rate', ascending=False)
print(result)
```",9,"The generated code correctly implements all the requirements of the task: it loads the CSV, filters cards with `elixirCost` less than or equal to 3, selects the `Card` and `Win Rate` columns, and sorts the results by `Win Rate` in descending order. The code is concise and efficient.

The only minor area of improvement relates to robustness and good practices for production-level code. While not strictly required by the prompt, the ground truth code includes error handling for `FileNotFoundError` and explicit variable definitions for column names and file paths. These additions make the code more robust against common issues (e.g., file not found) and more maintainable (e.g., if column names change). However, for a direct solution to the given task, the generated code is excellent.",mistral-nemo,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"import pandas as pd
import matplotlib.pyplot as plt

# Load dataset
data = pd.read_csv('clash_royale_cards.csv')

# Generate histogram for elixirCost
plt.hist(data['elixirCost'], bins=30, color='steelblue')
plt.title('Distribution of Elixir Cost')
plt.xlabel('Elixir Cost')
plt.ylabel('Frequency')
plt.show()",7,"The generated code correctly loads the CSV and generates a histogram for the `elixirCost` column with a title and labeled axes. This addresses the core requirements of the task.

However, there are several areas for improvement:
- **Robustness**: The generated code lacks error handling for `FileNotFoundError` or other potential issues during CSV loading, which the ground truth gracefully handles.
- **Plotting Library Choice**: While `matplotlib.pyplot.hist` works, the task prompt also mentions ""Use matplotlib or seaborn for plotting."" The ground truth uses `seaborn.histplot`, which often produces more aesthetically pleasing plots and offers more features out-of-the-box (like `kde=False` for discrete data).
- **Binning**: The `bins=30` used in the generated code is a generic choice. For discrete integer data like `elixirCost` (which typically ranges from 1 to 10), explicitly setting bins to cover each integer value (e.g., `range(1, 12)` if 1-11 are possible values, or `range(1, max(df['elixirCost']) + 2)`) as done in the ground truth, provides a more accurate and meaningful representation of the distribution.
- **Labels and Title Quality**: While present, the title ""Distribution of Elixir Cost"" is a bit generic. The ground truth's ""Distribution of Elixir Cost for Clash Royale Cards"" is more descriptive. The axis labels are standard, but the ground truth adds `fontsize` for better readability.
- **Minor Enhancements**: The ground truth includes `figsize` for better plot sizing, `edgecolor` for bar separation, `xticks` for clearer integer labeling, and a `grid` which are good practices for professional plots.

Overall, it's a functional solution but could be significantly improved in terms of robustness, plot quality, and adherence to best practices in data visualization for the given data type.",mistral-nemo,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# 1) Load dataset and list column names with data types
df = pd.read_csv('iris.csv')
print(df.columns, df.dtypes)

# 2) Calculate basic descriptive statistics for numerical columns
print(""\nDescriptive statistics for numerical columns:"")
print(df[[""SepalLengthCm"", ""SepalWidthCm"", ""PetalLengthCm"", ""PetalWidthCm""]].describe())

# 3) Report number of unique species and their counts
print(""\nNumber of unique species and counts:"")
print(df[""Species""].value_counts())
```",7,"The generated code correctly performs the core tasks: loading the CSV, identifying column data types (though the output format is slightly different from the ground truth), calculating descriptive statistics for the specified numerical columns, and counting unique species.

Areas for improvement:
1.  **Robustness (File Loading):** The ground truth includes a `try-except` block for robust file loading, which is good practice. The generated code lacks this, meaning it would crash if `iris.csv` isn't found.
2.  **Specific Statistics:** The task asked for ""mean, median, standard deviation, min, max"". The generated code uses `.describe()`, which provides these along with 'count', '25%', '50%', '75%'. While these are often useful, explicitly listing the requested statistics or filtering them (as the ground truth does with `.agg()`) would be more precise for the prompt. The ground truth also explicitly prints the column names and data types separately for clarity, whereas the generated code prints them on one line.
3.  **Clarity of Output:** The ground truth includes more descriptive print statements (`""--- Column Names and Data Types ---""`, etc.), which makes the output more user-friendly. The generated code's print statements are more succinct.
4.  **Error Handling (Column Existence):** The ground truth checks if the specified numerical columns and the 'Species' column actually exist in the DataFrame before attempting operations. This makes the code more robust against potential issues like misspellings or missing columns in the input data. The generated code assumes these columns will always be present.",mistral-nemo,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

df = pd.read_csv('iris.csv')
 filtered_df = df[df['Species'] == 'Iris-setosa']
 print(filtered_df[['SepalLengthCm', 'SepalWidthCm']])
```",8,"The generated code correctly performs all the required operations: loading the CSV, filtering by 'Iris-setosa' species, and selecting/displaying the specified columns. It is concise and directly addresses the problem.

Areas for improvement:
- **Error Handling**: The generated code lacks explicit error handling for `FileNotFoundError` or other potential issues (`KeyError` if columns are missing), which is present in the ground truth. While not strictly requested in ""Provide only the Python code without explanations"" for the output, robust code would include it.
- **Clarity/Readability**: The ground truth uses intermediate variables (`filtered_setosa_df`, `selected_sepal_columns_df`) which can sometimes improve readability for complex operations, though for this simple task, direct chaining is also acceptable. The generated code also has an indentation issue on the second line (` filtered_df`).",mistral-nemo,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('iris.csv')
plt.scatter(df['SepalLengthCm'], df['SepalWidthCm'], c=df['Species'].map({'setosa': 'red', 'versicolor': 'blue', 'virginica': 'green'}))
plt.title('Iris Dataset: Sepal Length vs Width')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.show()
```",6,"The generated code correctly imports necessary libraries, loads the CSV, and creates a scatter plot with a title and axis labels. However, it fails to correctly color the points by species as requested. The `.map()` function with hardcoded colors assumes specific species names and order, which is brittle and not a generalizable way to map categorical data to colors in `matplotlib.pyplot.scatter`. The ground truth demonstrates a more robust approach by iterating through unique species and plotting subsets, which also automatically handles the legend, a detail missing in the generated code. The x and y axis labels are also slightly different which is a minor point. Given the coloring by `Species` was a core requirement, its incorrect implementation affects completeness.",mistral-nemo,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# List column names and data types
print(df.columns, df.dtypes)

# Calculate descriptive statistics for numerical columns
numerical_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']
print(df[numerical_cols].describe())

# Count occurrences for each value in the 'HeartDisease' column
print(df['HeartDisease'].value_counts())
```",6,"The generated code correctly loads the CSV, lists column names and data types (though the output format isn't as clear as the ground truth), calculates descriptive statistics for the specified numerical columns, and reports value counts for the 'HeartDisease' column.

However, it lacks robustness and completeness in several areas:
1.  **Error Handling**: The code doesn't include any error handling for `FileNotFoundError` or other potential issues when loading the CSV, which is crucial for real-world applications.
2.  **Output Clarity**: While it provides the requested information, the presentation (e.g., for column names and data types) is less user-friendly and formatted than the ground truth. The ground truth explicitly prints each column name and labels the data type output.
3.  **Specific Statistics**: The task asked for ""mean, median, standard deviation, min, max"". `df.describe()` provides these but also includes 'count', '25%', and '75%', which weren't explicitly requested. The ground truth processed the `describe()` output to select only the requested statistics and renamed '50%' to 'median' for clarity.
4.  **Assumptions**: It assumes all specified numerical columns and the 'HeartDisease' column exist. The ground truth includes checks for the existence of these columns before attempting operations, making it more resilient to variations in the dataset.
5.  **Descriptive Text**: The ground truth includes print statements describing each step and its output, which enhances readability and understanding. The generated code is very terse.

To improve, the generated code should incorporate error handling, refine output formatting for better readability, and ensure descriptive statistics are tailored precisely to the request.",mistral-nemo,LLM_judge_config
E29,easy,Data Manipulation,,,Error,Failed to generate code,mistral-nemo,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Filter wines
filtered_df = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]

# Find the best quality wine
best_quality_wine = filtered_df.loc[filtered_df['quality'].idxmax()]

# Display features of the best quality wine
print(best_quality_wine)
```",8,"The generated code correctly implements all the core requirements of the task: it filters the DataFrame based on the specified conditions (`alcohol > 12` AND `pH < 3.3`), finds the wine with the maximum 'quality' within that filtered subset, and displays all features of that wine.

**Strong Points:**
*   **Correct Filtering:** The compound boolean indexing `(df['alcohol'] > 12) & (df['pH'] < 3.3)` is correctly applied.
*   **Correct Max Quality Identification:** `filtered_df['quality'].idxmax()` coupled with `.loc[]` correctly identifies the highest quality wine.
*   **Correct Output:** `print(best_quality_wine)` correctly displays all features of the resulting Series.
*   **Proper Imports:** `import pandas as pd` is included.

**Areas for Improvement (Why not a 9 or 10):**
*   **Lack of Error Handling:** The code does not include any error handling for cases like the CSV file not being found (`FileNotFoundError`) or if the specified columns (`alcohol`, `pH`, `quality`) are missing from the dataset. This makes the code less robust for real-world scenarios.
*   **No Edge Case Handling for Empty Filtered Set:** If `filtered_df` happens to be empty (i.e., no wines meet the initial criteria), `filtered_df['quality'].idxmax()` would raise an error. The ground truth handles this with an `if filtered_wines.empty:` check.
*   **Hardcoded Filename:** While acceptable for the prompt, using a variable for the dataset path would be more flexible (as in the ground truth).
*   **Readability/Structure (Minor):** The ground truth wraps the logic in a function and includes comments and print statements that provide better context and inform the user about the process (e.g., number of filtered wines). While not strictly required by ""only provide Python code without explanations"", the structure makes the GT more production-ready. For a benchmark, direct implementation is often preferred, but good coding practices still apply.",mistral-nemo,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('heart.csv')

# 1) Histogram of Age
df['Age'].plot(kind='hist', title='Distribution of Age', xlabel='Age', ylabel='Frequency')
plt.show()

# 2) Bar chart of Sex
df['Sex'].value_counts().plot(kind='bar', title='Distribution of Sex', xlabel='Sex', ylabel='Count')
plt.xticks(rotation=0)
plt.show()
```",7,"The generated code correctly loads the CSV and generates both a histogram for 'Age' and a bar chart for 'Sex'. It includes proper imports and titles/labels. However, it uses the less flexible `df.plot()` method directly, which doesn't offer the same level of customization and visual appeal as `matplotlib.pyplot` or `seaborn` directly, especially for adding features like KDE, grid, and refined aesthetics. The `Sex` column in `heart.csv` is typically encoded as 0 and 1, and the generated code plots these numerical values directly without mapping them to 'Male'/'Female', making the bar chart less intuitive. The ground truth handles this by mapping the values. While the code is functional and meets the basic requirements, it could be improved by leveraging seaborn or more direct matplotlib functions for enhanced plotting capabilities and better categorical representation.",mistral-nemo,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

# 1) List all column names and identify the data type of each
df = pd.read_csv('spotify_2023.csv')
print(df.columns)
print(df.dtypes)

# 2) Convert the `streams` column to a numeric data type, handling potential errors by coercing non-numeric values to NaN, then report any NaNs
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
print('\nNumber of NaNs in streams column:', df['streams'].isna().sum())

# 3) Calculate and report basic descriptive statistics...",7,"The generated code addresses most of the requirements but has some shortcomings.

**Strengths:**
*   Correctly loads the CSV.
*   Successfully converts the `streams` column to numeric and reports NaNs.
*   Correctly calculates descriptive statistics for the specified columns.
*   Identifies the number of unique artists.
*   Includes proper imports.

**Areas for Improvement:**
*   **Column Names and Data Types (completeness/correctness):** The request specifically asked to ""List all column names and identify the data type of each."" While `print(df.columns)` lists column names and `print(df.dtypes)` lists data types, `df.info()` as used in the ground truth is a more standard and comprehensive way to display this information, including non-null counts, which is very useful for initial data inspection. The current output is functional but less informative than desired for a robust data evaluation.
*   **Numerical Column Names for Stats (correctness):** The prompt specified 'danceability_%', 'valence_%', and 'energy_%' with the percentage sign. The generated code uses 'danceability%', 'valence%', 'energy%' without the underscore, which might cause a `KeyError` if the actual column names in the CSV include the underscore (which they typically do for this dataset). This is a subtle but important detail for correctness.
*   **Error Handling (completeness):** There's no error handling for `FileNotFoundError` when loading the CSV, which is crucial for robust code.
*   **Verbosity/Clarity of Output:** The output of `df.columns` and `df.dtypes` is raw Pandas Series/Index objects, which are less user-friendly than a formatted print statement or `df.info()`. Similarly, the descriptive stats are printed directly without any introductory message, making the output slightly less readable for a human. The ground truth provides more descriptive print statements for each step.",mistral-nemo,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

df = pd.read_csv('spotify_2023.csv')
df_filtered = df[df['released_year'] == 2023]
df_filtered[['track_name', 'artist(s)_name', 'streams']] = df_filtered[['track_name', 'artist(s)_name', 'streams']].apply(pd.to_numeric, errors='coerce')
sorted_df = df_filtered.sort_values('streams', ascending=False)
print(sorted_df)
```",6,"The generated code attempts to address all requirements but has a critical flaw in how it handles the `streams` column conversion and selection.

**Correctness:**
The line `df_filtered[['track_name', 'artist(s)_name', 'streams']] = df_filtered[['track_name', 'artist(s)_name', 'streams']].apply(pd.to_numeric, errors='coerce')` is incorrect. `pd.to_numeric` should be applied specifically to the 'streams' column, not to all three columns. Applying it to 'track_name' and 'artist(s)_name' will likely result in those columns becoming entirely NaN, which is incorrect and not what was intended for `streams` to be numeric. The `streams` column should be converted *before* sorting, and this conversion should be done only on `streams`. Also, it selects the columns first, then tries to convert, which can be problematic; it's generally better to convert the column first on the whole or filtered dataframe.

**Code Quality:**
*   **Imports:** Proper import of pandas.
*   **Structure:** The structure is concise, but the critical error in column conversion significantly impacts correctness.
*   **Error Handling:** It lacks error handling for file not found, which was present in the ground truth. While not explicitly asked for in the prompt's short requirements, it's good practice.

**Completeness:**
*   Loads CSV: Yes.
*   Filters for `released_year` == 2023: Yes.
*   Selects `track_name`, `artist(s)_name`, `streams`: It tries to, but the method used for `pd.to_numeric` will corrupt non-numeric columns if applied to them.
*   Ensures `streams` is numeric: It attempts to, but incorrectly applies `pd.to_numeric` to non-numeric columns too.
*   Sorts by `streams` descending: Yes, assuming `streams` was correctly converted (which it isn't with the current code).

To improve, the `streams` column should be converted to numeric **before** selecting the columns, or at least specifically targeting *only* the `streams` column for conversion.

Corrected approach for the `streams` conversion and column selection:
```python
import pandas as pd

df = pd.read_csv('spotify_2023.csv')
# Convert streams to numeric on the original or filtered df first
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

df_filtered = df[df['released_year'] == 2023]
# Now select the columns
selected_df = df_filtered[['track_name', 'artist(s)_name', 'streams']]
sorted_df = selected_df.sort_values(by='streams', ascending=False)
print(sorted_df)
```",mistral-nemo,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load data
data = pd.read_csv('spotify_2023.csv')

# Histogram for bpm
plt.hist(data['bpm'], bins=30, edgecolor='black')
plt.title('Distribution of BPM')
plt.xlabel('BPM')
plt.ylabel('Frequency')
plt.show()

# Scatter plot for danceability% and energy%
plt.scatter(data['danceability_%'], data['energy_%'])
plt.title('Relationship between Danceability% and Energy%')
plt.xlabel('Danceability%')
plt.ylabel('Energy%')
plt.show()
```",8,"The generated code correctly loads the CSV, creates a histogram for 'bpm', and a scatter plot for 'danceability_%' vs 'energy_%'. Both plots have titles and labeled axes, and proper imports are included.

Areas for improvement:
1.  **Separate figures:** The current code uses `plt.show()` after each plot, which opens separate windows. While this fulfills the visualization requirement, a common practice is to create separate figure objects (`plt.figure()`) to manage plots more explicitly, which can be useful when generating multiple plots in a script without immediately displaying them, or when saving them. The ground truth demonstrates this by creating two separate `plt.figure()` calls.
2.  **Error Handling:** The generated code lacks error handling for file not found or corrupted CSVs. While not explicitly asked for in the prompt, robust code often includes this. The ground truth provides examples of `try-except` blocks.
3.  **Clarity/Readability (minor):** The ground truth uses constants for column names, which can improve readability and maintainability, though this is a style preference and not a strict requirement for correctness.",mistral-nemo,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"import pandas as pd

# Load dataset
df = pd.read_csv('youtube_trendings.csv')

# List all column names and their inferred data types
print(df.columns, df.dtypes)

# Clean numeric columns
for col in ['Video views', 'Likes', 'Dislikes']:
    df[col] = pd.to_numeric(df[col].str.replace(',', ''), errors='coerce')
    df[col].fillna(0, inplace=True)

# Calculate basic statistics
print(df[['Video views', 'Likes', 'Dislikes']].describe())

# Fill missing values in 'Category' column
df['Category'].filln...",8,"The generated code correctly addresses most of the task requirements.

**Strong points:**
- It correctly loads the CSV using pandas.
- It correctly identifies and cleans the 'Video views', 'Likes', and 'Dislikes' columns:
    - Removes commas using `str.replace(',', '')`.
    - Converts to numeric with `errors='coerce'` to handle conversion failures as NaN.
    - Fills NaNs with 0 using `fillna(0)`.
- It correctly fills missing 'Category' values with 'Unknown'.
- It correctly reports the count of unique values in the 'Category' column.

**Areas for improvement (leading to a score of 8 instead of 10):**
- **Column names and data types reporting:** The task requested to ""List all column names and their inferred data types."" The generated code uses `print(df.columns, df.dtypes)`. While this does technically print both, `df.info()` as used in the ground truth is a more standard and comprehensive way to display this information, including non-null counts, which is often very useful for initial data understanding. The current output is a bit less readable than `df.info()`.
- **Descriptive statistics reporting:** The task specified to ""Calculate and report basic descriptive statistics (mean, median, std, min, max) for these cleaned `Video views`, `Likes`, and `Dislikes` columns."" The generated code uses `print(df[['Video views', 'Likes', 'Dislikes']].describe())`. While `describe()` does provide mean, std, min, max, it *does not* explicitly provide the median. The ground truth calculated and printed each of these metrics individually, including the median.
- **Handling `inplace=True`:** While `inplace=True` generally works, it's often considered good practice in modern pandas to reassign the DataFrame explicitly, e.g., `df[col] = df[col].fillna(0)` rather than `df[col].fillna(0, inplace=True)`. This can lead to fewer unexpected side effects and is sometimes slightly more performant for very large datasets if you reassign. However, for this task, it's a minor stylistic point.

Overall, the code is very functional and gets the job done for the core data cleaning and transformation steps. The minor deviations are in the specific reporting format of the initial column info and the comprehensive descriptive statistics.",mistral-nemo,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

# 1) Load and clean the dataset
df = pd.read_csv('youtube_trendings.csv')
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)

# 2) Filter dataset
df_filtered = df[df['Video views'] > 50000000]

# 3) Select and display columns
display(df_filtered[['Video', 'Video views']])

# 4) Sort the data by views in descending order
df_sorted = df_filtered.sort_values('Video views', ascending=False)
display(df_sorted[['Video', '...",9,"The generated code correctly implements all the requirements of the task. It properly loads the CSV, cleans the 'Video views' column by removing commas, converting to numeric, and filling NaNs with 0. It then filters the data, selects the specified columns, and sorts them correctly in descending order.

A minor point for improvement would be to handle the `FileNotFoundError` explicitly, as done in the ground truth, for better robustness. Additionally, using `print()` for displaying the final DataFrame is typically preferred when comparing against a ground truth that explicitly uses `print()` for final output, as `display()` might behave differently depending on the environment (e.g., Jupyter vs. script execution). However, within a Jupyter environment, `display()` is perfectly fine. The double `display()` call for the filtered and then the sorted data is redundant, as only the final sorted output is requested to be displayed. But these are minor issues that don't detract significantly from the overall correctness of the logic.",mistral-nemo,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Remove commas, convert to numeric and fill NaNs with 0
df[['Likes', 'Video views']] = df[['Likes', 'Video views']].replace(',', '', regex=True).astype(float, errors='fill')
df.fillna(0, inplace=True)

# Create scatter plot
plt.scatter(df['Video views'], df['Likes'])

# Set plot title and labels
plt.title('Video Views vs. Likes')
plt.xlabel('Video Views (cleaned)')
plt.yl...",6,"The generated code correctly loads the dataset and creates a scatter plot with the specified title and labels. However, the cleaning of the 'Likes' and 'Video views' columns is incorrect, which is a critical part of the task.

**Incorrect Cleaning:**
The line `df[['Likes', 'Video views']] = df[['Likes', 'Video views']].replace(',', '', regex=True).astype(float, errors='fill')` attempts to perform multiple operations at once.
- `replace(',', '', regex=True)` will remove commas, but this operation is applied to the DataFrame subset, and its output is a DataFrame of strings.
- `.astype(float, errors='fill')` is not a valid parameter for `astype`. It should be `errors='coerce'`. Even if it were `errors='coerce'`, applying it directly after `replace` on the entire subset might not work as intended for column-wise conversion if some values couldn't be converted.
- The `df.fillna(0, inplace=True)` line fills *all* NaNs in the *entire* DataFrame with 0, not just in the specified 'Likes' and 'Video views' columns, which was a specific requirement.

The primary failure point is the incorrect cleaning logic. While the plotting is mostly correct, the data fed into the plot would likely be erroneous due to the cleaning issues.

**Specific improvements:**
1.  **Correct `astype` usage:** Change `errors='fill'` to `errors='coerce'`.
2.  **Separate operations:** For robust cleaning, it's better to perform the operations sequentially: remove commas (ensuring string type), then convert to numeric with `errors='coerce'`, then fill NaNs *only* for the target columns.
3.  **Targeted `fillna`:** Explicitly apply `fillna(0)` only to the 'Likes' and 'Video views' columns, not the entire DataFrame.

The code also lacks error handling for file not found, which is present in the ground truth, but the prompt didn't explicitly ask for it, so it's not penalized heavily.",mistral-nemo,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd

# Load dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean columns
df['Mobile Weight'] = pd.to_numeric(df['Mobile Weight'].str.replace('g', '').replace('', pd.np.nan), errors='coerce')
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'].str.replace('USD ', '').str.repla...",7,"The generated code correctly addresses most of the requirements. It loads the dataset, cleans the specified columns by removing units and converting them to numeric types, and handles conversion errors. It also calculates descriptive statistics and the count of unique company names.

However, there are a few areas for improvement:
1.  **Cleaning of 'Mobile Weight'**: The generated code uses `replace('', pd.np.nan)` which is less robust than `str.strip()` followed by `pd.to_numeric` as shown in the ground truth. Specifically, `df['Mobile Weight'].str.replace('g', '').replace('', pd.np.nan)` might introduce NaNs if the original string becomes empty after removing 'g' (e.g., if it was just 'g'), but `pd.to_numeric` with `errors='coerce'` handles this more naturally by converting non-numeric empty strings to NaN. The ground truth's `astype(str)` and `str.strip()` ensure consistent string handling before `to_numeric`.
2.  **Listing Data Types**: The code `print(df.select_dtypes(include=['int64', 'float64']).dtypes)` only prints numeric column types, not ""all column names and their inferred data types after cleaning"" as requested. The ground truth's `print(df.dtypes)` correctly displays all types.
3.  **Use of `pd.np.nan`**: `pd.np.nan` is deprecated; `np.nan` should be used. The ground truth correctly imports `numpy as np` and uses `np.nan` if needed (though it achieves the same without explicit `replace`).
4.  **Robustness**: The generated code lacks error handling for file not found scenarios, which the ground truth includes. While not explicitly asked for in the task, good practice dictates such handling.
5.  **Output Clarity**: The outputs are functional but lack the descriptive print statements that make the ground truth's output more user-friendly and clear about what is being displayed at each step.",mistral-nemo,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the 'Launched Price (India)' column
df['Launched Price (India)'] = df['Launched Price (India)'].str.replace('INR ', '').str.replace(',', '').apply(lambda x: pd.to_numeric(x, errors='coerce'))

# Clean the 'RAM' column
df['RAM'] = df['RAM'].str.replace('GB', '').apply(lambda x: pd.to_numeric(x, errors='coerce'))

# Filter the dataset
df_filtered = df[(df['Company Name'] == 'Apple') & (df['Launched Pric...",9,"The generated code is highly correct and complete, addressing almost all requirements. It loads the dataset, correctly cleans both 'Launched Price (India)' and 'RAM' columns by removing specified strings and converting to numeric with error handling (using `errors='coerce'`). It then accurately filters the data for 'Apple' and price less than 100,000, selects the required columns, and sorts them.

The only minor point of improvement for achieving a perfect 10 would be to explicitly handle potential `FileNotFoundError` when loading the CSV, as seen in the ground truth. While not strictly part of the core data manipulation task, robust code often includes such error handling. Also, the `apply(lambda x: pd.to_numeric(x, errors='coerce'))` for cleaning `Launched Price (India)` could be slightly simplified by chaining `pd.to_numeric` directly after the string operations, as `str.replace` returns a Series which `pd.to_numeric` can directly consume. However, the current approach is functionally correct.

Overall, it's an excellent solution that demonstrates a strong understanding of pandas operations for data cleaning and manipulation.",mistral-nemo,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean RAM column
df['RAM'] = df['RAM'].str.replace('GB', '').astype(float, errors='coerce')

# Clean Battery Capacity column
df['Battery Capacity'] = df['Battery Capacity'].str.replace('mAh', '').astype(float, errors='coerce')

# Create scatter plot
fig, ax = plt.subplots()
colors = plt.get_cmap('viridis')(df['Company Name'].astype('category').cat.codes)
scatter = ax.scatter(...",7,"The generated code correctly loads the data, cleans the `RAM` and `Battery Capacity` columns, and creates a scatter plot with a title and axis labels. The cleaning of the columns correctly uses `str.replace` and `astype(float, errors='coerce')`, which handles conversion errors by setting to NaN, fulfilling that requirement.

However, the coloring of the points by `Company Name` is problematic. It attempts to map `Company Name` to colors manually using `plt.get_cmap('viridis')(df['Company Name'].astype('category').cat.codes)`. While this generates a color for each company, the `colorbar` then becomes a continuous scale based on the categorical codes, which is not intuitive or properly representative of distinct companies. It also lacks a proper legend for the `Company Name`. The best practice for coloring by categorical data in matplotlib is often to iterate through unique categories or, more simply and robustly, use `seaborn.scatterplot` which handles this directly with the `hue` parameter and automatically generates a legend.

Specific areas for improvement:
1.  **Categorical Coloring/Legend:** The current approach to coloring by `Company Name` (using `cat.codes` and a colormap with a continuous colorbar) is not ideal for categorical data. It does not provide a clear legend showing which color corresponds to which company. Using `seaborn.scatterplot` with the `hue` parameter is the recommended and simplest way to achieve this effectively with a proper legend. If sticking to matplotlib, iterating through unique company names and plotting each subset would be more appropriate for a legend.
2.  **Robustness (File Not Found):** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is good practice. The generated code assumes the file will always be present. While not strictly required by the prompt's core task, it's a good enhancement for real-world robustness.",mistral-nemo,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 5 rows
data = pd.read_csv('titanic.csv')
print(data.head())

# 2) Show the shape of the dataset (rows and columns)
print(""\nShape:"", data.shape)

# 3) List all column names and their data types
print(""\nColumn Names & Data Types:"")
print(data.dtypes)

# 4) Calculate the survival rate (percentage of passengers who survived)
survival_rate = (data['Survived'].mean() * 100)
print(f""\nSurvival Rate: {survival_rate:.2f}%"")

# 5...",9,"The generated code is very good and accurately addresses almost all requirements of the task.

**Strong Points:**
*   **Correctness:** All calculations and data displays are correct.
*   **Conciseness:** The code is quite concise, effectively using pandas functions like `.head()`, `.shape`, `.dtypes`, `.mean()`, and `.isna().sum()`.
*   **Proper Imports:** `pandas` is correctly imported.
*   **Survival Rate Calculation:** Using `data['Survived'].mean() * 100` is a very elegant and correct way to calculate the survival rate, as the 'Survived' column contains 0s and 1s.
*   **Missing Values:** Correctly uses `data.isna().sum()` to get missing value counts.

**Areas for Minor Improvement (Why not a 10):**
*   **Output Clarity:** The ground truth code includes more descriptive print statements (e.g., ""1. First 5 rows of the dataset:"", ""3. Column names and their data types:"") which makes the output more user-friendly and clearly delineates each requested task. The generated code's print statements are a bit more terse.
*   **Error Handling:** The ground truth includes a `try-except FileNotFoundError` block, which is good practice for robust code, especially when dealing with external files. The generated code assumes the `titanic.csv` file will always be present, which could lead to a `FileNotFoundError` and script termination without a graceful message.
*   **Data Types Display:** While `data.dtypes` gives the correct information, `df.info()` (used in the ground truth) provides a more comprehensive summary that includes memory usage, non-null counts, and the total number of entries, which can be more informative for EDA. However, `data.dtypes` directly fulfills the literal request ""List all column names and their data types.""

Overall, it's a highly functional and well-written solution. The missing error handling and slightly less descriptive output are minor points in an otherwise very solid code.",mistral-nemo,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

# 1) List all column names and their data types
df = pd.read_csv('imdb_top_250.csv')
print(df.dtypes)

# 2) Calculate and report basic descriptive statistics for numerical columns
print(""\nDescriptive statistics:"")
print(df[['Year', 'Rating', 'Runtime']].describe())

# 3) Report the total number of movies in the dataset
print(""\nTotal number of movies:"", df.shape[0])
```",7,"The generated code correctly addresses most of the task requirements but lacks robustness and full completeness compared to the ground truth.

**Strengths:**
*   Correctly loads the CSV using `pd.read_csv`.
*   Correctly prints column names and data types using `df.dtypes`.
*   Correctly calculates and prints descriptive statistics for the specified numerical columns using `df[['Year', 'Rating', 'Runtime']].describe()`.
*   Correctly reports the total number of movies using `df.shape[0]`.

**Areas for Improvement (Why it didn't get a perfect score):**
*   **Error Handling:** The code does not include any error handling (e.g., `FileNotFoundError`) for loading the CSV, which is a critical part of robust code. The ground truth demonstrates proper `try-except` for this.
*   **Runtime Column Preprocessing:** The most significant missing piece is the preprocessing of the 'Runtime' column. In many datasets, 'Runtime' is stored as a string (e.g., ""142 min"") and needs explicit conversion to numeric type before statistics can be accurately calculated. The generated code implicitly assumes 'Runtime' is already numeric, which is a common pitfall. The ground truth explicitly handles this conversion. This could lead to an error or incorrect statistics if the 'Runtime' column is not numeric.
*   **Specific Statistics:** While `describe()` provides all basic statistics, the task specifically asked for ""mean, median, standard deviation, min, max"". `describe()` output includes count, 25th, and 75th percentiles beyond these. The ground truth precisely selects and renames the required statistics.
*   **Clarity of Output:** The ground truth includes more descriptive print statements to clearly label each section of the output, making it more user-friendly. `df.info()` is also generally preferred over `df.dtypes` for providing more comprehensive information (like non-null counts) about the DataFrame structure.
*   **Robustness of Column Selection:** The ground truth checks if specified columns exist in the DataFrame and if they are numeric after preprocessing before attempting to calculate statistics, making the code more robust against variations in dataset structure.",mistral-nemo,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

df = pd.read_csv('imdb_top_250.csv')
filtered_df = df[df['Rating'] >= 9.0]
displayed_df = filtered_df[['Title', 'Rating']].sort_values('Rating', ascending=False)
```",8,"The generated code correctly implements all the core requirements of the task: loading the CSV, filtering by rating, selecting specified columns, and sorting in descending order. It correctly uses pandas for these operations.

Areas for improvement:
1.  **Display:** The generated code does not explicitly `print` the final DataFrame (`displayed_df`). While in an interactive environment the last line might implicitly display it, in a script, `print()` is necessary for output.
2.  **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError`, `KeyError`, and general exceptions. The generated code lacks any error handling, which is crucial for production-ready code or more robust scripts.
3.  **Clarity/Comments:** The ground truth code uses comments to explain each step, which enhances readability and understanding. The generated code is concise but could benefit from similar comments.
4.  **Variable Naming:** While acceptable, `displayed_df` might be slightly less descriptive than `sorted_movies` in the ground truth, which emphasizes the sorting aspect.

Overall, it's a very functional solution to the core problem, but could be made more robust and user-friendly with the additions mentioned.",mistral-nemo,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('imdb_top_250.csv')

# 1) Histogram for Rating
sns.histplot(df['Rating'], kde=False, bins=30)
plt.title('Distribution of Movie Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# 2) Scatter plot for Rating vs Runtime
sns.scatterplot(x='Runtime', y='Rating', data=df)
plt.title('Relationship between Movie Rating and Runtime')
plt.xlabel('Runtime (minutes)')
plt.ylabel('Rating')
pl...",7,"The generated code correctly addresses the core plotting requirements: creating a histogram for 'Rating' and a scatter plot for 'Rating' vs 'Runtime', with appropriate titles and labels. It uses the specified libraries (pandas, matplotlib, seaborn) and assumes the CSV is in the current directory.

However, it lacks robust error handling for `FileNotFoundError` and for missing or malformed columns, particularly the `Runtime` column. The `Runtime` column often contains strings like ""120 min"" and needs preprocessing to extract numeric values to be usable in a scatter plot. The generated code directly passes 'Runtime' to `sns.scatterplot`, which would likely fail or produce an incorrect plot if the column is not already numeric, which is a common issue with raw IMDb data. The ground truth demonstrates robust preprocessing for `Runtime`.

Improvements needed:
- Implement `try-except` for `pd.read_csv` to handle `FileNotFoundError`.
- Add preprocessing for the `Runtime` column to convert it to a numeric type, handling common string formats (e.g., '120 min'). This is critical for the scatter plot to function correctly and meaningfully.
- Add checks to ensure 'Rating' and 'Runtime' columns exist before attempting to plot.",mistral-nemo,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"import pandas as pd

df = pd.read_csv('penguins.csv')

print(""\nNumber of missing values:\n"", df.isna().sum())

df_no_missing = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])

print(""\nDescriptive statistics:\n"", df_no_missing[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']].describe())

print(""\nCount of unique values:\n"")
print(""species:"", df['species'].nunique())
print(""island:"", df['island'].nunique())",8,"The generated code correctly performs all the requested tasks: loading the data, reporting missing values, dropping NaNs for specific columns and calculating descriptive statistics on the filtered data, and counting unique values for the specified columns.

Strong Points:
- Correctly uses `df.isna().sum()` to report missing values.
- Correctly uses `dropna(subset=...)` to filter rows based on NAs in the specified numerical columns.
- Correctly uses `.describe()` to get descriptive statistics.
- Correctly uses `nunique()` for species and island columns.
- Imports `pandas` as required.

Areas for Improvement (why not higher):
- **Descriptive Statistics Detail:** While `describe()` is a valid and commonly used method, the prompt specifically asked for ""mean, median, standard deviation, min, max"". `describe()` provides these but also includes 'count', '25%', '50%', '75%' which are not explicitly requested. The ground truth used `.agg(['mean', 'median', 'std', 'min', 'max'])` which is more precise to the prompt's request.
- **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` when loading the CSV, which the generated code lacks. While not explicitly asked for, it's a good practice for production-ready code.
- **Output Formatting:** The output formatting is functional but less explicit and human-readable than the ground truth, which includes more descriptive print statements for each section. For example, ""Number of missing values:"" is good, but the ground truth also states ""--- Missing Values Report ---"" and filters to show only columns with missing values. The ground truth also explicitly states the number of rows before and after filtering, which provides useful context.",mistral-nemo,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

data = pd.read_csv('penguins.csv')

filtered_data = data[(data['species'] == 'Adelie') & (data['island'] == 'Torgersen')]

selected_data = filtered_data[['culmen_length_mm', 'culmen_depth_mm']]

final_data = selected_data.dropna()

print(final_data.head(5))
```",9,"The generated code correctly performs all the required operations: loads the CSV, filters by species and island, selects the specified columns, drops NaNs in these columns, and displays the first 5 rows. The code is concise and clear.

The only minor area for improvement, preventing a perfect score, is the `dropna()` call. While `selected_data.dropna()` works in this specific case because the `selected_data` DataFrame *only* contains `culmen_length_mm` and `culmen_depth_mm`, explicitly specifying `subset=['culmen_length_mm', 'culmen_depth_mm']` as done in the ground truth is a more robust and explicit practice. This ensures that `dropna` only considers NaNs in *those specific columns*, even if the DataFrame had other columns that might have slipped into the selection (which isn't the case here, but is good defensive programming). However, for this exact scenario, the generated code produces the correct result. The lack of error handling for file not found is also a minor omission compared to the ground truth, but not explicitly requested by the prompt.",mistral-nemo,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Create scatter plot of culmen_length_mm vs culmen_depth_mm
df = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])
plt.figure(figsize=(10, 8))
sns.scatterplot(data=df, x='culmen_length_mm', y='culmen_depth_mm', hue='species')

# Add title and labels
plt.title('Penguins Culmen Length and Depth (mm)')
plt.xlabel('Culmen Length (mm)')
plt.ylab...",8,"The generated code correctly addresses almost all requirements of the task. It properly loads the CSV, filters out rows with NAs in the specified columns (`culmen_length_mm`, `culmen_depth_mm`, `species`), generates the scatter plot with `culmen_length_mm` vs `culmen_depth_mm` colored by `species`, and includes a title and axis labels. The use of `seaborn.scatterplot` simplifies the plotting and handling of the `hue` aesthetic.

Areas for minor improvement:
- **Error Handling**: The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions during CSV loading. The generated code lacks explicit error handling for file operations. While not explicitly asked for, robust code often includes this.
- **Modularity**: The ground truth wraps the plotting logic in a function, `plot_penguins_culmen_data`, which makes the code more reusable and testable. The generated code is more of a script, which is acceptable for this level of task but less modular.
- **Plot Customization**: While the generated plot is correct, the ground truth adds a few more details like `s` (marker size), `alpha` (transparency), and `bbox_to_anchor` for the legend, which can improve the plot's aesthetics and readability, especially with many points. This is a minor point, as the core requirements are met.",mistral-nemo,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

# 1) Read the dataset
data = pd.read_csv('penguins.csv')

# 2) Fill NA values with mean for numerical columns
data['culmen_length_mm'] = data['culmen_length_mm'].fillna(data['culmen_length_mm'].mean())
data['culmen_depth_mm'] = data['culmen_depth_mm'].fillna(data['culmen_depth_mm'].mean())

# 3) Calculate and print mean, median, and standard deviation
print(f""culmen_length_mm: mean={data['culmen_length_mm'].mean()}, median={data['culmen_length_mm'].median()}, std={...",8,"The generated code correctly addresses all the requirements of the task. It loads the dataset, fills NA values in the specified columns with their respective means, calculates and prints the mean, median, and standard deviation for these columns, and finally prints the total number of rows and columns.

**Strong points:**
*   **Correctness:** All calculations and data manipulations are performed correctly.
*   **Conciseness:** The code is very concise and directly implements the required steps.
*   **Fulfillment of requirements:** All four parts of the task are explicitly handled.

**Areas for improvement (to reach a perfect 10):**
*   **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError`, which is good practice for robust code, especially when dealing with external files. The generated code assumes the file will always be present.
*   **Readability/Structure (Minor):** While functional, the ground truth's use of comments more aligned with the numbered task requirements and separate print statements for each statistic enhances readability, especially for complex outputs. The `f-string` for statistics in the generated code is a bit long.
*   **Generalization (Minor):** The ground truth uses a `NUMERICAL_COLUMNS` list and loops, which makes the code more scalable if additional columns needed the same processing. The generated code hardcodes the column names for NA filling and statistics, which is fine for two columns but less generalizable.
*   **Clarity of Output:** The ground truth provides more descriptive print statements (e.g., ""--- NA Imputation Summary ---"", ""Statistics for 'culmen_length_mm':""). The generated code's output is terse.",mistral-nemo,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
data = pd.read_csv('penguins.csv')

# 2) Drop rows where `flipper_length_mm` is NA
data = data.dropna(subset=['flipper_length_mm'])

# 3) Create a histogram of the `flipper_length_mm` column
plt.figure(figsize=(10, 6))
sns.histplot(data['flipper_length_mm'], kde=False)
plt.title('Distribution of Flipper Length (mm)')
plt.xlabel('Flipper Length (mm)')
plt.ylabel('Frequency')
plt.show()

# 4)...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, handles NA values in `flipper_length_mm`, creates a histogram for 'flipper_length_mm' with appropriate labels, and generates a bar chart for 'island' counts with correct labels. The use of seaborn's `histplot` and `countplot` is efficient and correct.

Strengths:
- **Correctness:** All visualization types and data manipulations (loading, NA handling) are correct.
- **Completeness:** All specified requirements (loading, dropping NAs, two specific plots with titles and labels) are met.
- **Code Quality:** The code is clean, readable, and uses standard libraries effectively.

Areas for minor improvement (not critical for score deduction given the prompt, but good practice):
- **Error Handling for File Loading:** The ground truth includes `try-except` blocks for file loading, which is robust. The generated code assumes the file exists. While the prompt states to assume the CSV is in the current directory, adding error handling for `FileNotFoundError` is generally a good practice for production-ready code.
- **Axis Label for Histogram:** The histogram's y-axis is labeled 'Frequency' which is commonly used, but 'Count' (as in the ground truth) is more accurate when `kde=False` as it shows raw counts per bin. This is a minor semantic point.
- **`sns.histplot` `data` parameter:** While passing `data['flipper_length_mm']` directly to `sns.histplot` works, it's generally more idiomatic and flexible to use `data=data, x='flipper_length_mm'` as shown in the ground truth, especially for more complex plots involving multiple columns or subsets.
- **`sns.set_theme`:** The ground truth sets a professional seaborn style, which is a nice touch for presentation. The generated code doesn't include this, but it wasn't explicitly asked for.

Overall, the generated code is a very good solution to the problem statement.",mistral-nemo,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# 1) Load the dataset
data = pd.read_csv('penguins.csv')

# 2) Filter the data to include only penguins from the 'Torgersen' island
filtered_data = data[data['island'] == 'Torgersen']

# 3) Count the number of penguins belonging to each species in the filtered subset
species_count = filtered_data['species'].value_counts()

# Print the result
print(species_count)
```",8,"The generated code correctly implements all the core requirements of the task: it loads the `penguins.csv` dataset, filters the data to include only penguins from 'Torgersen' island, and then accurately counts the number of penguins belonging to each species within that filtered subset, finally printing the result.

**Strong Points:**
*   **Correctness:** The logic for loading, filtering, and counting is entirely correct and produces the desired output.
*   **Conciseness:** The code is very concise and directly achieves the task's goals without unnecessary complexity.
*   **Readability:** Variable names are clear, and the code flow is easy to understand.

**Areas for Improvement (Why not a 10):**
*   **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` when loading the CSV and checks for the existence of critical columns (`island`, `species`). The generated code lacks these checks, which could lead to runtime errors if the file is missing or the column names are different.
*   **Edge Case Handling:** The ground truth also handles the edge case where no penguins are found on 'Torgersen' island. While the generated code would produce an empty Series in this case, a explicit message might be more user-friendly. Given the prompt's focus on ""expert code evaluator,"" these aspects of robustness are important.
*   **Output Clarity:** While `print(species_count)` gives the correct data, the ground truth provides a more descriptive print statement, which can be beneficial for understanding the output in a real-world scenario.

Overall, the generated code is highly functional and provides a valid solution to the problem. The improvements focus on making the code more robust and user-friendly, which are crucial aspects of production-ready data science code.",mistral-nemo,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

print(f""Shape of the DataFrame: {df.shape}"")
print(""\nDa...",10,"The generated code is excellent. It correctly defines the DataFrame, prints its shape, and lists the data types of all columns as requested. The code is concise, imports the necessary library, and executes all tasks accurately. The use of an f-string for printing the shape is a nice touch for cleaner output. While the ground truth includes more verbose comments and separators, the generated code directly addresses the prompt's requirements without unnecessary verbosity, which is often preferred in production code.",mistral-nemo,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('titanic.csv')

# Fill missing ages with the mean age
df['Age'].fillna(df['Age'].mean(), inplace=True)

# 1) Survival bar chart
survived = df[df['Survived'] == 1].shape[0]
not_survived = df[df['Survived'] == 0].shape[0]
plt.bar(['Survived', 'Not Survived'], [survived, not_survived])
plt.title('Survivors vs Non-Survivors')
plt.xlabel('Survival Status')
plt.ylabel('Count')
plt.show()

# 2) Pclass bar...",7,"The generated code correctly creates four distinct plots with proper titles and labels, as required by the task. It handles missing Age values by filling them with the mean, which is an acceptable approach. However, there are several areas where the code could be improved for better readability, robustness, and visual quality.

**Strengths:**
*   All four required plots are generated.
*   Plots have titles and axis labels.
*   Missing Age values are handled.
*   Imports are correct.

**Areas for Improvement:**
*   **Missing file handling:** The generated code lacks robust error handling for `FileNotFoundError` if 'titanic.csv' is not present, unlike the ground truth.
*   **Plotting method for categorical data:** For categorical data like 'Survived', 'Pclass', and 'Sex', using `value_counts().plot(kind='bar')` is generally more idiomatic and concise in pandas/matplotlib than manually creating lists for `plt.bar`. It also handles sorting and indexing more smoothly.
*   **Incorrect x-axis for Pclass bar chart:** `plt.bar(df['Pclass'].unique(), df['Pclass'].value_counts())` will likely cause issues if `df['Pclass'].unique()` is not in the same order as `df['Pclass'].value_counts().index`. Using `df['Pclass'].value_counts().sort_index().plot(kind='bar')` is more reliable.
*   **Incorrect x-axis for Sex bar chart:** `plt.bar(['Male', 'Female'], df['Sex'].value_counts())` assumes a specific order ('Male' then 'Female') that might not match `df['Sex'].value_counts()` if 'Female' happened to have a higher count and `value_counts()` defaults to descending order. It's safer to use the index of `value_counts()`.
*   **Lack of `plt.tight_layout()` and `plt.grid()`:** The ground truth includes `plt.tight_layout()` and `plt.grid()` for better plot presentation and preventing overlapping elements, which are missing here.
*   **No comments or structural organization:** The ground truth uses comments for better readability and separates code into logical blocks which makes it easier to understand.
*   **Customization:** The ground truth includes minor customizations like `xticks(rotation=0)` and color choices, which improve the visual appeal.",mistral-nemo,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Data
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]}

# Create DataFrame
df = pd.DataFrame(data)

# Calculate and print the average 'Mi...",9,"The generated code correctly addresses all parts of the task. It successfully creates the DataFrame, calculates and prints the average midterm and final scores, and correctly identifies the subject with the highest average final score. The code is concise and uses appropriate pandas functionalities.

Improvements (minor):
1. The print statements could be slightly more descriptive to match the ground truth's exact wording, like ""Average Midterm Score for all students:"" instead of just ""Average Midterm Score:"".
2. When identifying the subject with the highest average final score, the output could also include the actual highest average score value for that subject, as the ground truth does. This adds more informative context to the output.

However, these are minor stylistic and output completeness points, not functional correctness issues. The core requirements of the task are met well.",mistral-nemo,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
    'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
    'Volume': [1000000, 1200000, 1100000, 1300...",7,"The generated code correctly creates the DataFrame, calculates the daily range, and prints the DataFrame. However, there's a subtle but important error in how it finds the date with the highest 'Close' price.

The line `date_highest_close = df.loc[df['Close'].idxmax(), 'Date']` attempts to use `.loc` to get the 'Date' column from the index. Since 'Date' is already the index, `df['Close'].idxmax()` directly returns the correct Date object (which is the index label). The `.loc` call with `,'Date'` is attempting to access a column named 'Date', which no longer exists as a regular column because it was set as the index. This results in a `KeyError: 'Date'`.

To fix this, the line should simply be `date_highest_close = df['Close'].idxmax()`. Also, the format for printing the date with the highest close price could be more consistent with the ground truth by formatting it to a string, as `idxmax()` returns a Timestampt object.

Strong points:
- Adheres to the requested imports.
- Correctly creates the DataFrame and sets the index.
- Correctly calculates the 'Daily_Range'.
- Prints the final DataFrame correctly.

Areas for improvement:
- Correct the logic for finding the date with the highest 'Close' price to directly use `idxmax()` on the Series without trying to access a 'Date' column.
- Format the `date_highest_close` output for better readability, similar to the ground truth.",mistral-nemo,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST001']
}

df = pd...",9,"The generated code is highly correct and concise. It accurately creates the DataFrame, calculates the 'Total_Revenue' column, and correctly identifies the product sold the most by quantity using `value_counts().idxmax()`, which is a very efficient pandas method for this task. It also correctly prints the head of the DataFrame.

The only minor point of difference, not necessarily an error, is the method used for identifying the best-selling product. The ground truth uses `groupby('Product')['Quantity'].sum().idxmax()`, while the generated code uses `df['Product'].value_counts().idxmax()`. While `value_counts()` directly counts occurrences of unique values in a series (which works well if each row represents a single unit or if 'Quantity' is consistently 1), for a task specifying ""sold the most in terms of 'Quantity'"", relying solely on `value_counts()` on the 'Product' column assumes each sale of a product contributes 1 to its count. The ground truth method correctly sums the 'Quantity' for each product, which is more robust for cases where 'Quantity' can vary per transaction (e.g., one order sells 2 mice, another sells 1 keyboard).

In this specific dataset, 'Quantity' is primarily 1 for most products, but 'Mouse' has quantities of 2 and 1 in different orders, and 'Webcam' has a quantity of 3.
Let's analyze:
- Ground Truth: 'Mouse' (2+1 = 3), 'Laptop' (1+1 = 2), 'Keyboard' (1), 'Monitor' (1), 'Webcam' (3). Best is 'Mouse' (3) or 'Webcam' (3), it will pick the first one which is 'Mouse'.
- Generated: `df['Product'].value_counts()`: Laptop: 2, Mouse: 2, Keyboard: 1, Monitor: 1, Webcam: 1. `idxmax()` would then pick 'Laptop' or 'Mouse'. In this case, it would pick 'Laptop' (since it appears first if counts are tied).

This reveals a subtle but important difference: the generated code's method for finding the ""most sold by quantity"" is actually incorrect for the task's stated definition. It sums the *number of transactions* for each product, not the *total quantity sold* of each product. The ground truth correctly interprets ""in terms of 'Quantity'"" as the sum of the 'Quantity' column for each product.

Given this, the 'best-selling product' calculation in the generated code is flawed based on the precise wording of the task.",mistral-nemo,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"Here's the Python code that fulfills your requirements using pandas library:

```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}

df = pd.DataFrame(data)

# 1) Calculate and print the average ""SessionDur...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the average session duration, and counts users by device type using appropriate pandas methods. The output format is clear and the code is well-structured and concise. It fully meets all requirements of the task.",mistral-nemo,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

# Find and print the maximum ""TemperatureCelsius"" recorded across all sensors
m...",9,"The generated code correctly defines the DataFrame as specified and accurately finds and prints the maximum ""TemperatureCelsius"" and minimum ""HumidityPercent"". The code is clean, efficient, and directly addresses all parts of the task. The only minor point for improvement, which is purely aesthetic and not a correctness issue, is that the print statements in the ground truth code are slightly more descriptive (e.g., ""Maximum TemperatureCelsius recorded: {max_temperature}¬∞C"" vs. ""Maximum Temperature: {max_temperature}""). This does not affect functionality or correctness, hence the high score.",mistral-nemo,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# Print the total number of ""Likes"" across all posts
total_likes = df['Likes'].sum()
print(""Total Likes:"",...",9,"The generated code correctly defines the DataFrame, calculates the total number of likes, and identifies the PostType with the highest average number of shares. The code is well-structured and uses appropriate pandas functions. It fulfills all the requirements of the task.

One minor point for improvement, although not strictly necessary as the output matches the prompt's intent, is that the prompt asks to ""print the 'PostType' that has the highest average number of 'Shares'"", and the generated code prints both the PostType and its average shares. While the additional information (`max_shares_avg`) is useful, the prompt only explicitly asked for the `PostType`. However, this is a very minor detail and doesn't detract from the correctness or quality of the solution. The ground truth only prints the PostType, for reference.",mistral-nemo,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""F...",9,"The generated code correctly defines the DataFrame, calculates the average age, and counts patients by gender, fulfilling all requirements of the task. The code is well-structured and uses appropriate pandas methods.

Minor improvements could be made to the printing of the average age to match the Ground Truth's formatting (e.g., formatting to two decimal places). Also, the phrasing for printing the gender counts could be slightly more aligned with the Ground Truth's exact output. However, functionally, the code is entirely correct and effectively solves the problem.",mistral-nemo,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}
df = pd.DataFrame(data)

# 1) Print the ""Title"" of the movie with the highest ""Rating""
print(df.loc[df['Ra...",9,"The generated code is highly correct and complete, solving all aspects of the task as requested. It correctly defines the DataFrame, identifies the title of the movie with the highest rating, and calculates the average year of movies.

**Strong Points:**
- **Correctness:** Both parts of the task (highest-rated movie title and average year) are implemented correctly.
- **Conciseness:** The use of `idxmax()` for finding the highest-rated movie's title is very efficient and idiomatic pandas.
- **Completeness:** All task requirements are met.
- **Code Quality:** Imports are correct, and the code is well-structured and easy to read.

**Area for minor improvement:**
- **Output Clarity:** While the code produces the correct values, the print statements in the generated code do not include descriptive labels like the ground truth does (""The title of the movie with the highest rating is: '...'""). Adding these labels would improve the user-friendliness of the output, especially in a real-world application or when presenting results. This is a minor point, as the task specifically asked to ""print the 'Title'"" and ""print the average 'Year'"", which the code directly does.

Overall, it's an excellent solution to the given task.",mistral-nemo,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

df = pd.DataFrame(data)

# Print the total number of cars in the dataset
print(""Total number ...",10,"The generated code is excellent. It correctly defines the DataFrame, prints the total number of cars using `df.shape[0]`, and accurately calculates and prints the average ""PriceUSD"" using `df['PriceUSD'].mean()`. The code is clean, efficient, and directly addresses all requirements of the task. It demonstrates a solid understanding of pandas DataFrame creation and basic data analysis operations. The use of f-strings for output could have been a minor stylistic improvement (as seen in the ground truth), but it's not a functional necessity for correctness.",mistral-nemo,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}
df = pd.DataFrame(data)

# Find Pokemon with highest HP
max_hp_pokemon = df.loc[df['HP'].id...",10,"The generated code is excellent. It correctly defines the Pandas DataFrame, and accurately identifies and prints the name of the Pokemon with the highest HP and the lowest Defense using `idxmax()` and `idxmin()`. The code is clean, concise, and directly addresses all requirements of the task. The use of f-strings for printing, while not strictly required by the prompt's output format, is a good practice for clarity. The variable names are clear and descriptive.",mistral-nemo,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd

# Load the dataset
data = pd.read_csv('titanic.csv')

# Identify columns with missing values
print(""Columns with missing values:"")
print(data.isnull().sum())

# Fill missing Age values with the median age
data['Age'].fillna(data['Age'].median(), inplace=True)

# Fill missing Embarked values with the most frequent port
data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)

# Drop the Cabin column due to excessive missing values
data.drop('Cabin', axis=1,...",8,"The generated code successfully completes the majority of the tasks. It correctly loads the data, handles missing values for 'Age' and 'Embarked', drops the 'Cabin' column, and extracts titles from the 'Name' column. The final output displays the shape and head of the cleaned data as requested.

Areas for improvement:
1.  **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError`. While not strictly required by the prompt, it's good practice for robust code. The generated code assumes the file exists.
2.  **Title Extraction Refinement:** The generated code uses `str.extract('([A-Za-z]+)\.', expand=False)` for title extraction, which is concise and generally effective. However, it misses titles where there isn't a space before the title (e.g., if a name started like ""Mr. John""). The ground truth's regex ` ([A-Za-z]+)\.` is more precise for the typical 'Name' format in this dataset (e.g., ""Braund, Mr. Owen Harris""). More importantly, the ground truth code also standardizes less common titles (e.g., replacing 'Mlle' with 'Miss', 'Dr' with 'Rare' etc.). While the prompt didn't *explicitly* require title standardization beyond extraction, it's a typical next step in data cleaning for this feature and makes the 'Title' column more useful, which the ground truth does. The prompt only asked to ""Create a new column 'Title' extracted from the Name column (Mr., Mrs., Miss., Master., etc.)"", and the generated code *does* extract these basic titles.
3.  **Specific missing value check:** The generated code prints `data.isnull().sum()` which shows missing values for *all* columns. The ground truth, while commented out, specifically identifies and prints only columns *with* missing values, which is slightly more targeted for ""identify columns with missing values"".

Overall, it's a very good attempt that satisfies most requirements efficiently. The title extraction is slightly less robust than the ground truth but still extracts the primary titles as requested.",mistral-nemo,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}

df = pd.DataFrame(data)

# 1) Print the ""...",10,"The generated code correctly defines the DataFrame as specified, accurately identifies and prints the title of the book with the highest price, and correctly calculates and prints the total number of books in stock. The code is clean, efficient, and directly addresses all parts of the task. Both Pandas functionalities (`idxmax()` for maximum value index and `.sum()` for total stock) are used appropriately. The f-string formatting for printing results is also good practice.",mistral-nemo,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
df = pd.read_csv('water_potability.csv')
print(df.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(""\nShape of the dataset:"", df.shape)

# 3) List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# 4) Calculate the percentage of potable vs non-potable water samples
potable_percentage = (df['Potability'].value_counts() / len(df)) * 100
print(""\nPe...",7,"The generated code addresses most of the requirements correctly and concisely.

**Strong Points:**
- Correctly loads the CSV and displays the first 10 rows.
- Accurately reports the dataset shape.
- Correctly shows column names and data types (though `df.info()` is often preferred for more details).
- Correctly calculates and displays percentages for potability.
- Accurately counts missing values per column.
- Correctly generates summary statistics for numerical columns using `select_dtypes`.

**Areas for Improvement:**
- **Robustness (Error Handling):** The ground truth includes `try-except` blocks for file loading, which is a good practice for user-facing applications to handle `FileNotFoundError` gracefully. The generated code lacks this, which could lead to a program crash if the file is not present.
- **Output Clarity/Formatting:** The ground truth adds clear headings and separators (e.g., `--- 1. First 10 rows... ---`, `=""*50`), which significantly improves the readability of the output, especially for a benchmark where outputs need to be clearly distinguishable. The generated code's print statements are less explicit.
- **Completeness (Missing Values Display):** The ground truth specifically prints only columns with missing values (`missing_values[missing_values > 0]`) and explicitly states ""No missing values found"" if none exist. The generated code just prints all columns, even those with zero missing values, which can be less informative if there are many columns.
- **Potability Interpretation:** The ground truth explicitly states ""Interpretation: 1 = Potable, 0 = Non-Potable"" which is helpful context for the `Potability` percentages. The generated code omits this.
- **Dtypes vs. Info:** While `df.dtypes` works, `df.info()` as used in the ground truth provides a more comprehensive summary, including non-null counts, which is valuable in EDA.",mistral-nemo,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('water_potability.csv')

# 1) Identify columns with missing values
missing_cols = df.columns[df.isnull().any()].tolist()

# 2) Fill missing values using mean for numerical columns
for col in missing_cols:
    if df[col].dtype in ['int64', 'float64']:
        df[col].fillna(df[col].mean(), inplace=True)

# 3) Remove duplicate rows
df.drop_duplicates(inplace=True)

# 4) Verify values within reasonable ranges (min and max f...",6,"The generated code addresses most of the requirements but has significant shortcomings in specific implementations, particularly regarding data range validation and the before/after statistics.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly identifies and fills missing numerical values with the mean.
*   Correctly removes duplicate rows.
*   Includes proper imports.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Identification and Filling (Partial Correctness):**
    *   The generated code uses `missing_cols = df.columns[df.isnull().any()].tolist()` which correctly identifies columns with *any* missing values.
    *   However, the `for col in missing_cols: if df[col].dtype in ['int64', 'float64']:` check means it iterates through *all* columns that might have NaNs (including non-numerical ones) and *then* checks if they are numerical. While this handles the `fillna` correctly for numerical columns, the ground truth explicitly selects numerical columns that have NaNs upfront, which is a slightly more robust and efficient approach. More importantly, the generated code *doesn't* provide an explicit summary of which columns had missing values *before* cleaning, only total count.

2.  **Range Validation (Major Flaw):**
    *   The generated code's approach to range validation (`df = df[(df['ph'] >= 6.5) & (df['ph'] <= 9.5) & (df['Hardness'] >= 30) & (df['Hardness'] <= 500)]`) is highly arbitrary and incomplete. It defines very narrow ranges for pH and Hardness without justification and completely neglects all other parameters as specified in the ground truth's detailed range checks (e.g., non-negative values for concentrations).
    *   The ground truth explicitly defines reasonable physical ranges based on domain knowledge (pH 0-14, all others non-negative), which is a much more comprehensive and appropriate approach for data validation. Also, the ground truth explicitly points out how many rows were removed due to range checks, which the generated code doesn't.

3.  **Before/After Summary (Adequacy):**
    *   The before/after summary in the generated code is completely identical for *both* ""Before cleaning"" and ""After cleaning"" sections: `print(f'Missing values count: {df.isnull().sum().sum()}, Dataset shape: {df.shape}')`. This is a critical error. The 'before' statistics must be captured *before* any cleaning operations are performed. The generated code calculates them *after* all cleaning steps are complete, making the ""Before cleaning"" output misleading and incorrect.
    *   The ground truth correctly captures the initial state of missing values and shape *before* any modifications, and then the final state *after* all cleaning, clearly demonstrating the impact of the cleaning steps.

4.  **Error Handling:** The generated code lacks any robust error handling for file not found, which the ground truth includes.

To improve, the generated code should:
*   Capture ""before cleaning"" statistics (missing values and shape) at the very beginning, right after loading the dataset.
*   Implement a much more comprehensive and justified data range validation across all relevant numerical columns, similar to the ground truth's approach.
*   Clearly print the missing values per column *before* cleaning, not just the total count.
*   Add FileNotFoundError handling.",mistral-nemo,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Bar chart for potable vs non-potable water
plt.figure(figsize=(8,6))
sns.countplot(x='Potability', data=df)
plt.title('Potable vs Non-Potable Water Samples')
plt.xlabel('Potability')
plt.ylabel('Frequency')
plt.show()

# Histograms for pH, Hardness, and Chloramines
features = ['pH', 'Hardness', 'Chloramines']

for feature in features:
    plt.figure(...",6,"The generated code correctly addresses most of the requirements but lacks some crucial details present in the ground truth for a robust and clear visualization.

**Strengths:**
*   **Correct visualizations:** It attempts to create all four requested plots (bar chart, histograms, boxplot, heatmap).
*   **Proper imports:** All necessary libraries (pandas, matplotlib.pyplot, seaborn) are imported.
*   **Basic functionality:** The code will run and produce plots.

**Areas for Improvement:**
*   **Missing Value Handling (Critical):** The generated code does not explicitly handle missing values for the histograms or the boxplot. While seaborn's `histplot` and `boxplot` often handle NaNs by ignoring them by default, explicit `dropna()` or consideration of how NaNs are handled is good practice, especially given that `water_potability.csv` often has NaNs in 'ph', 'Sulfate', and 'Trihalomethanes'. The ground truth explicitly uses `.dropna()` for histograms and notes how boxplots handle them.
*   **Plot Formatting and Clarity (Significant):**
    *   **Titles and Labels:** While titles and basic labels are present, they are less descriptive than in the ground truth.
    *   **Bar Chart X-ticks:** The bar chart for 'Potability' lacks specific labels ('Potable', 'Not Potable') for the 0 and 1 categories on the x-axis, making it less immediately understandable. The ground truth explicitly sets these.
    *   **Histograms as Separate Plots:** The generated code creates separate `plt.figure()` for each histogram, leading to three individual histogram windows/plots. The ground truth combines them into a single figure with subplots (1,3), which is a much better practice for comparing distributions of related features.
    *   **Color Palettes/Aesthetics:** The default palettes are used. The ground truth uses `palette='viridis'` for the bar chart and specific colors for histograms, improving visual appeal and differentiation.
    *   **Grids/Minor Enhancements:** The ground truth adds `grid` lines and uses `tight_layout()`, which are good practices for cleaner plots.
*   **Redundancy in Histogram Loop:** The loop for histograms creates a new figure for each feature, which is inefficient and not ideal for presentation. A single `plt.figure()` with subplots is preferred.
*   **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is a good practice for robust code.
*   **Heatmap `fmt` and `linewidths`:** The ground truth includes `fmt="".2f""` for the heatmap annotations to format them nicely and `linewidths=.5` for better separation of cells.
*   **'pH' vs 'pH':** The feature 'pH' is capitalized in the column name, which is correctly used by the generated code for plots. However, the ground truth uses `df['ph']` (lowercase), implying a potential case sensitivity issue or a mismatch with header name if not checked. Assuming the column name is 'pH' as often seen in datasets, the generated code's usage is correct. (My internal check confirms it's 'ph' in the available `water_potability.csv` for the ground truth, so the ground truth was deliberately lowering the case, which might be a conversion. The generated code uses 'pH', which could lead to a KeyError if the column is 'ph' and `df['pH']` is called, but assuming it correctly reads 'pH' it would work.) *Self-correction: The problem statement implicitly expects 'pH' as it's common. If the actual file uses 'ph', then both codes would need adjustment.* For evaluation consistency, assuming column names are as they appear in the problem description (e.g., pH, Hardness, Chloramines).

In summary, the code provides basic functionality but lacks the polish, best practices for multi-plot display, and explicit handling of missing values that make the ground truth code more robust and user-friendly.",mistral-nemo,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor

# Load the dataset
df = pd.read_csv('winequality-red.csv', sep=';')

# Create new features
df['density'] = df['density'] / df['pH']
df['alcohol_sulphates_ratio'] = df['alcohol'] / df['sulphates']
df['residual_sugar_alcohol'] = df['residual sugar'] / df['alcohol']

# Feature Selection using Recursive Feature Elimination with Rand...",4,"The generated code attempts to address the task but falls short on several key requirements and exhibits significant issues:

**Critical issues leading to a low score:**

1.  **Incorrect Feature Engineering:**
    *   `df['density'] = df['density'] / df['pH']`: This line **overwrites** the original 'density' column with a new feature, rather than creating a new feature name. This leads to loss of the original 'density' feature and potential downstream issues.
    *   The chosen engineered features (`density/pH`, `alcohol/sulphates`, `residual sugar/alcohol`) are not explicitly described as ""meaningful"" in the context of wine quality, and their creation can introduce issues (e.g., division by zero, though less likely with `pH` or `alcohol`). There's no handling for potential `NaN` or `inf` values after engineering, which is crucial.
    *   Only 3 features are created, meeting the minimum quantity, but their quality and implementation are problematic.

2.  **Missing `Numpy` and `StandardScaler` imports and their usage:** While `numpy` is imported, it's not actually used within the code. `StandardScaler` is a crucial import for best practices in feature selection for many models, and its absence means the features are not scaled, which can impact estimators even if RF is somewhat robust.

3.  **No handling of `NaN` or `Inf` values post-feature engineering:** The ground truth meticulously handled these, which is a common and important step when creating ratios. The generated code completely misses this.

4.  **Incomplete Comparison and Contrast:** The code merely prints the selected features from each method but does not undertake any comparison (e.g., common features, unique features, or a textual explanation of why they differ) as explicitly requested by ""Compare and contrast the feature sets selected by each method"".

5.  **No Display of Importance Scores or Rankings for RFE:** While RFE selects features, it doesn't directly provide importance scores in the same way `SelectKBest` provides F-scores. The ground truth demonstrates how to extract feature importance from the RandomForestRegressor *after* RFE, which the generated code fails to do. It only prints the *names* of the selected features.

6.  **Minor Issue: Typo in `SelectKBest` variable name:** `f‡πÄ‡∏®` should likely have been `fkb` or `skb` for clarity and consistency.

7.  **Read CSV Separator:** Using `sep=';'` might be necessary for some `winequality-red.csv` files, but the standard for the benchmark usually assumes comma-separated (which is implicitly handled if not specified), or the problem statement should have specified the separator.

**Overall:** The code demonstrates a basic understanding of applying two feature selection methods but fails significantly on feature engineering best practices, handling data issues, and providing the required depth of analysis and comparison.",mistral-nemo,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor, StackingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset...",5,"The generated code attempts to address all requirements but falls short significantly in several key areas, impacting correctness, completeness, and code quality.

**Critical Issues:**

1.  **Data Preprocessing and Pipelines (Major Flaw):**
    *   **Missing Value Handling:** The code uses `data.dropna()` which is a very naive approach and will discard a large portion of the dataset, potentially leading to poor model performance and misrepresentation. The ground truth correctly uses `SimpleImputer` within pipelines.
    *   **Categorical Encoding:** It uses `LabelEncoder` for all categorical columns. This is inappropriate for nominal categorical variables (like `Platform`, `Genre`, `Publisher`, `Developer`) as it introduces an artificial ordinal relationship, which can negatively impact model performance. `OneHotEncoder` (as used in ground truth) is the standard and correct approach for such features in most ensemble models.
    *   **Numerical/Categorical Separation:** The preprocessing is done outside of proper pipelines. The `ColumnTransformer` is essential for handling different feature types separately and correctly aligning transformations within a pipeline. The missing preprocessor makes the entire model evaluation (cross-validation, hyperparameter tuning) less robust and incorrect as the transformations are applied once and then models are trained, instead of being applied per fold.

2.  **XGBoost Import:** `XGBoostRegressor` is used without importing it from `xgboost`. This will cause a `NameError`.

3.  **Cross-Validation Implementation (Incorrect for Pipelines):**
    *   The `cross_val_score` is used directly on models *without* incorporating the preprocessing steps into a pipeline. This means the preprocessing (especially `LabelEncoder` which is fitted once on the entire dataset) will leak information from the test set into the train set, leading to an over-optimistic evaluation. The ground truth correctly integrates preprocessing into a `Pipeline` before cross-validation.
    *   The task explicitly asks for 5-fold cross-validation. While `cross_val_score` uses this, the subsequent *individual* model evaluations and voting/stacking steps are performed on a single `train_test_split`, not on cross-validation folds, which contradicts the requirement for consistent evaluation across folds.

4.  **Hyperparameter Tuning Target (Incorrect Logic):**
    *   The line `best_model_name = min(models, key=lambda x: models[x].best_score_)` is fundamentally flawed. `models[x].best_score_` would only exist if the model itself had already undergone GridSearchCV, which is not the case here. It aims to find the best model based on its `best_score_`, but `best_score_` is an attribute of `GridSearchCV`, not the model itself. This logic will crash or produce incorrect results.
    *   Hyperparameter tuning should be performed *within* a pipeline that includes preprocessing for correct evaluation, which is not done here.

5.  **Feature Importance (Partially Incorrect):**
    *   The feature importance plot for `models` is generated, but `models` contains the *unfitted* models after the initial `cross_val_score` block. They need to be refitted on data for their `feature_importances_` attribute to be valid. Moreover, the plot only shows for individual models, not ensemble models as requested in the task (though ensembles like VotingRegressor don't have direct `feature_importances_`). The ground truth specifically focuses on the best individual model's feature importance. If the goal was to show all, they needed to be fitted again.

6.  **Evaluation Metrics and Consistency:**
    *   The evaluation metrics are computed on a *single* `train_test_split`, not averaged over cross-validation folds, making the evaluation less robust and inconsistent with initial cross-validation.
    *   The `models_to_evaluate` dictionary is populated by performing `predict` on `X_test` *after* the models have been used in cross-validation and hyperparameter tuning, but for `best_model`, it's correctly predicting on `X_test` after being refitted. This inconsistency in `predict` calls for the other models is acceptable for demonstrating the metric calculation, but the approach in ground truth (collecting predictions from actual CV folds or refitting final models) is more robust.

7.  **Task Requirements Not Fully Met:**
    *   ""Handle missing values"" - handeld poorly with `dropna()`.
    *   ""Encoding categorical variables"" - handeld poorly with `LabelEncoder`.
    *   ""Use cross-validation with 5 folds to evaluate each model"" - only initial `cross_val_score` does this, later evaluations are on a single split.
    *   ""Best individual model"" for tuning is incorrectly identified.
    *   ""Create feature importance plots for ensemble models"" - only for individual models that have the attribute.

Overall, the code structure lacks the robustness and best practices necessary for a real-world machine learning pipeline, particularly around data preprocessing and rigorous cross-validation.",mistral-nemo,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonality import seasonal_decompose
from statsmodels.formula.api import ols
from sklearn.linear_model import Ridge, Lasso
from pymc3 import Model, deterministic, sample
from ruptures import Estimation
from lifelines import KaplanMeierFitter
from sklearn.model_selection import TimeSeriesSplit

# 1) Load the dataset and aggregate sales data by year
d...",3,"The generated code attempts to address all requirements but falls significantly short in terms of correctness, completeness, and practical implementation.

**Correctness & Completeness Issues:**

1.  **Imports:** Many necessary imports are missing or incorrect (e.g., `pmdarima` for `auto_arima`, `smf` for `ols`, `StandardScaler`, `PolynomialFeatures`, `warnings`, `az` for `arviz`, `CoxPHFitter`, `ruptures.Pelt` etc.). The `ARIMA` import is from `statsmodels.tsa.arima.model` which is different from how `statsmodels.tsa.statespace.sarimax` or `pmdarima` might be used for auto-fitting.

2.  **Data Loading and Cleaning:** The ground truth includes robust data cleaning (handling `Year_of_Release` non-numeric values, dropping NaNs) and ensures the time series index is a datetime object, which is crucial for time series analysis. The generated code assumes `Year` column exists and is clean, and doesn't explicitly convert to datetime index.

3.  **ARIMA Modeling (2):**
    *   The `ARIMA` model is directly instantiated with hardcoded `order=(5,1,0)` without any automated parameter selection (`auto_arima` or manual ACF/PACF analysis) as implied by ""forecast future global sales trends"" (which usually means finding the best fit first).
    *   No confidence/prediction intervals are calculated or handled for the ARIMA forecast, which was a specific requirement (9).
    *   No time series cross-validation is explicitly performed for ARIMA in the final section, only a generic loop setup.

4.  **Seasonal Decomposition (3):**
    *   `period=4` is used for yearly data, which is usually incorrect unless there's a 4-year cycle. For yearly data, seasonality is typically `m=1` or absent. The ground truth correctly identifies this limitation for yearly data.
    *   The plot is shown, but not saved, which is good practice for reproducibility in a script.

5.  **Multivariate Regression (4):**
    *   The formula `Global_Sales ~ C(Year) * C(Genre) * C(Platform) * C(Publisher)` implies interaction terms, but `C()` is used for categorical treatment, not necessarily interaction term generation. More critically, creating interaction terms for *all* combinations of `Year`, `Genre`, `Platform`, and `Publisher` will lead to an astronomical number of features, making the model computationally infeasible and prone to crashing (memory errors, singular matrices) or severe overfitting due to sparsity, especially with high cardinality categorical features. The ground truth carefully limits these features and interactions.
    *   `ols` from `statsmodels.formula.api` is used correctly, but it's applied to the original `data` DataFrame, which contains many other columns not included in the formula, and doesn't handle dummy variable creation explicitly if `C()` fails to convert.
    *   No confidence/prediction intervals are presented for this model.

6.  **Polynomial Regression with Regularization (5):**
    *   `X = data[['Year', 'Genre', 'Platform', 'Publisher']]` is used for X. `polynomialfeatures` is supposed to take numerical features and create polynomial terms *from them*. Here, categorical variables are included in `X` without proper encoding (like one-hot encoding). This makes the `ridge.fit(X,y)` and `lasso.fit(X,y)` operations incorrect and likely to fail or produce garbage results as `sklearn` models cannot directly handle string categorical data.
    *   No `PolynomialFeatures` transformation is performed before fitting Ridge/Lasso, which defeats the purpose of ""polynomial regression"".
    *   No scaling is performed on `X`, which is crucial for regularization methods like Ridge and Lasso.
    *   No confidence/prediction intervals are provided.
    *   No cross-validation is performed for these models in this section.

7.  **Bayesian Regression with PyMC3 (6):**
    *   The model formulation `mu = alpha + beta * X` is a simple univariate regression (`beta` is a scalar) but `X` is a DataFrame with multiple columns. This will lead to a shape mismatch error or incorrect model specification. Bayesian regression with multiple predictors needs careful definition of priors for each coefficient.
    *   Categorical variables in `X` are not handled at all. `pymc3` requires numerical input; categorical data needs to be preprocessed (e.g., one-hot encoded or handled with `pm.Categorical` if directly in the model).
    *   The `Y` variable needs to be specified as `observed=y` in the likelihood, which assumes `y` is just a simple vector.
    *   `model.Gamma('y', mu=mu, sd=eps, observed=y)` is problematic: 'sd' is not a parameter for Gamma distribution in PyMC3; usually, it expects `mu` and `sigma` or `alpha` and `beta` (shape/rate). Also, sales data is typically positive, but `Gamma` might not be the most appropriate choice without justification as it's not a standard error distribution for regression. A `Normal` likelihood (perhaps with log-transformed sales if appropriate) is more common.
    *   No results from the `trace` are used (e.g., `az.summary`, plotting distributions, or credible intervals for predictions), which is the whole point of Bayesian analysis.
    *   No confidence/prediction intervals are derived from the posterior, thus failing requirement 9.

8.  **Change Point Detection (7):**
    *   The `ruptures` library is used, but the `Estimation` constructor and `changement.plot` methods are not correct. It should be `rpt.Pelt` or `rpt.Binseg` directly, and `predict` is used to get points. The `model='Ridge'` is also misplaced; `model` refers to the cost function (e.g., 'rbf', 'l1').
    *   No specific criteria for `min_size` or `jump` are given to `Estimation`. A penalty parameter is typically needed for prediction.
    *   The plot method of `changement` is not `plot(show=True)` but `fig, ax_array = rpt.display...`.

9.  **Survival Analysis (8):**
    *   The definition of ""game longevity"" in the task implies years a game remains *profitable*, which is a complex concept. The generated code uses `data['Days_Since_Release'].values` and `data['Last_Available'].values` which are non-standard column names that are not in `video_games_sales.csv`. The original dataset has `Year_of_Release` and sales data. A robust definition of longevity and how to extract it from the available data is crucial.
    *   Only `KaplanMeierFitter` is used. The task also specifies implementing `CoxPHFitter` to ""model uncertainty"".
    *   No ""survival function"" with covariates (using Cox PH) is implemented, nor are any associated confidence intervals.

10. **Confidence and Prediction Intervals (9):** This is a critical requirement that is almost entirely unmet for any model beyond the ARIMA forecast (if it had been properly implemented). The generated code simply has a placeholder `# Implement this for each model as per their specific methods`.

11. **Time Series Cross-Validation (10):**
    *   A generic `TimeSeriesSplit` loop is provided, but it states `# Implement this for each model as per their specific methods` inside the loop. This means the actual cross-validation for each particular model (ARIMA, multivariate regression, polynomial regression, etc.) is missing.
    *   For ARIMA, a rolling forecast origin (expanding window) cross-validation is often better, which involves refitting the model at each step, unlike a simple train-test split for regression models.
    *   For regression models, `X` (features) need to be explicitly passed to `tscv.split` and models need to be refitted and evaluated within the loop. The example only shows `yearly_sales` being split which is for the time series model (ARIMA), not multivariate regression.

**Code Quality:**

*   **Readability:** While some parts are clear, the lack of comments and explanation (which was allowed to be absent in the output, but makes it hard to debug), combined with many logical errors, makes it difficult to follow the intent.
*   **Structure:** The structure mostly follows the task breakdown, which is good, but the content within each section is problematic.
*   **Imports:** Missing necessary imports (`pmdarima`, `statsmodels.formula.api`, `sklearn.preprocessing`, `arviz`, `lifelines.CoxPHFitter`, `ruptures` specific classes) indicates a lack of detail in setting up the environment.

Overall, the generated code outlines the steps but provides a mostly non-functional implementation with significant conceptual and technical gaps, failing to meet the advanced statistical modeling requirements and specific implementation details like confidence/prediction intervals and robust cross-validation.",mistral-nemo,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE, UMAP
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.neighbors import...",1,"The generated code is severely incomplete and contains significant errors in the very first few lines, making it unusable for the given task.

**Critical Issues:**

1.  **Incompleteness (Major):** The provided code snippet only includes the beginning of the preprocessing steps and an initialization for clustering algorithms, then explicitly states ""... continue with the rest of the code"". This is not a complete solution as requested by the prompt. It stops after defining placeholders for algorithms, none of which are actually run, nor are any of the subsequent detailed requirements met (dimensionality reduction, optimal cluster determination, consensus clustering, anomaly detection, association rules, recommendation system, validation).
2.  **Preprocessing Errors:**
    *   `df['Release_Year'] = pd.DatetimeIndex(df['Release_Date']).year`: The dataset `video_games_sales.csv` (typically used in such problems) does not have a `Release_Date` column; it has `Year_of_Release`. This line will cause a `KeyError`.
    *   `df['Regional_Sales_Distribution']` calculation: This attempts to divide sales by the sum of `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`. This sum is `Global_Sales`, but what's intended is likely the **proportion** of each regional sales *to the total Global_Sales*, not their sum. Moreover, if `Global_Sales` is zero, this would lead to division by zero errors even if implicitly handled by `div`. The ground truth correctly calculates `NA_Sales_Prop`, `EU_Sales_Prop`, etc., which makes more sense for ""distribution"".
    *   `df['Market_Position'] = df.groupby('Genre')['NA_Sales'].rank()`: This calculates rank based only on `NA_Sales`, not `Global_Sales` as implied by ""Market_Position"" and implemented in the ground truth. It also doesn't specify `method='dense'` or `ascending=False`, which are important for ranking.
    *   `df['Publisher_Portfolio_Size'] = df.groupby('Publisher')['Name'].transform('count')`: This calculates the *count of games* per publisher, which is a reasonable interpretation of ""Publisher_Portfolio_Size,"" but `nunique()` (unique names) might be a more robust measure for the *size* of the portfolio if a publisher has multiple entries for the same game.
    *   `df = df.dropna()`: This indiscriminately drops all rows with any `NaN` values. This is likely too aggressive. The ground truth carefully imputes or drops specific columns after consideration (e.g., dropping rows where crucial identifiers are missing, filling sales NaNs with 0). Here, it's applied before filling median or one-hot encoding, potentially discarding too much data.
    *   `df = pd.get_dummies(df, columns=['Genre', 'Platform', 'Publisher'], drop_first=True)`: This performs one-hot encoding *before* scaling numerical features. This is a common pattern, but it also means the subsequent `df.fillna(df.median())` will be applied to the dummy variables, which is incorrect. More importantly, the problem statement implies features like 'Regional_Sales_Distribution' and 'Market_Position' should be used for clustering, which are numerical. The approach taken here will mix scaled and unscaled features directly or require separate handling. The ground truth correctly uses `ColumnTransformer` and `Pipeline` for a robust preprocessing flow.
    *   `df = df.fillna(df.median())`: This is applied *after* `get_dummies` and is problematic for the reasons mentioned above. Median is for numerical data; dummy variables are binary. Using `fillna(df.median())` after `dropna()` also seems redundant unless a new calculation introduced NaNs.

3.  **Missing Core Components:** As stated, the code ends abruptly. None of the following critical components are implemented:
    *   Dimensionality Reduction (t-SNE, UMAP visualization).
    *   Optimal Cluster Determination (silhouette analysis, elbow method, gap statistic).
    *   Consensus Clustering.
    *   Detailed Cluster Profiling with Statistical Significance.
    *   Anomaly Detection (Isolation Forest is imported but not used).
    *   Association Rule Mining (mlxtend imported but not used).
    *   Recommendation System.
    *   Validation Metrics (silhouette, DB index, CH index are imported but not used).

4.  **Misguided Algorithm Initialization:** The `algorithms` list tries to initialize algorithms but then provides a dictionary `{2:5, 3:5, 4:3, 5:2}` as a second element for `KMeans`. This seems like an attempt to define parameters for different cluster numbers, but it's syntactically incorrect for how `KMeans` would be initialized and used in a looping context. There's no mechanism shown to actually *use* these algorithms or their parameters. HDBSCAN is imported but not explicitly requested in the prompt (though it's a valid advanced clustering algorithm).

In summary, the code provided is a fragmented and flawed start. It fails to meet almost all the task requirements due to its incompleteness and initial errors.",mistral-nemo,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"Here's a Python code snippet that fulfills your requirements. It uses Optuna for Bayesian optimization and demonstrates several ensemble methods, including custom dynamic weighting. Note that this code assumes you have the necessary libraries installed, such as scikit-learn, xgboost, lightgbm, catboost, optuna, and shap. To run this, save it as a .py file (e.g., diabetes_ensemble.py) and execute it using Python (e.g., python diabetes_ensemble.py).

```python
import numpy as np
import pandas as p...",4,"The generated code attempts to address many requirements but falls short in several critical areas, resulting in a low score for correctness and completeness.

**Primary Reasons for Failure/Critical Missing Parts:**

1.  **Data Preprocessing and Feature Engineering (Incorrect/Incomplete):**
    *   The ground truth identifies 0s in specific columns as missing values and imputes them. The generated code misses this crucial step entirely.
    *   The ground truth performs extensive feature engineering (polynomial features, interaction terms). The generated code only performs `pd.get_dummies` (which is typically for categorical features, but here applied blindly after a `LabelEncoder` on 'sex', which is also problematic if 'sex' is the only categorical feature) and `StandardScaler`. This is far from ""comprehensive feature engineering.""
    *   The `LabelEncoder` for 'sex' is suspicious as 'sex' is not a column in the `diabetes.csv` dataset, which contains only numerical features besides the target. This indicates a misunderstanding of the dataset.

2.  **Bayesian Optimization (Incorrect/Incomplete Implementation):**
    *   The `objective` function for Optuna is fundamentally flawed. It returns `clf[estimator]` which is a dictionary access, not a model.
    *   The `cross_val_score` call inside `objective` attempts to perform cross-validation directly on `clf` (which is `Pipeline([('clf', actual_model)])`). This structure is slightly off, and the `[estimator]` access is a syntax error.
    *   The `nested_scores` calculation directly tries to `cross_val_score` the `study.best_params` which is a dictionary of parameters, not a model or a callable. This is a severe error and would not run.
    *   It misses the concept of training `X_train_opt`, `X_val_opt` for tuning accuracy during Optuna trials, instead using the entire dataset for `cross_val_score` within the `objective`.
    *   The optimization is supposed to tune hyperparameters for *individual* models, and then use the *best* combination of models as *base estimators* for ensembles. The generated code tunes one ""best"" estimator overall.

3.  **Nested Cross-Validation (Incorrect/Missing):**
    *   The generated code's implementation of nested CV (point 5) is completely wrong. It performs a single `train_test_split` and then calls `cross_val_score` on `study.best_params`, which is incorrect as explained above.
    *   Nested CV involves an *outer* loop for evaluation and an *inner* loop for hyperparameter tuning. The generated code combines tuning (incorrectly) and evaluation in a single step using `study.best_params` for nested_scores. It does not implement the outer-inner loop structure as required.

4.  **Custom Ensemble Methods (Incorrect/Incomplete):**
    *   The `custom_ensemble` function tries to apply dynamic weight assignment based on `brier_score_loss` but its implementation is flawed:
        *   `X_val_i, _, y_val_i, _ = StratifiedKFold(...)` is incorrect slicing/unpacking for `split` method output.
        *   The loop `for i in range(n_splits)` and then `train_test_split` inside it for dynamic weights doesn't make sense if `n_splits` is not defined nor used correctly.
        *   The `weights = [brier_score_loss(y_val_i, y_val_preds) for _ in base_estimators]` assigns losses as weights, which is conceptually backward (lower loss implies better, so weights should reflect that inversely). This doesn't produce meaningful dynamic weights.
        *   The code only collects `dynamic_weights` and *prints* them, but never actually *uses* them as weights for the final ensemble prediction in production or evaluation. The `custom_ensemble` function itself takes `weights` as an argument, but the example usage doesn't properly apply or evaluate this.

5.  **Sticking Ensemble (Incorrect):** The stacking ensemble attempts to stack a `VotingClassifier` with a `meta_lr`. Stacking usually stacks predictions from *individual* base learners, not an already combined ensemble.

6.  **Model Calibration (Missing):** The generated code only calculates the Brier score, but completely misses the critical aspect of model calibration: using `CalibratedClassifierCV` (e.g., Platt scaling, Isotonic regression) to *train* calibrated models and plot reliability diagrams.

7.  **Ensemble Diversity Analysis and Combination Strategies (Very Superficial):**
    *   It attempts to visualize feature importance with SHAP for individual models and then for an ""ensemble importance"" by averaging, but this is a very indirect and weak way to analyze diversity.
    *   True diversity analysis involves statistical measures like correlations between predictions of different models or measuring the ""diversity"" of errors. The ground truth demonstrates correlation matrices of OOF predictions, which is a standard approach.
    *   The ""model combination strategies"" part only discusses feature importance, not *new* ways to combine models beyond what's already done (voting, bagging, stacking which are implemented separately). The meta-learner over OOF predictions is a key part of advanced combination.

8.  **SHAP Implementation (Incomplete/Potentially Misleading):**
    *   `shap.Explainer(cb.fit(X_train, y_train))` fits CatBoost again (it was already fitted earlier), but more importantly, typically `shap.TreeExplainer` or `shap.KernelExplainer` are used depending on the model. `shap.Explainer` is a wrapper but might not always choose the most efficient explainer.
    *   It doesn't provide dependence plots or force plots, which are common for comprehensive SHAP interpretation.

9.  **General Code Quality Issues:**
    *   Many imports are unused (e.g., `Pipeline` is inconsistently used, `accuracy_score` is imported but `brier_score_loss` used for weighting).
    *   Confusing variable names (`n_splits` used repeatedly in custom ensemble without clear definition or correct context, `xgb` and `lgbm` defined multiple times).
    *   The use of `X` and `y` being globally transformed and split multiple times (e.g., in Bayesian optimization, then nested CV, then dynamic weights) without re-scoping or clear re-initialization can lead to data leakage or incorrect evaluation if not handled carefully.
    *   `use_label_encoder=False` is deprecated in XGBoost, `enable_categorical=True` should be used.

Overall, while the code attempts to include the required libraries and some function calls, the implementation details, especially for Bayesian optimization, nested cross-validation, custom ensemble weighting, and calibration, are either incorrect or significantly incomplete.",mistral-nemo,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"Here's a Python code snippet that covers most of the requirements using PyMC3 for Bayesian modeling, lifelines for survival analysis,.group for mixed-effects models, and other necessary libraries. Note that causal inference methods and Gaussian processes are not included as they require external data or specific assumptions not evident in the diabetes dataset.

```python
import pandas as pd
import pymc3 as pm
import numpy as np
from lifelines import KaplanMeierFitter
from group import MixedEffec...",2,"The generated code fails to address most of the complex requirements outlined in the prompt, leading to a low score.

**Critical Issues and Missing Implementations:**

1.  **Missing Value Handling:** The code uses `df.dropna()`, which is a very naive approach and not ""comprehensive missing value handling."" The prompt specifically mentioned that 0s often represent missing values in this dataset (Glucose, BMI, etc.), which `dropna()` would not address.
2.  **Hierarchical Bayesian Models:** The PyMC3 `model` implemented is a simple non-hierarchical logistic regression. It does not include any `group` effects or `alpha_group`/`beta_group` parameters, violating the ""hierarchical"" and ""mixed-effects"" requirements.
3.  **Mixed-Effects Models:** The generated code attempts to use `MixedEffectModel.from_formula` from a library called `group`. This library is not standard and is unlikely to be installed or compatible. The prompt explicitly suggested using PyMC3/PyMC4 for mixed-effects which aligns with the Bayesian approach. The formula `AGE ~ AGE + INCOME` is nonsensical (AGE explaining itself) and assumes `INCOME` exists, which is not in the `diabetes.csv` dataset.
4.  **Informative Priors:** The priors used (`mu=0, sd=1`) are default vague normal priors, not ""informative priors based on medical knowledge.""
5.  **Gaussian Processes:** Explicitly stated as not included: ""Gaussian processes... are not included."" This violates a key requirement (point 7).
6.  **Causal Inference:** Explicitly stated as not included: ""causal inference methods... are not included."" This violates another key requirement (point 8).
7.  **Survival Analysis:** The survival analysis portion makes incorrect assumptions. It uses `df['AGE']` as the time component and `df['Diabetes_Incident']` as the event indicator. The `diabetes.csv` dataset does *not* contain a 'Diabetes_Incident' column; it has an 'Outcome' column which indicates immediate diabetes status, not an onset event. The prompt specifically stated to ""simulate time component."" The `KaplanMeierFitter` only calculates survival probabilities, it does not involve regression like CoxPH which links covariates to survival.
8.  **PyMC3 Model Structure:**
    *   The `pm.math.dot(df.drop('AGE', axis=1), beta)` line tries to multiply `beta` (a scalar in this simplified model) by a DataFrame (features), which is incorrect. `beta` should be a vector with a dimension matching the number of features.
    *   The observed variable `y` is set to `observed=df['AGE']`, but `AGE` is a continuous variable, not a binary outcome suitable for `Bernoulli` likelihood. The target variable is `Outcome`.
9.  **Completeness:** Only 3-4 out of the 10 core requirements are superficially addressed (mostly with incorrect implementations or significant omissions). The overall approach is very simplistic and does not reflect ""advanced statistical modeling and Bayesian analysis.""
10. **Code Quality/Imports:** The prompt had specified `pymc3` or `pymc4`. The generated code uses `pymc3` but attempts to import `group` which is not a standard library. `pm.traceplot` and `pm.plot_posterior` are deprecated in newer ArviZ versions and often require `arviz` import. The final `print` statements within the model block are also misplaced, as sampling is asynchronous.

The code reflects a basic understanding of *some* Bayesian concepts but fails to integrate them into the specified complex model types (hierarchical, mixed-effects, GP, causal) or handle critical data aspects (zero-as-NaN, simulated time-to-event) as requested.",mistral-nemo,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture, SpectralClustering
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.manifold import TSNE, UMAP
from sklearn.decomposition import IncrementalPCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d i...",3,"The generated code attempts to address several aspects of the prompt but fails significantly in correctness, completeness, and code quality.

**Correctness:**
*   **Feature Engineering:** The generated code includes `pretai`, `formerlysm`, and `ms_tbi` as engineered features. These are completely arbitrary and appear to be hallucinations (`insulin * pregnancies` makes no clinical sense; `formerlysmoker` is not in the dataset; `obesity`, `t —Ç–µ—Ä–∏bus`, `dis consacr√©` are not in the dataset and use non-English characters, indicating a serious hallucination). The ground truth provides relevant clinical feature engineering based on metabolic syndrome.
*   **Missing Value Imputation:** The generated code completely misses the crucial step of handling missing values (0s in specific columns) which is vital for the `diabetes.csv` dataset. This will lead to incorrect clustering results.
*   **Target/Features Split:** It incorrectly drops 'age' as the target (`Y = df['age']`) and uses the rest as features (`X = df.drop(columns=['age'])`). The task explicitly asks for advanced clustering and patient segmentation, where the 'Outcome' (diabetes status) is typically what you want to understand how it relates to clusters, not predict 'age'. Also, scaling `X` without proper handling of `df.drop(columns=['age'])` where `age` is a feature for clustering, is incorrect in the context of the task. The ground truth correctly identifies features and target and scales only features for clustering.
*   **Hardcoded Number of Clusters:** For K-Means, Hierarchical, GMM, and Spectral Clustering, the number of clusters (`n_clusters=3`) is hardcoded. The task explicitly requires determining optimal clustering parameters using silhouette analysis, gap statistic, and elbow method with statistical validation. The ground truth implements this.
*   **Clustering Algorithm Evaluation:** Silhouette score is printed, but no effort is made to find optimal parameters or compare algorithms properly beyond just printing a score.
*   **Dimensionality Reduction:**
    *   `IncrementalPCA` is used with `Autoencoder` but the `Autoencoder` class is not defined. This results in a `NameError`.
    *   The `TSNE` and `UMAP` pipelines are redundant as these are typically applied directly after scaling.
    *   Visualization tries to plot in 3D but `n_components` for TSNE and UMAP are typically 2 for easy 2D visualization of clusters, and the generated code randomly chooses 3 without clear justification.
    *   The visualization loop uses `labels` which are only from the *last* clusterer (`MeanShift`) applied. It doesn't visualize each clusterer's output.
*   **Time-series Clustering:** The generated code makes an arbitrary assumption about 'age' being a time variable, which is incorrect for the (cross-sectional) `diabetes.csv` dataset. The ground truth correctly identifies this limitation and provides a conceptual explanation.
*   **Association Rule Mining:**
    *   The implementation is completely flawed. It uses `chi2_contingency` on `Y` and `X[col]` to build ""rules"", which is not how association rule mining works. Association rule mining requires a transaction-like dataset (e.g., one-hot encoded binned features) and algorithms like Apriori or FPGrowth. The ground truth correctly uses `mlxtend` for Apriori.
    *   `rules[contingency[0][0]]` implies contingency[0][0] is a meaningful key, which it is not (it's the chi2 statistic). This will likely lead to incorrect keys and meaningless rules.
*   **Anomaly Detection:**
    *   `rbp` and `multzoan` are undefined variables, causing a `NameError`.
    *   `X_scaled.reshape(-1,1)` is used for `IsolationForest`, implying a single feature or flattening, which is incorrect for a multi-feature dataset. IsolationForest should be applied to the scaled multi-dimensional data.
*   **Patient Phenotype Profiles:** `profile.to_csv(f'{name}_profile.csv', index=False)` attempts to save profiles, but `clusters_df.map(df.groupby(col).mean())` is logically flawed. It tries to map cluster labels to feature means grouped by the *original feature*, not by the cluster. The ground truth correctly groups the *original DataFrame* by cluster label and calculates statistics *per cluster*.
*   **Consensus Clustering:** Completely missing. This was a specific and advanced requirement.
*   **Optimal Parameter Determination (Elbow/Gap Statistic):** Largely missing beyond just printing silhouette scores.

**Code Quality:**
*   **Imports:** The generated code includes `MeanShift` and `GridSearchCV` that are commented out as not used, and `Axes3D`, `json`, `time`, `collections`, `Mannheim` which are either unused or used incorrectly. `Autoencoder` is imported but not defined. `Pipeline` and `ColumnTransformer` are imported but barely used for their intended purpose in the DR section.
*   **Readability/Clarity:** Variable names like `pretai`, `formerlysm`, `ms_tbi`, `jski`, `multzoan`, `rbp` are confusing and some are undefined.
*   **Structure:** Lacks clear functions or logical separation for different parts of the analysis, making it harder to follow.
*   **Error Handling:** No `try-except` blocks for file loading or other operations, unlike the ground truth.

**Completeness:**
*   **Consensus Clustering:** Missed entirely.
*   **Gap Statistic/Elbow Method for Optimal K:** Missed implementation.
*   **Time-series clustering (properly addressing cross-sectional data):** Missed.
*   **Clinical Interpretation for Risk Stratification:** Minimal to none, whereas the ground truth provides a strong example.
*   **Proper Anomaly Visualization:** The generated code attempts to visualize, but the underlying data for `hue` (`IsolationForest`, `OneClassSVM`, `LOF`) are not consistently plotted across the 3 DR plots, and the anomaly detection itself is broken.
*   **Statistical Validation of Optimal Parameters:** Not implemented.

In summary, the generated code contains numerous logical errors, undefined variables, and does not correctly implement several key requirements, making it largely non-functional and inappropriate for the given task.",mistral-nemo,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"Sure, here's the Python code for the advanced machine learning ensemble methods using the provided dataset:

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, KFold, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, PowerTransformer
from sklearn.ensemble import VotingRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sk...",3,"The generated code fails to meet most of the advanced requirements of the task.

**Critical Failures and Missing Components:**

1.  **Data Preprocessing and Feature Engineering:**
    *   The `preprocess_data` function completely renames the target column from 'Ladder score' to 'Score' without explanation, leading to inconsistencies.
    *   It drops too many critical columns ('Social support', 'Healthy life expectancy', 'Freedom', 'GDP per capita') which are key features, essentially removing most of the dataset's predictive power. This makes the models train on a severely reduced and incorrect feature set.
    *   The ""feature engineering"" for 'GDP pe pop' is a division by 'Population' which isn't present after column dropping (which happens *before* feature engineered features are used). The `data.fillna(0)` and `data.dropna()` contradict each other and are likely misapplied for handling missing values.
    *   No proper handling of categorical variables (e.g., 'Regional indicator') is implemented, which is crucial for this dataset. The ground truth uses `OneHotEncoder` and `ColumnTransformer`.

2.  **Ensemble Methods Implementation:**
    *   The ensemble methods are implemented, but they are directly fitted on `X, y` *after* the flawed preprocessing, making their results unreliable.
    *   The `VotingRegressor` `estimators` are directly passed model instances without `preprocessing` pipelines, meaning they would attempt to fit on raw `X`, which isn't scaled or one-hot encoded, breaking compatibility.
    *   No clear distinction between ""hard"" and ""soft"" voting for the VotingRegressor is shown as requested.

3.  **Stacking Ensemble:**
    *   The `make_pipeline` usage for stacking is incorrect. `make_pipeline(*base_learners, meta_learner)` does not perform stacking; it chains the base learners directly, passing the output of one as input to the next, which is not how stacking works. Stacking requires base learners to output predictions (or transformed features) for the meta-learner. The `StackingRegressor` class from `sklearn.ensemble` (used in ground truth) is the correct approach.

4.  **Bayesian Optimization:**
    *   The `bayesian_optimization` function takes a `model` argument but then tries to `set_params` on it and evaluate it directly. This setup is flawed because the `model` object passed (`xgb.XGBRegressor` or `lgb.LGBMRegressor` as implicitly expected) won't have preprocessing steps applied. The ground truth handles this correctly by wrapping the model inside a `Pipeline`.
    *   The `objective` function for Optuna tries to `fit` the model (and then `score`) on the *entire* `X, y` dataset, which causes data leakage and leads to overly optimistic (and incorrect) scores during hyperparameter tuning. It should use cross-validation within the objective function.

5.  **Nested Cross-Validation:**
    *   The `nested_cross_validation` function is fundamentally incorrect. It attempts to use `model.best_params` for the *inner* fit, but `model.best_params` would only exist *after* a previous `optuna` run. A TRUE nested CV requires the hyperparameter tuning (e.g., Optuna) to happen *inside* each outer fold using *only* the training data of that fold. The current implementation tries to apply `model.set_params(**inner_params)` to the `model` object itself after it might have already been fitted.
    *   `inner_params` is assigned `model.best_params`, but `model` is passed as a generic model, not an `Optuna` study result, further highlighting the conceptual misunderstanding. It then attempts to `cross_val_score` on `X_test, y_test` with `inner_cv`, which is not right either.

6.  **Custom Ensemble:**
    *   The weight assignment is based on `abs(score - 0.5)` which is a very arbitrary and unclear performance metric. It doesn't rely on OOF errors or any standard performance measure for regression.
    *   The ensemble combination is done by weighting the *predictions* from models fitted on the *entire* dataset, which is subject to data leakage for the evaluation of the ensemble's overall performance. Proper custom ensembles use Out-Of-Fold (OOF) predictions.

7.  **Advanced Techniques (Blending and Multi-level Stacking):**
    *   These requirements are completely missing.

8.  **Model Interpretation (SHAP and LIME):**
    *   The code includes imports for `shap` and `lime` but no implementation of SHAP value computations or LIME explanations.

9.  **Model Calibration and Uncertainty Quantification:**
    *   These requirements are completely missing.

10. **Ensemble Diversity Analysis and Model Combination Optimization:**
    *   These requirements are completely missing.

**Code Quality and Structure:**
*   Redundant imports (`xgboost as xgb`, `catboost as cb`, `lightgbm as lgb` are imported twice).
*   Lack of clear pipeline integration for `preprocessing` with models in many places, which is crucial for robust ML workflows.
*   The `preprocess_data` function modifies global data, which is bad practice.
*   No plotting or visualization requested.
*   The `main` block (or equivalent) to orchestrate data loading, preprocessing, model training, and evaluation is missing, making the code snippets disorganized.

In summary, the generated code demonstrates a very superficial understanding of advanced ensemble methods, hyperparameter tuning, and robust evaluation. It fails to correctly implement the key ideas behind tasks 4, 5, 6, 7, 8, 9, and 10, and severely misinterprets data preprocessing and feature engineering.",mistral-nemo,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"I'm unable to provide Python code without explanations, as the problem is quite complex and would likely span hundreds of lines of code with the specified tasks. Instead, I can provide a high-level outline of how you might approach each task using Python and popular libraries, along with brief explanations.

1. **Loading and preprocessing the dataset**

```python
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Missing value handling
#...",1,"The generated code fails to meet almost all requirements of the task.

**Critical Failures and Missing Components:**

1.  **""Provide only the Python code without explanations.""**: The generated output explicitly states, ""I'm unable to provide Python code without explanations... Instead, I can provide a high-level outline... along with brief explanations."" This immediately violates a primary instruction.
2.  **Code Functionality**: Most of the code snippets provided are either incomplete placeholders, syntactically incorrect, or conceptually flawed for the given task.
    *   **Preprocessing**: `df = df.fillna(df.mean())` is a rudimentary approach to ""comprehensive missing value handling"" and does not include robust scaling or one-hot encoding as expected for advanced modeling.
    *   **Hierarchical Bayesian Models**: The PyMC3 snippet is a highly simplified placeholder. `r[pm—á—å(dim)]` is not valid PyMC3 syntax. It demonstrates a very basic understanding of hierarchical models but doesn't connect to the actual data (e.g., `regional_indicator`).
    *   **SEM**: `from statsmodels.sem import fast` and `result = fast.wle(X, Y)` are completely incorrect. `statsmodels.sem` does not have a `fast` module or `wle` function for general SEM. SEM typically involves defining a model structure (e.g., factor loadings, regressions between latent variables) and fitting it, often with specialized libraries like `lavaan` (R) or `statsmodels.formula.api` with specific model string definitions, or custom PyMC3 implementations. The provided solution is a complete misinterpretation.
    *   **Causal Inference (IV/PSM)**: The `causalml` snippets are a chaotic mix of non-existent functions (`propensity_score_method.PropensityScore Psy match(Nros)`), incorrect usage, and a lack of understanding of IV (which requires specific assumptions and a two-stage process) or PSM (which requires matching logic). The conceptual setup (e.g., defining `income` as an IV without any dataset context) is poor.
    *   **Gaussian Processes**: The `sklearn.gaussian_process` approach is valid for a basic GP, but the prompt specifically asked for *PyMC3/PyMC4* for Bayesian approaches, which `sklearn` is not. A Bayesian GP would involve defining a kernel, mean function, and likelihood within PyMC3.
    *   **Mediation Analysis**: The `mediation` library usage is entirely incorrect with `Mediation(width=np.array([1]), values(np.array([-1, 1])), order='instr').set_data(df)`. This library is not a standard Python mediation analysis library, and its usage here is nonsensical. Mediation analysis builds on the output of regression models (often path analysis or SEM).
    *   **Difference-in-Differences**: The provided snippet for DiD (`DD = pd.DataFrame(...)`) is not a DiD analysis at all; it's just data slicing. DiD requires a regression model with interaction terms (`treated * time`).
    *   **Bayesian Model Comparison**: The PyMC3 `cross_val_score` `loo_cv_score` is not standard PyMC3 for LOO-CV. `arviz.compare` is the standard tool.
    *   **Credible Intervals, PPC, Sensitivity**: The snippets are rudimentary and don't provide a comprehensive implementation as requested.

3.  **Library Usage**: Although some correct libraries (`pandas`, `numpy`, `pymc3`, `sklearn.gaussian_process`) are imported, their application is largely incorrect, often relying on non-existent functions, or ignoring the explicit request for PyMC3/PyMC4 for specific tasks (like GP).

4.  **Assumptions and Data Handling**: The prompt clearly specified using `world-happiness-report-2021.csv` and detailed preprocessing. The generated code mostly ignores this complexity, jumping to placeholders like `df['happiness_score']` without showing how these columns are derived or handled, or defining `x` and `y` for PyMC3 models.

In summary, the generated code is more of a vague conceptual outline with incorrect/non-functional snippets rather than actual executable Python code that addresses the complex requirements. It fails to demonstrate expertise in any of the advanced statistical or causal inference methods requested.",mistral-nemo,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture
from sklearn.manifold import TSNE, MDS
from sklearn.decomposition import PCA
from sklearn.datasets import make_blobs
import umap
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.ensemble import IsolationFore...",3,"The generated code has significant issues in correctness, completeness, and code quality.

**Correctness:**
- **Data Preprocessing:** It drops all NaNs which is often not ideal for real-world datasets and can lead to significant data loss. The feature engineering is extremely basic (binary 'Happiness Profile', string concatenation for 'Regional Characteristics') and does not align with the ""advanced feature engineering including happiness profiles and regional characteristics"" requested. The `df.columns[4:]` for `df_stand` after reshaping `df` is fragile and depends on the order of columns remaining consistent, which is poor practice. It tries to standardize the `df` by dropping 'Country', 'Region', 'Happiness Profile', 'Regional Characteristics', but `df` still contains `Happiness Score`, `Perceptions of corruption` etc. It seems to confuse numerical features with string features after the initial `drop`.
- **Clustering implementation:** It attempts to run `KMeans` and `GaussianMixture` without specifying `n_clusters` which are mandatory parameters, leading to errors. `SpectralClustering` also requires `n_clusters`. HDBSCAN works without `n_clusters` but could benefit from `min_cluster_size` tuning.
- **Optimal Parameter Determination:** It calculates silhouette, Calinski-Harabasz, and Davies-Bouldin scores for a *fixed* number of clusters (10 for most, default for DBSCAN/KMeans). This entirely misses the point of ""determining optimal clustering parameters"" which implies iterating through parameter ranges and finding the best. The `cluster_algorithms` dict initializes KMeans and HDBSCAN with empty params, which will cause errors or default to sub-optimal settings.
- **Consensus Clustering:** It attempts to use a non-existent `sklearn.cluster.consensus_clustering` function. The ground truth shows how complex consensus clustering can be.
- **Dimensionality Reduction:** Only includes t-SNE, not UMAP, autoencoders, or manifold learning (MDS is listed but not explicitly run for 'manifold learning' and PCA is imported but unused).
- **Anomaly Detection:** Only implements Isolation Forest, missing LOF and One-Class SVM.
- **Network Analysis:** Only calculates `NearestNeighbors`, not forming or visualizing a graph with `networkx` to understand relationships via centrality measures as requested.
- **Association Rule Mining:** Relies on `mlxtend.preprocessing.somebody_transform` which is not a real function or module. The concept of converting continuous data to a suitable format for `apriori` using binning is completely missed.
- **Recommendation System/Policy Insights:** This crucial section is largely empty placeholders (`clusters = df_stand.drop_duplicates(subset='Country'), geopolitical_index = ...`). It doesn't build any recommendation system or provide policy insights backed by the analysis, let alone ""geopolitical validation"".

**Code Quality:**
- **Imports:** Imports `MDS` and `PCA` but doesn't use them. Imports `make_blobs` and `metrics` but doesn't use them. Imports `somebody_transform` which doesn't exist.
- **Structure:** The code is sequential and monolithic, lacking functions or classes for organization, making it hard to read, maintain, and debug. The ground truth uses a class which is much better.
- **Variable Naming:** `df_stand` is used inconsistently. `df` is modified in place multiple times.
- **Error Handling/Robustness:** No error handling. Many parts will simply crash due to missing parameters, non-existent functions, or incorrect data types.

**Completeness:**
- **Missing Features:**
    - Advanced feature engineering (beyond very basic examples).
    - Optimal K determination (robustly, with methods like gap statistic/stability analysis).
    - Full suite of dimensionality reduction techniques (UMAP, autoencoders, multiple manifold learning).
    - Proper consensus clustering.
    - Detailed profiling with statistical significance and effect size.
    - Full suite of anomaly detection (missing LOF, OCSVM).
    - Comprehensive network analysis (graph building, centrality measures, visualization).
    - Functional association rule mining (due to non-existent function).
    - Functional country recommendation system.
    - Actual policy insights with geopolitical validation.
- **Functionality is superficial or completely broken for most sections.**

Overall, the generated code attempts to address many of the points but fails to implement them correctly or completely, often introducing errors or using placeholder/non-existent solutions. It lacks the depth and sophisticated implementation expected for ""advanced"" analysis.",mistral-nemo,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from statsmodels.tsa.statespace.sarimax import SARIMAX
import pmdarima as pm
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error

# Load data
data = pd.read_csv('synthetic_covid19_data.csv')
data['date'] = pd.to_datetime(data['date'])
data = data[data['location'] == 'Germany']
data.set_index('date', inplace=True)

# Handle missing values
data['new_cases'].ffill(inplace...",6,"The generated code successfully implements several key components of the task, including data loading, filtering, setting the index, handling missing values, splitting data, training a SARIMA model using `auto_arima` with exogenous variables, forecasting, and calculating RMSE. It also generates a plot.

However, there are several issues that prevent it from achieving a higher score and affect its correctness and robustness:

1.  **Missing Exogenous Variable Handling in `test_train_split`**: The `train_test_split` function used by the generated code shuffles the data by default (`shuffle=False` is set, which is good for time series), but it splits the *entire* DataFrame. This means `train['stringency_index']` and `test['stringency_index']` etc., are correctly aligned. However, the `auto_arima` model returns a `pmdarima` model object, not a `statsmodels.tsa.statespace.sarimax.SARIMAXResultsWrapper` object. The `model.predict()` method in `pmdarima` needs the exogenous variables for the forecast period. The generated code attempts to handle `X_test` but then **incorrectly re-indexes `X_test` using `pd.date_range`**. This completely destroys the alignment of dates between `X_test` and `test['new_cases']`, making the forecast incorrect. The `X_test.index` should directly come from `test.index` or just implicitly align. This is a critical error.

2.  **Handling Missing Values - Incomplete**: The code only uses `ffill()`. While `ffill` is a reasonable start, time series data often has NaNs at the beginning of the series, which `ffill` won't handle. `bfill` or `interpolate` should also be considered as robust practices, as shown in the ground truth. The generated code's current approach is susceptible to NaNs at the start of the series.

3.  **No Separate Target/Exogenous Variables for Training**: The code directly passes `train['new_cases']` and `train[['stringency_index', 'people_fully_vaccinated']]` to `auto_arima`. While `auto_arima` is designed to handle this, the problem statement implies separating these explicitly (`y` and `X`) for clarity and consistency with `statsmodels` usage (though `pmdarima` often wraps `statsmodels`). This is more of a minor style point than a critical error.

4.  **No `statsmodels.SARIMAX` Integration**: The prompt explicitly asks to ""Train the SARIMA model on the training set, including exogenous variables"" (implying `statsmodels.tsa.statespace.sarimax.SARIMAX` as it's an import requirement). While `pmdarima.auto_arima` *fits* a SARIMA model, it doesn't explicitly call `statsmodels.SARIMAX` for training post-selection of orders, which was a subtle requirement. The prompt implies a two-step process: `auto_arima` for order selection, then `SARIMAX` for final training and forecasting with the chosen orders. The generated code only uses `auto_arima` to fit and predict, skipping the explicit `SARIMAX` training step mentioned in the prompt.

    *   **Clarification:** The prompt says ""Implement a SARIMA model. Determine appropriate (p,d,q)(P,D,Q,s) orders (e.g., using auto_arima)."" and then ""Train the SARIMA model on the training set, including exogenous variables."" This strongly suggests using `auto_arima` for hyperparameter tuning, and then `SARIMAX` from `statsmodels` for the final model training. The generated code stops at `auto_arima` for fitting and predicting.

5.  **Plotting - Missing Training Data**: The plot only shows the actual and forecast for the test period. The ground truth includes the training data in the plot, which provides better context for the forecast.

6.  **No Robustness Checks**: The ground truth includes checks for `FileNotFoundError`, sufficient data length, and handling potential issues during missing value imputation. The generated code lacks these robustness checks.

The most critical issue is the incorrect re-indexing of `X_test`, which will lead to an incorrect forecast. The missing explicit `statsmodels.SARIMAX` training and less robust missing value handling are also significant deviations from the requirements.",mistral-nemo,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.stats.multitest as multi_test

df = pd.read_csv('winequality-red.csv')
df['quality'] = pd.cut(df['quality'], bins=[1, 4, 6, 9], labels=['low', 'medium', 'high'])

 significant_features = []

 for col in df.columns[:-1]:
     stat, p = stats.shapiro(df[col][df['quality']=='low']), stats.shapiro(df[col][df['quality']=='medium']), stats.shapiro(df[col][df['quality']=='h...",3,"The generated code attempts to address the prompt but falls significantly short in multiple critical areas. It lacks proper statistical testing methodology, correct post-hoc implementation, and comprehensive visualization.

**Critical Issues:**

1.  **Incorrect Quality Grouping:** The `pd.cut` function maps bins `[1, 4, 6, 9]` to `low`, `medium`, `high`. This results in:
    *   `low`: `1` excl. to `4` incl. which aligns with `score <= 4`.
    *   `medium`: `4` excl. to `6` incl. which aligns with `5 <= score <= 6`.
    *   `high`: `6` excl. to `9` incl. which aligns with `score >= 7`.
    While this is technically correct, the ground truth uses an explicit function which is clearer and less prone to off-by-one errors or misinterpretation of bin edges.

2.  **Incorrect Normality Test Usage:** The line `stat, p = stats.shapiro(df[col][df['quality']=='low']), stats.shapiro(df[col][df['quality']=='medium']), stats.shapiro(df[col][df['quality']=='high'])` is fundamentally flawed. `stats.shapiro` returns a tuple `(statistic, p-value)`. The code assigns the result of the first `shapiro` test to `stat` and the result of the second `shapiro` test to `p`. This means `p` is not a list of p-values but rather a tuple `(statistic, p-value)` from only the medium quality group. The subsequent `if p[0] > 0.05 and p[1] > 0.05 and p[2] > 0.05` attempts to access indices `p[2]` which does not exist, and `p[0]` and `p[1]` would be the statistic and p-value of the medium group only. This will cause an `IndexError` or incorrect normality checks.

3.  **Missing Homogeneity of Variance Check:** It totally omits the Levene's test (or similar) for homogeneity of variances, which is a crucial assumption for ANOVA. Kruskal-Wallis is chosen *only* if normality fails, not considering variance heterogeneity, which is incorrect.

4.  **Incorrect P-value assignment and Conditional Logic:** After performing ANOVA or Kruskal-Wallis, both results are assigned to `p` but its value is immediately overwritten or used incorrectly in subsequent `if` conditions. The `if p <= 0.05:` check for significance is correct, but the print statements `if p > 0.05 and p[0] > 0.05 and p[1] > 0.05:` are confusing and likely lead to incorrect output due to the aforementioned `p` variable misuse.

5.  **Missing Post-hoc Tests (Critical Failure):**
    *   It tries to use `stats.posthoc_dunn` and `stats.posthoc_tukey`. These functions are generally found in the `scikit-posthocs` library, not `scipy.stats`. The code is missing the import for `scikit-posthocs`.
    *   Even if `scikit-posthocs` were imported, the usage `stats.posthoc_dunn(df[col], group, holm=True)` is incorrect. `group` is not defined nor passed correctly to distinguish between quality tiers.
    *   The `if len(df[col].unique()) > 2:` condition to choose between Dunn's and Tukey's is illogical and incorrect. Tukey's HSD is used *after* ANOVA, and Dunn's is used *after* Kruskal-Wallis, regardless of the number of unique *values* in the feature. It should depend on the *primary* test performed.
    *   It doesn't print or store the results of the post-hoc tests, making them effectively useless.

6.  **Incomplete Summarization:** The final summary and comprehensive output of results are completely absent. The code only prints the significance for primary tests and performs unrecorded post-hoc tests and plots.

7.  **Basic Code Structure and Readability:** The code lacks comments, clear variable names, and logical flow compared to the ground truth. It's difficult to follow the intended statistical logic.

8.  **Missing Error Handling:** It does not handle `FileNotFoundError` for the CSV, nor does it handle cases where a quality group might be empty for a feature, which could lead to errors in statistical tests (e.g., `stats.shapiro` requires at least 3 data points).

Overall, the generated code fails to correctly implement the core statistical analysis steps, particularly the crucial normality checks and post-hoc tests, and lacks the required comprehensive output and error handling.",mistral-nemo,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"Here's a Python script that follows your requirements to predict 'high_daily_death_increase' events using the provided synthetic_covid19_data.csv dataset. I've used pandas for data manipulation, xgboost for modeling, sklearn for feature scaling, grid search, and evaluation metrics, and shap for feature importance explanation. Make sure to have these libraries installed in your environment.

```python
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import...",4,"The generated code has significant issues that prevent it from fully solving the task correctly, especially regarding the target variable definition, handling of missing values, and the machine learning pipeline setup.

**Critical Issues:**

1.  **Incorrect Target Variable Definition:**
    *   The prompt specifies: ""A 'high_daily_death_increase' is defined as a day where 'new_deaths' for a 'location' is in the top 10% of its own historical 'new_deaths' (excluding days with zero new deaths).""
    *   The generated code uses `np.percentile(x[x > 0], 90)`. While `x[x > 0]` correctly filters zero deaths, the `apply` method with `np.percentile` within a `groupby` can be problematic. Specifically, `percentile` needs to be calculated *once per location* based on its history and *then* compared against each day's `new_deaths`. The current implementation calculates the percentile *for each row's 'new_deaths'* within the group and compares it to itself, which doesn't make logical sense for a historical threshold. It appears to compare each `new_deaths` value to the 90th percentile calculated from the current series `x`.
    *   The `replace({True: 1, False: 0})` on a boolean series that was assigned using `reset_index(drop=True)` is also a bit convoluted and potentially buggy if the indices don't perfectly align after `groupby` and `apply`.
    *   The outcome of this is likely a target variable that does not align with the definition, leading to incorrect predictions.

2.  **Incorrect `days_since_first_case` Feature:**
    *   The prompt asks for `days_since_first_case` for each location. This means `(current_date - each_location_min_date)`.
    *   The generated code `data.groupby('location')['date'].apply(lambda x: (x.max() - x).dt.days)` calculates `(each_location_max_date - current_date)`. This is ""days *until* last case,"" which is the inverse of what was requested and uses future information, a major data leakage.

3.  **Missing Value Handling:**
    *   The generated code uses a blunt `data = data.dropna()` after feature engineering. This will drop *all* rows with *any* NaN values, which is extremely harsh and will likely lead to significant data loss, especially at the beginning of each location's time series due to rolling and lag features. The prompt specifically asked for handling missing values from feature engineering (e.g., fill with 0 or backfill), implying a more nuanced approach than simply dropping.

4.  **Incorrect `GridSearchCV` Usage with `ColumnTransformer` and `XGBoost`:**
    *   The line `grid_search.fit(X, y, CategoricalFeaturesIndicator(categorical_columns=['continent']))` is syntactically incorrect. `CategoricalFeaturesIndicator` is not a standard `sklearn` utility used this way with `GridSearchCV` or `XGBoost`.
    *   The `preprocessor` is created but never integrated into a `Pipeline` with the `XGBClassifier` before `GridSearchCV`. This means `GridSearchCV` is fitted on the raw `X`, and the scaling/encoding is not applied during the cross-validation or fitting of the `XGBoost` model itself.
    *   As a result, `grid_search.fit(X, y)` will fail or produce incorrect results because `X` contains unenconded categorical features and unscaled numerical features.
    *   Later, `X_train_transformed = preprocessor.fit_transform(X_train)` and `X_test_transformed = preprocessor.transform(X_test)` are done *outside* the `GridSearchCV` context, meaning the best estimator returned by `grid_search` would not have seen preprocessed data.

5.  **Evaluation Split:**
    *   While `train_test_split(..., shuffle=False)` correctly performs a time-based split, it's applied *after* the `GridSearchCV` and *after* the `X` and `y` are defined. This means the `GridSearchCV` itself is performed on the *entire dataset* (or at least without a proper time-based split for its internal cross-validation folds), which introduces data leakage from the future into the parameter tuning process. The prompt explicitly asked for a time-based split for evaluation, which implies the model training, including CV, should respect this temporal order.

6.  **Feature Selection:**
    *   The code selects features by literally listing some and then using a catch-all `c for c in data.columns if c not in [...]`. This automatically includes engineered features like `hosp_patients_rolling_mean` even though `hosp_patients` was not explicitly listed for scaling in `num_features`. This creates a mismatch between the preprocessor's `num_features` list and the actual `X` columns, leading to errors or incorrect preprocessing.
    *   It's missing the ratio `people_fully_vaccinated / population` as a specified feature. It includes `people_fully_vaccinated_per_hundred` which might be a pre-existing column or an assumption.

7.  **SHAP Initialization:**
    *   `shap_values = explainer.shap_values(X_test_transformed)` is called directly on `X_test_transformed`. For `TreeExplainer`, SHAP values are typically calculated using the `X_test_transformed` *which was transformed by the same preprocessor that was part of the best model's pipeline*. If the `GridSearchCV` wasn't set up as a pipeline (which it wasn't here), then the `explainer` tries to explain a model that was fit on raw data, while `shap_values` are computed on transformed data. This is problematic.

**Minor Issues / Areas for Improvement:**

*   No imports for `matplotlib.pyplot` which is commonly used with SHAP plots (`shap.summary_plot` effectively calls it).
*   No logging/print statements to indicate progress or output key information, making it harder to debug compared to the ground truth.
*   The `use_label_encoder=False` for XGBoost is good, but `eval_metric` is missing (though often defaults to `logloss` for classification).
*   The `reset_index(0, drop=True)` for rolling features might not be necessary or could be integrated more cleanly using `groupby().transform()`.

Overall, the generated code demonstrates familiarity with the required libraries but fails to correctly implement several core logical and structural requirements of the task, particularly around target variable definition, time-series feature engineering, and the robust setup of an `sklearn` preprocessing and model pipeline with `GridSearchCV`.",mistral-nemo,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"Here's Python code that follows your specifications using `pandas`, `numpy`, `statsmodels`, and `scipy` libraries to perform the analysis. Note that this code assumes that the 'date' column is in a correct datetime format that can beparsed by pandas. Also, it uses median imputation for missing values.

```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy import stats

# 1) Load data, parse 'date', calculate 'vaccination_pe...",4,"The generated code attempts to address the task but fails significantly in several critical areas, particularly in data imputation and model specification.

**Major Issues:**

1.  **Incorrect Missing Value Imputation (Critical Failure):**
    *   The line `df[col] = df.groupby('location')['date'].transform(lambda x: x.fillna(x copias_and_statistics.mean()))` is syntactically incorrect (`copias_and_statistics.mean()` is not valid Python code).
    *   Even if corrected, `df.groupby('location')['date']` selects the 'date' column for grouping and transformation, not the `col` being processed (e.g., 'reproduction_rate', 'stringency_index', 'vaccination_percentage'). This means it would try to impute 'date' using itself, not the intended columns, and in a nonsensical way.
    *   The `fillna(x.mean())` approach for time-series data like 'reproduction_rate', 'stringency_index' is often less appropriate than interpolation, especially when grouped by location and date.
    *   It only handles `reproduction_rate`, `stringency_index`, `vaccination_percentage`, while `population` and `people_fully_vaccinated` are used to calculate the latter and also need imputation first.

2.  **Incorrect Mixed-Effects Model Specification (Critical Failure):**
    *   The formula `formula = 'reproduction_rate ~ stringency_index * vaccination_percentage + (1|location/continent)'` uses a common `lme4` (R package) syntax for nested random effects. `statsmodels.formula.api.mixedlm` does not directly interpret `(1|A/B)` for nested random effects.
    *   The `groups=df['location']` argument implies random effects are grouped only by 'location', not 'location' nested within 'continent' as required. For nested random effects `(1|B/A)` in `statsmodels`, one typically creates a unique `id` for each `(A, B)` combination (e.g., `continent_location_id`) and uses that as the `groups` argument. The prompt explicitly asked for ""random intercepts for 'location' nested within 'continent'"".

3.  **Incomplete Handling of Model Results:**
    *   `model_results = model.summary()` assigns the *summary object* to `model_results`. While you can access tables from it, it's generally cleaner to get the fitted model object first and then call `.summary()`. The ground truth does this correctly.
    *   The interpretation of fixed effects just prints coefficient, SE, and p-value. It lacks the qualitative interpretation of direction and significance in context as requested (e.g., ""A higher stringency index is associated with a decrease..."").

4.  **Minor Issues:**
    *   No error handling for file not found.
    *   Does not explicitly sort data by location and date before imputation, which is crucial for time-series imputation methods like interpolation.
    *   No capping of `vaccination_percentage` at 100%, which can lead to values > 100% if `people_fully_vaccinated` is greater than `population` in the raw data (which happens).
    *   No handling for `population` being zero or NaN which would cause division by zero.
    *   Does not print the variance components directly from `model_results.cov_re` which is the specific request. Instead, it prints `model_results.tables[2]`, which is part of the summary and might not be exactly what was intended for direct ""variance components"".

To improve, the generated code needs a complete overhaul of its imputation strategy and a deep understanding of `statsmodels.mixedlm`'s specific syntax for random effects. The interpretation of fixed effects also needs to be expanded.",mistral-nemo,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# 1) Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# 2) Select the features
features = ['elixirCost', 'Win Rate', 'Usage']

# 3) Standardize these features using StandardScaler
scaler = StandardScaler()
scaled_features = scaler.fit_transform(data[featur...",7,"The generated code correctly performs most of the required steps: loading data, selecting and standardizing features, applying K-Means, and performing PCA for visualization. All necessary imports are present.

However, there are a few areas for improvement:
1.  **K-means `n_init` parameter**: The `KMeans` constructor in scikit-learn versions 1.2+ requires the `n_init` parameter to be explicitly set (e.g., `n_init='auto'` or an integer like `10`). The generated code omits this, which would cause a warning or error in newer environments. The ground truth code correctly sets `n_init=10`.
2.  **K choice justification**: While the code calculates WCSS and silhouette scores, it hardcodes `chosen_k = 3` *without* plotting or analyzing these metrics. The task specifically asked for justification using the elbow method (plotting WCSS) or silhouette scores. The ground truth code plots the WCSS to justify the choice.
3.  **Visualization robustness**: The scatter plot is basic. The ground truth code adds a `colorbar`, more descriptive `xlabel`/`ylabel` (including explained variance), and better handles plot aesthetics. While the generated code's plot is functional, it lacks these details for interpretability.
4.  **Error Handling**: The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth gracefully handles. This is a minor point for pure functionality but good practice.

Overall, it's a good attempt, but the missing justification/plotting of K and `n_init` handling are the main points that prevent a higher score.",mistral-nemo,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# Select features and target
features = ['elixirCost', 'Usage', 'rarity']
target = 'Win Rate'

# Preprocess 'ra...",9,"The generated code is highly functional and correctly addresses almost all aspects of the task.

**Strong Points:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Feature Selection:** Correctly identifies `elixirCost`, `Usage`, and `rarity` as features and `Win Rate` as the target.
*   **One-Hot Encoding:** Correctly applies one-hot encoding to the `rarity` column using `OneHotEncoder` and `ColumnTransformer`, which is a robust and `sklearn`-idiomatic way to handle preprocessing within a pipeline. `drop='first'` correctly prevents multicollinearity.
*   **Data Splitting:** Correctly splits data into 80/20 train/test sets with `random_state=42`.
*   **Model Training:** Trains a `RandomForestRegressor` model.
*   **Evaluation Metrics:** Evaluates the model using R-squared and MSE, printing the results.
*   **Pipeline Usage:** The use of `Pipeline` and `ColumnTransformer` is excellent for creating a clean and reproducible workflow, especially for handling preprocessing steps alongside model training.

**Areas for Minor Improvement (Why not a 10):**
*   **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robust code. The generated code assumes the file exists.
*   **Missing Value Handling:** The prompt doesn't explicitly ask for it, but the ground truth demonstrates a simple `dropna` for selected columns. The generated code does not explicitly address missing values, which could be an issue if the dataset had them. However, for a benchmark, this isn't a critical failure if the prompt didn't strictly require it. Given the task specifically asks for 'prediction', assuming clean data is often acceptable unless specified.
*   **Verbosity/Prints:** The ground truth includes some print statements for user feedback (e.g., ""Training the model..."", ""Model training complete."", formatted output for results). While not strictly a correctness issue, it makes the code more user-friendly and informative during execution. The generated code only prints the final scores.",mistral-nemo,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 2) Identify the unique categories in the `rarity` column
unique_rarities = df['rarity'].unique()

# 3) Perform an Analysis of Variance (ANOVA) test
stats.f_oneway(*[df[df['rarity'] == r]['Win Rate'] for r in unique_rarities])

# 4) Null and alternative hypotheses:
# H0: ¬µ1 = ¬µ2 = ¬µ3 = ¬µ4 = ¬µ5 (equal means for all rarities)
# Ha: At least one pair of means is different

# 5) ...",6,"The generated code correctly loads the dataset, identifies unique rarity categories, and performs the ANOVA test. Requirements 1, 2, 3, and 5 are mostly met. However, there are several key shortcomings:

1.  **Missing Hypothesis Statement:** The code comments for section 4 (`# 4) Null and alternative hypotheses: ...`) are not actual print statements, which means the hypotheses are not ""stated"" (printed) as explicitly required by the prompt ""4) State the null and alternative hypotheses for the test.""
2.  **No F-statistic and p-value Printing:** The prompt explicitly asks to ""5) Print the F-statistic and the p-value from the ANOVA test."" The generated code performs the calculation but doesn't print `fstat` and `pvalue` before the interpretation.
3.  **Missing Significance Level:** The problem specifies using `alpha = 0.05` and asks for the interpretation based on ""a significance level of alpha = 0.05"". While the interpretation uses 0.05, it should ideally be defined as a variable (`alpha = 0.05`) for clarity and consistency, and potentially printed.
4.  **No Error Handling:** The ground truth includes robust error handling for `FileNotFoundError`, issues with `Win Rate` and `rarity` columns (e.g., non-numeric data, NaNs), and insufficient data for ANOVA. The generated code lacks any such error handling, making it less robust for real-world data.
5.  **Implicit Data Cleaning:** The generated code assumes `Win Rate` is always numeric and clean, and that there are no NaNs in `Win Rate` or `rarity`. The ground truth explicitly handles `pd.to_numeric` with `errors='coerce'` and `dropna`, which is crucial for real-world datasets.
6.  **Redundant ANOVA Call:** The `stats.f_oneway` function is called twice unnecessarily. It should be called once, and its results stored in `fstat, pvalue` for subsequent use.

To improve, the generated code should:
*   Explicitly print the stated hypotheses.
*   Explicitly print the F-statistic and p-value.
*   Define and use an `alpha` variable for the significance level.
*   Add basic error handling for file loading and data quality (e.g., `dropna`, handling non-numeric `Win Rate`).
*   Perform the ANOVA calculation only once.",mistral-nemo,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Load the dataset
data = pd.read_csv('iris.csv')

# Define features (X) and target (y)
X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = data['Species']

#...",9,"The generated code is very good and largely fulfills all requirements.

**Strong Points:**
- **Correct Data Loading and Splitting:** The data is loaded correctly, features and target are defined as specified, and the target is encoded. The train/test split is done correctly with `random_state=42`.
- **Correct Model Training and Evaluation:** It correctly trains the three specified models and evaluates them using `classification_report`, extracting accuracy, macro precision, macro recall, and macro F1-score.
- **Identification of Best Model:** It correctly identifies the best model based on the highest F1-score (macro average).
- **Good Code Structure:** The code is well-structured and easy to read.

**Minor Improvements/Points of Note:**
- **Missing `max_iter` for Logistic Regression:** The ground truth includes `max_iter=200` for `LogisticRegression` to ensure convergence, which is a good practice. While the generated code's default might work for this dataset, explicitly setting it can prevent warnings or errors on other datasets.
- **Import for `accuracy_score`, `precision_score`, `recall_score`, `f1_score`:** The ground truth specifically imports these metrics, allowing for individual calculation and printing. The generated code uses `classification_report` which aggregates these, and then extracts them. While functionally equivalent for correctness in this task, the prompt's mention of ""report accuracy, precision... and F1-score"" might subtly imply direct calculation rather than parsing a report. However, using `classification_report` is a valid and often preferred approach for conciseness.
- **`stratify` in `train_test_split`:** The ground truth uses `stratify=y_encoded` to ensure that the proportion of target classes is approximately the same in both the training and testing sets. This is crucial for balanced datasets and good practice. The generated code omits `stratify`, which could lead to less representative splits, especially if the dataset were imbalanced. For the Iris dataset, this might not cause a significant issue due to its balanced nature, but it's a good practice to include.
- **Clarity in Output:** While the output is correct, the formatting of the best model could be slightly cleaner, perhaps aligning with the presentation of individual model results.

These are minor points, and the code overall is a highly successful solution to the problem. The core logic and output are perfectly aligned with the requirements.",mistral-nemo,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load the dataset
data = pd.read_csv('iris.csv')

# 2) Select the features
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

# 3) Standardize these features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(data[features])

# 4) Apply K-Means clustering
kmeans = KMea...",7,"The generated code correctly performs most of the required steps: loading the dataset, selecting features, standardizing, applying KMeans, and performing PCA. It also generates two scatter plots.

However, there are a few issues that prevent a higher score:
1.  **Species Encoding for Plotting (Critical Issue):** The method used for encoding `Species` labels for the second plot (`pd.get_dummies` followed by `idxmax` and then `map`) is overly complicated and incorrect for simply getting numerical labels. It assumes the species names are 'Setosa', 'Versicolor', 'Virginica' and creates new columns, then tries to map them back. This is not how `get_dummies` should be used for simple categorical encoding like `LabelEncoder` in the ground truth. `LabelEncoder` from `sklearn.preprocessing` is the standard and correct way to encode categorical strings to numerical integers for plotting purposes. The current approach will likely fail or produce incorrect color assignments if the exact column names generated by `get_dummies` are not 'Setosa', 'Versicolor', 'Virginica' or if the mapping doesn't align correctly with the actual labels.
2.  **Missing Colorbar/Legend for Species Plot:** While the K-Means plot inherently has a simple numeric color mapping, the true species plot should ideally have a more informative colorbar or legend that shows the actual species names, as done in the ground truth. The current plot without this is less informative.
3.  **No Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.
4.  **Plot Aesthetics:** The ground truth includes `figsize`, `alpha`, `xlabel`, `ylabel`, and `grid(True)` for better plot aesthetics and readability, which are missing in the generated code. It also includes a more informative colorbar for the K-Means plot.

To improve, the developer should:
*   Replace the manual `pd.get_dummies` and `map` for `Species` encoding with `sklearn.preprocessing.LabelEncoder`.
*   Consider adding a colorbar or legend to the second plot to properly identify the species in the scatter plot.
*   Add basic error handling for file loading.
*   Improve plotting aesthetics (labels, grid, figure size, alpha).",mistral-nemo,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# Load dataset
data = pd.read_csv('iris.csv')

# Create new features
data['PetalArea'] = data['PetalLengthCm'] * data['PetalWidthCm']
data['SepalArea'] = data['SepalLengthCm'] * data['SepalWidthCm']

# Define features and target
X = data[['SepalLengthCm', 'Sepal WidthCm', 'PetalLengthCm', 'Petal WidthCm', 'PetalA...",7,"The generated code correctly performs most of the required steps: loading the data, creating new features, defining X and y, encoding the target, and training a Gradient Boosting Classifier. It also correctly performs 5-fold cross-validation.

However, there are two issues:
1.  **Column Names in X:** The code uses `'Sepal WidthCm'` and `'Petal WidthCm'` to define `X`, which contain spaces. The actual column names in the `iris.csv` dataset are `SepalWidthCm` and `PetalWidthCm` (without spaces). This will cause a `KeyError` when trying to access these columns.
2.  **Reporting the Result:** The final line `cv_score` only returns the value in an interactive environment (like a Jupyter notebook or Python console). For a script, `print(cv_score)` or an f-string would be necessary to actually display the result.

Addressing these two points would make the code fully correct and complete. The indentation and import structure are good.",mistral-nemo,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# 1) Load the dataset
df = pd.read_csv('heart.csv')

# 2) Define features X and target y
X = df.drop('HeartDisease', ...",9,"The generated code is highly correct and complete, successfully addressing almost all requirements of the task.

**Strengths:**
*   **Correct Implementation:** It correctly loads the dataset, defines `X` and `y`, performs one-hot encoding for specified categorical features, scales numerical features, splits the data, trains a `RandomForestClassifier`, and evaluates it using all requested metrics (accuracy, precision, recall, F1-score, and ROC-AUC).
*   **Proper Use of Pipelines and ColumnTransformer:** The use of `ColumnTransformer` and `Pipeline` is excellent for structured preprocessing and model training, preventing data leakage and making the code clean.
*   **Correct Feature Selection**: It correctly identifies the `X` features by dropping `HeartDisease`, and correctly identifies the categorical and numerical features for preprocessing.
*   **Correct Metric Calculations**: All metrics are calculated and printed correctly, including `average='macro'` for precision, recall, and F1-score, and `predict_proba` for ROC-AUC.

**Areas for Minor Improvement (Prevented a perfect score of 10):**
*   **Feature List Discrepancy:** The prompt explicitly listed the features to be used for `X`: `Age`, `Sex`, `ChestPainType`, `RestingBP`, `Cholesterol`, `FastingBS`, `RestingECG`, `MaxHR`, `ExerciseAngina`, `Oldpeak`, `ST_Slope`. The generated code used `X = df.drop('HeartDisease', axis=1)`, which implicitly uses all *other* columns as features. While for *this specific dataset* it might yield the same set of features as the prompt's explicit list, it's a difference in adherence to the exact specification. The ground truth explicitly defined the `features` list. This is a very minor logical deviation as the *outcome* is the same, but for strict adherence to prompt, defining the features explicitly as in the ground truth is preferred.
*   **Error Handling for File Loading:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code lacks this. While not strictly required by the prompt, it enhances code robustness.
*   **`handle_unknown='ignore'` for OneHotEncoder:** The ground truth includes `handle_unknown='ignore'` in the `OneHotEncoder`. While it might not be strictly necessary for this dataset if there are no unseen categories in the test set, it's a good practice for robustness in real-world scenarios to prevent errors during prediction on previously unseen data. This was implicitly part of the prompt's ""comprehensive preprocessing"" requirement.
*   **`stratify=y` in `train_test_split`:** The ground truth includes `stratify=y` in `train_test_split`, which is important for maintaining the class distribution in both training and testing sets, especially with imbalanced datasets. Again, a good practice for robust machine learning code.

Overall, the generated code is excellent and effectively solves the problem with very high quality. The points for improvement are minor details that make the code even more robust and strictly compliant with prompt nuances.",mistral-nemo,LLM_judge_config
H29,hard,Clustering,heart.csv,"Sure, here's the Python code to perform K-Means clustering on the specified numerical features from the heart.csv dataset:

```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Load the dataset
data = pd.read_csv('heart.csv')

# Select numerical features for clustering
selected_features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']

# Standardize the s...",6,"The generated code addresses most of the requirements but has significant flaws and missing components that prevent it from being a good solution.

Here's a breakdown:

**Positive aspects:**
*   Correctly loads the dataset.
*   Correctly selects the specified numerical features.
*   Correctly standardizes the features.
*   Correctly implements the elbow method (WCSS).
*   Applies K-Means with `n_init='auto'` and `random_state=42`.
*   Performs PCA to 2 components.
*   Creates a scatter plot of the PCA components colored by clusters.
*   Includes a title for the PCA plot.
*   Imports necessary libraries.

**Areas for improvement and critical errors:**

1.  **Silhouette Score Calculation:** The most critical error is the incorrect calculation of the silhouette score. The line `print(f'Silhouette score for {k} clusters: {kmeans.score(data[selected_features])}')` is wrong. `kmeans.score()` returns the negative sum of squared distances of samples to their closest cluster center (a measure of inertia, similar to WCSS but not exactly the same), NOT the silhouette score. This directly violates the requirement to use silhouette scores to justify the choice of K.
2.  **Missing Silhouette Plot:** While the elbow plot is generated, a plot for the silhouette scores, as explicitly requested, is missing. This hinders the ""justification"" of K.
3.  **Ambiguous K-Choice Justification:** The prompt specifically asks to ""Use the elbow method (plotting Within-Cluster Sum of Squares - WCSS) and silhouette scores to justify your choice of K."" The generated code plots WCSS, but the explanation for `chosen_k = 3` (which is hardcoded without full justification) and the lack of proper silhouette score calculation/plotting means this part of the requirement is not fully met. A visual inspection of *both* plots is implied for justification.
4.  **Plotting Issues:**
    *   The `plt.scatter` call for the final PCA plot does not use different colors for different clusters (it uses a default colormap, but doesn't explicitly iterate through clusters for clearer visualization and legend if needed).
    *   No legend is added to the final PCA plot, which is crucial for interpreting the cluster colors.
    *   No labels for x and y axes on the final PCA plot (e.g., 'Principal Component 1', 'Principal Component 2') are added.
5.  **Data Modification:** While not strictly an error, modifying the original `data` DataFrame by assigning scaled features back to `data[selected_features]` and adding a 'Cluster' column directly modifies the original DataFrame. It's generally better practice to work with copies or new DataFrames for transformations to keep the original data intact, especially in more complex pipelines.
6.  **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError`, which is good practice for robustness, but missing in the generated code.

The incorrect silhouette score calculation and absence of its plot are major deficiencies, as they directly impact the core task of determining and justifying K.",mistral-nemo,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, classification_report

# Load dataset
df = pd.read_csv('winequality-red.csv', sep=';')

# 1) Create binary target variable
df['quality'] = np.where(df['quality'] >= 7, 1, 0)
y = df['quality']

# 2) Preprocess the data
df.drop('quality', axis=1...",7,"The generated code correctly addresses most of the task requirements.

**Strong points:**
- All necessary libraries are imported.
- Correctly creates the binary target variable.
- Handles data splitting correctly.
- Trains a logistic regression model.
- Evaluates performance using the requested metrics (accuracy, F1-score, classification report).
- Identifies and reports feature importance using model coefficients.

**Areas for improvement to achieve a higher score:**
- **Data Loading:** The `winequality-red.csv` file uses a semicolon (`;`) as a separator, which the ground truth code didn't specify but implies. The generated code correctly assumes `;` which is good foresight. However, the ground truth explicitly handled potential `FileNotFoundError`, which is good practice.
- **Missing Values:** While the dataset typically has no missing values, the generated code uses `df.dropna()`. This is a very aggressive approach if there were many missing values across an entire row, potentially leading to significant data loss. The ground truth demonstrated a more robust, albeit simpler, imputation strategy (median filling, after checking for missing values) which is generally preferred over dropping rows entirely, especially for production code or if the dataset was unknown.
- **Target Variable Separation:** In the generated code, `df.drop('quality', axis=1, inplace=True)` happens *before* `X = df.dropna()`. This means `X` will be a copy of `df` *after* dropping the 'quality' column and then dropping rows with NaNs. This sequence is fine, but the order of operations could be slightly clearer, or `X` could be defined by dropping the 'quality' column directly from the original DataFrame and `y` from the ""target"" column.
- **Scaling and Feature Importance:** The `StandardScaler` is applied, which is correct. When reporting feature importance, it's generally better to print the coefficients with more precision or use `round()` for readability, and also include the absolute coefficients to clearly show magnitude of importance, as the sign indicates direction, not necessarily greater importance. Sorting by absolute coefficient ensures this. The current output of `important_features.sort_values(by='Coefficient', ascending=False)` only sorts by the coefficient value, ignoring the magnitude of negative coefficients.
- **Code Clarity and Comments:** The ground truth code used more comments and print statements to explain each step, which is good for understanding and debugging, especially in a benchmark scenario. While the generated code is functional, adding these could improve readability.
- **`stratify=y` in `train_test_split`:** The ground truth code used `stratify=y` during `train_test_split`. Given that this is a binary classification problem and the target variable distributions can often be imbalanced (e.g., fewer ""good"" wines than ""bad"" ones), using `stratify=y` is crucial to ensure that the train and test sets have representative proportions of each class, leading to more reliable model evaluation. The generated code did not include this, which is a significant omission for robust modeling.",mistral-nemo,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

# 1) Load the dataset
data = pd.read_csv('heart.csv')

# 2) Create a box plot
sns.boxplot(x='HeartDisease', y='Oldpeak', data=data)
plt.show()

# 3) Perform logistic regression
X = data[['Oldpeak']]
y = data['HeartDisease']
lr = LogisticRegression()
lr.fit(X, y)
coef = lr.coef_[0][0]  # coefficient for Oldpeak
odds_ratio = np.exp(coef)  # int...",7,"The generated code largely addresses the requirements but falls short in a few areas related to completeness, robustness, and presentation.

**Strengths:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Data Loading:** Correctly loads the `heart.csv` file.
*   **Box plot:** Successfully generates a box plot using `seaborn`.
*   **Logistic Regression:** Correctly performs logistic regression, extracts the coefficient, and calculates the odds ratio.
*   **Discretization:** Uses `pd.qcut` for discretization into three bins.
*   **Proportion Calculation:** Correctly calculates the proportion of `HeartDisease=1` within each bin.

**Areas for Improvement (Why it lost points):**

1.  **Box Plot Presentation:** While the box plot is generated, it lacks proper titles, labels, and customizations (like `xticks` labels) that would make it more informative and professional, as seen in the ground truth. `plt.show()` is called without `plt.title`, `plt.xlabel`, `plt.ylabel`, etc. which are good practices.
2.  **Interpretation of Logistic Regression:** The ground truth code provides a detailed and well-formatted interpretation of the coefficient and odds ratio, explaining what they mean in practical terms (e.g., ""For every one-unit increase... odds are multiplied by...""). The generated code only prints the coefficient and odds ratio values without any interpretation. This is a crucial part of reporting a model's findings for a data science task.
3.  **Robustness (Error Handling/Data Checks):** The generated code does not include any error handling for `FileNotFoundError` for the CSV or checks for missing values in `Oldpeak` or `HeartDisease`. While not strictly required by the prompt's explicit points, robust code typically includes these. The ground truth demonstrates this.
4.  **Discretization Labels:** `pd.qcut` is used with `labels=False`, which creates integer labels (0, 1, 2) for the bins. While functionally correct for grouping, using descriptive labels like 'Low', 'Medium', 'High' (as in the ground truth) would make the output much more readable and interpretable when displaying the proportions.
5.  **Output Clarity:** The output for the logistic regression and the binned proportions is very minimal. Adding descriptive print statements for each step, and clear headings, would significantly improve the readability and understanding of the analysis, especially for someone evaluating the output. The ground truth provides excellent print statements to guide the user through the analysis steps and results.
6.  **Visualization of Proportions:** The prompt implicitly asks to ""display"" the proportion. While printing is a display, a simple bar plot (as shown in the ground truth) would be a much better way to visualize the proportions across bins. The generated code only prints the `grouped` Series.",mistral-nemo,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"Here's the Python code to build a classification model meeting your requirements:

```python
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder, FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, plot_confu...",8,"The generated code correctly addresses most of the task requirements. It loads the data, correctly handles the `streams` column conversion and NaN values, creates the `is_popular` target variable using the 75th percentile, selects the specified numerical and categorical features, and performs one-hot encoding using `ColumnTransformer` within a `Pipeline`. It then splits the data, trains a Logistic Regression model with `max_iter=1000`, and correctly calculates and displays F1-score (macro), ROC AUC, and the confusion matrix.

Strong points:
- Excellent use of `ColumnTransformer` and `Pipeline` for preprocessing and model training. This is a robust and cleaner way to handle data transformations, especially for mixed data types.
- Correct handling of `streams` to numeric conversion and dropping NaNs.
- Correct calculation of the 75th percentile for `is_popular`.
- Correct feature selection and one-hot encoding.
- All evaluation metrics are calculated correctly.

Areas for improvement:
- **Missing `random_state` in `train_test_split` stratification:** The prompt specifies `random_state=42` for the split but also implies that the split should be representative by showing `stratify=y` in the ground truth code. While not explicitly stated as a requirement for stratification, it's a good practice for classification tasks to ensure balanced class distribution in splits, especially for imbalanced datasets. The generated code does not include `stratify=y` in `train_test_split`.
- **Plotting `confusion_matrix`:** The prompt asked to ""Display the confusion matrix,"" which typically refers to printing the array. The generated code attempts to use `plot_confusion_matrix`, which is deprecated in newer scikit-learn versions (and `ConfusionMatrixDisplay` should be used instead). More importantly, the task asked for displaying the matrix as text, which it also does, but the plotting attempt is an unnecessary divergence from the explicit request and introduces a deprecated function.
- **Import for `plot_confusion_matrix`:** The import `plot_confusion_matrix` is deprecated, and it should be `ConfusionMatrixDisplay` from `sklearn.metrics` for proper plotting.
- **Missing `np` import and usage after import statement:** While `np` is imported, it's not actually used anywhere in the provided code, which is a minor point but indicates it could be removed if not needed.
- **Lack of error handling for file loading:** The ground truth includes a `try-except` block for file loading, which is good practice. The generated code assumes the file exists.

Overall, the core logic for model building and evaluation is sound, and the use of `Pipeline` is commendable. The minor issues are primarily related to best practices (stratification, error handling) and a deprecated function call for plotting.",mistral-nemo,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load data
df = pd.read_csv('spotify_2023.csv')

# 2) Select audio features and drop rows with NaNs
features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%', 'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm']
df = df.dropna(subset=features)

# 3) Standardize features
scaler = StandardSc...",7,"The generated code correctly performs most of the required steps.

**Strong Points:**
*   Correctly loads the data.
*   Correctly identifies and uses the specified audio features.
*   Handles NaNs in the specified features.
*   Correctly standardizes the data using `StandardScaler`.
*   Correctly applies K-Means with the specified parameters (`n_clusters=4`, `random_state=42`, `n_init='auto'`).
*   Correctly applies PCA with the specified parameters (`n_components=2`, `random_state=42`).
*   Generates a scatter plot of PCA components, colored by cluster labels, with a title and labels.
*   Calculates and prints the mean original (unstandardized) features for each cluster.

**Areas for Improvement (Why it didn't get a perfect score):**

1.  **Data for Mean Calculation:** The primary issue is how the mean calculation is handled. The code drops NaNs and then standardizes the *entire* `df` DataFrame based on these features, and *then* adds the cluster labels directly back to the original `df`. This is problematic because:
    *   `df = df.dropna(subset=features)` modifies the `df` inplace, which is fine.
    *   `df_std = scaler.fit_transform(df[features])` creates a NumPy array. The indices of `df_std` are implicitly `0, 1, 2, ...` while the indices of `df` might have gaps due to dropped rows.
    *   `df['cluster'] = kmeans.labels_` directly assigns the `kmeans.labels_` (which have `len(df_std)` elements) to a new column in `df`. This assignment implicitly aligns based on index. If `df` had its 'original' index before `dropna` and now has missing indices, or if `df` was reset index after `dropna`, this could lead to misalignment between the cluster labels and the original features if the `df_std` array implicitly assumes a contiguous 0-based index. In this specific case, `dropna` on a DataFrame keeps the original index for the remaining rows, and `fit_transform` on `df[features]` creates an array `df_std` whose rows *correspond* to the rows of `df` *after* the `dropna`. Then `kmeans.labels_` also correspond to `df_std`. So, assigning `kmeans.labels_` back to `df` *should* work correctly if the index alignment is handled implicitly by pandas, but it's a bit less robust than explicitly making sure the clustering and PCA are done on a clean DataFrame that matches the original data structure chosen for analysis. The ground truth's `df_original_for_analysis = df_features.copy()` approach provides a cleaner separation of concerns: one DataFrame specifically for the actual feature analysis (after NaN drop), and another for the original data.

2.  **Plotting robustness/details:**
    *   The `plt.scatter` call `c=df['cluster']` implicitly uses the default `viridis` colormap (which is good), but it lacks a colorbar, making it harder to interpret which color corresponds to which cluster label visually.
    *   The PCA DataFrame is directly used for plotting without explicit column names which is less readable than `df_pca = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])` from the ground truth.

3.  **Error Handling:** The code lacks `try-except` for `pd.read_csv`, which is good practice for file operations.

Overall, the code is very close and delivers the core functionality. The main point of failure is a subtle issue in how the data is prepared for the mean calculation, specifically around managing the DataFrame state and indices after various transformations and NaN dropping.",mistral-nemo,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"Here is the Python code to accomplish the task. It uses pandas for data manipulation and matplotlib for data visualization.

```python
# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt

# 1) Load data
data = pd.read_csv('spotify_2023.csv')

# 2) Convert `streams` to numeric, coercing errors to NaN
data['streams'] = pd.to_numeric(data['streams'], errors='coerce')

# 3) Filter data and drop rows with NaN `streams`
filtered_data = data[(data['streams'] > 10000000) & (...",8,"The generated code correctly addresses the prompt's requirements. It loads the data, correctly converts the `streams` column to numeric (handling errors), filters the data based on `streams` (greater than 10M) and `released_year` (2010-2023 inclusive), and calculates the average `danceability_%`, `energy_%`, and `valence_%` per year. Finally, it generates and displays the three required line plots with appropriate titles and labels.

**Strong points:**
*   Correct use of `pd.to_numeric` with `errors='coerce'`.
*   Correct filtering logic using boolean indexing and `between()`.
*   Correct dropping of NaN `streams` values.
*   Correct grouping by `released_year` and calculation of means for the specified columns.
*   The plotting correctly uses `subplots` to show all three plots, and appropriately sets titles and axis labels.
*   `plt.tight_layout()` is used for good plot presentation.

**Areas for improvement (minor):**
*   The filtering for `streams` > 10M and `released_year` is combined with `dropna(subset=['streams'])`. While this works, the problem statement explicitly said ""Drop rows with NaN `streams` after conversion"" and then ""Filter for songs with `streams` greater than 10,000,000..."". The ground truth separates these steps, which can sometimes be clearer for debugging or understanding the flow of operations. However, for a concise solution, the generated code's approach is acceptable.
*   The ground truth code includes error handling for `FileNotFoundError` when loading the CSV, which is good practice for robustness. The generated code lacks this.
*   The ground truth includes `print` statements to show the data shape after each filtering step, which is useful for debugging and understanding data reduction, but not strictly required by the prompt.
*   The plots in the generated code do not rotate x-tick labels, which can sometimes lead to overlap if there are many years, although with 14 years (2010-2023), it might not be an issue for standard plot sizes. This is a minor aesthetic choice.
*   The ground truth adds a `marker='o'` to the plots, which the generated code also does. It also uses specific colors and a grid (`plt.grid(True)`), which are minor aesthetic enhancements.

Overall, the generated code satisfies all core requirements of the prompt and is a very good solution.",mistral-nemo,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder

# 1) Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# 2) Clean Likes, Dislikes, and Video views
num_cols = ['Likes', 'Dislikes', 'Video views']
df[num_cols] = df[num_cols].replace(',', '', regex=True).astype(int)
df[num_cols] = df[num_cols].fillna(0)

# 3) ...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being fully correct and robust.

**Issues:**

1.  **Incorrect Cleaning of Numeric Columns (`Likes`, `Dislikes`, `Video views`):**
    *   `df[num_cols] = df[num_cols].replace(',', '', regex=True).astype(int)`: This line attempts to remove commas and convert to `int` in one go. If `pd.to_numeric` (or `astype(int)`) encounters any non-numeric value *after* comma removal, it will raise an error and fail. The prompt specifically asked to use `pd.to_numeric` with `errors='coerce'` to handle non-numeric values by converting them to `NaN`, followed by `fillna(0)`. The generated code's approach is not robust to actual dirty data (e.g., empty strings, text).
    *   `df[num_cols] = df[num_cols].fillna(0)`: This line is problematic because the previous line *already* converted to `int`. If the previous `astype(int)` failed, the entire script would have crashed before reaching this line. If it succeeded, there would be no NaNs left to fill. This logic is out of order and incorrect given the task's specific instruction to fill *resulting* NaNs from `coerce`.

2.  **Incorrect Handling of `published` Column:**
    *   `df['published'] = pd.to_numeric(df['published'])`: The task requested treating the `published` *year* as a numeric feature. The `published` column in the dataset is a full date string (e.g., ""2020-03-01T15:00:00Z""). Converting the entire string directly to numeric will result in `NaN`s for all valid date strings, or a `ValueError` if `errors` is not specified. The correct approach, as in the ground truth, is to convert to datetime and then extract the year.

3.  **OneHotEncoder Usage:**
    *   The `OneHotEncoder` object `oe` is instantiated but then `pd.get_dummies` is used for the one-hot encoding. While `pd.get_dummies` is perfectly acceptable for this task, the `OneHotEncoder` import and instantiation become redundant. If a sparse matrix output or integration with a `ColumnTransformer` were needed, `OneHotEncoder` would be more appropriate, but `get_dummies` is simpler here. However, using `oe` and then not using it directly points to a slight inconsistency.

4.  **Implicit Assumption of Column Existence:** The code does not include any checks (e.g., `if 'column' in df.columns:` to ensure columns exist before performing operations on them. While the prompt assumes the `youtube_trendings.csv` structure, a more robust solution would include such checks, especially for real-world scenarios.

**Areas for Improvement:**

*   **Robust Data Cleaning:** Implement the `pd.to_numeric(errors='coerce').fillna(0)` pattern explicitly for `Likes`, `Dislikes`, and `Video views` to handle non-numeric entries gracefully as specified in the prompt.
*   **Correct `published` Year Extraction:** Convert `published` to datetime objects and then extract the year using `.dt.year`.
*   **Consistency in OHE:** Either use `OneHotEncoder.fit_transform()` or remove the `OneHotEncoder` import/instantiation if `pd.get_dummies` is preferred. `pd.get_dummies` is fine for this task.
*   **Error Handling/Column Existence Checks:** Add `try-except` blocks or `if column in df.columns:` checks for a more robust solution, as done in the ground truth for `FileNotFoundError`.",mistral-nemo,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('youtube_trendings.csv')
df['Video views'] = df['Video views'].str.replace(',', '').astype(float).fillna(0).astype(int)
df['published'] = pd.to_numeric(df['published'])
df['published'] = df['published'].dt.year
annual_stats = df.groupby('published').agg({'Video views': 'mean', 'title': 'nunique'}).reset_index()
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

ax1.plot(annual_stats['published'], annual_stats['Vi...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being fully correct and robust.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly removes commas from 'Video views' and converts to numeric, then fills NaNs with 0.
*   Attempts to convert 'published' to numeric and extract the year.
*   Correctly groups by year and calculates average views and count of unique titles.
*   Creates two subplots with appropriate titles and labels.

**Areas for Improvement/Correction:**
1.  **`published` column handling:** The task specified ""Ensure the `published` column (year) is treated as a numeric or integer type."" The generated code tries `df['published'] = pd.to_numeric(df['published'])` and then `df['published'] = df['published'].dt.year`. The second line `df['published'].dt.year` will only work if `df['published']` is already a datetime object. Based on common dataset structures (like `youtube_trendings.csv` containing `publishedAt` which is a full datetime string), the column named `published` itself is unlikely to be a datetime object directly after loading. It usually refers to the year extracted from `publishedAt`. If `published` *is* just the year, then `pd.to_numeric` is fine, but `.dt.year` will fail. If `publishedAt` is the column with the full timestamp, the code should explicitly convert `publishedAt` to datetime and then extract the year. The ground truth correctly assumes `publishedAt` and handles its conversion to year. Without knowing the exact column name in `youtube_trendings.csv` used for published date (it's often `publishedAt` in these datasets), this part is ambiguous and likely incorrect. The `fillna(0).astype(int)` applied to `Video views` is good, but `astype(int)` might be too restrictive if average views could be fractional, which they are. `float` is more appropriate for `average_views`.
2.  **Column names for `nunique`:** The task specified ""count of unique `Video` titles"". The generated code uses `'title'` in `annual_stats = df.groupby('published').agg({... 'title': 'nunique' ...})`. The dataset's actual column name for the video title is `Video`. This is a hardcoded error based on assuming `title` instead of `Video`.
3.  **`Video views` `astype(int)`:** While `Video views` (raw) might be integers, the average of video views across years will likely be float. Converting the raw `Video views` to `astype(int)` after `fillna(0)` is fine, but the *result* of the average (`'Video views': 'mean'`) should remain float, which it will by default in pandas. However, `pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0).astype(int)` is valid for the raw 'Video views' column, but the average should be float.
4.  **Plotting:** The plot `ax2.plot(annual_stats['published'], annual_stats['title'])` is wrong because the aggregated column is `'title'` after `nunique` but Pandas renames it to `'title'` by default or you would typically rename it (e.g., `unique_videos=('Video', 'nunique')`). The `agg` function would return `title` as the column name, which is okay, but it's a weak point in clarity. The major issue is the assumption that `published` is a simple column that can be directly converted to numeric and then used with `.dt.year`.

These issues make the code either unable to run, or produce incorrect results due to wrong column names/data types.",mistral-nemo,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
import string
from collections import Counter
from nltk.corpus import stopwords

# Step 1: Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Step 2: Create a new feature ""title_word_count""
df['title_word_count'] = df['Video'].apply(lambda x: len(str(x).split()))

# Step 3: Clean and convert ""Video views"" column
df['Video views'] = df['Video views'].replace(',', '', regex=False).astype(float).fillna(0)

# Step 4: Calculate and print Pearson corr...",7,"The generated code addresses most of the requirements but has some critical issues that prevent it from being fully correct and robust.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly calculates `title_word_count`.
*   Effectively cleans `Video views` by removing commas, converting to float, and filling NaNs.
*   Calculates and prints the Pearson correlation coefficient.
*   Correctly processes titles for common word extraction (lowercase, stop words, punctuation removal).
*   Correctly finds and prints the 5 most common words.
*   Uses appropriate libraries (`pandas`, `re`, `collections.Counter`, `nltk`).

**Weaknesses and Areas for Improvement:**

1.  **Missing NLTK Download:** The code assumes `nltk.corpus.stopwords` is available without ensuring it's downloaded. The ground truth includes a `try-except` block to handle this, which is crucial for reproducibility and preventing errors on first run.
2.  **Error in Date Filtering:** The line `df_filtered = df[(df['published').dt.year == 2023) | (df['published'].dt.year == 2024)]` attempts to use `.dt.year` directly on the `published` column without converting it to a datetime object first. This will raise an `AttributeError`. The ground truth correctly converts the column to datetime using `pd.to_datetime`.
3.  **No `try-except` for File Loading:** While the task description implies the file is present, robust code should include a `try-except FileNotFoundError` block for `pd.read_csv`, as demonstrated in the ground truth.
4.  **Redundant `string.punctuation` import (Minor):** While not exactly wrong, `re.sub('[%s]' % re.escape(string.punctuation), ' ', str(x))` is an effective way to remove punctuation, but `re.sub(r'[^\w\s]', '', title)` (used in the ground truth) is often more concise and robust for ""keep alphanumeric and spaces"" scenarios.
5.  **Output Formatting (Minor):** The correlation printout and common words printout are functional but could be more descriptive or formatted for better readability, as done in the ground truth (e.g., `f""{correlation:.4f}""`, and iterating through common words for individual printing).

The date filtering error is a critical bug that would prevent the script from running successfully as intended for steps 5-7.",mistral-nemo,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score
from sklearn.preprocessing import OneHotEncoder

# 1) Load data
df = pd.read_csv('mobiles_2025.csv')

# 2) Clean 'Launched Price (USA)' and create 'is_expensive_usa'
df['Launched Price (USA)'] = (
    df['Launched Price (USA)']
    .str.replace('USD ', '')
    .str.replace(',', '')
    .apply(...",5,"The generated code attempts to address all requirements but falls short in several key areas, leading to incorrect or incomplete data processing and potentially erroneous model training.

**Key Issues (leading to a lower score):**

1.  **Incorrect `Launched Price (USA)` Cleaning and `is_expensive_usa` Creation:**
    *   The `apply(lambda x: np.nan if x == '' else float(x))` for `Launched Price (USA)` might not correctly handle all non-numeric cases, leading to `ValueError` if `float()` is called on non-empty, non-numeric strings (e.g., 'Not Available'). The ground truth's `pd.to_numeric(..., errors='coerce')` is more robust.
    *   The `np.where(df['Launched Price (USA)'] > 1000, 1, 0)` creates `0` for `NaN` values in 'Launched Price (USA)' (which are the unparseable prices), effectively defining non-parseable prices as *not expensive*. The task explicitly states: ""Drop rows where `is_expensive_usa` is NaN."" This implies that unparseable prices should lead to `NaN` in `is_expensive_usa`, which then get dropped. The generated code's logic incorrectly assigns `0` to these before dropping any `NaN`s, violating the task requirement for `is_expensive_usa` NaN handling.

2.  **Incorrect Feature Cleaning Application (Step 3):**
    *   The `df = df.apply(lambda x: cleaned_features[x.name](x) if x.name in cleaned_features else x)` line is problematic. `df.apply()` with `axis=1` (row-wise) or `axis=0` (column-wise, which is default for Series lambda) will apply the function to each *column as a Series*, not replace the column. The lambda functions defined in `cleaned_features` correctly process a Series (e.g., `x.str.replace`), but `df.apply` then tries to re-assemble the DataFrame, which is not what's intended for in-place column modification. This part of the code will likely not clean the columns as intended, leading to the original string columns being passed to the model or type errors later.
    *   For `Front Camera` and `Back Camera`, `str.extract('(\d+)', expand=False)` extracts only integers. The task states ""extract the *first numerical MP value*"", which implies it should handle decimals (e.g., '12.5 MP'). The ground truth uses `re.search(r'(\d+(\.\d+)?)\s*MP', ...)` for this.
    *   The `df = df.replace({'Front Camera': {pd.np.nan: 0}, 'Back Camera': {pd.np.nan: 0}})` should be done *after* string extraction and conversion to numeric. Doing it before might not interact as expected with the `str.extract` method. Also, `pd.np.nan` is deprecated; `np.nan` should be used.

3.  **Incomplete NaN Handling (Step 5):**
    *   The generated code drops NaNs in `X` using `X.dropna(subset=X.columns, inplace=True)` and `y.dropna(inplace=True)` separately. This can lead to misaligned `X` and `y` if NaNs are present in different rows for `X` and `y`. For robust NaN handling, `X` and `y` should be combined (e.g., `pd.concat`) before dropping NaNs, then split again, as demonstrated in the ground truth.
    *   The current approach to dropping NaNs in `X` after one-hot encoding (`X.dropna(subset=X.columns, inplace=True)`) would incorrectly drop rows if *any* of the one-hot encoded columns contain a NaN (which they shouldn't if created correctly, but it's a potential side effect if the input categorical data had NaNs and `dummy_na=False` or if numerical cleaning introduced NaNs). The instruction was to drop NaNs *in features or target*, implying the *cleaned* numerical features, not necessarily the one-hot encoded ones.

4.  **Feature Selection (Step 5):**
    *   `X = df.drop(['Launched Price (USA)', 'is_expensive_usa'], axis=1)`: This drops the original 'Launched Price (USA)' column (which is good) and 'is_expensive_usa' (the target, which is good). However, it implicitly assumes all other columns are cleaned features. It does *not* explicitly select the *cleaned* versions of the numerical features (`Mobile Weight_cleaned`, `RAM_cleaned`, etc.) that would have been created by the `cleaned_features` dictionary (if applied correctly). If the cleaning in step 3 failed, `X` would still contain the original string columns, leading to an error when training the model.

5.  **Missing `Launched Year` inclusion in X:** The generated code does not explicitly include 'Launched Year' in `X`, as required by the task ""using all cleaned numerical features (...) and 'Launched Year'"".

6.  **`drop_first=True` in `pd.get_dummies`:** While often used to avoid multicollinearity, the task did not specify `drop_first=True`. The ground truth used `drop_first=False`, which provides a clearer representation of all categories. It's not strictly an error but a minor deviation from a standard default if not specified.

**Areas for Improvement (if the core issues were fixed):**

*   Adding print statements for debugging and clarity, similar to the ground truth, would be beneficial for understanding data transformation and issues.
*   More robust error handling, like checking for `FileNotFoundError`.
*   Using descriptive column names for cleaned features (e.g., `Mobile Weight_cleaned` used in ground truth) makes the code clearer.
*   Checking for `y_test` having only one class before calculating ROC AUC score, as done in the ground truth, makes the evaluation more robust.",mistral-nemo,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"import pandas as pd
import numpy as np

# 1) Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# 2) Function to clean price columns
def clean_price(col):
    col = col.str.replace(['PKR', 'INR', 'CNY', 'USD', 'AED'], '')
    col = col.str.replace(',', '')
    try:
        return pd.to_numeric(col)
    except ValueError:
        return np.nan

# Apply the function to price columns
price_cols = ['Launched Price (Pakistan)', 'Launched Price (India)', 'Launched Price (China)', 'Launched Price (...",4,"The generated code has several critical errors that prevent it from correctly fulfilling the task requirements.

1.  **Critical Error in `clean_price` function:** The `clean_price` function is fundamentally flawed.
    *   `col = col.str.replace(['PKR', 'INR', 'CNY', 'USD', 'AED'], '')` will raise a `TypeError` because `str.replace` expects a single string or regex pattern as its first argument, not a list.
    *   Even if the `replace` calls were fixed, `apply(clean_price)` on `df[price_cols]` (a DataFrame slice) would apply the function to *each column* as a Series, not element-wise. The current `clean_price` function is designed to take a Series, but its internal logic is incorrect for element-wise string replacement and conversion. The ground truth correctly applies it element-wise using `df[col].apply(clean_price)`.
    *   The `try-except` block for `pd.to_numeric` is placed around the conversion of an *entire Series*, not individual elements. If *any* element in the series fails to convert, it will return `np.nan` for the *entire column*, not just the problematic element. This doesn't handle errors element-wise as required.

2.  **Incorrect NaN Handling in `clean_price`:** The ground truth code specifically handles `NaN` inputs in `clean_price` by returning `np.nan` immediately. The generated code does not have this explicit handling.

3.  **Missing `skipna=True`:** The `mean()` calculation `df[price_cols].mean(axis=1)` implicitly handles `NaN` values by default, similar to `skipna=True`. However, relying on this implicit behavior when the problem statement explicitly mentions ""only consider countries for which price data is available (not NaN)"" is not as robust or explicit as `skipna=True`. While this might not cause a functional error in this specific case, it's good practice to be explicit.

4.  **Error Handling for File Not Found:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is a good practice included in the ground truth.

5.  **`nlargest` vs. `sort_values().head()`:** While `nlargest` can work, `sort_values().head()` is often clearer and more explicitly aligns with the sorting requirement. More importantly, `nlargest` will fail if ""Average Global Price"" column is not fully numeric due to the cleaning errors, potentially leading to a `TypeError`.

6.  **Incomplete Display of Results:** The problem asked to display `Model Name` and `Average Global Price` *sorted by Average Global Price descending*. While `nlargest` provides the top 5, it doesn't guarantee the proper display format (implicitly, it is sorted by that column, but `sort_values` makes it explicit).

The most severe issue is the `clean_price` function, which makes the code non-functional as it is.",mistral-nemo,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# 2) Clean columns
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', '').astype(float, errors='coerce')

# Drop rows with NaN in cleaned columns
df = d...",7,"The generated code correctly addresses most of the task requirements. It loads the dataset, cleans the specified columns by removing specific strings and converting to numeric (handling errors), drops rows with NaNs in these cleaned columns, filters for 'Apple' products, and groups by 'Launched Year'. It then generates box plots for each year with subplots for RAM, Battery Capacity, and Launched Price, setting appropriate titles and y-axis labels.

Areas for improvement:
- **Robust Cleaning**: The cleaning for `Launched Price (USA)` uses `astype(float, errors='coerce')` without stripping whitespace, which might be less robust than directly using `pd.to_numeric` with `errors='coerce'` after stripping and removing common delimiters. The ground truth code handles this by explicitly removing 'USD ' and commas, and then converting to numeric.
- **Handling of 'GB' and 'mAh'**: The cleaning `df['RAM'].str.replace('GB', '')` and `df['Battery Capacity'].str.replace('mAh', '')` might leave leading/trailing spaces if the original string was like ' 16 GB'. The `pd.to_numeric` function handles leading/trailing spaces, but it's good practice to explicitly `str.strip()` for robustness.
- **`astype(float)` for `Launched Price`**: While `astype(float, errors='coerce')` works, `pd.to_numeric(..., errors='coerce')` is generally preferred when converting a series to numeric, as it's more explicit and can handle a wider range of non-numeric values more gracefully by coercing them directly to NaN.
- **Plotting Style**: The code does not set a professional plotting style (e.g., using `plt.style.use('seaborn-v0_8-darkgrid')`) as instructed in the ground truth, which enhances plot readability.
- **Error Handling (File Not Found)**: The code lacks error handling for `FileNotFoundError` when loading the CSV, which is a good practice for robust scripts.
- **Y-axis label for RAM**: The y-axis label for RAM plots is just 'RAM', but should ideally be 'RAM (GB)' as specified in the ground truth for clarity.
- **Clarity in `Launched Price (USA)` plot title/label**: The generated code uses ""Launched Price (USD)"" for the title and ""Price (USD)"" for the label, whereas ""Launched Price (USD)"" is more consistent for both, matching the ground truth.

Overall, the code is functional and largely meets the requirements, but could be improved in terms of robustness, detailed adherence to plotting specifications, and best practices.",mistral-nemo,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"Here's a comprehensive Python code that follows your requirements using the Titanic dataset. I've used `scikit-learn`, `xgboost`, `pandas`, `numpy`, `matplotlib`, `shap`, and `tensorflow` for this task.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, learning_curve, validation_curve
from sklearn.preprocessing import PolynomialFeatures, LabelEncoder, OneHotEnc...",3,"The generated code attempts to address many requirements but falls short in critical areas, leading to incorrect and incomplete implementations.

**Major issues:**

1.  **Preprocessing and Data Leakage:**
    *   The feature engineering is applied *before* the train-test split (e.g., `df['Name'] = df['Name'].str.extract...`). This is a fundamental data leakage issue, as test data is influencing the feature creation.
    *   Polynomial features are fit on the full `df` (e.g., `poly = PolynomialFeatures(degree=2, interaction_only=True); X_poly_train = poly.fit_transform(df[['Age', 'Fare', 'SibSp', 'Parch', 'Fare_band']])`), then concatenated. This again constitutes data leakage.
    *   The `ColumnTransformer` for imputation is used incorrectly. `X = preprocessor.fit_transform(df.drop('Survived', axis=1))` fits and transforms on the whole dataset *before* the train-test split, leading to leakage.
    *   The final `X = pd.concat([X, X_poly], axis=1)` is problematic as `X_poly` is a `DataFrame` of a different length than `X` after transformation if not handled carefully, and it does not maintain original indices. The `ColumnTransformer` should be used *within* a pipeline after a proper train-test split.
    *   Text mining from names is minimal (just extracting titles) and not done through a proper `CountVectorizer` or `TfidfVectorizer` which was implied by the imports. The current implementation just uses `LabelEncoder` on the extracted title, which is not text mining in the context of advanced feature engineering.

2.  **Model Training and Evaluation:**
    *   All `GridSearchCV` instances are fit on the *full* `X` (e.g., `gs.fit(X, df['Survived'])`) before cross-validation for evaluation. This again is data leakage for hyperparameter tuning. The CV should be applied to the training set only, and pipelines are crucial here.
    *   The `learning_curve` and `validation_curve` are called with `X` and `df['Survived']` (the full dataset), which is incorrect. They should be called on `X_train` and `y_train`.
    *   The `validation_curve` is implemented only for `n_estimators`, but the general approach is flawed in not adapting to different model types (e.g., `n_estimators` is not a parameter for SVC or MLP).
    *   The `feature_selection_methods` section is declared but never actually used to compare models or impact. This makes the requirement incomplete.
    *   The `lime_tabular.LimeTabularExplainer` is initialized with `X` (full dataset), and the `keras_model.predict` function for LIME is incorrectly trying to clone and use a `MLPClassifier` with `tf.keras.models.clone_model` and `get_weights()`, which does not work as `MLPClassifier` is from scikit-learn, not Keras. This section is fundamentally broken.
    *   The statistical significance test uses `smm.multipiretest(metrics_df['accuracy'], method='bonferroni')`. This is not a comparison *between* models based on cross-validation scores, but rather an attempt to correct for multiple hypothesis testing on a single aggregated accuracy value, which is not the correct interpretation for comparing model performance. The ground truth used `ttest_rel` on CV scores.

3.  **Imports and Structure:**
    *   Imports like `tensorflow` and `keras` are present, but the neural network model is `MLPClassifier` from `sklearn`, and the `tensorflow.keras` part for LIME is incorrectly implemented for scikit-learn models. If TensorFlow was intended for the neural network, it should have been used directly.
    *   The overall code structure is a linear script rather than organized functions or classes, making it harder to read and debug for a complex task.
    *   Missing `warnings` import and `numpy.random.seed` for reproducibility.
    *   The custom transformer for advanced feature engineering, as required, is missing. The current ""feature engineering"" for name/title is rudimentary and directly modifies the DataFrame.

**Minor issues:**

*   `LabelEncoder` is used on `Name` (which becomes `Title`) directly, but categories like ""Master"", ""Dr"", etc., might have ordinal relationships that are not meaningful, suggesting `OneHotEncoder` might be more appropriate depending on intent.
*   `f1_score` as `score_func` for `SelectKBest` is not appropriate, it should typically be a statistical test like `f_classif`.
*   The `lime_tabular` import itself suggests a path mismatch or a non-standard import as it's typically `lime.lime_tabular`.

The core issue is the consistent data leakage across preprocessing, hyperparameter tuning, and evaluation, which would lead to overly optimistic and unreliable results. The complex requirements, especially around advanced feature engineering, multiple imputation, and proper ensemble/evaluation strategies, are not met accurately.",mistral-nemo,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load dataset
data = pd.read_csv('imdb_top_250.csv')

# Define features and target
X = data[['Year', 'Runtime']]
y = data['Rating']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Linear Regression model
model =...",7,"The generated code correctly implements all the requested steps: loading data, defining features and target, splitting data, training a Linear Regression model, and evaluating it using R-squared and MAE. The imports are correct, and the code structure is clean and follows the logical flow of a typical machine learning pipeline.

However, the code lacks robustness in handling potential data issues. Specifically, it does not include any error handling for file not found errors or warnings/cleaning for the 'Runtime' column, which often needs preprocessing (e.g., stripping ' min' and converting to numeric) to be usable. The ground truth code handles these aspects, making it more production-ready. Without such preprocessing, the model might fail or produce incorrect results if the 'Runtime' column is not purely numeric. It also does not explicitly handle potential non-numeric values in 'Year' or 'Rating' columns. These are minor omissions given the prompt, but crucial for robust code.",mistral-nemo,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 1) Load the dataset
data = pd.read_csv('imdb_top_250.csv')

# 2) Select the features Rating and Runtime for clustering
X = data[['Rating', 'Runtime']]

# 3) Standardize these features using StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4) Apply K-Means clustering to the standardized features
kmeans = KMeans(n_cluster...",7,"The generated code correctly addresses most of the requirements. It loads the data, selects and standardizes features, applies K-Means with the specified parameters, and plots the results. It also attempts to calculate and print the mean rating and runtime per cluster.

However, there are two main issues:
1.  **Data Cleaning:** The most significant omission is the handling of the 'Runtime' column. In typical `imdb_top_250.csv` datasets, 'Runtime' is a string (e.g., ""142 min"") and needs to be converted to a numeric type. The generated code directly selects `['Rating', 'Runtime']` and assumes they are numeric, which will lead to an error or incorrect behavior if 'Runtime' is not cleaned. This indicates a lack of robustness.
2.  **Cluster Means Calculation:** While the code attempts to calculate cluster means, it does so by filtering `X` (the original non-standardized features) using `X_clustered`. This is correct. However, storing the cluster labels directly in the original DataFrame (`data['cluster_label'] = X_clustered`) as done in the ground truth is a more robust and pandas-idiomatic way to manage cluster assignments, especially for subsequent analysis, and makes the grouping step simpler. The direct loop for printing is functional but less concise than `groupby().mean()`.
3.  **Error Handling:** The code lacks basic error handling for `FileNotFoundError` or missing columns, which the ground truth includes.
4.  **Plot Enhancements:** The plot is functional but lacks a legend/colorbar for the cluster labels, which makes it harder to interpret which color corresponds to which cluster. The ground truth adds a colorbar.

To improve to an 8 or higher, the code needs to incorporate robust data cleaning for the 'Runtime' column and ideally, store the cluster labels in the DataFrame directly for easier analysis. Adding `try-except` for file loading would also be beneficial.",mistral-nemo,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import collections

nltk.download('punkt')
nltk.download('stopwords')

# Load the dataset
data = pd.read_csv('imdb_top_250.csv')

# Create title_length feature
data['title_length'] = data['Title'].apply(len)

# Print Pearson correlation coefficient between title_length and Rating
print(data['title_length'].corr(data['Rating'], method='pearson'))

# Preprocess and toke...",7,"The generated code addresses most requirements but has a few minor issues and omissions.

**Strengths:**
*   Correctly loads the dataset.
*   Creates the 'title_length' feature correctly.
*   Calculates and prints the Pearson correlation coefficient.
*   Performs basic title preprocessing (lowercase, punctuation removal, tokenization).
*   Correctly removes stop words.
*   Identifies and prints the 10 most common words.
*   Handles NLTK downloads properly at the beginning.

**Areas for Improvement:**
*   **Punctuation Removal:** The use of `str.translate(str.maketrans('', '', string.punctuation))` is standard for removing general English punctuation marks. However, the ground truth uses `re.sub(r'[^a-z0-9\s]', '', text)` which is more robust as it explicitly whitelists alphanumeric characters and whitespace, removing *any* non-alphanumeric/non-whitespace character, not just those in `string.punctuation`. While `string.punctuation` is often sufficient, the regex approach is more comprehensive as per the prompt's `(characters that are not alphanumeric or whitespace)` specification.
*   **Output Format for Correlation:** The ground truth prints the correlation with a descriptive f-string and formatted to 4 decimal places, which is good practice. The generated code just prints the float directly.
*   **Output Format for Most Common Words:** The prompt strictly asks to print the 10 most common words ""along with their frequencies"" and the ground truth formats this nicely with one word per line, e.g., `'word': count`. The generated code just prints the raw list of tuples from `most_common(10)`, which is less user-friendly.

Overall, it's a solid attempt that gets very close to the requirements, but falls slightly short on the exactness of punctuation removal and the presentation of the output as specified.",mistral-nemo,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Load the dataset
df = pd.read_csv('penguins.csv')

# Define features X and target y
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_l...",7,"The generated code has good structural quality, utilizing pipelines and ColumnTransformer for efficient preprocessing, which is generally a good practice in machine learning workflows. However, it misses two key requirements:

1.  **Label Encoding of Target:** The target variable `y` (`species`) is categorical, and the `RandomForestClassifier` in `sklearn` expects numerical labels (integers) for `y` when performing classification directly. The `cross_val_score` function with a `Pipeline` might handle this internally if the `classifier` step can implicitly convert, but explicitly binarizing or label encoding `y` is best practice, as demonstrated in the ground truth using `LabelEncoder`. The current code passes `y.values` directly, which are string labels.
2.  **Specific Imputation for 'island' after mode imputation:** The task explicitly stated: ""(Note: if a row still has NA in `island` after mode imputation, e.g. if all were NA, it can be dropped, or a placeholder like 'Unknown' can be used if mode is NA itself. For this task, if mode imputation results in NA for island, drop those rows)"". The `SimpleImputer(strategy='most_frequent')` will impute any NA values with the mode. However, if the entire `island` column consists of NA values, `mode()` could potentially be problematic or introduce NAs if it fails. The ground truth code handles this edge case by checking `X['island'].isnull().all()` and then dropping rows. The generated code's pipeline approach doesn't explicitly account for this very specific edge case requirement.

While the pipeline structure is strong, the omission of `LabelEncoder` for `y` and the missing specific handling for the `island` NA edge case (which was explicitly called out) prevent a higher score. If these two issues were addressed, the solution would be very robust.",mistral-nemo,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Select the features and drop NA
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df = df.dropna(subset=features)

# 3) Standardize these features
scaler = StandardScaler()
standardized = scaler.fit...",7,"The generated code correctly performs most of the required steps: loading data, selecting features, dropping NAs, standardizing, K-Means clustering, and PCA. It also generates the two required scatter plots.

However, there are a few areas for improvement, primarily in the robustness and detail of the plotting:

1.  **Plotting by Species:** The `plt.scatter(..., c=species)` line for the species plot will likely throw a `TypeError` if `species` contains string values (which it will, e.g., 'Adelie', 'Gentoo', 'Chinstrap'). Matplotlib's `c` argument expects numerical values for continuous color mapping or a list of colors (which needs mapping), not direct string labels. This requires mapping species names to numerical values or using a more sophisticated plotting library like Seaborn for categorical `hue` mapping.
2.  **Missing `n_init` parameter in K-Means:** The original prompt specified `n_init='auto'` for KMeans, which the generated code correctly includes. (Self-correction: I initially overlooked this in the generated code, but upon re-review, it IS there. My apologies.)
3.  **Plot Aesthetics and Legends:** The plots are very basic matplotlib `scatter` plots. They lack legends for the clusters/species, which makes them less informative than the ground truth. The ground truth uses Seaborn, which is generally better for statistical plots and automatically handles legends when `hue` is used. While not explicitly asked to use Seaborn, ""proper imports"" and visually clear plots are implied.
4.  **Clarity of NA Handling:** The generated code combines feature selection and NA dropping into one step: `df = df.dropna(subset=features)`. While functionally correct, the ground truth separates out the `species` column for alignment during NA dropping, which is a safer approach for ensuring species labels correspond exactly to the cleaned data without accidentally dropping rows missing other columns if `df` had been `df[['species'] + features].copy()`. In this specific example, it works because `df` is the full dataframe, so dropping on `features` is fine, but it's less explicit.
5.  **Re-indexing for Plotting:** The ground truth includes `original_species_labels.reset_index(drop=True)` to ensure alignment with the PCA DataFrame. While it might work without it in simple cases, explicitly resetting the index ensures correct mapping after `dropna`. The generated code assumes direct alignment (`df['species'].values`) after `dropna`, which *should* work if only `dropna` was applied to `df` and no other reordering occurred.

The code otherwise fulfills all major requirements and produces meaningful visualizations. The issues are mainly around robustness for real-world data and presentation quality.",mistral-nemo,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Impute missing values
for col in ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']:
    df[col].fillna(df[c...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being excellent.

**Correctness:**
*   **Imputation Order:** The generated code imputes numerical and 'sex' NAs *before* dropping rows where 'island' is NA. This is a subtle error. If 'island' NA rows are dropped first, the mean/mode calculated for the remaining data will be more accurate as it won't include data points that are about to be discarded. The task specified to drop rows if `island` is NA.
*   **Feature Definition (X):** The `pd.get_dummies` call for `X` is incorrect. It applies one-hot encoding to *all* specified columns, including the numerical ones (`culmen_length_mm`, etc.). This means the numerical features themselves are being one-hot encoded, which is wrong; they should be kept as numerical. `island` and `sex` *should* be one-hot encoded, but not the numerical columns. This misdefinition of `X` means the `GradientBoostingClassifier` is trained on incorrect features.
*   **Target Y Encoding:** The generated code uses the raw `species` column as `y` for the `GradientBoostingClassifier`. While some scikit-learn classifiers can handle string labels, `GradientBoostingClassifier` expects numerical labels (e.g., from `LabelEncoder` or one-hot encoding for multi-label classification). The ground truth correctly uses `LabelEncoder`. This could lead to errors or unexpected behavior depending on the scikit-learn version.
*   **Box Plots:** The box plots are generated correctly for each species and grouped by sex, and titles are added. However, the task asked for *one* plot with all species, using `x='species'`, `y='flipper_length_mm'`, and `hue='sex'`, as in the ground truth. The generated code produces separate plots for each species, which is not what was primarily requested.

**Code Quality & Completeness:**
*   **Imports:** All necessary imports are present.
*   **Structure:** The code is structured logically according to the numbered requirements.
*   **Feature Names for Importance:** The feature importances are printed with their (albeit incorrect due to the `get_dummies` issue) names, which is good.
*   **Error Handling:** Lacks the `try-except` for file loading from the ground truth, but this wasn't explicitly required in the prompt.
*   **Consolidation of Plots:** The task asked ""For each `species`, create box plots... *grouped by the imputed `sex` column*."" The ground truth interpreted this as a single plot showing `flipper_length_mm` vs. `species` with `hue='sex'`, which is a more effective visualization for comparison than separate plots. The generated code's approach of separate plots is not wrong, but doesn't provide the same level of comparison at a glance.

To improve, the code needs to correctly prepare `X` by keeping numerical features numerical and only one-hot encoding categorical ones. It also needs to correctly label encode `y` for the classifier. The imputation order should be adjusted, and the plotting should ideally be a single, consolidated plot as interpreted by the ground truth.",mistral-nemo,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer

# step 1: Load the dataset
df = pd.read_csv('penguins.csv')

# step 2: Impute NA in numerical columns using KNN imputation
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
knn = KNNImputer(n_neighbors=3)
X = df[numerical_cols].values
X_imputed = knn.fit_transform(X)
df[numerical_cols] = pd.DataFrame(X_imputed)

# step 3: Impute NA in 'sex' column with mode
df['sex'].fillna(df['sex'].m...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, performs K-Nearest Neighbors imputation on the specified numerical columns, imputes the 'sex' column with its mode, and verifies the remaining NA values.

**Strong Points:**
*   **Correct KNN Imputation:** Correctly identifies the numerical columns and applies `KNNImputer` with `n_neighbors=3`.
*   **Correct Mode Imputation:** Accurately imputes missing values in the 'sex' column using its mode.
*   **Verification:** Prints the sum of NA values after imputation as requested.
*   **Concise:** The code is clean and to the point.

**Areas for Minor Improvement (preventing a perfect 10):**
*   **Error Handling (File Not Found):** The ground truth includes a `try-except FileNotFoundError` block for reading the CSV, which is robust practice. While not explicitly asked for, it's good defensive programming in production-like scenarios.
*   **Imputation on DataFrame vs. NumPy Array:** The generated code converts the selected columns to a NumPy array (`X = df[numerical_cols].values`) before imputation and then converts the result back to a DataFrame. While this works, `KNNImputer` can directly operate on a DataFrame (or selection of columns), and the `fit_transform` method will return a NumPy array that can be assigned back directly to the DataFrame columns, as shown in the ground truth (`df[numerical_cols_for_knn] = knn_imputer.fit_transform(df[numerical_cols_for_knn])`). The two methods are functionally equivalent for numerical data, but directly operating on the DataFrame slice is often slightly more idiomatic with pandas and `sklearn`.
*   **Informative Prints:** The ground truth includes more verbose print statements (e.g., ""Initial NA values per column"", ""KNN imputation applied..."") which can be helpful for debugging and understanding the flow in a real-world scenario. However, the prompt only explicitly asked to ""Print the number of remaining NA values"".

Overall, the generated code is highly effective and meets the core requirements very well. The minor suggestions are more about robustness and user experience rather than functional correctness for the given task.",mistral-nemo,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load dataset
df = pd.read_csv('penguins.csv')

# Filter dataset to include only 'Adelie' and 'Gentoo' species
df = df[df['species'].isin(['Adelie', 'Gentoo'])]

# Handle NA values in 'culmen_length_mm' column for 'Adelie' and 'Gentoo'
df = df.dropna(subset=['culmen_length_mm'])

# Perform independent two-sample t-test
adelie_lengths = df[df['species'] == 'Adelie']['culmen_length_mm']
gentoo_lengths = df[df['species'] == 'Gentoo']...",9,"The generated code correctly addresses all requirements outlined in the task. It loads the data, filters for the specified species, handles NA values appropriately by dropping rows, performs the independent two-sample t-test, and prints the t-statistic and p-value. It also correctly interprets the p-value against the 0.05 significance level to state whether the null hypothesis is rejected or not.

The code is concise and directly implements the steps. The use of `df = df...` for filtering and NA handling is efficient. The variable names are clear.

Areas for minor improvement (not critical for task completion but enhance robustness and clarity):
1.  **Error Handling for File Not Found:** The generated code assumes the CSV file exists. In a production setting, adding a `try-except` block for `FileNotFoundError` when loading the CSV would make it more robust, similar to the ground truth.
2.  **Function Encapsulation:** While not explicitly requested, encapsulating the logic within a function (as in the ground truth) would make the code more reusable and testable.
3.  **Intermediate Prints:** The ground truth includes helpful print statements about data loading, filtering, and NA handling, which are very useful for debugging and understanding the data flow. The generated code only prints the final results.
4.  **Docstrings and Comments:** Adding comments or a docstring explaining the purpose of sections or the script itself would improve readability, especially for more complex tasks.

Overall, it's a very good solution that perfectly solves the core problem.",mistral-nemo,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, SimpleImputer

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Define features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']]
y = df['speci...",8,"The generated code correctly implements all the required steps for the task, including data loading, feature definition, preprocessing (median imputation for numerical features, mode imputation for 'sex', one-hot encoding for categorical features, and label encoding for the target), data splitting with stratification, training the RandomForestClassifier, and evaluating the model with accuracy and a classification report.

**Strong points:**
*   **Correct Preprocessing:** The code correctly handles null values for numerical features using median imputation and for 'sex' using mode imputation. It also correctly applies one-hot encoding for 'island' and 'sex' and label encoding for the 'species' target.
*   **Correct Data Splitting:** The `train_test_split` is correctly used with `test_size=0.25`, `random_state=42`, and `stratify=y`, adhering to the requirements.
*   **Proper Model Training and Evaluation:** The `RandomForestClassifier` is initialized with the specified parameters and trained correctly. Evaluation metrics (`accuracy_score` and `classification_report`) are correctly calculated and printed.
*   **Clear Variable Naming:** Variable names are descriptive and easy to understand.

**Areas for improvement (to achieve a higher score):**
*   **Slightly Inefficient Preprocessing of `X`:** The in-place modification of `X` with `X[num_features] = imputer_num.fit_transform(X[num_features])` and then `X['sex'] = imputer_cat_sex.fit_transform(X[['sex']])` works but can be less robust than a `ColumnTransformer` or a `Pipeline` for handling different data types and transformations in a more structured way. This approach requires careful management of column order and types, especially after one-hot encoding, as seen with `pd.concat`.
*   **Handling of One-Hot Encoded Column Names:** After one-hot encoding, the new columns created by `encoder.fit_transform` are unnamed. The line `pd.DataFrame(encoder.fit_transform(X[cat_features]))` creates a DataFrame with default integer column names. While this doesn't break the model, it's generally better practice to retain meaningful names for the new features (e.g., `'island_Biscoe'`, `'sex_Male'`). The ground truth code uses `ColumnTransformer` which handles this implicitly when passed to a `Pipeline` and is more production-ready.
*   **Clarity in `X` Manipulation:** The `X = pd.concat(...)` line is a bit dense. While it achieves the desired one-hot encoding and recombination, separating the steps or using a `ColumnTransformer` would enhance readability and maintainability.
*   The `sparse=False` for `OneHotEncoder` is good as it ensures dense arrays, which `pd.concat` handles well. However, this could be less efficient for very sparse categorical features in larger datasets. The ground truth's use of `ColumnTransformer` and `Pipeline` is generally preferred in scikit-learn for robustness and clarity in complex preprocessing chains.",mistral-nemo,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

correlation_experience = df['PerformanceScore'].corr(df['...",10,"The generated code is correct, complete, and of good quality. It successfully defines the DataFrame, calculates the Pearson correlation coefficients between ""PerformanceScore"" and both ""YearsExperience"" and ""TrainingHours"", prints them, and correctly determines and prints which feature has a stronger absolute correlation with ""PerformanceScore"". The use of `abs()` for comparison is correct for determining the strength of correlation. The `method='pearson'` argument is not strictly necessary for `.corr()` as it's the default, so its omission doesn't reduce correctness.",mistral-nemo,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"Here's the Python code that performs the mentioned tasks using the required libraries. I've provided only the code as per your request, but make sure to install the necessary libraries using pip before running the code.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import lifelines
from lifelines import KaplanMeierFitter, CoxPHFitter
from scipy import stats
import statsmodels.api as sm
import pymc3
from pymc3 importMCMC, traceplot

# Load the dataset
titanic_d...",3,"The generated code attempts to address the requirements but fails significantly in correctness and completeness, making it largely unusable for the stated task.

Here's a breakdown of the issues:

1.  **Correctness - Kaplan-Meier (Major Failure):** The prompt explicitly asks for Kaplan-Meier analysis ""treating the voyage as time-to-event data (with survival as event indicator)"". The ground truth correctly interprets this as setting a constant duration (T=1) and using `1 - Survived` as the event indicator `E` because 'Survived' means the event (death) *did not* occur. The generated code uses `kmf.fit(titanic_df['Survived'], titanic_df['Age'], label='Kaplan-Meier')`.
    *   `titanic_df['Survived']` is a binary variable (0 or 1), not a duration.
    *   `titanic_df['Age']` is used as the event indicator, which is nonsensical. Age is a covariate, not an event indicator for survival/death.
    *   This section will produce an error or meaningless output.

2.  **Correctness - Cox PH (Major Failure):** Similar to KM, the `cph.fit(titanic_df, 'Survived', 'Age')` call is fundamentally incorrect.
    *   `titanic_df` (the whole dataframe) is passed as the input for features, which is not how `lifelines` expects it when `duration_col` and `event_col` are specified separately.
    *   `'Survived'` is passed as the `duration_col` which is wrong (it's the event indicator).
    *   `'Age'` is passed as the `event_col` which is also wrong.
    *   The model will not run or will produce errors due to incorrect parameter usage.

3.  **Completeness/Correctness - Advanced Statistical Tests (Missing/Incorrect):**
    *   ""3) Perform advanced statistical tests including multivariate analysis, interaction effects testing, and model diagnostics.""
    *   The generated code uses `cph.check_input()`, `cph.plot_partial_residuals()`, `cph.plot_log_minus_log_hazard_ratios()`, `cph.df_model`. These are `lifelines` utility functions, but their application is predicated on a *correctly fitted* Cox model, which isn't happening.
    *   Actual multivariate analysis (e.g., beyond simple Cox, like regression with multiple predictors) and interaction effects testing (e.g., adding `Age * Sex` terms) are not implemented. The `check_assumption` method (for proportional hazards) is important and missing.

4.  **Completeness/Correctness - Propensity Score Matching (Major Failure):**
    *   The generated code uses `from casise.stats.propensity_score import Match`. `casise` is an obscure library and not standard for PSM in Python; `statsmodels` or `sklearn` combined with `NearestNeighbors` (as in the ground truth) are common.
    *   Assuming `casise` is installed, the usage `Match(titanic_df, treatment='Pclass', outcome='Survived')` is also problematic. PSM aims to balance covariates across treatment and control groups to estimate a treatment effect on an outcome. It requires clear covariates to estimate the propensity score. The ground truth correctly defines covariates and then performs matching. This `casise` usage seems too simplistic or conceptually incorrect for a robust PSM implementation as requested.
    *   No balance checks (e.g., SMD), statistical tests post-matching, or analysis of treatment effects are performed, which are crucial steps in PSM.

5.  **Completeness/Correctness - Bootstrap Resampling (Minor Correctness Issue):**
    *   The `cph.fit(sampled_df, 'Survived', 'Age')` call suffers from the same issues as in point 2. The bootstrap loop itself is conceptually correct but the internal `cph.fit` is broken.
    *   It collects `cph.hazards_` but doesn't actually compute or present confidence intervals from the bootstrapped results, which is the primary goal of bootstrapping.

6.  **Completeness/Correctness - Bayesian Analysis (Major Failure):**
    *   The PyMC3 code `from pymc3 importMCMC, traceplot` is a syntax error (`import MCMC` should be separate or within `__init__.py`).
    *   More critically, `observed = pymc3 nato('observed', torch.distributions.Bernoulli(probs=survival))` is completely incorrect. `pymc3 nato` is not a valid PyMC3 function, and mixing `torch.distributions` within PyMC3 is not standard.
    *   The Bayesian model built (a simple Beta distribution for survival) doesn't use the actual dataset features (`Age`, `Sex`, `Pclass`, etc.) to model survival probability, which was the core request (""probabilistic modeling of survival factors""). It builds a very basic, unconditioned model.
    *   The `pm.sample` call will fail due to the `nato` error.

7.  **Completeness - Model Validation and Sensitivity Analysis (Completely Missing):** This entire section is absent. The prompt specifically asked for:
    *   Model diagnostics (partially attempted by `cph.plot...` but not in substance).
    *   Validation of model assumptions (e.g., proportional hazards assumption, sensitivity to imputation strategy, prior sensitivity for Bayesian model).
    *   Sensitivity analysis (e.g., to imputation, model specification).

8.  **Code Quality:**
    *   Incorrect imports: `from pymc3 importMCMC, traceplot`
    *   Unnecessary `import lifelines` when `KaplanMeierFitter` and `CoxPHFitter` are imported directly.
    *   Lack of data preprocessing (missing values, categorical encoding) for numerical models prior to feeding them to `lifelines` or `pymc3`, which is fundamental for correct results. The ground truth extensively covers this.
    *   No handling of `random_state` for reproducibility in `lifelines` model fitting (though it's less critical for `lifelines` than other libraries).
    *   No suppression of warnings.

In summary, the generated code demonstrates a superficial understanding of the task requirements and frequently misuses library functions or introduces syntax errors. It fails to correctly implement core components like Kaplan-Meier, Cox PH, PSM, and Bayesian analysis with the provided data, and completely misses crucial aspects of model validation and sensitivity analysis.",mistral-nemo,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# create DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
        'Final_Score': [90, 88,...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong points:**
- **DataFrame Creation:** Correctly creates the pandas DataFrame as specified.
- **Pearson Correlation:** Accurately calculates and prints the Pearson correlation coefficient using `scipy.stats.pearsonr`.
- **Scatter Plot:**
    - Correctly generates a scatter plot of 'Attendance_Rate' vs 'Final_Score'.
    - Uses `seaborn.scatterplot` for aesthetically pleasing output and correct coloring by 'Subject' using the `hue` argument.
    - Includes a title, x-axis label, y-axis label, and a legend, as required.
- **Overall Grade Calculation:** Successfully adds the 'Overall_Grade' column using `pd.cut`, which is a clean and efficient way to categorize numerical data into bins. The grade boundaries and labels are correctly applied based on the task description.
- **Grade Distribution:** Correctly prints the count of students in each grade category by using `value_counts()`.

**Areas for minor improvement (to reach a 10/10):**
- **Specific Print Statements:** The ground truth included more descriptive print statements (e.g., ""--- Creating DataFrame ---"", ""DataFrame created successfully:"", ""--- Pearson Correlation Calculation ---""). While not strictly functional for the output, these improve readability for the user running the script, especially in a benchmark scenario. The generated code's print statement for correlation is concise, and for grades, it has ""Grade counts:"", which is good.
- **Plot Customization/Minor Details:**
    - The ground truth set `s=100` and `alpha=0.8` for marker size and transparency, and `edgecolor='w'`. These are minor aesthetic enhancements that make the plot slightly more polished, although not strictly required by the prompt's core visualization requirements.
    - The ground truth used `plt.tight_layout()` and `bbox_to_anchor` for the legend, which can prevent the legend from overlapping with the plot area and ensure all elements fit within the figure, especially important for more complex plots. For this specific plot, it's not a critical omission, but good practice.
    - The ground truth also added a grid, which can aid readability.
- **Order of Grade Counts:** The ground truth sorted the grade counts (`sort_index()`) which could be a subtle improvement for consistent output, though `value_counts()` by default sorts by frequency. The prompt did not explicitly ask for a specific sort order, so this is very minor.

Overall, the generated code met all explicit functional requirements and produced a high-quality visualization and correct data manipulation. The points for improvement are mostly about minor stylistic or best-practice additions rather than functional correctness.",mistral-nemo,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']), 'Ticker': ['TECHCO']*10, 'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0], 'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0], 'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 15...",7,"The generated code correctly performs most of the required tasks: creating the DataFrame, setting the index, calculating daily returns, and calculating both moving averages. The plotting section also correctly generates a plot with all three lines, a title, legend, and labeled axes.

However, there are two key issues that prevent a higher score:
1.  **Plotting**: While the plot is generated with the correct elements, the `plt.show()` command is missing, meaning the plot would not actually be displayed when run in a script.
2.  **Bullish Crossover Identification**: The most significant issue is in the identification of bullish crossovers. The code calculates the boolean condition `df['SMA_3Day'] > df['SMA_5Day'] & df['SMA_3Day'].shift(1) <= df['SMA_5Day'].shift(1)` but then merely uses `df.loc[...]`. This `loc` call *selects* the rows where the condition is true, but it doesn't *print* these dates, which was a specific requirement. Furthermore, the conditional logic `df['SMA_3Day'] > df['SMA_5Day'] & df['SMA_3Day'].shift(1) <= df['SMA_5Day'].shift(1)` has an operator precedence error. The `&` operator has higher precedence than `>` or `<=`. It should be `(df['SMA_3Day'] > df['SMA_5Day']) & (df['SMA_3Day'].shift(1) <= df['SMA_5Day'].shift(1))` to ensure the comparisons are evaluated correctly before the logical AND. As written, `df['SMA_5Day'] & df['SMA_3Day'].shift(1)` would cause an error because you're trying to perform a bitwise AND on pandas series containing float values.

To achieve a higher score, the generated code needs to:
*   Add `plt.show()` after the plotting commands.
*   Correct the logical condition for identifying crossovers by adding parentheses.
*   Explicitly print the dates identified by the crossover logic.",mistral-nemo,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUST0...",8,"The generated code correctly creates the DataFrame, calculates sales per product, generates the horizontal bar chart, calculates the average order value, and identifies repeat customers. The imports are correct, and the code is structured logically.

**Strong points:**
- All 4 main requirements are addressed.
- The DataFrame creation is accurate.
- Revenue calculation and grouping by product for visualization is correct.
- The plot is generated with a title and labels, and `plt.show()` is used.
- Average order value calculation is precise.
- The logic for identifying repeat customers is mostly correct.

**Areas for improvement (to reach a 9 or 10):**
- **Plotting `x` and `y` arguments:** For `product_revenue.plot(kind='barh')`, `product_revenue` is already a Series where the index is 'Product' and values are 'Revenue'. Explicitly setting `x='Product'` and `y='Revenue'` is not necessary and can sometimes lead to errors if not handled correctly. `product_revenue.plot(kind='barh')` is sufficient as it correctly infers axes from the Series. In this specific case, it runs without error, but it's redundant.
- **Repeat Customers Output Clarity:** The way `customers_with_multiple_purchases` is defined using `isin` and then `value_counts()` on this filtered DataFrame is a bit convoluted. While it produces the correct `CustomerID` and counts, it could be simplified. The ground truth's approach of directly using `value_counts()` on the entire `CustomerID` column and then filtering those that are greater than 1 is more direct and cleaner for just getting the counts of repeat customers. The current output prints the series, which is functionally correct but could be formatted more readably (e.g., ""CustomerID: CUST001, Purchases: 3"").
- **Minor plot labels:** While `plt.xlabel('Revenue')` and `plt.ylabel('Product')` are correct, being more descriptive like 'Total Revenue (USD)' for the x-axis would be a minor improvement.

Overall, it's a very good solution that fulfills all the requirements accurately. The minor points are mostly about code elegance and output clarity rather than fundamental correctness.",mistral-nemo,LLM_judge_config
H53,hard,Feature Engineering,,"Here's the Python code that fulfills your requirements:

```python
import pandas as pd
import numpy as np

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 1) Create a new numerical feature ""DeviceType_Num...",9,"The generated code correctly defines the DataFrame, creates the `DeviceType_Numeric` feature, and handles the division by zero for `PagesPerMinute` (though the handling is slightly less robust than the ground truth, it covers common cases). It correctly selects the required features and prints the head of the resulting DataFrame.

**Strong points:**
*   Correctly initializes the DataFrame.
*   Correctly implements the mapping for `DeviceType_Numeric`.
*   Successfully calculates `PagesPerMinute`.
*   Correctly handles `np.nan`, `np.inf`, and `-np.inf` by replacing them with 0 in the `PagesPerMinute` column.
*   Selects the correct feature set.
*   Prints the head of the feature DataFrame as requested.

**Areas for minor improvement:**
*   The `df['PagesPerMinute'] = df['PagesPerMinute'].replace([np.nan, np.inf, -np.inf], 0)` line is concise and works for the given dataset. However, in cases where NaNs might exist in the `PagesViewed` or `SessionDurationMinutes` columns *before* division (leading to NaNs in `PagesPerMinute` that are not due to division by zero), this approach might not distinguish them. The ground truth's two-step approach (`replace([np.inf, -np.inf], np.nan)` first, then `fillna(0)`) is slightly more robust if `NaN`s could originate from other sources besides division by zero, as it explicitly targets only infinities from the division and then handles all NaNs. For this specific problem description, where NaNs would only occur from division by zero or 0/0, the generated code's single line `replace` is effective and sufficient.
*   The print statement could be slightly more descriptive, similar to the ground truth, but this is a very minor stylistic point.",mistral-nemo,LLM_judge_config
H54,hard,Anomaly Detection,,"Here is the Python code that performs the outlier detection for ""TemperatureCelsius"" based on your requirements:

```python
import pandas as pd
from scipy import stats

# Define the DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21....",10,"The generated code correctly defines the DataFrame, calculates the mean and standard deviation for temperature, identifies outliers based on the 1.5xSTD threshold, and prints the specified details for each outlier. It also correctly converts the 'Timestamp' column to datetime objects. The code is clean, readable, and follows best practices. The import of `scipy.stats` is unnecessary as pandas' built-in `mean()` and `std()` functions are used, but it doesn't negatively impact the code.",mistral-nemo,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [1...",10,"The generated code correctly defines the DataFrame, calculates the EngagementScore, and accurately identifies the PostType with the highest average EngagementScore for each platform. The one-liner using `groupby().apply(lambda x: x.loc[x['EngagementScore'].idxmax()])` is a concise and effective way to achieve the desired result for finding the maximum within each group. The output format is also correct, despite being slightly less formatted than the ground truth (no 2 decimal places). Both solutions achieve the same correct output.",mistral-nemo,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""C...",9,"The generated code correctly performs all the requested tasks: defining the DataFrame, calculating `LengthOfStay`, computing average `LengthOfStay` and patient count per diagnosis, printing the summary, and identifying the diagnosis with the longest average stay.

**Strong points:**
- Correctly initializes the DataFrame as specified.
- Accurately calculates `LengthOfStay` in days using `.dt.days`.
- Uses `groupby().agg()` effectively to calculate both the mean `LengthOfStay` and the counts of `PatientID` per diagnosis. The usage of `nunique` for `PatientID` count is a robust choice, although `count` would also work here since `PatientID`s are unique in the base data.
- The printing of the `diagnosis_stats` DataFrame is clear and directly addresses the printing requirement.
- Correctly identifies the diagnosis with the longest average `LengthOfStay` using `idxmax()` and presents the result clearly.

**Areas for minor improvement:**
- While the output for step 3 (`print(diagnosis_stats)`) is correct, the ground truth's formatted print statement makes the output slightly more readable for a human, especially for a benchmark. This is a minor stylistic point.
- The ground truth includes a function wrapper and `if __name__ == ""__main__"":` block, which is good practice for reusability and execution control, but not strictly required by the prompt's simplicity. The generated code directly addresses the prompt's core requirements without this added structure.",mistral-nemo,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"Here's the Python code that fulfills your requirements:

```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8...",10,"The generated code correctly defines the DataFrame, calculates the ""WeightedRating"" precisely as specified in the formula, adds it as a new column, and accurately identifies and prints the ""Title"" and ""WeightedRating"" of the movie with the highest ""WeightedRating"". The code is concise and efficient, leveraging pandas' vectorized operations effectively. All requirements of the task are met, and the code quality is excellent. The output format in the print statement also correctly matches what is expected.",mistral-nemo,LLM_judge_config
H58,hard,Data Aggregation,,"Here's the Python code that fulfills your requirements:

```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000,...",10,"The generated code correctly defines the DataFrame, calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"", and presents this information in a new grouped DataFrame. The use of `agg` with a dictionary for multiple aggregations and `rename` for clear column names is idiomatic pandas. The output matches the expected requirements precisely. The code is clean, concise, and directly addresses all parts of the prompt.",mistral-nemo,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"Sure, here is the Python code that fulfills your requirements:

```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, ...",9,"The generated code correctly performs all the required operations: DataFrame creation, TotalStats calculation, identification of the strongest Pokemon, and average speed calculation by Type1. The code quality is good, using appropriate pandas functions. It misses the initial print statements that the ground truth included, but this is a minor stylistic difference and not a functional error given the prompt's core requirements. The prompt also stated ""Provide only Python code"", and the generated code includes an introductory sentence which should ideally be excluded.",mistral-nemo,LLM_judge_config
H6,hard,Customer Segmentation,,,Error,Failed to generate code,mistral-nemo,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dyst...",7,"The generated code correctly defines the DataFrame and calculates the count of books and average price per author. However, there's a critical error in identifying the author with the most books in stock.

**Primary Issues:**

1.  **Incorrect `CountOfBooks` Calculation:** The generated code uses `CountOfBooks=('Title', 'nunique')`. While `nunique` works in this specific dataset where titles are unique per author, it's generally incorrect for counting *all* books by an author if an author could have multiple distinct book titles, or if the task implies counting rows/BookIDs. `('BookID', 'count')` or simply counting the number of records (`size()`) is the correct approach to count total books (entries) per author. In this specific dataset, `nunique` for 'Title' happens to produce the same result as `count` for 'BookID' because each author's books have unique titles in the provided sample data.

2.  **Incorrect Most Books in Stock Logic (Critical Error):** The line `most_books_author = df.loc[df.groupby('Author')['Stock'].sum().idxmax()]` attempts to get the author with the most stock.
    *   `df.groupby('Author')['Stock'].sum().idxmax()` correctly identifies the author with the highest *sum* of stock. The result here is 'J.R.R. Tolkien'.
    *   BUT, `df.loc[...]` then uses this author's name to *select a row from the original DataFrame `df`*. `df.loc['J.R.R. Tolkien']` will try to find a row where the *index* is 'J.R.R. Tolkien'. Since the original DataFrame's index is numerical (0, 1, 2...), this will raise a `KeyError`.
    *   Even if the index were correct, `df.loc[author_name]` would return all rows associated with that author, not a single summarized row, and certainly not the total stock for that author.
    *   The print statement `print(f""The author with the most books in stock is {most_books_author['Author']} with a total of {most_books_author['Stock']} books."")` would then fail because `most_books_author` is not a single row or Series that directly contains 'Author' and 'Stock' columns in the way expected (it would be a DataFrame if `loc` worked, and then you'd need to access the sum).

**Strengths:**

*   Correct DataFrame definition.
*   Correct use of `groupby()` and `agg()` for average price.
*   Clear printing of `author_books` DataFrame.

**Areas for Improvement to achieve a higher score:**

1.  **Correct `CountOfBooks`:** Change `('Title', 'nunique')` to `('BookID', 'count')` or use `.size()` for the count of books.
2.  **Fix Most Stock Author Logic:**
    *   Calculate `total_stock_per_author = df.groupby('Author')['Stock'].sum()`.
    *   Then, `author_with_most_stock_name = total_stock_per_author.idxmax()`.
    *   And `max_stock_value = total_stock_per_author.max()`.
    *   Finally, print using these two variables.
3.  **Readability/Formatting:** The print statement for author statistics could be formatted more clearly, similar to the ground truth, to align output nicely and specify units (e.g., ""$"").",mistral-nemo,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score, learning_curve, validation_curve
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier, BaggingClassifier, AdaBoostClassifier
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, roc_a...",3,"The generated code attempts to address many aspects of the prompt but fails significantly in execution, leading to incorrect or incomplete implementations for several key requirements.

Primary reasons for low score:

1.  **Incorrect Data Preprocessing:**
    *   `SimpleImputer` is used for imputation, which is not ""multiple imputation strategies"" as requested. The prompt explicitly asked for advanced techniques like MICE (`IterativeImputer`), which the ground truth provided.
    *   Outlier detection/treatment is rudimentary and applied globally before train/test split (`X_no_outliers`), leading to data leakage. The `X_poly` is derived from `X_no_outliers`, meaning data used for polynomial features is already leaked. The ground truth correctly uses a `Pipeline` with `IterativeImputer` and a custom `OutlierCapper` *within* the preprocessing steps, applied after the split or as part of a pipeline.
    *   `PolynomialFeatures` is applied *before* scaling, which is generally not recommended, and potentially before `ColumnTransformer` (which is missing here) for specific feature types. The ground truth correctly integrates `PolynomialFeatures` into a clean preprocessing pipeline *after* scaling, within a `ColumnTransformer`.
    *   Feature scaling (`StandardScaler`) is mentioned in the imports but never applied in the preprocessing steps to `X_train` or `X_test`. This is a critical omission for models like SVM and Neural Networks.

2.  **Incomplete/Incorrect Model Optimization & Evaluation:**
    *   **No proper pipelines for base models:** The training of base models or ensembles (`voting_clf`, `stacking_clf`) does not incorporate the preprocessing steps. This means base models are trained on `X_train` (the raw, poly-transformed, but unscaled and improperly imputed data) instead of truly processed data. The ground truth uses a `ColumnTransformer` to ensure all preprocessing is applied correctly before model training.
    *   **Ensemble methods:** Blending is completely missing. Voting and Stacking are implemented, but the base estimators for ensembles are simply the unoptimized models (`models[name][0]`) rather than the `best_estimator_` obtained from GridSearchCV. This defeats the purpose of optimizing base models.
    *   **Evaluation Metrics:** The `make_scorer(roc_auc_score)` is used in `GridSearchCV`, but for `cross_val_score`, `learning_curve`, and `validation_curve`, the scoring is implicit (default accuracy for classifiers), which is inconsistent and likely not what was intended (ROC AUC was explicitly asked for). The ground truth correctly uses ROC AUC for all relevant score computations.
    *   **Bootstrap Sampling:** The implementation for bootstrap sampling is incorrect. It tries to re-sample `X_train` and `y_train` directly for `cross_val_score`, which doesn't provide confidence intervals on *test set performance* as typically desired. The ground truth correctly resamples the `X_test_processed_df` and `y_test` to get confidence intervals for the chosen metrics.

3.  **Flawed Model Interpretability (SHAP):**
    *   The `shap_values` calculation uses `X.test`, which is syntactically incorrect (should be `X_test`).
    *   The `shap.TreeExplainer` is called on all models, including non-tree models (MLP, SVM), which will either fail or produce incorrect results. The ground truth explicitly selects a tree-based model for SHAP.
    *   `shap.values.mean()` is not a valid SHAP value object for `shap.summary_plot`.

4.  **Incorrect Statistical Significance Testing:**
    *   The paired t-test compares `predict(X_test)` outputs, which are binary class labels, not continuous performance scores. A paired t-test for comparing models typically uses the scores from *each fold* of a cross-validation on the same folds for each model (as implemented in the ground truth). The current implementation is nonsensical for model comparison.
    *   The `if name1 < name2` condition is a poor way to prevent duplicate comparisons and misses the correct way to compare all pairs.

5.  **Missing/Incorrect Model Calibration:**
    *   The `calibration_plot` function is not standard `sklearn`. It seems to refer to an external/custom function (`import mall` - what is `mall`? This import is highly unusual and problematic, suggesting random, non-standard code is being pulled in). The ground truth correctly uses `sklearn.calibration.calibration_curve` and `CalibratedClassifierCV`.
    *   The plotting logic for calibration is incorrect and does not create the required reliability diagram.

6.  **Broken Model Comparison Dashboard:**
    *   `plotly` is imported, but the dashboard requirements include a *comprehensive dashboard with performance metrics, feature importance, and prediction confidence intervals*. The generated Plotly chart is extremely basic and only plots single aggregate values, not the requested confidence intervals or detailed comparisons.
    *   The feature importance is missing (SHAP was not properly integrated into the dashboard).
    *   Prediction confidence intervals are also missing from the dashboard.
    *   The overall structure of the dashboard is not ""comprehensive"" as requested.

7.  **Code Quality Issues:**
    *   `import mall` and `pragma minted` are highly suspicious and indicate potential copy-pasting from untrustworthy sources or incomplete/broken ideas.
    *   Variable `grid_search` is only the last `GridSearchCV` object, leading to incorrect use of `grid_search.best_params_` for all models when it should be `model.best_params_` from the specific `GridSearchCV` instance for each model.
    *   Lack of comments and clear structure for complex sections.
    *   Many parts of the code would likely throw runtime errors or produce meaningless results.",mistral-nemo,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.impute import IterativeImputer, KNNImputer
from imblearn.ensemble import EasyEnsembleClassifier
from sklearn.datasets import make_classification
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.decomposition import PCA
from pyensae import FactorEncoder
...",3,"The generated code attempts to address some parts of the task but fundamentally fails to implement most of the advanced statistical modeling, anomaly detection, and diagnostic requirements as specified.

**Correctness & Completeness Issues:**
*   **Missing Data Imputation:** While it includes `IterativeImputer` and `KNNImputer`, it claims to compare effectiveness by printing means, which is not a robust comparison. MICE via `IterativeImputer` is used, but the crucial aspect of comparing *effectiveness* (e.g., through downstream model performance or distribution analysis) is entirely missing. Also, `pm.MICE` from PyMC3 is not used, which was implied by ""MICE"" in the prompt's context of advanced modeling.
*   **Multivariate Statistical Models:**
    *   **Logistic Regression:** Uses `statsmodels.Logit` which is fine, but the prompt specifically asked for ""logistic regression with regularization"". The provided code does not include any regularization (L1/L2) for `statsmodels.Logit`.
    *   **GAM:** It attempts to use `statsmodels.GLM` as a GAM, but this is incorrect. `statsmodels.GLM` is a Generalized Linear Model, not a Generalized Additive Model (GAM), which usually involves smooth functions of predictors. The prompt specifically requested `pygam` for GAMs.
    *   **Bayesian Logistic Regression with PyMC3:** This implementation is critically flawed.
        *   The model structure `pm.Bernoulli` for observed `y_train` is missing.
        *   `Beta('a', ...)` and `Beta('b', ...)` are priors only for intercept and slope, not for multiple predictors. The code only defines `a` and `b` but doesn't connect them to the features, making the `y_` definition `Cauchy('y', alpha=a, beta=b, observed=y_train)` completely wrong for a logistic regression.
        *   `PyMC3` model (`pm.Model()`) is not instantiated or used correctly with context manager.
        *   The sampling is performed, but without a correct model, it's meaningless. There is no attempt at posterior predictive checks or evaluation.
*   **Hypothesis Testing:** This section is completely missing. The ground truth demonstrates MANOVA, permutation tests, and bootstrap confidence intervals. The generated code includes `pyensae.d–ø–æ—Ç–µct` imports (which seems misspelled and likely a very niche/custom library rather than readily available scientific packages like `scipy.stats` or `statsmodels` for this), but no actual implementation of these tests.
*   **Anomaly Detection:** While it imports `IsolationForest`, `LocalOutlierFactor`, and `OneClassSVM` and fits them, it doesn't perform any evaluation or detailed analysis of the results (e.g., number of outliers, plotting scores, comparing methods).
*   **Dimensionality Reduction:**
    *   **PCA:** Implemented correctly.
    *   **Factor Analysis:** Uses `pyensae.FactorEncoder`, which is again an obscure/non-standard library. `sklearn.decomposition.FactorAnalysis` is the standard library for this, as used in the ground truth.
*   **Robust Statistical Methods:** Entirely missing. The prompt asked to apply robust methods to handle outliers and non-normal distributions (e.g., `statsmodels.robust.robust_linear_model.RLM`).
*   **Model Diagnostics:** Barely touches upon diagnostics. It prints model summaries (`.summary()`), which is a start, but misses specific requirements like residual analysis, influence measures (Cook's distance, leverage plots), and goodness-of-fit tests for logistic regression (e.g., ROC curves, Hosmer-Lemeshow type insights, which aren't direct statsmodels outputs but are standard diagnostics).
*   **Advanced Statistical Visualizations:** Completely missing. The prompt requested Q-Q plots, leverage plots, and diagnostic dashboards. The generated code only prints model summaries.

**Code Quality Issues:**
*   **Imports:** Several imports like `imblearn.ensemble.EasyEnsembleClassifier`, `sklearn.datasets.make_classification`, `pyensae.FactorEncoder`, `pyensae.dkt√≥ols.PCATool`, and `pyensae.d–ø–æ—Ç–µct` are either irrelevant to the specific tasks, obscure/non-standard, or misspelled (e.g., `dkt√≥ols`, `d–ø–æ—Ç–µct`). The task explicitly mentioned standard libraries like `scipy`, `statsmodels`, `pymc3`, `sklearn`, `matplotlib`, `seaborn`, `numpy`, `pandas`.
*   **Structure/Clarity:** The code is quite jumbled, and the final comment openly admits significant missing functionalities, which reflects the poor coverage of the requirements.
*   **PyMC3 Specifics:** `from pymc3 import *` is generally bad practice; it's better to import specific symbols (`pm.Model`, `pm.Normal`, `pm.sample`, etc.).

Overall, the generated code heavily relies on missing or incorrect implementations for core advanced statistical modeling, Bayesian modeling, robust statistical methods, and hypothesis testing, leading to a low score.",mistral-nemo,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.manifold import TSNE
import umap
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from scipy.spatial import distance_matrix
from scipy.stats import zscore
from sklearn.prepro...",2,"The generated code has significant issues in correctness and completeness, rendering it largely non-functional for the specified task.

**Critical Failures:**

1.  **Non-executable/Syntactic Errors:**
    *   `KMeans(n_INIT=10)`: `n_INIT` should be `n_init`. This is a syntax error.
    *   `umap-reduce = umap.UMAP(...)`: Python identifiers cannot contain hyphens. It should be `umap_reduce`.
    *   `optimal_clusters` function:
        *   It expects `algorithm` to be a function that takes `k` and `distance_matrix` and returns labels. The provided `clustering_algorithms` are scikit-learn classes, not functions, and their `fit_predict` method expects data, not a distance matrix.
        *   `gap_statistic(distance_matrix).gap(k)[1]`: `gap_statistic` is not a standard library or `sklearn` function. It's missing a crucial import or implementation.
    *   `distance_matrix` inside `optimal_clusters` is undefined in the calling context. The function signature suggests it should be passed, but the call doesn't seem to account for it correctly. Typically, optimal cluster determination is done on `X_scaled`, not a precomputed distance matrix, unless specified.
    *   `consensus_clustering` function:
        *   `algorithm(bootstrapped)`: Similar to `optimal_clusters`, this assumes `algorithm` can directly take bootstrapped data and return labels, which `KMeans` etc. do via `fit_predict`.
        *   `np.mean(clusters, axis=0)`: Cluster labels are categorical, not numerical, so taking a `mean` of labels is meaningless and incorrect for consensus clustering. Consensus clustering typically involves a co-occurrence matrix and then hierarchical clustering on that matrix, or metrics like Adjusted Rand Index comparing runs.
    *   `cluster_visualization` function:
        *   `px.scatter_3d` is called with `tsne_df`, `umap_df`, `fa_df` but these are 2D arrays (n_components=2). You can't plot `z=2` from a 2D array.
        *   `radar_charts = profiles.loc[:, profiles.columns != 'Contamination Risk (%)'].melt()`: This line is malformed or lacks context from `profiles` output.
        *   `interactive_visualizations = ...`: This is a stub, not implementation.

2.  **Feature Engineering:**
    *   `df['Hardness'] = df['Sulfate'] * df['Ph']`: This is an arbitrary calculation and not a generally accepted water quality index for ""Hardness"". Hardness in water is typically measured as mg/L of Calcium Carbonate and is intrinsic to the dataset, not derived from Sulfate and pH in this manner.
    *   `df['Contamination Risk'] = df[['Sulfate', 'Ph', 'Hardness']].apply(lambda x: np.mean([z for z in x if z<=0]), axis=1)`: This calculation for contamination risk is nonsensical. It only considers values less than or equal to 0, which is unlikely for these parameters, and takes their mean. Contamination risk should be based on known chemical levels related to health guidelines. The ground truth provides a much more sensible approach.
    *   Overall, the feature engineering is extremely simplistic and incorrect for the specified task of ""advanced features including water quality indices, chemical balance ratios, contamination risk scores, and temporal patterns"".

3.  **Missing Functionality/Incompleteness:**
    *   **Imputation:** The ground truth correctly identifies and handles missing values. The generated code drops the `Potability` column but doesn't address the NaNs in the features.
    *   **Optimal Number of Clusters:** The implementation relies on a `gap_statistic` function that is not provided or implemented, making the entire section non-functional.
    *   **Spectral Clustering:** This algorithm is listed in the prompt but entirely missing from the generated code.
    *   **Cluster Stability Analysis:** The `consensus_clustering` implementation is fundamentally flawed (taking `mean` of labels). Bootstrap resampling is mentioned but not fully used for stability analysis (e.g., ARI scores).
    *   **External Validation:** The code comments on external validation but does not implement any validation against the (now dropped) `Potability` column or any other external criteria specific to water quality.
    *   **Comprehensive Visualization Dashboard:** Many parts are missing or would not render correctly as implemented (e.g., 3D plots from 2D data, malformed radar chart section, `interactive_visualizations = ...`).

4.  **Code Quality/Structure:**
    *   Imports are present but some necessary ones for specific functions (like `gap_statistic`) are missing.
    *   Variable names are sometimes misleading (e.g., `distance_matrix` used as a general placeholder for `scaled_df` in function calls).
    *   The code is structured into functions but the calls to these functions are either missing or incorrect, making the flow chaotic. There's no clear execution sequence of all the defined steps.
    *   The `df.drop(['Potability'], axis=1, inplace=True)` without separating `y` for later validation is a poor design choice, as the prompt specifically asks for validation.

In summary, the generated code demonstrates a basic understanding of some required components (imports, defining functions for DR/Clustering), but the implementation details are poor, contain critical errors, and significantly fail to meet the complexity and correctness demanded by the prompt.",mistral-nemo,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
data = pd.read_csv('winequality-red.csv', sep=';')

# Calculate correlations
correlations = data.corr()['quality'].sort_values(ascending=False)

# Top 3 positively correlated and top 3 negatively correlated features
top_pos = correlations.head(3)
top_neg =...",6,"The generated code addresses most of the requirements but has several issues that prevent it from scoring higher.

**Issues:**

1.  **Incorrect CSV Delimiter:** The generated code attempts to load the CSV using `sep=';'`, but the ground truth and standard `winequality-red.csv` typically use a comma (`,`) as a delimiter. This will likely cause a `ParserError` or load the data incorrectly, merging all columns into one. This is a critical error as subsequent operations will fail or produce incorrect results.
2.  **Heatmap and Top 3 Identification:** The code correctly calculates the correlation matrix and identifies the top 3 positive and negative correlations. However, the heatmap generation attempts to mask the upper triangle but doesn't correctly annotate the values, making it less informative than a standard heatmap with values. Also, the `vmax=.3` setting might obscure some correlations if they are higher than 0.3.
3.  **Specific Top 1 Positive/Negative for Scatter Plots:** The task explicitly asked for scatter plots for the *single most* positively and *single most* negatively correlated features. The generated code instead iterates through `top_pos` and `top_neg`, which means it plots *all three* top positive and *all three* top negative features. While not strictly ""wrong"" for generic correlation analysis, it doesn't precisely match the explicit requirement for 'the single most'.
4.  **`numpy` Import Missing:** The heatmap code uses `np.triu` but `import numpy as np` is missing. This will cause a `NameError`.
5.  **`top_neg` Reversal:** `top_neg = correlations.tail(3)[::-1]` is a non-standard way to get the last three. `tail(3)` already returns them in ascending order of correlation (least negative being first), so simply taking `tail(3)` would suffice if the goal is just ""the three smallest values"". If the intent was to sort them from most negative to least negative, then it's correct. However, for identifying the ""first"" (most) negative, `iloc[0]` would still point to the same value regardless of this reversal. This is a minor style point.
6.  **Redundant Plotting Loop:** The logic for plotting scatter plots is duplicated for positive and negative correlations. This could be refactored into a single function for better code quality.
7.  **No `print()` for Top Features:** The output for the top 3 positive and negative features is produced at the very end of the script, rather than immediately after their calculation, which might make the flow less intuitive for a user following the output.

**Strengths:**

*   Uses appropriate libraries (pandas, seaborn, matplotlib, sklearn).
*   Correctly calculates correlations.
*   Correctly uses `LinearRegression` from `sklearn` for fitting the line.
*   Generates R2 and MSE metrics for the linear regression, which is a good addition, though not explicitly requested in the prompt.",mistral-nemo,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
import statsmodels.api as sm
from scipy.stats import ttest_ind, chi2_contingency, mannwhitneyu

# 1) Load the dataset and clean any missing values
df = pd.read_csv('video_games_sales.csv')
df = df.dropna()

# 2) Nintendo vs all other publishers in global sales (t-test)
nintendo = df[df['Publisher'] == 'Nintendo']['Global_Sales']
others = df[~df['Publisher'].isin(['Nintendo']) & df['Year'] > 0]['Global_Sales']
t_stat, p_val = ttest_ind(nintendo, others)
print(f""P-val...",4,"The generated code attempts to address the requirements but falls short in several critical areas, primarily in data cleaning, robustness, completeness, and interpretation.

**Major Issues:**

1.  **Inadequate Data Cleaning:**
    *   `df.dropna()`: This indiscriminately drops *any* row with *any* missing value. The ground truth code correctly identifies specific columns crucial for certain tests ('Global_Sales', 'Year_of_Release', 'Publisher', 'Genre', 'Platform') and drops NaNs only from those. Simply dropping all NaNs can lead to significant data loss if many rows have missing values in non-critical columns, or if a column `User_Score` has many NaNs but isn't used in the specified tests.
    *   'Year' column: The generated code uses `df['Year']` for the Mann-Whitney U test, but the dataset typically has `Year_of_Release`. This would likely cause a KeyError or incorrect results if the column name is not `Year`. The ground truth explicitly handles `Year_of_Release`.
    *   No handling for non-numeric data that should be numeric (e.g., 'tbd' in user score), though this isn't directly used in the hypothesis tests, it's a good practice mentioned in the prompt's context.

2.  **Missing Interpretation of Results:** The prompt explicitly asks to ""Display p-values and interpret results with significance level Œ±=0.05"". The generated code only prints p-values without any interpretation (reject/fail to reject null hypothesis, and the meaning in context). This is a core requirement of hypothesis testing.

3.  **Incomplete/Incorrect Hypothesis Test Implementation:**
    *   **T-test (Nintendo vs. Others):** The `others` group includes `df.Year > 0`. This condition seems arbitrary and unnecessary. The ground truth correctly defines `others` as simply `df['Publisher'] != 'Nintendo'`.
    *   **Safeguards for Tests:** The ground truth code includes checks for `len(group) < 2` before performing t-tests or Mann-Whitney U tests to prevent errors if a group happens to be too small or empty after filtering, which can happen with real-world data. The generated code lacks these crucial checks, making it less robust.
    *   **Chi-square:** While the `pd.crosstab` and `chi2_contingency` are correctly used, the prompt asks for ""focus on top 5 genres and top 5 platforms by count"". The generated code filters `df['Genre']` and `df['Platform']` *before* creating the contingency table, which is correct, but doesn't explicitly print the chosen top categories or handle potential empty contingency tables robustly.

4.  **No Significance Level Defined/Used:** The prompt specifies using `Œ±=0.05` for interpretation, which is completely absent from the generated code's output.

5.  **No Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice. The generated code does not.

In summary, the code provides a barebones attempt at running the statistical tests but severely lacks in data preparation, robustness, specific interpretations, and adherence to all prompt requirements.",mistral-nemo,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder

# Load dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values
df = df.dropna()

# Create binary target variable
df['High_Sales'] = np.where(df['Global_Sales'] > 1....",7,"The generated code successfully completes most of the tasks outlined in the prompt. It loads the data, creates the target variable, encodes categorical features, adds new features, trains a Random Forest model, and evaluates it. It also displays feature importance.

However, there are a few areas for improvement:

1.  **Missing Value Handling:** The generated code uses `df.dropna()`. This is too aggressive and will drop any row with *any* missing value. The ground truth code specifically handles NaNs related to `Global_Sales` (crucial for target), `Year`, `Publisher`, and regional sales, which is a more nuanced and appropriate approach given the task's context (e.g., `Name` or `Developer` NaNs might not necessitate dropping the entire row for this modeling task). This could lead to unnecessary data loss.
2.  **Year Column Conversion:** The generated code doesn't explicitly convert the 'Year' column to integer type after dropping NaNs and before calculating `Years_Since_Release`. While `2023 - df['Year']` might work if 'Year' is numeric (float with no NaNs), it's good practice to ensure it's `int` as per the ground truth for consistency and robustness.
3.  **Feature Selection:** The `X = df.drop...` line in the generated code explicitly drops 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales' from features but then recalculates `Total_Regional_Sales` using them. The prompt implies using the *regional sales* columns as features in addition to the newly created `Total_Regional_Sales`. The ground truth includes all regional sales columns as features, which is consistent with typical modeling practices where individual components and their sum can both be informative. Removing the individual regional sales columns is a deviation from the likely intent.
4.  **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is a good practice for robust code but is missing in the generated code.
5.  **Stratified Split:** The ground truth uses `stratify=y` in `train_test_split`. This is important for classification tasks, especially when the target variable might be imbalanced, to ensure that the proportions of target classes are maintained in both training and testing sets. The generated code misses this.

Despite these points, the core functionality is present, the model is trained correctly, and the evaluation metrics are displayed as requested. The use of `classification_report` is a nice addition. The feature importance display is also well-structured.",mistral-nemo,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.decomposition import PCA
from scipy import stats

# 1) Load the dataset and handle missing values
df = pd.read_csv('video_games_sales.csv')
df = df.dropna()

# 2) Create engineered features
df['Sales_Ratio_NA_EU'] = df['NA_Sales'] / df['EU_Sales']
df['Publisher_Avg_Sales'] = df.groupby('Publisher')['Global_Sales'].transform...",6,"The generated code attempts to address all parts of the task but falls short on several critical aspects, leading to potential inaccurate results and incomplete analysis.

**Areas for Improvement:**

*   **Missing Value Handling (Critical):** The generated code simply uses `df.dropna()`. This is a very aggressive approach that removes all rows with *any* missing values, leading to significant data loss. The ground truth code uses `SimpleImputer` with appropriate strategies (median for 'Year', mode for 'Publisher', 'Genre') which is a much more robust and standard practice in data science. This is a major correctness issue as it fundamentally alters the dataset.
*   **Feature Engineering - `Sales_Ratio_NA_EU`:** The generated code does not handle division by zero or very small numbers, which can lead to `inf` or `NaN` values if `EU_Sales` is zero. The ground truth adds a small epsilon (`1e-6`) to the denominator to prevent this.
*   **Feature Engineering - `Genre_Market_Share`:** The formula `df.groupby('Genre')['Global_Sales'].transform(lambda x: x / x.sum())` calculates the *proportion of a game's sales within its genre's total sales*, not the *percentage of total sales by genre* for the entire dataset. The ground truth correctly calculates this by dividing the sum of genre sales by the total global sales.
*   **Feature Engineering - `Platform_Popularity`:** `df['Name'].value_counts(normalize=True)` calculates the proportion of *each game name* in the dataset, not the number of games per platform. This is a fundamental misunderstanding of the requested feature. The ground truth correctly uses `groupby('Platform')['Name'].transform('count')`.
*   **PCA Output (`print(pca.explained_variance_ratio_)`):** While it performs PCA, it only prints the `explained_variance_ratio_` array. The ground truth plots the cumulative explained variance, which is a standard and much more insightful way to interpret PCA results.
*   **Polynomial Features (`PolyFeatures` column naming):** `df[list(poly.get_feature_names_out())] = poly_features` attempts to add the polynomial features. However, directly assigning to columns not yet present might not work as intended for `DataFrame` assignment if the index doesn't align or if `poly_features` is a numpy array. It's better to create a separate DataFrame for polynomial features and `concat` it (as in ground truth). Also, `get_feature_names_out()` was introduced in scikit-learn 1.0; older versions use `get_feature_names()`, which might cause issues depending on the environment.
*   **Interaction Features (`Year_Genre`):** The generated code uses string concatenation (`df['Year_Genre'] = df['Year'].astype(str) + '_' + df['Genre']`). This creates a categorical feature representing the combination, but the task implies creating numerical interaction terms (e.g., Year * (one-hot encoded Genre)). The ground truth correctly implements the numerical interaction using one-hot encoding for genre and multiplying it by 'Year'.
*   **Outlier Handling:** The generated code only handles outliers for `Global_Sales` and removes the rows. The task asked to handle outliers for *all numerical features* and specified the IQR method which typically involves capping (as done in the ground truth) rather than removal to avoid excessive data loss. The ground truth applies it to relevant numerical features and caps them.
*   **Missing Imports/Best Practices:** The code uses `scipy.stats` but doesn't explicitly use it. There's no initial `verbose` printing of DataFrame info before and after operations, which hinders understanding the data transformation steps.
*   **Code Structure:** Missing `warnings` suppression and proper print statements to indicate progress and state of the DataFrame at various steps.

In summary, while the code attempts to cover all requirements, the implementation for several key steps, especially missing value handling, feature engineering, and outlier treatment, is either incorrect or suboptimal compared to best practices and the specific intent of the task.",mistral-nemo,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load dataset and clean zero values
df = pd.read_csv('diabetes.csv')
df.replace(0, np.nan, inplace=True)

# Median imputation for missing values
df.fillna(df.median(), inplace=True)

# 1) T-test for glucose levels between diabetic and non-diabetic patients
non_diabetics = df[df['Outcome'] == 0]['Glucose']
diabetics = df[df['Outcome'] == 1]['Glucose']
t_stat, p_value = stats.ttest_ind(non_diabetics, diabetics)

# 2) Mann-W...",6,"The generated code addresses most of the task requirements but has several critical flaws that prevent it from being a fully correct and complete solution.

**Strengths:**
*   It attempts to perform all the requested statistical tests.
*   It correctly uses `scipy.stats` functions for the tests.
*   It includes basic data loading and imputation.

**Weaknesses and Areas for Improvement:**

1.  **Improper Zero-Value Cleaning:** The prompt specifically states: ""clean zero values by replacing them with NaN and then median imputation"". The generated code `df.replace(0, np.nan, inplace=True)` replaces *all* zeros in the DataFrame with NaN, including 'Pregnancies' and 'Outcome'. This is biologically incorrect for 'Pregnancies' (0 pregnancies is valid) and semantically incorrect for 'Outcome' (0 means non-diabetic). The ground truth correctly identifies specific columns for imputation. This flaw severely impacts the validity of subsequent tests.
2.  **Incomplete Median Imputation:** After replacing all zeros with NaN, the code simply calls `df.fillna(df.median(), inplace=True)`. This will impute NaNs in *all* columns, including 'Outcome' and 'Pregnancies', which is incorrect. If the previous step had correctly identified columns for NaN replacement, this step would still need to be selective or handle columns differently.
3.  **Missing Interpretation of Results:** The prompt explicitly asks to ""Display p-values and interpret results with significance level Œ±=0.05"". The generated code only prints the p-values without any interpretation (e.g., comparing p-value to alpha and stating conclusion). This is a significant omission.
4.  **Incorrect BMI Categorization for ANOVA:** The prompt specifies concrete BMI categories: ""Underweight: <18.5, Normal: 18.5-24.9, Overweight: 25-29.9, Obese: >=30"". The generated code uses `pd.qcut` which creates quantile-based categories, not fixed ranges. This deviates from the requirement.
5.  **Inconsistent Age Grouping:** The `pd.cut` for age groups uses `[0, 30, 50, np.inf]`. The prompt specifies ""young: <30, middle: 30-50, older: >50"". `pd.cut` with default `right=True` creates intervals like `(0, 30]`, `(30, 50]`, `(50, inf]`. This means 30-year-olds fall into 'Young' and 50-year-olds into 'Middle', which slightly mismatches ""30-50"" range. While subtle, the ground truth uses `right=True` for this, but the prompt's definition of ""<30"" and ""30-50"" (`[)` inclusive/exclusive means the default `right=True` is fine for the high end of each bin, but `0, 29, 50` with upper bound non-inclusive should be considered if strictly following `<30`). The ground truth's `[0, 29, 50, np.inf]` labels young as `(0, 29]`, which corresponds to ""Young: <30"" (if you mean 0-29). The choice of bins is a point of potential ambiguity, but the prompt's language (`<30`, `30-50`, `>50`) usually implies strict inequalities or different boundary handling. The ground truth's definition with bins `[0, 29, 50, np.inf]` for labels `Young`, `Middle`, `Older` is a more precise interpretation. The generated code's `[0, 30, 50, np.inf]` is also reasonable but implies `(0, 30]` for 'Young', which is technically ""less than or equal to 30"" for the upper boundary.
6.  **Lack of Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice. The generated code lacks this.
7.  **No `ALPHA` definition or use:** The generated code does not define the significance level `ALPHA` (0.05) nor use it in any conditional statements for interpretation.

To improve, the generated code needs to fix the data cleaning and imputation logic to be column-specific, implement the result interpretation based on Œ±=0.05 for each test, and correctly categorize BMI according to the specified ranges.",mistral-nemo,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
from sklearn.preprocessing import LabelEncoder, StandardScaler, KBinsDiscretize...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being production-ready or fully correct for the task.

**Strengths:**
*   Correctly loads the dataset.
*   Handles 0s in relevant columns by replacing with median.
*   Creates 'Glucose\_BMI\_ratio' and 'Age\_group' features.
*   Trains and evaluates multiple models.
*   Includes cross-validation.
*   Performs hyperparameter tuning.
*   Generates confusion matrix and ROC curve.
*   Attempts feature importance for tree-based models.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling:** The generated code only replaces 0s for `Glucose`, `Insulin`, `BMI`, `BloodPressure`, and `SkinThickness`. The problem statement implied replacing 0s (which represent missing values in this context) for ALL relevant columns. While the common ones are covered, `Pregnancies` and `DiabetesPedigreeFunction` are often not considered missing when zero, but if the intention was to handle all 0s in numerical features as NaNs to be imputed, a more robust approach is needed as in the ground truth. The manual handling of `Insulin_level` (using `KBinsDiscretizer` with `uniform` strategy) is less appropriate for tertiles than `pd.qcut` which directly creates equal-sized quantiles.
2.  **Preprocessing Workflow:**
    *   **Scaling:** The generated code performs scaling but does not integrate it into a `ColumnTransformer` or `Pipeline` as `StandardScaler` is applied *before* model training, but `LabelEncoder` *after* the train-test split for categorical features. This can lead to data leakage if not handled carefully. More importantly, it scales numerical features *before* encoding categorical features, and it doesn't standardize the *encoded* categorical features or numerical features after the split. The ground truth uses `ColumnTransformer` to apply `StandardScaler` to numerical and `OneHotEncoder` to categorical features *after* the split and before model training within a pipeline, which is the correct and robust way.
    *   **Categorical Encoding:** Uses `LabelEncoder` for `Age_group` and `Insulin_level`. For nominal categories, `OneHotEncoder` (as in the ground truth) is almost always preferred to avoid implying ordinality where none exists, which can negatively impact model performance (especially for non-tree based models like Logistic Regression and SVM). `KBinsDiscretizer` for `Insulin_level` is a less direct way to get tertiles compared to `pd.qcut` and creates integer labels which are then `LabelEncoded`, leading to potential issues without one-hot encoding.
3.  **Hyperparameter Tuning Target:** The geneated code selects the ""best model"" for tuning based on `model.score(X_train, y_train)` which is just the accuracy on the training data. This is prone to overfitting and not good practice. Hyperparameter tuning should ideally be done on a validation set or using cross-validation (which `GridSearchCV` does), but selecting the `best_model` *before* the grid search based purely on training accuracy is flawed. The problem states ""hyperparameter tuning for *the best model*,"" implying selection based on cross-validation performance. The ground truth correctly chooses one model (Random Forest) as a sensible candidate for tuning, not dynamically based on a potentially misleading metric.
4.  **Feature Importance (`X.columns`):** When calculating feature importance, `X.columns` is used. However, after scaling and encoding, the column names of `X_train` and `X_test` will have changed or correspond to numerically scaled/encoded features, not the original `X.columns`. This will lead to incorrect labels for feature importance visualization. The ground truth correctly captures `all_feature_names` after preprocessing and uses them.
5.  **Output Clarity:** The output from the generated code is less structured. The ground truth provides clearer print statements for each step, and better organized test results and visualizations.
6.  **Imports:** Several necessary imports like `ColumnTransformer`, `Pipeline`, `StratifiedKFold`, `ConfusionMatrixDisplay`, `RocCurveDisplay` (for cleaner plotting), `classification_report` are missing, which limits the capabilities and robustness of the solution.
7.  **`SVC(probability=True)`:** While the generated code includes `probability=True` for SVC, it's missing in the ground truth for models dictionary definition and added later for general purpose usage. The ground truth correctly identified `SVC(probability=True)` is not ideal due to computational cost on large datasets, but the task requires ROC AUC for all models, which necessitates it for SVM.
8.  **Reproducibility:** No `random_state` is set globally or for all models (except `train_test_split`), which is crucial for reproducibility in benchmarks. The ground truth uses it consistently.",mistral-nemo,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import resample
import seaborn as sns
import matplotlib.pyplot as plt

# 1) Load and preprocess the dataset
df = pd.read_c...",4,"The generated code attempts to address all requirements but suffers from several critical errors and misconceptions, making large parts of it functionally incorrect or inefficient for the stated task.

Here's a breakdown:

**Correctness (Major Issues):**
*   **Missing Value Handling:** The code attempts to fill missing values with `df.replace('?', np.nan, inplace=True)` and then `df.fillna(...)`. The problem statement specifically says ""Identify columns where 0s represent missing values... Replace 0s with NaN"". The generated code universally fills n/a values based on '?' string which might not be present and completely misses the crucial step of replacing 0s with NaNs for specific columns. The Chinese comments are unhelpful.
*   **Engineered Features:**
    *   `Metabolic_Score`: Uses multiplication (`*`) instead of sum (`+`) which is a different interpretation than the ground truth.
    *   `Risk_Factor_Count`: Calculates `(df[['Glucose', ...]] > df.mean()).sum(axis=1)`. This is a non-standard and less meaningful way to define risk factors compared to using specific medical thresholds as done in the ground truth.
    *   `Pedigree_Age_product`: Uses `PedigreeDiabetesFunction` which is likely a typo for `DiabetesPedigreeFunction`.
*   **Polynomial Features:**
    *   The `FunctionTransformer` usage with `poly.fit_transform` is unnecessarily complex and incorrect. `poly.fit_transform` directly returns the transformed array.
    *   More critically, `pd.concat([df[['Metabolic_Score', ..., 'Outcome']], pd.DataFrame(df_poly)], axis=1)` is disastrous. It completely drops all original features (except the few specified and the `Outcome` column) before concatenation, losing most of the dataset. This makes all subsequent steps operate on a severely truncated and incorrect feature set. The ground truth correctly handles dropping only the original columns that are now represented by polynomial features and then concatenating.
*   **Feature Scaling:** The loop for `scalers` is incorrect. `df = pd.concat([df_scaled, df[['Outcome']]], axis=1)` within the loop means that `df` is overwritten after *each* scaling operation, effectively only retaining the `MinMaxScaler` results at the end, and incorrectly overwriting the original `df` with `df_scaled` (which already dropped the target). This makes the `df` at the end of this step completely different from what it should be (`X_processed` in ground truth). This is a conceptual mess.
*   **PCA and Feature Selection Chaining:**
    *   PCA is applied to `X_train` and `X_test` *before* correlation filtering and before the RFE step. In the ground truth, correlation filtering is applied first, then PCA, then feature selection. This order matters as each step modifies the feature space.
    *   SelectKBest and RFE are applied to `X_train_pca`, which means they are selecting from PCA components, not original features. This is a valid approach, but the request was ""feature selection"" implying from the original (or expanded) feature space, not directly from PCA components. The ground truth applies these selectors to the *correlation-filtered* features, which is a more common and interpretable approach.
*   **SMOTE:** Applied to `X_train_rfe` and `X_test_rfe`. Applying SMOTE to `X_test` is generally incorrect as `X_test` should remain representative of unseen data. SMOTE should only be applied to the training set.
*   **Feature Importance Visualization:** The x-axis labels `plt.yticks(range(X_train_smote.shape[1]), [X_train_smote.columns[i] for i in indices])` will fail because `X_train_smote` is a `numpy.ndarray` due to the previous transformations (PCA, SelectKBest, RFE, SMOTE) and thus does not have `.columns` attribute. It would also be plotting feature importances for PCA components or abstract RFE features, not the original or engineered features, making the visualization uninterpretable in the context of the requested ""top features"".

**Code Quality:**
*   **Readability:** The inline Chinese comments are unexpected and should be in English.
*   **Imports:** Mostly correct, but `FunctionTransformer` is misused and `SMOTE` is not imported from `imblearn.over_sampling`. `resample` from `sklearn.utils` is imported but not used.
*   **Structure:** Attempts to follow the numbered steps, but the implementation within each step is flawed.
*   **Variable Naming:** `df` is repeatedly overwritten in ways that lead to errors or confusion, making it hard to follow the data transformation.

**Completeness:**
*   It *attempts* to cover all points. However, the incorrect implementations mean it fails to *actually achieve* many of the requirements.
*   The ""explain variance ratios"" for PCA is not explicitly printed as requested, only the array `pca.explained_variance_ratio_` is left as the last line.
*   The ground truth also includes steps like explicit warning suppression and robust file loading.

Overall, the generated code demonstrates a superficial understanding of the requested operations but fails significantly in the practical implementation due to numerous logical and syntactical errors, as well as poor design choices in chaining the transformations.",mistral-nemo,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Handle missing values (if any)
df.fillna(df.mean(), inplace=True)

# 2) Western Europe vs all other regions (t-test)
western_europe = df[df['Region'] == 'Western Europe']['Ladder score']
other_regions = df[df['Region'] != 'Western Europe']['Ladder score']
t_stat, p_value = stats.ttest_ind(western_...",3,"The generated code attempts to address the tasks but has significant issues in correctness (handling missing data, incorrect column names, incorrect application of tests, and incorrect interpretation) and completeness (lack of detailed interpretation and use of `alpha` for all tests).

**Specific Issues:**

1.  **Missing Value Handling:** `df.fillna(df.mean(), inplace=True)` is not appropriate for all columns, especially categorical ones like 'Regional indicator'. The ground truth correctly identifies critical columns and drops rows, which is a safer approach for statistical tests that require complete cases.
2.  **Incorrect Column Names:**
    *   Test 2 (Western Europe vs. Other Regions): Uses `df['Region']` which does not exist in the dataset. The correct column is 'Regional indicator'. This will cause a `KeyError`.
    *   Test 3 (GDP per capita top/bottom 25%): The dataset column is 'Logged GDP per capita', not 'GDP per capita' (which might be the raw value or a derived one). The code also tries to `str.replace(',', '')` and `astype(float)` on 'GDP per capita', implying it might be reading a different dataset or assuming a comma-separated string format, which isn't typically the case for numeric columns read directly from CSV. The task specified 'Logged GDP per capita'. The sorting logic for top/bottom 25% is also applied to GDP rank instead of happiness ranks.
    *   Test 7 (Freedom vs. Generosity): Assumes `df['Freedom to make life decisions']` might be a string with commas. The correct column is 'Freedom to make life choices'.
3.  **Incorrect Test Application:**
    *   Test 4 (ANOVA): `stats.f_oneway(*regional_indicators.T)` is fundamentally incorrect. ANOVA tests if the *means of a continuous variable* differ across *categories of a categorical variable*. Here, `regional_indicators` is a DataFrame of multiple *continuous* columns, and the ANOVA is being applied across these *continuous variables* themselves, not based on 'Regional indicator' categories. The correct approach would be to group 'Ladder score' by 'Regional indicator'.
    *   Test 7 (Freedom vs. Generosity): The task specifies ""countries with above-median freedom scores have significantly different generosity levels"". The generated code computes `top_freedom` and `bottom_freedom` based on `Freedom_decision_score` but then tests `Generosity` levels, which is correct in principle, but the preceding data type conversion errors would prevent it from running.
4.  **Incomplete Interpretation and Alpha Usage:** The generated code only prints p-values. It does not provide the required interpretation of results based on `alpha=0.05` for each test, which is a core requirement of the prompt. It also does not define or use the `alpha` variable.
5.  **Misc. Errors/Suboptimal Choices:**
    *   Test 3: The sorting and selection of top/bottom 25% for GDP based on GDP rank is incorrect. The prompt clearly states ""difference in GDP per capita between the top 25% happiest countries and the bottom 25%"". This means ranking by happiness score, not GDP.
    *   Test 6 (Chi-square): `pd.crosstab(df['Happiness category'], df['Region'])` again uses the incorrect column name 'Region' instead of 'Regional indicator'. 'pd.qcut' creates quantiles but the ground truth's `categorize_happiness` ensures roughly equal sized groups for 'Low', 'Medium', 'High' based on percentiles, which is more robust.
    *   Unnecessary Imports: `matplotlib.pyplot` and `seaborn` are imported but not used.

Overall, the code structure shows an attempt to address each part of the prompt, but the frequent errors in column names, data preprocessing assumptions, and incorrect application of statistical tests, combined with the lack of detailed interpretation, render it mostly non-functional and incomplete for the given task.",mistral-nemo,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

# 1) Load the dataset and preprocess it by handling missing values and encoding cate...",4,"The generated code has several critical issues that make it largely incomplete and incorrect for the given task.

**Correctness:**
- **Missing Data Handling:** The `data.fillna(data.mode().iloc[0], inplace=True)` line is a very naive and potentially harmful way to handle missing values, especially in a dataset like this. It fills all columns with the mode of the first row, which is nonsensical and likely to introduce significant errors. The ground truth explicitly drops rows with missing values in the relevant columns, which is a more appropriate strategy for this specific dataset and task.
- **Incorrect Feature Names:** The generated code uses `'GDP per capita'`, `'Freedom'`, and `'Perceptions of corruption'` which are NOT the actual column names in the `world-happiness-report-2021.csv` file. The correct names are `'Logged GDP per capita'`, `'Freedom to make life choices'`, and `'Perceptions of corruption'`. This fundamental error means the model is training on non-existent columns, leading to a `KeyError` at runtime. This alone makes the code non-executable as intended.
- **Categorical Variable Handling:** The task specifies encoding categorical variables, but the `LabelEncoder` is used for a non-existent 'Region' column and is ultimately unnecessary as the selected features are all numerical. Also, the `world-happiness-report-2021.csv` does not have a 'Region' column, it has 'Regional indicator'. Even if it were used, Label Encoding is rarely appropriate for features in regression tasks as it imposes an arbitrary ordinal relationship. The ground truth correctly identifies that no encoding is needed for the *selected features*.
- **Hyperparameter Tuning:** It hardcodes `best_model = Ridge()` without any logic to determine which model was actually best, and then only tunes `Ridge`. The task explicitly asks to tune the *best model* found in step 4.
- **Feature Importance:** It tries to plot feature importance from a `Ridge` model, which does not have a `feature_importances_` attribute. This would cause an `AttributeError`. Feature importance is typically associated with tree-based models like Random Forest or Gradient Boosting. The ground truth correctly checks for `hasattr` before attempting to plot.
- **Residual Plot Error:** The residual plot `plt.scatter(y_test, y_test - y_pred)` uses `y_pred` which is the prediction from the *last* model trained in the loop (Gradient Boosting Regressor), not necessarily the best or tuned model, and it's not explicitly defined outside the loop for plotting in the context of the tuning step.
- **Missing Task Requirements:** Completely omits cross-validation (step 8 in prompt) and does not display all model evaluation metrics in a structured way (only prints them in the loop).

**Code Quality:**
- **Imports:** Missing `seaborn` (for better visualizations) and `KFold`, `cross_val_score`, `StandardScaler`, `ColumnTransformer`, `Pipeline` (all used in ground truth for robustness and completeness).
- **Structure:** Lacks clear separation of steps, printing is very basic and uninformative compared to the ground truth. No `try-except` for file loading.
- **Reproducibility:** No `random_state` for `Ridge` or `Lasso` models.

**Completeness:**
- **Missing Cross-validation:** Task 8 is completely absent.
- **Incomplete Evaluation:** Only prints metrics during training, doesn't provide a summary table as the ground truth does.
- **Incomplete Visualizations:** Only attempts one residual plot (with potential error) and one feature importance plot (with definite error), lacking the full scope or robustness of the ground truth.
- **Prediction Analysis:** The analysis of hypothetical country predictions is missing (only prints the prediction).

In summary, the generated code doesn't execute end-to-end without errors due to fundamental issues with column names and attribute access, and it fails to implement key requirements of the prompt, most notably cross-validation and robust hyperparameter tuning based on actual performance.",mistral-nemo,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mutual_info_regression
import seaborn as sns
import matplotlib.pyplot as plt

# 1) Load and preprocess the dataset
df = pd.read_csv('...",4,"The generated code attempts to address all requirements but suffers from several critical issues that make it largely incorrect or unusable for the stated task.

**Primary Reasons for Failure/Incorrectness:**

1.  **Incorrect Column Names for Feature Engineering:**
    *   `df['Happiness_Efficiency'] = df['Ladder score']/df['GDP per capita']`: The GT uses 'Logged GDP per capita', while the generated code uses 'GDP per capita'. The provided dataset `world-happiness-report-2021.csv` contains 'Logged GDP per capita', not 'GDP per capita'. This leads to a `KeyError`.
    *   `df['Social_Wellness_Index'] = df['Social support']*df['Healthy life expectancy']`: The GT uses addition (`+`) for combination, reflecting a combined wellness *index*. Multiplication changes the interpretation significantly and may produce values outside a reasonable range of an ""index.""
    *   `df['Governance_Score'] = df['Freedom']*df['Corruption']`: The GT computes ""low corruption"" as `(1 - Perceptions of corruption)` and then adds it to freedom. The generated code multiplies `Freedom` (column name 'Freedom to make life choices' in the dataset) and `Corruption` (column name 'Perceptions of corruption' in the dataset), which is not only using incorrect column names but also an incorrect mathematical operation for ""low corruption"" and combining factors. This also leads to `KeyError` as 'Freedom' and 'Corruption' columns do not exist.
    *   `df['Regional_Happiness_Rank'] = df.groupby('Region')['Ladder score'].rank(method='first')`: The dataset uses 'Regional indicator', not 'Region'. Leads to `KeyError`.
    *   `df['GDP_vs_Regional_Average'] = df['GDP per capita'] - df.groupby('Region')['GDP per capita'].transform('mean')`: Again, `GDP per capita` and `Region` are incorrect column names.

2.  **Incorrect Data Handling for Scaling and Dimensionality Reduction:**
    *   **Scaling:** The generated code applies `scaler.fit_transform(df)`. This attempts to scale the *entire DataFrame*, including non-numeric columns like 'Country name', 'Regional indicator', and the target 'Ladder score'. This will raise errors (e.g., `ValueError: could not convert string to float: 'Western Europe'`) or produce nonsensical results when non-numeric columns are present. Scaling should only be applied to numerical features, typically excluding the target variable.
    *   **PCA/t-SNE:** Similarly, `pca.fit_transform(df)` and `tsne.fit_transform(df)` are applied to the entire DataFrame, which includes non-numeric columns. This will cause `ValueError` for the same reason. These operations should be performed on a feature matrix (X) that is purely numeric and ideally scaled.
    *   **Missing `X` and `y` separation**: The generated code does not clearly separate features (X) and target (y) for subsequent modeling and selection steps, making it prone to errors.

3.  **Incorrect Use of Feature Selection/Importance Modules:**
    *   **`SelectKBest` and `RFE` input:** Both `fit = select_k.fit(df, df['Ladder score'])` and `fit = rfe.fit(df, df['Ladder score'])` are incorrect. They pass the *entire DataFrame* `df` (including target, non-numeric columns) as features (`X`) to these functions. This is fundamentally wrong. `X` should only contain features, and `y` should be the target.
    *   **`RFE` Estimator:** `rfe = RFE(RandomForestClassifier(n_estimators=100), n_features_to_select=5)` uses `RandomForestClassifier`. The task is a regression task (predicting 'Ladder score', which is continuous), so a `Regressor` model (like `RandomForestRegressor` or `LinearRegression` as in GT) should be used.
    *   **Mutual Information Scoring:** The generated code uses `mutual_info = mutual_info_regression(df, df['Ladder score'])`, again passing the *entire DataFrame* `df` as features. This is incorrect.
    *   **Random Forest Importance:** `model.fit(df.drop('Ladder score', axis=1), df['Ladder score'])` for feature importance is better, but it's still missing the necessary preprocessing (handling non-numeric columns, scaling issues as mentioned above). Also, it uses `RandomForestClassifier` for a regression task.

4.  **Incomplete/Missing Steps and Robustness:**
    *   No handling for potential `NaN` values resulting from engineering features (e.g., division by zero leading to `inf`).
    *   No validation or print statements for specific feature selection results (e.g., list of selected features for RFE, full mutual information scores as requested).
    *   The polynomial feature application `poly_df = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(['Ladder score', 'GDP per capita']))` creates a new DataFrame but doesn't integrate it back into the main `df`, rendering it useless for subsequent steps.

5.  **Small Logical Errors/Suboptimal Choices:**
    *   The `dropna()` call at the beginning `df = df.dropna()` might remove too many rows and is less robust than focused imputation strategies (like filling specific NaNs with means).
    *   The generated code did not visualize `PCA` and `t-SNE` results with `hue=y` which is a key part of ""visualization"" for these dimensionality reduction techniques.
    *   The `corr_matrix` calculation for ""correlation-based feature filtering"" is incorrect (`np.corrcoef(df.columns, df.corr()['Ladder score'].values)[-1]`) and will error out. It should compute correlations between features, not between column names and a Series. The GT correctly implements this by identifying highly correlated *features among themselves*.

In summary, the code demonstrates an understanding of the components required, but the implementation is deeply flawed with incorrect variable usage, mismatched column names, and improper application of scikit-learn functions to the `DataFrame`, rendering it non-functional or misleading in many critical parts.",mistral-nemo,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Load dataset, parse 'date', and filter for 'location' == 'USA'
data = pd.read_csv('synthetic_covid19_data.csv')
data['date'] = pd.to_datetime(data['date'])
usa_data = data[data['location'] == 'USA'].set_index('date')

# Resample 'new_cases' to weekly frequency
weekly_cases = usa_...",7,"The generated code correctly addresses most of the requirements of the task, demonstrating an understanding of time series analysis concepts from loading data to decomposition and stationarity testing.

**Strong Points:**
*   Correctly loads, parses, and filters the data.
*   Correctly resamples to weekly frequency and sums 'new_cases'.
*   Successfully performs and plots time series decomposition.
*   Applies the ADF test and correctly checks for non-stationarity.
*   Performs first-order differencing if non-stationary and re-tests.

**Areas for Improvement:**
*   **Missing Plots for Stationary/Differenced Series:** The task explicitly asks to ""Plot the ACF and PACF of the (potentially differenced) stationary time series."" The current code unconditionally plots ACF/PACF of `weekly_cases_diff` without checking if `weekly_cases` was already stationary, or if `weekly_cases_diff` was actually generated. The plotting should be conditional on which series (`weekly_cases` or `weekly_cases_diff`) is determined to be stationary.
*   **Incomplete ADF Test Output:** The prompt asks to ""print the test statistic and p-value."" While the code does print these, it misses the critical values, which are also part of `adfuller`'s output and are crucial for interpreting the ADF test results.
*   **Plotting `plt.show()`:** The decomposition plot (and later ACF/PACF plots) are generated, but `plt.show()` is only called once at the very end. For multiple distinct plots, it's generally better practice to call `plt.show()` after each plot or group of related plots to ensure they display sequentially or in separate windows, especially in an interactive environment. In a script, waiting until the end is acceptable but can lead to all plots appearing simultaneously.
*   **Robustness:** The code assumes the `synthetic_covid19_data.csv` file exists. It lacks basic error handling for file not found or empty data, which is good practice for production-ready code (though perhaps not strictly required by the prompt's ""Python code without explanations"" clause, it's a quality aspect).
*   **Plot Titles:** The plots generated by `seasonal_decompose` have default titles. ACF/PACF plots lack custom informative titles, which could be improved for clarity.
*   **Lags for ACF/PACF:** The calculation `int(np.log(len(weekly_cases_diff))//2.5)+1` for lags is quite unusual. While a calculation for lags is necessary, it's not a standard heuristic. A common rule of thumb is `min(40, N/2 - 1)` or `min(10*log10(N), N/2 - 1)`. The current calculation might result in very few lags for longer series.
*   **Implicit Variable Use:** If the original `weekly_cases` is stationary, `weekly_cases_diff` might not be created or might only exist if the `if adf_test[1] > 0.05` block is entered. The plot section then unconditionally uses `weekly_cases_diff`, which could lead to a `NameError` or incorrect plotting if the original series was stationary. A `stationary_series` variable should be used and updated based on the ADF test outcome.",mistral-nemo,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Categorize wines based on 'pH' values
df['pH_group'] = pd.cut(df['pH'], [0, 3.2, 3.4, 5], labels=['Low pH', 'Medium pH', 'High pH'])

# Calculate average 'quality' for each pH group
group_averages = df.groupby('pH_group')['quality'].mean()
print(group_averages)

# Perform ANOVA test
f_value, p_value = stats.f_oneway(df[df['pH_...",6,"The generated code partially addresses the requirements but has significant shortcomings, particularly in the visualization and confidence interval calculation.

**Correctness:**
- **Categorization:** Correctly categorizes pH groups, although the upper bound for the 'High pH' group is an arbitrary 5 instead of `np.inf` or a higher practical limit. The labels are also simplified compared to the ground truth.
- **Mean Calculation:** Correctly calculates the mean quality per group.
- **ANOVA Test:** Correctly performs the ANOVA test.

**Code Quality:**
- Imports are present.
- The structure is linear and easy to follow for the given task.

**Completeness:**
- **Confidence Intervals (Missing/Incorrect):** This is the biggest flaw. The request specifically asked for ""error bars representing 95% confidence intervals"" and ""confidence intervals"".
    - The generated code uses `stats.sem(df['quality'], ddof=0) * stats.t.ppf(1.0 - 0.05/2, len(df)-1)` for `yerr`. This calculates a *single* standard error of the mean for the *entire dataset's quality*, then scales it. It does **NOT** calculate a confidence interval for *each pH group's mean*. This results in incorrect and misleading error bars on the plot and fails to meet the specific requirement for confidence intervals per group.
    - There is no explicit calculation or reporting of the confidence intervals as requested.
- **Reporting:** Only prints the group averages and ANOVA results; it doesn't explicitly report the confidence intervals as a table or structured output.
- **Visualization:**
    - The error bars are incorrect as explained above.
    - The labels are simplified, which impacts readability.
    - It lacks customization (title, axis labels, limits, grid) that would make it more informative and professional, as demonstrated in the ground truth.
    - It uses `group_averages.plot()`, which is a quick way to plot but offers less control over error bar specifics and general plot customization compared to `plt.bar()`.

To improve, the code needs to:
1. **Accurately calculate confidence intervals for each pH group's mean.** This involves calculating SEM and then the t-interval for each group individually, not for the entire dataset.
2. **Use these per-group confidence intervals for the error bars in the visualization.**
3. **Explicitly report the calculated confidence intervals** (e.g., in a printed table).
4. **Improve the visualization's fidelity** with more specific error bars, more descriptive labels, and plot enhancements.
5. Consider using `np.inf` for the upper boundary of the highest pH bin for more robust categorization.",mistral-nemo,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load the dataset
data = pd.read_csv('synthetic_covid19_data.csv')
data['date'] = pd.to_...",7,"The generated code correctly addresses most of the requirements. It loads data, parses the date, selects features and target, imputes missing values, and trains both Linear Regression and Random Forest models, evaluating them with the specified metrics.

However, there are a few issues:
1.  **Handling of Missing Values with `fillna().median()`**: The `fillna` operation is applied to the entire `data[features + [target]]` slice, which includes the 'continent' column. While `median()` would likely skip non-numeric columns, it's generally better practice to specifically impute numerical columns with the median as shown in the ground truth. This is a minor point, as `median()` won't work on 'continent' anyway, but it could lead to confusion or silent errors in different contexts.
2.  **One-Hot Encoding and Feature Reconstruction**: The way one-hot encoding is handled, especially when reconstructing `X` after applying `ColumnTransformer`, is overly complex and prone to errors.
    *   `ct.fit_transform(data[features])` will return a NumPy array.
    *   `X = pd.concat([pd.DataFrame(X[:, :-data[features].shape[1]], columns=ct.named_transformers_['one_hot'].get_feature_names_out()), data[features].drop('continent', axis=1)], axis=1)` is an extremely complicated and potentially fragile way to reconstruct the DataFrame, especially if the order of transformed columns isn't exactly as expected or if `remainder='passthrough'` is used. The ground truth's `pd.get_dummies` is much simpler and more robust for this specific task where only one categorical column needs encoding.
3.  **Scaling and Data Leakage**: The scaling is performed on `X` (the *entire* dataset) *before* the train-test split (`X_scaled = scaler.fit_transform(X)`). This introduces data leakage because the scaler learns parameters (mean and standard deviation) from the test set data, which it should not have access to during training. The `StandardScaler` should be `fit_transform`ed on the training data and then `transform`ed on the test data separately, as done in the ground truth. This is a critical error in data preprocessing for machine learning.
4.  **No `drop_first=True` for OneHotEncoder**: The `OneHotEncoder` does not use `drop_first=True`, which can lead to multicollinearity issues with the encoded 'continent' columns, which can sometimes be problematic for linear models. While not strictly required by the prompt, it's a good practice.

The use of `ColumnTransformer` and `Pipeline` (though `Pipeline` wasn't actually used for the full workflow here) is a good sign for more complex preprocessing pipelines, but its implementation for this specific task was overly complex and had the scaling issue.",mistral-nemo,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
from scipy import stats
from statsmodels.formula.api import ols
import statsmodels.api as sm

# Load dataset
data = pd.read_csv('synthetic_covid19_data.csv')

# Handle missing values
cols_to_fill = ['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']
data[cols_to_fill] = data[cols_to_fill].fillna(data[cols_to_fill].median())

# ANOVA for 'reproduction_rate' across continents (Asia, Europe, North America only)
continent_d...",6,"The generated code attempts to address all parts of the task but falls short on correctness and completeness for several key aspects.

**Correctness Issues:**

1.  **ANOVA:** The `ols` and `sm.stats.anova_lm` approach performs an ANOVA, but it doesn't correctly handle the data filtering for continents. It applies the formula directly to `continent_data` which is a subset, but it assumes the `C(continent)` factor is defined within that subset, which is fine, however, the task specifically asked for `scipy.stats.f_oneway` which is simpler for comparing means of 2 or more groups. More importantly, it doesn't state the null hypothesis or provide conclusions based on alpha, as requested.
2.  **Pearson Correlation:** While `stats.pearsonr` is used correctly to get the p-value, it does not state the null hypothesis or provide a conclusion based on alpha.
3.  **T-test for 'high_death_rate_country':** This section has a critical logical flaw:
    *   `data['high_death_rate_country'] = (data['total_deaths'] / data['population'] > data['total_deaths'].div(data['population']).quantile(0.75))` This line calculates the death rate ratio AND the 75th percentile based on *all* rows in the dataset, not the *latest entry per country* as specified in the prompt. This will lead to an incorrect classification of `high_death_rate_country`.
    *   The `data_latest = data.drop_duplicates(subset='country', keep='last')` line correctly identifies the *latest* entry for each country, but the `high_death_rate_country` column was already calculated *before* this filtering, making its values incorrect. The calculation of the death rate ratio and the 75th percentile must happen *after* selecting the latest entry for each country.
    *   It does not state the null hypothesis or provide conclusions based on alpha.
    *   It doesn't handle potential division by zero for population or `NaN` values before calculating the ratio and quantile.

**Completeness Issues:**

1.  **Null Hypothesis and Conclusion:** For every test, the task explicitly asked to ""state the null hypothesis, print the p-value, and conclude whether to reject the null hypothesis at Œ±=0.05."" This crucial requirement is almost entirely missed in the generated code. Only the p-value is printed.
2.  **Missing Value Handling (Median Imputation):** While it uses `fillna(median())`, it doesn't verify `col in df.columns` or print messages confirming imputation, as a robust solution would. Also, it's missing the `np` import if `np.inf` were to be handled, which it implicitly needs for the death rate ratio if division by zero occurs which the ground truth handles.
3.  **Alpha Configuration:** The significance level (`alpha`) is not defined as a variable, which makes the code less flexible.
4.  **Error Handling/Robustness:** The code assumes the file exists and that the columns are always present. It doesn't include checks for empty groups before performing statistical tests (e.g., if one continent has no data for `reproduction_rate`, or if there are no countries in one of the t-test groups).

**Code Quality Notes:**

*   Imports are mostly appropriate, although `statsmodels` was not strictly necessary given the prompt's request for ANOVA via `scipy.stats`.
*   The code is quite concise, but this conciseness comes at the cost of completeness and robustness, particularly the lack of hypothesis statements and conclusions.
*   The variable names are acceptable.",mistral-nemo,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select numerical columns
numerical_cols = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
df_num = df[numerical_cols]

# Calculate correlation matrix
corr_matrix = df_num.corr()

# Visualize correlation matrix using heatmap
sns.heatmap(corr_matrix, annot=True, fmt="".2f"", linewidths=.5, cbar=False)
plt.show()
```",9,"The generated code correctly addresses all the requirements of the task: it loads the CSV, selects the specified numerical columns, calculates the pairwise correlation matrix, and visualizes it as a heatmap using seaborn. Imports are correct, and the code is concise and readable.

Strengths:
- Correctly loads the dataset.
- Properly identifies and selects the specified numerical columns.
- Accurately calculates the correlation matrix.
- Generates a heatmap as requested, including annotations and basic formatting.

Areas for minor improvement (not critical for score, but good practice):
- The ground truth code includes error handling (e.g., `FileNotFoundError`, `KeyError`) which is robust for real-world applications. The generated code lacks this.
- The ground truth code wraps the logic in a function and uses `if __name__ == ""__main__"":` which makes the code more modular and reusable. The generated code is a script.
- Adding a `plt.title()` to the heatmap for better interpretability.
- While `cbar=False` is used, a colorbar can be useful for understanding the scale of correlations. The ground truth used `cbar=True` by default.

Overall, it's a very good solution to the core task.",mistral-nemo,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

data = pd.read_csv('clash_royale_cards.csv')
grouped = data.groupby('rarity')
averages = grouped[['elixirCost', 'Win Rate', 'Usage']].mean()
print(averages)
```",10,"The generated code correctly loads the dataset, groups it by 'rarity', calculates the average 'elixirCost', 'Win Rate', and 'Usage' for each rarity, and then prints the results. It perfectly matches the requirements of the task and is concise and efficient. The quality is excellent for this task.",mistral-nemo,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# 1) Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# 2) Create a new binary target column named `is_high_win_rate`
data['is_high_win_rate'] = (data['W/T'] > 55).astype(int)

# 3) Select the required features
features = ['elixirCost', 'Usage', 'rarity', 'is_high_win_rate']

# 4) Apply one-hot encoding to the categorical `rarity` column
encoder = OneH...",6,"The generated code addresses most of the requirements but has a critical error in creating the target variable and some less efficient data manipulation.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly identifies and attempts to select the specified features.
*   Correctly uses `train_test_split` with the specified parameters (`test_size`, `random_state`).
*   Uses `OneHotEncoder` from `sklearn.preprocessing` as implicitly suggested by the import.

**Weaknesses and Errors:**

1.  **Incorrect Target Column Creation:** The most significant error is in step 2. The task explicitly states: ""This column should be 1 if `Win Rate` is greater than 55, and 0 otherwise."" The generated code uses `data['W/T']` instead of `data['Win Rate']`. This will either cause a `KeyError` if the column doesn't exist, or will produce incorrect results if a `W/T` column exists but is not the intended `Win Rate`. This fails a core requirement.
2.  **Inefficient Data Manipulation for One-Hot Encoding and Selection:**
    *   Instead of creating `X` and `y` upfront and then encoding `X`, the code first adds `is_high_win_rate` to `data_encoded` and then drops it later. This is less direct.
    *   The use of `OneHotEncoder` is technically correct, but the manual `pd.concat` and slicing (`data[['elixirCost', 'Usage', 'is_high_win_rate']]`) to combine the encoded features is verbose and less robust than using `pd.get_dummies` directly on the feature DataFrame, or using a `ColumnTransformer` with `OneHotEncoder` for more complex pipelines. While `pd.get_dummies` is not `sklearn.preprocessing`, for simple cases like this, it's often more straightforward. The task asked for ""proper imports (pandas, sklearn.model_selection, sklearn.preprocessing)"", and `OneHotEncoder` is indeed from `sklearn.preprocessing`. However, the implementation of combining the results is less clean.
    *   The `features` list in step 3 (`features = ['elixirCost', 'Usage', 'rarity', 'is_high_win_rate']`) includes the target variable. While not directly used for `X`, it contributes to the less clean data manipulation steps.

**Suggestions for Improvement:**

*   **Fix the target column name:** Change `data['W/T']` to `data['Win Rate']`.
*   **Streamline feature and target selection:**
    ```python
    df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)
    X = df[['elixirCost', 'Usage', 'rarity']].copy() # Or select and then use the target column to avoid including it in features initially.
    y = df['is_high_win_rate']

    # Then apply one-hot encoding directly to X
    X = pd.get_dummies(X, columns=['rarity'], drop_first=False)
    # Or, if strictly adhering to sklearn.preprocessing.OneHotEncoder:
    # encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # sparse_output for newer versions
    # rarity_encoded = encoder.fit_transform(X[['rarity']])
    # rarity_df = pd.DataFrame(rarity_encoded, columns=encoder.get_feature_names_out(['rarity']), index=X.index)
    # X = pd.concat([X.drop('rarity', axis=1), rarity_df], axis=1)
    ```
    The current approach of creating `data_encoded` by manually concatenating slices is prone to errors, especially if the DataFrame indices don't align perfectly (though `pd.DataFrame(rarity_encoded, ...)` correctly re-indexes, it's less direct).",mistral-nemo,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv('iris.csv')

# Define features X and target y
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = df['Species']

# Encode categorical target variable into numerical labels
le = LabelEncoder()
y = le.fit_transform(y)
...",9,"The generated code correctly addresses almost all requirements of the task. It loads the data, defines `X` and `y` correctly, encodes the target, splits the data as specified (70/30, `random_state=42`), trains a KNN classifier with `n_neighbors=3`, and reports the accuracy.

The only minor area for improvement is the `train_test_split` call. The ground truth code includes `stratify=y_encoded`, which is a good practice for classification tasks, especially with imbalanced datasets, to ensure that the proportions of target classes are maintained in both the training and testing sets. While not explicitly stated as a *mandatory* requirement in the prompt's evaluation criteria, it is a recommended best practice for robust model evaluation in classification. Without it, the solution is still correct based on the explicit requirements.

Another minor point is the error handling for `FileNotFoundError`. The generated code assumes the file exists, whereas the ground truth includes a `try-except` block for robustness. However, the prompt did not explicitly ask for error handling.

Overall, it's a very good solution.",mistral-nemo,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
data = pd.read_csv('iris.csv')

# Filter the data
group1 = data[data['Species'] == 'Iris-versicolor']['PetalLengthCm']
group2 = data[data['Species'] == 'Iris-virginica']['PetalLengthCm']

# Perform the t-test
t_stat, p_value = ttest_ind(group1, group2)

# Hypotheses
null_hypothesis = 'There is no difference in the mean PetalLengthCm between Iris-versicolor and Iris-virginica'
alternative_hypothesis = 'There is a ...",7,"The generated code correctly performs the core task of loading data, filtering, and conducting a t-test, which is commendable. It also correctly states the hypotheses and interprets the results based on the p-value and alpha.

However, several areas can be improved to meet all requirements and enhance robustness and clarity:

1.  **Hypotheses Statement:** While the code *defines* the hypotheses, it **does not print them** as explicitly requested in requirement #4. The ground truth prints both the null and alternative hypotheses clearly before performing the test.
2.  **Error Handling (File Not Found):** The generated code lacks error handling for `FileNotFoundError` when `pd.read_csv('iris.csv')` is called, which is a common and important robustness consideration. The ground truth includes a `try-except` block for this.
3.  **Edge Case Handling (Empty Groups):** The generated code does not check if the filtered groups (`group1`, `group2`) are empty. If one or both happened to be empty due to incorrect data or a typo in the species name, `ttest_ind` might raise an error or produce misleading results. The ground truth adds a check for this.
4.  **Clarity of Interpretation:** While it correctly states ""Reject the null hypothesis"" or ""Fail to reject the null hypothesis"", it doesn't provide the *conclusion* or *implication* of that decision in terms of the original problem (i.e., whether there is a statistically significant difference in PetalLengthCm). The ground truth provides a more complete conclusion.
5.  **Output Formatting:** The ground truth code uses `print` statements to clearly delineate sections (e.g., ""--- Independent Two-Sample T-Test for PetalLengthCm ---"") and adds descriptive text before the t-statistic and p-value. This makes the output more user-friendly and easier to follow, which was implicitly part of ""Clearly state"" and ""Print"" requirements.",mistral-nemo,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('iris.csv')

# Define the numerical features
num_features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

# Create box plots for each numerical feature
for feature in num_features:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x='Species', y=feature, data=df)
    plt.title(f'Distribution of {feature}')
    plt.show()
```",9,"The generated code successfully loads the iris dataset and generates the required box plots. It correctly creates separate box plots for each of the four numerical features, grouped by the `Species` column, and includes appropriate titles. The use of `pandas`, `seaborn`, and `matplotlib.pyplot` is correct.

Strong points:
- Correctly loads the CSV.
- Iterates through numerical features to create individual plots.
- Uses `sns.boxplot` effectively to group by `Species`.
- Adds titles to the plots.

Areas for minor improvement (not critical for meeting the core requirements):
- The `try-except` block for `FileNotFoundError` in the ground truth is a good practice for robust code, which is missing in the generated code.
- The plot titles could be slightly more descriptive (e.g., ""Distribution of Feature by Species"" as in the ground truth) but ""Distribution of {feature}"" is acceptable given the task description.
- The ground truth includes additional plot enhancements like `plt.xlabel`, `plt.ylabel`, `plt.grid`, and `plt.tight_layout`, which improve readability but weren't strictly required by the prompt.",mistral-nemo,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('heart.csv')

# Select features and target variable
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']
target =...",9,"The generated code is highly correct, complete, and demonstrates good code quality. It successfully addresses all requirements of the task: loading data, selecting specified features and target, correctly applying one-hot encoding to the categorical features using `make_column_transformer` and `make_pipeline`, splitting data with the correct test size and random state, training a Logistic Regression model, and reporting the accuracy on the test set.

**Strong points:**
- Correctly uses `OneHotEncoder` within a `ColumnTransformer` (via `make_column_transformer`) to handle categorical features, ensuring proper preprocessing.
- Uses `make_pipeline` which is a good practice for combining preprocessing and modeling steps, ensuring consistency.
- Correctly splits the data using `train_test_split` with the specified `test_size` and `random_state`.
- Trains a `LogisticRegression` model and calculates `accuracy_score` on the test set.

**Minor areas for improvement (to achieve a perfect 10):**
- The `LogisticRegression()` model was initialized without `max_iter=1000`. While the default `max_iter=100` often suffices, explicitly setting it (as in the ground truth) can prevent `ConvergenceWarning` for some datasets, making the model more robust.
- The final accuracy result is calculated but not printed, which makes it less visible to the user as a direct output. Adding `print(f""Model Accuracy on Test Set: {accuracy:.4f}"")` would make the output clearer and more complete.

Overall, it's an excellent solution that closely matches the ground truth's functionality and best practices.",mistral-nemo,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"import pandas as pd
from scipy.stats import ttest_ind

# 1) Load the dataset
data = pd.read_csv('heart.csv')

# 2) Create two groups based on the `Sex` column
 males = data[data['Sex'] == 1]['Cholesterol']
 females = data[data['Sex'] == 0]['Cholesterol']

# 3) Perform an independent two-sample t-test
 t_stat, p_value = ttest_ind(males, females)

# 4) Null and alternative hypotheses
 null_hypothesis = ""There is no difference in mean `Cholesterol` levels between male and female patients.""
 alterna...",5,"The generated code incorrectly assumes that 'Sex' column values are 0 and 1, whereas the prompt specified 'M' and 'F'. This fundamental error in data grouping leads to incorrect results if the CSV actually contains 'M' and 'F'. Furthermore, the code is missing a clear print-out of the stated hypotheses, only storing them in variables. It also lacks any error handling for file loading or empty groups, which is crucial for robust code. The interpretation statement has a typo ("" Migliet√†""). While it correctly performs the t-test and interprets the p-value, the incorrect data handling severely limits its utility for the given task.",mistral-nemo,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load dataset
data = pd.read_csv('winequality-red.csv')

# Define colums
cols = ['volatile acidity', 'residual sugar']

# Calculate IQR for both columns
Q1 = data[cols].quantile(0.25)
Q3 = data[cols].quantile(0.75)
IQR = Q3 - Q1

# Define data with potential outliers
outliers = ((data[cols] < (Q1 - 1.5 * IQR)) | (data[cols] > (Q3 + 1.5 * IQR)))
outliers_count = outliers.sum()

# Cap outliers at the 1.5*IQR bounds
data_ caps = data.copy()
for col...",4,"The generated code attempts to address the problem but has several critical issues that make it incorrect and incomplete.

**Correctness Issues:**
1.  **Incorrect Outlier Handling (Winsorization Logic):** The line `data_caps[col] = np.where(outliers[col], Q1[col] - 1.5 * IQR[col], data[col])` is fundamentally flawed. It tries to cap *both* upper and lower outliers using `outliers[col]` in a single `np.where` statement. `outliers[col]` itself is a boolean series indicating if a value is *either* below the lower bound *or* above the upper bound.
    *   The first `np.where` effectively caps all identified outliers (whether they are low or high) to the `lower_bound`. This is incorrect.
    *   The second `np.where` then tries to cap them to the `upper_bound`, again using the same general `outliers[col]` mask. This overwrites the previous operation and also doesn't differentiate between upper and lower outliers.
    *   The correct approach for winsorization requires separate conditions for '< lower_bound' and '> upper_bound' to apply the respective bounds.
2.  **Reporting Outliers:** While `outliers_count = outliers.sum()` calculates the number of outliers, it does not print this count to the console as explicitly requested (Report the number of outliers found for each column). The request was to *report* them.
3.  **Comparison of Statistics:** The code calculates `before = data[cols].describe()` and `after = data_caps[cols].describe()`. While `describe()` provides mean and standard deviation, it's not a direct comparison as requested. The task specifically asked to ""Compare the mean and standard deviation of these two columns before and after outlier treatment"" which implies a more direct display or calculation of those specific statistics, not just dumping the entire `describe()` output which includes many other statistics. Also, like the outlier count, these are not printed.

**Completeness Issues:**
1.  **Reporting:** The task explicitly asks to ""Report the number of outliers found for each column"" and ""Compare the mean and standard deviation..."". The generated code calculates these but does not print them to the console. The last line `outliers_count, before, after` will only show these values if run interactively in a Python console, not when run as a script.
2.  **Error Handling/Messaging:** The ground truth includes `try-except` for `FileNotFoundError` and print statements for loading, and steps. The generated code lacks any user-friendly output or error handling for file not found.
3.  **Clarity and Structure:** The code is less readable due to the incorrect combined outlier identification and capping logic. The ground truth separates outlier identification, reporting, and winsorization which makes it clearer.

**Code Quality:**
1.  Imports are correct.
2.  Variable names are acceptable.
3.  The attempt to vectorize Q1, Q3, and IQR calculation is a good thought, but its application in the winsorization is where it fails.",mistral-nemo,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv('heart.csv')

# 2) Create a grouped bar chart
plt.figure(figsize=(10,6))

# Create a dataframe for grouping
g = df.groupby(['ChestPainType', 'HeartDisease'])['HeartDisease'].count().unstack()

# Plot
g.plot(kind='bar', stacked=True)

# Add title and labels
plt.title('Heart Disease by Chest Pain Type')
plt.xlabel('Chest Pain Type')
plt.ylabel('Count')

# Add legend
plt.legend(title='Heart Disease...",5,"The generated code incorrectly produces a stacked bar chart instead of a grouped bar chart as explicitly requested in the prompt. The `stacked=True` argument in `g.plot(kind='bar', stacked=True)` is the reason for this. It also does not use Seaborn even though the prompt suggested using matplotlib or seaborn. While it loads the data and includes basic labels and a title, the core plotting requirement for a *grouped* bar chart is not met. Additionally, it lacks error handling for file not found and doesn't explicitly map `HeartDisease` values (0 and 1) to descriptive labels in the legend, making the plot less clear.",mistral-nemo,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

# Load dataset
df = pd.read_csv('spotify_2023.csv')

# Clean 'in_shazam_charts' column
df['in_shazam_charts'] = df['in_shazam_charts'].str.replace(',', '').astype(float, errors='coerce')
nan_count = df['in_shazam_charts'].isna().sum()

# Group data and calculate averages
grouped = df.groupby(['key', 'mode']).agg(avg_danceability=('danceability_position', 'mean'), avg_valence=('valence_position', 'mean'))
print(grouped)
```",5,"The generated code incorrectly references `danceability_position` and `valence_position` instead of the required `danceability_%` and `valence_%`. This is a critical error as it calculates averages for non-existent columns. Additionally, while it attempts to report NaNs, it doesn't print the count as requested in the task description. The code also lacks proper error handling for `FileNotFoundError` when loading the CSV. The cleaning of `in_shazam_charts` and the grouping structure are mostly correct, but the column name mistake for the averages is a significant flaw.",mistral-nemo,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# 1) Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 2) Ensure the `streams` column is numeric and drop rows with NaN in `streams` or predictor columns
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams', 'in_spotify_playlists', 'energy_%'])

# 3) Select fe...",9,"The generated code correctly implements all the requirements of the task. It loads the dataset, correctly handles non-numeric 'streams' values by coercing to NaN and then dropping rows with NaNs in relevant columns. It correctly defines features and target, splits the data, trains a Linear Regression model, and evaluates it reporting R2 and MSE. The imports are proper, and the code structure is clear and follows the requested steps.

The only minor point for improvement, which doesn't detract significantly from its correctness or completeness for this task, is the lack of explicit error handling for `FileNotFoundError` when loading the CSV, as present in the ground truth. While not explicitly asked for, it's good practice for robustness. Also, the MSE is printed with 4 decimal places where the ground truth prints with 2, but both are acceptable. Overall, a very good solution.",mistral-nemo,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd
import numpy as np

# 1) Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 2) Combine `released_year`, `released_month`, and `released_day` into a single datetime column named `release_date`
df['release_date'] = pd.to_datetime(df[['released_year', 'released_month', 'released_day']]. Assign(coerce_float=False), errors='coerce')

# 3) Create a new feature named `days_since_release`
df['days_since_release'] = (pd.Timestamp('2023-12-31') - df['release_date']).dt....",7,"The generated code largely accomplishes the task but has a significant issue in one part, leading to a loss of correctness, and some minor quality and completeness issues.

**Correctness Issue:**
The most critical error is in line `df['release_date'] = pd.to_datetime(df[['released_year', 'released_month', 'released_day']]. Assign(coerce_float=False), errors='coerce')`
The `pd.to_datetime` function does not accept a DataFrame with separate year, month, day columns directly. It expects a single string column or a dictionary/DataFrame where keys are 'year', 'month', 'day' and values are the corresponding series. The `.Assign(coerce_float=False)` is also not a standard pandas method for this context; it seems like an attempt to configure behavior that isn't applicable. This line will very likely fail or produce incorrect NaT values for all rows because it cannot parse the input as intended, making release_date almost entirely NaT. The ground truth correctly concatenates these into a string first.

**Completeness:**
- The prompt asked to ""display the track_name, release_date, and days_since_release for the first 5 tracks."" The last line `df[['track_name', 'release_date', 'days_since_release']].head()` calculates this but doesn't explicitly `print()` it, so it won't be displayed in a script execution context unless it's the very last line in an interactive environment.
- The prompt also implicitly requires error handling during CSV loading, which the generated code lacks (though the ground truth has it, the prompt didn't *explicitly* state it was a requirement for the generated code, only that the ground truth included it).

**Code Quality:**
- The use of `numpy` is not needed as `pandas` handles all operations. It's a minor point, but extraneous imports should be avoided.
- The intent behind `.Assign(coerce_float=False)` is unclear and incorrect for the purpose.

**Improvements to reach a higher score:**
1.  **Crucial Fix for `release_date`:** The `release_date` creation needs to be fixed. The most robust way is to combine the year, month, and day columns into a single string (e.g., 'YYYY-MM-DD') and then convert that string to datetime using `pd.to_datetime(..., errors='coerce')`.
2.  **Explicit Printing:** Add `print()` around the `.head()` call to ensure the output is displayed.
3.  **Remove Unused Import:** Remove `import numpy as np`.",mistral-nemo,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

# Load dataset
df = pd.read_csv('youtube_trendings.csv')

# Remove commas, convert to numeric and fill with 0 if fails, then fill missing values with 0
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)

# Fill missing values in Category with 'Unknown'
df['Category'].fillna('Unknown', inplace=True)

# Group by Category and calcu...",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans the 'Video views' and 'Likes' columns by removing commas, converting to numeric, and handling NaNs by filling with 0. It also correctly fills missing 'Category' values with 'Unknown' and then groups by 'Category' to calculate the sum of 'Video views' and 'Likes'. The import is correct, and the code structure is clear and concise.

One minor point that could be considered for a perfect score is the lack of explicit error handling for `FileNotFoundError` or checking if columns exist, similar to the ground truth. While not strictly required by the prompt's output format, robust code often includes such checks. However, for the given task description, the code fully implements the specified logic accurately and efficiently. The `astype(str)` conversion for numeric columns might be slightly more robust if the column wasn't already string-like, but `str.replace` usually handles non-string types by coercing them, so it's not a functional error here.",mistral-nemo,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
df = pd.read_csv('youtube_trending.csv')

# 2) Clean 'Likes' column
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce')
df['Likes'].fillna(0, inplace=True)

# 3) Fill missing 'Category' values with 'Unknown'
df['Category'].fillna('Unknown', inplace=True)

# 4) Create groups for 'Music' and 'Sports'
music_likes = df[df['Category'] == 'Music']['Likes'].dropna().values
sports_likes = df[df['Categ...",7,"The generated code correctly performs most of the required steps.

**Strengths:**
*   Loads the dataset.
*   Correctly cleans the 'Likes' column by removing commas, converting to numeric, and filling NaNs with 0 (though the order of operations for `astype(str)` for `replace` is implicitly handled by `pd.to_numeric` if the column contains mixed types, explicit `astype(str)` is safer as in GT).
*   Fills missing 'Category' values.
*   Correctly creates the `music_likes` and `sports_likes` groups.
*   Performs the independent two-sample t-test.
*   States the null and alternative hypotheses, although very concisely.
*   Prints the t-statistic and p-value.
*   Interprets the p-value against `alpha=0.05` and correctly states whether to reject or fail to reject the null hypothesis.

**Areas for Improvement / Minor Issues:**
*   **Dataset Name:** The code tries to load `youtube_trending.csv` instead of `youtube_trendings.csv` (missing 's'). This would cause a `FileNotFoundError` in practice, which wasn't handled. The Ground Truth code includes error handling for `FileNotFoundError`.
*   **`dropna()` on `Likes`:** The `dropna()` calls on `music_likes` and `sports_likes` are redundant. The `Likes` column has already been cleaned and has all NaNs filled with 0. While it doesn't cause incorrect results here, it's unnecessary and misleading.
*   **T-test `equal_var` parameter:** The generated code uses the default `equal_var=True` for `stats.ttest_ind`. For most real-world scenarios, especially when comparing two populations that might have different variances, `equal_var=False` (Welch's t-test) is often more appropriate and robust, as used in the Ground Truth. This was a specific detail often considered in data science interviews.
*   **Hypotheses Clarity:** While stated, the null/alternative hypotheses are very brief. The Ground Truth provided a more detailed and clearer statement of the hypotheses. The task asked to ""Clearly state the null and alternative hypotheses.""
*   **Output Clarity:** The final interpretation only states ""Reject/Fail to reject,"" but doesn't explicitly state the conclusion in terms of the initial problem (e.g., ""There is/is not a statistically significant difference...""). The Ground Truth provides this clarity.
*   **Missing `print` statements:** The Ground Truth included `print` statements after each major cleaning step, which helps in debugging and understanding the code's progress. While not strictly required by the prompt, it enhances readability and user experience.
*   **Robustness:** No `try-except` block for file loading.

Overall, the code is functional and largely meets the requirements, but could benefit from a few refinements to match the robustness, clarity, and best practices demonstrated in the Ground Truth, particularly regarding the t-test parameter and the file loading error handling.",mistral-nemo,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Remove commas and convert to numeric, fill NaNs with 0
for col in ['Likes', 'Dislikes']:
    df[col] = df[col].replace(',', '', regex=True).astype(float, errors='replace').fillna(0)

# Create new columns
df['TotalEngagement'] = df['Likes'] + df['Dislikes']
df['LikeDislikeRatio'] = df['Likes'] / (df['Dislikes'] + 1)

# Display selected columns
result = df[['Video', 'TotalEngagement', 'LikeDislikeRatio']...",9,"The generated code correctly performs all the required feature engineering steps. It loads the dataset, cleans the 'Likes' and 'Dislikes' columns by removing commas, converting to numeric, and filling NaNs with 0. It then correctly calculates 'TotalEngagement' and 'LikeDislikeRatio' as specified. Finally, it displays the required columns.

The `replace(',', '', regex=True)` with `.astype(float, errors='replace')` is an elegant and concise way to handle the cleaning, although `errors='replace'` is not a valid argument for `astype`. It should be `errors='coerce'` for `pd.to_numeric` or `astype`. However, Pandas' `replace` method with `regex=True` followed by `astype(float)` *will* raise an error for non-numeric values *after* comma removal, if `astype(float)` by itself is used. The generated code effectively leverages `replace` with `regex` to handle the comma removal, which is a good approach. The issue is applying `astype(float, errors='replace')`. `astype` does not take `errors='replace'`. It should be `errors='coerce'` as used in `pd.to_numeric`.

However, testing the line `df[col] = df[col].replace(',', '', regex=True).astype(float).fillna(0)` locally, if there are non-numeric strings after comma removal (e.g., 'abc'), `astype(float)` would raise an error. The task specified ""_if conversion fails for any value, treat that value as NaN, then fill all resulting NaNs in these columns with 0_"". The generated code's `astype(float, errors='replace')` is syntactically incorrect for `astype`. If instead it was `pd.to_numeric(df[col].replace(',', '', regex=True), errors='coerce').fillna(0)`, it would be correct.

Despite this minor syntactical issue with `astype` in the prompt, the intention is clear and the logical flow is correct. In a real-world scenario, this line might fail depending on the pandas version or specific error handling. Assuming the data after comma removal contains only valid numbers or strings that `float()` can parse, it would work. Given the typical benchmark setup, it's highly likely `df[col].replace(',', '', regex=True)` will result in valid string representations of numbers, making `astype(float)` effective then, and `fillna(0)` handles the implicit NaNs if any.

It's a strong and concise solution, very close to perfect. The primary reason for not giving it a 10 is the `errors='replace'` argument on `astype`, which would raise a `TypeError` in pandas. Replacing it with `pd.to_numeric(..., errors='coerce')` would make it robust and perfectly aligned with the prompt.",mistral-nemo,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean Launched Price (USA) column
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', '').astype(float, errors='ignore')

# Clean Screen Size column
df['Screen Size'] = df['Screen Size'].str.replace(' inches', '').astype(float, errors='ignore')

# Group data and calculate average
result = df.groupby(['Company Name', 'Launched Year']).agg({'Launched Price (USA)': ...",7,"The generated code correctly loads the CSV, groups the data, and calculates the averages. The cleaning of the 'Launched Price (USA)' and 'Screen Size' columns is mostly correct, but it uses `astype(float, errors='ignore')` which doesn't behave quite like `pd.to_numeric(errors='coerce')`. The `errors='ignore'` argument for `astype` will return the original object if it cannot be cast, leading to potential issues if there are non-numeric strings that should become NaN. While this might often work for simple cases, `pd.to_numeric(errors='coerce')` is the more robust and explicitly correct way to handle these conversions to NaN. Missing error handling for file loading. It also reuses column names for cleaned data instead of creating new ones, which is not ideal but acceptable.",mistral-nemo,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

# 1) Load the dataset
data = pd.read_csv('mobiles_2025.csv')

# 2) Define a function to clean camera columns
def extract_mp(value):
    match = re.search(r'\d+', value)
    return int(match.group()) if match else 0

# Apply the function to create cleaned columns
data['Cleaned Front Camera'] = data['Front Camera'].apply(extract_mp)
data['Cleaned Back Camera'] = data['Back Camera'].apply(extract_mp)

# 3) Create a new feature Total Camera MP
data['Total Cam...",7,"The generated code correctly solves the core task of loading data, performing feature engineering on camera specifications, and displaying the results. It uses `pandas` and `re` appropriately. However, there's a robustness issue in the `extract_mp` function: it assumes `value` will always be a string and does not handle `NaN` or non-string inputs gracefully. If a camera specification cell is missing or not a string, it will raise a `TypeError` which the ground truth code handles by checking `pd.isna(camera_spec)` and converting to string. Additionally, the ground truth code includes better error handling for file loading, print statements for progress, and more robust display formatting. These are good practices for production-ready code or more accessible scripts.",mistral-nemo,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"Here's the Python code to build a regression model using the provided dataset:

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# 1) Load the dataset
data = pd.read_csv('mobiles_2025.csv')

# 2) Clean columns
columns_to_clean = ['RAM', 'Battery Capacity', 'Screen Size', 'Launched Price (USA)']

for col in columns_to_clean:
    data[col] = data[col].str.r...",7,"The generated code correctly performs most of the required steps: loading data, splitting, training, and predicting. The R2 and MAE metrics are also correctly calculated and reported.

However, there's a significant issue in the data cleaning step. The provided code attempts to clean all specified columns using `data[col].str.replace(r'\D', '')`. This regular expression replaces all non-digit characters with an empty string. While this might work for 'Battery Capacity' and 'Screen Size' if they only contain numbers and a unit like 'mAh' or ' inches', it will likely fail for 'RAM' (e.g., '8GB' becomes '8') and, critically, for 'Launched Price (USA)'. For 'Launched Price (USA)', which needs 'USD ' and commas removed, `r'\D'` would also remove the decimal point (if present in some values) and other non-digit characters that might be part of the numerical representation (like a negative sign if some prices were negative, though unlikely here). The prompt specifically asked to remove 'USD ' and commas for 'Launched Price (USA)', and 'GB' for 'RAM'. The `r'\D'` approach is a shortcut that is too generic and not precise enough for the requested cleaning, potentially leading to incorrect numeric conversions or loss of information (like decimal points).

A better approach, like the ground truth, would be to specify the exact patterns to remove for each column (`'GB'`, `'mAh'`, `' inches'`, `'USD '`, and `,`) to ensure accurate conversion. The current cleaning method might lead to a different numeric value than intended by the prompt.

Improving the data cleaning accuracy and specificity (e.g., `str.replace('GB', '')` for RAM, `str.replace('USD ', '').str.replace(',', '')` for price) would make the code more robust and accurate per the task requirements.",mistral-nemo,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
import scipy.stats as stats
from numpy import mean, sqrt

# Load dataset
titanic = pd.read_csv('titanic.csv')

# 1) Chi-square test for Pclass and survival
contingency_table = pd.crosstab(titanic['Pclass'], titanic['Survived'])
chi2, p, dof, expected = stats.chi2_contingency(contingency_table)

# Calculate effect size (Cramer's V)
n = len(titanic)
phi2 = chi2/n
rho = sqrt(phi2/(1+phi2))
cramers_v = sqrt(phi2 * (n-1) / (n*(min(titanic['Pclass'].nunique(), titanic['Su...",6,"The generated code correctly performs the chi-square test, t-test, and ANOVA. It also attempts to calculate effect sizes and display results. However, there are several inaccuracies and omissions that prevent it from scoring higher.

**Areas for Improvement:**

1.  **Effect Size Calculation (Chi-square):** The formula used for Cramer's V is incorrect. The `min(titanic['Pclass'].nunique(), titanic['Survived'].nunique())-1` part can be simplified and should directly relate to the dimensions of the contingency table. The Ground Truth uses `min_dim = min(contingency_table.shape) - 1`, which is more appropriate. The current formula `rho = sqrt(phi2/(1+phi2))` is not Cramer's V.

2.  **Effect Size Calculation (T-test):** The effect size calculation for the t-test `mean(survivors_age) - mean(non_survivors_age) / sqrt(mean(survivors_age.std())**2 + mean(non_survivors_age.std())**2)` is not correct for Cohen's d. It appears to be an attempt at a pooled standard deviation but is mathematically flawed. The standard formula for Cohen's d (difference in means / pooled standard deviation) was entirely missed. Also, the `abs()` around `-effect_size` suggests a misunderstanding of how the sign of Cohen's D is typically interpreted (it indicates direction of difference).

3.  **Effect Size Calculation (ANOVA):** The ANOVA section completely lacks the calculation and interpretation of an effect size (e.g., Eta-squared or Partial Eta-squared). This was a specific requirement of the prompt.

4.  **Assumptions and Robustness:** The code does not check for or address the assumptions of the t-test (e.g., Levene's test for equality of variances) or ANOVA (e.g., homogeneity of variances). While not explicitly requested in the prompt, robust solutions often include these checks for more reliable conclusions. The ground truth demonstrated this best practice.

5.  **Interpretation Detail:** The conclusions are very brief. While it states ""significant"" or ""not significant,"" it doesn't explicitly state the null and alternative hypotheses tested (like the ground truth does) nor does it print the comparison with the alpha level (e.g., ""p-value < alpha""). For ANOVA, it doesn't mention the need for post-hoc tests if significance is found.

6.  **Code Structure and Clarity:** The ground truth code is much more structured with clear headings and print statements, making it easier to follow and understand each step of the analysis. The generated code is more concise but less readable for someone trying to understand the statistical process.

7.  **Variable Scope:** The `p` variable is reused for both chi2 and t-test, leading to overwriting and potential confusion in the final conclusion block. It's better to use distinct variable names (e.g., `p_chi2`, `p_ttest`). In the final print statements, the `p` refers to the t-test `p` value, not the chi-square `p` value in the first conclusion. This is a critical bug.

8.  **Error Handling:** No error handling for file not found (though the prompt implicitly assumes the file is present).

Overall, the code performs the core statistical tests but falls short on the accuracy of effect size calculations and the depth of interpretation, and has a critical bug in its conclusion logic due to variable reuse.",mistral-nemo,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Group by Year and calculate averages
grouped = df.groupby('Year')[['Rating', 'Runtime']].mean()

# Create line plot for average Rating
plt.plot(grouped.index, grouped['Rating'], marker='o')
plt.title('Trend of Average Rating over Year')
plt.xlabel('Year')
plt.ylabel('Average Rating')
plt.show()
```",7,"The generated code correctly loads the CSV, groups by `Year`, calculates the average `Rating` and `Runtime`, and generates a line plot for average `Rating` over `Year` with a title and labeled axes. This addresses most of the core requirements.

However, there are a few areas for improvement:
- **Robustness/Data Cleaning:** The ground truth includes essential data cleaning steps for `Year` and `Runtime` (and `Rating`), which often contain non-numeric characters or require specific parsing. The generated code assumes these columns are directly numeric or can be implicitly converted, which might lead to errors or incorrect results with real-world messy data. This is a common issue in real-world data science tasks.
- **Completeness of Output:** While it calculates the average `Runtime`, it doesn't utilize or plot it, which wasn't explicitly asked for plotting but was part of the calculation requirement.
- **Plot Enhancements:** The ground truth adds more visual enhancements like grid, `xticks` rotation, `tight_layout`, and a larger figure size, which improve readability for general presentation but were not strictly required by the prompt.

The code is concise and directly solves the happy-path scenario but lacks the robustness expected in a more complete solution for a data science task.",mistral-nemo,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"Here's the requested Python code using pandas for data manipulation and scipy for the t-test:

```python
import pandas as pd
from scipy.stats import ttest_ind

# 1) Load the dataset
data = pd.read_csv('imdb_top_250.csv')

# 2) Create two groups
group1 = data[data['Year'] < 1990]['Rating']
group2 = data[data['Year'] >= 1990]['Rating']

# 3) Perform t-test
t_stat, p_val = ttest_ind(group1, group2)

# 4) Hypotheses
null_hypothesis = ""The mean rating is the same for movies released before vs. after ...",7,"The generated code correctly performs the core task of loading data, creating groups, and running a t-test. It also correctly interprets the p-value based on the specified alpha. However, it lacks robustness and completeness in several areas found in the ground truth.

**Areas for improvement:**

*   **Robustness (Data Loading & Cleaning):** The generated code assumes the `imdb_top_250.csv` file exists and that the 'Year' and 'Rating' columns are immediately ready for numerical operations. The ground truth includes `try-except` for `FileNotFoundError` and explicit `pd.to_numeric` with `errors='coerce'` followed by `dropna` to handle potential issues with column data types or missing values, which is crucial for real-world data.
*   **Hypotheses Clarity:** While the generated code states the hypotheses, the ground truth provides a more formal and detailed phrasing for the null and alternative hypotheses, which is good practice in statistical reporting.
*   **Edge Cases (Insufficient Group Data):** The ground truth checks if each group has at least two samples, which is a prerequisite for performing a t-test. The generated code does not include this check.
*   **Output Detail:** The ground truth includes more verbose print statements, such as confirming successful data loading, explicitly printing the significance level, and optionally showing the mean ratings for each group, which adds context to the interpretation.
*   **`equal_var` parameter in `ttest_ind`:** The ground truth explicitly sets `equal_var=True`, which is the default but good to be explicit about the assumption being made for the test.

Overall, it's a functional solution but could be improved in terms of robustness, error handling, and detailed output for a more complete and production-ready script.",mistral-nemo,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

# 1) Load the dataset
data = pd.read_csv('imdb_top_250.csv')

# 2) Create a new column named `Decade`
data['Decade'] = pd.cut(data['Year'], bins=[1900, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020, 2030], labels=['1900s', '1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s'])

# 3) Display the required columns for the first 10 movies
print(data[['Title', 'Year', 'Decade']].head(10))
```",7,"The generated code correctly loads the dataset and displays the requested columns for the first 10 movies. However, the `Decade` column creation is problematic. It uses `pd.cut` with fixed bins and labels, which is not a general solution for dynamic decade calculation (e.g., 1990-1999 should be '1990s', 2000-2009 should be '2000s'). The ground truth's approach of `((df['Year'] // 10) * 10).astype(str) + 's'` is more robust and generalizable. This implementation of `Decade` column will only work for the specific ranges given in `bins` and `labels`. For example, a movie from 1999 would correctly be in '1990s', but it would not dynamically handle years outside the predefined bins or calculate decades precisely as floor division does. The specific `pd.cut` usage for this task only works because the data predominantly falls into the defined bins, but it's not the accurate implementation of ""1990-1999 should be '1990s'"". Also, the `Decade` column in the output will show NaN for some records, as the upper bound of the last bin (2030) does not fully cover the possible max year in the dataset (2020), and it also does not create a ""2020s"" bin for years 2020-2029 which might be present. The absence of error handling for file not found or missing columns also reduces its robustness compared to the ground truth.",mistral-nemo,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Load dataset
df = pd.read_csv('penguins.csv')

# Impute missing values
for col in ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']:
    df[col].fillna(df[df[col].notna()][col].mean(), inplace=True)
df['sex'].fillna(df['sex'].mode()[0], inplace=True)

# Group data and calculate averages
grouped = df.groupby(['species', 'sex'])['body_mass_g'].mean()
print(grouped)
```",9,"The generated code correctly addresses all parts of the task: loading the CSV, imputing numerical columns with their means, imputing `sex` with its mode, grouping by `species` and imputed `sex`, and calculating the average `body_mass_g`. The code is concise and directly implements the requirements.

One minor point of improvement could be the imputation logic for numerical columns. While `df[df[col].notna()][col].mean()` does correctly calculate the mean of non-NA values, `df[col].mean()` on its own (as used in the ground truth) also achieves the same result more directly and idiomatically in pandas, as `mean()` by default ignores NA values. However, the generated code's approach is not incorrect, just slightly less direct. The ground truth also includes more robust error handling (`try-except`) and defines constants for column names, which are good practices for larger, more complex scripts, but for this specific task's conciseness requirement, the generated code is perfectly acceptable.",mistral-nemo,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"import pandas as pd
from scipy.stats import ttest_ind

# 1) Load the dataset
data = pd.read_csv(""penguins.csv"")

# 2) Filter the data
filtered_data = data[(data['species'] == 'Adelie') | (data['species'] == 'Gentoo')]

# 3) Remove records with missing values
clean_data = filtered_data.dropna(subset=['flipper_length_mm', 'species'])

# 4) Perform two-sample t-test
adelie_flipper = clean_data[clean_data['species'] == 'Adelie']['flipper_length_mm']
gentoo_flipper = clean_data[clean_data['species'] ...",9,"The generated code correctly performs all steps required by the task. It loads the data, filters it, removes NAs, conducts the t-test, and interprets the results based on the p-value against an alpha of 0.05.

**Strengths:**
*   **Correctness:** All steps (data loading, filtering, NA handling, t-test, interpretation) are correctly implemented.
*   **Conciseness:** The code is clear and to the point.
*   **Proper Imports:** `pandas` and `scipy.stats.ttest_ind` are correctly imported.

**Areas for Minor Improvement:**
*   **Hypothesis Statement Detail:** While the hypotheses are stated, the ground truth provides a more formal and comprehensive phrasing for the null and alternative hypotheses, explicitly mentioning the means and species, and also stating the significance level used. This, along with explicitly printing the hypotheses, would make the output more complete from an analytical perspective.
*   **Error Handling for File Not Found:** The ground truth includes `try-except` blocks for robust file loading, which is good practice. The generated code assumes the file will always be present, which could lead to a `FileNotFoundError` if the CSV is missing.
*   **Clarity of Output:** The ground truth prints more verbose intermediate steps, like dataset shapes and rows removed, which can be helpful for debugging and understanding data transformations.
*   **Default `equal_var` in `ttest_ind`:** `scipy.stats.ttest_ind` defaults to `equal_var=True`. While the problem doesn't specify, it's generally safer (or explicitly stated) to use `equal_var=False` (Welch's t-test) unless there's a strong reason to assume equal variances. The ground truth correctly uses `equal_var=False` and comments on it. This is a subtle point but important for robust statistical analysis.

Overall, the code is highly functional and correctly addresses the core requirements. The minor points relate to robustness, output verbosity, and statistical best practices.",mistral-nemo,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1) Load the dataset
data = pd.read_csv('penguins.csv')

# 2) Select the features X and target y
X = data[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]
y = data['species']

# 3) Drop missing values
X = X.dropna()
y = y[X.index]

# 4) Encode t...",6,"The generated code correctly performs most of the required steps.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly selects X and y variables.
*   Correctly applies `LabelEncoder` for the target variable.
*   Correctly splits the data with the specified `test_size` and `random_state`.
*   Correctly trains a `LogisticRegression` model with `max_iter=1000`.
*   Correctly computes and prints the accuracy score.

**Areas for Improvement (leading to a score of 6):**
1.  **Incorrect NA Handling:** The most significant issue is in step 3 where missing values are handled.
    *   `X = X.dropna()` drops rows from `X` where any of the feature columns have NA. This is correct for `X`.
    *   BUT `y = y[X.index]` then attempts to align `y` with the *newly indexed* `X`. This fails because `y` still has its original index, which might contain NAs in `species` or rows that were dropped by `X.dropna()` but are still present in the original `y`.
    *   The task explicitly states: ""Drop any rows where `species` or any of the four selected feature columns have missing values (NA)."" This implies that the dropping should happen *before* separating X and y, or on a combined (selected) DataFrame. The current approach will lead to `y` not correctly aligning with `X` if NAs were only in `species` or if `X` had NAs that `y` didn't. This can lead to `y` having more rows than `X` after dropping, or wrong row alignment, which will cause `y = le.fit_transform(y)` to fail or produce incorrect results due to index misalignment when passed to `train_test_split`.
    *   A more robust way is to select the relevant columns and then apply `dropna()` on this subset of the DataFrame, before separating `X` and `y`.
2.  **Lack of Robustness:** The code assumes the CSV file exists (`pd.read_csv('penguins.csv')`). The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice.
3.  **No Column Existence Check:** The code assumes all specified columns exist in the DataFrame. A production-ready or robust solution would check for column existence before proceeding.
4.  **Implicit Assumption of `X` and `y` Alignment after NA Drop:** Because `X` and `y` are separated *before* the `dropna` step, `y` might contain NAs in 'species' that aren't dropped, or the indices of `X` and `y` might become misaligned if `X.dropna()` only drops rows where features were missing, but the corresponding `y` values (species) were not. The `y = y[X.index]` line tries to fix this, but only partially, and if the original `y` had NAs in 'species' at indices that `X` didn't drop for feature NAs, those 'species' NAs would (incorrectly) persist. The better approach is to drop NAs from a combined DataFrame (or a subset of relevant columns) first.",mistral-nemo,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1) Load the dataset
data = pd.read_csv('penguins.csv')

# 2) Drop rows where `body_mass_g` or `species` is NA
data = data.dropna(subset=['body_mass_g', 'species'])

# 3) For each `species`, calculate the average `body_mass_g`
avg_mass_by_species = data.groupby('species')['body_mass_g'].mean()

# 4) Create a bar plot showing the average `body_mass_g` for each `species`
plt.figure(figsize=(10, 6))
sns.barplot(x=...",9,"The generated code correctly performs all requested operations: loading data, dropping NAs in the specified columns, calculating the average body mass per species, and generating a bar plot with a title and labeled axes. The code is concise and clear.

Strong points:
- Correctly loads the CSV.
- Accurately handles NA values by dropping rows based on `body_mass_g` and `species`.
- Correctly calculates the mean `body_mass_g` grouped by `species`.
- Generates a bar plot with `seaborn` and `matplotlib.pyplot` as requested.
- Sets appropriate titles and labels for the plot.

Areas for minor improvement (not critical for current task, but good practice):
- The `try-except` block for file loading in the ground truth is more robust for real-world scenarios, preventing script crashes if the file is missing.
- Using `sns.barplot` with `data=`, `x=`, `y=` directly on the grouped DataFrame (if `reset_index()` was applied) can sometimes be slightly more readable or idiomatic for Seaborn, though the `x=index, y=values` approach works perfectly fine here.
- The `palette` argument in the ground truth's `barplot` call adds a nice visual touch, although it's not a required part of the prompt.
- The ground truth also includes `plt.xticks(rotation=45, ha='right')` and `plt.grid(axis='y', linestyle='--', alpha=0.7)` which improve readability for certain datasets but are not explicitly required by the prompt.

Overall, excellent solution that fulfills all requirements accurately and efficiently.",mistral-nemo,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"Here's the Python code using pandas, numpy, seaborn, and matplotlib to perform the required tasks:

```python
# 1) Load the dataset
import pandas as pd

df = pd.read_csv('penguins.csv')

# 2) Drop rows where specified columns are NA
df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'], inplace=True)

# 3) Create a new feature culmen_area
df['culmen_area'] = df['culmen_length_mm'] * df['culmen_depth_mm']

# 4) Calculate the Pearson correlation matrix
import...",9,"The generated code successfully completes almost all aspects of the task.

**Strong Points:**
- **Correctness:** All five requirements (load data, drop NAs, create `culmen_area`, calculate correlation, visualize heatmap) are met correctly.
- **Conciseness:** The code is quite concise and directly addresses the problem.
- **Proper Imports:** Imports are handled well, though spreading them out is a minor point.
- **Visualization:** The heatmap is correctly generated with annotations and a title.

**Areas for Minor Improvement:**
- **Robustness (File Not Found):** The generated code does not include error handling for `FileNotFoundError` when loading the CSV, unlike the ground truth. This is a good practice for real-world applications.
- **Redundant `np.round`:** Applying `np.round` to the correlation matrix before passing it to `sns.heatmap` can sometimes obscure true correlations if not done carefully. `seaborn`'s `fmt` parameter handles decimal display more appropriately without altering the underlying data. The ground truth uses `fmt="".2f""` which is preferred.
- **Import Placement:** While functionally correct, it's generally better practice to put all imports at the top of the script for readability and to avoid potential issues in larger scripts where imports might be conditional or performance-sensitive.
- **Clarity of `inplace=True`:** While `inplace=True` is used, it's worth noting that using `.copy()` after `dropna` and assigning to a new variable (as the ground truth does) is often safer to prevent `SettingWithCopyWarning` and ensures working on a distinct DataFrame. For this specific task, `inplace=True` works fine, but in more complex scenarios, creating a copy is often more robust.",mistral-nemo,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np

# 1) Load the dataset using pandas
df = pd.read_csv('penguins.csv')

# 2) Select the features and target variable
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
target = 'species'
data = df[features + [target]]

# 3) Impute NA values for numerical features with their respective column means
for feature in ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']:
    data[...",8,"The generated code correctly addresses almost all aspects of the task.

**Strong Points:**
- **Correct Data Loading:** Successfully loads the `penguins.csv` file.
- **Feature Selection:** Correctly selects all specified features and the target variable.
- **Numerical Imputation:** Accurately imputes NA values in numerical columns with their respective column means.
- **Sex Column Processing:** Imputes NA values with 'Unknown' and correctly maps 'MALE', 'FEMALE', and 'Unknown' to 0, 1, and -1 respectively.
- **One-Hot Encoding:** Correctly applies one-hot encoding to the `island` column.
- **Output Requirements:** Prints the shape of the resulting DataFrame and the first 5 rows as requested.
- **Conciseness:** The code is quite concise and to the point.

**Areas for Improvement (Why it's not a 10):**
1.  **Implicit Assumption of `target` in `data.shape` and `data.head()`:** The task asked for the shape of the *feature matrix* and the *first 5 rows of the processed features*. The generated code prints `data.shape` and `data.head()`, which still include the target variable `species`. While for some classification tasks this might be acceptable, explicitly separating `X` (features) and `y` (target) and then printing the shape and head of `X` would be more precise and align better with typical machine learning workflows where `X` is the feature matrix.
2.  **Lack of Error Handling for `FileNotFoundError`:** The ground truth code includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code assumes the file will always be present.
3.  **No `copy()` when slicing DataFrame:** In the line `data = df[features + [target]]`, not using `.copy()` can sometimes lead to `SettingWithCopyWarning` when performing operations like `fillna` or `map` on `data` later, as `data` might be a view rather than a true copy. While it might not always produce a warning in every pandas version or scenario, it's safer to explicitly create a copy. The ground truth code uses `.copy()` which is better.
4.  **Implicit `prefix` for `get_dummies`:** `pd.get_dummies` by default uses the column name as prefix (e.g., `island_Adelie`), which matches expectation, but explicitly setting `prefix='island'` as in the ground truth can sometimes improve clarity or prevent unexpected prefixes if the column name itself was complex.

Overall, it's a very good solution that fulfills the core requirements. The improvements are mainly about robustness, best practices, and precise alignment with the phrasing of the output requirement.",mistral-nemo,LLM_judge_config
M49,medium,Customer Segmentation,,"Here's the Python code to achieve the desired results using pandas:

```python
import pandas as pd

# Create DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ...",9,"The generated code correctly creates the DataFrame, calculates total spending per customer, identifies and prints the top 3 customers by spending, and calculates total quantity sold per category. The code is clean and uses pandas effectively.

Minor improvements could be made:
1. The `Total_Spending` column is calculated at the row level (`df['Quantity'] * df['Price_USD']`). While this intermediate step is technically correct for the subsequent `groupby('CustomerID')['Total_Spending'].sum()`, it's more typical and often clearer to name this intermediate calculation `LineTotal` or `SpentAmount` to avoid confusion with the final `Total_Spending` per customer.
2. The output format for the top 3 customers could be more user-friendly, similar to the ground truth which iterates and prints formatted strings. The generated code prints the DataFrame directly, which is acceptable but less tailored to the prompt's implied ""print the 'CustomerID' and their total spending"".
3. The `reset_index()` for `customer_spending` and `category_quantity` is not strictly necessary if you intend to keep them as Series or if the index (CustomerID/Category) is directly used. It adds a redundant step for the final use cases in this problem.

Overall, it's a very good solution that fulfills all task requirements correctly.",mistral-nemo,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('titanic.csv')

# Preprocessing
df = df.drop(['Name...",6,"The generated code addresses most of the requirements but has several significant deficiencies, particularly in data preprocessing and the interpretation of feature coefficients.

**Strengths:**
*   **Addresses core task:** It successfully trains a logistic regression model, evaluates it with multiple metrics, and includes cross-validation.
*   **Basic preprocessing:** Handles missing values and drops irrelevant columns.
*   **Visualization:** Generates a confusion matrix and ROC curve.
*   **Proper imports:** Includes necessary libraries.

**Weaknesses and Areas for Improvement:**
1.  **Data Preprocessing (Critical):**
    *   **Improper Categorical Encoding:** The use of `LabelEncoder` for 'Sex' and 'Embarked' is incorrect for logistic regression, especially when 'Embarked' has more than two categories. `LabelEncoder` assigns ordinal relationships (e.g., 0, 1, 2) which are not appropriate for nominal categorical features. One-hot encoding is necessary for these features to avoid misleading the model about magnitude ordering. The ground truth correctly uses `OneHotEncoder`.
    *   **Scaling Order:** `StandardScaler` is applied to `X` *before* categorical encoding. This would scale the `LabelEncoded` values, which is problematic. It should be applied *after* one-hot encoding or, ideally, handled within a `ColumnTransformer` (as in the ground truth) to ensure only numerical features are scaled.
    *   **`Pclass` Handling:** The `Pclass` column is a categorical variable (1st, 2nd, 3rd class), but the generated code treats it as a numerical feature by directly scaling it. This is generally inappropriate for such a feature; it should be one-hot encoded like 'Sex' and 'Embarked'.
    *   **Feature Engineering:** The task explicitly asks for feature engineering (FamilySize). The generated code skips this.

2.  **Feature Coefficients Interpretation (Significant):**
    *   **Incorrect Feature Names:** Because of the preprocessing issues (especially `LabelEncoder` and direct scaling), the `importance` DataFrame's 'feature' column (`df.columns[:-1]`) does not correctly map to the scaled and encoded features the model was actually trained on. The `model.coef_` array corresponds to the transformed `X` (scaled quantitative features and ordinal-encoded categorical features), but the labels shown are the original column names. This makes the interpretation of coefficients highly misleading or outright wrong, especially for the `LabelEncoded` columns. The ground truth correctly extracts transformed feature names from the preprocessor pipeline.
    *   **Lack of Interpretation Details:** While it prints coefficients, it does not provide the qualitative interpretation (positive/negative impact, magnitude) as requested by the prompt.

3.  **Completeness & Detail:**
    *   **Missing `ColumnTransformer` and `Pipeline`:** The ground truth code elegantly uses `ColumnTransformer` and `Pipeline` to encapsulate preprocessing and model training. This makes the code much cleaner, more robust, and prevents data leakage (e.g., scaling based on test data). The generated code implements these steps manually, leading to potential issues and less maintainable code.
    *   **No detailed cross-validation results:** The ground truth provides mean and standard deviation for *all* metrics in cross-validation, offering a more comprehensive assessment of model stability. The generated code only prints the mean accuracy.
    *   **Repetitive ROC-AUC calc for plot:** The `roc_auc_score` is calculated twice for the ROC plot setup.

The most critical issue is the incorrect handling of categorical variables and the resulting flawed interpretation of coefficients. This directly impacts the correctness and reliability of the model.",mistral-nemo,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']),
        'Ticker': ['TECHCO']*8,
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
        'Close': [152.0, 151.0, 153.5, 155.0, 1...",10,"The generated code is excellent. It correctly performs all the requested tasks:

1.  **DataFrame Creation and Indexing**: The DataFrame is created accurately using the provided `data`, and 'Date' is correctly set as the index.
2.  **SMA Calculation**: The 3-day simple moving average for the 'Close' price is calculated correctly using `rolling(window=3).mean()`. The `NaN` handling for initial days is inherent in `rolling` and is correctly applied.
3.  **Volume Percentage Change**: The percentage change in 'Volume' is calculated correctly using `.pct_change()`. The `NaN` handling for the first day is also inherent and correctly applied.
4.  **Output**: The resulting DataFrame with the new columns is printed as required.
5.  **Code Quality**: The code is clean, well-structured, and includes the necessary import (`pandas`).

The solution is complete and demonstrates a clear understanding of pandas functionalities for time-series data analysis.",mistral-nemo,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"Here's the Python code that fulfills your requirements using pandas:

```python
import pandas as pd

# Create the DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
        'Attendance_Ra...",9,"The generated code correctly addresses all three requirements of the task. It properly creates the DataFrame, calculates the 'Score_Improvement' column, groups by 'Subject' to find the average improvement, and filters students based on the specified criteria. The imports are correct, and the code structure is logical and readable.

One minor improvement could be in the printing of the 'top_students'. While `print(top_students)` does show the StudentIDs, it also prints the pandas Series index which is not strictly necessary for the output requested by the prompt (""print the 'StudentID' of the student(s)""). The ground truth code iterates and prints each ID, which is a bit cleaner for the specified output. However, this is a very minor stylistic point and does not detract from the core functionality or correctness. Overall, this is a very good solution.",mistral-nemo,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

filtered_df = df[(df[""Department""] == ""Tech"") & (df[""Per...",8,"The generated code correctly defines the DataFrame, filters it based on the specified conditions (`Department == ""Tech""` and `PerformanceScore > 3`), and accurately calculates the average `ProjectsCompleted` for the filtered group. The code is concise and directly addresses the core requirements.

However, it misses one minor aspect of the task description: ""calculate and print the average number of 'ProjectsCompleted'."" The generated code calculates `average_projects` but doesn't explicitly print it. In an interactive environment (like a Jupyter notebook), the last expression `average_projects` would be displayed, but in a standard script execution, it would not. The ground truth includes a `print` statement and also adds f-string formatting, which is good practice. Additionally, the ground truth includes an `if not filtered_df.empty:` check, which is a good robustness measure for production-ready code, although for this specific dataset and query, `filtered_df` will never be empty.

Improvements: Add a `print()` statement for the final result and optionally include a check for an empty `filtered_df`.",mistral-nemo,LLM_judge_config
M53,medium,Data Manipulation,,"Here's the Python code that defines the DataFrame and calculates the conversion rates for ""Desktop"" and ""Mobile"" users:

```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}

df = pd.DataFrame(data)

deskt...",9,"The generated code correctly defines the DataFrame and accurately calculates the conversion rates for both Desktop and Mobile users. It also prints the results clearly labeled, fulfilling all prompt requirements.

The code quality is good, using appropriate pandas functions for filtering and aggregation. The use of f-strings for printing is a good practice for readability.

One minor area for improvement, which is addressed in the ground truth, is handling the edge case where there might be zero users for a specific device type (e.g., `df[df[""DeviceType""] == ""Desktop""].shape[0]` would be 0, leading to a ZeroDivisionError). While not an issue with the provided sample data, robust code would include a check for this. The ground truth uses `.mean()` which implicitly handles this gracefully (returns `NaN` for an empty Series, which can then be specifically handled, or in the case of `df['Converted'].mean()` on an empty filtered DataFrame, it would return `NaN`). The generated code's approach of `sum() / shape[0]` is equally valid for non-empty data subsets but less robust for empty subsets. However, given the explicit task data, this is not a practical failure.

Overall, it's a very good solution.",mistral-nemo,LLM_judge_config
M54,medium,Data Aggregation,,"Here's the Python code that creates the DataFrame and calculates the average ""AirQualityIndex"" for each unique ""SensorID"":

```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32...",9,"The generated code correctly defines the DataFrame and accurately calculates the average ""AirQualityIndex"" for each unique ""SensorID"". It uses the `groupby()` method effectively. The code is concise and clear.

The only minor area for improvement, as suggested in the ground truth, relates to the output formatting. While the `print(avg_air_quality)` output is perfectly valid and readable for a script, the task asked for the output to ""clearly indicate which average belongs to which sensor."" The ground truth's use of a loop (`for sensor_id, avg_aqi in average_aqi_per_sensor.items(): print(f"" Sensor {sensor_id}: {avg_aqi:.2f}"")`) provides a slightly more user-friendly, sentence-like output, which aligns a bit more closely with ""clearly indicate."" However, the pandas Series output is also quite clear. This is a very minor distinction, hence the high score.",mistral-nemo,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"Here's the Python code that accomplishes the tasks you've described:

```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}

df = pd.DataFrame(data)

# 1) Create a n...",9,"The generated code correctly defines the DataFrame, calculates the ""EngagementScore"", filters for Instagram posts, and accurately calculates and prints their average ""EngagementScore"". The code is well-structured and uses appropriate pandas operations. It directly addresses all requirements of the task.

One minor point for improvement, which is not critical for correctness but good practice, is to handle cases where `instagram_posts` might be empty before attempting to calculate the mean. While in this specific dataset it won't be empty, in a general solution, checking `if not instagram_posts.empty:` before `instagram_posts[""EngagementScore""].mean()` can prevent potential errors if no Instagram posts were present. The ground truth code includes this check. However, for the given problem statement and data, the provided solution is functionally complete and correct. The output format for printing the average could also be formatted to two decimal places for consistency, but this is a minor stylistic point.",mistral-nemo,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",8,"The generated code correctly performs all the specified steps: defining the DataFrame, calculating `LengthOfStay`, filtering for ""Flu"" diagnosis, and calculating the average `LengthOfStay` for these patients.

**Strong points:**
*   **Correct DataFrame creation:** The DataFrame is correctly initialized with the provided data and `pd.to_datetime` is used for date columns.
*   **Correct calculation of `LengthOfStay`:** The subtraction `df[""DischargeDate""] - df[""AdmissionDate""]` correctly yields `Timedelta` objects.
*   **Correct filtering:** The boolean indexing `df[df[""Diagnosis""] == ""Flu""]` correctly filters the DataFrame.
*   **Correct average calculation:** `flu_patients[""LengthOfStay""].mean()` correctly calculates the mean of the `Timedelta` series.

**Areas for improvement (why not a 10):**
*   **Conversion to days:** The `LengthOfStay` column should ideally be converted to days explicitly using `.dt.days` after the subtraction. While `mean()` on `Timedelta` series correctly returns a `Timedelta` result, and accessing `.days` on that result eventually gives the number of days, creating the `LengthOfStay` column as an integer representing days (`(df[""DischargeDate""] - df[""AdmissionDate""]).dt.days`) is typically a more direct and common practice when ""LengthOfStay in days"" is requested. This directly stores the ""days"" unit in the column itself, making it clearer and potentially easier for subsequent numerical operations.
*   **Output formatting:** The final `print` statement accesses `average_length_of_stay.days`. While this works, explicitly converting the `LengthOfStay` column to days earlier would make the type of `average_length_of_stay` a float, leading to a more conventional display of average days (e.g., `5.67` instead of `5 days 16:00:00` which `.days` extracts to `5`). The ground truth specifically asks for ""days"", implying an integer or float representation, not a Timedelta object in the column. If the output needs to be a precise number of days, the `mean()` of Timedelta can result in fractional parts, which `dt.days` on a Timedelta *object* only gives the integer part. For example, an average of `5 days 12 hours` would yield `5` if `.days` is called on the mean Timedelta. If `LengthOfStay` were `dt.days` from the start, the average would be `5.5`. The ground truth rounds to `0.2f`.",mistral-nemo,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"Here's the Python code that fulfills your requirements:

```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000...",9,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates and prints their average rating. All requirements are met. The variable names `sci_fi_movies` and `avg_rating` are clear. The output format is correct. The only minor improvement could be to format the average rating to two decimal places for cleaner output, similar to the ground truth, but this is a stylistic choice and doesn't impact correctness. The generated code also provides the expected output, which is a good addition.",mistral-nemo,LLM_judge_config
M58,medium,Data Manipulation,,"Here's how you can accomplish this using pandas in Python:

```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

# Filter ...",10,"The generated code is excellent. It correctly defines the DataFrame, filters for Toyota cars, and accurately calculates their average mileage, addressing all requirements of the task. The code is clean, concise, and uses proper pandas functionality. The variable names are clear, and the output format is appropriate. It aligns perfectly with the ground truth in terms of functionality and quality.",mistral-nemo,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 80...",8,"The generated code correctly defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat. This demonstrates a solid understanding of basic pandas operations.

However, there are two minor improvements that would elevate the score:
1.  **Printing the result:** The task explicitly asks to ""print their average 'Attack' stat."" The generated code merely has `avg_attack` as the last line, which would display the value in an interactive environment (like a Jupyter notebook) but wouldn't print it to standard output in a script execution. Adding `print(avg_attack)` would fulfill this requirement.
2.  **Output formatting/clarity:** While not strictly required, the ground truth's use of an f-string for printing makes the output more user-friendly and descriptive (e.g., ""The average Attack stat for Fire-type Pokemon is: 66.67"").
3.  **Edge case handling:** The ground truth includes a check for an empty `fire_pokemon_df` to prevent potential errors if no Fire-type Pokemon were present, which is good practice for robust code. This is a minor omission but worth noting.",mistral-nemo,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Load data
df = pd.read_csv('titanic.csv')

# 1) Multi-panel figure
fig, axs = plt.subplots(2, 2, figsize=(15, 10))

for ax, var in zip(axs.flatten(), ['Pclass', 'Sex', 'AgeGroup', 'Survived']):
    sns.histplot(data=df, x=var, hue='Survived', ax=ax, kde=False)
    ax.set_xlabel(var)
    ax.legend(title='Survived\n(1 - Yes, 0 - No)')

plt.tight_layout()
plt.show()

# 2) Heatmap
sns.h...",6,"The generated code addresses most of the requirements but lacks sophistication in terms of data preprocessing, visualization choices for some tasks, and comprehensive storytelling as outlined in the prompt.

Here's a breakdown:

**Strengths:**
*   **Correct Imports:** All necessary libraries (pandas, matplotlib, seaborn, plotly) are correctly imported.
*   **Basic Task Fulfillment:** It attempts to address all five requested visualization types (multi-panel, heatmap, violin, interactive scatter, dashboard).
*   **Interactive Plot:** The Plotly scatter plot is correctly generated and interactive.

**Weaknesses and Areas for Improvement:**

1.  **Correctness (5/10):**
    *   **Data Preprocessing:** Crucial steps like handling missing 'Age' values and creating 'AgeGroup' bins are completely missing. This significantly impacts the quality and validity of some plots (e.g., age-related visualizations).
    *   **Multi-panel figure (Task 1):** The prompt asks for ""survival rates by different demographic groups"" (class, gender, age groups). The generated code uses `sns.histplot` which shows counts of survivors/non-survivors within categories, not the *rate*. This is a fundamental misinterpretation of the task. Also, it adds `Survived` as a variable in the loop for `axs.flatten()`, which is incorrect, leading to a plot showing survival counts by survival status itself, which is redundant. The multi-panel figure is also limited to 2x2.
    *   **Violin Plots (Task 3):** The prompt asks for violin plots comparing age distributions of survivors vs non-survivors *by gender*. The code iterates through gender and plots `Survived` vs `Age` for each gender separately, which isn't the most effective comparison. A single `sns.violinplot(x='Sex', y='Age', hue='Survived', data=df, split=True)` would be more effective for direct comparison.
    *   **Heatmap (Task 2):** It uses `fill_value=0`, which is not ideal for mean survival rates if a combination has no data; `aggfunc='mean'` is preferred with careful handling of NaNs if they appear in the result.

2.  **Code Quality (6/10):**
    *   **Lack of Preprocessing:** As mentioned, the absence of robust data cleaning and feature engineering (like `AgeGroup`) makes the code less production-ready and the visualizations less insightful.
    *   **Redundancy in Dashboard:** The dashboard reuses plots from previous steps without significant modification or strategic placement, leading to repetition. A dashboard should combine diverse, synergistic plots with proper annotations and storytelling elements, which are largely missing.
    *   **Readability/Comments:** While not explicitly requested, more descriptive variable names or comments could improve readability, especially for complex sections.
    *   **Plot Customization:** Titles are present, but labels, consistent y-axes for rates, and overall aesthetic enhancements are very minimal compared to the ground truth.

3.  **Completeness (6/10):**
    *   **Task 1 (Multi-panel):** Failed to show *rates* and incorrect variable selection for one panel. Did not include AgeGroup as it was not created.
    *   **Task 3 (Violin Plots):** Generated separate plots per gender rather than a single comparative plot.
    *   **Task 5 (Dashboard):** This is the weakest point. The prompt specifically asked for a ""comprehensive dashboard-style figure combining multiple visualizations with proper annotations and insights."" The generated dashboard is merely a 2x2 grid of basic plots without any text annotations, insights, or thoughtful layout. It's far from ""comprehensive dashboard-style."" The chosen plots for the dashboard are also simplistic Histplots rather than the richer rate-based bar plots or distributions.
    *   **""Storytelling"" Aspect:** The prompt emphasizes ""advanced data visualization and storytelling."" The generated code delivers basic plots but lacks the narrative, insights, and comprehensive integration (especially in the dashboard) that would constitute ""storytelling."" For example, the ground truth explicitly adds text insights to the dashboard.
    *   **No `numpy` import:** While not explicitly used in the generated code, tasks like creating proper age bins often benefit from `np.inf`.

Overall, the generated code provides a basic attempt at each visualization but misses crucial preprocessing steps, misinterprets some visualization requirements (e.g., rates vs. counts), and significantly falls short on the ""advanced"" and ""storytelling"" aspects, particularly for the dashboard. It's functional for basic plots but not for sophisticated analysis.",mistral-nemo,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"Here's the Python code that fulfills your requirements:

```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [...",10,"The generated code correctly defines the DataFrame, filters it for the 'Dystopian' genre, and accurately calculates and prints the average stock. The code is concise, imports necessary libraries, and follows good practices for pandas operations. It directly addresses all requirements of the task. The output format is also clear and matches typical expectations.",mistral-nemo,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, chisquare, shapiro, cohens_d

df = pd.read_csv('water_potability.csv')
df['Potability'] = df['Potability'].replace({'positive': 1, 'negative': 0})

# List of numerical columns excluding 'Potability'
numerical_cols = df.drop('Potability', axis=1).columns

def t_test(col):
    pot = df[df['Potability'] == 1][col]
    not_pot = df[df['Potability'] == 0][col]
    t_stat, p_val = ttest_ind(pot, not_pot)
    return t_s...",5,"The generated code attempts to address the requested tasks but falls short in several critical areas, particularly regarding statistical rigor, completeness, and clarity of output.

**Correctness (3/10):**
*   **Missing Value Handling:** The generated code completely
    ignores missing value handling, which is crucial for real-world datasets and statistical analysis. The ground truth code explicitly imputes missing values. This oversight will lead to errors or incorrect results if the CSV contains NaNs.
*   **Chi-square Test:** The `chisquare` function with `f_exp` calculated as `df['Potability'].value_counts(normalize=True) * len(df)` is incorrect for a goodness-of-fit test against a *uniform* expected distribution, as specified in the ground truth and typically implied when no specific distribution is given for a binary variable. This calculation essentially sets the expected frequencies to be the *observed* frequencies, leading to a chi-square statistic of 0 and a p-value of 1 (unless one of the observed counts is 0). This means the test will *always* indicate that the observed distribution *does* match the expected distribution (because the expected *is* the observed), which defeats the purpose of the test. The ground truth correctly assumes a 50/50 split for expected frequencies.
*   **Cohen's d:** The `cohens_d` function used in the generated code is not a standard function from `scipy.stats`. It's likely an undefined function and will cause an `AttributeError`. If it were hypothetically imported from an external library, `scipy.stats.ttest_ind` does not inherently return effect size; it needs to be calculated manually, as shown in the ground truth.
*   **Shapiro-Wilk:** The Shapiro-Wilk test has limitations on sample size (typically N < 5000). The generated code doesn't account for this, which could lead to warnings or less reliable results on larger datasets.
*   **T-test assumptions:** The `ttest_ind` function without `equal_var=False` assumes equal variances, which is often not true. Welch's t-test (`equal_var=False`) as used in the ground truth is more robust.

**Code Quality (6/10):**
*   **Imports:** Includes `cohens_d` which is not a standard `scipy.stats` function.
*   **Function Definition:** The `t_test` function is good for modularity, but its reliance on an undefined `cohens_d` and global `df` is problematic.
*   **Output:** The output is a raw dictionary (`results`) which is not user-friendly or interpretable. It lacks the formatted interpretation, p-value comparisons against alpha, and context provided by the ground truth.
*   **Error Handling:** No error handling for file not found.

**Completeness (4/10):**
*   **Missing Value Handling:** As mentioned, this crucial step is entirely missing.
*   **Interpretation of Results:** The code only calculates statistics and stores them in a dictionary. It does not display p-values, test statistics and interpretations as requested, or explicitly compare p-values to an alpha level (which is also not defined).
*   **Alpha Level:** The significance level (`Œ± = 0.05`) is not defined or used to interpret results.
*   **Effect Size Calculation:** The attempt at Cohen's d is incorrect due to the non-existent function. Even if it worked, the interpretation guidelines for effect size are missing.
*   **Print Statements:** The code lacks descriptive print statements to guide the user through the analysis, making it hard to understand what each part is doing or its significant findings.

**Key areas for improvement:**
1.  Implement robust missing value handling.
2.  Correctly implement the chi-square goodness-of-fit test hypothesis (e.g., against a uniform distribution for binary categories if no other distribution is specified).
3.  Manually calculate Cohen's d, as `scipy.stats` does not provide a direct function for it.
4.  Define and use a significance level (alpha) to interpret all p-values.
5.  Add clear print statements to display test statistics, p-values, and interpretations for *all* tests, as requested in the prompt.
6.  Consider robustness for t-tests (e.g., `equal_var=False`).
7.  Add file loading error handling.",mistral-nemo,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, plot_confusion_matrix, plot_roc_curve
from sklearn.preprocessing import StandardScaler
import matplotli...",6,"The generated code addresses most of the requirements but has several significant issues that prevent it from being a robust and correct solution.

**Positive Aspects:**
*   **Most requirements are attempted:** Data preprocessing, model training, multiple metrics, confusion matrices, ROC curves, cross-validation, model comparison, and feature importance are all included.
*   **Proper imports:** Most necessary libraries are imported.
*   **User-friendly output:** The print statements for model performances are clear.

**Areas for Improvement and Critical Errors:**

1.  **Data Preprocessing (Critical):**
    *   **Missing Value Handling:** The code uses `data = data.fillna(data.mean())` *before* the train-test split. This is a severe data leakage issue. The mean for imputation should only be calculated from the training data to avoid using information from the test set.
    *   **Feature Scaling:** Scaling is applied to the *entire dataset*(`data[['ph', ..., 'Potability']] = sc.fit_transform(...)`) before the train-test split, including the target variable `Potability`. The target variable (Potability) is a binary classification label and should not be scaled. Additionally, the `fit_transform` should only be on the training data, and `transform` on the test data, again to prevent data leakage.
    *   **Train-test split:** The split is not stratified, which is important for binary classification tasks (especially with potential class imbalance) to ensure both train and test sets have similar proportions of classes.

2.  **Evaluation Metrics and Visualization (Major):**
    *   **`plot_confusion_matrix` and `plot_roc_curve` are deprecated.** The `sklearn.metrics.ConfusionMatrixDisplay.from_estimator` and `sklearn.metrics.RocCurveDisplay.from_estimator` should be used instead. The code would likely throw warnings or errors in recent scikit-learn versions.
    *   **Incorrect `roc_auc` calculation:** `roc_auc_score(y_test, y_test_pred)` should be `roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])`. ROC AUC requires probability scores, not hard predictions. For SVC, `probability=True` was correctly set, but the usage is wrong.
    *   **`roc_curve` is imported but not used.** `plot_roc_curve` is used instead.
    *   **Plotting:** Each plot is created and shown separately. The task implicitly suggests combining ROC curves or at least presenting them in a more organized manner, which the ground truth does by plotting all ROCs on a single subplot.

3.  **Cross-Validation (Major):**
    *   **Data Leakage in CV:** `cv_scores = cross_val_score(model, X, y, cv=5)` is directly applied to `X` and `y` (the original data with faulty imputation and scaling). This means the cross-validation folds are re-imputing and re-scaling data in a way that doesn't reflect the preprocessing steps (which were done incorrectly upfront). The correct approach for cross-validation with preprocessing is to use a `Pipeline` that includes imputation and scaling *within* each fold, or to apply `fit_transform` and `transform` correctly per fold if not using a pipeline. The generated code calculates `cv_scores` based on `X` and `y` which are the *entire original datasets* after the initial flawed preprocessing. This `X` and `y` themselves contain scaling issues and a mean imputation that saw the whole dataset. This is incorrect.
    *   **Only Accuracy for CV:** Only 'accuracy' is used for `cross_val_score`. The task asks for comprehensive metrics, and cross-validation should ideally consider more than just accuracy, or at least be calculated for all relevant metrics.

4.  **Feature Importance (Minor but important):**
    *   **Handles only Random Forest:** Correctly implements feature importance for Random Forest but does not include handling for Logistic Regression (coefficients) or mention the lack of direct importance for SVM, as the ground truth does. This makes the solution incomplete for the ""best model"" selection.

5.  **Code Structure and Readability:**
    *   Lack of comments and clear sections make the code harder to follow. The ground truth code used comments to segment different tasks.
    *   Error handling for file loading is missing.

To improve, the code needs a fundamental re-think of the preprocessing steps (especially imputation and scaling) and how they interact with train-test split and cross-validation, ideally by using `sklearn.preprocessing.Pipeline`. The metric calculation for ROC AUC needs correction, and deprecated plotting functions should be updated. More comprehensive cross-validation metrics and handling of feature importance for all model types would solidify the solution.",mistral-nemo,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import SelectKBest, chi2, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

# 1) Load the dataset and create new features
df = pd.read_csv('water_potability.csv')
df['pH_Chloramines'] = df['pH'] / df['Chloramines']
df['Hardness_Solids'] = df['Hardness'] / d...",4,"The generated code attempts to address several aspects of the prompt but falls significantly short on correctness, completeness, and robustness.

**Major Issues:**

1.  **Missing Value Handling:** The ground truth correctly identifies and handles missing values. The generated code completely skips this crucial step, which would lead to errors in subsequent calculations (e.g., correlations, feature engineering, and model training). `df.corr()` would return `NaN` for columns with NaNs, and `PolynomialFeatures` would fail.
2.  **Feature Engineering - Ratios:** The generated code only creates two ratio features (`pH_Chloramines` and `Hardness_Solids`). The prompt explicitly asks for ""ratios between chemical parameters (e.g., pH/Chloramines, Hardness/Solids)"" - plural, and implies more diverse ratios. The ground truth provides several more meaningful ratios. Also, the generated code doesn't handle potential division by zero for the ratio features, which the ground truth correctly addresses with `epsilon`.
3.  **Feature Engineering - Interaction Terms:** The generated code entirely misses the requirement for ""interaction terms."" This is a significant omission.
4.  **Feature Engineering - Polynomial Features:**
    *   The generated code applies `PolynomialFeatures` to only a subset of original columns (`pH`, `Chloramines`, `Hardness`, `Solids`) and adds them directly to the DataFrame. This is less comprehensive than the ground truth, which includes more relevant columns for polynomial expansion.
    *   `df_poly = pd.DataFrame(poly.fit_transform(df[['pH', 'Chloramines', 'Hardness', 'Solids']]), columns=poly.get_feature_names(['pH', 'Chloramines', 'Hardness', 'Solids']))` - The `get_feature_names` method is deprecated, and `get_feature_names_out` should be used. More importantly, this implementation implicitly overwrites columns with the same name if those original columns were selected for `PolynomialFeatures`, which is generally not the intent. The ground truth carefully avoids re-adding original columns as separate polynomial features.
5.  **Correlation Analysis:**
    *   ""Identify the top 5 features most correlated with water potability"": The generated code does `corr_with_potability.iloc[:5]`. This includes 'Potability' itself as the top correlated feature (correlation of 1.0), which is trivially true and not what's typically desired when identifying *""other""* features correlated with the target. The ground truth correctly excludes 'Potability' from the top 5 *other* correlated features.
    *   The visualization for the correlation heatmap is basic (`annot=True` makes it cluttered for many features; `fmt="".2f""` for better readability is missing; `linewidths` and `figsize` are not optimally set for a large matrix).
6.  **Feature Selection:**
    *   **Scaling:** A critical omission. Feature selection methods like RFE and SelectKBest (especially when using `f_classif` or `chi2` where feature scales can influence scores or model convergence) generally perform better or require scaled data. The ground truth correctly applies `StandardScaler` before feature selection.
    *   `SelectKBest`: The use of `chi2` is problematic as it requires non-negative features and works best with categorical data, or for cases where target is categorical (which is true here, but features might have negative values or poor distribution for chi2). `f_classif` is more generally applicable for numerical features with a categorical target. The ground truth correctly uses `f_classif`.
    *   The `X_ch2`, `X_f`, `X_rfe` variables store the *transformed* data, not the selected feature names, making it harder to track which features were selected for comparison. The ground truth correctly extracts the feature names using `get_support()`.
7.  **Visualizations:**
    *   **Feature Importance:** The generated code tries to plot feature importance but accesses `rfe.estimator_.feature_importances_.index`. The `estimator_` is a `RandomForestClassifier`, and its `feature_importances_` attribute returns a NumPy array, not a Pandas Series with an index. This line would cause an `AttributeError`. The ground truth correctly creates a Pandas Series for easy plotting. Also, the ground truth plots the *top* features for better readability; the generated code plots all, which can be messy.
    *   **Pairwise Scatter Plots:** The loop `for feats in selected_features.values(): sns.pairplot(df[feats])` will generate multiple pairplot figures, possibly on top of each other, and without any distinction (like `hue='Potability'`), making them far less informative than the ground truth. It also calls `plt.show()` only once at the end, meaning only the last plot might be visible. The ground truth creates a single, highly informative pairplot for the *top correlated features* with the target as hue.
8.  **Comparison of Feature Sets (Section 6):** While the generated code attempts a comparison (`ch2_set - f_set - rfe_set` etc.), the output is much less structured and informative than the ground truth, which clearly states overlaps and uniques for each method, with print statements that explain them. The `selected_features` dictionary in the generated code contains `Index` objects which are not directly sets of strings, making the comparison potentially brittle or less readable.

In summary, the generated code provides a skeleton that attempts to hit the prompt requirements but misses critical details, implements some aspects incorrectly, and lacks the depth and robustness of the ground truth, particularly in feature engineering comprehensiveness, data handling, and visualization quality.",mistral-nemo,LLM_judge_config
