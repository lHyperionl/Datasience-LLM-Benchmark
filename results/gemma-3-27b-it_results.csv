task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: winequality-red.csv not found in the current directory."")
    exit()

# 1. List column names and data types
print(""Column Names and Data Types:"")
for col in df.columns:
    print(f""- {col}: {df[col].dtype}"")

print(""\n"")

# 2. Calculate descriptive statistics for numerical columns
numerical_cols = df.select_dtypes(include=['number']).columns
print(""Descri...",8,"The generated code correctly loads the dataset, identifies data types, calculates descriptive statistics for numerical columns, and reports dataset dimensions. It handles the `FileNotFoundError` gracefully.

**Strong points:**
*   Correctly uses `pd.read_csv` and handles file not found.
*   Accurately lists column names and their pandas dtypes.
*   Correctly identifies numerical columns using `df.select_dtypes(include=['number'])`.
*   Calculates all required descriptive statistics (mean, median, std, min, max, quartiles) for numerical columns.
*   Correctly reports the total number of rows and columns using `df.shape`.

**Areas for improvement:**
*   **Identification of Categorical/Ordinal Columns:** The approach for identifying categorical/ordinal columns is heuristic and not entirely robust.
    *   It identifies `object` dtype columns, which is correct for pandas string/mixed types, but `winequality-red.csv` typically doesn't have these.
    *   It then identifies columns with `nunique() <= 10` as ""low cardinality"". While this is a common heuristic for potential categorical/ordinal columns, it's not a definitive identification of *data type*. For instance, `quality` (which ranges from 3 to 8) will be flagged as ""low cardinality"" and potentially categorical/ordinal, which is often *treated* as ordinal, but its *data type* is numerical. The ground truth code handles this by differentiating between `object` dtypes and then adding a specific note about `quality` being treated as ordinal. The generated code's output for this section is a bit simplistic compared to the detailed analysis in the ground truth.
    *   Using `pd.api.types` for more robust type checking, as shown in the ground truth, would be more precise.
*   **Output Formatting:** While functional, the output formatting for descriptive statistics is very verbose (printing each statistic on a new line). Using `df[numerical_cols].describe()` would provide a more compact and readable summary, consistent with standard data analysis practices, and also aligns better with the ground truth's efficiency.
*   **Redundant Output:** The prompt asks for ""only the Python code without explanations,"" but the generated code includes multiple `print(""\n"")` statements that separate sections, which is fine for readability but technically part of the output layout, not just the logical code.

Overall, it's a solid solution that covers most requirements effectively. The main area for enhancement is the categorical/ordinal column identification logic and output formatting.",gemma-3-27b-it,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Display the first 10 rows
print(df.head(10))

# Show the shape of the dataset
print(df.shape)

# List all column names and their data types
print(df.dtypes)

# Display the top 10 games by Global_Sales
top_10_games = df.nlargest(10, 'Global_Sales')
print(top_10_games)

# Show the unique values in the Platform and Genre columns
print(""Unique Platforms:"", df['Platform'].unique())
print(""Unique Genres:"", d...",7,"The generated code addresses most of the requirements but lacks some robustness and completeness compared to the ground truth.

**Strengths:**
- Correctly loads the data and displays the first 10 rows.
- Correctly shows the shape of the dataset.
- Correctly displays unique values for 'Platform' and 'Genre'.
- Correctly calculates and displays summary statistics for sales columns.
- Correctly identifies and displays missing values.
- Uses `nlargest` for finding top games, which is an efficient method.

**Areas for Improvement:**
- **Missing FileNotFoundError handling:** The ground truth includes a `try-except` block to handle cases where the CSV file is not found, making the code more robust. The generated code will crash if the file is missing.
- **`df.info()` vs `df.dtypes`:** While `df.dtypes` shows the data types, `df.info()` from the ground truth provides a more comprehensive overview including non-null counts and memory usage, which is often preferred in EDA for quickly assessing data quality. The prompt specifically asked for ""all column names and their data types"", and `df.info()` delivers this better.
- **Top 10 Games Output:** The generated code prints `top_10_games` which includes all columns of the top 10 games. The ground truth code selectively displays relevant columns (`Name`, `Platform`, `Genre`, `Publisher`, `Global_Sales`), which is better practice for readability in EDA.
- **Column Existence Checks:** The ground truth includes checks (`if 'Global_Sales' in df.columns:`) before attempting operations on specific columns. This prevents errors if the dataset structure deviates slightly from expectations. The generated code assumes all specified columns exist.
- **Clarity and Formatting:** The ground truth code includes print statements with clear headers (`--- 1. Loading Dataset...`, `--- 2. Dataset Shape...`) and separators (`=`*80), making the output much more readable and organized. The generated code lacks these, making the output a bit jumbled.
- **Completeness of missing values display:** While `df.isnull().sum()` is correct, the ground truth code further filters to show *only* columns with missing values and presents a clearer message, which is more useful for EDA as it declutters the output.

Overall, the code is functional and performs the requested tasks but could be significantly improved for robustness, user experience, and comprehensiveness in reporting EDA results.",gemma-3-27b-it,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values in Year
median_year = df['Year'].median()
df['Year'].fillna(median_year, inplace=True)

# Remove games with missing Publisher
df.dropna(subset=['Publisher'], inplace=True)

# Create 'Years_Since_Release' column
current_year = 2023
df['Years_Since_Release'] = current_year - df['Year'].astype(int)

# Remove duplicate games
df.drop_duplicates(subset=['Name', 'Platform'], inplace=True...",7,"The generated code correctly addresses most of the task requirements, including loading data, handling missing 'Year' and 'Publisher' values, creating 'Years_Since_Release', removing duplicates, and filtering low-selling games. It also correctly displays the shape and head.

However, there are a few areas for improvement:
1.  **Robustness for 'Year' column conversion**: The ground truth code explicitly converts the 'Year' column to numeric with `errors='coerce'` before calculating the median and filling NaNs. This is crucial as the 'Year' column might contain non-numeric strings, which would cause an error in the generated code's `df['Year'].median()` or `df['Year'].astype(int)` if `fillna` doesn't resolve all non-numeric issues.
2.  **Robustness for 'Global_Sales' column conversion**: Similar to 'Year', the 'Global_Sales' column should be explicitly converted to numeric with `errors='coerce'` before filtering. If there are non-numeric values in 'Global_Sales', the comparison `df['Global_Sales'] >= 0.1` will likely raise a `TypeError`. The ground truth handles this by converting first and then dropping NaNs created during conversion.
3.  **Missing error handling for file loading**: The generated code lacks a `try-except` block for `pd.read_csv`, which means it would crash if the `video_games_sales.csv` file is not found. The ground truth includes robust error handling.
4.  **Implicit type conversion for 'Year' after fillna**: While the generated code uses `astype(int)` before calculating `Years_Since_Release`, it doesn't explicitly cast 'Year' to integer *after* filling NaNs, which might be desirable for consistency if the column isn't already integer-like. The ground truth does this.
5.  **Handling negative Years_Since_Release**: While less common, if some 'Year' values are in the future or incorrect, `current_year - df['Year']` could result in negative values. The ground truth adds a check to ensure `Years_Since_Release` is non-negative. This is a minor point but adds to robustness.

Overall, it's a good attempt, but lacks some of the robustness and edge-case handling present in the ground truth code, particularly concerning data type conversions and file loading.",gemma-3-27b-it,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 1. Bar chart: Top 10 gaming platforms by total global sales
top_platforms = df.groupby('Platform')['Global_Sales'].sum().nlargest(10)
plt.figure(figsize=(12, 6))
top_platforms.plot(kind='bar')
plt.title('Top 10 Gaming Platforms by Global Sales')
plt.xlabel('Platform')
plt.ylabel('Global Sales (in millions)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 2. Pie...",6,"The generated code addresses most of the visualization requirements. However, there are significant issues that prevent it from being a fully correct and robust solution.

**Positive Aspects:**
*   It successfully generates all the requested plot types (bar, pie, histogram, scatter, line).
*   Titles and labels are generally present and appropriate.
*   Basic plotting commands are correct.

**Areas for Improvement/Incorrectness:**
1.  **Data Loading and Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and proper type conversion for 'Year_of_Release' and sales columns. The generated code lacks any error handling for file loading, which is a critical omission for a production-ready script. It also assumes 'Year' column exists and is clean, whereas the ground truth correctly uses 'Year_of_Release' and performs necessary cleaning to ensure it's numeric and usable.
2.  **Data Cleaning/Preprocessing:** The generated code completely skips essential data cleaning steps that the ground truth performs (converting 'Year_of_Release' and sales columns to numeric, handling NaNs). This could lead to errors or incorrect plots if the raw data isn't perfectly clean. For instance, `df['Year']` in the line plot might not exist or be of the wrong type without preprocessing.
3.  **Line Plot 'Year' column:** The ground truth correctly uses `Year_of_Release` after cleaning. The generated code uses `df['Year']` which is likely incorrect based on typical dataset column names and the ground truth's use of `Year_of_Release`. Even if it existed, without cleaning it could lead to errors. The `xticks` generation in the line plot is also problematic if the 'Year' column contains non-numeric values or NaNs.
4.  **Pie Chart Readability:** The ground truth includes a thoughtful approach to dealing with too many categories in the pie chart by combining small genres into an 'Other' category, which enhances readability. The generated code simply plots all categories, which can make the pie chart cluttered and unreadable if there are many genres with small counts.
5.  **Robustness/Details:** The ground truth includes details like `grid` lines, `alpha` for scatter plots, and more specific `color` choices, contributing to better aesthetics and data presentation. It also ensures 'Year_of_Release' is an integer, which is good practice. The histogram's `range` in the ground truth is also a good improvement for skewed data.",gemma-3-27b-it,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Display the first 10 rows
print(""First 10 rows:"")
print(df.head(10))
print(""\n"")

# Show the shape of the dataset
print(""Shape of the dataset:"")
print(df.shape)
print(""\n"")

# List column names and data types
print(""Column names and data types:"")
print(df.dtypes)
print(""\n"")

# Calculate the percentage of diabetic vs non-diabetic patients
print(""Percentage of diabetic vs non-diabetic patients:"")
print(df['Outco...",7,"The generated code correctly addresses most of the requirements but has a few shortcomings.

**Strengths:**
- Correctly loads the CSV and displays the first 10 rows.
- Correctly shows the shape of the dataset.
- Lists column names and data types accurately (though `df.info()` from ground truth is more comprehensive as it also shows non-null counts).
- Correctly calculates and displays the percentage of diabetic vs non-diabetic patients.
- Accurately displays the count of missing values for each column.
- Correctly shows basic summary statistics for numerical columns.

**Areas for Improvement:**
- **Missing Error Handling:** The generated code lacks `try-except` blocks for file loading, which is crucial for robust applications.
- **Ambiguous Zero Value Identification:** The task specifically asked to ""Identify any columns with zero values that might represent missing data."" The generated code uses `df.columns[df.eq(0).any()].tolist()` which identifies *any* column containing *at least one* zero. This is less precise than the ground truth which explicitly lists `COLUMNS_TO_CHECK_FOR_ZERO_AS_MISSING` and then counts zeros *within those specific columns*, which is more aligned with the common challenge in this dataset (e.g., BloodPressure cannot be 0).
- **Output Clarity/Formatting:** While functional, the output formatting is less structured and clear compared to the ground truth which uses headers and separators (`---` and `===`) for better readability, making it easier to follow each step of the EDA process. The ground truth also provides more specific labels (e.g., ""rows, columns"" for shape).
- **Use of `df.info()` vs `df.dtypes`**: While `df.dtypes` is correct for listing types, `df.info()` (used in the ground truth) is generally preferred for this step in EDA as it also provides non-null counts, which is valuable information regarding data completeness.
- **Completeness of zero-value analysis:** The ground truth also provides the *count* and *percentage* of zero values, which is more informative than just listing the columns that contain zeros. The generated code only lists the columns.",gemma-3-27b-it,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Identify columns with zero values representing missing data
zero_value_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

# Replace zero values with NaN
for col in zero_value_cols:
    df[col] = df[col].replace(0, pd.NA)

# Handle missing values by filling with the median
for col in zero_value_cols:
    df[col].fillna(df[col].median(), inplac...",7,"The generated code addresses most of the requirements but has several areas for improvement in terms of robustness, completeness, and best practices.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly identifies and replaces zero values with `pd.NA` (or `np.nan`).
*   Correctly fills missing values with the median.
*   Successfully removes duplicate rows.
*   Creates the `BMI_Category` column although with a slightly less efficient method than `pd.cut`.
*   Scales numerical features.
*   Displays the shape and head of the cleaned dataset.

**Areas for Improvement:**
*   **Robustness (File Handling):** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for real-world scenarios. The generated code lacks this.
*   **Robustness (Column existence):** The generated code assumes all `zero_value_cols` and `numerical_cols` exist. If a column is missing, it will raise a `KeyError`. The ground truth includes checks (e.g., `if col in df.columns`).
*   **Completeness (BMI Categorization):** The `categorize_bmi` function uses `< 25` and `< 30` which might lead to `BMI 24.9` falling into `Normal` and `BMI 29.9` falling into `Overweight`, which is correct. However, `pd.cut` is generally more efficient and idiomatic for this task in pandas. The generated code also implicitly handles the `<= 24.9` and `<= 29.9` aspects by just using `< 25` and `< 30`, but the bins in `pd.cut` are more explicitly defined to match the exact problem statement's ranges (e.g., `18.5-24.9`).
*   **Completeness (Numerical Features for Scaling):** The `numerical_cols` list is hardcoded, which is less flexible. The ground truth dynamically selects all numerical columns and then explicitly removes 'Outcome', which is generally a better approach as it adapts to datasets with different numbers of numerical features and correctly excludes the target.
*   **Completeness (Logging/Information):** The ground truth provides more informative print statements throughout the process, indicating what step is being performed, showing original vs. cleaned shapes, and confirming actions like duplicate removal or scaling.
*   **Clarity (`pd.NA` vs `np.nan`):** While `pd.NA` is pandas' generic missing value indicator, `np.nan` is more commonly used for numerical missing data and integrates well with `median()` and other numpy-based operations. Both work here, but `np.nan` is more traditional for numerical NaNs.
*   **Formatting of Output:** The final print statements' formatting could be slightly cleaner for readability (e.g., adding newlines for better separation).",gemma-3-27b-it,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

# Histograms
df.hist(figsize=(12, 10))
plt.suptitle('Histograms of Numerical Features', y=0.92)
plt.show()

# Box plots
numerical_features = df.select_dtypes(include=['number']).columns.tolist(...",7,"The generated code successfully creates most of the requested visualizations.

**Strengths:**
*   Correctly loads the dataset.
*   Generates a correlation heatmap, histograms, box plots, pie chart, scatter plots, and a pair plot as requested.
*   Includes proper imports.
*   Titles are generally present for most plots.

**Areas for Improvement:**
*   **Histograms:** The generated code uses `df.hist()`, which is a quick way to plot all histograms but lacks the `kde` (Kernel Density Estimate) and specific color/bin customization seen in the ground truth. It also creates separate figures for each histogram sub-plot, making the output less organized than a single figure with subplots. The ground truth groups them efficiently into one figure.
*   **Box Plots:** The generated code creates a separate figure for *each* box plot, which can lead to many pop-up windows and an unorganized output. The ground truth combines them into subplots within a single figure. Also, the `numerical_features` in the generated code include 'Outcome', which is a categorical target variable, not a numerical feature to be compared against itself in a box plot. The ground truth correctly identifies numerical columns *excluding* 'Outcome' for these plots.
*   **Pie Chart:** The generated code's pie chart doesn't include explicit labels for `0` and `1` values of `Outcome`, making it less readable than the ground truth's `Non-Diabetic (0)` and `Diabetic (1)`. It also lacks custom colors and `explode` for better visual presentation.
*   **Scatter Plots Legends:** The legends for the scatter plots in the generated code just show '0' and '1', which are less informative than the `labels=['Non-Diabetic', 'Diabetic']` used in the ground truth.
*   **Pair Plot `hue`:** The pair plot in the generated code correctly includes `Outcome` in the selected features but does not use `hue='Outcome'`, which is crucial for distinguishing patterns based on diabetes outcome in the plot. The ground truth uses `hue='Outcome'` and `diag_kind='kde'` for a more insightful visualization.
*   **Missing Error Handling/Info:** The generated code lacks the `try-except` block for `FileNotFoundError` and the initial `df.head()`, `df.info()`, `df.describe()` print statements, which are useful for understanding the dataset upon loading.
*   **No `sns.set_style`:** The ground truth sets a `sns.set_style` for better aesthetics across all plots.

Overall, the generated code fulfills the core visualization requirements but could significantly improve in terms of plot organization, customization, and readability for a more complete and professional presentation.",gemma-3-27b-it,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""world-happiness-report-2021.csv"")

# Display the first 10 rows
print(""First 10 rows:"")
print(df.head(10))

# Show the shape of the dataset
print(""\nShape of the dataset:"")
print(df.shape)

# List column names and data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# Display unique regional indicators and count of countries per region
print(""\nUnique regional indicators and country counts:"")
print(df['Regional in...",7,"The generated code correctly addresses most of the requirements.

**Strong Points:**
*   Correctly loads the dataset.
*   Displays the first 10 rows.
*   Correctly identifies the shape.
*   Shows column data types.
*   Correctly calculates and displays unique regional indicators and counts.
*   Correctly identifies and displays the top 10 happiest countries.
*   Calculates and displays summary statistics for the specified columns.
*   Identifies and displays missing values.
*   Includes proper imports.

**Areas for Improvement:**
*   **Robustness (File Not Found):** The generated code does not include any error handling for `FileNotFoundError`. If the CSV file is not present, the script will crash. The ground truth handles this with a `try-except` block, which is crucial for real-world robustness.
*   **Clarity vs. `df.info()`:** For task 3 (column names and data types), `df.dtypes` is correct, but `df.info()` as used in the ground truth provides a more comprehensive overview, including non-null counts, which is generally more informative. While `df.dtypes` is valid, `df.info()` aligns better with typical exploratory data analysis ""info"" needs.
*   **Column Existence Checks:** The generated code assumes all specified columns (e.g., 'Ladder score', 'GDP per capita', 'Regional indicator') exist. If a column name is misspelled or absent, it will throw a `KeyError`. The ground truth includes checks (e.g., `if 'Regional indicator' in df.columns:`) which makes the code more robust against variations in dataset schema.
*   **Column Name Discrepancy:** For summary statistics, the generated code uses 'GDP per capita', whereas the actual column in the dataset (and used in the ground truth) is 'Logged GDP per capita'. This would lead to a `KeyError` if not for the fact that Pandas `describe()` on a non-existent column in a list will just not include it, but it's an oversight. The ground truth explicitly uses `Logged GDP per capita` and includes a validation loop to ensure only existing columns are described.
*   **Output Formatting:** While functional, the output formatting lacks the clear separation and descriptive headers seen in the ground truth, which enhances readability significantly for an EDA task.",gemma-3-27b-it,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
import numpy as np

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Handle missing values
for col in df.columns:
    if df[col].isnull().any():
        if df[col].dtype == 'float64' or df[col].dtype == 'int64':
            df[col].fillna(df[col].mean(), inplace=True)
        else:
            df[col].fillna(df[col].mode()[0], inplace=True)

# Create new derived features
df['Happ...",6,"The generated code addresses most of the requirements but has several issues affecting its correctness and robustness.

**Strengths:**
*   Correctly loads the dataset.
*   Handles missing values reasonably well for numerical and categorical types (though the ground truth's specific type selection is better).
*   Creates `Happiness_Category` and `Well_being_Score`
*   Applies IQR method for outlier removal on 'Ladder score'.
*   Standardizes some numerical features.
*   Encodes `Regional indicator` using `LabelEncoder`.
*   Displays the requested shape and head.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling:**
    *   **Incorrect `GDP_Category` Feature:** The generated code uses `'GDP per capita'` for `GDP_Category` generation, but the task explicitly mentions `'Logged GDP per capita'` as used in the ground truth. This is a critical error as it works with a different column and might lead to errors if 'GDP per capita' doesn't exist or has different properties.
    *   The `if df[col].dtype == 'float64' or df[col].dtype == 'int64':` check for numerical columns is less robust than `df.select_dtypes(include=np.number).columns` as used in the ground truth. It might miss `int32` or other numerical types, although for this specific dataset it might work.
    *   The missing value handling loop iterates over all columns. The ground truth first identifies numerical and categorical columns, then iterates only over those that *have* missing values, which is slightly more efficient and clearer.

2.  **Derived Features:**
    *   **`Happiness_Category`:** While `pd.cut` is a valid approach, the bins/labels don't perfectly match the precise conditions given in the prompt (`>6.5`, `4-6.5`, `<4`).
        *   `bins=[0, 4, 6.5, np.inf]` and `labels=['Low', 'Medium', 'High']` imply:
            *   (0, 4] -> Low
            *   (4, 6.5] -> Medium
            *   (6.5, inf) -> High
        *   The ground truth `np.select` is more precise for the given conditions, especially for the boundary conditions (e.g., `<=6.5` vs. `>6.5`). For example, a `Ladder score` of exactly 4 would be ""Low"" in the generated code and ""Medium"" in the ground truth, based on the prompt's `4-6.5` range.
    *   **`Well_being_Score`:** The average calculation `(A + B + C) / 3` is correct, but using `.mean(axis=1)` on a selection of columns (as in the ground truth) is generally more concise and scalable for more columns.

3.  **Standardization of Numerical Features:**
    *   The `numerical_features` list hardcodes specific columns. This is less robust than dynamically selecting numerical columns (`select_dtypes`) as done in the ground truth, especially for datasets that might have slight column name variations or additional numerical features.
    *   Crucially, `Ladder score` *before* outlier removal was used to define the bounds, but it's *after* outlier removal that it's being standardized. More importantly, the `Ladder score` is the only column that *had* outliers removed, and it's being standardized along with other columns that *didn't* necessarily have outliers removed in this specific way. While not strictly an error for `StandardScaler`, it's an assumption that all numerical features are treated equally.

4.  **Error Handling:** The ground truth includes `try-except FileNotFoundError`, which is good practice for robust code. The generated code lacks this.

5.  **Hardcoded Column Names:** The hardcoded list of numerical features for scaling is prone to errors if the dataset schema changes slightly. `select_dtypes` is superior.

To improve, the code should:
*   Correctly use `'Logged GDP per capita'` for `GDP_Category`.
*   Refine `Happiness_Category` binning/logic to precisely match the given ranges.
*   Use `select_dtypes` for more robust numerical/categorical column identification.
*   Consider adding basic file loading error handling.",gemma-3-27b-it,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt
import folium
import numpy as np

# 1. Load the dataset
df = pd.read_csv(""world-happiness-report-2021.csv"")

# 2. World map visualization (using plotly)
fig_map = px.choropleth(df, locations=""Country"",
                        locationmode='country names',
                        color=""Ladder score"",
                        hover_name=""Country"",
                        color_continuous...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being a strong solution.

**Positive Aspects:**
*   **Most Visualizations Implemented:** It successfully attempts to create all the requested visualization types: world map, bar chart, heatmap, scatter plots, box plots, and radar chart.
*   **Correct Libraries Used:** It uses `plotly`, `seaborn`, and `matplotlib` as requested.
*   **Basic Functionality:** For many plots, the basic functionality is present (e.g., plotting data points, showing a map).

**Areas for Improvement / Issues:**

1.  **Dataset Loading and Column Renaming:** The ground truth includes robust error handling for `FileNotFoundError` and renames columns for better readability and consistency. The generated code lacks these, using original column names like `Ladder score` (which is less readable) and assuming the file exists.
2.  **World Map (Plotly):**
    *   The `locations=""Country""` and `hover_name=""Country""` might not correctly map all country names if the dataset uses different conventions than Plotly's internal mappings. The ground truth uses `Country name` which is the correct column name in the `world-happiness-report-2021.csv` and `locationmode=""country names""` is specified, which helps.
    *   No custom layout updates are added to the map for better presentation (e.g., `showframe`, `showcoastlines`, `projection_type` in `geo` attribute).
3.  **Horizontal Bar Chart:**
    *   The generated code uses `plt.plot(kind=""barh"")` directly on the series, which works but styling is very basic. The ground truth uses `sns.barplot` which provides better aesthetics with minimal effort (e.g. palette, sorting).
    *   The ground truth explicitly sets `figsize` for Matplotlib plots, which is a good practice for consistent output. The generated code has inconsistent figure sizes.
4.  **Correlation Heatmap:**
    *   The `df.corr()` call includes non-numeric columns if any exist in the dataframe, which can lead to errors or unnecessary columns in the heatmap. The ground truth explicitly selects relevant numerical columns for correlation, which is much safer and more specific to the task.
    *   Missing `plt.xticks(rotation=...)` for readability if column names are long.
5.  **Scatter Plots:**
    *   Uses basic `plt.scatter`, which works, but `seaborn.scatterplot` (used in ground truth) offers richer features like automatic hue mapping for regional indicators, different marker sizes, and better aesthetics, which align with the task of creating ""comprehensive visualizations.""
    *   No legends are applied, making it hard to interpret if multiple groups are plotted.
6.  **Box Plots:**
    *   The x-axis labels (`Region`) can overlap if there are many regions. The ground truth correctly rotates these for readability.
    *   The generated code uses `Region` column, but the dataset uses `Regional indicator`. This will cause a KeyError. This is a critical error.
7.  **Radar Chart:**
    *   This is the most significant deviation and a major point of failure. The ground truth implements a proper radar chart using Matplotlib and `MinMaxScaler` for normalization, which is essential for comparing different factors on the same scale. The generated code attempts to use `px.parallel_radar`, which is not a standard Plotly Express function and appears to be an incorrect usage. `px.parallel_coordinates` or `px.line_polar` might be an alternative, but `px.parallel_radar` is not correct. Even if it were a valid function, directly using raw values for different factors without normalization on a radar plot is problematic, as factors with larger inherent numerical ranges (e.g., GDP) would dominate the visualization.
    *   The `factors` list also includes `Ladder score` (which is the result, not a contributing factor) and `Country` is not a factor. This indicates a misunderstanding of what should be plotted on a radar chart for happiness factors.
    *   The ground truth's approach of normalizing values and then plotting with `matplotlib` creates a correct and interpretable radar chart. The generated code's approach for the radar chart is fundamentally incorrect and likely not to produce the intended visualization.

**Overall:** The code attempts to fulfill all requirements but often falls short in robustness, aesthetic quality, and, critically, correctness for the radar chart and column naming for box plots. Many plots are very basic Matplotlib without leveraging the enhanced styling and features of Seaborn or Plotly where appropriate, which was part of the prompt. Error handling and consistent column naming/usage are also missing.",gemma-3-27b-it,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),,,Error,Failed to generate code,gemma-3-27b-it,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: winequality-red.csv not found. Make sure the file is in the current directory."")
    exit()

# 1. Calculate and report the average 'quality'
average_quality = df['quality'].mean()
print(f""Average quality of red wines: {average_quality:.2f}"")

# 2. Group by 'quality' and calculate average 'alcohol'
qual...",9,"The generated code successfully addresses all requirements of the task. It correctly loads the dataset, calculates the mean quality, groups by quality to find the average alcohol content, and generates a bar chart visualizing this relationship. The use of `try-except` for file loading is good practice. The plot uses `seaborn` and `matplotlib` effectively, including appropriate labels and a title. The only minor improvement could be slightly more sophisticated plot embellishments like data labels or explicit `sns.set_style`, which the ground truth code includes, but these aren't critical for meeting the core requirements. The code is clean, readable, and follows Python best practices.",gemma-3-27b-it,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Parse 'date' column as datetime
df['date'] = pd.to_datetime(df['date'])

# Handle missing values in 'hosp_patients' and 'icu_patients'
df['hosp_patients'].fillna(0, inplace=True)
df['icu_patients'].fillna(0, inplace=True)

# Handle missing values in other numerical columns with the mean
for col in ['positive_rate', 'reproduction_rate']:
    df[col].fillna(df[col].mean(), inplace...",7,"The generated code is largely correct and addresses most of the requirements. It successfully loads the data, parses the date, handles missing values for `hosp_patients` and `icu_patients` by filling with 0, calculates `mortality_rate` and `vaccination_rate_per_capita`, and filters the data for 2021. The display of shape and head is also correct.

However, there are a few areas for improvement:
1.  **Handling missing values for 'positive\_rate' and 'reproduction\_rate'**: While the code fills NaNs with the mean, it does not explicitly convert these columns to numeric types first (`errors='coerce'`) as done in the ground truth. If these columns contain non-numeric data, `mean()` might fail or return incorrect results, and `fillna()` won't work as expected. The ground truth handles this more robustly.
2.  **Robustness for division by zero**: For `mortality_rate` and `vaccination_rate_per_capita`, the code correctly handles `np.nan` and `np.inf`. However, the ground truth also includes `np.inf` which explicitly covers negative infinity as well (`[np.inf, -np.inf]`). While `np.inf` would catch `inf` in general, being explicit is better. Also, more importantly, the ground truth explicitly uses `pd.to_numeric(..., errors='coerce')` for `total_deaths`, `total_cases`, `people_fully_vaccinated`, and `population` before calculating rates. This prevents errors if these columns are not already numeric, making the calculation more robust. The generated code assumes these columns are already numeric, which might not always be the case in real-world messy data.
3.  **`inplace=True` in `replace`**: While `df['mortality_rate'].replace([np.nan, np.inf], 0, inplace=True)` works, it's generally recommended to avoid `inplace=True` in chained operations or when assigning back to the same column, as it can sometimes lead to `SettingWithCopyWarning` or unexpected behavior, especially when filtering first. The ground truth first calculates and then replaces, which is slightly different but still fine.
4.  **Error Handling for File Not Found**: The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice for robust code. The generated code lacks this.

These points reduce the score from perfect, primarily due to less robust handling of potential non-numeric data and missing explicit checks.",gemma-3-27b-it,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Parse the 'date' column
df['date'] = pd.to_datetime(df['date'])

# Time series line plot of 'new_cases' for 'India'
india_data = df[df['location'] == 'India'].sort_values('date')
plt.figure(figsize=(12, 6))
plt.plot(india_data['date'], india_data['new_cases'])
plt.title('New Cases in India Over Time')
plt.xlabel('Date')
plt.ylabel('New Cases')
...",7,"The generated code correctly addresses most of the plotting requirements and uses the specified libraries (matplotlib and seaborn). It successfully loads the data, parses the date, and generates a time series plot, bar chart, scatter plot, histogram, and box plot.

However, there are several areas for improvement which prevent a higher score:

1.  **Robustness (Error Handling):** The ground truth includes `try-except` blocks for file loading, which is good practice. The generated code lacks this.
2.  **Scatter Plot Data Selection:** The task for the scatter plot explicitly asks to use the ""latest entry for each location"" for 'gdp_per_capita' and 'total_vaccinations'. While the generated code attempts `df.loc[df.groupby('location')['date'].idxmax()]`, it's not filtering for non-null values for the actual plotting columns, which can lead to a sparse or misleading plot if NaNs are present in the latest entries. The ground truth (and a more robust approach) first sorts by date and then drops duplicates, ensuring `latest_data_per_location` is truly the latest, and then explicitly drops NaNs for the relevant columns before plotting. Also, the ground truth adds `hue='continent'` and `size='population'` for better insights in the scatter plot, which is missing from the generated code and would be a good addition for data science visualizations.
3.  **Bar Chart Data Aggregation:** For the bar chart of 'total_deaths' by 'continent', summing `df.groupby('continent')['total_deaths'].sum()` directly without considering distinct locations and potentially duplicate entries for 'total_deaths' per continent can be inaccurate. The ground truth correctly identifies that the sum should be based on the *latest* `total_deaths` value *for each country*, and then summed by continent, which gives a more accurate representation of continental death tolls.
4.  **Plot Customization & Readability:**
    *   The generated code uses `plt.plot` for the time series instead of `sns.lineplot`, which is generally preferred with seaborn for consistency and features like automatic confidence intervals (though not explicitly required here).
    *   Many plots in the generated code lack various visual enhancements present in the ground truth, such as specific palettes (`viridis`, `pastel`), more detailed grid lines (`plt.grid(True, linestyle='--', alpha=0.7)`), `plt.tight_layout()` for every plot, and `plt.show()` after each `plt.figure()` for clear separation.
    *   The scatter plot would benefit from log scales on both axes given the nature of the variables, which the ground truth correctly applies.
    *   The scatter plot's legend is not handled well in the generated code, whereas the ground truth includes a detailed legend for hue and size.
5.  **Data Preprocessing for Plots:** The ground truth explicitly handles `dropna()` for columns like 'reproduction_rate' and 'stringency_index' before plotting the histogram and box plot, which is a good practice to avoid plotting issues or misleading representations from NaN values, though `sns.histplot` and `sns.boxplot` generally handle NaNs gracefully by default.

To achieve a higher score, the generated code should incorporate more robust data aggregation logic (especially for the bar and scatter plots), add error handling, and enhance plot aesthetics and informative elements as demonstrated in the ground truth.",gemma-3-27b-it,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,".

```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 1) List column names and data types
print(""Column Names and Data Types:"")
for col in df.columns:
    print(f""- {col}: {df[col].dtype}"")

# 2) Calculate descriptive statistics for numerical columns
numerical_cols = ['elixirCost', 'Win Rate', 'Usage']
print(""\nDescriptive Statistics:"")
for col in numerical_cols:
    print(f""\nColumn: {col}"")
    print(f""  - Mean: {df[col].mean()}"")
    print(f""  - Me...",8,"The generated code correctly addresses all requirements of the task: it loads the CSV, lists column names and data types, calculates descriptive statistics for the specified numerical columns, and reports the total number of rows. The code is well-structured and uses proper imports.

Areas for improvement:
- **Error Handling:** The generated code lacks error handling for `FileNotFoundError` or other exceptions during file loading, which is present in the ground truth. This makes it less robust.
- **Output Clarity for Data Types:** While it prints each column and its dtype, `df.dtypes` (as used in the ground truth) provides a more concise and Pandas-idiomatic way to display all dtypes, which is often preferred for readability, especially with many columns.
- **Efficiency for Statistics:** The descriptive statistics are calculated with a `for` loop, printing each statistic individually. The ground truth's use of `.agg(['mean', 'median', 'std', 'min', 'max'])` for multiple columns simultaneously is more efficient and provides a cleaner, table-like output, which is generally more desirable for presenting summary statistics.
- **Docstrings and Comments:** Minimal comments; however, given the simplicity of the task, this is not a significant drawback.

Overall, it's a good solution that fulfills the core requirements, but it could be more robust and efficient in its approach.",gemma-3-27b-it,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""clash_royale_cards.csv"")

# Filter for elixirCost <= 3
filtered_df = df[df['elixirCost'] <= 3]

# Select Card and Win Rate columns
selected_df = filtered_df[['Card', 'Win Rate']]

# Sort by Win Rate in descending order
sorted_df = selected_df.sort_values(by='Win Rate', ascending=False)

# Display the sorted data
print(sorted_df)
```",9,"The generated code correctly addresses all requirements: it loads the CSV, filters by 'elixirCost' <= 3, selects the 'Card' and 'Win Rate' columns, and sorts by 'Win Rate' in descending order. The code is concise and follows standard pandas operations. The only minor improvement would be to include error handling for `FileNotFoundError` as the ground truth does, which makes the code more robust. However, for a data science benchmark focusing on core logic, the current solution is excellent.",gemma-3-27b-it,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
try:
    df = pd.read_csv(""clash_royale_cards.csv"")
except FileNotFoundError:
    print(""Error: clash_royale_cards.csv not found. Make sure the file is in the current directory."")
    exit()

# Generate and display the histogram
plt.figure(figsize=(10, 6))
plt.hist(df['elixirCost'], bins=10, color='skyblue', edgecolor='black')
plt.title('Distribution of Elixir Cost')
plt.xlabel('Elixir Cost')
plt.ylabel('Frequency'...",8,"The generated code correctly addresses the core requirements of loading the CSV and generating a histogram of the `elixirCost` column with a title and labeled axes. It includes proper imports and handles a `FileNotFoundError`.

**Strong points:**
*   Correctly uses `pandas.read_csv` to load the data.
*   Correctly uses `matplotlib.pyplot.hist` to create the histogram.
*   Sets the title and labels for both axes as required.
*   Includes basic error handling for `FileNotFoundError`.

**Areas for improvement to achieve a higher score:**
*   **Column Existence Check:** The ground truth includes a check to ensure the `elixirCost` column actually exists in the DataFrame, which is a good practice for robustness. The generated code lacks this.
*   **Histogram Bins:** The ground truth uses `bins=range(1, 12)` which is more appropriate for discrete integer values like elixir costs (1-10), ensuring each integer value gets its own bar. The generated code uses `bins=10`, which defaults to 10 equally spaced bins, potentially lumping different integer elixir costs together or splitting them, making the distribution less clear for discrete data.
*   **Plot Enhancements (Minor):** While not strictly required by the prompt, the ground truth uses `seaborn.histplot` which often produces more visually appealing plots, and also sets `xticks` explicitly for better discrete axis labeling. These are good practices for visualization.
*   **`exit()` vs. `sys.exit()`:** While `exit()` works in interactive sessions, `sys.exit()` is generally preferred in scripts for cleaner termination. This is a minor point, but worth noting for robust scripting.

Overall, it's a solid solution that meets the fundamental requirements of the task. The suggested improvements would make it more robust and produce a more accurate and visually clear histogram for the specific nature of the `elixirCost` data.",gemma-3-27b-it,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,".```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: iris.csv not found. Please ensure the file is in the current directory."")
    exit()


# 1. List column names and data types
print(""Column Names and Data Types:"")
print(df.dtypes)
print(""\n"")

# 2. Calculate descriptive statistics for numerical columns
numerical_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
descriptive_stats = df[numeric...",7,"The generated code correctly loads the dataset, identifies column data types, reports unique species and their counts, and calculates descriptive statistics.

However, there are two areas for improvement:
- **Redundancy in descriptive statistics:** The `describe()` method already provides mean, median, standard deviation, min, and max. Calculating median and standard deviation separately after `describe()` is redundant and clutters the output. The request specifically asked for these 5 statistics, and `describe()` alone covers them.
- **Specific print formatting:** The ground truth code demonstrated clearer section headers and separators, which improves readability. The generated code uses simpler print statements. While not a functional error, better formatting would enhance the user experience.

The code is generally correct and addresses the core requirements, but could be more concise and better formatted in its output.",gemma-3-27b-it,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

# Load the dataset
iris_df = pd.read_csv('iris.csv')

# Filter for Iris-setosa
setosa_df = iris_df[iris_df['Species'] == 'Iris-setosa']

# Select SepalLengthCm and SepalWidthCm
selected_columns = setosa_df[['SepalLengthCm', 'SepalWidthCm']]

# Display the selected columns
print(selected_columns)
```",8,"The generated code correctly addresses all functional requirements of the task: it loads the CSV, filters for 'Iris-setosa' species, and then selects and displays the specified columns (`SepalLengthCm` and `SepalWidthCm`). The code is clean, concise, and uses proper pandas syntax.

However, it lacks error handling (e.g., `FileNotFoundError`, `KeyError`), which is present in the ground truth and is good practice for robust code. For a simple benchmark, this might be acceptable, but in a real-world scenario, error handling is crucial. The ground truth also explicitly defines `DATA_FILE`, which makes the code slightly more organized, though not strictly required for correctness. Overall, it's a very good attempt.",gemma-3-27b-it,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
iris = pd.read_csv('iris.csv')

# Create the scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(iris['SepalLengthCm'], iris['SepalWidthCm'], c=iris['Species'].astype('category').cat.codes, cmap='viridis')

# Add labels and title
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.title('Iris Dataset: Sepal Length vs. Sepal Width')

# Add a colorbar
colors = iris['Species'].unique()
handles = [plt.pl...",6,"The generated code correctly loads the dataset, creates a scatter plot of `SepalLengthCm` vs `SepalWidthCm`, and includes a title and axis labels. However, the coloring by species is not correct in the final plot. The `c=iris['Species'].astype('category').cat.codes` approach assigns numerical codes to species, which `viridis` colormap maps to a continuous spectrum, not distinct colors per category, making it hard to differentiate species. Additionally, the legend generation is overly complex and does not properly map the colors to the species names as intended by the ground truth. A simpler approach using `plt.scatter` multiple times per species (as in the ground truth) or using `seaborn` would provide correct categorical coloring and a straightforward legend. The axis labels also deviate slightly in wording from the column names, which wasn't strictly asked for but less precise than the ground truth.",gemma-3-27b-it,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv(""heart.csv"")
except FileNotFoundError:
    print(""Error: heart.csv not found in the current directory."")
    exit()

# 1. List column names and data types
print(""Column Names and Data Types:"")
for col in df.columns:
    print(f""{col}: {df[col].dtype}"")

# 2. Calculate descriptive statistics for numerical columns
numerical_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']
print(""\nDescriptive Statistics for Numerical Co...",8,"The generated code correctly addresses all parts of the task. It properly loads the CSV, lists column names and their data types, calculates the required descriptive statistics for the specified numerical columns (`Age`, `RestingBP`, `Cholesterol`, `MaxHR`), and reports the value counts for `HeartDisease`.

Strong points:
- Proper use of `pandas` for data loading and manipulation.
- Correctly identifies column names and data types.
- Accurately computes mean, median, standard deviation, min, and max for each specified numerical column.
- Correctly uses `value_counts()` for the `HeartDisease` column.
- Includes basic error handling for `FileNotFoundError`.

Areas for minor improvement:
- While it calculates descriptive statistics, the output format is verbose (looping through each column and printing each statistic individually). Using `df[numerical_cols].describe().loc[['mean', '50%', 'std', 'min', 'max']]` as in the ground truth would provide a more concise and tabular output, which is generally preferred for statistical summaries.
- The output for `HeartDisease` value counts doesn't explicitly state the interpretation (0 for no disease, 1 for disease) as clearly as the ground truth, which adds user-friendliness.
- The column and data type listing could be more succinct using `df.dtypes` directly for printing.",gemma-3-27b-it,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

# Load the dataset
heart_data = pd.read_csv(""heart.csv"")

# Filter the dataset
filtered_data = heart_data[(heart_data[""Age""] > 50) & (heart_data[""HeartDisease""] == 1)]

# Select and display the desired columns
selected_columns = filtered_data[[""Age"", ""Sex"", ""ChestPainType"", ""Cholesterol""]]
print(selected_columns)
```",9,"The generated code correctly performs all the required steps: it loads the CSV, filters the DataFrame based on the specified conditions (`Age` > 50 AND `HeartDisease` == 1), and then selects and displays the requested columns (`Age`, `Sex`, `ChestPainType`, `Cholesterol`). The imports are correct, and the code is concise and readable.

The main difference from the ground truth is the lack of error handling (e.g., `FileNotFoundError`, `KeyError`) and the absence of a function to encapsulate the logic, which makes the ground truth more robust and reusable. However, given the prompt ""Provide only the Python code without explanations,"" and the core task requirements, the generated code successfully achieves the objective in a straightforward manner. For a benchmark evaluating core task completion, it is highly effective. The prompt also didn't explicitly ask for error handling or encapsulation, so its absence doesn't significantly detract from its score in this context.",gemma-3-27b-it,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Filter wines based on alcohol and pH
filtered_df = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]

# Identify the wine with the highest quality score in the filtered subset
if not filtered_df.empty:
    best_wine = filtered_df.loc[filtered_df['quality'].idxmax()]

    # Display all features of the best wine
    print(best_wine)
else:
    print(""No wines found matching the specified criteria."")
```",9,"The generated code correctly implements all the core requirements of the task: filtering with compound conditions, identifying the highest quality wine in the subset, and displaying its features. It also handles the edge case where no wines match the criteria.

Strong points:
- Correct use of boolean indexing for filtering.
- Correct use of `idxmax()` to find the highest quality.
- Handles the `filtered_df.empty` case, which is good practice.

Areas for minor improvement:
- **Error Handling**: The ground truth includes more robust error handling (e.g., `FileNotFoundError`, `KeyError`), which is crucial for production-ready code. The generated code lacks this.
- **Modularity**: The ground truth wraps the logic in a function, making it more reusable and testable. The generated code is a script.
- **Output Clarity**: While `print(best_wine)` works, the ground truth's use of `to_string()` and additional print statements for context (`Number of wines meeting...`) makes the output more user-friendly and informative.",gemma-3-27b-it,LLM_judge_config
E30,easy,Data Visualization,,,Error,Failed to generate code,gemma-3-27b-it,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),,,Error,Failed to generate code,gemma-3-27b-it,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

try:
    df = pd.read_csv('spotify_2023.csv')
except FileNotFoundError:
    print(""Error: spotify_2023.csv not found in the current directory."")
    exit()

df['released_year'] = pd.to_datetime(df['released_date'], format='%Y-%m-%d').dt.year
df_2023 = df[df['released_year'] == 2023]

df_2023['streams'] = pd.to_numeric(df_2023['streams'], errors='coerce')

result = df_2023[['track_name', 'artist(s)_name', 'streams']]
result = result.sort_values(by='streams', ascendi...",8,"The generated code correctly addresses most of the requirements. It successfully loads the CSV, filters for `released_year` == 2023, selects the specified columns, ensures `streams` is numeric, and sorts by `streams` descending.

**Strong points:**
*   Correctly loads the CSV and handles `FileNotFoundError`.
*   Correctly filters the data for `released_year` being 2023.
*   Correctly converts the `streams` column to numeric with `errors='coerce'`.
*   Correctly selects the required columns (`track_name`, `artist(s)_name`, `streams`).
*   Correctly sorts the result by `streams` in descending order.

**Areas for improvement (why not a 10):**
*   **Deriving `released_year`**: The generated code attempts to derive `released_year` from a `released_date` column, which was not explicitly mentioned in the task description. The ground truth assumes `released_year` is directly available in the dataset as a column. While this might be a robust way to handle dates, it's an unnecessary deviation given the task's stated assumption. If `released_year` was already a column in the original dataset, this step is redundant and potentially introduces issues if `released_date` isn't present or isn't in that format.
*   **Efficiency/Clarity**: The `pd.to_datetime` and `.dt.year` step adds complexity that isn't required if `released_year` is already a column.
*   **Order of Operations**: The `streams` column is converted to numeric *after* filtering for 2023. While this works, it's generally safer and sometimes more efficient to perform type conversions on the original DataFrame before filtering, especially if the conversion could affect the filtering logic (though not in this case). The ground truth's approach of converting `streams` early is slightly more robust.",gemma-3-27b-it,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 1. Histogram of bpm
plt.figure(figsize=(10, 6))
plt.hist(df['bpm'], bins=50, color='skyblue', edgecolor='black')
plt.title('Distribution of BPM')
plt.xlabel('BPM')
plt.ylabel('Frequency')
plt.show()

# 2. Scatter plot of danceability_% vs energy_%
plt.figure(figsize=(10, 6))
plt.scatter(df['danceability_%'], df['energy_%'], alpha=0.5, color='coral')
plt.title('Relationship be...",9,"The generated code correctly addresses all parts of the task: it loads the CSV, generates a histogram for `bpm`, and a scatter plot for `danceability_%` vs `energy_%`. Both plots include titles and axis labels. The code is well-structured and uses appropriate imports.

One minor suggestion for improvement would be to use `plt.show()` only once at the end after all plots have been defined. Calling `plt.show()` after each plot creates separate plot windows, which is usually not the desired behavior when generating multiple plots from a script. While it technically shows all plots, it's less efficient and user-friendly than a single `plt.show()` call which displays all figures created up to that point. Also, adding error handling for file not found or missing columns would make the code more robust.",gemma-3-27b-it,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,".```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv(""youtube_trendings.csv"")
except FileNotFoundError:
    print(""Error: youtube_trendings.csv not found.  Make sure the file is in the current directory."")
    exit()

# 1. List column names and data types
print(""Column Names and Data Types:"")
print(df.dtypes)
print(""\n"")

# 2. Clean 'Video views', 'Likes', and 'Dislikes' columns
for col in ['Video views', 'Likes', 'Dislikes']:
    df[col] = df[col].str.replace(',', '')
  ...",9,"The generated code correctly addresses all requirements outlined in the task. It loads the dataset, lists column names and datatypes (though `df.dtypes` is less verbose than `df.info()`, it still fulfills the request), cleans and converts the specified numeric columns, calculates descriptive statistics, fills missing values in the 'Category' column, and reports unique category counts.

**Strong points:**
- Correctly uses `pd.read_csv` and includes error handling for `FileNotFoundError`.
- Effectively cleans numeric columns by removing commas, converting to numeric with `errors='coerce'`, and filling NaNs with 0 in a concise loop.
- Calculates descriptive statistics using `df.describe()` for the specified columns, which is a very efficient way to get mean, median (though not explicitly, it's part of the quartiles shown in `describe()`), std, min, and max.
- Correctly fills missing 'Category' values with 'Unknown'.
- Correctly reports the count of unique categories using `nunique()`.

**Minor observations for improvement (not critical for a score of 9):**
- For ""List all column names and their inferred data types"", `df.info()` from the ground truth is often preferred in data science for its more comprehensive output (including non-null counts and memory usage), but `df.dtypes` also fulfills the requirement.
- The task explicitly asked for mean, median, std, min, max to be reported. While `df.describe()` provides these, it also provides count, 25%, 50%, and 75% quartiles. If the goal was *only* those specific statistics, individual calls (e.g., `df[col].mean()`, `df[col].median()`) would be more direct, but `describe()` is a common and acceptable way to achieve this.",gemma-3-27b-it,LLM_judge_config
E35,easy,Data Manipulation,,,Error,Failed to generate code,gemma-3-27b-it,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Likes' column
df['Likes'] = df['Likes'].str.replace(',', '').astype(float, errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

# Clean 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '').astype(float, errors='coerce')
df['Video views'] = df['Video views'].fillna(0)

# Create scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(df['Vid...",9,"The generated code correctly addresses almost all aspects of the task.

**Strong Points:**
- **Correct Imports:** Imports `pandas` and `matplotlib.pyplot` as required.
- **Data Loading:** Correctly loads the `youtube_trendings.csv` dataset.
- **Column Cleaning (Likes & Video views):**
    - Successfully removes commas using `.str.replace(',', '')`.
    - Correctly converts to a numeric type (`float`) using `astype(float, errors='coerce')`, which handles non-convertible values by turning them into `NaN`.
    - Fills `NaN` values with 0 using `fillna(0)`.
- **Scatter Plot Generation:**
    - Creates a scatter plot with the cleaned `Video views` on the x-axis and `Likes` on the y-axis.
    - Sets the correct title 'Video Views vs. Likes'.
    - Sets appropriate x-axis label 'Video Views (cleaned)' and y-axis label 'Likes (cleaned)'.
    - Includes `plt.grid(True)` as is good practice for plots.
    - Shows the plot using `plt.show()`.

**Areas for Minor Improvement (prevented a perfect 10):**
- **Robustness in `astype`:** The ground truth uses `pd.to_numeric` which is generally more robust for type conversion as it can infer types and handle different formats, whereas `astype` can sometimes be less flexible, especially when dealing with mixed types before the `str.replace`. However, for this specific task, `astype(float, errors='coerce')` works because the `str.replace` precedes it. The ground truth's `astype(str)` before `str.replace` might be slightly more robust if the original column contains non-string types that `str.replace` might struggle with directly.
- **Error Handling for File Not Found:** The generated code lacks explicit error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. While the prompt didn't strictly require robust error handling for file operations, it's a good practice in real-world scenarios.
- **Idempotency/Redundancy:** The prompt is specific that `astype(float)` should be used, but in real pandas, `pd.to_numeric` is generally better if the column can have mixed data types, and it also allows for `errors='coerce'`. While the chosen approach works, `pd.to_numeric` is often preferred for converting to numeric types especially when dealing with potential non-numeric strings.

Overall, the generated code is highly effective and meets almost all requirements specified in the prompt. The minor points are mostly about robustness and best practices rather than functional correctness for the given task.",gemma-3-27b-it,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: 'mobiles_2025.csv' not found. Please make sure the file is in the current directory."")
    exit()

# Clean Mobile Weight
df['Mobile Weight'] = df['Mobile Weight'].str.replace('g', '').astype(float, errors='ignore')

# Clean RAM
df['RAM'] = df['RAM'].str.replace('GB', '').astype(float, errors='ignore')

# Clean Battery Capacity
df['Battery Capacity'] = df['Ba...",8,"The generated code correctly addresses most of the requirements.

**Strong Points:**
- **Correct Loading:** Handles `FileNotFoundError` for loading the CSV.
- **Accurate Cleaning:** Successfully removes specified substrings ('g', 'GB', 'mAh', 'USD ', ',') from the respective columns.
- **Correct Type Conversion:** Uses `astype(float, errors='ignore')` which is a valid way to handle errors by converting them to `NaN`, though `pd.to_numeric(..., errors='coerce')` is generally more robust for mixed types.
- **Proper Data Type Listing:** Prints `df.dtypes` as requested.
- **Correct Descriptive Statistics:** Calculates and prints mean, median, std, min, and max for the specified columns.
- **Unique Value Count:** Correctly uses `nunique()` for the 'Company Name' column.

**Areas for Improvement:**
- **Robustness in Cleaning (`astype` vs `to_numeric`):** While `astype(float, errors='ignore')` works for strings that can be converted to floats, `pd.to_numeric(..., errors='coerce')` is often preferred as it's designed to parse various numeric representations and explicitly handles non-numeric values by coercing them to `NaN`. For example, if a column contained `[['100g', '200g', 'invalid_entry']]`, `astype` might raise an error if `invalid_entry` can't be implicitly converted, whereas `to_numeric(..., errors='coerce')` would handle it gracefully by placing `NaN`. In this specific case, since 'g', 'GB', etc., are removed first, `astype` usually works fine for pure numbers, but `to_numeric` is a safer general practice.
- **Column Existence Check:** While the code assumes 'Company Name' exists, adding a check like `if 'Company Name' in df.columns:` before `df['Company Name'].nunique()` would make it more robust, similar to how the ground truth code handles it.
- **Output Formatting for Stats:** The output for descriptive statistics is functional but could be slightly more concise (e.g., using `df[columns_to_analyze].describe().loc[['mean', '50%', 'std', 'min', 'max']]` as in the ground truth) for large numbers of columns. However, the current loop explicitly prints each metric which is also clear.
- **Missing `numpy` import:** Although not used directly, `numpy` is often imported alongside `pandas` for data analysis tasks (e.g., for `np.nan` or other mathematical operations), and its absence is a minor point if more complex operations were needed. However, for this specific task, it wasn't strictly necessary.",gemma-3-27b-it,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv(""mobiles_2025.csv"")
except FileNotFoundError:
    print(""Error: mobiles_2025.csv not found. Make sure the file is in the current directory."")
    exit()

# Clean Launched Price (India)
df['Launched Price (India)'] = df['Launched Price (India)'].replace('INR ', '', regex=True)
df['Launched Price (India)'] = df['Launched Price (India)'].str.replace(',', '', regex=True)
df['Launched Price (India)'] = pd.to_numeric(df['Launc...",9,"The generated code correctly performs all the required steps: loading the CSV, cleaning the 'Launched Price (India)' and 'RAM' columns (including handling errors with `coerce`), filtering the data for 'Apple' and price under 100,000, selecting the specified columns, and sorting by price. The error handling for file not found is also good.

One minor improvement, though not strictly required by the prompt, would be to convert 'Launched Price (India)' and 'RAM' columns to string before applying `.replace()` or `.str.replace()` if there's a possibility of them containing non-string types initially. The ground truth code includes this for robustness. Additionally, `regex=True` is not necessary for simple string replacements like 'INR ' or 'GB' and can sometimes have performance implications or unexpected behavior if the string contains special regex characters; `regex=False` (or omitting it, as `regex=False` is the default for `.str.replace()`) is safer and clearer for these cases. However, for `.replace()` on a Series, `regex=True` is the default when the value is a string, so making it explicit `regex=False` is better for `df['Launched Price (India)'].replace('INR ', '', regex=False)` if it were changed to `.str.replace()`. The current use of `replace()` directly on the Series followed by `str.replace()` might be slightly less efficient than doing all `str.replace()` calls on a series that is guaranteed to be type string. Nevertheless, the code is highly functional and correct for the given task.",gemma-3-27b-it,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the RAM column
df['RAM'] = df['RAM'].str.replace('GB', '').astype(float, errors='ignore')

# Clean the Battery Capacity column
df['Battery Capacity'] = df['Battery Capacity'].str.replace('mAh', '').astype(float, errors='ignore')

# Create the scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(df['RAM'], df['Battery Capacity'], c=df['Company Name'].astype('category').c...",6,"The generated code correctly loads the dataset and attempts to clean the `RAM` and `Battery Capacity` columns. It also creates a scatter plot with a title and axis labels.

However, there are several issues:
1.  **Cleaning columns - Error Handling:** The `astype(float, errors='ignore')` method for cleaning columns does not set problematic values to NaN. Instead, it leaves them as their original type if conversion fails, which can lead to `TypeError` or `ValueError` later if the column is mixed and operations expecting floats are performed. The ground truth uses `pd.to_numeric(..., errors='coerce')` which is the correct and robust way to handle this, converting non-numeric entries to NaN. This is a critical requirement from the prompt.
2.  **Coloring by Company Name:** The prompt explicitly asks to color `by the Company Name column`. The generated code uses `c=df['Company Name'].astype('category').cat.codes`. While this technically assigns a unique color to each company, it uses a single continuous color spectrum (`cmap='viridis'`) which implies a numerical relationship between companies, which is incorrect for categorical data. It also adds a `colorbar` which is misleading for discrete categories. The ground truth, using `hue='Company Name'` with `seaborn`, correctly handles categorical coloring, assigning distinct colors to each category and automatically generating a legend.
3.  **Missing Imports and Robustness:** The `seaborn` library (used in the ground truth) is ideal for this type of plotting with categorical coloring but wasn't imported or used. For robustness, the ground truth also includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice.

To improve, the code should:
*   Use `pd.to_numeric(..., errors='coerce')` for robust error handling during column cleaning.
*   Use `seaborn.scatterplot` with `hue` for proper categorical coloring, which also automatically handles the legend. If sticking to `matplotlib.pyplot.scatter`, it's much more involved to get a distinct color per category with a proper legend.",gemma-3-27b-it,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,".```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv(""titanic.csv"")
except FileNotFoundError:
    print(""Error: titanic.csv not found. Please make sure the file is in the current directory."")
    exit()

# Display the first 5 rows
print(""First 5 rows of the dataset:\n"", df.head())

# Show the shape of the dataset
print(""\nShape of the dataset:"", df.shape)

# List column names and their data types
print(""\nColumn names and data types:\n"", df.dtypes)

# Calculate the surviv...",9,"The generated code successfully performs all requested tasks. It loads the dataset, displays the first 5 rows, shows the shape, lists column names and data types, calculates the survival rate, and counts missing values. The code is clean, concise, and uses appropriate pandas functions.

Areas for minor improvement:
1.  **Output Formatting**: While the output is correct, the ground truth's use of `df.info()` provides more comprehensive information (including non-null counts and memory usage) for data types, which is generally preferred in EDA over just `df.dtypes`.
2.  **Print Statements**: The ground truth separates the descriptive text from the data output more clearly, making it slightly more readable for each section. For example, `print(""1. First 5 rows of the dataset:"")` followed by `print(df.head())` is slightly better than `print(""First 5 rows of the dataset:\n"", df.head())` as it keeps the introductory text on a separate line.
3.  **Survival Rate Formatting**: Ground truth formatted the survival rate to two decimal places, which is a nice touch for presentation.

Despite these minor points, the code fully addresses the requirements and is highly functional.",gemma-3-27b-it,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 1) List column names and data types
print(""Column Names and Data Types:"")
print(df.dtypes)

# 2) Calculate and report descriptive statistics
numerical_cols = ['Year', 'Rating', 'Runtime']
descriptive_stats = df[numerical_cols].describe()
print(""\nDescriptive Statistics:"")
print(descriptive_stats)

# 3) Report the total number of movies
total_movies = len(df)
print(f""\nTotal Number of Movies: {total_movies}""...",5,"The generated code addresses most parts of the task but falls short on robustness, completeness, and specific requirements for descriptive statistics.

**Correctness & Completeness:**
*   **Loading CSV:** Correctly loads the CSV.
*   **Column Names & Data Types:** Correctly prints all column names and their data types using `df.dtypes`. This fulfills requirement (1).
*   **Total Movies:** Correctly reports the total number of movies using `len(df)`. This fulfills requirement (3).
*   **Descriptive Statistics:**
    *   **Partial Fulfillment:** It uses `df[numerical_cols].describe()`, which calculates basic descriptive statistics. However, the task specifically asked for ""mean, median, standard deviation, min, max"". While `describe()` includes these, the output also includes count, and 25th/75th percentiles. The ground truth correctly filters `describe()` to show *only* the requested statistics (`mean`, `50%` renamed to `median`, `std`, `min`, `max`). The generated code just prints the full `describe()` output, which is not as precise as requested.
    *   **Runtime Preprocessing (Critical Missing Piece):** The most significant issue is the absolute lack of handling for the `Runtime` column. Often, 'Runtime' in such datasets is a string (e.g., '142 min') and needs to be converted to numeric before statistical calculations can be performed. The generated code assumes 'Runtime' will always be numeric, which is a common point of failure for this specific dataset. If 'Runtime' is an object (string) type, `describe()` would either omit it or raise an error for non-numeric operations, leading to incomplete or incorrect statistics for 'Runtime'. This is a major omission compared to the ground truth's robust preprocessing.
    *   **Error Handling:** There is no error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes.
    *   **Clarity of Output:** The ground truth has more descriptive print statements, explicitly stating what each section of output represents.

**Code Quality:**
*   **Imports:** Proper import for `pandas`.
*   **Structure:** Simple and straightforward, but lacks the robustness of the ground truth (e.g., configuration, error handling, preprocessing steps).
*   **Readability:** Generally readable, but the lack of inline comments for major steps makes it less self-documenting than the ground truth.

To score higher, the generated code would need to:
1.  Explicitly preprocess the 'Runtime' column to ensure it is numeric.
2.  Filter the output of `describe()` to *only* include the requested statistics (mean, median, std, min, max).
3.  Add basic error handling for `FileNotFoundError`.",gemma-3-27b-it,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,".```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Filter movies with Rating >= 9.0
filtered_df = df[df['Rating'] >= 9.0]

# Select Title and Rating columns
selected_df = filtered_df[['Title', 'Rating']]

# Sort by Rating in descending order
sorted_df = selected_df.sort_values(by='Rating', ascending=False)

# Display the sorted data
print(sorted_df)
```",9,"The generated code correctly addresses all the core requirements of the task: loading the CSV, filtering by rating, selecting specific columns, and sorting the results. It is concise and uses Pandas effectively.

The code is very close to perfect for the given task. The only minor points for improvement, which are present in the ground truth but missing here, are:
1.  **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and `KeyError`, which is good practice for production-ready code or more robust scripts. The generated code assumes the file exists and has the correct columns.
2.  **Comments/Explanations:** The ground truth has more extensive comments explaining each step, which can be useful for readability and understanding, especially in a learning or documentation context.

However, for a direct ""write Python code"" task, the generated code is highly effective and directly achieves the desired outcome without unnecessary verbosity. The lack of extensive error handling is a pragmatic choice given the typical constraints of a benchmark task where minimal, functional code is often preferred over production-grade robustness.",gemma-3-27b-it,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv(""imdb_top_250.csv"")
except FileNotFoundError:
    print(""Error: imdb_top_250.csv not found. Make sure the file is in the current directory."")
    exit()

# Create histogram of Rating
plt.figure(figsize=(10, 6))
sns.histplot(df['Rating'], kde=True)
plt.title('Distribution of IMDB Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# Create scatter plot o...",7,"The generated code correctly loads the CSV, creates a histogram for 'Rating', and generates a scatter plot for 'Rating' vs 'Runtime', with appropriate titles and labels for both. It successfully uses pandas, matplotlib, and seaborn as required.

However, a significant area for improvement lies in the handling of the 'Runtime' column. The `imdb_top_250.csv` dataset typically has 'Runtime' as a string (e.g., ""120 min""). The generated code attempts to directly plot this string column, which will likely lead to an error or an uninformative plot depending on the specific seaborn/matplotlib version and data structure inference. The ground truth code correctly preprocesses 'Runtime' by extracting the numeric values and converting them to integers, which is crucial for a meaningful scatter plot.

Without this preprocessing, the scatter plot will either fail or produce an incorrect visualization if the 'Runtime' column is not purely numeric as expected by `sns.scatterplot` for the x-axis. This is a critical oversight for a data science task where data types are often inconsistent and require cleaning.

The code otherwise demonstrates good practices like `try-except` for file loading and clearly labeled plots. The lack of robust data preprocessing for `Runtime` is the main reason for not achieving a higher score.",gemma-3-27b-it,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# 1. Report the number of missing values for each column
print(""Missing values per column:"")
print(df.isnull().sum())

# 2. Drop rows with missing values in specific columns and calculate descriptive statistics
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_cleaned = df.dropna(subset=numerical_cols)

print(""\nDescriptive statistics for numerical columns after dropp...",7,"The generated code correctly addresses most of the requirements.

**Strong points:**
*   Correctly loads the CSV.
*   Correctly reports NA counts for each column (`df.isnull().sum()`).
*   Correctly filters rows based on NAs in the four specified numerical columns (`df.dropna(subset=numerical_cols)`).
*   Correctly calculates descriptive statistics on the filtered data using `.describe()`.
*   Correctly counts and reports unique values for 'species' and 'island'.
*   Includes proper imports.

**Areas for improvement to achieve a higher score:**
*   **Descriptive Statistics Detail:** While `describe()` provides central tendency (mean, median), dispersion (std), and ranges (min, max), the prompt specifically requested *mean, median, standard deviation, min, max*. The `describe()` output also includes count, 25th, 50th (median), and 75th percentiles. The ground truth used `.agg(['mean', 'median', 'std', 'min', 'max'])` which is more precise to the requested output.
*   **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and other general exceptions when loading the dataset, which is a good practice for robust code. The generated code assumes the file will always be present.
*   **Output Clarity and Formatting:** The ground truth provides more distinct and labeled print statements, making the output easier to read and understand, clearly separating each task's output. For example, explicitly stating ""--- Missing Values Report ---"" or ""--- Descriptive Statistics for Filtered Numerical Columns ---"". Also, reporting the number of rows before and after filtering is helpful context missing in the generated code.
*   **Missing Values Reporting (Specificity):** The prompt asked to ""Report the number of missing values (NA) for each column."" While `df.isnull().sum()` does this, the ground truth specifically filters to show *only* columns with missing values and reports ""No missing values found in any column."" if none exist, which is a slightly more refined output.",gemma-3-27b-it,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the dataset
filtered_df = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')]

# Select the desired columns
selected_df = filtered_df[['culmen_length_mm', 'culmen_depth_mm']]

# Drop rows with missing values
cleaned_df = selected_df.dropna()

# Display the first 5 rows
print(cleaned_df.head())
```",9,"The generated code correctly implements all specified requirements: loading the CSV, filtering an Adelie species on Torgersen island, selecting the specified culmen columns, dropping NA values, and displaying the first 5 rows. The code is well-structured and uses appropriate pandas functions. The primary difference from the ground truth is the lack of robust error handling for `FileNotFoundError` during CSV loading and the slightly less specific `dropna()` call. While `dropna()` without `subset` argument works here because only the two culmen columns remain, specifying `subset` as in the ground truth is more robust and explicit, especially if there were other columns in `selected_df` that might contain NAs and should not cause a row to be dropped. The ground truth also explicitly defines `file_path` and adds a print statement before `cleaned_df.head()`, which are minor cosmetic differences. The task requested ""Provide only the Python code without explanations"", and the ground truth contained comments/explanations, which the generated code also included, but these are generally acceptable for clarity.",gemma-3-27b-it,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Handle missing values
df_clean = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])

# Create the scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(df_clean['culmen_length_mm'], df_clean['culmen_depth_mm'], c=df_clean['species'])

# Add title and labels
plt.title('Culmen Length vs. Culmen Depth by Species')
plt.xlabel('Culmen Length (mm)')
plt.ylabel('Culmen ...",6,"The generated code correctly loads the dataset, filters missing values for the specified columns, and creates a scatter plot with appropriate labels and title. This fulfills most of the requirements.

However, there are two significant issues that prevent a higher score:
1.  **Incorrect Coloring by Species:** The `plt.scatter` function's `c` argument expects numerical values for coloring. When `c=df_clean['species']` is used, Matplotlib attempts to map string species names to numerical values, which can lead to unexpected behavior (e.g., all points being the same color or an incorrect color scheme if not explicitly handled via a colormap or numerical encoding). The task specifically requested to color points *according to* the species, which implies distinct colors for distinct species categories. Seaborn's `hue` parameter handles this automatically and correctly.
2.  **Use of Matplotlib for Categorical Coloring:** While Matplotlib can be used, for categorical coloring like `species`, Seaborn's `scatterplot` is generally preferred because it handles the mapping of categories to distinct colors and automatically generates a legend. Using `plt.scatter` for this requires manual handling of color mapping and legend creation, which was not done effectively here. The `plt.colorbar` approach is not suitable for categorical data like species names.

To improve, the code should:
*   Switch to `seaborn.scatterplot` for correct categorical coloring.
*   If sticking with `matplotlib.pyplot.scatter`, it would need to numerically encode the `species` column and then manually create a legend mapping colors back to species names, which is more complex than intended for this task.",gemma-3-27b-it,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv(""penguins.csv"")
except FileNotFoundError:
    print(""Error: penguins.csv not found.  Make sure the file is in the current directory."")
    exit()

# Fill NA values in culmen_length_mm and culmen_depth_mm with the mean
df['culmen_length_mm'].fillna(df['culmen_length_mm'].mean(), inplace=True)
df['culmen_depth_mm'].fillna(df['culmen_depth_mm'].mean(), inplace=True)

# Calculate and print statistics for cleaned columns
prin...",8,"The generated code correctly addresses all requirements of the task. It successfully loads the dataset, fills NA values in the specified numerical columns with their respective means, calculates and prints the mean, median, and standard deviation for these cleaned columns, and finally prints the total number of rows and columns. The code is clean and uses pandas effectively.

Strong points:
- Correctly loads the CSV and handles `FileNotFoundError`.
- Correctly fills NAs using `fillna` and `mean()`.
- Calculates and prints all required statistics (mean, median, std dev).
- Correctly prints dataset dimensions.
- Uses f-strings for clear output.

Areas for minor improvement (to achieve a higher score):
- The Ground Truth code demonstrates a more robust approach by iterating through a list of `NUMERICAL_COLUMNS` for both NA imputation and statistics calculation. This makes the code more scalable and less prone to errors if more columns needed similar processing. The generated code hardcodes the column names repeatedly, which is less elegant for larger tasks.
- The Ground Truth also includes additional print statements for clarity during the NA imputation process (e.g., initial NA count, mean used, NA count after imputation), which is good for debugging and understanding the data transformation.
- While using `inplace=True` is functional, it's generally considered better practice in pandas to assign the result back to the DataFrame (e.g., `df['col'] = df['col'].fillna(...)`) to avoid unexpected side effects, although for simple `fillna` it's often acceptable.",gemma-3-27b-it,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv(""penguins.csv"")

# Drop rows with NA in flipper_length_mm
df = df.dropna(subset=['flipper_length_mm'])

# Create a histogram of flipper_length_mm
plt.figure(figsize=(10, 6))
sns.histplot(df['flipper_length_mm'], kde=False)
plt.title('Distribution of Flipper Length (mm)')
plt.xlabel('Flipper Length (mm)')
plt.ylabel('Frequency')
plt.show()

# Create a bar chart of penguin count...",8,"The generated code correctly addresses all parts of the task. It loads the dataset, handles NA values for `flipper_length_mm`, creates a histogram with the specified title and x-axis label, and generates a bar chart for island counts with appropriate labels.

Strengths:
- Correctly uses `pandas` for data loading and NA handling.
- Correctly uses `seaborn` for plotting, which is generally preferred for data visualization.
- Labels for titles and axes are correct as per the prompt.

Areas for improvement (minor):
- The y-axis label for the histogram is 'Frequency' in the generated code, while the ground truth and often contextually 'Count' is more appropriate for `histplot` without normalization. While 'Frequency' isn't strictly wrong, 'Count' is typically what `histplot` displays by default.
- It doesn't include error handling for `FileNotFoundError` when loading the CSV, which is good practice for robust code.
- The bar chart for island counts uses `sns.barplot` on pre-calculated `value_counts`. While functional, `sns.countplot` is often a more direct and concise way to visualize counts of categorical data, as shown in the ground truth.
- Does not explicitly set a seaborn style (`sns.set_theme`), which is a nice touch for consistent and professional-looking plots, though not strictly required by the prompt.",gemma-3-27b-it,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: penguins.csv not found. Make sure the file is in the current directory."")
    exit()

# Filter data for Torgersen island
torgersen_penguins = df[df['island'] == 'Torgersen']

# Count penguins per species on Torgersen island
species_counts = torgersen_penguins['species'].value_counts()

# Print the counts
print(species_counts)
```",8,"The generated code correctly implements all the core requirements of the task: it loads the dataset, filters for 'Torgersen' island, and counts the species occurrences within that subset. The code is concise and directly addresses the problem.

However, it lacks some robustness and completeness compared to the ground truth:
- **Error Handling:** While it handles `FileNotFoundError`, it doesn't handle other potential `Exception` types during file loading or check for the existence of required columns (`island`, `species`). This makes it less robust to malformed CSVs or unexpected data structures.
- **Informative Output:** The output is just the `species_counts` series. The ground truth provides more user-friendly messages for successful loading, filtering, and clear headers for the final output.
- **Edge Cases:** It doesn't explicitly check if `torgersen_penguins` is empty after filtering, which could lead to an empty `species_counts` series without an explicit message to the user that no penguins were found on that island.

Overall, it's a very good attempt that solves the core problem efficiently, but could be improved with more comprehensive error handling, informative print statements, and handling of edge cases for a production-ready script.",gemma-3-27b-it,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

print(df.shape)
print(df.dtypes)
```",9,"The generated code correctly defines the DataFrame, prints its shape, and lists the data types of all columns, fulfilling all requirements of the task. The code is concise and directly addresses the problem.

The only minor area for improvement, especially for a real-world scenario or a more formal output, would be to add descriptive print statements for the `shape` and `dtypes` outputs. For example, `print(""Shape of the DataFrame:"")` before `print(df.shape)` would make the output clearer to someone reading it. However, given the prompt only asked for ""Python code"", the current implementation is technically correct and does not explicitly require such print statements. This is a stylistic choice rather than a functional error.",gemma-3-27b-it,LLM_judge_config
E5,easy,Data Visualization,,,Error,Failed to generate code,gemma-3-27b-it,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]}

df = pd.DataFrame(data)

# Calculate and print the average 'Midterm_Score'
average_midte...",10,"The generated code is correct and effectively addresses all requirements of the task. It properly creates the DataFrame, accurately calculates and prints both the average midterm and final scores, and correctly identifies the subject with the highest average final score. The code quality is good, with necessary imports and a clear structure. All specified outputs are generated as expected.",gemma-3-27b-it,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
        'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
        'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
        'Volume': [1000000, 120...",9,"The generated code correctly creates the DataFrame, sets the 'Date' as the index, calculates the 'Daily_Range', finds the date with the highest 'Close' price, and prints the updated DataFrame as requested. The code is well-structured and uses appropriate pandas functions.

One minor improvement, the `strftime('%Y-%m-%d')` used in the ground truth for printing the date with the highest close price ensures a consistent and readable date format for output, especially if the index is a Timestamp object. The generated code directly prints the `highest_close_date` which will print the full Timestamp object, including the time component (e.g., `2024-01-04 00:00:00`), which is less clean than just the date. This is a very minor stylistic point and does not affect correctness.",gemma-3-27b-it,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {'OrderID': [101, 102, 103, 104, 105, 106, 107], 
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 
        'Quantity': [1, 2, 1, 1, 3, 1, 1], 
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150], 
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', '...",10,"The generated code is excellent. It correctly imports pandas, creates the DataFrame, calculates the 'Total_Revenue' column, identifies the best-selling product by quantity, and prints the first 5 rows of the DataFrame, including the newly calculated column. The code is concise and follows good practices. All requirements of the task have been met accurately and efficiently.",gemma-3-27b-it,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate and print the average ""SessionDurationMinutes""
average_session_duration = df[""SessionDurationMinutes""].mean()
print...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It successfully defines the DataFrame, calculates the average session duration, and counts users by device type using appropriate pandas methods.

Strong points:
- **Correctness:** All calculations and DataFrame creation are accurate.
- **Completeness:** Both parts of the task are fully addressed.
- **Code Quality:** The code is clean, readable, and uses standard pandas operations efficiently. The variable names are descriptive.

Areas for minor improvement (not critical for functionality but can enhance output clarity or robustness):
- The `print` statements in the ground truth are slightly more verbose with numbering and separators, which can be beneficial for larger scripts or benchmarks to define distinct outputs. The generated code's print output is good, but could be matched to the ground truth's verbosity for perfect alignment.
- The formatting of the `average_session_duration` print statement could include formatting (e.g., `:.2f`) as seen in the ground truth for consistent output precision.

Overall, excellent job.",gemma-3-27b-it,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

max_temperature = df[""TemperatureCelsius""].max()
print(""Maximum TemperatureCels...",9,"The generated code correctly defines the DataFrame and accurately finds and prints the maximum temperature and minimum humidity, fulfilling all task requirements. The code is clean, concise, and uses proper pandas methods.

The only minor area where the ground truth is slightly better is in the print statements. The ground truth uses f-strings with units (C and %) which makes the output more informative and user-friendly, although the generated code's output is still perfectly understandable and correct. This is a very minor aesthetic improvement rather than a functional one.",gemma-3-27b-it,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# 1) Print the total number of ""Likes"" across all posts.
total_likes = df[""Likes""].sum()
print(""Total Likes:"", total_likes)

# 2)...",10,"The generated code is correct, complete, and well-structured. It successfully defines the DataFrame, calculates the total number of likes, and identifies the PostType with the highest average shares, fulfilling all specified requirements. The code is clean and directly addresses the problem without unnecessary verbosity, making it very efficient and easy to understand. The use of f-strings for printing in the Ground Truth is a minor stylistic difference, but the generated code's print statements are perfectly acceptable and clear.",gemma-3-27b-it,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",9,"The generated code correctly defines the DataFrame, calculates the average age, and counts patients by gender, fulfilling all requirements of the task. The imports are correct, and the code structure is clean and readable. The only minor difference from the ground truth is the exact wording and formatting of the print statements, particularly for the average age (e.g., no f-string formatting to two decimal places and slightly different leading text), but this does not affect the correctness of the calculations. The core logic and output are identical.",gemma-3-27b-it,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}
df = pd.DataFrame(data)

highest_rated_movie = df.loc[df[""Rating""].idxmax()][""Title""]
print(highest_rated_...",10,"The generated code is correct, concise, and effectively addresses all aspects of the task. It correctly defines the DataFrame, prints the title of the movie with the highest rating using `idxmax()`, which is an efficient way to find the index of the maximum value, and accurately calculates and prints the average year. The code is clean, well-structured, and includes the necessary import. The use of `idxmax()` is particularly elegant for finding the highest-rated movie's title.",gemma-3-27b-it,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

print(f""Total number of cars: {len(df)}"")
print(f""Average PriceUSD: {...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates and prints the total number of cars using `len(df)` (which is a perfectly valid and common way to get the number of rows), and correctly calculates and prints the average ""PriceUSD"". The code is concise and directly addresses all requirements of the task. Both the `len(df)` method and `df.shape[0]` are equally valid for counting rows, so `len(df)` is not a detractor. The output format for the average price differs slightly from the ground truth (no currency symbol or 2 decimal places), but the prompt asked to ""Print the average ""PriceUSD"" of all cars"", which it does accurately. Therefore, it fully meets the task's requirements.",gemma-3-27b-it,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}
df = pd.DataFrame(data)

highest_hp_pokemon = df.loc[df[""HP""].idxmax()][""Name""]
p...",9,"The generated code correctly creates the DataFrame and identifies the Pokemon with the highest HP and lowest Defense. The use of `idxmax()` and `idxmin()` followed by `.loc[]` to extract the 'Name' is efficient and correct. The code is concise and achieves all requirements of the task.

Minor areas for improvement (which do not affect correctness but could enhance readability/robustness):
1. The print statements could be more descriptive (e.g., ""The Pokemon with the highest HP is: Charizard"" instead of just ""Charizard""). This was explicitly mentioned in the ground truth's output format, though the prompt asked ""Print the name of the Pokemon with the highest 'HP'"" not ""Print the descriptive sentence..."". So arguably, the generated code satisfies the prompt.
2. Variable names like `highest_hp_pokemon` and `lowest_defense_pokemon` are good, but adding full descriptive print messages would be beneficial for the user of the output.",gemma-3-27b-it,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('titanic.csv')

# Identify columns with missing values
print(""Columns with missing values before cleaning:"")
print(df.isnull().sum())

# Fill missing Age values with the median
df['Age'].fillna(df['Age'].median(), inplace=True)

# Fill missing Embarked values with the most frequent port
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# Drop the Cabin column
df.drop('Cabin', axis=1, inplace=True)

# Extract Title fr...",8,"The generated code correctly performs most of the required data cleaning steps. It loads the data, correctly fills missing Age and Embarked values, and drops the Cabin column. It also correctly displays the shape and first 5 rows. The main area for improvement is the title extraction. While it works for many cases, the regex-based extraction in the ground truth is more robust as it handles potential variations better and the ground truth also includes standardization of titles. Additionally, the generated code identifies but doesn't specifically print only the columns with missing values as the ground truth does, which was a minor deviation from clarity on that specific sub-task. The ground truth also included error handling for `FileNotFoundError`.",gemma-3-27b-it,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}
df = pd.DataFrame(data)

# Print the ""Titl...",9,"The generated code correctly defines the DataFrame, identifies the title of the book with the highest price, and calculates the total stock. It is clean, concise, and directly addresses all parts of the task.

Strong points:
- **Correctness:** The code accurately performs all requested operations.
- **Conciseness:** It achieves the desired results with minimal, yet readable, code.
- **Proper pandas usage:** It correctly uses `idxmax()` and `sum()` for efficient DataFrame operations.

Areas for minor improvement (to reach a perfect 10):
- **Output clarity:** The `print` statements could be more descriptive, similar to the ground truth. For instance, instead of just `print(highest_price_book)`, `print(f""The title of the book with the highest price is: '{highest_price_book}'"")` provides better context to the user. This is a minor stylistic point, but enhances user experience.
- **Comments:** While the code is straightforward, adding a few inline comments explaining the purpose of each step (e.g., finding the highest price book, calculating total stock) would improve readability and maintainability, especially for more complex tasks.",gemma-3-27b-it,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Display the first 10 rows
print(""First 10 rows:\n"", df.head(10))

# Show the shape of the dataset
print(""\nShape of the dataset:"", df.shape)

# List column names and their data types
print(""\nColumn names and data types:\n"", df.dtypes)

# Calculate the percentage of potable vs non-potable water samples
potable_percentage = (df['Potability'].sum() / len(df)) * 100
non_potable_percentage = (len(df) - df['...",7,"The generated code correctly performs most of the required tasks.

**Strengths:**
*   Correctly loads the dataset.
*   Displays the first 10 rows.
*   Shows the dataset shape.
*   Lists column names and data types accurately.
*   Displays missing values per column.
*   Provides summary statistics for numerical columns using `df.describe()`.

**Areas for Improvement:**
*   **Potability Percentage Calculation:** The calculation for potability percentage assumes that 'Potability' is a binary column (0 or 1) and that `sum()` accurately represents the count of potable samples. While this is often true for this dataset, a more robust approach is `value_counts()` as shown in the ground truth, which will work universally for any categorization in the 'Potability' column (e.g., if it contained 'Yes'/'No' strings) and provides a more explicit breakdown.
*   **Output Clarity and Formatting:** The ground truth code includes more descriptive print statements and separators (`=""*50""`) which enhance readability and organization of the output, making it easier to parse the different sections of the EDA. The generated code's output is functional but less visually structured.
*   **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code lacks this.
*   **Missing Value Display:** The ground truth specifically filters to show only columns with missing values greater than 0, making the output cleaner when there are many columns but only a few have missing data. It also explicitly states ""No missing values found"" if that's the case.

To achieve a higher score, incorporating more robust potability percentage calculation, improving output clarity, and adding basic error handling would be beneficial.",gemma-3-27b-it,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv(""water_potability.csv"")
except FileNotFoundError:
    print(""Error: water_potability.csv not found."")
    exit()

# 1. Identify columns with missing values
missing_values = df.isnull().sum()
print(""Missing values before cleaning:\n"", missing_values[missing_values > 0])

# 2. Fill missing values
for col in df.columns:
    if df[col].isnull().any():
        if pd.api.types.is_numeric_dtype(df[col]):
            df[col].fil...",6,"The generated code addresses most of the requirements but has significant shortcomings in certain areas compared to the ground truth.

**Strengths:**
- **Loading and Error Handling:** Properly loads the dataset and includes basic error handling for `FileNotFoundError`.
- **Missing Value Identification:** Correctly identifies and prints missing values before cleaning.
- **Duplicate Removal:** Correctly identifies and removes duplicate rows.
- **First 5 Rows:** Displays the first 5 rows of the cleaned dataset as requested.

**Areas for Improvement/Weaknesses:**
- **Missing Value Strategy:** The prompt specifically asks for filling missing numerical values with the *mean*. The generated code uses `mode()` for non-numeric types, which wasn't requested and isn't relevant for this primarily numerical dataset, although it wouldn't cause an error here, it's an unnecessary deviation. More importantly, it doesn't print any confirmation messages about *what* was filled and how many values.
- **Data Range Verification (Crucial Difference):**
    - The ground truth code *removes* rows that have values outside of ""reasonable ranges"" (e.g., pH 0-14, other parameters >= 0), which is a key part of data *cleaning*.
    - The generated code *only prints a warning* if values are out of range for *some* columns. It does not perform any actual cleaning (removal or modification) based on these ranges. This is a critical omission regarding data cleaning and validation.
    - The defined `range_checks` dictionary in the generated code includes 'Solid' instead of 'Solids', which is a data type mismatch for the columns in the actual dataset. It also includes 'Total_Trihalomethanes' which might not be a column in the dataset (it's 'Trihalomethanes').
    - The ground truth correctly identifies and handles `pH` between 0-14 and all other relevant parameters as non-negative, which aligns better with actual data validation.
- **Before/After Statistics:**
    - The generated code prints the dataset shape *before* cleaning, but then prints the *same* shape ""after cleaning"" without any intervening cleaning steps (like range removal). This demonstrates a misunderstanding of how to properly present ""before and after"" stats if cleaning steps affecting shape are not applied.
    - It shows missing values *after* the imputation step but doesn't explicitly state ""no missing values remaining"" if that's the case. It also doesn't show the final shape and missing values *after* duplicate removal and range-based row removal, which would be the ""true after"" state.

The generated code only performs imputation and duplicate removal, but critically misses the range-based row removal which is fundamental to ""verify that all values are within reasonable ranges"" as a cleaning step. The summary statistics are also not well-structured to reflect the true ""before"" and ""after"" state across *all* cleaning steps.",gemma-3-27b-it,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('water_potability.csv')

# 1. Bar chart of potable vs non-potable water
plt.figure(figsize=(8, 6))
df['Potability'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])
plt.title('Distribution of Potable vs. Non-Potable Water Samples')
plt.xlabel('Potability')
plt.ylabel('Count')
plt.xticks([0, 1], ['Non-Potable', 'Potable'], rotation=0)
plt.show()

# 2. Histograms for...",7,"The generated code correctly implements the requested visualizations and follows a logical structure. All four required plots are generated.

Areas for improvement to achieve a higher score:

1.  **Handling Missing Values**: The ground truth explicitly addresses missing values for plotting (e.g., `dropna()` for histograms or notes that seaborn / pandas handles them). The generated code generally relies on default seaborn/pandas behavior, which is often okay for non-critical NaNs, but explicitly calling `dropna()` for histograms (as done in the ground truth) is a safer and clearer approach. For the boxplot and heatmap, `sns.boxplot` and `df.corr()` handle NaNs by default, which is acceptable.
2.  **Plot Aesthetics and Readability**:
    *   **Bar Chart**: Using `df['Potability'].value_counts().plot(kind='bar')` for the bar chart is a functional way, but `sns.countplot` (as in the ground truth) is typically preferred for categorical distributions in seaborn as it's more flexible and often produces better-looking plots by default with less manual intervention for colors, labels, etc. The ground truth also explicitly sets `xticks` with descriptive labels which is clearer than just '0' and '1'.
    *   **Histograms**: While the histograms are correct, the ground truth adds `grid(axis='y', linestyle='--', alpha=0.7)` which enhances readability. Also, the ground truth uses different colors for each histogram, which can improve visual distinctiveness.
    *   **General**: The ground truth adds `tight_layout()` and `grid` lines to many plots and also sets `sns.set_style(""whitegrid"")` for a consistent look, which is a good practice for better presentation. Setting font sizes for titles and labels also improves readability.
3.  **Error Handling**: The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust. The generated code lacks this. This is a common and important practice for scripts that interact with files.",gemma-3-27b-it,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Feature Engineering
df['total_acid'] = df['fixed acidity'] + df['volatile acidity']
df['acidity_ratio'] = df['fixed acidity'] / df['volatile acidity']
df['alcohol_sug...",6,"The generated code addresses most of the requirements but has several significant issues and omissions that prevent a higher score.

**Correctness:**
- **Feature Engineering:** Creates 3 new features, but `alcohol_sugar_ratio` is problematic due to potential division by zero if 'sugar' (residual sugar) is zero in the dataset, and this isn't handled. Also, `total_acid` is simply a sum, which might not be as ""meaningful"" as ratios or interactions in the context of wine quality, though it isn't explicitly wrong. The ground truth's choices (acidity_ratio, alcohol-sulphates interaction, free-to-total SO2 ratio) are generally more impactful.
- **RFE Implementation:** The `rfe.fit(X_train, y_train)` uses `X_train` which is already scaled. However, the line `rfe_feature_importances = model_rfe.feature_importances_` is problematic. `model_rfe` was initialized but not explicitly fit *on the selected features*. RFE itself wraps an estimator, and to get importances from the *estimator that RFE used on the final subset of features*, you need to access `rfe.estimator_` and then its `feature_importances_`. Simply taking `model_rfe.feature_importances_` does not work because `model_rfe` hasn't been fit independently. This is a critical logical error for extracting RFE importance scores. Also, the `zip` for `rfe_importance_dict` tries to zip `rfe_selected_features` (which are 7 features) with `model_rfe.feature_importances_` (which would be importances from a model trained on *all* features, or possibly 0 if not fit). This will lead to an incorrect mapping or an error.
- **SelectKBest Implementation:** The selection of features is mostly correct, but the `kbest_score_dict` tries to zip `selected_features_kbest` (7 features) with `kbest_scores` (which is `selector.scores_` - scores for *all* features). This is a mismatch in dimensionality and will lead to incorrect or incomplete scores for the intended selected features. To get the scores for *only* the selected features, you need `selector.scores_[selector.get_support()]`.
- **Handling NaNs/Infs:** The generated code does not handle potential NaN/Inf values introduced by feature engineering (e.g., `acidity_ratio` or `alcohol_sugar_ratio` if `volatile acidity` or `sugar` is zero). This is a common failure point in real-world data.

**Code Quality:**
- **Readability:** Generally readable.
- **Imports:** Correct and grouped.
- **Structure:** Follows a logical flow for the most part (load, engineer, scale, select, compare).
- **Comments/Explanations:** Lacks in-depth comments explaining the engineered features or the rationale behind the feature selection methods.

**Completeness:**
- **Minimum 3 engineered features:** Yes, 3 are created.
- **At least two different feature selection techniques:** Yes, RFE and SelectKBest are used.
- **Identify top 5-7 features:** Yes, `k=7` is used.
- **Compare and contrast feature sets:** Yes, it prints the selected features and common features. However, it completely lacks the *analysis* or *explanation* of why the sets might differ, which was explicitly asked for in ""Compare and contrast the feature sets selected by each method."" and ""Display the final selected features with their importance scores or rankings."" The output lacks the ranking/scores for RFE and the correct scores for KBest.
- **Display final selected features with importance/rankings:** This part is incorrect due to the issues in how feature importances/scores are extracted and mapped for both RFE and SelectKBest. The RFE part is logically flawed, and the KBest part has a data mismatch.

Overall, while the skeletal structure is there, the crucial parts of correctly extracting and displaying feature importances/scores for comparison, and handling potential data issues, are done incorrectly or are missing. The comparison and contrast section also lacks the required depth.",gemma-3-27b-it,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble imp...",6,"The generated code attempts to address all requirements but falls short in several areas regarding correctness, completeness, and best practices.

**Strengths:**
- Implements multiple ensemble methods (RF, GB, XGB, AdaBoost).
- Includes Voting and Stacking ensembles.
- Performs hyperparameter tuning using GridSearchCV.
- Attempts feature importance plotting and residual analysis.
- Calculates standard regression metrics (MAE, MSE, RMSE, R).

**Weaknesses and Areas for Improvement:**

1.  **Over-simplification/Incorrect Missing Value Handling:**
    *   `df.fillna(df.mean(), inplace=True)` is used for all columns. This is incorrect for categorical columns and `Year` (which is often a float after `df.mean()` if not properly handled). The Ground Truth uses `SimpleImputer` within a pipeline for both numerical and categorical features, which is robust and correct.
    *   `df.fillna(df.mean())` on `User_Score` (which might contain 'tbd' or non-numeric if not converted first) would fail or produce incorrect results. The Ground Truth correctly converts `User_Score` to numeric first.

2.  **Incorrect Categorical Encoding:**
    *   `LabelEncoder` is used for `Platform`, `Year`, `Genre`, `Publisher`. `LabelEncoder` assigns ordinal values (0, 1, 2...) which implies an order that doesn't exist for nominal categorical data, potentially misleading models. `OneHotEncoder` (as in Ground Truth) is the correct approach.
    *   `'Name'` and `'Rank'` are included in `categorical_cols` for encoding, but these are often dropped as they are irrelevant or unique identifiers. The Ground Truth correctly drops `'Name'` and `'Rank'`.
    *   `Year` is treated as categorical and encoded. It should be handled as numerical data, likely imputed with median or mode, not label encoded.

3.  **Data Leakage/Pipeline Issue:**
    *   The `StandardScaler` and `LabelEncoder` are applied *before* splitting the data (`features = scaler.fit_transform(features)`). This introduces data leakage because the scalers/encoders learn from the entire dataset, including the test set, leading to overly optimistic performance estimates. The Ground Truth handles preprocessing within `Pipeline` and `ColumnTransformer` *after* splitting (or within the `cross_val_score` loop), ensuring proper data separation.

4.  **Cross-Validation Implementation:**
    *   The `evaluate_model` function uses `X_train`, `y_train`, `X_test`, `y_test` for evaluation, which reflects a single train-test split, not cross-validation.
    *   A separate `cross_validation` function exists, but it's only used for `neg_mean_squared_error` and its results are not comprehensively displayed or used as the primary evaluation metric for *all* models, which was a core requirement. The prompt specifically asked for 5-fold CV to evaluate *each* model and then report *all* metrics (MAE, MSE, RMSE, R). The Ground Truth performs this loop for each model.

5.  **Feature Importance Plotting:**
    *   `features[indices]` in `plt.xticks` will index rows, not column names. It should be `df.columns[:-1][indices]` assuming `df.columns[:-1]` accurately represents feature names, which it likely won't after `LabelEncoder` and `StandardScaler` transformations.
    *   The plot might not be meaningful if `LabelEncoder` was used, as the feature names will just be the original column names, not the specific one-hot encoded features, which might be more relevant for interpretation.
    *   Missing `plt.show()` after the `print` statement for XGBoost feature importance. (Correction: it's outside the function, so it only shows one plot, but still has issues with xticks).
    *   No feature importance for the voting/stacking ensemble, which is a common limitation but worth noting or explaining.

6.  **Hyperparameter Tuning Target:**
    *   The prompt asked for tuning the *best individual model*. The generated code hardcodes tuning for XGBoost, without dynamically identifying the best model first.

7.  **Clarity and Structure:**
    *   The code jumps between tasks (e.g., individual model evaluation, then voting, then tuning, then back to feature importance). The Ground Truth follows the numbered requirements more logically.
    *   Lack of clear print statements to indicate which section of the task is being executed, making it harder to follow the output.

8.  **Minor Issues:**
    *   `df.columns[:-1]` assumes `Global_Sales` is always the last column, which might be brittle.
    *   `sns.residplot(y_pred_stacking, residuals, ...)` is correct but the Ground Truth provides slightly more detailed residual analysis with two plots (scatter plot of residuals vs. predicted, and a histogram of residuals), offering more insights.
    *   The generated code uses `X_train`, `y_train`, `X_test`, `y_test` throughout for actual model fitting and prediction, but then uses `X` and `y` for `cross_validation` and `GridSearchCV` without explicit re-handling, potentially leading to inconsistencies. The Ground Truth uses `X` and `y` consistently for cross-validation steps after the initial split for demonstration (though typically you'd fit on `X_train` within the CV loop).

These issues, particularly around data preprocessing (missing values, encoding), data leakage, and proper cross-validation, significantly impact the reliability and correctness of the model evaluation.",gemma-3-27b-it,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error
import pymc3 as pm
import theano.tensor as tt
import ruptures as rpt
from lifelines import KaplanMeierFitter
import matplot...",5,"The generated code attempts to address all requirements but falls significantly short in terms of correctness, completeness, and practical implementation for several tasks.

**Critically Missing/Incorrect Points:**

*   **Data Loading & Aggregation:** The generated code assumes 'Year' column in CSV is the release year, while the ground truth correctly identified 'Year_of_Release'. It also lacks robust error handling for file not found and proper cleaning of year/sales data (e.g., dropping NaNs, converting to int) before aggregation. The ground truth also correctly sets the index to datetime objects, which is crucial for time series analysis directly, which the generated code does not.
*   **ARIMA Modeling:**
    *   **Manual ARIMA Order:** Uses a hardcoded `order=(5,1,0)` for ARIMA, which is an arbitrary choice. The task explicitly asks for ""forecast future global sales trends"" which often implies finding an optimal model (as done by `auto_arima` in ground truth).
    *   **No Confidence Intervals:** The generated code calculates `conf_int_arima` but doesn't print or visualize it, failing requirement 9.
*   **Seasonal Decomposition:** While it calls `seasonal_decompose`, it does not set `period` parameter or check if the data length is sufficient, which is crucial for yearly data (yearly data typically doesn't have ""seasonal"" component in the traditional sense unless a multi-year cycle is assumed). It also doesn't plot the decomposition.
*   **Multivariate Regression:**
    *   **Incorrect Feature Engineering:** Uses `LabelEncoder` for 'Genre', 'Platform', 'Publisher'. While this converts them to numbers, it assigns arbitrary ordinal relationships which is incorrect for nominal categorical variables. One-hot encoding (as used by `pd.get_dummies` in ground truth) is the correct approach.
    *   **Interaction Terms:** The interaction terms `X['Year_Genre'] = X['Year'] * X['Genre']` etc. are problematic due to `LabelEncoder`'s ordinal assumption and also do not match the task's implicit requirement of interaction terms with *specific* categories.
    *   **No Confidence/Prediction Intervals:** Fails requirement 9 for this model.
*   **Polynomial Regression:** It correctly uses `StandardScaler` and `Ridge`/`Lasso`. However, it trains on the entire dataset `X_poly_scaled` instead of using the `TimeSeriesSplit` for validation, and the `y` variable includes `Global_Sales` for all games, not just yearly aggregated sales, which is inconsistent. No confidence/prediction intervals are provided.
*   **Bayesian Regression:**
    *   **Overly Simplified Model:** `mu = alpha + beta * df['Year']` is a very basic linear model. The task implies a more complex model perhaps incorporating categorical or interaction terms, similar to the multivariate regression.
    *   **Scaling:** It uses `df['Year']` directly without scaling, which is generally bad practice for MCMC sampling.
    *   **No Explicit Prediction Intervals:** While `pm.sample_posterior_predictive` is called, the output `bayesian_predictions` is not processed to extract prediction intervals or a summary.
    *   **Dependencies:** While PyMC3 is imported, Theano is also required for it to run which is often a point of failure for users not having it pre-installed.
*   **Change Point Detection:** Correctly identifies change points, but doesn't plot them on the time series or print the corresponding years.
*   **Survival Analysis:**
    *   **Incorrect ""Profit_Years"":** The `df['Profit_Years'] = np.random.randint(1, 10, size=len(df))` is a placeholder that simulates data instead of deriving 'game longevity' from the actual dataset as described in the task (years a game remains profitable/observed). This is a critical failure of the requirement.
    *   **Only Kaplan-Meier:** Only implements Kaplan-Meier, missing the Cox Proportional Hazards model which is a key part of the task for incorporating covariates.
    *   **Event Definition:** The `event_observed = df['Global_Sales'] > 0` is too broad; for longevity, it should be about whether the game was still active/profitable at the end of the observation period, or if its ""death"" event was observed.
*   **Confidence/Prediction Intervals (Overall):** Only provides a placeholder for ARIMA's CI. The ground truth provides detailed explanation about how `sklearn` models lack direct methods and proposes approximations. The generated code largely ignores this crucial requirement for all other models.
*   **Time Series Cross-Validation:** Only provides a basic `TimeSeriesSplit` for the `LinearRegression` model, not for ARIMA, Ridge, or Lasso, as explicitly required. The `X` used for TS-CV in the generated code is the full non-aggregated DataFrame columns, while the ground truth correctly uses aggregated data for ARIMA and then filtered, prepped data for the regression models, ensuring data ordering.
*   **Missing Imports/Plots/Output:** Several necessary imports (e.g., `matplotlib.pyplot`, `pmdarima`, `statsmodels.formula.api`, `arviz`, `seaborn`) are missing or incorrectly assumed. Critical plotting (ARIMA forecast, decomposition, change points, survival functions) is missing, and print statements for results are minimal.",gemma-3-27b-it,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture
from sklearn.manifold import TSNE, UMAP
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.datasets import make_blobs
from sklearn.ensemble import IsolationForest
from mlxtend.frequent_patterns import apriori, association_rules
from scipy.stats import ttest_ind, f_oneway
im...",5,"The generated code attempts to address many parts of the prompt but fails significantly in correctness, completeness, and code quality in several key areas.

**Primary Reasons for Failure:**

*   **Incorrect Feature Engineering:**
    *   `Regional_Sales_Distribution` calculation: `df[['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']].sum(axis=1)` calculates *total* sales, not a *distribution*. The subsequent `NA_Share`, `EU_Share`, etc., are correct in concept for *proportion* but are misnamed relative to the `Regional_Sales_Distribution` definition.
    *   `Market_Position`: This is defined as ""rank within genre"" but the generated code simply maps `df['Genre'].map(genre_counts)`. This calculates the *frequency* of each genre, not the rank of a specific game within its genre by sales. This is a fundamental misunderstanding of the feature.
    *   `Publisher_Portfolio_Size`: Similarly, this is defined as `df['Publisher'].map(publisher_counts)`, which maps the frequency of publishers, not the *number of unique games* published by that publisher.
    *   Missing `Year_of_Release` in preprocessing and feature creation, which is a crucial numerical feature.
    *   Missing `df.dropna()` for specific columns initially, which is handled more robustly in the ground truth. The blanket `df.dropna()` after loading will remove many rows if any single column (like Critic_Score) has NaNs.

*   **Incomplete Preprocessing:**
    *   Categorical features (`Platform`, `Genre`, `Publisher`) are dropped (`df.drop(['Genre'], axis=1)`, `df.drop(['Publisher'], axis=1)`) instead of being one-hot encoded and included in the clustering features. This severely limits the richness of the clusters and violates a core data science practice for using categorical data in numerical algorithms like K-Means. The numerical feature selection `X = df[features]` then completely ignores these.
    *   The features `Critic_Score` and `User_Score` are included without any handling of potential non-numeric types or missing values, which are common in real-world datasets for these a. The ground truth explicitly handles all sales cols as numeric and fills NaNs.

*   **Clustering Implementation Flaws:**
    *   **Optimal Cluster Determination:** This is completely absent. The code hardcodes `n_clusters=3` for K-Means, Hierarchical, and GMM, and gives no method for determining `eps` and `min_samples` for DBSCAN. This is a critical failure given the prompt's explicit requirement for ""determining optimal number of clusters using silhouette analysis, elbow method, and gap statistic."" Only silhouette and Davies-Bouldin are calculated for validation *after* arbitrary cluster counts are chosen.
    *   **DBSCAN:** `silhouette_dbscan` and `davies_bouldin_dbscan` will likely fail if DBSCAN produces only one cluster or too many noise points, as the ground truth points out by handling the -1 label. The generated code also doesn't plot the k-distance graph or provide any logic for `eps` and `min_samples`, which are vital for DBSCAN.

*   **Consensus Clustering:** The generated code's ""Consensus Clustering"" section is extremely simplified and *wrong*. It calculates `adjusted_rand_score` between K-Means and DBSCAN labels, which is a *metric for comparing clusterings*, not a method for *consensus clustering* (which usually involves aggregating stability across multiple runs/algorithms). The ground truth's approach using a co-occurrence matrix is the correct implementation of consensus clustering.

*   **Association Rule Mining:** The implementation `basket = df['Genre'].tolist(); itemsets = apriori(pd.DataFrame({'items': basket}))` is fundamentally flawed. `apriori` requires a transactional dataset, typically in a one-hot encoded format where each row represents a transaction and columns are items. Passing a single list of genres like this will treat each row (game) as a single item, not a transaction. It would only find rules like ""Genre A implies nothing because Genre A is the only thing in its transaction."" The ground truth correctly uses `OneHotEncoder` on multiple categorical features.

*   **Recommendation System:** The recommendation system is very basic and has logical flaws. The `recommend_games` function tries to find a ""relevant cluster"" based on `mean_na_sales` and `game_genre` but the logic for `relevant_cluster` is highly arbitrary (`if profile['mean_na_sales'] > 0 and game_genre in df['Genre'].unique():`). The use of `rules` is also problematic given the incorrect ARM implementation. It doesn't connect meaningfully to the clusters or provide concrete recommendations.

*   **Validation:** While some metrics are calculated, the context of optimal cluster determination and external validation is missing. The `calinski_harabasz_score` is only shown for KMeans, not all algorithms.

*   **Code Quality/Structure:**
    *   Imports are a mix of standard and unnecessary `make_blobs` (for internal testing, not for this task).
    *   No proper output or print statements to indicate progress or results, unlike the ground truth which guides the user through the process.
    *   Lack of error handling (e.g., `FileNotFoundError` for CSV).
    *   Hardcoded `n_clusters=3` implies a lack of proper methodology.
    *   Variable names are sometimes confusing (e.g., `X` for data, not feature names).

In summary, while the generated code includes many requested libraries and aims to implement features, the actual implementations are either incorrect, incomplete, or based on significant misunderstandings of the underlying concepts for several critical components, most notably feature engineering, optimal cluster selection, clustering, and association rule mining.",gemma-3-27b-it,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,,,Error,Failed to generate code,gemma-3-27b-it,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import pymc as pm
import arviz as az
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from lifelines import KaplanMeierFitter
from scipy.stats import norm
from causalinference import CausalModel
import warnings
warnings.filterwarnings(""ignore"")


# 1. Load and Preprocess Dataset
data = pd.read_csv(""diabetes.csv"")
data = data.dropna()  # Simple ...",4,"The generated code attempts to address many requirements but falls short in implementation correctness, completeness, and adherence to specific instructions.

**Major Issues:**

*   **PyMC3 vs. PyMC4 (PyMC):** The ground truth explicitly asked for PyMC3 or PyMC4, and the generated code uses `pymc` (which is PyMC v5+ and has significant API changes from PyMC3). While PyMC v5 is the current version, the prompt specified PyMC3 or PyMC4. The code seems to be written with PyMC v5's API in mind, but fails to use it correctly in several places, leading to errors or conceptual inaccuracies.
*   **Missing Value Handling:** The code uses `data.dropna()`, which is explicitly called out as ""Simple dropna for demonstration. More advanced handling needed in a real application."" The prompt asked for ""comprehensive missing value handling."" This is a significant omission.
*   **Hierarchical Model Implementation:**
    *   The `group_idx` is `np.random.randint(0, 5, size=len(data))`, which creates *random* groups. The prompt implies using *existing* groups or creating *meaningful* ones (like the Ground Truth's `Age_Group`). Random groups do not represent true hierarchical structure for modeling.
    *   The `linear_predictor` for the hierarchical model is incorrect: `pm.Deterministic(""x_pred"", pm.math.dot(X, pm.Normal('beta', mu=0, sigma=1, shape=X.shape[1])))` attempts to wrap the dot product in a `Deterministic` with a `Normal` prior for `beta` *inside* the dot product. This is syntactically and conceptually wrong for defining the linear predictor and its coefficients. It should be a `pm.Normal('beta', ...)` *directly* multiplied by `X`.
*   **Mixed-Effects Model:** The code comments out a `statsmodels` mixed-effects model, stating ""(using statsmodels - not pymc, as pymc has limitations)"". This is incorrect; PyMC (and PyMC3/4) is specifically designed for mixed-effects (hierarchical) models, as demonstrated by the ground truth. This is a critical misunderstanding of PyMC's capabilities.
*   **Survival Analysis:** Uses `lifelines.KaplanMeierFitter`, which is for descriptive survival curves, not a *modeling technique* like Cox Proportional Hazards (which the prompt might imply for ""survival analysis techniques"") or Bayesian survival models (which would align with the overall Bayesian theme). The prompt also asked for *simulating* a time component, which is done very simplistically here (just `np.random.exponential`) without linking it to predictors or outcome systematically.
*   **Gaussian Processes:** Uses `GPy` library. While GPy is a valid GP library, the prompt implicitly leans towards a PyMC3/4 solution for Bayesian modeling. More importantly, the GP implementation is simplistic; it models `X` vs `y` directly as a GP, which is more like GP regression but not necessarily how one would use GPs for non-parametric *relationships between variables* in a diagnostic/analytical context, especially given the binary `y`. The ground truth example of modeling `Glucose` vs `Age` (two continuous vars) is more aligned with typical GP application in this context.
*   **Causal Inference:** The `causalinference` library is a good choice for PSM, but the code comments out the actual PSM implementation. It defines `data['treatment']` correctly but then provides commented-out code for `CausalModel` that is incomplete and doesn't fully execute or estimate the ATE. This does not fulfill the requirement. Instrumental Variables (IV) are completely omitted.
*   **Model Comparison and PPC:** The `WAIC` and `LOO` lines are commented out, and the `arviz.WAIC` and `arviz.LOO` functions are not used. The `plot_predictive` line is also commented out. Therefore, these requirements are not fully met or demonstrated as working.
*   **Assumptions/Simplifications:** Multiple critical parts are commented out or mentioned as ""for demonstration"" / ""not pymc"" / ""more advanced handling needed"". This indicates a lack of completeness in addressing the requirements.
*   **Code Quality:**
    *   `pymc` import conflicts with `pymc3` context.
    *   `X[numerical_features] = scaler.fit_transform(numerical_features)` where `numerical_features` was previously defined as `X.select_dtypes(include=np.number)`. This tries to assign to a slice of `X` using `pd.DataFrame` indexing, but `numerical_features` is already a DataFrame of just the numerical columns, so `X[numerical_features]` will cause a warning about setting a copy or incorrect assignment if executed directly without aligning indices. A more robust way would be `X[numerical_features.columns] = ...` or `X_scaled = scaler.fit_transform(X)`.
    *   Excessive use of comments like ""Shape needed"" or ""example"" where actual implementation is lacking or incorrect.
    *   Unused imports (e.g., `train_test_split`, `norm`).",gemma-3-27b-it,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture, SpectralClustering, OPTICS
from sklearn.manifold import TSNE, UMAP
from sklearn.autoencoders import Autoencoder
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.datasets import make_blobs
from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection impo...",3,"The generated code attempts to address several aspects of the prompt but falls significantly short in implementation quality, correctness, and completeness for a task labeled ""advanced.""

**Critical Missing/Incorrect Implementations:**

1.  **Imports:** The generated code includes `from sklearn.autoencoders import Autoencoder` which is incorrect. Autoencoders are part of deep learning frameworks like TensorFlow/Keras, not directly in scikit-learn. Similarly, `sklearn.datasets.make_blobs` and `sklearn.ensemble.BaggingClassifier`, `sklearn.model_selection.train_test_split`, `sklearn.svm.SVC`, `sklearn.neural_network.MLPClassifier`, `sklearn.ensemble.RandomForestClassifier` are imported but never used, indicating a lack of focused implementation for the specific task requirements. `UMAP` is imported but not correctly installed via `pip install umap-learn` in the environment setup, and typically `umap.umap_ as umap` is used.
2.  **Dataset Preprocessing & Feature Engineering:**
    *   `df.dropna(inplace=True)` is used for missing values. The ground truth correctly identifies that 0s in certain columns (Glucose, BP, SkinThickness, Insulin, BMI) are *missing* values, not actual zeros, and handles them via imputation (median imputer). `dropna` would remove a large portion of the dataset, which is inappropriate.
    *   The feature engineering is speculative and assumes columns like 'Waist', 'Height', 'Diastolic' exist in `diabetes.csv`, which they do not. This leads to `KeyError` or incorrect calculations. `HOMA_IR` calculation is also simplified and likely incorrect from a clinical standpoint without the full context of the actual data and parameters.
    *   The ""Metabolic Syndrome Indicators"" requirement is very specific in the prompt, and the generated code's attempt fails to create meaningful indicators based on the provided features.
3.  **Clustering Algorithms:**
    *   The `cluster_data` function is defined but never called or used to actually perform clustering and store results. There's no loop or explicit execution of the different algorithms.
    *   Parameter tuning is mentioned (`n_clusters_kmeans`, `eps_dbscan`, etc.) but no actual loop or logic to determine optimal parameters is implemented. The placeholder comments for `evaluate_clustering` clearly state it's ""as needed"" and doesn't implement gap statistic or elbow method. The ground truth shows correct implementation with plots and actual selection of optimal `k`.
4.  **Dimensionality Reduction:**
    *   The `Autoencoder` implementation is fundamentally flawed (incorrect import, and `autoencoder.predict(df)` won't return the *embeddings* (latent space) but rather the *reconstruction* if it were a simple `Autoencoder` class). The ground truth correctly builds a Keras model and extracts the encoder's output.
    *   `UMAP` is imported as `UMAP` directly, but the package is usually `umap-learn` and often imported as `umap.umap_ as umap`.
    *   The dimensionality reduction functions are defined but not *executed* to produce the `X_tsne`, `X_umap`, `X_ae` variables needed for visualization in later steps.
5.  **Consensus Clustering:** The code explicitly states ""not fully implemented due to complexity,"" which is a direct failure on a core requirement of the prompt.
6.  **Patient Phenotype Profiling:**
    *   The `profile_clusters` function is defined but again, never called.
    *   The statistical testing (`ttest_ind`) is only for continuous features and compares *each cluster* to the *overall dataset mean*, which isn't the standard approach for comparing clusters to each other or for identifying unique characteristics. The ground truth correctly uses ANOVA/Kruskal-Wallis to compare *across clusters* and Chi-squared for categorical features.
    *   It doesn't include visualizations of cluster profiles.
7.  **Anomaly Detection:** While `IsolationForest` is implemented, it's just one method, and it's not visualized on the dimensionality-reduced plots as required. Other methods (OneClassSVM, LOF) are not implemented.
8.  **Time-Series Clustering:** The code has a placeholder comment, failing to provide even a conceptual outline or acknowledge the dataset's nature.
9.  **Association Rule Mining:**
    *   The `binary_df` creation is highly problematic: `df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].applymap(lambda x: 1 if x > df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].mean().mean() else 0)` takes the *mean of means* across *all* these columns, which is not a meaningful threshold for binarization of individual clinical parameters. Ground truth correctly discretizes continuous features into bins (e.g., quantiles) for ARM.
    *   It doesn't include any of the engineered features or target 'Outcome' in the ARM, which would be crucial for discovering patterns related to diabetes risk.
10. **Risk Stratification:**
    *   The `risk_stratification` function defines arbitrary risk levels based on cluster numbers (0, 1, 2) without any relation to the actual characteristics or risk factors of those clusters. The ground truth derives risk order from diabetes prevalence within clusters and provides clinical interpretation.
    *   The function is defined but implicitly assumed to be called within some larger structure, which is not shown.

Overall, the code provides fragments of functions and concepts but fails to integrate them into a cohesive, correct, and complete solution to the prompt. Many functions are defined but not executed, critical imports are wrong or unused, fundamental data preprocessing steps are incorrect, and core requirements are explicitly stated as not implemented or are poorly implemented.",gemma-3-27b-it,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,,,Error,Failed to generate code,gemma-3-27b-it,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import numpy as np
import pandas as pd
import pymc as pm
import arviz as az
import statsmodels.formula.api as smf
from sklearn.experimental import enable_causal
from sklearn.causal import CausalModel
from sklearn.preprocessing import MinMaxScaler
from scipy.stats import norm
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt

# 1. Load and Preprocess Data
data = pd.read_csv('world-happiness-report-2021.csv')
data = data.rename(columns={col: col.replace('...",3,"The generated code attempts to address all requirements but largely fails in implementation details, making most sections either incorrect, incomplete, or highly simplified.

**Critical Failures and Missing Elements:**

*   **PymC3 vs. PymC4 (PymC):** The prompt explicitly requested PyMC3 or PyMC4. The generated code uses `pymc` which is PyMC4 (now just PyMC). While technically correct in using one of the specified versions, it's important to note the changes in API between PyMC3 and PyMC4, which the generated code does not fully adhere to in its modeling.
*   **Data Preprocessing:**
    *   Renaming columns is good, but `GDP_per_capita` is manually renamed, while the ground truth uses `Logged_GDP_per_capita`. This affects subsequent model uses.
    *   Missing value handling is extremely basic (`fillna` with median/mode) and does not include robust scaling or one-hot encoding for categorical variables with `ColumnTransformer` as demonstrated in the ground truth for comprehensive handling. The ground truth also correctly renames columns to `Logged_GDP_per_capita` after loading, which is crucial for consistency with the actual dataset's column names.
    *   It drops rows with *any* missing values in a few key columns, which is less comprehensive than imputing and scaling for *all* relevant numerical and categorical features.
*   **Hierarchical Bayesian Model:**
    *   **Incorrect `region_effect` application:** `region_effect` is defined as a single `pm.Normal`, not a plate for different regions. In PyMC, for hierarchical models, `shape=n_regions` is crucial for random effects to be indexed by region. The `region_effect[data['Region'].astype('category').cat.codes]` will fail or behave unexpectedly.
    *   **Mean function:** `mu = pm.Deterministic('mu', 0)` then `intercept = pm.Normal('intercept', mu=mu, sigma=sigma)` is redundant and likely incorrect. The hierarchical model should have an `intercept` (population-level mean) and then `a[region_id]` for regional deviations.
    *   **Fixed effects:** Only `GDP_per_capita` is included as a fixed effect, and its coefficient is hardcoded to `0.01` (`data['GDP_per_capita']*0.01`) instead of being a learned parameter (`beta_gdp * data['GDP_per_capita']`). This is a fundamental error.
    *   `az.plot_trace(trace)` is called, but it doesn't specify what to plot (e.g., specific variables), and it's missing `arviz` import from `trace` output. Also, it doesn't save the plot.
*   **Structural Equation Modeling (SEM):**
    *   The `statsmodels.formula.api.path.PathEffectModel` is an attempt, but `statsmodels` is generally not considered a full SEM package like `lavaan` in R or `semopy` (not requested here). More importantly, the prompt requested SEM *with PyMC3/PyMC4*, implying Bayesian SEM, which is a much more complex task involving defining latent variables and pathways explicitly within the probabilistic programming framework. The generated code uses `statsmodels` for SEM, which is not Bayesian and doesn't align with the spirit of the prompt's focus on PyMC. The ground truth correctly approximates SEM using a series of interconnected Bayesian regressions in PyMC.
*   **Causal Inference (IV & PSM):**
    *   **Instrumental Variables (IV):** The `sklearn.experimental.enable_causal` and `sklearn.causal.CausalModel` are part of `scikit-learn-causal`, which is an experimental package and not standard `scikit-learn`. There's no `Time_since_independence` column in the dataset, so this section will likely fail or produce meaningless results. The ground truth correctly identifies the lack of a suitable instrument and *simulates* one for demonstration purposes, then uses `statsmodels.formula.api.ols` for 2SLS, which is a standard approach. The generated code also directly passes `treatment=data['GDP_per_capita']` to `CausalModel`, implying treatment as a continuous variable, which isn't suited for typical IV setups where treatment is often binary or exogenous shock.
    *   **Propensity Score Matching (PSM):** Uses `RandomForestRegressor` to predict propensity scores (continuous `GDP_per_capita`) which is not how PSM works. PSM typically uses a logistic regression to predict the *probability of treatment* (a binary variable) based on covariates. The `treated` variable is then defined as `data['GDP_per_capita'] > data['GDP_per_capita'].mean()`, which is a treatment definition, but the PSM calculation is oversimplified (just taking `sample()` of treated and comparing means, without actual matching or weight application based on propensity scores). The ground truth correctly implements logistic regression for propensity scores and then uses `NearestNeighbors` for actual matching.
*   **Bayesian Regression with Informative Priors:**
    *   Similar to the hierarchical model, the GDP coefficient is hardcoded (`data['GDP_per_capita']*0.01`). Model parameters must be learned, not fixed. Priors are defined for `mu` and `sigma`, but not for the coefficients themselves.
*   **Gaussian Processes:**
    *   The `sklearn.gaussian_process.GaussianProcessRegressor` is used. While it's a GP, the prompt implied *Bayesian* Gaussian Processes (often done with `pymc.gp`). This is a non-Bayesian implementation and does not align with the overarching Bayesian theme of the prompt.
*   **Mediation Analysis:** Acknowledged as ""not implemented in full"" and ""requires more complex SEM"". This is a significant omission given it was a key requirement. The ground truth explicitly shows how to calculate direct, indirect, and total effects from the estimated SEM paths.
*   **Difference-in-Differences:**
    *   `data['Time']` is `np.random.randint(0, 2, len(data))`, meaning it randomly assigns ""pre"" or ""post"" status to each country. This is completely nonsensical for a DiD analysis, which requires actual time periods and a consistent policy application to specific groups over those periods. A meaningful DiD requires a genuine time dimension and a treatment group that *only* receives treatment in the ""post"" period, and a control group that doesn't. The simulation is flawed.
*   **Bayesian Model Comparison:**
    *   The two models compared (`model1` and `model2`) are not well-defined. `model1` incorrectly defines `mu` and `intercept` as a single parameter. `model2` has the same error as the hierarchical model regarding `region_effect` not being a plate.
    *   WAIC is calculated, but Bayes Factors (a specific request) are cursorily mentioned as ""approximated via bridge sampling (requires additional package installation)"" rather than attempting to explain their conceptual basis or reason for omission in this context.
*   **Credible Intervals, PPC, Sensitivity Analysis:** ""Further work needed for full implementation"" for these crucial Bayesian diagnostics. The ground truth provides detailed examples for all of these.

**Overall:** The generated code presents a superficial understanding of many advanced concepts. It uses placeholder values, incorrect modeling approaches, and relies on non-Bayesian methods where Bayesian ones were explicitly requested, or completely omits tasks. The data preprocessing is also insufficient. The code primarily demonstrates knowledge of library calls but lacks the statistical rigor and correct application of the requested methods.",gemma-3-27b-it,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture, SpectralClustering, HDBSCAN
from sklearn.manifold import TSNE, UMAP
from sklearn.neural_network import Autoencoder
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.random_projection import GaussianRandomProjection
from sklearn.ensemb...",3,"The generated code attempts to address many parts of the prompt but fails significantly in correctness, completeness, and code quality.

**Correctness & Completeness Issues:**

*   **Feature Engineering (1)):** The initial feature engineering is very weak. `happiness_profile` is created as a concatenated string, which is useless for numerical analysis. `regional_characteristics` is also just a placeholder. `df.drop(['Ladder score'], axis=1)` is incorrect; 'Ladder score' is a key feature and should be part of the analysis, either as a target or as an engineered feature (as the ground truth does). The selection of `features` for `X` is incomplete, missing the crucial regional indicators and the `Ladder score` itself, which was explicitly mentioned in the prompt (""happiness profiles"").
*   **Clustering Algorithms (2)):**
    *   The `perform_clustering` function is deeply flawed. It attempts to instantiate models using `**params` but doesn't pass the necessary `random_state` for `KMeans` nor `n_init` (which is necessary for `KMeans` stability in current scikit-learn).
    *   It also claims to handle `HDBSCAN` but `HDBSCAN` is imported as `hdbscan`, not as a class `HDBSCAN` directly from `sklearn.cluster` as implied.
    *   `Autoencoder` is imported from `sklearn.neural_network` which does not exist in standard scikit-learn. This import errors out immediately.
    *   The `fit_predict` method might fail for `GaussianMixture` and `SpectralClustering` if `n_components` or `n_clusters` are not handled carefully (e.g., if there's only one cluster formed).
    *   Optimal parameter determination for these algorithms is largely absent.
*   **Dimensionality Reduction (3)):**
    *   `Autoencoder` implementation is completely wrong due to the incorrect import. The ground truth shows how to implement a basic autoencoder using Keras/TensorFlow.
    *   `PCA` was not requested.
    *   No visualization of reduced dimensions is provided with cluster labels as requested.
*   **Optimal Clustering Parameters (4)):** Only `silhouette_score` and `davies_bouldin_score` are mentioned. The prompt specifically asked for gap statistic, elbow method, and stability analysis. The current implementation doesn't actually ""determine optimal parameters"", it just evaluates a given set of parameters.
*   **Consensus Clustering (5)):** explicitly marked as `(Placeholder)`. This is a critical requirement and is completely missing.
*   **Country Profiles (6)):** The `create_country_profiles` function blindly takes `df[clusters == cluster_id].mean().to_dict()`. This will calculate means for *all* columns in `df`, including string columns, which will produce errors or nonsensical results. It does not perform statistical significance testing or effect size analysis as explicitly requested.
*   **Anomaly Detection (7)):** The implementation uses a very basic `NearestNeighbors` approach and defines anomalies as points whose average distance to neighbors is above a certain percentile. This is a crude method and doesn't explicitly use standard anomaly detection algorithms like Isolation Forest, LOF, or One-Class SVM requested in the prompt.
*   **Network Analysis (8)):** The `create_network` function is fundamentally flawed. `correlation = np.corrcoef(row[feature], other_row[feature])[0, 1]` is incorrect for calculating correlation between two countries based on a single feature. `row[feature]` and `other_row[feature]` will be single scalar values, whose `np.corrcoef` will always be NaN or 1. It should correlate the *feature vectors* of two countries. The prompt also asked for understanding relationships based on ""happiness factors"", not just arbitrary features.
*   **Association Rule Mining (9)):** `encoded_features = df[features].apply(lambda x: ','.join([str(int(val)) for val in x]))` is completely wrong. Association rule mining requires a boolean transaction format, not a comma-separated string of binned numerical features. `mlxtend` expects a dataframe where columns are items and rows are transactions (e.g., `True`/`False` or `1`/`0` indicating presence). The way the ground truth uses `get_dummies` after binning is the correct approach.
*   **Recommendation System and Policy Insights (10)):** This section is missing entirely from the code, with only a ""Simplified"" comment. The prompt asked for geopolitical validation and detailed policy insights.

**Code Quality Issues:**

*   **Imports:** Several incorrect imports (e.g., `sklearn.neural_network.Autoencoder`) and missing necessary imports (`SimpleImputer`, `TSNE` and `UMAP` are imported but not the correct libraries, `hdbscan`, `tensorflow.keras`, etc.).
*   **Structure:** No class-based structure, which leads to scattered variables and functions, making it harder to manage state and reuse components compared to the ground truth's `AdvancedHappinessAnalysis` class.
*   **Hardcoding vs. Parameters:** Many parameters are hardcoded (e.g., `n_clusters=5` for KMeans), preventing flexible experimentation or reusability.
*   **Error Handling/Robustness:** Minimal error handling. For instance, `evaluate_clustering` doesn't properly handle cases where only one cluster is found (leading to errors for silhouette/DB score). The `perform_clustering` function's `fit_predict` could fail for algorithms requiring specific initialization or parameters not handled by the general `**params` approach.
*   **Missing Visualizations:** Many types of visualizations were requested, particularly for DR and clustering, which are not implemented comprehensively.

Overall, the generated code demonstrates a superficial understanding of the requirements and contains multiple fundamental errors in implementation, making it non-functional for the specified task.",gemma-3-27b-it,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX
import pmdarima as pm
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import numpy as np

# Load the data
df = pd.read_csv('synthetic_covid19_data.csv')

# Parse 'date' and set as index
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)

# Filter for Germany
df_germany = df[df['location'] == 'Germany'].copy()

# Define target and exogenous variables
target =...",8,"The generated code correctly implements the core requirements of the task: data loading and preprocessing, SARIMA model training with exogenous variables, forecasting, and evaluation with RMSE, along with plotting. It correctly identifies the `new_cases` as the target and `stringency_index`, `people_fully_vaccinated` as exogenous variables. The use of `auto_arima` for order selection and `SARIMAX` from `statsmodels` for modeling is appropriate.

Areas for improvement to reach a higher score:
1.  **Missing Value Handling:** The generated code only uses `ffill` for missing value handling. The ground truth code includes a more robust approach with `ffill` followed by `bfill`, and then interpolation as a fallback, with checks for remaining NaNs. This makes the missing value handling in the generated code less comprehensive and potentially less robust to different missing data patterns (e.g., NaNs at the beginning of the series).
2.  **Exogenous Variable Handling for Forecasting:** The generated code directly fits the SARIMAX model and then calls `get_forecast` *without explicitly passing* `exog` to the `forecast` method. While `statsmodels` might implicitly pick up the `exog` when forecasting from the fitted `SARIMAXResultsWrapper` if the `exog` was passed during fitting, it's a best practice and safer to explicitly provide the future exogenous values when calling `get_forecast` or `predict`. The ground truth explicitly passes `X_test` to `get_forecast`, which is clearer and more robust, especially if the `exog` index doesn't perfectly align with the forecast period.
3.  **Error Handling/Robustness:** The ground truth includes `try-except` for file loading and checks for sufficient data length, which are good practices for more robust code. The generated code lacks these direct checks.
4.  **Negative Forecasts:** The ground truth includes a step to set negative forecasts to zero, which is a practical consideration for 'new cases' that cannot be negative. This is missing in the generated code.
5.  **Code Comments/Explanations:** While the prompt requested only code, the ground truth includes more detailed comments explaining each major step, which improves readability and understanding for a more complete solution.
6.  **Minor Plotting Detail:** The ground truth includes training data in the plot, which provides better context for the forecast's performance relative to the full dataset. The generated code only plots the test period.

Overall, it's a strong solution that correctly addresses the fundamental requirements but could be improved in terms of robustness, comprehensive data handling, and explicit handling of exogenous variables during forecasting.",gemma-3-27b-it,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.formula.api as sm
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scipy.stats import kruskal

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Define wine quality categories
def categorize_quality(score):
    if score <= 4:
        return 'low'
    elif 5 <= score <= 6:
        return 'medium'
    else:
        return 'h...",6,"The generated code addresses most of the requirements but has several areas for improvement in correctness and completeness.

**Correctness Issues:**
- **Normality Check:** The normality check with `stats.shapiro` is performed only on groups with available data. The code assumes that if `shapiro_test.pvalue > 0.05` for *each individual group*, then the combined data is normal for ANOVA. This is fundamentally flawed. Shapiro-Wilk tests for normality of *each group's distribution*, but ANOVA assumes normality of *residuals*, which is a different concept, though group normality can be a proxy. More importantly, it does not check the equality of variances (homoscedasticity) which is a critical assumption for ANOVA. The ground truth correctly includes Levene's test for this.
- **Post-hoc for Kruskal-Wallis:** The code correctly identifies that Dunn's test is needed but does not implement it, instead printing a message. The ground truth implements a proxy (pairwise Mann-Whitney U with Bonferroni correction), which is a common and acceptable alternative when Dunn's test isn't directly available via `scipy.stats`. The generated code simply omits the analysis for non-normal cases, which is a major missing piece of the task.
- **Tukey's HSD Implementation:** The `sm.ols` model is built but its output or details are not used for `pairwise_tukeyhsd`. `pairwise_tukeyhsd` can be called directly on the data and groups, as shown in the ground truth, without necessarily fitting a full OLS model first for this specific post-hoc test.
- **Data Filtering for Tests:** The normality checks and primary statistical tests (`f_oneway`, `kruskal`) are applied to potentially empty or incomplete dataframes if a quality category doesn't exist for a feature, which could lead to errors or warnings. Error handling for `FileNotFoundError` is missing.
- **Categorical Order:** The quality categories in the `quality_category` column are not explicitly ordered. While `sns.boxplot` might handle default sorting, maintaining the order ('low', 'medium', 'high') is good practice for consistent plotting and analysis, as done in the ground truth.

**Completeness Issues:**
- **Post-hoc for Non-Parametric:** As noted, post-hoc tests for Kruskal-Wallis are entirely omitted.
- **Detailed Summary:** The final summary is very brief, only listing significant features. The ground truth provides a much more detailed summary including the primary test used, its p-value, and the results of the post-hoc tests which is crucial for interpreting the results.
- **Error Handling:** No `try-except` block for file loading.
- **Alpha Level:** The significance level (`0.05`) is hardcoded multiple times instead of being defined as a constant, which makes it less maintainable.

**Code Quality:**
- Imports: The `statsmodels.formula.api as sm` import is present but not strictly necessary for just calling `pairwise_tukeyhsd`.
- Readability: Generally readable, but the lack of structure for statistical results (storing p-values, test types) makes the later summary less informative compared to the ground truth's `statistical_results` dictionary.

To improve, the generated code should:
1. Implement proper post-hoc tests for non-parametric cases (e.g., pairwise Mann-Whitney U with Bonferroni correction).
2. Include a check for homogeneity of variance (Levene's test) before deciding on ANOVA.
3. Handle cases where quality groups might be empty or have insufficient data for statistical tests.
4. Provide a more comprehensive summary of the findings, including post-hoc results.
5. Add error handling for file loading.
6. Define parameters like `ALPHA` as constants.",gemma-3-27b-it,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,":

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, f1_score
import shap
import warnings
warnings.filterwarnings('ignore')

# 1. Load data and parse 'date'
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

# 2. Determine 90th percentile and create the ta...",6,"The generated code attempts to address all requirements of the task, but has several significant issues that prevent it from being a fully correct or robust solution.

**Positive Aspects:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Basic Structure:** The code generally follows the requested steps (load data, target, FE, etc.).
*   **XGBoost and Hyperparameter Tuning:** Correctly uses `GridSearchCV` with `XGBClassifier`.
*   **Evaluation Metrics:** Uses `roc_auc_score` and `f1_score`.
*   **SHAP Implementation:** Attempts to use SHAP for feature importance.

**Areas for Improvement/Critical Errors:**

1.  **Target Variable Definition (Critical Error):** The definition of `high_daily_death_increase` is flawed.
    *   `group = group[group['new_deaths'] > 0]` and then calculating the quantile on this filtered group. This causes rows with `new_deaths=0` (which should *not* be considered for the percentile threshold) to be dropped from the group *entirely* before the target is assigned. This will lead to many original rows losing their target definition or being incorrectly assigned.
    *   The prompt states: ""A 'high_daily_death_increase' is defined as a day where 'new_deaths' for a 'location' is in the top 10% of its own historical 'new_deaths' (**excluding days with zero new deaths**)."" and implies that the target should be 0 if `new_deaths` is zero. The generated code implicitly drops these or assigns the target to the remaining filtered rows, which will lead to data loss or misalignment when the group is rejoined.
    *   The `groupby().apply()` method as used here will return a DataFrame with a MultiIndex, which needs to be handled (e.g., `reset_index(drop=True)` or `droplevel`). The current code doesn't explicitly handle this, which can cause issues down the line.

2.  **`days_since_first_case` Calculation (Incorrect):** `df.groupby('location')['date'].diff().dt.days` calculates the difference from the *previous observed day* for that location, not the difference from the *first day* for that location. This is a common mistake and needs to be `(x - x.min()).dt.days` within the groupby transform.

3.  **Missing Value Handling (Incorrect/Incomplete):**
    *   `df = df.fillna(0)` is highly problematic. It fills *all* NaNs with 0, including those in important features like `people_fully_vaccinated` or `reproduction_rate` that might benefit from more sophisticated imputation or that might be better left as NaNs and handled by XGBoost. This is especially bad for `population` or other static features, which should likely not be 0.
    *   Engineered features (like lags/rolling stats) should ideally be filled *within* each group after the shift/rolling operations before a global fill. A simple global `fillna(0)` can mask issues. The prompt specifically asked for handling missing values *from feature engineering* (e.g., fill with 0 or backfill), and the proposed solution is too broad.

4.  **Feature Selection & Preprocessing (Issues):**
    *   `people_fully_vaccinated` and `population` are used as raw features, but the prompt requested `'people_fully_vaccinated' / 'population'`. The generated code does not create this ratio.
    *   `X = pd.get_dummies(X, columns=['location'], prefix='location')` is incorrect. `location` is explicitly grouped by throughout feature engineering, but it's not a feature explicitly requested for modeling, and if it were, it should be treated as a categorical feature in `ColumnTransformer` alongside `continent`. Including it (implicitly) as a feature to be one-hot encoded after all other features are selected is syntactically odd and semantically questionable given the prompt. The prompt only explicitly asks for `continent` to be one-hot encoded.
    *   The preprocessing steps (one-hot encoding and scaling) are done *before* the `GridSearchCV` and *outside* of a `Pipeline`. This is a major structural issue. `GridSearchCV` expects features to be processed *within* the pipeline, especially when `X_train` and `X_test` are defined. If `X` is already scaled and one-hot encoded, and then `X_train` and `X_test` are derived, subsequent processing by `GridSearchCV` will not correctly handle transformations on the `X_test` part. This breaks the standard GridSearchCV/Pipeline workflow. The `ColumnTransformer` and `Pipeline` approach is crucial for correct preprocessing and model reproducibility.
    *   The `continent` encoding is applied using `fit_transform` on the entire `df[['continent']]` and then concatenated. This is separated from the other features and not part of a robust preprocessing pipeline. If it were part of a `pipeline`, it would correctly learn categories on `X_train` and apply it to `X_test`.

5.  **Time-based Split (Partially Incorrect):** `train_test_split(X, y, test_size=0.2, shuffle=False)` is used. While `shuffle=False` maintains order, it's applied *after* all the preprocessing and feature creation, which might have messed up the order if not handled correctly. A explicit date-based split (`df_sorted_for_split.iloc[:split_idx]`) is more robust.

6.  **SHAP Feature Names (Incorrect):** `feature_names = X.columns[top_features]` implies that `X` still has column names after `scaler.fit_transform(X)`. `scaler.fit_transform` returns a NumPy array, losing column names. SHAP will then have trouble mapping feature importances back to original names. A `ColumnTransformer` within a `Pipeline` handles this correctly.

7.  **`use_label_encoder` Warning (Missing):** The `XGBClassifier` should ideally have `use_label_encoder=False, eval_metric='logloss'` to suppress warnings.

In summary, while the code aims for the right solutions, its implementation of key steps like target creation, feature engineering, missing value handling, and preprocessing pipelines has several critical flaws that would either lead to incorrect results or break the code directly.",gemma-3-27b-it,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import statsmodels.formula.api as smf
import numpy as np

# Load the data
df = pd.read_csv('synthetic_covid19_data.csv')

# Data preprocessing
df['date'] = pd.to_datetime(df['date'])
df['vaccination_percentage'] = df['people_fully_vaccinated'] / df['population']
df['vaccination_percentage'] = df['vaccination_percentage'].replace([np.inf, -np.inf], np.nan)  # Handle potential division by zero

# Handle missing values
# Impute missing reproduction_rate using median im...",6,"The generated code attempts to address all parts of the task but falls short on several critical aspects, particularly around data preprocessing, missing value handling, and correctly implementing the specified random effects structure.

**Strengths:**
*   **Correct Libraries:** Uses `pandas` and `statsmodels.formula.api` correctly for data handling and modeling.
*   **Model Formula:** The fixed effects formula `reproduction_rate ~ stringency_index + vaccination_percentage + stringency_index:vaccination_percentage` is correctly implemented.
*   **Reporting:** It correctly prints fixed effects and attempts to report variance components.
*   **Assumption Discussion:** The discussion of model assumptions is present, albeit brief.

**Weaknesses and Areas for Improvement:**

1.  **Data Preprocessing and Calculation of `vaccination_percentage`:**
    *   The `vaccination_percentage` calculation `df['vaccination_percentage'] = df['people_fully_vaccinated'] / df['population']` is missing the multiplication by 100 to make it a percentage as implied by the task and common usage for a rate.
    *   It doesn't handle potential NaN values in `people_fully_vaccinated` or `population` *before* the division, which can lead to `NaN`s in `vaccination_percentage` even if `people_fully_vaccinated` and `population` were initially fine.
    *   It also doesn't cap `vaccination_percentage` at 100%, which is a practical necessity since a percentage cannot exceed 100 by definition.
    *   Crucially, `population` and `people_fully_vaccinated` are *not converted to numeric* with `errors='coerce'`, which is a common and necessary step for robustness when reading CSV data. If these columns contain non-numeric strings, the division will fail or produce incorrect results.

2.  **Missing Value Handling:**
    *   The task explicitly asked for *interpolation* or *median imputation grouped by location*. The code only uses median imputation. While median imputation is a valid strategy, interpolation is often preferred for time-series data as it preserves trends.
    *   More importantly, the imputation for `population` and `people_fully_vaccinated` is entirely missing. If these columns have NaNs, the `vaccination_percentage` calculation will produce NaNs, and subsequent median imputation on `vaccination_percentage` alone might not be sufficient or accurate. The ground truth correctly imputes the components (`population`, `people_fully_vaccinated`) *before* calculating `vaccination_percentage`.
    *   Sorting the data by location and date (`df.sort_values(by=['location', 'date'])`) is crucial *before* any group-wise interpolation or forward/backward fill to ensure correct time-series imputation, which is not done in the generated code.

3.  **Random Effects Structure (Critical Error):**
    *   The task specifies ""random intercepts for 'location' nested within 'continent'"".
    *   The generated code uses `groups=df[""location""]` and `vc_formula={""continent"":""1""}`. This is an incorrect setup for nested random effects in `statsmodels.mixedlm`.
    *   `groups=df[""location""]` creates random intercepts for each *location*, independently of the continent.
    *   `vc_formula={""continent"":""1""}` adds a random slope (or rather, a random intercept whose variability is allowed to differ by continent, but it doesn't create nesting in the way required). It is not the standard way to model ""location nested within continent"" as a random intercept for each unique (continent, location) combination.
    *   The correct way, as shown in the ground truth, is to create a unique identifier for each `(continent, location)` combination (`df['continent'].astype(str) + '_' + df['location'].astype(str)`) and then use this combined ID as the `groups` variable. This ensures that each `(continent, location)` pair gets its own random intercept.

4.  **Reporting and Interpretation:**
    *   The interpretation of fixed effects is too generic. It mentions ""average change"" and ""p-values indicate significance"" but doesn't connect these to the specific values in the output or the context of the variables (e.g., what a positive/negative coefficient means for stringency or vaccination). It doesn't attempt to interpret the interaction term's significance.
    *   The variance components report just prints the table. While it gives a brief generic interpretation, it doesn't specifically refer to the variance for the ""continent"" group vs. ""location"" group in the way a nested model would ideally allow (though its current model config doesn't allow that). The ground truth correctly identifies the single random intercept variance.

5.  **Robustness and Error Handling:**
    *   No `try-except` block for `pd.read_csv`, meaning the script would crash if the file is not found.
    *   No warnings suppression for Statsmodels.
    *   No print statements to show progress or confirm steps are completed successfully, making it harder to debug or understand the flow.

Overall, the model setup for random effects is fundamentally incorrect for the specified nesting, and the data preprocessing and missing value handling are incomplete and less robust.",gemma-3-27b-it,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select features
features = ['elixirCost', 'Win Rate', 'Usage']
X = df[features]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Determine the optimal number of clusters (K) using the elbow method
wcss = []
for ...",9,"The generated code is highly correct and effectively solves the task, demonstrating a strong understanding of K-Means clustering, feature scaling, PCA, and visualization.

**Strong points:**
*   **Correct Implementation:** It correctly loads the data, selects features, standardizes them, determines K using the elbow method, applies K-Means, performs PCA, and visualizes the results.
*   **Adherence to Requirements:** All specified steps in the task description are addressed, including specific imports, feature selection, K range, and output visualization.
*   **Appropriate K-Means Parameters:** `n_init=10` is correctly used in KMeans, which is important for robust clustering. `random_state` is also used for reproducibility.
*   **Clear Plotting:** The elbow plot and PCA scatter plot are well-generated, providing clear visualizations for the task.

**Areas for minor improvement (to reach a perfect 10):**
*   **Robust File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust. The generated code assumes the file exists, which is acceptable for a benchmark but less robust for production.
*   **Clarity on Chosen K:** While the code correctly plots the elbow method, it hardcodes `K = 3` without a comment directly linking it to the plot (e.g., ""Based on the elbow plot, choose K=3""). The ground truth explicitly states the chosen K based on observation.
*   **Informativeness of Plots:** The PCA plot could benefit from showing the explained variance ratio for each principal component on the axis labels, as the ground truth does. This provides better context for the reduced dimensions.
*   **Scatter Plot Customization:** Adding a color bar to the PCA scatter plot would make it clearer which color corresponds to which cluster, enhancing interpretability.
*   **Warning Handling:** Although `n_init=10` is used, the code could still subtly handle warnings or provide more print statements for progress, similar to the ground truth.

Overall, this is an excellent solution that fulfills the requirements effectively. The minor points above are mostly about adding robustness and detail to the output for a production-ready solution, not about correctness of the core algorithm implementation.",gemma-3-27b-it,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select features and target
features = ['elixirCost', 'Usage', 'rarity']
target = 'Win Rate'

# Preprocess the 'rarity' column
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)...",7,"The generated code correctly addresses most of the task requirements: loading data, selecting features/target, performing one-hot encoding for 'rarity' using `OneHotEncoder`, splitting data, training a `RandomForestRegressor`, and evaluating with R2 and MSE.

However, there is a critical flaw in how the one-hot encoded features are integrated into the `X` (features) DataFrame. The line `X = df[features]` is executed *before* the one-hot encoding is fully integrated into the `df` in a way that `X` would correctly capture it.

Specifically:
1. `df = pd.concat([df, rarity_df], axis=1)` correctly adds the encoded columns to `df`.
2. `df = df.drop('rarity', axis=1)` correctly removes the original 'rarity' column.
3. However, `X = df[features]` is then called, which *only* selects `['elixirCost', 'Usage', 'rarity']`. Since 'rarity' has been dropped from `df` and replaced by `rarity_encoded_...` columns, `X` will be missing the essential one-hot encoded features. This will likely cause the model training to fail or train on an incorrect set of features if `X` is later modified.

The code works if the `X` and `y` selection is moved *after* the preprocessing of `rarity` and the `features` list is adjusted to exclude 'rarity' (as the encoded columns would be automatically included if correctly handled). In its current state, `X` will not contain the OneHotEncoded columns.

Strong points:
- Correct imports and task flow.
- Correct use of `RandomForestRegressor` and evaluation metrics.
- `OneHotEncoder` is used as specified.

Areas for improvement:
- The logic for integrating `OneHotEncoder` output into the feature set (`X`) needs correction. The features (including the newly encoded ones) should be selected *after* all preprocessing steps are completed and consolidated into the DataFrame from which `X` is derived.
- Error handling for file not found or missing columns, similar to the ground truth, would improve robustness.",gemma-3-27b-it,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Identify unique rarity categories
rarity_categories = df['rarity'].unique()

# Prepare data for ANOVA
win_rates = []
for category in rarity_categories:
    win_rates.append(df[df['rarity'] == category]['Win Rate'])

# Perform ANOVA test
f_statistic, p_value = stats.f_oneway(*win_rates)

# State null and alternative hypotheses
# Null hypothesis: There is no significant differenc...",6,"The generated code loads the data and performs the ANOVA, correctly using `scipy.stats.f_oneway`. It also correctly states the null and alternative hypotheses implicitly in comments and interprets the p-value against the alpha level.

However, there are several crucial improvements needed:
1.  **Robust Data Loading/Handling:** The generated code lacks error handling for `FileNotFoundError` or other issues during CSV loading.
2.  **Data Type Conversion and Cleaning:** It does not explicitly convert the 'Win Rate' column to numeric, which is crucial for statistical tests. It also doesn't handle potential NaN values that might arise from non-numeric data or missing entries in 'Win Rate' or 'rarity' columns. The `f_oneway` function can raise errors if it encounters non-numeric data or `NaN`s in the input arrays.
3.  **Handling Empty Groups/Insufficient Data:** The code doesn't check if enough valid data remains after potential cleaning or if individual rarity groups have enough observations (at least 2) for ANOVA. If some categories have only one or zero valid `Win Rate` entries, it could lead to an error or incorrect results in `f_oneway`.
4.  **Clarity of Hypothesis Statement:** The hypotheses are stated as comments, which isn't ideal for outputting the results as requested. The ground truth prints them explicitly.
5.  **Output Formatting:** The output for F-statistic and p-value is somewhat. The ground truth provides more descriptive output and also prints the significance level which is helpful for context.
6.  **Completeness of Interpretation:** While it states whether to reject or fail to reject, it doesn't offer a concluding statement about the implication of that decision (e.g., ""There is a statistically significant difference..."").

The provided code is a good starting point but lacks the robustness and detailed output expected from production-ready or benchmark-quality code.",gemma-3-27b-it,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
df = pd.read_csv('iris.csv')

# Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
...",8,"The generated code correctly implements all the required steps for the task, including loading data, defining features and target, encoding the target, splitting the data, training the specified models, and evaluating their performance. It also correctly determines the best model based on F1-score.

**Strong Points:**
- All necessary imports are present.
- Data loading and feature/target selection are correct.
- Target variable encoding is handled correctly.
- Data splitting parameters are correct (`test_size=0.2`, `random_state=42`).
- All three specified models are trained with the correct parameters.
- Performance metrics (accuracy, precision, recall, F1-score) are calculated and printed correctly using macro averaging.
- The logic for determining the best model based on F1-score is sound.

**Areas for Improvement:**
- **`LogisticRegression` `max_iter`:** The ground truth includes `max_iter=200` for `LogisticRegression`. While not strictly required by the prompt, `LogisticRegression` often benefits from an increased `max_iter` to ensure convergence, especially with default settings. The generated code uses default, which might trigger a `ConvergenceWarning` on some systems/versions (though not a fatal error for this small dataset).
- **Hardcoded Best Model Logic:** The `if/elif/else` structure to find the best model is functional but less robust and scalable than using a dictionary to store models and their F1 scores, then finding the max, as demonstrated in the ground truth. This approach is more flexible if more models were to be added.
- **`stratify` parameter in `train_test_split`:** The ground truth includes `stratify=y_encoded` (or `y` in the generated code's context). While not explicitly requested in the prompt, stratifying the split is good practice for classification tasks, especially with imbalanced datasets, to ensure that the class distribution in the train and test sets is similar to the original dataset. For Iris, given its balanced classes, its omission might not significantly affect results but is a best practice.
- **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good robustness practice that the generated code lacks.",gemma-3-27b-it,LLM_judge_config
H26,hard,Clustering,iris.csv,".```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
iris = pd.read_csv('iris.csv')

# Select features
X = iris[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=3, n_...",8,"The generated code correctly addresses most of the task requirements. It loads the dataset, selects and standardizes features, applies K-Means clustering, and performs PCA. It then generates two scatter plots, one colored by K-Means labels and another by true species labels, which is the core of the task.

However, there are a few minor deviations/improvements:
1.  **Plot Titles:** The specified plot titles were ""PCA of Iris Dataset: Points Colored by K-Means Cluster Labels"" and ""PCA of Iris Dataset: Points Colored by True Species Labels"". The generated code uses ""K-Means Clustering with PCA"" and ""True Species with PCA"", which are acceptable but not exact matches.
2.  **Color Bar:** The ground truth includes color bars for both plots, which is good practice for interpretability. The generated code omits color bars. While not explicitly stated as a required plot element, it significantly enhances the plot's utility.
3.  **Label Encoding:** For the true `Species` labels, the ground truth uses `sklearn.preprocessing.LabelEncoder`, which is more robust than a hardcoded `species_mapping` dictionary, especially if the dataset's species order might change or if there are more unique species than anticipated.
4.  **Figure Size:** The `figsize` is slightly smaller (8,6 vs 10,7). This is a minor aesthetic point.

Overall, the code is highly functional and achieves the main objectives. The feedback points are mostly about refinement and best practices for visualization and data handling.",gemma-3-27b-it,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# Load the dataset
iris = pd.read_csv(""iris.csv"")

# Create new features
iris[""PetalArea""] = iris[""PetalLengthCm""] * iris[""PetalWidthCm""]
iris[""SepalArea""] = iris[""SepalLengthCm""] * iris[""SepalWidthCm""]

# Define features (X)
X = iris[[""SepalLengthCm"", ""SepalWidthCm"", ""PetalLengthCm"", ""PetalWidt...",10,"The generated code correctly addresses all requirements outlined in the task. It properly loads the dataset, creates the specified new features (`PetalArea` and `SepalArea`), defines `X` to include both original and new features, encodes the `Species` target `y`, initializes and trains a Gradient Boosting Classifier with the given parameters, performs 5-fold cross-validation, and reports the mean accuracy score. The imports are correct and the code structure is clean and readable. The inclusion of `train_test_split` in the imports, while not strictly used, does not detract from the correctness or functionality for this specific task.",gemma-3-27b-it,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import numpy as np

# Load the dataset
data = pd.read_csv(""heart.csv"")

# Define features X and target y
X = data[['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG', '...",8,"The generated code correctly performs all the required tasks: loading data, selecting features, preprocessing (one-hot encoding and scaling), splitting data, training a RandomForestClassifier, and evaluating its performance.

Strong points:
- Correct selection of features and target.
- Proper identification and handling of categorical and numerical features for preprocessing.
- Correct usage of `OneHotEncoder` and `StandardScaler`.
- Correct data splitting with `random_state=42` and `test_size=0.2`.
- Correct instantiation and training of `RandomForestClassifier` with specified parameters.
- Accurate calculation of all required evaluation metrics (accuracy, macro precision, recall, F1, and ROC-AUC).

Areas for improvement:
- The preprocessing steps are applied sequentially and modify `X` in place. While functionally correct, this approach can make the code less robust and harder to maintain compared to using `ColumnTransformer` and `Pipeline` (as seen in the ground truth). A pipeline approach ensures that the same preprocessing steps are applied consistently to both training and test data without manual re-application, and helps prevent data leakage.
- The `stratify=y` parameter was missing in `train_test_split`. While not explicitly stated as a requirement in the prompt, it's good practice for classification tasks, especially with potential class imbalance, to ensure the class distribution is maintained across splits.
- The `sparse_output=False` for `OneHotEncoder` is good, but `encoder.get_feature_names_out` requires exact feature names, which makes the code slightly more brittle if the categorical feature list changes. A `ColumnTransformer` handles this more elegantly.
- No error handling for file loading (e.g., `try-except` for `FileNotFoundError`). While the prompt didn't explicitly ask for it, it's a good practice.",gemma-3-27b-it,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

# Load the dataset
df = pd.read_csv('heart.csv')

# Select numerical features
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']
X = df[features]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Determine the op...",9,"The generated code successfully completes almost all aspects of the task.

**Strong Points:**
*   **Correctness:** The code correctly loads the dataset, selects specific numerical features, standardizes them, and performs K-Means clustering. It also correctly applies PCA for dimensionality reduction and visualizes the results.
*   **K-Selection:** Both the elbow method (WCSS plot) and silhouette scores are calculated and plotted to aid in choosing K, which is a core requirement.
*   **K-Means Parameters:** `n_init='auto'` and `random_state=42` are correctly used for K-Means.
*   **PCA:** PCA is applied correctly with `n_components=2`.
*   **Visualization:** A scatter plot with points colored by cluster labels is generated after PCA, with appropriate labels and title.
*   **Imports:** All necessary libraries are imported.

**Areas for Minor Improvement (preventing a perfect 10):**
*   **Error Handling (Missing File):** The ground truth includes a `try-except` block for `FileNotFoundError` when loading `heart.csv`. While the problem statement says to ""assume the CSV file is in the current directory,"" robust code (and the ground truth's example) would typically include this error handling. The generated code does not.
*   **Plot Titles/Labels for K-Selection:** While the plots for WCSS and silhouette scores have titles and axis labels, the prompt asks for ""the elbow method (plotting Within-Cluster Sum of Squares - WCSS) and silhouette scores to justify your choice of K."" The generated plots just show the curves, without any explicit indication on the plot itself of the chosen K (e.g., a vertical line or arrow, though this is a minor detail). The ground truth's plots also don't explicitly mark the chosen K on the plot, so this is more of a suggestion for a perfect solution, rather than a direct deviation from the prompt's explicit requirements list.
*   **Readability of K-Selection Plots:** The size of the plots for WCSS and silhouette scores could be slightly larger for better readability, and `plt.tight_layout()` would ensure good spacing between plots if they were shown together as in the ground truth.
*   **Legend in Final Plot:** The final PCA plot correctly colors points by cluster, but it lacks a legend to explain which color corresponds to which cluster, which the ground truth includes. This makes the plot less informative.",gemma-3-27b-it,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Create binary target variable
df['good'] = (df['quality'] >= 7).astype(int)
X = df.drop(['quality', 'good'], axis=1)
y = df['good']

# Handle missing values (...",7,"The generated code correctly addresses most of the task requirements: it loads the data, creates the binary target, preprocesses the data, splits it, trains a logistic regression model, and evaluates its performance. It also attempts to report important features.

However, there are several areas for improvement:
1.  **Missing Value Handling:** The code includes a comment about missing values but doesn't implement any explicit check or handling mechanism for them, which was a requirement (""handle missing values if any""). Although this dataset might not have missing values, a robust solution would include the check.
2.  **Scaling before Splitting:** The features `X` are scaled *before* the train-test split (`X = scaler.fit_transform(X)`). This causes data leakage, as the scaler learns parameters (mean and variance) from the entire dataset, including the test set. The correct approach is to `fit` the scaler only on the training data and then `transform` both the training and test data separately (`scaler.fit_transform(X_train)` and `scaler.transform(X_test)`).
3.  **Feature Importance Reporting:**
    *   It prints only the ""Top 5 Most Important Features"" names, not their coefficients or a complete sorted list as requested implicitly by ""Report the most important features according to the model coefficients."" The ground truth provides a `DataFrame` with features, their actual coefficients, and absolute coefficients, which is much more informative.
    *   The `feature_names` are derived from `df.drop(['quality', 'good'], axis=1).columns`, which is correct. However, because `X` was scaled in-place (`X = scaler.fit_transform(X)`), `X` is no longer a Pandas DataFrame, but a NumPy array, losing its column names. When the `LogisticRegression` model is trained, it operates on this array. While `model.coef_[0]` will give coefficients in the correct order, it's generally good practice to keep track of feature names for clarity, especially when reporting importance. The ground truth handles this by converting scaled arrays back to DataFrames for interpretation.
4.  **Reproducibility in Splits:** While `random_state=42` is used for `train_test_split`, `stratify=y` was omitted. Stratification ensures that the proportion of good/bad wines is maintained in both training and test sets, which is particularly important for imbalanced datasets (and this dataset might have a slight imbalance for the 'good' wines).
5.  **Code Output/Readability:** The ground truth includes `print` statements that explain each step, which improves readability and understanding of the code's flow, making it more user-friendly as a script. The generated code is more concise but less informative in its output.

These improvements would elevate the code from a functional solution to a robust, best-practice implementation.",gemma-3-27b-it,LLM_judge_config
H30,hard,Statistical Modeling,,,Error,Failed to generate code,gemma-3-27b-it,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt

# 1. Load data
df = pd.read_csv(""spotify_2023.csv"")

# 2. Convert streams to numeric and handle NaN values
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df.dropna(subset=['streams...",6,"The generated code goes in the right direction but has a critical flaw in its one-hot encoding implementation, leading to incorrect feature concatenation.

Here's a detailed breakdown:

**Strong Points:**
*   **Correctness in parts:**
    *   Loads data correctly.
    *   Handles `streams` conversion to numeric and NaN dropping correctly.
    *   Correctly creates `is_popular` based on the 75th percentile.
    *   Identifies the correct numerical features.
    *   Splits data into training/testing sets with `random_state=42` and correct `test_size`.
    *   Trains a Logistic Regression model with `max_iter=1000`.
    *   Calculates F1-score (macro) correctly.
    *   Displays the confusion matrix correctly.
*   **Code Quality:** Imports are proper. The code is structured well, following the general steps outlined in the task.

**Areas for Improvement / Critical Issues:**

1.  **One-Hot Encoding Flaw:**
    *   The most significant issue is with the `OneHotEncoder` usage. It's applied separately to `df[['key']]` and `df[['mode']]`. This creates two `OneHotEncoder` instances, each fitted independently.
    *   Crucially, `pd.get_dummies` (which is simpler for this task) or a properly applied `OneHotEncoder` on the *entire DataFrame* (or the relevant columns at once) would ensure that the index aligns.
    *   When concatenating `df[features]`, `encoded_key_df`, and `encoded_mode_df`, the `encoded_key_df` and `encoded_mode_df` are new DataFrames created from NumPy arrays. Their indices are reset to 0 to N-1, whereas `df[features]` retains the original DataFrame's index. If the original DataFrame `df` had any dropped rows (due to `streams` NaN), `df[features]` would have a non-contiguous or different index than the newly created one-hot encoded DataFrames. This will lead to misaligned data when `pd.concat` is used, causing incorrect feature-target relationships. The `pd.concat` without `join='inner'` or ensuring index alignment becomes problematic.
    *   The `Ground Truth` code uses `pd.get_dummies` on the `X` DataFrame *after* it's been selected, which is a much more robust way to handle one-hot encoding within a pipeline or for fixed initial feature sets, automatically aligning the features with the rest of the DataFrame.

2.  **ROC AUC Calculation:**
    *   The `roc_auc_score` function expects prediction probabilities for the positive class (i.e., `model.predict_proba(X_test)[:, 1]`), not the binary predictions directly (`y_pred`). The generated code passes `y_pred` to `roc_auc_score`, which is incorrect and will produce an erroneous ROC AUC score or an error depending on the `y_pred` values and `sklearn` version.

3.  **Unnecessary Plotting:** The task explicitly asks *only* for Python code and ""Display the confusion matrix for the test set."" It does not ask for plotting the confusion matrix. While not strictly ""wrong,"" it adds an element that wasn't requested and pulls in `matplotlib`, which wasn't part of the core requirements to solve the machine learning task.

4.  **No `stratify` in `train_test_split`:** The `Ground Truth` code used `stratify=y` for `train_test_split`. While not explicitly required by the prompt, it's a good practice, especially in imbalanced classification problems (which popularity often is), to ensure the class distribution is maintained in both training and testing sets. The prompt didn't explicitly forbid omitting it, but its absence could lead to less reproducible or representative splits if the class distribution is skewed.

The one-hot encoding issue is a major correctness flaw that would lead to a model trained on misaligned data, rendering the results invalid. The incorrect ROC AUC calculation is also a significant error in evaluation.",gemma-3-27b-it,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv(""spotify_2023.csv"")

# Select audio features
features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%', 'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm']
df_selected = df[features]

# Drop rows with NaNs
df_selected = df_selected.dropna()

# Standardize feat...",8,"The generated code correctly performs all the required steps: loading data, selecting features, handling NaNs, standardizing, applying K-Means, performing PCA, plotting the results, and calculating cluster means.

Strong points:
- All required imports are present.
- Uses `n_init='auto'` for KMeans as requested, which is a good practice for modern scikit-learn versions.
- Correctly uses `random_state` for both KMeans and PCA for reproducibility.
- Produces the PCA plot with clusters colored correctly.
- Correctly calculates and displays the mean of original (unstandardized) features per cluster. The critical detail here is that `df_selected` was dropped NaNs and then the cluster labels were added to *this* dataframe, ensuring the mean calculation is on the correct set of rows for which clustering happened.

Areas for improvement (minor):
- The `try-except FileNotFoundError` block for loading the CSV is missing. While not strictly part of the data science logic, it's good practice for robust code.
- The print statements for NaN handling (`Original rows with selected features`, etc.) are missing. These provide useful insight into the data processing.
- The plot title could be more descriptive, similar to the ground truth code (`K-Means Clusters (K=4) on PCA-Reduced Spotify Audio Features`).
- The `principal component 1` and `principal component 2` column names in `df_pca` are lowercase compared to the Ground Truth's `PC1`, `PC2`. This is a minor stylistic difference.
- The `df_selected = df_selected.dropna()` line might be slightly less efficient or clear if large dataframes are being copied implicitly, but for this dataset size, it's generally fine. The ground truth explicitly uses `.copy()` and then `inplace=True` which can be slightly clearer about dataframe modifications. However, the generated code's approach for `dropna()` on `df_selected` which was already a slice, works correctly.

Overall, it's a solid solution achieving all core requirements.",gemma-3-27b-it,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the data
try:
    df = pd.read_csv(""spotify_2023.csv"")
except FileNotFoundError:
    print(""Error: spotify_2023.csv not found. Make sure the file is in the current directory."")
    exit()

# Convert 'streams' to numeric, coercing errors to NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# Filter the data
df_filtered = df[
    (df['streams'] > 10000000) &
    (df['released_year'].between(2010, 2023))
]

# Dro...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
*   **Correct Logic:** It accurately loads the data, converts 'streams' to numeric, filters by `streams` and `released_year` as specified, groups by year, calculates the average audio features, and generates the line plots.
*   **Proper Imports and Structure:** The necessary libraries are imported, and the code follows a logical flow (load, process, plot).
*   **Handling `streams` Conversion:** Correctly uses `pd.to_numeric(errors='coerce')`.
*   **Filtering:** The combined filtering condition `(df['streams'] > 10000000) & (df['released_year'].between(2010, 2023))` is efficient.
*   **Subplots:** Uses `plt.subplot` effectively to display all three plots.
*   **Labels and Titles:** All plots have titles and axis labels.
*   **`tight_layout()`:** Included `plt.tight_layout()` for better plot presentation.

**Areas for Improvement (Why not a 9 or 10):**
*   **Order of Filtering/Dropping NaNs:** The ground truth first drops NaNs from 'streams' then applies the numerical filter. The generated code applies the numerical filter *before* dropping NaNs (`df_filtered = df[(df['streams'] > 10000000) & (df['released_year'].between(2010, 2023))]` then `df_filtered = df_filtered.dropna(subset=['streams'])`). While `pd.to_numeric(errors='coerce')` will turn non-numeric strings into NaN, trying to compare e.g., 'abc' > 10000000 will raise a `TypeError` *before* the `.dropna()` can remove it if `dropna()` wasn't applied sooner or if the non-numeric string wasn't coerced by `pd.to_numeric`. In this specific case, `pd.to_numeric` *does* handle the coercion, so the `> 10000000` comparison will result in `False` for NaN values, and thus these rows would be excluded from the `df_filtered` regardless. However, the explicit instruction was ""Drop rows with NaN `streams` after conversion,"" implying that missing streams are not to contribute to the analysis, and performing `dropna` after initially filtering *might* miss some NaNs that are *not* caught by the numerical comparison (e.g., if NaN was interpreted differently). The ground truth's approach (drop NaNs *then* filter) is logically cleaner and more robust for ""ensure only valid numerical `streams` are considered for subsequent numerical operations.""
*   **Explicitness on Filtered Data:** The prompt specifically requested ""Drop rows with NaN `streams` after conversion"" and then ""Filter for songs with `streams` greater than 10,000,000"". The generated code combines the filtering, and then drops NaNs *after* some of the filtering has already occurred. This order, while possibly yielding the same result for this specific dataset and `pd.to_numeric`'s behavior, isn't strictly sequential to the prompt's instructions.
*   **Minor Plot Customization:** The plots lack markers, colors, or grid lines, which are present in the ground truth and generally make plots more readable and professional. The ground truth also rotated x-axis labels which can be very helpful for busy X-axes with many years.

Overall, it's a solid solution achieving the core task requirements. The improvements are mainly about stricter adherence to the specified sequence of operations and minor plot enhancements.",gemma-3-27b-it,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean Likes, Dislikes, and Video views
for col in ['Likes', 'Dislikes', 'Video views']:
    df[col] = df[col].str.replace(',', '').astype(float)
    df[col] = df[col].fillna(0)

# Treat published ...",8,"The generated code correctly addresses most of the task requirements.

**Strong points:**
*   **Correct Loading and Cleaning:** The code correctly loads the dataset, removes commas from numeric columns, converts them to float, and fills NaNs with 0.
*   **Published Year Handling:** It correctly extracts the year from the 'published' column and treats it as a numeric feature.
*   **Category Handling:** Missing 'Category' values are filled with 'Unknown', and `OneHotEncoder` is used correctly with `handle_unknown='ignore'` and `sparse_output=False`. The concatenation is also handled well.
*   **Feature and Target Definition:** `X` and `y` are correctly defined using the specified columns.
*   **Train-Test Split:** The data split is done correctly with the specified `test_size` and `random_state`.
*   **Model Training and Evaluation:** A `LinearRegression` model is trained, and `R2` and `MAE` are calculated and printed as required.
*   **Imports:** All necessary imports are present.

**Areas for Improvement (to achieve a higher score):**
*   **Robustness in Cleaning:** The `astype(float)` call for `Likes`, `Dislikes`, and `Video views` might fail if there are non-numeric characters other than commas, leading to errors. The ground truth's use of `pd.to_numeric(errors='coerce')` provides more robustness by turning such errors into NaNs, which are then filled. While the prompt implied primarily comma removal, robust parsers are generally better.
*   **Error Handling for File Not Found:** The generated code lacks a `try-except` block for `pd.read_csv`, which is good practice for file operations.
*   **Explicit Column Selection for X:** While the list comprehension `[col for col in df.columns if col.startswith('Category_')]` works, explicitly listing the columns or using `pd.concat` as in the ground truth can sometimes be clearer and less prone to accidental inclusion of other columns if the naming convention changes. However, for this specific problem, the generated approach is perfectly valid.
*   **Conversion to `int` for Cleaned Numeric Columns:** Although `float` is acceptable for regression inputs, the prompt implies ""cleaned `Likes`"", etc., and often these are treated as integers after cleaning, as in the ground truth. `astype(int)` might be preferred if the prompt specifically asked for integer conversion after cleaning. In this case, `float` is not wrong, but `int` might be more aligned with how one typically perceives 'likes' or 'views'.",gemma-3-27b-it,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv(""youtube_trendings.csv"")

# Clean and convert 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '').astype(float)
df['Video views'] = df['Video views'].fillna(0)

# Convert 'published' column to numeric
df['published'] = pd.to_numeric(df['published'], errors='coerce').fillna(0).astype(int)

# Group by year
grouped = df.groupby('published')

# Calculate metrics
yearly_avg_v...",5,"The generated code correctly loads the dataset and attempts to clean the 'Video views' column, but it incorrectly assumes the 'published' column directly contains the year as a numeric value. The dataset typically has a 'publishedAt' column with a full datetime string, requiring extraction of the year. This fundamental misunderstanding of the 'published' column leads to incorrect data processing and plotting. While the plotting structure is generally correct, the data itself will be flawed due to the parsing error. The code also lacks proper error handling for file loading.",gemma-3-27b-it,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
from nltk.corpus import stopwords

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Create title_word_count feature
df['title_word_count'] = df['Video'].apply(lambda x: len(str(x).split()))

# Clean Video views
df['Video views'] = df['Video views'].str.replace(',', '').astype(float)
df['Video views'] = df['Video views'].fillna(0)

# Calculate Pearson correlation
correlation = df['title_word_count'].corr(df['Vi...",7,"The generated code correctly addresses most of the requirements. It loads the data, creates the `title_word_count`, cleans `Video views`, calculates and prints the correlation, filters by year, and finds the most common words.

However, there are a few areas for improvement:
1.  **NLTK download:** The ground truth includes `nltk.data.find` and `nltk.download('stopwords')` to ensure stopwords are available, which is a robustness improvement missing in the generated code. While it might work if already downloaded, it's not guaranteed.
2.  **Error Handling for CSV:** The generated code assumes the CSV file exists. The ground truth includes a `try-except FileNotFoundError` block, which is good practice for robust scripts.
3.  **Column Name Mismatch:** The generated code uses `df['Published']` for parsing the date, while the ground truth correctly uses `df['published']`. This is a critical bug as it will likely result in an error or failed parsing.
4.  **`Video views` conversion to float vs. integer:** The ground truth converts `Video views` to numeric (which in pandas often implies float if NaNs are present) but doesn't explicitly cast to `float`. The generated code explicitly casts to `float`. While `float` works for correlation, views are typically integers. Both are technically correct for the given task (correlation), but `int` would be more semantically appropriate if no decimals are expected.
5.  **Output Formatting:** The output for correlation is just ""Pearson Correlation: value"", while the ground truth specifies ""Pearson correlation between 'title_word_count' and 'Video views': X.XXXX"" and adds separation, which is more user-friendly and informative. Similarly, the common words output is just a raw list of tuples; the ground truth formats it nicely.
6.  **Redundant `len(word) > 0` check:** The ground truth uses `word.isalpha()` which implicitly handles empty strings and removes numbers, making `len(word) > 0` somewhat redundant if `isalpha()` is used carefully. However, just splitting and checking `len(word) > 0` is adequate if `isalpha()` isn't used. The generated code `[^\w\s]` keeps numbers (digit is part of `\w`), so `isalpha()` would be better or `len(word) > 0` is crucial. The current regex `[^\w\s]` allows numbers, so `isalpha()` would indeed be a good addition or improvement for the generated code. The generated code's current approach with `[^\w\s]` and `len(word)>0` means numbers would be included in the word list, which might not be desired for ""words"". The ground truth explicitly uses `word.isalpha()` to prevent numbers from being counted as words. This is a subtle difference but important for text processing.

Addressing the column name mismatch (`Published` vs `published`) is crucial for the code to run correctly. The other points are mainly about robustness, better output formatting, and semantic correctness of text processing.",gemma-3-27b-it,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score
from sklearn.preprocessing import OneHotEncoder

# Load the data
df = pd.read_csv(""mobiles_2025.csv"")

# Clean Launched Price (USA)
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', '').astype(float, errors='coerce')

# Create target variable is_e...",6,"The generated code addresses most of the task requirements but has several issues that affect its correctness and robustness.

**Criticisms and Areas for Improvement:**

1.  **Incorrect `is_expensive_usa` NaN Handling:**
    *   **Ground Truth (GT) Approach:** GT correctly creates `is_expensive_usa` as `1` if price > 1000, `0` if price <= 1000 (and `pd.notna(x)`), and `np.nan` if the original `'Launched Price (USA)'` was unparseable (e.g., `errors='coerce'` resulted in NaN). It then explicitly drops rows where `is_expensive_usa` is NaN. This ensures the target is only defined for valid prices.
    *   **Generated Code (GC) Approach:** `df['is_expensive_usa'] = np.where(df['Launched Price (USA)'] > 1000, 1, 0)`. If `df['Launched Price (USA)']` contains `NaN`s (from `errors='coerce'`), `np.where` treats `NaN > 1000` as False, incorrectly assigning `0` to `is_expensive_usa` for cases where the price couldn't be parsed. This means rows with unparseable prices are included in the dataset and incorrectly marked as ""not expensive"". The subsequent `df.dropna(subset=['is_expensive_usa'], inplace=True)` will then do nothing for these cases as `is_expensive_usa` will never be NaN.
    *   **Fix:** The `np.where` logic needs to be more robust, or a two-step process similar to the GT should be used to first correctly assign NaNs for unparseable prices, then drop them.

2.  **`pd.Series.str.replace`  `regex=False` and `str.strip()`:**
    *   For cleaning columns like `Mobile Weight`, `RAM`, `Battery Capacity`, `Screen Size`, the GT uses `str.replace('g', '', regex=False).str.strip()`. The `strip()` is important to remove any leading/trailing whitespace after removing the unit.
    *   The GC uses `df['Mobile Weight'].str.replace('g').astype(float, errors='coerce')`. Not specifying `regex=False` means it assumes the pattern is a regex, which might not be an issue for simple 'g', 'GB', but it's good practice. More importantly, it misses `str.strip()`, which could leave whitespace and potentially cause `astype(float)` to fail (though `errors='coerce'` would handle it by turning it into `NaN`).

3.  **Camera Feature Extraction:**
    *   **GT:** Uses `re.search(r'(\d+(\.\d+)?)\s*MP', ...)` which correctly and robustly extracts numerical MP values (e.g., ""12 MP"", ""48.5MP""). It also handles the `NaN` cases gracefully by returning 0.0 directly.
    *   **GC:** `extract_mp(text)` uses `text.split('MP')[0]`. This is brittle. If the input is ""12MP dual"", it splits at ""MP"" and takes ""12"". If it's ""12 megapixel"", it won't find ""MP"". If it's just ""MP"", it could break. It also doesn't handle decimal points as effectively and might fail if the number isn't immediately before ""MP"". The `try-except` is a good fallback but indicates the core logic is not robust.

4.  **Incorrect NaN Handling for Features/Target before Split:**
    *   **GT:** Creates `X` and `y` separately, then `pd.concat` them to drop NaNs *consistently* from both `X` and `y` simultaneously, ensuring that `X` and `y` retain their correct row alignment. This is crucial for maintaining data integrity.
    *   **GC:** First does `df.dropna(subset=['is_expensive_usa'], inplace=True)`, which is okay for the target. But then after feature engineering and one-hot encoding, it does `df.dropna(inplace=True)`. This drops NaNs from *all* columns in `df`, including original columns that might not be used as features or target, and crucially it modifies `df` *before* `X` and `y` are definitively defined in their final state. This can lead to inconsistencies if some features become NaN due to cleaning *after* `is_expensive_usa` was checked. More importantly, `X = df[numerical_features + list(encoded_df.columns)]` and `y = df['is_expensive_usa']` are redefined *after* `df.dropna(inplace=True)`. This is problematic if `encoded_df` was created *before* the final `df.dropna`, as `encoded_df` itself might not align perfectly with the now-reduced `df`'s index if `df.dropna` dropped rows. The GT's approach of defining `X` and `y` first, then dropping NaNs *from X and y together* is much safer. The `concat` method applied to `X` and `y` (or `df[all_features + [target_col]]`) before splitting is the most reliable way.

5.  **Placement of `encoder.fit` and `encoder.transform` and `pd.concat`:**
    *   The `encoder.fit` and `transform` logic is applied to `df` *before* the final `df.dropna(inplace=True)`. This `dropna` (which is problematic anyway - see point 4) would then potentially drop rows *after* the one-hot encoding has created the `encoded_df`. If `encoded_df` is created based on the index of the original `df`, but `df` then has rows removed, `encoded_df` might not align correctly with the modified (shorter) `df` when `pd.concat` attempts to join them by index implicitly. Explicitly resetting indices or ensuring all operations (including one-hot encoding) are performed on a `df` that has already had all NaNs dropped is safer. The GT's approach of creating `df_encoded` based on a `df_cleaned` that has *only* had the `is_expensive_usa` NaNs dropped, then defining `X` and `y` and dropping NaNs together is more robust.

**Positive Aspects:**

*   Correct imports and basic structure (loading, cleaning, splitting, training, evaluating).
*   Correct use of `pd.read_csv`, `train_test_split`, `RandomForestClassifier`, `f1_score`, `roc_auc_score`.
*   Attempts to clean all specified features and one-hot encode.
*   Correct `random_state` for reproducibility.
*   Correct evaluation metrics and their parameter `average='macro'`.

The code shows a good understanding of the overall ML pipeline but needs refinement in the data cleaning and NaN handling steps to be truly robust and correct according to the task's specific requirements, especially regarding the `is_expensive_usa` target creation and the final NaN removal before splitting.",gemma-3-27b-it,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

def clean_price(price_str):
    """"""
    Cleans a price string by removing currency symbols and commas,
    and then converts it to a numeric value.
    Returns NaN if the conversion fails.
    """"""
    if isinstance(price_str, str):
        price_str = price_str.replace('PKR', '').replace('INR', '').replace('CNY', '').replace('USD', '').replace('AED', '').replace(',', '')
        try:
            return pd.to_numeric(price_str)
        except Valu...",7,"The generated code correctly addresses most of the task requirements but has some areas for improvement.

**Strengths:**
*   **Correct Imports:** `pandas` and `numpy` are correctly imported.
*   **Price Cleaning Function:** The `clean_price` function correctly handles currency symbols, commas, and attempts conversion to numeric, returning `NaN` on error. It correctly uses `pd.to_numeric` which is robust.
*   **Column Application:** The cleaning function is correctly applied to all specified price columns using a loop and `df[col].apply()`.
*   **Top 5 Identification:** `df.nlargest(5, 'Average Global Price')` is an efficient way to find the top 5 models.
*   **Correct Output:** The `Model Name` and `Average Global Price` are displayed as requested.

**Areas for Improvement:**
*   **`clean_price` function robustness:** The original `clean_price` in the ground truth handles cases where the input is `NaN` initially much better (returning `np.nan` immediately without attempting string operations). The generated code performs `isinstance(price_str, str)` check which works for typical strings but if `NaN` is not of type `float`, it still falls through to the else returns `np.nan` which is okay. However, the `str.replace` chain could fail if `price_str` is a number (e.g., 123.0) and not converted to string first before `replace` calls, though `pd.to_numeric` is generally good at handling such inputs. The `Ground Truth` explicitly converts to `str(price_str)` before `replace` operations, which is safer.
*   **`calculate_average_global_price` efficiency:** The `calculate_average_global_price` function iterates through the DataFrame row by row using `iterrows()`, which is generally less efficient in pandas than vectorized operations. The ground truth leverages `df[price_columns].mean(axis=1, skipna=True)` which is a highly optimized and preferred way to calculate row-wise means while ignoring NaNs. This is a significant performance difference for larger datasets.
*   **Error Handling (File Not Found):** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is good practice for robust code. The generated code lacks this.
*   **Direct calculation of `Average Global Price`:** Instead of creating a `global_price_dict` and then mapping it back, the `Average Global Price` could be calculated directly as a new column, similar to the Ground Truth's `df[price_columns].mean(axis=1, skipna=True)`. This would remove the need for `calculate_average_global_price` function and subsequent mapping.",gemma-3-27b-it,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv(""mobiles_2025.csv"")

# Clean the data
def clean_ram(ram):
    try:
        return float(ram.replace('GB', ''))
    except:
        return pd.NA

def clean_battery(battery):
    try:
        return float(battery.replace('mAh', ''))
    except:
        return pd.NA

def clean_price(price):
    try:
        return float(price.replace('USD ', '').replace(',', ''))
    except:
    ...",8,"The generated code correctly addresses most of the task requirements. It loads the dataset, cleans the specified columns (`RAM`, `Battery Capacity`, `Launched Price (USA)`), handles conversion errors by setting to `pd.NA` (which correctly leads to NaNs and subsequent dropping), filters for Apple products, groups by `Launched Year`, and generates separate box plots for each year for the three metrics. The plots have appropriate titles and y-axis labels.

**Strong points:**
* Correctly implements custom cleaning functions for the specified columns and handles potential errors with `pd.NA`.
* Uses `dropna` on the correct subset of columns.
* Filters for 'Apple' products as required.
* Iterates through year groups to create plots per year.
* Plots are correctly generated using `matplotlib.pyplot` and `seaborn`.
* Titles and y-axis labels are created for each plot.
* `plt.tight_layout()` is used for better subplot spacing.

**Areas for improvement (to achieve a higher score):**
* **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is robust practice. The generated code assumes the file will always be present, which could lead to a crash if it isn't.
* **Consistency in Cleaning:** While the custom functions work, using vectorized `str.replace` and `pd.to_numeric(errors='coerce')` (as in the ground truth) is generally more idiomatic, efficient, and direct for pandas operations, especially on large datasets, reducing the need for explicit loops or `apply` with custom functions. Additionally, the ground truth explicitly handles stripping whitespace, which the custom functions might miss if the input strings have leading/trailing spaces.
* **`Launched Year` Type Conversion:** The ground truth explicitly converts `Launched Year` to numeric and then integer, dropping NaNs before doing so. The generated code implicitly relies on `groupby` casting it correctly, but explicit conversion ensures robustness, especially if `Launched Year` could be non-numeric initially or have missing values that might affect grouping.
* **Professional Plot Style:** The ground truth includes `plt.style.use('seaborn-v0_8-darkgrid')` for aesthetic consistency, which was mentioned in the prompt's implied good practices (""Set a professional style for plots"" in ground truth comment). The generated code doesn't set a specific style.
* **Clarity of `pd.NA` vs. `np.nan`:** While `pd.NA` is pandas' generic missing value indicator, `np.nan` is often used when coercing values to numeric types, and `dropna` works seamlessly with `np.nan`. Both are correct for this context.

Overall, the generated code provides a very good and functional solution to the problem. The suggested improvements are primarily about robustness, best practices, and marginal code efficiency rather than fundamental correctness issues.",gemma-3-27b-it,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder
from sklearn.impute import IterativeImputer, KNNImputer, SimpleImputer
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neural_netw...",6,"The generated code attempts to address many aspects of the prompt but falls short in several areas, particularly concerning the `advanced` nature of the request and adherence to best practices in a complex data science pipeline.

**Correctness:**
- **Preprocessing:** The preprocessing in the generated code is problematic. It tries to integrate polynomial features and scaling directly into a function that processes the DataFrame *before* the train-test split. This leads to data leakage (scaling and polynomial features are fitted on the entire dataset) and makes it impossible to apply the same transformations consistently to new data or within a `Pipeline`.
- **`IterativeImputer`:** It imputes numeric columns using `IterativeImputer` *before* the train-test split, which is a significant data leakage issue.
- **`LabelEncoder`:** Using `LabelEncoder` can introduce ordinality that isn't present in the data, which is generally not suitable for nominal categorical features. `OneHotEncoder` or `OrdinalEncoder` (if appropriate) should be used.
- **`random_state` in `SVC` and `MLPClassifier` params:** These models do not accept `random_state` as a parameter during initialization like this in `build_and_tune_model` setup. It should be set once on the model object, not as a parameter to be tuned.
- **`make_pipeline`:** While `make_pipeline` is imported, it's not actually used for the preprocessing steps or model building, which complicates the overall flow and consistency.
- **Feature Selection:** The `evaluate_feature_selection` function is poorly structured. It performs feature selection and then tries to evaluate an *unspecified* `model` on the transformed data. The `model` passed to it is not fitted *after* feature selection, which renders the evaluation incorrect. Also, it uses `X_train[:, rfe.get_support()]` without converting `X_train` to a numpy array, which would cause an error if `X_train` remains a pandas DataFrame. This part would likely fail.
- **Learning/Validation Curves:** The import `from sklearn.learning_curve import learning_curve, validation_curve` is deprecated. They should be imported from `sklearn.model_selection`.
- **SHAP/LIME:** The code explicitly states that SHAP/LIME are ""Not fully implemented due to complexity"", which directly violates the prompt's requirement.

**Code Quality:**
- **No `ColumnTransformer`:** The lack of `ColumnTransformer` makes the preprocessing less robust and harder to manage for different column types.
- **Global Variables/Functions:** Preprocessing is done via a global function applied before splitting, hindering a proper pipeline structure.
- **Missing `warnings` and `re` imports:** The prompt asked for specific imports, some of which are missing (e.g., `re` for name processing, `warnings` for suppression).
- **Docstrings/Comments:** Lacks comprehensive comments or docstrings, making it harder to understand the intent of complex sections.
- **Consistency:** Uses `GridSearchCV` only, while the prompt explicitly mentioned `RandomizedSearchCV` too.

**Completeness:**
- **Advanced Feature Engineering:** While it attempts polynomial features and some name parsing, it's not as extensive as ""text mining from names"" (e.g., length, unique titles handled robustly, cabin handling) and lacks interaction terms beyond basic polynomial.
- **Multiple Imputation:** `IterativeImputer` is used, but the prompt asked for ""multiple imputation strategies,"" implying consideration or comparison of different ones (e.g., KNN imputer, simple imputer, and comparing their impact). The current implementation only uses `IterativeImputer` for numeric and `SimpleImputer` for categorical.
- **Ensemble Strategies:** Stacking is mentioned but not fully implemented, which is a major miss for an ""advanced ensemble"" requirement.
- **Feature Selection Impact Comparison:** The feature selection section is incomplete and does not truly compare the impact of different techniques clearly, nor does it integrate them into trainable pipelines for evaluation.
- **Extensive Model Evaluation:**
    - **Stratified k-fold cross-validation:** Used in `build_and_tune_model`, but not explicitly for final ensemble models or in a comparative way across all models *after* tuning, as requested by ""extensive model evaluation.""
    - **Learning/Validation Curves:** Only learning curve is plotted. Validation curve is mentioned but not implemented.
    - **Performance Metrics:** While metrics are defined, ""statistical significance tests"" are ""simplified"" or missing beyond a basic comparison.
- **Model Interpretability:** SHAP/LIME are mentioned but explicitly noted as not fully implemented, which is a major unmet requirement.
- **Missing Imports:** `xgboost`, `seaborn` and `scipy.stats` are not imported or used.
- **`random_state` handling:** Not consistently applied for reproducibility across all sklearn components (e.g., `train_test_split` has it, but many model initializations don't specify it, or it's incorrectly included in `params` for `GridSearchCV`).

Overall, the code has good intentions but fails to deliver on the ""advanced"" and ""comprehensive"" aspects of the prompt due to flawed pipeline design, incomplete implementations, and outright missing functionality (especially model interpretability and comprehensive evaluation of feature selection).",gemma-3-27b-it,LLM_judge_config
H40,hard,Regression,,,Error,Failed to generate code,gemma-3-27b-it,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Select features for clustering
features = df[['Rating', 'Runtime']]

# Standardize the features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
df['Cluster'] = kmeans.fit_predic...",6,"The generated code correctly loads the data, selects features, standardizes them, applies K-Means with the correct parameters, and creates a scatter plot with cluster coloring. It also attempts to calculate and print mean ratings and runtimes per cluster, which is a good start.

However, there are critical shortcomings:
1.  **Data Cleaning:** The most significant issue is the **complete lack of data cleaning for the 'Runtime' column**. The 'Runtime' column in the `imdb_top_250.csv` dataset is typically in a string format (e.g., ""142 min""). The generated code attempts to use it directly in `pd.read_csv`, `df[['Rating', 'Runtime']]`, and `plt.scatter`, which will lead to a `TypeError` or `ValueError` because 'Runtime' cannot be converted to a numeric type for `StandardScaler` or plotting without preprocessing. This makes the code non-functional as-is for the given dataset.
2.  **Robustness (File Handling):** The code does not include any error handling for `FileNotFoundError` when loading the CSV, unlike the ground truth.
3.  **Completeness (Output Formatting):** While it calculates means, the ground truth's `groupby` approach is more concise and robust for this aggregation. The loop for printing means is functional but slightly less elegant.
4.  **Plot Enhancements:** The scatter plot lacks a color bar and grid, which were present in the ground truth and improve readability.

To improve, the code needs to:
*   Add robust data cleaning for the 'Runtime' column (e.g., remove "" min"" and convert to numeric, handling errors).
*   Add `try-except` block for `pd.read_csv`.
*   Improve the calculation and presentation of cluster means (e.g., using `groupby`).
*   Add a color bar to the scatter plot.",gemma-3-27b-it,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter

nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create the title_length feature
df['title_length'] = df['Title'].str.len()

# Calculate Pearson correlation
correlation = df['title_length'].corr(df['Rating'])
print(f""Pearson correlation between title_l...",7,"The generated code correctly addresses most of the requirements. It loads the dataset, creates the `title_length` feature, calculates correlation, preprocesses titles (lowercase, remove punctuation, tokenize, remove stopwords), and finds the 10 most common words.

However, there are a few areas for improvement:
1.  **Punctuation Removal Robustness**: The `c.isalnum() or c.isspace()` check for punctuation removal is less robust than regular expressions (`re`) for handling a wider range of non-alphanumeric characters, including international characters or more obscure symbols that `isalnum()` might miss or handle differently than intended for ""punctuation removal"". The prompt also explicitly mentioned ""characters that are not alphanumeric or whitespace,"" which `re.sub(r'[^a-z0-9\s]', '', text)` explicitly handles.
2.  **Modularity of Preprocessing**: While functional, placing all preprocessing steps (lowercase, punctuation, tokenize, stopwords) inside a single `preprocess_title` function makes it less modular for debugging or if one needed to inspect intermediate steps. The ground truth code breaks these down into separate steps for clarity and potentially better use of pandas' vectorized operations.
3.  **Correlation Calculation**: The generated code calculates `df['title_length'].corr(df['Rating'])` without explicitly handling potential `NaN` values in the 'Rating' column or ensuring 'Rating' is numeric. The ground truth explicitly uses `pd.to_numeric` with `errors='coerce'` and `dropna` on the relevant columns, which is a more robust approach for real-world data science tasks. While the prompt assumes clean data to some extent, defensive programming here is good practice.
4.  **Print Formatting for Correlation**: The prompt asks for `correlation:.4f` for specific formatting. The generated code prints `correlation` without specific formatting.
5.  **`string` import**: The generated code imports `string` but doesn't use it, which is a minor point but indicates a non-essential import. (The ground truth imports it because the prompt explicitly asked for it, but then used `re` because it's superior).

Overall, it's a good attempt that gets the core logic right but could benefit from more robust data handling and slightly better modularity.",gemma-3-27b-it,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer

# Load the dataset
df = pd.read_csv('penguins.csv')

# Define features X and target y
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']]
y = df['species']

# Impute missing value...",6,"The generated code addresses most of the requirements but has several issues that affect its correctness and robustness, particularly concerning imputation and data alignment.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly defines X and y.
*   Uses `SimpleImputer` for numerical median imputation, which is a good practice.
*   Correctly identifies and performs mode imputation for categorical features.
*   Correctly applies `StandardScaler` to numerical features (though the dynamic identification of numerical columns has a flaw after one-hot encoding).
*   Correctly performs one-hot encoding.
*   Correctly uses `LabelEncoder` for the target variable.
*   Correctly initializes and uses `RandomForestClassifier`.
*   Correctly performs 5-fold cross-validation and prints the mean accuracy.

**Areas for Improvement / Issues:**

1.  **Handling Missing Target Values:** The ground truth explicitly handles `nan` values in the `species` column. The generated code does not. While typically `species` doesn't have missing values in this dataset, a robust solution should account for it, as `LabelEncoder` will likely fail if `y` contains `nan`.
2.  **Imputation of Categorical Features (Island specific):**
    *   The requirement states: ""if a row still has NA in `island` after mode imputation, e.g. if all were NA, it can be dropped, or a placeholder like 'Unknown' can be used if mode is NA itself. For this task, if mode imputation results in NA for island, drop those rows.""
    *   The generated code's check `if pd.isna(X['island'].mode()[0]):` is flawed. The `mode()` function on a Series will return an empty Series or a Series with just the mode if all values are NaN. If `X['island']` was entirely NaN before imputation, `X['island'].mode()` would still be `NaN` after `fillna(NaN)` because the mode *is* NaN. However, if `X['island']` was entirely NaN, `mode()` would return an empty Series, and `[0]` would raise an `IndexError`. If it *wasn't* entirely NaN, and mode imputation filled it with a non-NaN value, then `X['island'].mode()[0]` will not be NaN.
    *   The ground truth handles the specific case where the entire `island` column *might* be NA, explicitly dropping rows if mode imputation isn't possible, which is more robust. The current `if pd.isna(X['island'].mode()[0])` check is unlikely to trigger correctly for the intended scenario (i.e., when mode imputation effectively results in NAs *because the mode itself was NA, or if the entire column was NA*).
3.  **Order of Operations and Data Alignment (Pre-processing with `X` and `y`):**
    *   The generated code imputes `X`, then performs one-hot encoding, and then scales. This is generally fine.
    *   However, if rows were dropped due to `island` imputation (even with the flawed logic), `X` would be modified, but `y` would *not* be aligned. This is a critical error as cross-validation will fail or produce incorrect results due to mismatched `X` and `y` sizes or row order. This is a significant flaw compared to the ground truth which explicitly maintains `y` alignment when dropping rows from `X`.
4.  **Scaling Numerical Features - Dynamic Column Identification:**
    *   `numerical_cols = [col for col in X.columns if X[col].dtype == 'float64']` is problematic after one-hot encoding. While many original numerical columns are float64, the one-hot encoded columns are typically `uint8` or `bool`. If an original numerical column was integer but contained NaNs (which are then imputed to float), it would become float64. The approach of re-identifying float64 columns *after* one-hot encoding is less precise and could potentially miss some numerical columns if they remained integer type (e.g., if they had no NaNs and were integers to begin with). It's better to stick to the original `numerical_cols` definition as the ground truth does, applying scaling only to those specified features.
5.  **`drop_first=True` in `pd.get_dummies`:** The requirement doesn't specify `drop_first=True` or `False`. While `drop_first=True` is common to avoid multicollinearity, the ground truth uses `drop_first=False`. For a benchmark where direct comparison is made, it's a minor deviation but worth noting. It doesn't violate the *spirit* of the requirement but differs from the ground truth implementation.
6.  **Redundant `numerical_cols` definition:** `numerical_cols` is defined twice. While not an error, it's less clean code.

Overall, the core sequence of operations is present, but the details in handling missing values in specific edge cases (especially `island`) and ensuring data alignment when dropping rows are not robustly implemented, leading to potential failures or incorrect results on slightly varied datasets. The `StandardScaler` application method is also less robust than the ground truth.",gemma-3-27b-it,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Select features
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df = df[features]

# Drop rows with missing values
df = df.dropna()

# Standardize features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(...",5,"The generated code attempts to address the task but fails significantly on several key requirements, particularly regarding data handling and the second plot.

1.  **Incorrect Data Handling for Species Plot:** The most critical error is how the original `species` labels are handled for the second plot.
    *   The original `species` column is not carried through the initial data cleaning and processing (`df = df[features]`, `df = df.dropna()`). This means that when `kmeans` is applied and `df['cluster']` is assigned, `df` no longer contains the `species` column from the original dataset.
    *   For the second plot, the code re-loads the `penguins.csv` and re-applies the feature selection and `dropna()`. However, the line `c=original_df['species']` will fail because the `species` column is dropped by `original_df = original_df[features]`. Even if `species` *was* correctly kept, the `original_df` after `dropna()` would have a different index than the `df_pca_species` created from `principal_components_species`, leading to misalignment if not handled carefully (e.g., by resetting indices or ensuring all transformations happen on a synchronized dataframe).
    *   Essentially, the second plot will either raise an error (if `species` column is missing) or will plot incorrect species labels if it somehow manages to execute because the data used for PCA for the species plot is re-processed, not the *same* data that was used for clustering.

2.  **Missing `species` column in first `df` processing:** The initial processing `df = df[features]` discards the `species` column prematurely. This means `df` later only contains the numeric features and the new `cluster` column, but not `species`. To correctly align species for the second plot *after* the NA removal, the `species` column must be retained throughout the initial cleaning and processing steps.

3.  **Redundant and Incorrect PCA for Species Plot:** The code performs PCA *twice* on potentially different subsets of the data (`scaled_features` vs `scaled_original`). The task implicitly requires both plots to use the *same* PCA results (from the *same* set of clean, scaled features) to ensure comparability, just differing in the coloring variable. The ground truth correctly performs PCA once on the cleaned and scaled data, and then uses that single set of PCA components for both plots.

4.  **No `sns` for plotting:** While `matplotlib.pyplot` is sufficient, the ground truth uses `seaborn` which often produces more aesthetically pleasing plots with less code. This is a minor point for correctness but a general good practice in data science.

5.  **Lack of `try-except` for file loading:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice.

The code's inability to correctly generate the second scatter plot based on original species labels, stemming from fundamental data management issues (dropping `species` too early and reprocessing data for PCA), leads to a low score.",gemma-3-27b-it,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.ensemble import GradientBoostingClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv(""penguins.csv"")

# Impute missing values
df['culmen_length_mm'].fillna(df['culmen_length_mm'].mean(), inplace=True)
df['culmen_depth_mm'].fillna(df['culmen_depth_mm'].mean(), inplace=True)
df['flipper_length_mm'].fillna(df['flipper_length_mm'...",7,"The generated code correctly addresses most of the requirements but has a few shortcomings that prevent it from achieving a higher score.

**Strong Points:**
*   Correctly loads the dataset.
*   Correctly imputes numerical columns with the mean and 'sex' with the mode.
*   Correctly drops rows where 'island' is NA.
*   Correctly one-hot encodes 'island' and 'sex' and combines them with numerical features to form `X`.
*   Correctly label encodes 'species' to form `y`.
*   Successfully trains a `GradientBoostingClassifier`.
*   Extracts and prints feature importances.
*   Generates box plots for `flipper_length_mm` for each species by `sex`.

**Areas for Improvement / Shortcomings:**

1.  **Feature Importance Printing:** While it prints feature importances, the way it handles naming for one-hot encoded features is not ideal. It uses `f'island_{i}'` and `f'sex_{i}'`, which are not semantically meaningful. The ground truth correctly retrieves the actual category names from the `OneHotEncoder` for better interpretability (or uses `pd.get_dummies` which automatically assigns meaningful names). This makes understanding which specific island or sex category is important difficult.
2.  **Box Plot Grouping:** The task asked for ""For each `species`, create box plots to visualize the distribution of `flipper_length_mm` (using imputed data) grouped by the imputed `sex` column."" The generated code creates *separate* plots for each species, where each plot shows `flipper_length_mm` vs `sex`. The ground truth uses `hue='sex'` within a single plot for all species, which provides a more direct comparison across species on the same axis for the `sex` grouping. While the generated code's approach is not entirely incorrect, it makes the comparison across species harder than the requested grouping implies.
3.  **Efficiency of OneHotEncoder:** Using `OneHotEncoder` directly on the columns `df[['island']]` and `df[['sex']]` and then creating new DataFrames `pd.DataFrame(X_island, ...)` with generic column names `island_0`, `island_1`, etc., is less efficient and readable compared to `pd.get_dummies()`, which automatically handles prefixing and generates meaningful column names (e.g., `island_Biscoe`, `sex_Male`). The generated code then needs to manually reconstruct the column names, which it does poorly for feature importance as mentioned above.
4.  **Redundant Imports:** `numpy` is imported but not explicitly used.
5.  **Plotting Structure:** The generated code opens a new figure for each species. While it works, plotting all species on a single plot with `hue='sex'` as the ground truth does often provides a better overview and adheres more closely to typical data visualization practices for such comparisons.

The main critical missing piece is the meaningful feature names for one-hot encoded columns, which directly impacts the interpretability of the feature importance output. The plotting aspect is a matter of better visualization choice rather than outright incorrectness, but it deviates from what a more direct interpretation of ""grouped by"" suggests.",gemma-3-27b-it,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer

# Load the dataset
df = pd.read_csv('penguins.csv')

# Impute numerical columns using KNN
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
imputer = KNNImputer(n_neighbors=3)
df[numerical_cols] = imputer.fit_transform(df[numerical_cols])

# Impute categorical column 'sex' with the mode
df['sex'] = df['sex'].fillna(df['sex'].mode()[0])

# Print the number of remaining NA values in each...",8,"The generated code correctly implements all core requirements of the task. It loads the dataset, applies KNN imputation to the specified numerical columns with `n_neighbors=3`, and imputes the 'sex' column with its mode. Finally, it prints the sum of NA values to verify.

**Strong points:**
*   Correctly uses `KNNImputer` for numerical columns.
*   Correctly uses `fillna` with `mode()[0]` for the categorical column.
*   Clearly addresses all parts of the prompt.
*   Concise and readable.

**Areas for improvement (to reach a perfect 10):**
*   **Error Handling:** The code doesn't include robust error handling for `FileNotFoundError` when loading the CSV, which the ground truth gracefully handles. This is a common practice in production-ready code.
*   **Verification Output:** While it prints `df.isnull().sum()`, adding explicit print statements for verification of the specific imputed columns (as seen in the ground truth) would make the output clearer and directly address the ""verify no NAs remain in these columns"" aspect of the prompt.
*   **Comments/Docstrings:** A few more comments explaining the steps would enhance readability for complex tasks, though this particular task is quite straightforward.",gemma-3-27b-it,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
try:
    df = pd.read_csv(""penguins.csv"")
except FileNotFoundError:
    print(""Error: penguins.csv not found. Make sure the file is in the current directory."")
    exit()

# Filter for Adelie and Gentoo species
df_filtered = df[df['species'].isin(['Adelie', 'Gentoo'])]

# Handle NA values in culmen_length_mm
df_filtered = df_filtered.dropna(subset=['culmen_length_mm'])

# Separate culmen lengths for Adelie and Gentoo pengu...",9,"The generated code correctly addresses all the requirements of the task. It loads the data, filters for the specified species, handles NA values, performs the t-test, and interprets the results based on the p-value and significance level. The code is concise and directly implements the requested steps.

**Strong points:**
*   Correctly loads the dataset and handles `FileNotFoundError`.
*   Accurately filters the DataFrame for 'Adelie' and 'Gentoo' species.
*   Correctly drops NA values in `culmen_length_mm` for the filtered data.
*   Performs the independent two-sample t-test using `scipy.stats.ttest_ind`.
*   Correctly prints the t-statistic and p-value.
*   Correctly interprets the p-value against the 0.05 significance level and states the conclusion regarding the null hypothesis.

**Areas for minor improvement (why it's not a perfect 10, but still very good):**
*   The ground truth code implemented the solution within a function, making it reusable and more modular. While not explicitly required, it's good practice for such tasks.
*   The ground truth also included more descriptive print statements (e.g., shape after filtering, number of rows dropped), which can be helpful for debugging and understanding the data flow.
*   The ground truth explicitly set `equal_var=True` in `ttest_ind`, which is the default but good for clarity in statistical tests if that assumption is intended.
Overall, the generated code performs the core task exceptionally well and meets all specified direct requirements.",gemma-3-27b-it,LLM_judge_config
H48,hard,Classification,,,Error,Failed to generate code,gemma-3-27b-it,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Calculate Pearson correlation coefficients
correlation_...",9,"The generated code correctly performs all the required tasks: it defines the DataFrame, calculates the specified Pearson correlation coefficients, prints them, and correctly identifies the feature with the stronger absolute correlation.

Strengths:
- Correct DataFrame creation.
- Correct calculation of Pearson correlation coefficients using `df.corr()`. Note that `method='pearson'` is the default, so explicitly stating it is not strictly necessary but good for clarity. The generated code omits it, which is fine since it's the default.
- Correct printing of correlation coefficients.
- Correct logic for determining the stronger absolute correlation using `abs()`.

Area for Minor Improvement:
- The `else` block in the stronger correlation determination could be slightly refined to handle the edge case where the absolute correlations are exactly equal. The current code defaults to `TrainingHours` in this specific scenario, whereas an ideal solution would state that they are equal. Given the nature of floating-point arithmetic, exact equality is rare, but good practice dictates handling it. However, for this task, it's a very minor point.

Overall, the code is very clean, concise, and effectively solves the problem.",gemma-3-27b-it,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
from lifelines import KaplanMeierFitter
from lifelines.models import CoxPHModel
from scipy import stats
import statsmodels.formula.api as smf
import pymc3 as pm
import numpy as np
import matplotlib.pyplot as plt
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from scipy.stats import norm

# Load the dataset
try:
    df = pd.read_csv(""titanic.csv"")
except FileNotFoundError:
    print(""Error: titanic.csv not found....",4,"The generated code attempts to address several aspects of the prompt but falls short in correctness, completeness, and statistical rigor, particularly concerning the survival analysis components.

**Critical Issues and Missing Components:**

1.  **Fundamental Misunderstanding of Kaplan-Meier and CoxPH for ""voyage as time-to-event""**: The prompt explicitly stated to treat ""voyage as time-to-event data (with survival as event indicator)"". This implies a *fixed time* for all observations, and the event is whether they survived or not at the end of that fixed time. The ground truth correctly interprets this by setting `T = np.ones(len(df_encoded))` and `E = 1 - df_encoded['Survived']`.
    *   **Generated code's error**: The generated code uses `df['Age']` as `durations` for Kaplan-Meier and `duration_col` for CoxPH. This is fundamentally incorrect. Age is a covariate, not the time spent on the voyage. This completely invalidates the survival analysis.
    *   **Impact**: Because of this, Kaplan-Meier curves are plotted for Age, which isn't survival time, and CoxPH models `Survived` based on `Age` as time, leading to nonsensical results regarding the task's survival analysis objective.

2.  **Data Preprocessing and Handling Missing Values**:
    *   **Generated code's error**: `df = df.dropna()` is a highly aggressive and often inappropriate way to handle missing data for analysis, especially when `Age` (a crucial variable) has NaNs. It will significantly reduce the dataset size and introduce bias. The ground truth correctly imputes `Age` and `Embarked`.
    *   The conversion of `Embarked` to {0, 1, 2} is ordinal, but `Embarked` is a nominal categorical variable. One-hot encoding (as done in the ground truth) is more appropriate for statistical models.

3.  **Kaplan-Meier Grouping**: The prompt asked for Kaplan-Meier analysis ""with survival as event indicator"". The ground truth then expands this to show KM curves by `Sex` and `Pclass`. The generated code only provides one overall KM curve, which uses `Age` as time, completely missing the point.

4.  **Propensity Score Matching (PSM)**:
    *   **Incorrect `treatment` variable**: The generated code uses `'Survived'` as the `treatment` variable in PSM, and `'Sex', 'Age', 'Embarked'` as covariates. PSM is used to estimate the effect of a specific *treatment* (e.g., being in Pclass 1 vs Pclass 3) on an *outcome* (e.g., survival). `'Survived'` is the *outcome*, not the treatment. The ground truth correctly sets `Treatment = (df_psm['Pclass'] == 1).astype(int)`.
    *   **Simplified Matching**: The implementation `df = df.sort_values('propensity_score')` and `matched_df = df.copy()` does not perform *matching* (e.g., 1-to-1 matching, caliper matching). It just sorts the data by propensity score. This is not what PSM entails.
    *   **Missing Analysis**: No balance checks (like SMD) or analysis of treatment effect after matching are performed as requested.

5.  **Bootstrap Resampling**:
    *   The bootstrap for CoxPH is performed on the *incorrectly defined* CoxPH model (due to `Age` as duration). While the mechanics of bootstrapping are there, the results are meaningless due to the underlying model error.
    *   Only covers CoxPH, not other components like treatment effects from PSM if it were correctly implemented.

6.  **Bayesian Analysis (PyMC3)**:
    *   **Model Specification**: The Bayesian model is for logistic regression of `Survived` based on `Pclass`, `Sex`, `Age`, `Embarked`. This is a binary classification model, not a survival analysis model using PyMC3. While logistic regression is a valid statistical model, the prompt also asked for ""probabilistic modeling of survival factors"", which often implies Bayesian versions of survival models (e.g., Weibull or CoxPH in PyMC3). The ground truth's approach (logistic regression of `Survived`) is acceptable, but the generated code's input data for the model (`df['Pclass']` etc.) is directly used without proper processing/standardization, which can cause sampling issues.
    *   The use of `df['Pclass']` directly in the linear predictor without one-hot encoding means it's treated as a continuous variable, which is incorrect for `Pclass`.
    *   Missing `arviz` imports and usage for better diagnostics.

7.  **Advanced Statistical Tests and Model Diagnostics**:
    *   **Multivariate Analysis/Interaction Effects**: Uses `statsmodels.formula.api.glm` with correct syntax, which is good.
    *   **Model Diagnostics for CoxPH**: `cph.check_proportional_hazards()` is called, but given the incorrect `Age` duration, its output is irrelevant.
    *   **Multicollinearity**: Providing `df.corr()` is a very basic check for multicollinearity, but not a full diagnostic or solution. More robust methods (e.g., VIF) are typically used.
    *   **Missing Advanced Diagnostics**: ""Model diagnostics"" in survival analysis context typically includes checking proportional hazards assumption (beyond just calling a function, interpreting the results), linearity, influential observations, etc. The provided check is superficial at best given the `Age` issue.

8.  **Model Assumptions and Sensitivity Analysis**:
    *   The `SchoenfeldResiduals` check is attempted, but again, on the incorrect model setup.
    *   The conceptual point about changing missing value handling is mentioned, but no implementation is provided, unlike the ground truth which implements it for `Age` imputation.
    *   No sensitivity analysis for Bayesian priors, different model specifications (beyond adding interactions), etc., as requested.

In summary, the generated code demonstrates some familiarity with the libraries but fundamentally misinterprets the core survival analysis task (time-to-event definition) and implements several advanced techniques (PSM, Bayesian) incorrectly or superficially with respect to the prompt's requirements.",gemma-3-27b-it,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import pearsonr
import seaborn as sns

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85...",10,"The generated code correctly addresses all parts of the task. It successfully creates the DataFrame, calculates and prints the Pearson correlation coefficient, generates a clear scatter plot colored by subject with appropriate labels and title, correctly assigns grades based on the specified criteria, and prints the distribution of these grades. The imports are correct and the code structure is logical and readable. The use of `seaborn` for the scatter plot's hue parameter is a good choice for automatic legend generation and aesthetics. The `apply` method for assigning grades is also a clean and effective approach.",gemma-3-27b-it,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']),
        'Ticker': ['TECHCO']*10,
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0],
        'Low': [149.5, 150.5, 1...",9,"The generated code is largely correct and addresses most of the requirements.

**Strong Points:**
- Correctly creates the DataFrame and sets 'Date' as the index.
- Accurately calculates daily percentage returns.
- Correctly calculates both 3-day and 5-day Simple Moving Averages.
- Generates a plot with all required lines ('Close', 'SMA_3Day', 'SMA_5Day'), title, labels, legend, and grid.
- Identifies bullish crossovers using the correct logic (current SMA_3Day > SMA_5Day AND previous SMA_3Day <= SMA_5Day).

**Areas for Improvement:**
- **Plot Labels:** The legend labels for the SMAs are ""3-Day SMA"" and ""5-Day SMA"" in the generated code, while the ground truth and prompt imply using ""SMA_3Day"" and ""SMA_5Day"" which align with the column names. This is a minor aesthetic point, but consistency is good.
- **Boolean Indexing Parentheses:** In the bullish signal identification line: `bullish_signals = df[df['SMA_3Day'] > df['SMA_5Day'] & (df['SMA_3Day'].shift(1) <= df['SMA_5Day'].shift(1))]`
The `&` operator has higher precedence than `>` or `<=`. This means `df['SMA_5Day'] & (df['SMA_3Day'].shift(1) <= df['SMA_5Day'].shift(1))` would be evaluated first, leading to a `TypeError` because you're trying to perform a bitwise AND on a Series of floats/NaT and a boolean Series. The correct way to combine boolean Series in pandas is to wrap each condition in parentheses: `(df['SMA_3Day'] > df['SMA_5Day']) & (df['SMA_3Day'].shift(1) <= df['SMA_5Day'].shift(1))`.
Because of this `TypeError`, the bullish crossovers would not be correctly identified and the program execution would halt. This is a critical error in the logic for identifying crossovers.
- **Output of Bullish Signals:** The generated code prints `bullish_signals.index`, which would be a `DatetimeIndex` object. The ground truth (and a more user-friendly output) formats each date. This is a minor point related to output formatting.

Fixing the boolean indexing for the bullish signals would raise the score to a 10.",gemma-3-27b-it,LLM_judge_config
H52,hard,Data Visualization & Analysis,,,Error,Failed to generate code,gemma-3-27b-it,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Create DeviceType_Numeric
df[""DeviceType_Numeric""] = df[""DeviceType""].map({""Desktop"": 0, ""Mobile"": 1})

# Create PagesPerMinu...",8,"The generated code correctly performs most of the tasks.

**Strong Points:**
- Correctly initializes the DataFrame.
- Correctly creates the `DeviceType_Numeric` column using `map`.
- Correctly calculates `PagesPerMinute`.
- Correctly selects and prints the head of the final feature DataFrame.

**Areas for Improvement / Minor Issues:**
- **Division by Zero Handling:** The handling of division by zero for `PagesPerMinute` (specifically `df[""PagesPerMinute""] = df[""PagesPerMinute""].replace([float('inf'), float('nan')], 0)`) is slightly incomplete.
    - `float('nan')` will only replace existing NaNs. It won't catch NaNs that result from 0/0.
    - `float('inf')` will replace positive infinity, but it won't catch negative infinity (`-np.inf`), which could theoretically arise from other contexts (though not from `x/0` where `x` is positive as in this dataset).
    - The ground truth's approach of `replace([np.inf, -np.inf], np.nan, inplace=True)` followed by `fillna(0, inplace=True)` is more robust as it covers positive/negative infinities and then any NaNs (including those from 0/0 or converted infinities) uniformly. While the given data won't produce negative infinities, a truly robust solution should consider them.
    - For 0/0, pandas typically produces `NaN`. For `x/0` where `x !=0`, pandas produces `inf` or `-inf`. The generated code's `replace` call would effectively handle the `inf` case and any `NaN` that might have already been there, but it's less explicit about handling *all* forms of problematic division results.

Despite this minor robustness issue in NaN/Inf handling, the code effectively solves the problem for the given dataset and is quite concise. The missing `import numpy as np` from the ground truth is not strictly necessary here because `np.nan` and `np.inf` are not used in the exact same way, but using `float('nan')` and `float('inf')` is acceptable.",gemma-3-27b-it,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26, 45, 28]
}
df = pd.DataFrame(data)
...",9,"The generated code correctly defines the DataFrame, calculates the mean and standard deviation for 'TemperatureCelsius', identifies outliers based on the 1.5 standard deviation rule using the `abs()` function which correctly accounts for both upper and lower bounds, and prints the required information for each outlier.

**Strong points:**
*   Correctly defines the DataFrame as specified.
*   Accurately calculates mean and standard deviation.
*   Correctly identifies outliers using the `abs()` function, which implicitly checks for deviations both above and below the mean.
*   Correctly iterates and prints the SensorID, Timestamp, and TemperatureCelsius for each outlier.

**Minor areas for improvement (to reach a perfect 10):**
*   The output format for printing outliers is slightly different from the ground truth. While it prints the requested information, the formatting (e.g., adding "" (Outlier)"" or specifying a float format) could be improved for clarity and consistency with a more robust output.
*   It doesn't handle the case where no outliers are found; it would simply print nothing. The ground truth includes a check for an empty `outliers_df` and prints a message indicating no outliers were found.
*   The `Timestamp` column is not converted to datetime objects. While not strictly necessary for this specific outlier detection, it's good practice for time-series data and was included in the ground truth as an optional but beneficial step.",gemma-3-27b-it,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [10, 5, 25, 15, 8, 30, 9,...",10,"The generated code correctly defines the DataFrame, accurately calculates the ""EngagementScore"" feature, and correctly identifies the PostType with the highest average EngagementScore for each platform. The results are printed as requested, formatted to two decimal places. The code uses clear variable names and is well-structured, fulfilling all task requirements.

One minor stylistic difference from the ground truth is the iteration method for finding the best post type. The ground truth leverages pandas' vectorized operations and sorting (`groupby`, `sort_values`, `drop_duplicates`) which is often more idiomatic and performant for larger datasets. The generated code uses a loop (`for platform in df[""Platform""].unique():`) which, while correct and readable for this small dataset, might be less efficient for very large DataFrames. However, for the given problem size and clear readability, this implementation is perfectly acceptable and achieves the desired outcome. The output formatting is also slightly different but equally valid as it presents all required information clearly.",gemma-3-27b-it,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu"", ""Injury"", ...",8,"The generated code correctly performs all the required calculations and identifies the diagnosis with the longest average length of stay. All four steps of the task are addressed.

**Strong Points:**
- Correctly defines the DataFrame as specified.
- Accurately calculates `LengthOfStay` using `dt.days`.
- Correctly uses `groupby().agg()` to calculate both `AverageLengthOfStay` and `PatientCount` per diagnosis.
- Correctly identifies the diagnosis with the longest average stay using `idxmax()`.
- The code is concise and efficient.

**Areas for Improvement:**
- The printing format for the `diagnosis_summary` is basic (`print(diagnosis_summary)`). The ground truth code provided more user-friendly formatted output for the summary (using f-strings for alignment and clearer labels), which makes the output more readable, especially for a benchmark task that explicitly asks to ""print this information"". While the generated code does print the information, it doesn't do so with the same level of clarity and formatting as the ground truth.
- The prompt explicitly asks to ""print this information (Diagnosis, Average LengthOfStay, Patient Count)"". While `print(diagnosis_summary)` does print these, a more explicit printout structured like the ground truth (e.g., iterating through rows and printing formatted strings) would fulfill the ""print this information"" requirement more thoroughly, especially considering the detailed nature of other requirements.

Overall, it's a very good solution that successfully achieves the task's core objectives. The minor improvement would be in the detailed formatting of the output for readability and explicit adherence to the ""print this information"" request.",gemma-3-27b-it,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",10,"The generated code correctly defines the DataFrame, calculates the ""WeightedRating"" precisely as specified using the given formula, adds it as a new column, and accurately identifies and prints the title and weighted rating of the movie with the highest ""WeightedRating"". The code is concise and efficient, demonstrating proper use of pandas functionalities. All requirements of the task are met, and the code quality is excellent.",gemma-3-27b-it,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 10000, 8000]
}
df = pd...",9,"The generated code correctly defines the DataFrame, calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"", and prints the resulting grouped DataFrame, fulfilling all requirements of the task.

**Strong points:**
*   **Correctness:** The `groupby().mean()` method correctly computes the averages for the specified columns.
*   **Conciseness:** The solution is very compact and efficient, achieving the desired outcome with minimal lines of code.
*   **Readability:** The code is easy to understand.

**Areas for minor improvement:**
*   **Column Naming in Output:** While functionally correct, the output column names (e.g., 'PriceUSD', 'Mileage') are the same as the original columns. The ground truth used named aggregations (e.g., `Average_PriceUSD`) which can make the output DataFrame's columns more descriptive, especially when performing multiple different aggregations on the same column. For this specific task, `mean` is implied, so it's a minor point.
*   **Explicit Printing Context:** The ground truth added a descriptive print statement (`print(""DataFrame showing average PriceUSD and Mileage per Brand:"")`) before the DataFrame. This is a small stylistic point that can improve the presentation of the output for the user.

Overall, it's a very good solution.",gemma-3-27b-it,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63, 83],
    ""De...",10,"The generated code is entirely correct and effectively addresses all aspects of the task. It accurately defines the DataFrame, correctly calculates and adds the ""TotalStats"" column, precisely identifies the ""Name"" of the Pokemon with the highest ""TotalStats"", and accurately calculates and prints the average ""Speed"" for each ""Type1"". The code is concise, well-structured, and demonstrates a good understanding of pandas operations. There are no missing imports or logical errors.",gemma-3-27b-it,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, OneHotEncoder
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, GaussianMixture
from sklearn.decomposition import PCA, TSNE, UMAP
from sklearn.metrics import silhouette_score, calinski_harabasz_index
from sklearn.feature_selection import VarianceThreshold
from scipy.stats import kurtosis
from sklear...",6,"The generated code attempts to address most requirements but falls short in several key areas compared to the ground truth, particularly in advanced feature engineering, robust cluster validation, and comprehensive visualization.

**Strengths:**
*   **Basic Structure:** Follows a logical flow of data loading, feature engineering, preprocessing, clustering, and visualization.
*   **Multiple Algorithms:** Includes KMeans, Hierarchical (Agglomerative), DBSCAN, and Gaussian Mixture Models.
*   **Dimensionality Reduction:** Uses PCA, t-SNE, and UMAP for visualization.
*   **Core Metrics:** Calculates silhouette and calinski-harabasz scores.
*   **Survival Analysis:** Attempts to analyze survival rates per cluster and validate with a survival prediction model.

**Weaknesses and Areas for Improvement:**

1.  **Feature Engineering (Advancedness):**
    *   **Ground Truth (GT) vs. Generated:** The GT's feature engineering is significantly more advanced and nuanced. It includes:
        *   Proper handling of `FarePerPerson` by imputing `Fare` *before* division and dividing by `TicketGroupSize` (which is more accurate for shared tickets) instead of `FamilySize`.
        *   Sophisticated `Title` grouping (e.g., combining Mlle/Ms, Mme, and grouping rare titles). The generated code simply `fillna('Unknown')` and then Label Encodes, which doesn't capture semantic similarity.
        *   Detailed `TicketPrefix` extraction and grouping, which is distinct from `SharedTicket` (generated code's `SharedTicket` based on `TicketPrefix` count is not fully equivalent to `TicketGroupSize` in GT).
        *   `CabinDeck` extraction and `HasCabin` boolean, which is better than just `CabinLetter` and `Deck` as in generated.
        *   Crucially, GT includes **Interaction Features** (Age\_Class, Fare\_Class, FarePerPerson\_Class) which are vital for a ""comprehensive"" analysis. The generated code lacks these.
    *   **Imputation Order:** The generated code does `df['Fare'] = df['Fare'].fillna(df['Fare'].mean())` *after* `df['FarePerPerson'] = df['Fare'] / df['FamilySize']`, which would create NaNs in `FarePerPerson` if `Fare` was initially NaN. GT imputes `Fare` first.

2.  **Preprocessing:**
    *   **Label Encoding:** The generated code uses `LabelEncoder` for many categorical features (`Sex`, `Embarked`, `Title`, `CabinLetter`, `Deck`) **before** `OneHotEncoder`. This is problematic. Label encoding introduces an ordinal relationship that doesn't exist for nominal categories, which can mislead clustering algorithms. GT correctly uses `OneHotEncoder` for all nominal categorical features.
    *   **Pipeline:** The generated code lacks a proper `ColumnTransformer` and `Pipeline` for preprocessing, making the code less modular and harder to manage (e.g., `df.drop(['Name', 'Ticket', 'Cabin'], axis=1)` is done globally, not within a pipeline). GT has a robust pipeline for numerical scaling and categorical one-hot encoding.

3.  **Optimal Cluster Determination:**
    *   **Crucial Omission:** The generated code **completely misses** the requirement to determine the optimal number of clusters using *multiple validation metrics* and *visualizations*. It hardcodes `n_components=3` for GMM and `n_clusters=3` for K-Means/Agglomerative without any justification or exploration.
    *   **Missing Metrics:** The generated code only uses Silhouette and Calinski-Harabasz. GT also includes **Davies-Bouldin Index** and, very importantly, the **Elbow Method (Inertia)** for K-Means, along with comprehensive plots to analyze these metrics across a range of `k` values. GT provides an example of how to make an informed choice for `optimal_k`.

4.  **DBSCAN Parameter Tuning:** The generated code uses arbitrary `eps=0.5, min_samples=5` for DBSCAN. GT acknowledges that these are ""highly dataset-dependent"" and suggests practical methods like k-distance graphs, indicating a higher level of awareness for this algorithm's specific needs. It also correctly handles noise points (-1 label) in subsequent analysis and visualization, which the generated code partially does but less explicitly.

5.  **Cluster Stability Analysis:**
    *   **Limited Scope:** The generated code acknowledges that ""Consensus Clustering is complex and beyond the scope,"" which is fair. However, it provides *no* stability analysis at all. GT demonstrates a simplified but effective stability check for K-Means using **Adjusted Rand Index (ARI)** over multiple runs, which directly addresses the requirement for ""cluster stability analysis.""

6.  **Visualization and Analysis Depth:**
    *   **Cluster Characteristics:** GT provides detailed cluster profiles using median/mode of *original* features, which is essential for `interpretability`. It also provides a **heatmap of scaled feature means** for each cluster, which is excellent for understanding which features define each cluster after scaling. The generated code only prints means of `Survived` and `size`, which is insufficient.
    *   **Survival Rate Visualization:** While the generated code prints survival rates, GT also visualizes them clearly with bar plots.
    *   **Feature Distribution Plots:** GT includes `violin plots` and `count plots` to show distributions of key original features (Age, Fare, Pclass, Sex) across clusters, aiding in interpretation. The generated code lacks this.

7.  **External Validation:**
    *   **Methodology:** The generated code's ""Validation with Survival Outcomes"" using `RandomForestClassifier` on `cluster_df[['Cluster']]` to predict `Survived` is a weak form of external validation. It tests if the cluster label itself (a categorical feature) can predict survival, which is less about validating the *goodness* of the clustering in how well it separates survival outcomes, and more about how much predictive signal the cluster label *alone* carries.
    *   **Ground Truth Approach:** GT's external validation directly calculates and visualizes survival rates *within each cluster* and for each algorithm, allowing *direct comparison* of how well different cluster separations align with survival outcomes. This is a more direct and commonly used method for external validation when a ground truth outcome (like `Survived`) is available.

8.  **Imports:** The generated code includes `RobustScaler`, `VarianceThreshold`, `kurtosis`, `RandomForestClassifier`, `train_test_split`, `accuracy_score` which are not directly used for the primary clustering task (except RF for a weak validation). The GT's imports are lean and precise to the task.

In summary, while the generated code provides a functional pipeline, it lacks the depth, robustness, and advanced techniques demonstrated in the ground truth, especially concerning feature engineering, validation, and interpretability.",gemma-3-27b-it,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dystopian""],
    ""Stock"": [...",8,"The generated code correctly performs all the required tasks.

**Strengths:**
*   **Correct DataFrame Creation:** The DataFrame is initialized perfectly as per the task description.
*   **Accurate Author Statistics:** The `groupby().agg()` method is used correctly to calculate the count of books and the average price per author. The column renaming is also appropriate.
*   **Correct Author with Most Stock:** The `groupby().sum().idxmax()` chain is the most efficient and correct way to find the author with the most books in stock.
*   **Clear and Concise:** The code is well-structured and easy to understand.

**Areas for Improvement:**
*   **Printing Author Statistics:** The output for ""Author Summary"" is `print(author_summary)`, which prints the entire DataFrame. While correct, the ground truth code formats this output more clearly with f-strings, making it more readable for a user. For instance, `Author: F. Scott Fitzgerald | Books: 1 | Avg Price: $10.99`. This is a minor stylistic difference, but improves presentation.
*   **Completeness in Printing Most Stock:** The generated code only prints the author's name (`author_with_most_stock`), not the actual total stock count for that author. The ground truth also prints `Total stock for this author: {max_stock_value} units`. This is a small omission from the full requirement of ""Identify and print the 'Author' who has the most books in stock (sum of 'Stock' for all their books)"". The prompt asks ""Print this information"", implying both the author and the sum.

Overall, it's a very good solution that successfully addresses the core requirements. The improvements are mainly about the presentation of the output and fully matching a small detail in the printing requirement.",gemma-3-27b-it,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, validation_curve
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
import xgboost as xgb
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler
from sklearn.imp...",4,"The generated code attempts to address many aspects of the prompt but fails significantly in several key areas.

**Correctness:**
*   **Data Preprocessing:**
    *   `LabelEncoder` is used on `ph` which is a numerical column in the dataset, this is incorrect and inappropriate.
    *   The `water_potability.csv` dataset has a 'Potability' column, not 'suitability'. This causes a `KeyError` as the target variable is incorrectly referenced throughout the code.
    *   Outlier detection only *removes* rows with outliers, which drastically reduces the dataset size and often leads to biased models if not handled carefully, especially for benchmarking. The prompt asked for ""outlier detection and treatment"", and capping (Winsorization) is a more robust treatment.
    *   The `X` and `y` dataframes become mismatched after outlier removal, meaning `y` still contains all original rows while `X` has fewer. This leads to an `IndexError` when used in subsequent steps like `GridSearchCV` or `StratifiedKFold`.
    *   `MinMaxScaler` is imported but never used. `StandardScaler` (which is in the ground truth) is more commonly used for general numerical features.
*   **Imputation Strategies:** While different imputers are defined (`simple`, `knn`, `iterative`), only `SimpleImputer` is implicitly used in the base structure, so the ""multiple imputation strategies"" requirement is not met in practice. The code also incorrectly applies imputation *after* the initial data splitting and feature engineering, which is a pipeline misstep.
*   **Advanced Feature Engineering:** Polynomial and interaction features are created, but they are applied to the raw `X` *before* scaling, which is generally not good practice for models sensitive to scale (like SVM, Neural Networks). The ground truth correctly applies them within a pipeline after scaling. The generated code also re-creates `numerical_cols` *after* outlier removal, which will likely lead to issues if categorical data was present (though not in this specific dataset).
*   **Model Optimization:** `GridSearchCV` is used directly on the full dataset (`X`, `y`) instead of `X_train`, `y_train`. This is an improper use of cross-validation for hyperparameter tuning. While it does cross-validation *within* the search, training on the entire dataset means no held-out test set for final evaluation. The ground truth correctly splits the data first.
*   **Ensemble Models:**
    *   Blending is overly simplified and just uses the Random Forest model. It doesn't actually implement a blend of multiple models.
    *   `StackingClassifier` is passed `optimized_models['RF']` as a `final_estimator`, which is redundant if RF is already one of the base estimators. A different model, like `LogisticRegression` (as in ground truth), typically serves as a meta-learner.
*   **Evaluation:**
    *   The `evaluate_model` function performs K-Fold CV *within* itself, which is redundant given that `GridSearchCV` already performs CV for hyperparameter tuning. Also, the final `model_results` are based on the *mean* CV score, but the final evaluation for metrics should be on a completely unseen test set. The code does not perform a single final evaluation on a held-out test set.
    *   The `stratify` parameter is used with `skf` but not with the initial `train_test_split` (if it were implemented correctly).
    *   Validation curves are not plotted (only calculated) and learning curves are only for RF.
*   **Statistical Significance Testing:** The paired t-test implementation is flawed. It repeatedly calculates ROC AUC on the full `X`, `y` for 10 times for each model comparison, which does not simulate paired cross-validation folds. It also doesn't compare the *best* model to others.
*   **Model Interpretability (SHAP):** The SHAP plots are commented out, so the requirement for providing them visually is not met. It also uses `TreeExplainer` for all models, which is not suitable for SVM or MLP.
*   **Model Calibration:** `CalibratedClassifierCV` is applied to the full `X`, `y` dataset, which is incorrect. Calibration should be done on training data or cross-validation folds. The `calibration_curve` is computed but not plotted.
*   **Dashboard:** No actual dashboard or comprehensive comparison visualization is created, only a comment placeholder.

**Code Quality:**
*   Lack of clear separation between train/test data throughout the code. Most operations are performed on the full dataset (`X`, `y`), which blurs the lines between training, validation, and testing phases.
*   Error handling for file loading is missing.
*   Variable names are sometimes vague (e.g., `df` is modified heavily).
*   No comments explain complex logic beyond brief headers.
*   Redundant imports (e.g., `MinMaxScaler`).
*   The `LabelEncoder` on a numerical column is a critical logical error.
*   `warnings.filterwarnings(""ignore"")` is a lazy way to suppress errors and can hide real problems.

**Completeness:**
*   **Missing from prompt requirements:**
    *   Multiple imputation strategies are mentioned but not effectively implemented (only one is used in the pipeline structure).
    *   Outlier treatment is attempted but is destructive (removes rows) instead of capping/transformation, which is less ideal for a ""comprehensive"" system.
    *   Bootstrap sampling for confidence intervals is entirely missing.
    *   Prediction confidence intervals in the dashboard are missing (linked to bootstrap).
    *   Comprehensive model comparison dashboard with visualisations (performance metrics, feature importance, prediction confidence intervals) is essentially missing, only a comment.
    *   Probability calibration plots are calculated but not plotted.
    *   The core issue of `KeyError` due to incorrect column name `suitability` makes the code non-functional for the given dataset name.

Overall, the generated code has severe functional and methodological errors that prevent it from being a robust and correct solution to the task. It fundamentally misunderstands the proper workflow for machine learning model development, especially regarding data splitting, hyperparameter tuning, and independent evaluation sets.",gemma-3-27b-it,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
import pymc3 as pm
import scipy.stats as stats
from sklearn.impute import MICE, KNNImputer, IterativeImputer
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.model_selection...",3,"The generated code attempts to address many aspects of the prompt but fails significantly in several key areas, leading to a low score.

**Primary Reasons for Failure/Critical Missing/Incorrect:**

1.  **Missing MICE Implementation:** The code claims to implement MICE but uses `sklearn.impute.MICE` which does not exist. `sklearn.experimental.enable_iterative_imputer` is required and `IterativeImputer` is the MICE-like approach. The ground truth correctly uses `IterativeImputer` for MICE-like behavior. This is a fundamental error in meeting the 'multiple imputation techniques' requirement.

2.  **Incorrect PyMC3 Implementation:**
    *   `invlogit` is not defined and would cause a runtime error. PyMC3 uses `pm.math.sigmoid`.
    *   The `pm.Bernoulli(""p"", ...)` and `pm.Bernoulli(""observed"", p, observed=y_train)` structure is incorrect. `p` should be the probability derived from the linear model, and `Y_obs` (or similar) should be the observed binary outcomes. The provided code attempts to make `p` a Bernoulli variable itself and then `observed` another Bernoulli based on `p`, which isn't how it's done. The ground truth correctly builds the linear model (`mu`), then applies the sigmoid to get probabilities (`p`), and then creates the likelihood (`pm.Bernoulli('Y_obs', p=p, observed=y_train)`).
    *   There's no prediction on the test set from the PyMC3 model, which is crucial for evaluating model performance.
    *   `pm.traceplot` and `az.plot_trace` were expected given typical PyMC3 usage, and the code only uses `pm.traceplot` without importing `arviz`.

3.  **Incomplete/Incorrect Hypothesis Testing:**
    *   **MANOVA:** The code attempts to use `statsmodels.formula.api.ols` for MANOVA, which is for OLS regression. MANOVA requires `statsmodels.multivariate.manova.MANOVA`, which the ground truth uses. The provided `'Potability ~ pH + Sulfate + Magnesium + Total_dissolved_solids'` is a regression formula, not a proper MANOVA setup.
    *   **Permutation Tests:** While it attempts `scipy.stats.permutation_test`, the implementation is poor. `scipy.stats.permutation_test` takes groups directly, not a DataFrame and then relies on an internal `stat` function on the dataframe itself. The ground truth uses a more robust manual permutation test to compare group means, which is often more common and allows for custom statistics.
    *   **Bootstrap Confidence Intervals:** The provided `bootstrap_mean` function also incorrectly takes a DataFrame and assumes columns, whereas bootstrapping is typically performed on a *single* array/series. The way it constructs `sample` and tries to access `'pH'` will lead to errors if `data` is not a DataFrame, or if `'pH'` is missing. The ground truth correctly performs bootstrapping on a Series (`data_hardness`).

4.  **Very Limited Model Diagnostics and Robust Methods:**
    *   **Residual Analysis:** For logistic regression, just `residuals = lr.predict(X_test) - y_test` is not standard for analyzing residuals of a classification model. Deviance residuals or Pearson residuals are more appropriate, which the ground truth attempts via `statsmodels.genmod.generalized_linear_model.GLM`.
    *   **Robust Statistical Methods:** The prompt explicitly asked for this (e.g., using `statsmodels.robust.robust_linear_model.RLM`). The generated code has a placeholder comment `# 6. Robust Statistical Methods` but no implementation.
    *   **Goodness-of-fit:** No specific goodness-of-fit tests for logistic regression (e.g., Hosmer-Lemeshow or likelihood ratio tests) are present beyond the AUC. The ground truth does implicitly show this through the GLM summary and ROC curve.

5.  **Simplified Visualizations:** While some visualization attempts are made, they are very basic compared to the ""advanced statistical visualizations including Q-Q plots, leverage plots, and diagnostic dashboards"" requested.
    *   The leverage plot is not a proper leverage plot from `statsmodels.graphics.regressionplots.plot_leverage_resid2` but a simple scatter plot of X vs residuals, which is incorrect.
    *   No diagnostic dashboards beyond a basic pairplot.

6.  **Imports and Code Structure:** While imports are present, `impute.MICE` directly imports a non-existent class, and `invlogit` is missing. The structure is fragmented, lacking the logical flow and clear print statements indicating progress as seen in the ground truth.

In summary, the generated code contains numerous critical errors in implementation for several core requirements (MICE, PyMC3, MANOVA, Robust Methods, specific diagnostic plots), making it largely non-functional or incorrect for the stated task.",gemma-3-27b-it,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, GaussianMixture, SpectralClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.decomposition import PCA, FactorAnalysis
import umap
import t_sne
from sklearn.random_projection import GaussianRandomProjection
from sklearn.ensemble import BaggingClassifier
fr...",4,"The generated code attempts to address several aspects of the prompt but falls short in correctness, completeness, and overall code quality.

**Critical Missing/Incorrect Elements:**

1.  **Feature Engineering:** The generated feature engineering is simplistic and largely incorrect based on the dataset column names.
    *   `df['pH_ORP'] = df['pH'] * df['oxidation_reduction_potential']`: There is no 'oxidation_reduction_potential' column.
    *   `df['Conductivity_TotalDissolvedSolids'] = df['conductivity'] * df['total_dissolved_solids']`: There is no 'total_dissolved_solids' column.
    *   `df['Sulfate_Chloride'] = df['sulfates'] / (df['chlorides'] + 1e-6)`: Column names are 'Sulfate' and 'Chloramines', not 'sulfates' and 'chlorides'. `1e-6` is a good practice for division by zero.
    *   `df['Nitrate_Sulfate'] = df['nitrate'] / (df['sulfates'] + 1e-6)`: There is no 'nitrate' column.
    *   `df['Contamination_Risk'] = df['turbidity'] + df['solids_1'] + df['coliform'] + df['organic_carbon'] + df['trihalomethanes']`: Column name is 'Turbidity', 'Organic_carbon', 'Trihalomethanes'. There is no 'solids_1' or 'coliform' column.
    *   The prompt specifically asked for ""water quality indices, chemical balance ratios, contamination risk scores, and temporal patterns if applicable"". The generated code provides only very basic, often incorrect, ratios and a simple sum for contamination, completely missing the more complex WQI and temporal pattern aspects.
2.  **Dataset Handling & Imputation:** The code directly uses `df.drop('Potability', axis=1)` without any missing value imputation. The `water_potability.csv` dataset typically has missing values (e.g., 'ph', 'Sulfate', 'Trihalomethanes') which *must* be handled before scaling or clustering. This is a fundamental flaw.
3.  **Imports:** `import t_sne` is incorrect; it should be `from sklearn.manifold import TSNE`. Also, unnecessary imports like `sklearn.random_projection.GaussianRandomProjection`, `sklearn.ensemble.BaggingClassifier`, `sklearn.model_selection.train_test_split`, `sklearn.svm.SVC`, `scipy.stats.kurtosis`, `scipy.stats.skew`, `sklearn.ensemble.RandomForestClassifier`, `sklearn.metrics.accuracy_score` suggest a copy-paste without proper context for the given task which is clustering, not classification.
4.  **Optimal Number of Clusters:** The `find_optimal_clusters` function only uses Silhouette score for KMeans and hardcodes `best_n_clusters = 3` for other algorithms, ignoring the requirement to use ""multiple validation techniques (silhouette analysis, gap statistic, elbow method, Davies-Bouldin index, Calinski-Harabasz index)"". The plotting of these metrics is also missing.
5.  **DBSCAN:** The `cluster_and_evaluate` function for DBSCAN attempts to calculate silhouette/DBI/CH scores, but these metrics are not well-suited for DBSCAN because it identifies noise points (-1 label) which typically need to be handled separately for these internal validation metrics. Also, the `eps` and `min_samples` parameters are hardcoded with arbitrary values rather than being determined systematically (e.g., k-distance graph) which is crucial for DBSCAN.
6.  **Dimensionality Reduction:** Only PCA, UMAP, and t-SNE are performed. Factor Analysis is mentioned in the prompt but not implemented. The `t_sne` import is incorrect. There's no use of the 3D options requested.
7.  **Cluster Stability Analysis:** This critical requirement is completely missing.
8.  **Analysis of Water Quality Profiles:** While it calculates `cluster_profiles = df.groupby(clusters).mean()`, it doesn't perform the detailed analysis or discussion on chemical composition, contamination risks, and potability rates for each cluster as required, nor does it present it effectively.
9.  **Visualization:** Only a single 2D PCA plot is generated. The dashboard requirement with ""2D/3D plots, radar charts for cluster profiles, and interactive visualizations"" is barely met.
10. **External Validation & Domain Knowledge:** This is completely absent.
11. **Cluster-based Anomaly Detection:** The code uses `EllipticEnvelope` for anomaly detection, which is a global anomaly detector, not specifically ""cluster-based"" in the context of identifying anomalies *within* or relative to specific clusters. The prompt specifically asked for cluster-based methods (e.g., distance to centroid, Isolation Forest per cluster).

In summary, the generated code contains numerous functional errors due to incorrect column names and missing value handling, lacks substantial parts of the required functionality (stability analysis, comprehensive validation, detailed profiling, advanced dashboard, true cluster-based anomaly detection), and demonstrates poor code quality through incorrect imports and arbitrary parameter choices.",gemma-3-27b-it,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import linregress
from sklearn.linear_model import LinearRegression
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Calculate correlations with 'quality'
correlations = df.corr()['quality'].drop('quality')

# Find the top 3 positively and negatively correlated features
top_3_positive = correlations.nlargest(3)
top_3_negative = correlations.nsmallest(3)

# Print t...",8,"The generated code correctly addresses most of the task requirements. It calculates correlations, identifies top positive and negative features, creates a heatmap, and generates scatter plots with regression lines.

**Strong Points:**
*   Correctly uses `df.corr()['quality'].drop('quality')` to get correlations with the target and `nlargest`/`nsmallest` for top features.
*   Successfully generates a correlation matrix heatmap with `seaborn.heatmap`.
*   Correctly identifies the most positive and negative features for scatter plots.
*   Accurately creates scatter plots and overlays linear regression lines using `scipy.stats.linregress`.

**Areas for Improvement (Why not a 9 or 10):**
*   **Missing Error Handling:** The ground truth includes robust error handling for file not found, which is a good practice for real-world scripts. The generated code assumes the CSV file is always present.
*   **Hardcoded File Name**: The ground truth uses a `DATASET_PATH` constant for better maintainability.
*   **Plot Customization**: While functional, the plots in the ground truth have more detailed titles, labels, legends (especially for the regression line), a grid, and `tight_layout` calls, which improve readability and presentation. The regression line in the scatter plots does not extend across the full range of the data in the generated code; it merely connects dots, whereas the ground truth uses `np.array([min_val, max_val])` to ensure the line spans the data's extent, and also labels the R-squared value.
*   **Unused Import**: `sklearn.linear_model.LinearRegression` is imported but not used, indicating a potential for cleaner imports.
*   **Numerical Features Selection for Heatmap**: While `df.select_dtypes(include=np.number)` is technically correct, `df.corr()` by default only considers numerical columns, so this step is redundant for the heatmap unless there's a specific reason to exclude non-numeric columns from the DataFrame before calculating correlation (which isn't applicable here).",gemma-3-27b-it,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Data Cleaning
df.dropna(inplace=True)
df = df[df['Year'] >= 0]  # Remove rows with invalid year
df = df[df['Global_Sales'] >= 0]

# 1. Nintendo vs. Other Publishers
nintendo_sales = df[df['Publisher'] == 'Nintendo']['Global_Sales']
other_sales = df[df['Publisher'] != 'Nintendo']['Global_Sales']
t_statistic, p_value = stats.ttest_ind(nintendo_sales, other_sales...",6,"The generated code attempts to address all requirements of the task, performing the requested statistical tests. However, it falls short in several critical aspects of data cleaning, robustness, and interpretation, leading to potential inaccuracies and less informative output.

**Strengths:**
*   **Correct Imports:** Imports `pandas`, `numpy`, and `scipy.stats` as required.
*   **Test Variety:** Includes t-test, chi-square, and Mann-Whitney U test, as requested.
*   **Basic Structure:** Organizes tests logically.
*   **Chi-square filtering:** Correctly identifies top 5 genres and platforms for a more focused analysis.
*   **P-value output:** Displays p-values and interprets based on alpha=0.05.

**Weaknesses and Areas for Improvement:**

1.  **Incomplete/Incorrect Data Cleaning:**
    *   `df.dropna(inplace=True)`: This is too aggressive. It drops *any* row with *any* NaN. The ground truth correctly identifies specific columns for cleaning (e.g., `Global_Sales`, `Year_of_Release`, `Publisher`, `Genre`, `Platform`) which are crucial for the tests. Dropping rows based on irrelevant NaNs (like `User_Score` if not used) can unnecessarily reduce the dataset size.
    *   `df = df[df['Year'] >= 0]`: The column is `Year_of_Release`, not `Year`. This line will likely cause a KeyError or do nothing effectively if the column name is wrong. The ground truth correctly converts `Year_of_Release` to int after specific NaNs are handled.
    *   `df = df[df['Global_Sales'] >= 0]`: While conceptually reasonable, `Global_Sales` are likely already non-negative. This is redundant if the data is well-formed, but doesn't harm. The primary concern is the `Year` column name and the general `dropna`.

2.  **Lack of Robustness:**
    *   Does not handle `FileNotFoundError` for the CSV. The ground truth includes a `try-except` block for this.
    *   Does not check if groups for t-tests or Mann-Whitney U have enough data (e.g., `len(series) < 2`). This can lead to errors or warnings if a filtered group is empty or has only one value.
    *   The chi-square contingency table creation for `df[df['Platform'].isin(top_5_platforms)]` uses the *unfiltered* `df` for the platform selection, which might lead to an empty or incorrectly shaped table if these filters remove all corresponding genres or vice-versa. The ground truth correctly filters the DataFrame *first* for both top genres and top platforms, then creates the contingency table from *that filtered DataFrame*. This is a critical logical error.
    *   Does not check for empty contingency tables or other conditions where chi-square might fail.

3.  **Missing Interpretation Details:**
    *   The interpretations are very generic: ""There is a significant difference/association."" The ground truth provides more specific interpretations, stating what the *alternative hypothesis* implies (e.g., ""Nintendo games have significantly higher average global sales..."").
    *   It doesn't define `ALPHA` explicitly at the start, though it uses `0.05` directly. Defining it makes the code more readable and maintainable.
    *   It doesn't print the t-statistic, U-statistic, or chi2 statistic, which are typically part of good statistical output.
    *   It does not state the null and alternative hypotheses for each test explicitly, which is crucial for clear interpretation and understanding.

4.  **Minor Issues:**
    *   The task explicitly asks for an `alternative='greater'` for the Nintendo test as it states ""significantly *higher* global sales."" The generated code uses the default two-sided test, which is less specific to the question.
    *   The `Year` column name mismatch.

Overall, while the generated code passes the basic steps, the data cleaning and robustness issues are significant, potentially leading to incorrect results or runtime errors in real-world scenarios. The interpretation also lacks the detail and clarity of the ground truth.",gemma-3-27b-it,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values (impute with median for numerical, mode for categorical)
for col in df.columns:
    if df[col].is...",7,"The generated code largely addresses the requirements but has several critical issues that prevent it from scoring higher.

**Pros:**
*   Correctly loads the dataset.
*   Successfully creates the `High_Sales` target variable.
*   Correctly applies `LabelEncoder` to specified categorical columns.
*   Creates `Total_Regional_Sales` and `Years_Since_Release` features.
*   Splits data into training and testing sets.
*   Trains a Random Forest classifier with 100 estimators.
*   Evaluates the model using accuracy, precision, recall, F1-score, and confusion matrix.
*   Attempts to display feature importance.

**Cons/Areas for Improvement:**

1.  **Incorrect Missing Value Handling (Major Issue):** The code attempts to impute missing values using median for numerical and mode for categorical. While this is a valid general strategy, for this specific dataset and task, it's problematic.
    *   `Global_Sales` is used to create the target variable. Imputing it can lead to incorrect `High_Sales` labels. The ground truth correctly drops rows with missing `Global_Sales`.
    *   `Year` and `Publisher` are crucial for feature creation and encoding. Imputing `Year` with a median before converting to `int` might lead to float values where integers are expected, and imputing `Publisher` (a categorical feature) with the mode is generally okay, but `Year` needs careful handling, often dropping if year is missing or imputing in a way that respects data type and meaning. The ground truth drops NaNs for these critical columns, which is a safer approach given the problem constraints.
    *   The prompt implies that the ""sales"" columns (`NA_Sales`, `EU_Sales`, etc.) might have NaNs, and summing them for `Total_Regional_Sales` requires them to be non-null. The imputation handles this but could introduce noise if NaNs are truly unrecoverable. The ground truth drops `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales` NaNs.

2.  **Incomplete Feature Set (Major Issue):** The `X` dataframe is defined as `X = df[['Platform', 'Genre', 'Publisher', 'Total_Regional_Sales', 'Years_Since_Release']]`. This *misses* several crucial numerical features present in the dataset, namely `NA_Sales`, `EU_Sales`, `JP_Sales`, and `Other_Sales`. These were explicitly mentioned in the ground truth's `features` list and are direct sales figures, which are highly predictive. Omitting them significantly degrades model performance and correctness.

3.  **Missing `random_state` for `train_test_split` (Minor Issue):** Although `random_state=42` was used for `RandomForestClassifier`, it was omitted for `train_test_split`. This means the split will be different each time the code is run, making results non-reproducible. The ground truth correctly sets it for `train_test_split`.

4.  **No Stratification in `train_test_split` (Minor Issue):** For classification tasks, especially with imbalanced classes (which `High_Sales` likely is), using `stratify=y` in `train_test_split` ensures that the proportion of target classes is maintained in both training and testing sets. The ground truth includes this.

5.  **Feature Importance Visualization (Minor Issue):** While a plot is generated, the prompt asked to ""Display feature importance"" which typically implies printing the sorted list rather than a matplotlib plot. A plot is a good addition, but the primary requirement would be the textual output, which is not provided. The ground truth provides a sorted printout of feature importance.

6.  **Missing `n_jobs=-1` for RandomForest (Minor):** The ground truth uses `n_jobs=-1` to leverage all CPU cores, which is good practice for performance with RandomForest. This is a minor omission.

To improve, the code needs to:
*   Correctly handle missing values for `Global_Sales`, `Year`, `Publisher`, and regional sales by dropping rows, as these are critical for target variable formation and feature creation.
*   Include all relevant sales features (`NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`) in the `X` dataframe.
*   Add `random_state=42` and `stratify=y` to `train_test_split`.
*   Print the sorted feature importances.",gemma-3-27b-it,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values
df.fillna(df.median(), inplace=True)

# Feature Engineering
df['Sales_Ratio_NA_EU'] = df['NA_Sales'] / (df['EU_Sales'] + 1e-9)  # Adding a small value to avoid division b...",6,"The generated code attempts to address most of the requirements but has several significant issues and omissions that prevent it from being a high-scoring solution.

**Correctness:**
*   **Missing Value Handling:** `df.fillna(df.median(), inplace=True)` is overly simplistic and incorrect for handling categorical columns like 'Publisher' and 'Genre'. It also implicitly fills *all* numerical NaNs, which might not be ideal (e.g., 'Year' should be integer). The ground truth uses `SimpleImputer` specific to column types (median for Year, mode for Publisher/Genre).
*   **Feature Engineering:**
    *   `Sales_Ratio_NA_EU`: Correct.
    *   `Publisher_Avg_Sales`: Correct.
    *   `Genre_Market_Share`: The multiplication by 100 is unnecessary if it's expected as a ratio/proportion, though not strictly incorrect.
    *   `Platform_Popularity`: Correct.
    *   `Sales_Momentum`: Correct.
*   **Correlation Analysis:** Correctly calculates and visualizes the heatmap.
*   **PCA:**
    *   `scaled_features = StandardScaler().fit_transform(numerical_features)`: `numerical_features` includes columns that might have NaNs (e.g., if any sales columns or new features generated NaNs due to initial data or previous steps). Also, it doesn't correctly handle `Rank` if that were intended to be excluded from PCA, which is a common practice.
    *   The `numerical_features` DataFrame might contain non-numeric data if some columns like 'Year' were not properly converted to numeric after imputation.
    *   The PCA plots `explained_variance_ratio` as a bar chart, while typically a cumulative explained variance plot is more informative.
*   **Polynomial Features:**
    *   `poly.get_feature_names_out`: This is good practice.
    *   However, `numerical_features` might still contain non-numeric data or NaNs if not handled properly.
    *   It only applies polynomial features to `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`, *not* `Global_Sales` as specified in the ground truth, which is a small omission. More critically, it creates `poly_df` and then `pd.concat` without aligning indices, which can lead to `NaN` values if the original `df` had a non-default index or if rows were dropped (e.g., by improper outlier handling). This method is prone to errors.
*   **Interaction Features:** `df = pd.get_dummies(df, columns=['Year', 'Genre'], drop_first=True)` *one-hot encodes* 'Year' and 'Genre', it does *not* create interaction features between 'Year' and 'Genre' as requested. The ground truth creates interaction terms `Year * (Genre_dummy_var)`. This is a major misinterpretation of the requirement.
*   **Outlier Handling:** `df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]` performs *row removal* for outliers. The task explicitly asks to ""Identify and handle outliers using IQR method"" and the ground truth caps them, which is a more robust approach than simply removing rows, especially if outliers are numerous or important. Removing rows this way can drastically reduce dataset size and introduce bias. Also, `numerical_features` is defined earlier, and if the `df` was modified, `numerical_features` might not reflect the updated state for the loop.

**Code Quality:**
*   **Imports:** Proper.
*   **Structure:** Relatively logical flow.
*   **Comments/Prints:** Lacks informative print statements to show progress or data state, which makes debugging and understanding harder.
*   **Error Handling:** No `try-except` for file loading.
*   **Warnings:** No warning suppression, but not a critical flaw.

**Completeness:**
*   **Missing Value Handling:** Incomplete as it doesn't handle categorical columns correctly.
*   **Engineered Features:** All 5 features are attempted.
*   **Correlation Analysis:** Done.
*   **PCA:** Performed, but visualization choice could be better.
*   **Polynomial Features:** Applied, but not to all specified sales columns and with potential indexing issues.
*   **Interaction Features:** Completely misunderstood and implemented as one-hot encoding, not interactions.
*   **Outlier Handling:** Implemented as row removal (dropping data) instead of capping, which is generally not preferred for outlier treatment in feature engineering unless specified.

Overall, the code shows an understanding of the individual steps but struggles with the nuances of each requirement (e.g., proper imputation for different data types, creating interaction features, handling outliers by capping vs. removal, and indexing in DataFrame operations). The interaction features part is a definitive failure to meet the requirement.",gemma-3-27b-it,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Clean zero values
df = df.replace(0, np.nan)

# Median imputation
for col in df.columns:
    if df[col].isnull().any():
        df[col] = df[col].fillna(df[col].median())


# 1. T-test: Glucose levels and Diabetes
glucose_diabetic = df[df['Outcome'] == 1]['Glucose']
glucose_non_diabetic = df[df['Outcome'] == 0]['Glucose']
t_statistic, p_value = stats.ttest_ind(glucose_...",7,"The generated code correctly performs all the requested statistical tests and provides some level of interpretation. However, there are several areas where it falls short compared to the ground truth code and the prompt's requirements, primarily around the data cleaning process and the depth of interpretation.

**Strengths:**
*   **Correctness of Statistical Tests:** All five requested statistical tests (t-test, Mann-Whitney U, Chi-square, Pearson correlation, ANOVA) are correctly implemented using `scipy.stats`.
*   **P-value Interpretation:** It correctly uses the `0.05` significance level to determine if a result is significant.
*   **Structure:** The tests are logically separated.

**Areas for Improvement (Why it lost points):**

1.  **Data Cleaning - Zero Values:**
    *   **Incomplete:** The prompt explicitly states: ""clean zero values by replacing them with NaN and then median imputation."" It also specifies which columns should be treated (e.g., 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI') because 'Pregnancies' and 'Outcome' can legitimately be zero. The generated code globally replaces ALL zeros with NaN (`df = df.replace(0, np.nan)`), which incorrectly treats 'Pregnancies' and 'Outcome' zeros as missing values. This is a significant flaw in the data preparation phase.
    *   **Lack of Robustness:** The ground truth includes a `try-except` block for file loading, which is good practice. The generated code lacks this.

2.  **Imputation Specificity:** It imputes all columns with NaNs, even those where NaN might have been introduced by the incorrect zero replacement or should not be imputed (e.g., 'Outcome'). The ground truth correctly identifies `cols_to_impute`.

3.  **Interpretation Detail:**
    *   The prompt asked for ""Display p-values and interpret results with significance level =0.05."" While it displays the p-value and gives a simple ""Significant difference..."" or ""No significant difference..."", the interpretation is very basic.
    *   The ground truth code provides more detailed conclusions, such as which group has *higher* glucose levels in the t-test, the median/mean values for groups, U-statistic/F-statistic/Chi-square statistic, correlation coefficient, and clearer explanations of what ""reject the null hypothesis"" means in context. This adds much more value to the analysis.
    *   For the correlation, it only states ""Significant correlation"" but doesn't mention the direction (positive/negative) or strength, which is essential for interpretation.
    *   For ANOVA, it doesn't print the mean glucose levels for each BMI category, which would provide context for the significance.

4.  **Clarity of Output:** The ground truth code includes helpful `print` statements before and after cleaning, and detailed output for each test including the test statistic (t-stat, U-stat, chi2-stat, F-stat, correlation coefficient), which the generated code mostly omits (except for correlation and ANOVA F-stat). This makes it harder to fully understand the results without manually inspecting variables.

5.  **Age Grouping:** The prompt specified ""young: <30, middle: 30-50, older: >50"". The generated code uses `labels=['young', 'middle', 'older']` and `bins = [0, 30, 50, 100]`. Given `right=False`, this means `[0,30)`, `[30,50)`, `[50,100]`. This is subtly different from the prompt's `<30` (meaning `(-inf, 30)`) and `>50` (meaning `(50, inf)`), and the ground truth's `[0, 29], 50, np.inf]`. While functionally close, it's not exact to the prompt's definition. The ground truth's `right=True` and specific bins handle the boundaries more explicitly as `(0, 29]`, `(29, 50]`, `(50, inf)`.

The crucial issue is the incorrect and overly broad zero-value cleaning, which affects the validity of the subsequent analyses, and the lack of comprehensive interpretation as requested.",gemma-3-27b-it,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import matplotlib.pyplot as...",6,"The generated code addresses most parts of the task but has significant omissions and inaccuracies in implementing specific requirements, especially regarding preprocessing, comprehensive evaluation, and visualization.

**Missing/Incorrect Elements:**

1.  **Missing `BMI` Imputation:** The preprocessing step correctly replaces 0s with NaNs for 'Glucose', 'BloodPressure', 'SkinThickness', and 'Insulin', but completely misses 'BMI' which also contains 0s that need to be handled based on the problem description. This is a critical omission.
2.  **Incorrect Categorical Encoding:** The problem explicitly asks for one-hot encoding for categorical variables. The generated code uses `LabelEncoder` for 'Age_group' and 'Insulin_level'. This is incorrect for nominal categorical variables, as it introduces an artificial ordinal relationship that can negatively impact model performance. `OneHotEncoder` should have been used.
3.  **No `ColumnTransformer`:** The generated code performs scaling *after* encoding categorical variables using `LabelEncoder` directly on the DataFrame. This is not ideal. A `ColumnTransformer` is standard practice for handling different preprocessing steps for different column types (numerical vs. categorical), ensuring proper scaling of numerical features and correct encoding of categorical ones within a robust pipeline, as shown in the ground truth.
4.  **Limited Evaluation:** While it calculates the specified metrics, it only *prints* them. The requirement was to visualize them (e.g., bar plots comparing models), which is not done.
5.  **Partial Visualization:**
    *   **Confusion Matrices:** Only one confusion matrix (for Logistic Regression) is plotted. The requirement was to create confusion matrices *for all models*.
    *   **ROC Curves:** Similar to confusion matrices, only one ROC curve (for Logistic Regression) is plotted. The requirement was to create ROC curves *for all models*.
    *   **Feature Importance Visualization:** The feature importance plot uses `permutation_importance` on the *original* (unscaled and un-one-hot-encoded) feature names directly from `X.columns`, but the models were trained on `X_train` which was scaled. This means the feature names on the plot do not correctly correspond to the features the model was trained on with one-hot encoding. It should ideally show importance for the transformed features. The ground truth correctly maps feature importances back to the one-hot encoded feature names.
6.  **Hyperparameter Tuning Scope:** The task asked for hyperparameter tuning for the *best model*. While Random Forest is a reasonable choice, the code doesn't explicitly state the reasoning or compare how the tuned RF performs against other models comprehensively after tuning. The ground truth explicitly adds the tuned model as a separate entry in the results for comparison.
7.  **Reproducibility:** No global random seed is set (like `np.random.seed(42)`), which can affect reproducibility across runs, especially for stochastic algorithms.
8.  **Output Readability:** The ground truth includes print statements for loading data, shapes, and progress, which significantly improves the user's understanding of the code's execution. The generated code lacks these informative print statements.

**Strengths:**

*   Loads the dataset.
*   Handles zero values in some specified columns by median imputation.
*   Creates the required new features (`Glucose_BMI_ratio`, `Age_group`, `Insulin_level`).
*   Splits data into training and testing sets.
*   Trains multiple classification models.
*   Performs cross-validation (though only for accuracy).
*   Performs hyperparameter tuning for Random Forest.
*   Calculates accuracy, precision, recall, F1-score, and ROC-AUC.
*   Attempts confusion matrix, ROC curve, and feature importance.
*   Uses appropriate libraries.",gemma-3-27b-it,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, RFE, f_classif
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from imblearn.over_sampling import S...",5,"The generated code attempts to address all parts of the task but falls short on several critical aspects, primarily due to incorrect or incomplete implementation of certain steps, leading to potential errors or suboptimal results.

Here's a breakdown of the issues:

1.  **Missing Value Imputation:** The ground truth correctly identifies that '0's in certain columns (Glucose, BloodPressure, SkinThickness, Insulin, BMI) are actually missing values and replaces them with `np.nan` *before* imputation. The generated code directly imputes '0's using `SimpleImputer`'s 'mean' strategy. This is problematic because '0' is a valid value for 'Pregnancies' and 'Outcome', and directly imputing '0's in the specified columns without first converting them to `NaN` will lead to incorrect imputation, as it will treat the 0s as actual values in the calculation of the mean (or median). Also, the ground truth uses median, which is generally more robust to outliers than mean for imputing numerical data like this.

2.  **Polynomial Features:**
    *   The generated code creates polynomial features for `numerical_cols = ['Glucose', 'BMI', 'Age']`. The ground truth selects a more comprehensive set of core numerical features: `['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Age']`.
    *   **Major Error:** The column naming for the polynomial features in the generated code `[f'{col}_{col}' for col in numerical_cols] + [f'{col1}_{col2}' for col1 in numerical_cols for col2 in numerical_cols if col1 != col2]` is incorrect and leads to the creation of duplicate column names (e.g., `Glucose_Glucose`) if the original `numerical_cols` are present in the final `poly_df` and are later merged. The `get_feature_names_out()` method from `PolynomialFeatures` is the correct and robust way to get meaningful column names. This incorrect naming directly affects later steps like PCA and feature importance.
    *   The generated code concats polynomial features but does *not* drop the original `numerical_cols`, leading to redundancy. The ground truth correctly handles this by dropping the original columns after creating polynomial features.

3.  **Feature Scaling:**
    *   The task explicitly asks for *both* `StandardScaler` and `MinMaxScaler`. The generated code fully implements `StandardScaler` but comments out `MinMaxScaler`, making it incomplete for this requirement.
    *   The `numerical_cols_scaled` list in the generated code is hardcoded and only includes a subset of the *original* numerical features and the *newly engineered* features. It completely ignores all the *polynomial features* that were just created. This is a significant flaw, as scaling should ideally apply to *all* numerical features that will be used for modeling, especially before PCA. The ground truth correctly scales *all* numerical features (post-polynomial transformation).

4.  **PCA Application:**
    *   The generated code applies PCA without prior scaling. The `PCA` object is instantiated without `n_components`, so it defaults to min(n_samples, n_features), which is good for exploration but doesn't meet the ""explain variance ratios"" aspect of the prompt explicitly in terms of selecting a threshold.
    *   Most importantly, PCA *should be applied to scaled data*, which the generated code fails to do accurately due to the scaling issues mentioned above. The ground truth first scales the data, then removes highly correlated features (which can improve PCA performance), and then applies PCA to the processed data.

5.  **Correlation-based Feature Filtering:**
    *   The generated code implements correlation filtering *after* PCA, SelectKBest, and RFE. This is out of order. Feature selection/filtering steps should generally happen on the pre-processed data *before* applying dimensionality reduction techniques like PCA or complex feature selection methods, unless specifically intended for a multi-stage process. The ground truth correctly places this step before PCA.
    *   It drops features based on correlation from the entire `df` including the 'Outcome' column in `corr_matrix = df.corr()`, which is incorrect. Correlation should be calculated on `X` (features only). The ground truth correctly calculates correlation `X_processed.corr().abs()`.

6.  **SMOTE Application:** SMOTE is applied correctly to the X and y, but the `X` it uses has already undergone potentially incorrect feature engineering, scaling, and correlation filtering order. The ground truth correctly separates features and target, performs feature processing, then splits into train/test, and applies SMOTE *only to the training data* to prevent data leakage. The generated code applies SMOTE to the entire dataset (X, y) before any train-test split, which is a significant data leakage issue for any subsequent model evaluation.

7.  **Feature Importance Visualization:** The generated code plots top 10 features, while the prompt asks for top features (ground truth uses 15). This is a minor point, but contributes to incompleteness.

The generated code attempts the steps, but the incorrect order of operations and implementation details in critical steps make it less robust and potentially lead to misleading results compared to the ground truth.",gemma-3-27b-it,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,,,Error,Failed to generate code,gemma-3-27b-it,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.re...",6,"The generated code addresses most of the requirements but has several significant deficiencies in correctness and completeness for a data science benchmark.

**Strong points:**
- Imports necessary libraries.
- Loads the dataset.
- Splits data into training and testing sets.
- Trains and evaluates multiple regression models.
- Performs hyperparameter tuning using GridSearchCV.
- Calculates MAE, MSE, RMSE, and R scores.
- Creates residual plots and feature importance visualizations.
- Implements cross-validation.
- Makes predictions for new hypothetical countries.

**Deficiencies and Areas for Improvement:**

1.  **Preprocessing - Missing Values:** The code uses `df.dropna()` as a simple approach. While this handles missing values, it's a very generic approach. The Ground Truth (GT) specifically checks `df[FEATURE_COLUMNS + [TARGET_COLUMN]].isnull().sum()` and then drops only rows where *relevant* columns have missing values. The GT is more robust by explicitly defining the columns to consider for dropping missing values, which could be crucial if other columns (not used in the model) have NaNs.

2.  **Preprocessing - Categorical Variable Encoding (Major Issue):**
    *   The generated code attempts to encode `Country` and `Region` columns using `LabelEncoder`.
    *   The GT correctly identifies that the specified features for the model (`GDP per capita`, `Social support`, etc.) are *all numerical*.
    *   The GT explicitly states: ""No categorical features are used in the specified model features, so no encoding is applied to X.""
    *   The GT correctly identifies `Country name` as high cardinality and not suitable for direct encoding without advanced techniques, and `Regional indicator` is a categorical column but not part of the specified features.
    *   Using `LabelEncoder` on `Country` and `Region` and then attempting to use `df['Country']` and `df['Region']` as features implies they *should* be part of the `features` list. However, they are *not* included in the `features` list. This leads to redundant and potentially misleading preprocessing steps.
    *   Furthermore, the prompt specifies ""features using GDP per capita, Social support, Healthy life expectancy, Freedom, Generosity, and Perceptions of corruption."" The generated code uses ""GDP per capita"", ""Freedom"", and ""Perceptions of corruption"" as column names, which are slightly different from the actual dataset column names (""Logged GDP per capita"", ""Freedom to make life choices"", ""Perceptions of corruption""). The GT correctly uses the exact column names from the dataset. This is a critical error in aligning with the raw data.

3.  **Feature and Target Definition:**
    *   The generated code uses `'GDP per capita'`, `'Freedom'`, and `'Perceptions of corruption'` in its `features` list. The actual column names in the provided `world-happiness-report-2021.csv` are `Logged GDP per capita`, `Freedom to make life choices`, and `Perceptions of corruption`. This mismatch will cause a `KeyError` or produce incorrect results if the code was run directly without modification of the CSV headers. The GT correctly identifies and uses the actual column names from the dataset.
    *   The GT defines these column names at the top as `TARGET_COLUMN` and `FEATURE_COLUMNS` for clarity and maintainability.

4.  **Hyperparameter Tuning Target:** The generated code performs hyperparameter tuning *only* for Random Forest. The prompt asks to ""Perform hyperparameter tuning for the best model using GridSearchCV"". While Random Forest is a plausible choice, the GT correctly identifies the best initial model based on R2 score and *then* decides which high-performing model to tune (or picks a robust one like GBR for tuning). This demonstrates a more data-driven approach to tuning.

5.  **Evaluation Summary:** The generated code prints results for each model individually but doesn't provide a concise summary table of all model performances, which the GT does, making comparison easier.

6.  **Residual Plot:** The generated code's residual plot is a scatter plot of Actual vs. Predicted values. While sometimes useful, a standard residual plot displays `Residuals (Actual - Predicted)` vs. `Predicted Values` (or `Actual Values`), which is what the GT provides. This plot is essential for assessing homoscedasticity and potential biases.

7.  **Feature Importance for Best Model:** The generated code explicitly calculates and plots ""Feature Importance (Random Forest)"". However, if a different model were the ""best"" without tuning (say, Gradient Boosting Regressor), its feature importance would be more relevant. The GT correctly checks `hasattr(best_tuned_model, 'feature_importances_')` to make this dynamic.

8.  **Prediction for New Countries:**
    *   The generated code defines arbitrary `GDP per capita`, `Freedom`, etc., for new countries, which again don't match the dataset's column names (`Logged GDP per capita`, `Freedom to make life choices`). This would lead to a `KeyError` unless the `new_countries` DataFrame columns were manually aligned with the `features` used for training.
    *   The GT correctly ensures the hypothetical data uses the exact column names used during training (`Logged GDP per capita`, `Freedom to make life choices`).
    *   The analysis of predictions is very brief in the generated code compared to the GT, which provides a more structured and indicative analysis connecting feature values to predicted happiness.

In summary, the generated code has good structure and covers the main points but suffers from critical data handling and data-model alignment issues (incorrect column names, misguided categorical encoding) that would prevent it from running correctly or producing sensible results out-of-the-box for the specified dataset. The GT is superior in its precision regarding dataset specifics and robust data science practices.",gemma-3-27b-it,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA, TSNE
from sklearn.feature_selection import SelectKBest, RFE, f_regression, mutual_info_regression
from sklearn.polynomial.features import PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load and Preprocess
df = pd.read_csv('world-happiness-report-2021.c...",5,"The generated code attempts to address all parts of the task but suffers from multiple critical errors and inconsistencies throughout.

**Critical Errors/Missing Functionality:**

*   **Column Name Inconsistency:** The most significant issue is the inconsistent use of column names. The prompt specifies column names like 'Logged GDP per capita', 'Ladder score', 'Perceptions of corruption', 'Freedom to make life choices'. The generated code frequently uses 'GDP per capita', 'Ladder Score' (different capitalization), 'Region' instead of 'Regional indicator', which will lead to `KeyError` or incorrect processing. This issue alone makes much of the code non-functional as-is.
*   **Preprocessing:** `df.dropna()` is too aggressive. The ground truth correctly identifies that only 'Perceptions of corruption' has a few NaNs and fills them with the mean, keeping more data. The generated code drops any row with *any* NaN, which is likely to remove a significant portion of the dataset.
*   **Engineered Features:**
    *   `Happiness_Efficiency`: Uses 'Ladder Score' and 'GDP per capita', which are likely incorrect column names.
    *   `Social_Wellness_Index`: Divides by 2, which wasn't specified and changes the meaning. The ground truth simply adds them.
    *   `Governance_Score`: Divides by 2 and adds `Perceptions of corruption` directly, which is problematic. 'Perceptions of corruption' is a score where higher values mean *more* corruption. To make ""low corruption"" contribute positively to a ""governance score,"" it should be `(1 - Perceptions of corruption)` as in the ground truth, not `+ Perceptions of corruption`.
    *   `Regional_Happiness_Rank`: Uses `Region` instead of `Regional indicator`. Uses `method='first'` which is arbitrary; ground truth uses no specific method for `rank` which defaults appropriately or specifies `ascending=False`.
    *   `GDP_vs_Regional_Average`: Uses `Region` instead of `Regional indicator`.
*   **Polynomial Features:**
    *   Applies polynomial features only to a very limited subset of ""key happiness factors."" The prompt and ground truth specify applying them to `Logged GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption`. The generated code only uses 3 of these and misses new engineered features that are also relevant.
    *   The `pd.concat` might fail if the `df` and `poly_df` indices don't align correctly, which they often won't after `dropna()` and creating new dataframes.
*   **Feature Scaling:**
    *   Critically, scaling is applied to *individual columns* (`df[['GDP per capita']]`, etc.), not to the full set of engineering features. This is a fundamental misunderstanding of the scaling requirement for dimensionality reduction and feature selection. The scaled features are then immediately overwritten or unused for the downstream steps.
    *   The prompt asked for *multiple* methods, implying a comparison or presentation of their results, and then using *one* consistent scaled dataset for subsequent steps. The generated code scales different columns with different scalers without a clear purpose for downstream use of *all* scaled data. The ground truth correctly uses `StandardScaler` for the full feature set `X` for PCA, t-SNE, etc.
*   **Dimensionality Reduction (PCA & t-SNE):**
    *   Again, applied only to a tiny subset of original features, not the full set of engineered features, which is the core purpose of feature engineering prior to DR.
    *   `TSNE` is imported from `sklearn.decomposition`, but it's typically in `sklearn.manifold`. While it works in some versions, it's not the canonical import path.
    *   No handling of `perplexity` for t-SNE based on data size, which can be crucial for small datasets.
    *   Visualization is basic, but functional for the limited data they use.
*   **Feature Selection:**
    *   `X` is defined using only 5 original features, ignoring all engineered and polynomial features. This completely invalidates the feature selection process in the context of the prompt which asks to select from *all* created features.
    *   `RFE` uses `RandomForestRegressor` as an estimator but doesn't set `step=1` or `n_features_to_select` appropriately given the number of features.
*   **Correlation-based Feature Filtering & Mutual Information:**
    *   `high_corr_features` is created but never used to drop columns from `X`.
    *   Mutual information is calculated only on the limited `X` (5 features).
*   **Interaction Features:** Only one interaction feature is created, whereas the prompt asked for ""interaction features"" (plural) and specified interactions ""between economic and social factors"" (plural). Ground truth creates two explicit ones.
*   **Feature Importance Validation:**
    *   Again, the model is trained on a tiny subset of features, ignoring all engineered features and only using ones created *by the code itself*. This undermines the entire point of validating feature importance on the engineered set.
    *   The `plt.bar` call for feature importances uses `df[['GDP per capita', ...]].columns` directly, which isn't robust practice and assumes the order aligns perfectly with `importances`.

**Code Quality:**

*   **Imports:** Mostly correct, but `TSNE` path is not canonical.
*   **Structure:** Follows the task's numbered points, which is good.
*   **Clarity:** Lack of comments makes it harder to follow the specific choices made for engineered features or scaling.
*   **Readability:** Variable names are okay.
*   **Robustness:** Lacks error handling (e.g., `try-except` for file loading). Doesn't handle potential `inf` or `NaN` values that can arise from divisions in engineered features which the ground truth handles.

In summary, the generated code demonstrates a general understanding of the requested techniques but fails significantly in execution due to fundamental errors in data handling, column management, and applying transformations to the correct feature sets. It does not produce a correctly engineered and analyzed dataset as per the prompt.",gemma-3-27b-it,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Parse date and filter for USA
df['date'] = pd.to_datetime(df['date'])
usa_data = df[df['location'] == 'USA'].copy()

# Set date as index
usa_data.set_index('date', inplace=True)

# Resample to weekly frequency
weekly_cases = usa_data['new_cases'].resample('W').sum()

# Time series decomposit...",7,"The generated code correctly performs most of the required tasks: loading and filtering data, resampling, decomposition, ADF test, and conditional differencing with ACF/PACF plots.

However, there are several areas for improvement:
- **Decomposition Period:** The `seasonal_decompose` function is called without specifying the `period` argument. For weekly data, a period of 52 (weeks in a year) is typical for annual seasonality. Without it, `statsmodels` tries to infer, which might not be optimal or might default to 1, making the seasonality component uninformative.
- **Robustness:** There's no handling for missing data in `new_cases` before resampling, which could affect the sum if NaNs are present.
- **Plotting:** While the decomposition is plotted, it's done manually using `plt.subplot` which isn't as clean or idiomatic as using `decomposition.plot()` directly, which handles layout nicely. Also, the ACF/PACF plots are created for the original series if stationary, but the ground truth handles this more robustly by setting a `stationary_series` variable that is either the original or differenced data and then plotting that. The `lags` for ACF/PACF are hardcoded (40) rather than being dynamically determined based on the data length.
- **Error Handling/Edge Cases:** There's no error handling for `FileNotFoundError` or if the specified `location` (USA) doesn't exist in the dataset, leading to an empty DataFrame and potential errors later. There's also no check for insufficient data points after resampling or differencing for decomposition or ADF test, which could lead to errors.
- **Implicit Imports:** `plot_acf` and `plot_pacf` are imported *inside* the `if/else` block for stationarity, which is unusual and not standard practice. They should be at the top with other imports.

Overall, it's a functional solution that addresses the core requirements but lacks the robustness, best practices, and fine-tuning seen in an expert-level solution.",gemma-3-27b-it,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Categorize wines based on pH
def categorize_ph(ph):
    if ph < 3.2:
        return 'Low pH'
    elif 3.2 <= ph <= 3.4:
        return 'Medium pH'
    else:
        return 'High pH'

df['pH_Category'] = df['pH'].apply(categorize_ph)

# Calculate average quality for each pH group
avg_quality = df.groupby('pH_Category')['quality...",6,"The generated code correctly performs some aspects of the task but falls short on several critical points, particularly regarding the pH categorization, robustness, and the confidence interval calculation for visualization.

Here's a breakdown:

**Strengths:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Data Loading:** Successfully loads the CSV file.
*   **Basic Categorization & Averaging:** It correctly uses a function to categorize pH and calculates the mean quality per category.
*   **ANOVA Test:** Performs the `f_oneway` test correctly.
*   **Basic Bar Plot:** Creates a bar plot with error bars.

**Weaknesses and Areas for Improvement:**

1.  **Incorrect pH Grouping (Categorization):**
    *   The task specifies ""Medium pH (3.2 to 3.4)"". The generated `categorize_ph` function uses `3.2 <= ph <= 3.4`, which is correct for inclusive boundaries.
    *   However, the ground truth uses `pd.cut` with `right=False` for bins `[0, 3.2, 3.4, np.inf]`. This means:
        *   Low pH: `[0, 3.2)`
        *   Medium pH: `[3.2, 3.4)`
        *   High pH: `[3.4, inf)`
    *   The generated code's definition of ""High pH"" is `> 3.4`, which aligns with `[3.4, inf)`.
    *   The definition of ""Low pH"" is `< 3.2`, which aligns with `[0, 3.2)`.
    *   The main discrepancy is ""Medium pH"". The ground truth defines it as `[3.2, 3.4)`, meaning values *up to but not including* 3.4. The generated code uses `3.2 <= ph <= 3.4`, meaning values *including* 3.4. This is a subtle but important difference that affects group assignments. The task instruction ""3.2 to 3.4"" is ambiguous, but `pd.cut` with `right=False` and specified bins `[0, 3.2, 3.4, np.inf]` is a more standard and robust way to handle such categorizations, and usually means the upper bound is exclusive.

2.  **Visualization Error Bars (Confidence Intervals):**
    *   The task explicitly asks for ""error bars representing 95% confidence intervals"".
    *   The generated code calculates `quality_std / np.sqrt(quality_counts)`, which is the **Standard Error of the Mean (SEM)**, not a 95% confidence interval.
    *   While SEM is a measure of variability of the mean, displaying SEM directly as error bars is not the same as a 95% CI. A 95% CI is typically calculated as `mean +/- (t-score * SEM)` or `mean +/- (1.96 * SEM)` for large samples. For smaller samples, the t-distribution factor (t-score) is crucial, as implemented in the ground truth. This is a significant omission.

3.  **Missing Error Handling:** The code assumes the `winequality-red.csv` file always exists. Robust code should include a `try-except` block for `FileNotFoundError`.

4.  **Reporting of Results:** While the average qualities and ANOVA results are printed, the ground truth provides a more structured and informative output, including the conclusion of the ANOVA test based on the p-value, and rounded values for better readability.

5.  **Code Structure and Readability:** The ground truth separates configuration, data loading, and each step with comments, which improves readability and maintainability. The generated code is more monolithic.

To improve the score, the generated code needs to exactly match the grouping logic (or at least provide a clear, consistent definition), implement proper 95% confidence intervals for error bars, and include basic error handling for file loading.",gemma-3-27b-it,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Parse 'date'
df['date'] = pd.to_datetime(df['date'])

# Select features
features = ['new_cases', 'ho...",6,"The generated code addresses most of the requirements but has significant issues in its data preprocessing pipeline which can lead to incorrect model training and evaluation.

**Strengths:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Model Training and Evaluation:** Both Linear Regression and Random Forest models are correctly trained and evaluated using MAE, MSE, and R-squared.
*   **Prints Metrics:** Evaluation metrics are printed as requested.
*   **Random State:** `random_state` is used for both `train_test_split` and `RandomForestRegressor` for reproducibility.
*   **Feature and Target Selection:** Correctly identifies the features and target variable.
*   **One-Hot Encoding:** Applies one-hot encoding to 'continent'.

**Areas for Improvement (Reasons for not higher score):**

1.  **Incorrect Parsing of Date:** The prompt explicitly states ""parse 'date'"" implicitly meaning it should be handled in `pd.read_csv` if possible or converted early. While `df['date'] = pd.to_datetime(df['date'])` is correct, the ground truth's `parse_dates=['date']` during loading is generally preferred for efficiency and robustness. This is a minor issue but worth noting for precision.

2.  **Order of Preprocessing (Critical):**
    *   **Scaling before Split:** The most significant flaw is that the code scales the *entire dataset* (`df[features]`) *before* splitting into training and testing sets. This leads to **data leakage**, as information from the test set influences the scaling parameters (mean and standard deviation) learned from the training set. Scaling should always happen *after* the train-test split, with the `fit_transform` applied only on the training set and `transform` applied on the test set.
    *   **Imputation before Encoding:** The prompt specifies to encode 'continent' and add it *to the features* (Step 4), implying `continent` is part of the features selection before encoding. The generated code handles missing values *before* one-hot encoding, applying median imputation to 'continent' if it were included in the `features` list. However, 'continent' is a categorical variable and should *not* be median imputed; its missing values should be handled appropriately (e.g., creating a 'missing' category during encoding, or dropping rows, though the prompt implies imputing all **selected** features unless explicitly excluded). The ground truth correctly excludes 'continent' from median imputation while including it in the features for later encoding.

3.  **Handling Missing Values (Minor Logical Flaw):**
    *   The prompt asks to handle missing values in ""selected features and target"". The generated code applies `fillna(median())` to `features + [target]`. While it correctly imputes selected numerical features and the target, it misses an important nuance: `continent` should *not* be imputed with a median as it's categorical. Although the `features` list initially passed to the loop does not contain 'continent', the ground truth's approach of explicitly listing numerical columns for median imputation and handling categorical 'continent' separately is more robust and prevents potential errors if 'continent' were added to `features` earlier. The generated code effectively skips 'continent' from median imputation by omitting it from the initial `features` list, which is okay, but its placement of median imputation *before* encoding steps could be less robust than the ground truth.

4.  **Feature List Management:**
    *   After one-hot encoding, the `features` list is rebuilt by removing 'continent' and adding the new 'continent' OHE columns. This is correct logic, but the ground truth's approach of performing OHE and then separating X and y directly from the encoded DataFrame (which implicitly includes the new OHE columns) is slightly cleaner and less prone to errors in managing the `features` list explicitly.

In summary, the most critical issue is the data leakage due to scaling before splitting, which violates fundamental machine learning best practices and would lead to an overly optimistic evaluation of model performance. The handling of categorical features' missing values and the order of preprocessing also show room for improvement compared to the robust ground truth.",gemma-3-27b-it,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
import statsmodels.formula.api as sm

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Handle missing values
cols_to_impute = ['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']
for col in cols_to_impute:
    df[col] = df[col].fillna(df[col].median())

# 1. ANOVA test for 'reproduction_rate' across continents
continents = ['Asia', 'Europe', 'North America']
...",6,"The generated code addresses most parts of the task but has several significant issues that prevent it from being fully correct and robust.

**Strengths:**
*   Correctly imports necessary libraries.
*   Handles missing values using median imputation as requested.
*   Attempts ANOVA and Pearson correlation as specified.
*   Includes a t-test and attempts to create the `high_death_rate_country` variable.
*   States conclusions based on p-values for all tests.

**Weaknesses and Areas for Improvement:**

1.  **Missing Null Hypothesis Statements:** The task explicitly asks to ""state the null hypothesis"" for each test. The generated code does not explicitly print these null hypotheses, which is a specific requirement.

2.  **ANOVA Data Preparation for Continents:**
    *   The task specifies ""Asia, Europe, North America only"". The generated code correctly filters for these.
    *   However, `stats.f_oneway` expects arrays of data for each group. The current `data` list could contain `NaN` values if `reproduction_rate` for a continent has `NaN`s, which might cause issues or lead to incorrect results if `f_oneway` doesn't handle `NaN`s gracefully (or drops rows with `NaN`s, which might not be what's intended for group comparison if only *some* values are missing). The ground truth specifically uses `.dropna()` on the per-continent `reproduction_rate` series before passing them to `f_oneway`. While the median imputation step should have handled most `NaN`s, it's good practice to ensure `NaN`s are not present in the columns used for statistical tests.

3.  **T-test Logic for `high_death_rate_country`:** This is the most critical flaw.
    *   **Incorrect `death_rate` calculation:** `df.groupby('country')['total_deaths'].last()` and `df.groupby('country')['population'].last()` will get the last *recorded* value for each country, but these two `.last()` calls are performed on separate Series and then divided. This does *not* necessarily ensure that the `total_deaths` and `population` values come from the *same latest date* for each country within the original `df`. The ground truth correctly sorts by date first, then groups and takes the `last()` entry for the entire row to ensure consistency (`df.sort_values(by=['country', 'date']).groupby('country').last()`).
    *   **Application of `death_rate_75th`:** The calculated `death_rate_75th` is based on the *latest entry per country*. However, the line `df['high_death_rate_country'] = df['total_deaths'] / df['population'] > death_rate_75th` attempts to apply this threshold to *every single row* in the original `df`, regardless of date. This means a country might be classified as 'high_death_rate' on an early date but not on a later date, or vice-versa, which contradicts the task requirement to base it on the ""latest entry per country"". The `high_death_rate_country` should be a *country-level attribute*, not a row-level attribute that changes with date. The t-test should then be performed on `median_age` grouped by this country-level attribute. The ground truth correctly derives this flag at the country level using the latest data and then uses `median_age` from those *same latest country entries* for the t-test.
    *   **Handling of `NaN`s/`inf` in `death_rate` calculation:** The division `total_deaths / population` can result in `inf` if `population` is 0. The ground truth handles this by replacing `inf` with `NaN` and then dropping `NaN`s, ensuring robust calculation of the 75th percentile. The generated code does not handle this, which could lead to errors or incorrect percentiles.

4.  **Overall Structure and Robustness:**
    *   No error handling for file loading (e.g., `FileNotFoundError`).
    *   Missing `ALPHA` constant for significance level. Using magic number `0.05` directly.
    *   The task specifies to include `statsmodels`, but it's imported and not used. It's not strictly necessary for this task, so it's a minor point, but it indicates potential unnecessary imports or design debt.

To improve, the generated code need to:
*   Explicitly state the null hypotheses.
*   Refine the `high_death_rate_country` derivation to correctly create a country-level flag based on the latest entries and handle potential division by zero.
*   Ensure that the data used for statistical tests (especially the t-test for `median_age`) is consistently drawn from the appropriate level (e.g., latest country data for the t-test).",gemma-3-27b-it,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select numerical columns
numerical_cols = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
df_numerical = df[numerical_cols]

# Calculate the correlation matrix
correlation_matrix = df_numerical.corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cma...",9,"The generated code correctly loads the dataset, selects the specified numerical columns, calculates the pairwise correlation matrix, and visualizes it as a heatmap. All core requirements are met. The code is concise and directly addresses the prompt.

Minor improvements could include:
- Adding comments for better readability, similar to the ground truth.
- Incorporating error handling (e.g., `try-except` blocks for `FileNotFoundError` or `KeyError` if columns are missing) for robustness, although the prompt didn't explicitly ask for it.
- Wrapping the logic in a function for better modularity and reusability, again not strictly required by the prompt but good practice.
- The `plt.title` is slightly less descriptive than the ground truth's.

Overall, it's a very good solution that directly answers the prompt's requirements.",gemma-3-27b-it,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

def analyze_card_rarity():
  """"""
  Loads the clash_royale_cards.csv dataset, groups by rarity, 
  and calculates/displays average elixirCost, Win Rate, and Usage for each rarity.
  """"""
  try:
    df = pd.read_csv(""clash_royale_cards.csv"")
  except FileNotFoundError:
    print(""Error: clash_royale_cards.csv not found. Make sure the file is in the current directory."")
    return

  grouped = df.groupby('rarity').agg({
      'elixirCost': 'mean',
      'Win Rate': 'me...",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, groups the data by `rarity`, and calculates the average `elixirCost`, `Win Rate`, and `Usage` for each rarity, then prints the result. The use of `.agg()` is an alternative and perfectly valid way to achieve the same result as the ground truth's direct `.mean()` on selected columns. The inclusion of `try-except` for `FileNotFoundError` and wrapping the logic in a function with `if __name__ == ""__main__"":` demonstrates good software engineering practices, though these were not explicitly requested by the prompt for this specific task. The only minor point is the unrequested function and error handling, but these don't detract from the correctness or quality of the solution for the core task.",gemma-3-27b-it,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Create the target column
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)

# Select features
features = ['elixirCost', 'Usage', 'rarity']
X = df[features]
y = df['is_high_win_rate']

# One-hot encode the 'rarity' column
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
rar...",9,"The generated code correctly addresses all requirements of the task. It loads the data, creates the `is_high_win_rate` target column, selects specified features, performs one-hot encoding on the `rarity` column, and splits the data into training and testing sets with the specified parameters.

The use of `OneHotEncoder` from `sklearn.preprocessing` is technically correct and explicitly imported, which aligns with typical data science workflows even if `pd.get_dummies` is often simpler for this specific case. The `sparse_output=False` and `handle_unknown='ignore'` are good practices for `OneHotEncoder`.

One minor point for improvement, though not a critical error, is that when concatenating the one-hot encoded `rarity_df` back to `X`, the `X` DataFrame might lose its original index if not handled carefully. In this specific case, since `X` was created from `df` and `rarity_df` is created based on the transformed `X[['rarity']]`, the indices should align implicitly due to the sequential processing. However, for robustness, especially if `X` had undergone filtering or reordering, resetting the index before concatenation or ensuring index alignment could prevent potential issues. The ground truth's use of `pd.get_dummies` directly on `X` avoids this potential indexing mismatch entirely.

Overall, the code is well-structured and performs the task effectively.",gemma-3-27b-it,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
iris = pd.read_csv(""iris.csv"")

# Define features
X = iris[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Define target
y = iris['Species']

# Encode categorical target variable
le = LabelEncoder()
y_encoded = le.fit_transform(y)

#...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task. It properly loads the data, defines features and target, encodes the target variable, splits the data with the specified parameters, trains the KNN classifier, makes predictions, and reports the accuracy.

The only minor point for improvement, preventing a perfect score, is the omission of the `stratify=y_encoded` parameter in the `train_test_split` function. While not explicitly asked for in the prompt's requirements section (only the ground truth explicitly listed it), it's a good practice for classification tasks, especially with imbalanced datasets, to ensure representative splits. However, for a simple benchmark of the core requirements, its absence is a minor detail. The generated code also lacks the error handling for `FileNotFoundError` in `pd.read_csv`, which was present in the ground truth, but this specific type of error handling wasn't explicitly asked for in the task description requirements either. Therefore, the code still largely fulfills the direct prompt instructions. Overall, it's a very good solution.",gemma-3-27b-it,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('iris.csv')

# Filter the data
versicolor = df[df['Species'] == 'Iris-versicolor']['PetalLengthCm']
virginica = df[df['Species'] == 'Iris-virginica']['PetalLengthCm']

# Define null and alternative hypotheses
# Null hypothesis (H0): There is no significant difference in the mean PetalLengthCm between Iris-versicolor and Iris-virginica.
# Alternative hypothesis (H1): There is a significant difference in the...",7,"The generated code correctly performs the core task of loading data, filtering, running the t-test, and interpreting the p-value. The imports are correct, and the structure is logical.

However, it falls short on completeness in a few areas compared to the prompt's explicit requirements:
- **Hypotheses Statement:** The prompt asked to ""Clearly state the null and alternative hypotheses for this test."" While the generated code includes comments for the hypotheses, it doesn't **print** them to the console as exemplified in the ground truth, which is a common practice for clearly reporting the hypotheses in analysis.
- **Reporting Detail:** The prompt asked to ""Clearly state the null and alternative hypotheses for this test,"" and subsequently listed ""Print the calculated t-statistic and the p-value."" While it prints the T-statistic and P-value, the ground truth's formatting and detailed output for hypothese and conclusions significantly enhance clarity and adherence to the prompt's request for clear statement. The conclusion is also quite brief (""Reject the null hypothesis."") compared to the ground truth's more expansive and informative conclusion about the statistical difference.
- **Robustness:** The generated code lacks error handling for `FileNotFoundError` or empty groups, which is a good practice for robust code, though not explicitly asked for in the prompt's core requirements. The ground truth includes this.

To get a higher score, the generated code should print the hypotheses before the test results and provide a more detailed interpretation of the conclusion, similar to the ground truth.",gemma-3-27b-it,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: iris.csv not found in the current directory."")
    exit()

# Features to plot
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

# Create box plots for each feature
for feature in features:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='Species', y=feature, data=df)
    plt.title(f'...",9,"The generated code correctly addresses all the core requirements of the task. It properly loads the `iris.csv` dataset, iterates through each specified numerical feature, and generates a box plot for each feature, grouped by the `Species` column. Each plot also has an appropriate title.

The code quality is good, with all necessary imports included and a clear, sequential logic for data loading and plotting. The `try-except` block for `FileNotFoundError` is a good practice for robust file handling.

One minor suggestion for improvement, as seen in the ground truth, would be to make the plot titles more descriptive (e.g., ""Box Plot of {feature} by Species"") and possibly add `plt.tight_layout()` for better plot aesthetics, though its absence doesn't hinder the functionality. The current titles are sufficient as per the prompt but could be more explicit. Also, adding `plt.grid()` could enhance readability. However, these are minor aesthetic considerations and do not detract from the correctness or completeness of the solution for the given task.",gemma-3-27b-it,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv(""heart.csv"")

# Select features
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']
X = data[features]
y = data['HeartDisease']

# Preprocess categorical features
encoder = OneHotEncoder(handle_unknown='igno...",8,"The generated code correctly performs all the required steps: loading the data, selecting specified features, one-hot encoding categorical features (`Sex`, `ChestPainType`), splitting data into training and testing sets with the correct ratio and `random_state`, training a Logistic Regression model, and reporting the accuracy.

Strong points:
- All required imports are present.
- Correct features and target are selected.
- One-hot encoding is applied correctly using `OneHotEncoder` and `pd.concat` to combine the encoded features with the numerical ones.
- Data splitting parameters are correct.
- Model training and prediction are executed as required.
- Accuracy is correctly calculated and printed.

Areas for minor improvement (to achieve a higher score):
- The ground truth uses `ColumnTransformer` and `Pipeline`, which is generally a more robust and cleaner way to handle preprocessing steps, especially when dealing with different transformations for different columns and ensuring consistent application across training and test data (e.g., preventing data leakage if scaling were involved). While the generated code's manual concatenation works here, the `Pipeline` approach is considered best practice in scikit-learn for production-ready code.
- The `max_iter` parameter for `LogisticRegression` was increased in the ground truth for potential convergence issues, although it's not strictly specified as a requirement in the prompt. Ignoring this is acceptable given the prompt.",gemma-3-27b-it,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv(""heart.csv"")

# Create groups based on sex
male_cholesterol = df[df['Sex'] == 'M']['Cholesterol']
female_cholesterol = df[df['Sex'] == 'F']['Cholesterol']

# Define null and alternative hypotheses
# Null hypothesis (H0): There is no significant difference in mean cholesterol levels between males and females.
# Alternative hypothesis (H1): There is a significant difference in mean cholesterol levels between...",8,"The generated code correctly performs an independent two-sample t-test as requested. It loads the data, separates the `Cholesterol` levels by `Sex`, performs the `ttest_ind`, and correctly interprets the p-value against the alpha level. The hypotheses are correctly stated as comments.

However, there are a few areas for improvement:
1.  **Robustness/Error Handling**: The ground truth includes robust error handling for `FileNotFoundError` and checks for the existence of required columns, which is good practice for real-world applications. The generated code lacks this.
2.  **Explicit Hypotheses Printing**: The ground truth explicitly prints the null and alternative hypotheses, which was a specific requirement (""Clearly state the null and alternative hypotheses""). The generated code only includes them as comments.
3.  **Handling Missing Values (`.dropna()`):** The ground truth uses `.dropna()` on the cholesterol series derived from the groups. While `ttest_ind` generally handles NaNs by default (skipping them), explicitly dropping them can make the data preparation step clearer and prevent potential issues with other statistical functions that might not handle NaNs gracefully. For this specific task and `ttest_ind`, it might not be strictly necessary, but it's often a good practice when preparing data for statistical analysis.
4.  **`equal_var` parameter**: The ground truth thoughtfully discusses and uses `equal_var=False` (Welch's t-test), which is generally more robust when population variances are unknown or unequal. The generated code uses the default `equal_var=True` (Student's t-test). While not explicitly forbidden, assuming equal variances without justification is less robust.

Overall, it's a good attempt that delivers the core functionality. The missing error handling and explicit printing of hypotheses are the main reasons it's not a perfect score.",gemma-3-27b-it,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Identify outliers using IQR for 'volatile acidity'
Q1_va = df['volatile acidity'].quantile(0.25)
Q3_va = df['volatile acidity'].quantile(0.75)
IQR_va = Q3_va - Q1_va
lower_bound_va = Q1_va - 1.5 * IQR_va
upper_bound_va = Q3_va + 1.5 * IQR_va

outliers_va = df[(df['volatile acidity'] < lower_bound_va) | (df['volatile acidity'] > upper_bound_va)]
num_outliers_va = len(outliers_va)

# Ide...",7,"The generated code correctly implements the core logic for identifying outliers using IQR, performing winsorization, and comparing statistics before and after treatment for the specified columns. It uses appropriate libraries (pandas, numpy).

However, there are several areas for improvement:
- **Hardcoding:** Instead of iterating through columns to treat outliers and report statistics, the code hardcodes the operations for 'volatile acidity' and 'residual sugar' separately. This makes the code less flexible and harder to extend to more columns. The ground truth uses a loop, which is a better practice.
- **Output Clarity/Formatting:** The printed output is less structured and clear compared to the ground truth. It just prints the numbers without clear headings or comparisons (e.g., ""Change in Mean"").
- **Error Handling:** There is no error handling for `FileNotFoundError` when loading the CSV, which could cause the script to crash if the file is missing.
- **Implicit Reporting:** The task stated to ""Report the number of outliers found for each column"". While the numbers are printed at the end, they could be reported immediately after identification for better flow, as done in the ground truth.

Overall, it gets the job done but lacks the robustness, modularity, and comprehensive output of the ground truth code.",gemma-3-27b-it,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('heart.csv')

# Group the data by ChestPainType and HeartDisease and count occurrences
grouped_data = df.groupby(['ChestPainType', 'HeartDisease']).size().unstack()

# Create the grouped bar chart
ax = grouped_data.plot(kind='bar', figsize=(10, 6))

# Set the title and labels
plt.title('Heart Disease Count by Chest Pain Type')
plt.xlabel('Chest Pain Type')
plt.ylabel('Count')
...",8,"The generated code correctly loads the dataset, creates a grouped bar chart showing the count of HeartDisease for each ChestPainType, and includes proper titles, labels, and a legend. It effectively addresses all the core requirements of the task.

**Strong points:**
*   Uses `pandas` for data loading and manipulation.
*   Uses `matplotlib.pyplot` for plotting, and `seaborn` is imported correctly, though not explicitly used for the plot generation in the same way `countplot` would be.
*   The `groupby().size().unstack().plot(kind='bar')` approach is a valid and concise way to generate a grouped bar chart from aggregated data.
*   Includes a title, x-axis label, y-axis label, and a legend.
*   `plt.tight_layout()` is used for good practice.

**Areas for improvement (to reach a 9 or 10):**
*   **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and other potential issues when loading the CSV, which is a good practice for robust code. The generated code lacks this.
*   **Encapsulation/Modularity:** The ground truth wraps the logic in a function, `create_heart_disease_chest_pain_grouped_bar_chart`, making the code reusable and testable. The generated code is a script-like execution without a function.
*   **Clarity of Legend Labels:** While `labels=['No Disease (0)', 'Disease (1)']` works, mapping the numerical values (0 and 1) to more descriptive string labels directly in the DataFrame for the `hue` or `columns` would make the plot generation more intuitive and align better with the `seaborn.countplot` approach of the ground truth (e.g., using `df['HeartDisease_Label']` as seen in ground truth). The current method requires manual specification of legend labels which can be prone to error if the order changes or more categories are added.
*   **Explicit Plotting Library Choice:** While it works, the code uses `groupby().size().unstack().plot(kind='bar')` which is a pandas wrapper around matplotlib, rather than directly using `seaborn.countplot` as the ground truth does, which is often preferred for categorical plots of counts. While `seaborn` is imported, its functionality isn't leveraged for the plot itself (e.g., for direct counting and plotting like `sns.countplot`).
*   **ChestPainType order:** The ground truth explicitly orders the `ChestPainType` categories, which ensures consistent plotting order (TA, ATA, NAP, ASY). The generated code relies on the default alphabetical or DataFrame order, which may not match what's expected for this domain.",gemma-3-27b-it,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,".```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv(""spotify_2023.csv"")
except FileNotFoundError:
    print(""Error: spotify_2023.csv not found in the current directory."")
    exit()

# Clean the 'in_shazam_charts' column
df['in_shazam_charts'] = df['in_shazam_charts'].str.replace(',', '').astype(float, errors='coerce')
nan_count = df['in_shazam_charts'].isnull().sum()
print(f""Number of NaNs in 'in_shazam_charts' after cleaning: {nan_count}"")

# Group data by 'key' and 'm...",10,"The generated code correctly loads the CSV, handles `FileNotFoundError`, and accurately cleans the `in_shazam_charts` column by removing commas and converting it to numeric, coercing errors to NaN, and reporting the NaN count. It then correctly groups the data by `key` and `mode` and calculates the average `danceability_%` and `valence_%` for each group. The display of results is also appropriate. The `astype(float, errors='coerce')` is a concise and effective way to achieve the cleaning and conversion, making it slightly more efficient than the ground truth's two-step process of `astype(str).str.replace` then `pd.to_numeric`. The use of named aggregations (`avg_danceability`, `avg_valence`) improves readability of the output index compared to the ground truth which retains original column names. Overall, excellent solution.",gemma-3-27b-it,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Ensure 'streams' is numeric and handle missing values
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df.dropna(subset=['streams', 'in_spotify_playlists', 'energy_%'], inplace=True)

# Select features (X) and target (y)
X = df[['in_spot...",9,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, correctly handles the `streams` column by coercing to numeric and dropping NaNs, selects the specified features and target, splits the data, trains a Linear Regression model, and reports the R2 and MSE scores.

Strong points:
- All required libraries are imported.
- `pd.to_numeric` with `errors='coerce'` is used correctly.
- `dropna` is correctly applied to the specified subset of columns, ensuring both `streams` and predictor columns are free of NaNs before model training.
- The `train_test_split` parameters (`test_size=0.2`, `random_state=42`) are correct.
- `LinearRegression` model is initialized and fitted correctly.
- Both R2 and MSE metrics are calculated and printed.

Area for slight improvement (not critical for correctness, but good practice):
- The ground truth code includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust. The generated code lacks this, which means it would crash if the file isn't found. This is a minor omission given the prompt assumed the file would be in the directory.
- The print statements in the ground truth code provide more context and better formatting for the output, which is generally good practice in data science scripts for user clarity.

Overall, the core functionality is perfectly executed. The missing `try-except` block for file loading is the only minor point preventing a perfect score.",gemma-3-27b-it,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

def feature_engineering(csv_file):
    """"""
    Performs feature engineering on the spotify_2023.csv dataset.

    Args:
        csv_file (str): The path to the CSV file.

    Returns:
        pandas.DataFrame: The DataFrame with the engineered features.
    """"""
    df = pd.read_csv(csv_file)

    try:
        df['release_date'] = pd.to_datetime(df[['released_year', 'released_month', 'released_day']], errors='coerce')
    except KeyError:
        print(""Error: 'rele...",7,"The generated code correctly loads the dataset and calculates `days_since_release`. However, the calculation of `release_date` directly from the list of columns `df[['released_year', 'released_month', 'released_day']]` is not the standard or most robust way to create a datetime object from separate year, month, and day columns using `pd.to_datetime`. While this might work in some specific pandas versions or contexts, it's not explicitly documented for creating a date from a DataFrame of components this way. The standard approach involves creating a string column first or using `pd.to_datetime` with a dictionary of components (if available, which is not the case here).

Also, the `astype(int)` for `days_since_release` will raise an error if there are NaT values in `release_date` (which result in `NaN` after `.dt.days`), as `NaN` cannot be directly converted to `int`. The prompt specifically asked for NaT handling for invalid dates and for `days_since_release` to be an integer, which implies the use of a nullable integer type like `Int64` to accommodate `NaN` values resulting from `NaT` in `release_date`.

Finally, the function returns only the head of the DataFrame, instead of the full DataFrame with the engineered features, which deviates from typical feature engineering function design, and the display logic is coupled with the function's return. The prompt asked to display specific columns for the first 5 rows, implying this should be done after the full feature engineering process.",gemma-3-27b-it,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Video views' and 'Likes' columns
for col in ['Video views', 'Likes']:
    df[col] = df[col].str.replace(',', '').astype(float, errors='ignore')
    df[col] = df[col].fillna(0)

# Fill missing values in 'Category' column
df['Category'] = df['Category'].fillna('Unknown')

# Group by 'Category' and calculate sums
category_sums = df.groupby('Category')[['Video views', 'Likes']].sum()

# Display the ...",8,"The generated code correctly performs all the required steps: loading the data, cleaning the 'Video views' and 'Likes' columns (removing commas, converting to numeric, handling failed conversions by filling with 0), filling missing 'Category' values with 'Unknown', and then grouping to calculate and display the sums.

The use of a loop for cleaning 'Video views' and 'Likes' is concise. The `astype(float, errors='ignore')` is a good attempt to handle conversion errors, though `pd.to_numeric(errors='coerce')` is generally more robust for this specific task as `astype` with `'ignore'` doesn't actually convert to NaN but leaves the original type if it fails which can cause subsequent errors. However, given the `str.replace(',', '')` preceding it, if there were non-numeric strings other than commas (e.g., 'N/A' or 'bad data'), `astype(float)` would raise an error, and `errors='ignore'` would suppress it but not convert to NaN, which could lead to issues with `fillna(0)`. For `pd.to_numeric`, `errors='coerce'` explicitly converts non-numeric values to NaN, which is then correctly handled by `fillna(0)`. In the context of `str.replace` first, it's likely fine as only pure non-numeric or empty strings would remain.

The main difference from the ground truth is the error handling around column existence, which the ground truth explicitly includes with `if in df.columns:` checks and `try-except` blocks for file not found and general exceptions. While the task description didn't explicitly ask for robust error handling beyond the data cleaning, the ground truth's inclusion of it makes it more production-ready. However, for the core data manipulation task, the generated code is correct and efficient.

Strong points:
- Concise and readable code.
- Correctly implements all transformation logic.
- Proper use of pandas functions for cleaning and aggregation.
- Handles missing values as specified.",gemma-3-27b-it,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,".

```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the 'Likes' column
df['Likes'] = df['Likes'].str.replace(',', '')
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

# Fill missing values in 'Category'
df['Category'] = df['Category'].fillna('Unknown')

# Create two groups based on 'Category'
music_likes = df[df['Category'] == 'Music']['Likes']
sports_likes = df[df['Category'...",7,"The generated code correctly performs the requested cleaning and t-test. It loads the dataset, cleans the 'Likes' column by removing commas, converting to numeric, and filling NaNs with 0. It also correctly fills missing 'Category' values. The t-test is performed on the correct groups (`Music` and `Sports` `Likes`). It prints the t-statistic and p-value and provides an interpretation based on alpha = 0.05.

However, it missed a few explicit requirements:
1.  **Hypotheses Statement:** The prompt explicitly asked to ""Clearly state the null and alternative hypotheses for this test."" While the generated code includes comments about the hypotheses, it does not *print* them to the console as requested by ""Clearly state..."".
2.  **Robustness for 'Likes' Cleaning:** The ground truth converts `df['Likes'].astype(str)` before `str.replace`. This makes the code more robust if the `Likes` column initially contains non-string types that might cause errors with `.str.replace()` directly. The generated code directly applies `.str.replace()`, which could fail if `Likes` contains numeric or NaN values before string conversion, although `pd.to_numeric(errors='coerce')` usually handles subsequent steps fine.
3.  **Specific Interpretation Wording:** The prompt asked to ""state whether to reject the null hypothesis"" and implicitly requested a more detailed conclusion beyond just ""Reject the null hypothesis."" The ground truth provides a more complete interpretation: ""Conclusion: There is a statistically significant difference..."" or ""Conclusion: There is no statistically significant difference..."".
4.  **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code assumes the file will always be present.
5.  **`equal_var` in `ttest_ind`:** The ground truth uses `equal_var=False` for Welch's t-test, which is generally recommended for independent samples when equal variances cannot be assumed. While not explicitly stated in the prompt, it's a good practice for real-world data science tasks.

Overall, it's a good attempt that covers the core logic but lacks some specific details and best practices for robustness and explicit output as per the requirements.",gemma-3-27b-it,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""youtube_trendings.csv"")

# Clean and convert Likes
df['Likes'] = df['Likes'].str.replace(',', '').astype(float, errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

# Clean and convert Dislikes
df['Dislikes'] = df['Dislikes'].str.replace(',', '').astype(float, errors='coerce')
df['Dislikes'] = df['Dislikes'].fillna(0)

# Create TotalEngagement
df['TotalEngagement'] = df['Likes'] + df['Dislikes']

# Create LikeDislikeRatio
df['L...",8,"The generated code correctly addresses all the core requirements of the task. It loads the dataset, cleans `Likes` and `Dislikes` columns by removing commas, converting to numeric with error handling, and filling NaNs with 0. It then correctly creates `TotalEngagement` and `LikeDislikeRatio` columns, and finally displays the requested `Video`, `TotalEngagement`, and `LikeDislikeRatio` columns.

Strengths:
- **Concise Cleaning:** The cleaning steps for `Likes` and `Dislikes` (replace, convert, fillna) are correctly implemented in a compact manner.
- **Correct Calculations:** `TotalEngagement` and `LikeDislikeRatio` are calculated accurately according to the problem description, including the '+1' to avoid division by zero.
- **Correct Output:** The final output displays the required columns.

Areas for Improvement (to achieve a perfect score):
- **Robust File Loading:** The ground truth includes `try-except` blocks for file loading, which is good practice for real-world applications to handle `FileNotFoundError` or other loading issues gracefully. The generated code assumes the file will always be present. While not explicitly asked for robust error handling, it improves overall code quality in a real-world scenario.
- **Readability of Cleaning Step:** While `astype(float, errors='coerce')` works, `pd.to_numeric(..., errors='coerce')` is often preferred for clarity when coercing errors, especially when the initial type might not be directly convertible to a string, or if there's an intermediate step like `.astype(str)` that the ground truth uses for robustness. However, for this specific problem, `astype(float, errors='coerce')` after `str.replace` works fine.
- **Clarity of `astype(str)`:** The ground truth explicitly converts `Likes` and `Dislikes` to `str` before replacement. While Pandas often handles this implicitly for string operations on object columns, being explicit can prevent subtle issues if a column contains mixed types or non-string numbers.

Overall, the generated code is highly functional and meets all the specific requirements of the prompt. The suggested improvements are mainly for robustness and best practices in a production environment rather than correctness for the given task.",gemma-3-27b-it,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean Launched Price (USA)
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', '').astype(float, errors='coerce')

# Clean Screen Size
df['Screen Size'] = df['Screen Size'].str.replace(' inches', '').astype(float, errors='coerce')

# Group by Company Name and Launched Year
grouped_data = df.groupby(['Company Name', 'Launched Year'])

# Calculate the average of La...",9,"The generated code correctly loads the dataset, cleans both the `Launched Price (USA)` and `Screen Size` columns by removing specified strings and converting to numeric, handles conversion errors by setting to NaN, groups by `Company Name` and `Launched Year`, and calculates the average of the cleaned columns. The output format is also correct as it directly prints the aggregated DataFrame.

Strengths:
- Concise and efficient cleaning steps using `.str.replace()` chaining and `astype(float, errors='coerce')`.
- Correct grouping and aggregation logic.
- All core requirements of the task are met.

Areas for minor improvement (not critical enough to reduce score significantly for this task, but good practice):
- The output column names for the aggregated data remain 'Launched Price (USA)' and 'Screen Size'. While technically correct as these are the columns being averaged, the ground truth code used more descriptive names like 'Average_Launched_Price_USD' and 'Average_Screen_Size_inches' which can improve readability in the final output. The generated code implicitly renames the columns implicitly when it does `mean()`.
- Error handling for file loading (e.g., `FileNotFoundError`) is present in the ground truth but not in the generated code, which is good practice for robust scripts, though not explicitly asked as a requirement beyond ""assume the CSV file is in the current directory.""
- The ground truth explicitly uses `.reset_index()` to convert the grouped keys back into columns, which is a common practice when you want the grouping keys as regular columns in the final DataFrame. The generated code also produces a similar output structure (multi-index) which implicitly satisfies the ""display the resulting aggregated data"" requirement.

Overall, the code is very effective and directly addresses the core problem well.",gemma-3-27b-it,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

def clean_camera_specs(camera_string):
    match = re.search(r'\d+', str(camera_string))
    if match:
        return int(match.group(0))
    else:
        return 0

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean camera columns
df['Cleaned Front Camera'] = df['Front Camera'].apply(clean_camera_specs)
df['Cleaned Back Camera'] = df['Back Camera'].apply(clean_camera_specs)

# Create Total Camera MP feature
df['Total Camera MP'] = df['Clean...",9,"The generated code successfully completes all aspects of the task, including loading the data, correctly cleaning camera specifications using regex to extract the first digit sequence, handling cases where no digits are found (returning 0), creating the 'Total Camera MP' feature, and displaying the requested columns for the first 10 rows. The code is concise and correct.

Potential minor improvements (not critical for score as it directly fulfilled requirements):
- The `clean_camera_specs` function could benefit from a docstring for better readability and maintainability, similar to the ground truth.
- Explicitly handling `NaN` values in the `clean_camera_specs` function might be slightly more robust, though `str(camera_string)` in this case implicitly handles `NaN` by converting it to the string 'nan', for which `re.search(r'\d+', 'nan')` correctly returns `None`, leading to a 0.
- The ground truth code includes print statements for logging progress and error handling for `FileNotFoundError`, which are good practices for robust scripts but were not explicitly required by the prompt.",gemma-3-27b-it,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Data Cleaning
df['RAM'] = df['RAM'].str.replace('GB', '').astype(float, errors='coerce')
df['Battery Capacity'] = df['Battery Capacity'].str.replace('mAh', '').astype(float, errors='coerce')
df['Screen Size'] = df['Screen Size'].str.replace(' inches...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, cleans the specified columns by removing non-numeric characters and converting them to numeric, handles conversion errors by coercing to NaN and then dropping rows with NaNs in those columns. It correctly defines features and target, splits the data, trains a Linear Regression model, and evaluates it using R2 and MAE.

Strong points:
- All required imports are present.
- Column cleaning is correctly implemented using `str.replace` and `astype(float, errors='coerce')`.
- NaN handling by dropping rows on the specified subset of columns is correct.
- Data splitting, model training, and evaluation are performed as specified.
- The output format for R2 and MAE is clear.

Minor Improvement:
- The generated code would be slightly more robust if it included a `try-except` block for `pd.read_csv` to handle a `FileNotFoundError`, similar to the ground truth. This is a good practice for real-world applications but doesn't detract from the core logic of the data science task.
- The `astype(float, errors='coerce')` implicitly removes any leading/trailing whitespace before conversion, but an explicit `str.strip()` before `astype` might be slightly more robust to various malformed strings, though for this specific task, it's likely not critical given the provided patterns.",gemma-3-27b-it,LLM_judge_config
M4,medium,Hypothesis Testing,,,Error,Failed to generate code,gemma-3-27b-it,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Group by Year and calculate average Rating and Runtime
yearly_data = df.groupby('Year').agg({'Rating': 'mean', 'Runtime': 'mean'})

# Create the line plot
plt.figure(figsize=(12, 6))
plt.plot(yearly_data.index, yearly_data['Rating'], marker='o')

# Add title and labels
plt.title('Average Rating of Top 250 IMDb Movies Over Years')
plt.xlabel('Year')
plt.ylabel('Average Rating'...",6,"The generated code correctly addresses most of the requirements: it loads the CSV, groups by `Year`, calculates average `Rating` and `Runtime`, and generates a line plot of average `Rating` over `Year` with a title and labels. However, it lacks robustness in data handling, specifically regarding the 'Year' and 'Runtime' columns. The ground truth code includes essential data cleaning steps (e.g., extracting year from strings like '(2000)', converting 'Runtime' from string to int, handling potential non-numeric values or NaNs in 'Rating'). Without these cleaning steps, the generated code is brittle and might fail or produce incorrect results if the 'Year' or 'Runtime' columns are not perfectly clean numbers, which is often the case in real-world datasets. This significantly impacts its reliability. To improve, the generated code should incorporate data cleaning and type conversion for relevant columns. Additionally, while minor, the Ground Truth also includes error handling for file not found and missing columns, which adds to the robustness of the solution.",gemma-3-27b-it,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create two groups based on the 'Year' column
group1 = df[df['Year'] < 1990]['Rating']
group2 = df[df['Year'] >= 1990]['Rating']

# Define the null and alternative hypotheses
# Null hypothesis: There is no significant difference in the mean rating between movies released before 1990 and those released in or after 1990.
# Alternative hypothesis: There is a significant difference in the...",7,"The generated code correctly performs the core task of loading data, splitting into groups, and running a t-test. It also correctly interprets the p-value against the alpha level.

However, there are several areas where it falls short of the ground truth and the full requirements:
1.  **Hypotheses Statement:** The hypotheses are commented out rather than printed as a clear statement, which was explicitly requested (""Clearly state the null and alternative hypotheses"").
2.  **Robustness:** The code lacks error handling for `FileNotFoundError` when loading the CSV. It also doesn't explicitly convert 'Year' and 'Rating' columns to numeric types or handle potential `NaN` values, which can lead to errors or incorrect results if the data types aren't as expected in the CSV.
3.  **Clarity of Output:** While it prints the t-statistic and p-value, the output is less verbose and user-friendly compared to the ground truth. The interpretation also lacks the full conclusion based on the rejection or failure to reject the null hypothesis, i.e., what that means for the difference in movie ratings.
4.  **Implicit `equal_var`:** The `ttest_ind` function defaults to `equal_var=True`. While this might be a reasonable assumption in some cases, the ground truth explicitly mentions and sets it, demonstrating a more thorough understanding of the statistical test's parameters.

To improve, the code should:
*   Print the hypotheses clearly.
*   Add error handling for file not found and data type conversion.
*   Make the output interpretation more detailed and conclusive.
*   Consider explicitly setting `equal_var` in `ttest_ind`.",gemma-3-27b-it,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create the Decade column
def get_decade(year):
    decade = (year // 10) * 10
    return f'{decade}s'

df['Decade'] = df['Year'].apply(get_decade)

# Display the first 10 movies with Title, Year, and Decade
print(df[['Title', 'Year', 'Decade']].head(10))
```",9,"The generated code correctly performs all the requested tasks: loading the CSV, creating the `Decade` column by categorizing the `Year`, and displaying the specified columns for the first 10 rows. The use of `apply` with a helper function `get_decade` is a clear and effective way to achieve the decade categorization.

**Strong points:**
*   Correctly loads the CSV.
*   Correctly implements the decade categorization logic (`(year // 10) * 10`).
*   Successfully displays the requested columns (`Title`, `Year`, `Decade`) for the first 10 rows.
*   Properly imports pandas.

**Areas for minor improvement (preventing a perfect 10):**
*   **Error Handling:** The code lacks error handling for `FileNotFoundError` or other potential issues (e.g., if the 'Year' column is not found or contains non-numeric data, though for this specific dataset, 'Year' is generally clean). The ground truth includes comprehensive error handling.
*   **Encapsulation/Modularity:** While for this small script it's not strictly necessary, defining functions for loading and feature engineering (as in the ground truth) would improve reusability and maintainability for larger projects.
*   **Output Formatting:** The output `print(df[['Title', 'Year', 'Decade']].head(10))` includes the DataFrame index and a default header, which is standard. The ground truth explicitly uses `.to_string(index=False)` for cleaner output as presented in the problem description, without the index. This is a minor stylistic difference.",gemma-3-27b-it,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Impute missing numerical values with the mean
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numerical_cols:
    df[col] = df[col].fillna(df[col].mean())

# Impute missing categorical values with the mode
df['sex'] = df['sex'].fillna(df['sex'].mode()[0])

# Group by species and sex and calculate the average body mass
grouped_data = df.groupby(['species', ...",9,"The generated code correctly performs all the required steps: loading the dataset, imputing missing numerical values with their respective means, imputing missing categorical 'sex' values with the mode, grouping by 'species' and 'sex', and calculating the average 'body_mass_g'. The code is concise and directly addresses the prompt.

**Strong points:**
*   Correctly uses `fillna()` with `mean()` for numerical columns.
*   Correctly uses `fillna()` with `mode()[0]` for the categorical 'sex' column.
*   Correctly performs `groupby()` on 'species' and 'sex' and calculates the `mean()` of 'body_mass_g'.
*   Includes `pandas` import.
*   Assumes the CSV is in the current directory as per the prompt.

**Areas for minor improvement:**
*   Lack of error handling (e.g., `FileNotFoundError` for the CSV, or `KeyError` if columns are missing). While not explicitly requested to ""handle errors,"" robust code often includes this. The prompt asked for ""only the Python code without explanations"", and robust error handling often goes beyond basic functionality. Given the constraint, this is a very minor point.
*   The ground truth code defines constants for column names and paths, which can improve readability and maintainability for larger projects, but for this specific task, direct string usage is acceptable.",gemma-3-27b-it,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the data
df_filtered = df[df['species'].isin(['Adelie', 'Gentoo'])]

# Remove rows with missing values
df_filtered = df_filtered.dropna(subset=['flipper_length_mm', 'species'])

# Separate data for each species
adelie_flipper_lengths = df_filtered[df_filtered['species'] == 'Adelie']['flipper_length_mm']
gentoo_flipper_lengths = df_filtered[df_filtered['species'] == 'Gentoo']['flip...",8,"The generated code correctly performs all the required steps: loading the data, filtering by species, handling missing values, performing the t-test, printing results, and interpreting the p-value. The imports are correct, and the structure is logical.

Areas for improvement:
1.  **Hypotheses Statement:** While the hypotheses are stated in comments, the prompt asked to ""Clearly state the null and alternative hypotheses for this test."" The Ground Truth code explicitly prints them, which is a better way to show they are ""stated.""
2.  **Dataset Loading Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and other general errors when loading the CSV, which makes the code more robust. The generated code lacks this.
3.  **Explicit `equal_var` in `ttest_ind`:** `scipy.stats.ttest_ind` by default assumes `equal_var=True`. While not explicitly stated in the prompt whether to assume equal or unequal variances, in real-world scenarios, it's often safer to use `equal_var=False` (Welch's t-test) unless equality of variances is confirmed. The Ground Truth makes a note on this and chooses `equal_var=False`. While not a correctness issue for this specific problem (as `equal_var=True` is the default), it's a point of consideration for robustness.
4.  **Clarity of Output:** The ground truth provides more descriptive print statements for each step (e.g., dataset shape, rows removed, clear labels for t-statistic and p-value), which enhances readability and understanding of the execution flow.
5.  **Edge Case Handling:** The ground truth includes checks to ensure there's enough data in each group after filtering and NA removal to perform the t-test (i.e., not empty, and at least 2 data points). This makes the code more robust against unexpected data conditions.

Overall, it's a very good attempt that fulfills most of the requirements accurately. The identified improvements primarily relate to robustness, clarity of output, and a more explicit adherence to the ""clearly state"" requirement.",gemma-3-27b-it,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv(""penguins.csv"")

# Select features
X = df[[""culmen_length_mm"", ""culmen_depth_mm"", ""flipper_length_mm"", ""body_mass_g""]]
y = df[""species""]

# Drop rows with missing values
df = df.dropna(subset=[""species"", ""culmen_length_mm"", ""culmen_depth_...",8,"The generated code correctly implements all the core requirements of the task: loading the dataset, selecting features and target, handling missing values, encoding the target, splitting the data, training a Logistic Regression model, and reporting accuracy.

Strong points:
- All required libraries are imported.
- Correct features and target are selected.
- Missing value handling is correct for the specified columns.
- `LabelEncoder` is used correctly for the target variable.
- `train_test_split` is used with the correct `test_size` and `random_state`.
- `LogisticRegression` is initialized with `max_iter=1000`.
- Accuracy is calculated and printed.

Areas for improvement (minor):
- The `X` and `y` assignment happens twice: initially before dropping NAs, and then again after dropping NAs. While it works, it's slightly redundant. A more efficient approach would be to select `X` and `y` *after* the `df.dropna` operation.
- The `df = df.dropna(...)` command modifies the DataFrame in place (or rather, reassigns `df` to the cleaned version), which is fine but sometimes users prefer to create a new DataFrame for
  cleaned data (`df_cleaned = df.dropna(...)`) to keep the original intact, especially in more complex pipelines. For this task, it's acceptable.
- Error handling for `FileNotFoundError` is missing, which was present in the ground truth. While not explicitly asked for, it's good practice for robust code.
- No print statements indicating progress or confirming steps, which are helpful for debugging and understanding what the code is doing (as seen in the ground truth).

Overall, the code is functionally correct and meets all explicit requirements for the core data science task. The minor points are related to code robustness and user experience rather than functional correctness for the given task.",gemma-3-27b-it,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows with missing values in 'body_mass_g' or 'species'
df = df.dropna(subset=['body_mass_g', 'species'])

# Calculate the average body mass for each species
average_mass = df.groupby('species')['body_mass_g'].mean()

# Create a bar plot
plt.figure(figsize=(8, 6))  # Adjust figure size for better readability
sns.barplot(x=average_mass.index, y=average_ma...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, handles NAs in the specified columns by dropping rows, calculates the average body mass per species, and generates a bar plot with a title and labeled axes.

Strong points:
- **Correctness:** The code performs all the required data manipulations and plotting accurately.
- **Conciseness:** It is well-written and to the point, avoiding unnecessary complexity.
- **Clarity:** The variable names are clear, and the steps follow a logical progression.
- **Plotting:** The use of `sns.barplot` is appropriate, and the plot includes a title, labeled axes, and rotated x-axis labels for readability, demonstrating good plotting practices.
- **NA handling:** Correctly uses `dropna(subset=[...])` for targeted NA removal.

Areas for minor improvement (to reach a perfect 10):
- **Error handling for file loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust. The generated code assumes the file will always be present. While not explicitly asked in the prompt, it's good practice for production-ready code.
- **Consistency in `ylabel`:** The ground truth used `TARGET_COLUMN.replace(""_"", "" "").title()` for dynamic and clean axis labels. The generated code hardcodes ""Average Body Mass (g)"", which works but is less flexible if the target column name changes.
- **Comments/Docstrings:** While readable, adding a few more comments to explain each major step (load, clean, calculate, plot) would further enhance readability for others less familiar with the code.",gemma-3-27b-it,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows with NA values in specified columns
df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'], inplace=True)

# Create the 'culmen_area' feature
df['culmen_area'] = df['culmen_length_mm'] * df['culmen_depth_mm']

# Calculate the correlation matrix
correlation_matrix = df[['culmen_length_mm', 'culmen_depth_mm', 'fl...",8,"The generated code correctly addresses all requirements of the task. It successfully loads the dataset, drops NA values from the specified columns, creates the `culmen_area` feature, calculates the Pearson correlation matrix for the specified columns, and visualizes it as an annotated heatmap with the correct title. The use of `inplace=True` for dropping NAs is commonly seen, though creating a new DataFrame (as in the ground truth) can sometimes be safer to avoid `SettingWithCopyWarning` in more complex chained operations, but for this simple case, it's acceptable. The code also manages imports correctly and is well-structured.

Areas for minor improvement (not critical for functionality but for robustness and best practices):
1.  **Error Handling for File Not Found:** The generated code lacks a `try-except` block for `FileNotFoundError` when loading the CSV, which is present in the ground truth. This makes the code less robust to missing data files.
2.  **`fmt="".2f""` in heatmap:** The ground truth includes `fmt="".2f""` in the `sns.heatmap` call, which formats the annotations to two decimal places, making the visualization cleaner. While not strictly required by the prompt, it's a good practice for correlation matrices.
3.  **`plt.tight_layout()` and rotation:** The ground truth also adds `plt.tight_layout()` and rotates x-axis labels, which can improve readability, especially if column names are long.

Despite these minor points, the core functionality and output are correct and complete as per the task description.",gemma-3-27b-it,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""penguins.csv"")

# Select features
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
X = df[features]
y = df['species']

# Impute numerical features with the mean
X['culmen_length_mm'] = X['culmen_length_mm'].fillna(X['culmen_length_mm'].mean())
X['culmen_depth_mm'] = X['culmen_depth_mm'].fillna(X['culmen_depth_mm'].mean())
X['flipper_length_mm'] = X['flipper_length_mm'].filln...",9,"The generated code correctly performs all the specified steps: loading data, selecting features, imputing numerical NAs with the mean, imputing and mapping the 'sex' column, and one-hot encoding the 'island' column. It also correctly prints the shape and head of the resulting feature matrix.

Strengths:
- All required transformations are applied correctly.
- The use of `fillna` and `map` for the 'sex' column, and `get_dummies` for 'island' is appropriate and efficient.
- The target variable `y` is correctly separated, although not further used for training.

Areas for Improvement (minor):
- The `SettingWithCopyWarning` might occur when modifying `X` (e.g., `X['culmen_length_mm'] = X['culmen_length_mm'].fillna(...)`). While it often works, it's best practice to explicitly create a copy using `.copy()` during feature selection to avoid this, e.g., `X = df[features].copy()`.
- The code could benefit from comments explaining specific steps, although the task description was clear enough that it's understandable without them.
- Error handling for file not found (though the ground truth included it, the prompt didn't strictly require it for the generated code but it's good practice).",gemma-3-27b-it,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST0...",9,"The generated code correctly performs all the required operations: DataFrame creation, calculation of total spending per customer, identification of top 3 customers, and calculation of total quantity per category.

**Strengths:**
- Correctly uses pandas for DataFrame creation and manipulation.
- Accurately calculates `Total_Spending` per row.
- Correctly groups by `CustomerID` to sum spending.
- Correctly identifies top 3 customers using `sort_values` and `head`.
- Correctly groups by `Category` to sum quantity.
- All necessary imports are present.

**Areas for minor improvement:**
- While the task specified 'printing' the customer ID and total spending for the top 3, the generated code prints the pandas Series directly. The ground truth code iterates through the Series to print a more descriptive string for each customer, which could be considered slightly more user-friendly. However, the Series output is clear and contains the required information.
- The generated code lacks explicit print statements indicating what each section of output represents, unlike the ground truth which uses descriptive print statements (e.g., ""--- Creating DataFrame ---""). This is a minor aesthetic point and doesn't affect correctness.
- The problem statement asks to calculate the total spending for each 'CustomerID' (sum of 'Quantity' * 'Price_USD') which implies 'total spending' is the sum of (quantity * price). The generated code performs `df['Total_Spending'] = df['Quantity'] * df['Price_USD']` first, which is the *line item total*, and then sums this for each customer. This is the correct approach to achieve the desired result, but the column name `Total_Spending` could be misleading as it represents the spending *per line item*, not the total spending *per customer* until it's summed. However, this is a very minor detail and the logic is sound.",gemma-3-27b-it,LLM_judge_config
M5,medium,Classification,,,Error,Failed to generate code,gemma-3-27b-it,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']),
        'Ticker': ['TECHCO']*8,
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
        'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0, 157.0, 154...",10,"The generated code is functionally identical to the ground truth code and achieves all requirements of the task. It correctly creates the DataFrame, sets the 'Date' as the index, calculates the 3-day Simple Moving Average for 'Close' price, calculates the percentage change in 'Volume', and handles NaN values appropriately for both calculations. The code is clean, well-structured, and directly addresses all parts of the prompt. The use of `df.set_index('Date')` instead of `df.set_index('Date', inplace=True)` is also perfectly fine and idiomatic.",gemma-3-27b-it,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'], 
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'], 
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91], 
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89], 
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]}

df = pd.DataFrame(dat...",9,"The generated code is highly correct and complete, addressing all parts of the task. It successfully creates the DataFrame, calculates the 'Score_Improvement' column, groups by subject to calculate average improvement, and filters for students meeting the specified criteria.

Strengths:
- Correctly imports pandas.
- Precisely creates the DataFrame as specified.
- Accurately calculates the 'Score_Improvement' column.
- Correctly groups by 'Subject' and calculates the mean of 'Score_Improvement'.
- Accurately applies both conditions (`Final_Score > 90` and `Attendance_Rate >= 0.95`) for student identification.

Area for minor improvement:
- While the output for the identified student IDs (`high_achievers`) is correct and pandas Series are readable, the ground truth explicitly iterates and prints each student ID individually. The prompt asked to ""Identify and print the 'StudentID' of the student(s)..."" which the generated code does by printing the Series. An iteration to print each ID on a new line (as in the ground truth) would be a slightly more direct interpretation of ""print the 'StudentID' of the student(s)"". This is a very minor stylistic difference and does not impact functionality or correctness.",gemma-3-27b-it,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Filter the DataFrame
filtered_df = df[(df[""Department""]...",9,"The generated code correctly defines the DataFrame, filters it according to the specified conditions, and accurately calculates the average `ProjectsCompleted` for the filtered group. It also handles the edge case where no employees match the criteria by checking `filtered_df.empty`.

The only minor point for improvement is the print statement for the average. The ground truth provides a more descriptive output using an f-string, which is generally better for conveying information to the user. The generated code simply prints the raw number. While not incorrect, it could be more user-friendly. No precision formatting was specified, so printing the raw mean is acceptable. This is a very well-executed solution.",gemma-3-27b-it,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate conversion rate for Desktop users
desktop_users = df[df[""DeviceType""] == ""Desktop""]
desktop_conversion_rate = (desk...",9,"The generated code correctly defines the DataFrame and accurately calculates and prints the conversion rates for both Desktop and Mobile users. The structure is clean and readable. The use of `.sum()` for boolean Series and `len()` for division is a valid alternative to `.mean()`.

One minor area for improvement, which is handled in the ground truth, is safeguarding against `ZeroDivisionError` if a `DeviceType` group happens to be empty (e.g., `len(desktop_users)` could be 0). While not an issue with the provided sample data, robust code would include checks for empty filtered DataFrames, or use methods like `.mean()` directly which handle empty Series by returning `NaN` (which can then be handled). However, for the given task and data, the current code works perfectly. The formatting of the output is also clear.",gemma-3-27b-it,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

# Calculate the average AirQualityIndex for each SensorID
average_air_quality =...",10,"The generated code is excellent. It correctly defines the DataFrame exactly as specified in the task. It then accurately calculates the average ""AirQualityIndex"" for each unique ""SensorID"" using `groupby()` and `mean()`, which is the standard and efficient pandas way to achieve this. Finally, it iterates through the results and prints them in a clear, readable format that indicates which average belongs to which sensor, fulfilling all requirements of the task. The formatting to two decimal places (`.2f`) is a nice touch for presentation. The code is clean, concise, and perfectly addresses the prompt.",gemma-3-27b-it,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

# Create ""EngagementScore"" feature
df[""EngagementScore""] = df[""Likes""] + (df[""Shares""]...",9,"The generated code correctly defines the DataFrame, accurately calculates the ""EngagementScore"", filters for Instagram posts, and computes their average EngagementScore. The imports are correct, and the code structure is clear and concise.

The only minor area for improvement, preventing a perfect 10, is the output format. The ground truth code formats the output to two decimal places and provides a more descriptive print statement, which is good practice for user-facing output. The generated code simply prints the raw float value. Additionally, the ground truth includes a check for an empty filtered DataFrame, which adds robustness, though for this specific dataset, it's not strictly necessary as Instagram posts are guaranteed to exist. However, in a general-purpose function, such a check is valuable.",gemma-3-27b-it,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",10,"The generated code is correct, complete, and of good quality. It precisely defines the DataFrame, calculates the ""LengthOfStay"" column accurately, filters for ""Flu"" patients, and correctly computes and prints their average ""LengthOfStay"". The use of `pd.to_datetime` for date columns is correct. The code adheres to all requirements of the task. While the ground truth includes more verbose print statements for verification, the generated code directly addresses the core computational requirements.",gemma-3-27b-it,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}
df = pd.DataFrame(data)

sci_fi_movies = df[df[""Genre...",9,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates their average rating. It adheres to all requirements of the task. The code is concise and efficient.

One minor improvement would be to format the output of the average rating for better readability, similar to the ground truth code which formats it to two decimal places. Additionally, while not strictly required by the prompt, considering edge cases like an empty DataFrame (no Sci-Fi movies found) could make the code more robust, though for this specific task and dataset, it's not a critical omission.",gemma-3-27b-it,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

toyota_cars = df[df[""Brand""] == ""Toyota""]
average_mileage = toyota_ca...",9,"The generated code correctly defines the DataFrame, filters for Toyota cars, and accurately calculates their average mileage. All required tasks are fulfilled.

Strong points:
*   **Correctness**: The code produces the correct output (average mileage for Toyota cars).
*   **Completeness**: All parts of the task (DataFrame creation, filtering, average calculation, printing) are addressed.
*   **Readability**: The code is concise and easy to understand.
*   **Efficiency**: The filtering and aggregation are done efficiently using pandas capabilities.

Area for minor improvement (not critical for this specific task, but good practice):
*   **Output Formatting**: The ground truth uses an f-string to provide a more descriptive output (""The average Mileage for Toyota cars is: 22500.00""). The generated code just prints the number. While `print(average_mileage)` is correct, adding context makes the output more user-friendly. Given the prompt only asked to ""print the average 'Mileage'"", the generated code technically fulfills this, but a more descriptive print statement is often preferred in real-world scenarios. This is a very minor point and doesn't detract significantly from the solution's quality.",gemma-3-27b-it,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 80...",9,"The generated code correctly performs all the requested tasks: it defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat. The code is concise and efficient. The only minor improvement would be to add a more descriptive print statement for the output, as done in the ground truth, but this doesn't detract from the correctness or functionality of the core logic. Also, the ground truth includes an `if not fire_pokemon_df.empty:` check which is robust against an empty filter result, although not strictly necessary for this specific dataset.",gemma-3-27b-it,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Load the dataset
df = pd.read_csv('titanic.csv')

# Data Cleaning and Preparation
df = df.dropna()
df['Age'] = pd.cut(df['Age'], bins=[0, 12, 18, 60, 100], labels=['Child', 'Teen', 'Adult', 'Senior'])
df['Fare'] = df['Fare'].apply(lambda x: x if x < 200 else 200)

# 1. Multi-panel Figure: Survival Rates by Demographics
fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# Survival by ...",6,"The generated code attempts to address all parts of the task, but falls short in several areas regarding data visualization best practices, interpretation, and adherence to specific visualization requirements.

**Strengths:**
*   Uses the correct libraries (pandas, matplotlib, seaborn, plotly).
*   Correctly calculates and displays the heatmap for survival rates by class and gender.
*   Creates an interactive scatter plot using Plotly, though its mapping could be more insightful.
*   Attempts a dashboard-style figure, integrating multiple plots.

**Weaknesses and Areas for Improvement:**

1.  **Data Preprocessing and Interpretation:**
    *   `df.dropna()` is too aggressive. Dropping all rows with any `NaN` values significantly reduces the dataset (e.g., `Age` and `Embarked` have missing values) affecting the robustness of the analysis. The ground truth handles missing age by imputation, which is a better approach for this dataset.
    *   Age binning is different and less granular than the ground truth, which impacts the ""Age Group"" analysis.
    *   `Fare` transformation `lambda x: x if x < 200 else 200` is arbitrary and not explained. It might distort the distribution for higher fares.

2.  **Visualization Choice and Clarity (Task 1 & 3):**
    *   **Task 1 (Multi-panel figure showing survival rates by different demographic groups):** The requirement was for ""survival rates."" The generated code uses `sns.countplot(hue='Survived')`. While this shows counts of survivors vs. non-survivors, it doesn't directly visualize the *rate* (proportion or percentage) which is more directly comparable across groups of different sizes. Bar plots of the mean survival rate (as in the ground truth) are more appropriate for this. Also, it includes `Embarked`, `SibSp`, and `Parch` which were not explicitly requested as ""different demographic groups"" in the prompt's context, making the figure less focused than the ground truth's selection (class, gender, age).
    *   **Task 3 (Violin plots comparing age distributions of survivors vs non-survivors by gender):** The requested visualization was `age distributions of survivors vs non-survivors by gender`. The generated code plots `Age` vs `Survived` with `hue='Sex'` (which is okay, showing gender within each survival group) and then `Fare` vs `Survived` with `hue='Sex'`, which was not requested. The prompt specifically asked for `Age` distributions. The `split=True` in the ground truth's `violinplot` is a very effective way to compare two distributions side-by-side within the same violin, making it more concise and directly comparable than separate violins.

3.  **Interactive Scatter Plot (Task 4):**
    *   The prompt asked for passengers classes to be represented by *different colors* (`color='Pclass_Cat'`) and `survival` to be shown. The generated code uses `color='Survived'`, which is okay, but then uses `size='Pclass'` where `Pclass` is an ordinal categorical variable, making `size` mapping less intuitive than `color` or `symbol` for this. The ground truth uses `color` for `Pclass` and `symbol` for `Survived`, which is a more effective visual encoding given the prompt's explicit `color for passenger classes`.

4.  **Dashboard-style figure (Task 5):**
    *   The prompt asked for a ""comprehensive dashboard-style figure combining multiple visualizations with proper annotations and insights.""
    *   The generated dashboard contains some of the plots from previous tasks but misses the critical ""annotations and insights"" component. The inclusion of textual insights, as demonstrated in the ground truth, adds significant value to a dashboard, shifting it from just a collection of plots to a storytelling tool.
    *   The selection of plots in the dashboard (two countplots, a `histplot`, and a heatmap) is decent but less varied and insightful than the ground truth's choice (mixing different types like bar plots, KDE plots, and bar plots again for different variables to show various facets).

Overall, the generated code handles file loading and basic plotting, but its data preprocessing choices are less robust, and its visualization selections for specific tasks don't always align with the prompt's implied best practices for data storytelling (e.g., survival *rates* instead of counts, explicit annotations/insights).",gemma-3-27b-it,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, ...",9,"The generated code correctly defines the DataFrame, filters for the 'Dystopian' genre, and accurately calculates the average stock for that genre. The code is concise and directly addresses all requirements of the task. It properly imports `pandas` and uses standard pandas operations for filtering and aggregation.

The only minor area where the ground truth code is slightly more robust is the explicit check for an empty filtered DataFrame (`if not dystopian_books_df.empty:`). While for this specific dataset, we know Dystopian books exist, in a general scenario where the filtered result might be empty, `mean()` on an empty Series would return `NaN`. The ground truth handles this gracefully. However, for the given task and data, the generated code performs perfectly, and the expected behavior is to directly calculate the mean. The print statement could also be more descriptive, similar to the ground truth's f-string, but it still outputs the correct value. These are minor omissions that don't detract significantly from the correctness or quality for the given problem context.",gemma-3-27b-it,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load the dataset
try:
    df = pd.read_csv(""water_potability.csv"")
except FileNotFoundError:
    print(""Error: water_potability.csv not found. Make sure the file is in the current directory."")
    exit()

# Drop rows with missing values
df = df.dropna()

# Significance level
alpha = 0.05

# 1. T-tests
numerical_features = ['pH', 'Hardness', 'Total Dissolved Solids', 'Chloramines', 'Sulfate', 'Organic Carbon', 'Trihalomet...",6,"The generated code attempts to address all requirements but falls short in several aspects, particularly regarding statistical robustness and handling of missing values. While it correctly identifies the necessary tests, the execution details are flawed or suboptimal.

**Strengths:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Basic Structure:** The code follows a logical flow for performing the requested tests.
*   **File Handling:** Includes a basic `try-except` block for file loading.
*   **Significance Level:** Correctly uses `alpha = 0.05`.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling:**
    *   **CRITICAL FLAW:** The generated code uses `df = df.dropna()`. This is almost never the preferred method for handling missing data in a real-world scenario, especially for statistical analysis, as it leads to significant data loss and potential bias. The ground truth correctly imputes missing values. This single decision severely impacts the reliability and representativeness of all subsequent statistical tests.

2.  **T-tests:**
    *   **Incorrect Cohen's d Calculation:** The `np.std(np.concatenate((group1, group2)))` for Cohen's d calculation is incorrect. It should be the pooled standard deviation, which involves degrees of freedom and is more accurately calculated by the formula `sqrt(((n1-1)*s1^2 + (n2-1)*s2^2) / (n1+n2-2))`. The generated code uses the standard deviation of the *combined* dataset, which is not the pooled standard deviation needed for Cohen's d between two independent groups.
    *   **Redundant Calculation:** The t-tests and Cohen's d calculations are performed in separate loops. It's more efficient to combine them into one loop, so the t-test results (p-value for significance) can be used directly for Cohen's d calculation.
    *   **Interpretation:** The interpretation is basic (""Significant difference"" or ""No significant difference""). The ground truth provides more verbose and standard interpretations, including the null hypothesis.

3.  **Chi-square Goodness-of-Fit Test:**
    *   **Assumed Distribution:** While assuming an equal distribution (50/50) is a plausible default if no other expected distribution is specified, it should be explicitly stated as an assumption in the output or comments, as the ground truth does. The prompt only says ""check if potability follows expected distribution,"" which is ambiguous. The ground truth clarifies this.
    *   **Data Type of `f_exp`:** `f_exp` in `stats.chisquare` expects a list or array of expected frequencies for each category, not just a single value multiplied by the number of categories. `f_exp=[expected_frequencies, expected_frequencies]` is correct for a binary outcome with equal expectation, but the internal variable `expected_frequencies` might be misleading (`len(df) / 2`).

4.  **Normality Tests:**
    *   **No Handling for Large Sample Sizes:** Shapiro-Wilk is known to be sensitive to large sample sizes and might always reject normality for large datasets. The ground truth correctly adds a check for sample size and suggests alternative tests, which is a good practice for real-world data analysis.

5.  **Completeness & Detail:**
    *   **Missing Features from Prompt:** The prompt specifically mentions ""Display p-values, test statistics, and interpretation of results for each test,"" which the generated code does, but the interpretation is less detailed than the ground truth.
    *   **No Numerical Features List:** The ground truth dynamically identifies numerical features, ensuring all are included and excluding the target variable. The generated code hardcodes `numerical_features`, which is less robust if the dataset columns change.
    *   **Lack of Reporting Details:** The ground truth explicitly states means for t-tests, which provides more context.

The most critical issue is the `dropna()` which fundamentally alters the dataset for all subsequent analyses, making the results potentially invalid or unrepresentative. The incorrect Cohen's d calculation is also a significant statistical error.",gemma-3-27b-it,LLM_judge_config
M8,medium,Classification,,,Error,Failed to generate code,gemma-3-27b-it,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import SelectKBest, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_selection import f_classif

# Load the dataset
df = pd.read_csv(""water_potability.csv"")

# Data Cleaning (Handle Missing Values)
df = df.fillna(df.mean())

# Feature Engineering
df['pH_Chloramines'] = df['pH'] / (df[...",6,"The generated code addresses most of the tasks but has several issues that prevent it from being a strong solution.

**Strengths:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Missing Value Handling:** Basic imputation of missing values is performed.
*   **Feature Engineering:** Attempts to create ratio and interaction terms, and polynomial features.
*   **Correlation Analysis:** Calculates the correlation matrix and identifies top correlated features.
*   **Feature Selection:** Applies both SelectKBest and RFE for feature selection.
*   **Visualizations:** Includes correlation heatmap, feature importance plot, and pairwise scatter plots.
*   **Feature Set Comparison:** Attempts to compare the selected feature sets.

**Weaknesses and Areas for Improvement:**

1.  **Meaningful Feature Engineering (Ratios):**
    *   The generated code uses `df['Total Dissolved Solids']` and `df['Magnesium']` for ratio calculations. The original dataset columns are `Solids` and there is no `Magnesium` column. This indicates a lack of understanding or verification of the actual dataset columns.
    *   The ratios `pH_Chloramines`, `Hardness_Solids`, `Sulfate_Magnesium`, `Organic_Carbon_pH`, `Chlorine_pH`, `Turbidity_Solids` are not as descriptive or diverse as in the ground truth. The ground truth creates a wider variety of ratios and interaction terms that are more likely to be meaningful given the context of chemical parameters.

2.  **Polynomial Features Implementation:**
    *   The ground truth selects a subset of features for polynomial expansion (`poly_features_cols`) to avoid excessive dimensionality. The generated code uses only 3 features (`pH`, `Hardness`, `Total Dissolved Solids`) for polynomial expansion and generates generic column names (`poly_0`, `poly_1`, etc.). This makes it harder to interpret which original features contribute to these new polynomial features, and limits the potential for discovery from a broader set of features.
    *   The ground truth explicitly checks `if col not in X_fe.columns:` when adding polynomial features to avoid duplicating original columns if they already exist in the `X_fe` DataFrame. The generated code concatenates `poly_df` directly, which might lead to redundant columns (`pH`, `Hardness`, `Total Dissolved Solids` potentially being duplicated as original columns and as polynomial features if `include_bias=False` creates them, or simply creating new columns named generically).

3.  **Visualization - Feature Importance Plot:**
    *   The generated code creates a feature importance plot based on `model.feature_importances_` where `model` is the `RandomForestClassifier` used for RFE. Crucially, the `fit` method for this `model` in the RFE step (`rfe.fit_transform(X, y)`) applies the RFE process, fitting the model internally multiple times on *reduced* feature sets. The `feature_importances_` attribute after `rfe.fit_transform` might not reflect the importances of the *full* set of features, or even all features initially present in `X`. To get a proper feature importance plot for *all* features, the `RandomForestClassifier` should be fitted *once* on the entire feature set `X` (or `X_scaled_df` as in the ground truth), independently of the RFE process.
    *   The ground truth correctly fits a separate `RandomForestClassifier` (`rf_full`) trained on the entire `X_scaled_df` to obtain comprehensive feature importances.

4.  **Overall Feature Engineering Complexity and Completeness:**
    *   The generated code does not explicitly handle interaction terms as a separate category, though polynomial features might create some. The ground truth specifically creates interaction terms, which are a common and useful type of engineered feature.
    *   The generated code lacks the `epsilon` addition for division to prevent `inf` or `NaN` values when denominators are zero or very small.
    *   The ground truth performs standardization of features (`StandardScaler`) before feeding them into `SelectKBest` and `RFE`. This is a crucial step for many machine learning algorithms, especially those sensitive to feature scales (like f\_classif in SelectKBest or distance-based algorithms) and general good practice. The generated code skips this.

5.  **Visualization - Pairwise Scatter Plots Title:**
    *   The title for the pairwise scatter plots is generic (`""Pairwise Scatter Plots of Top Correlated Features""`). The ground truth adds contextual information (`""by Potability""`) and sets the title position correctly.

6.  **Feature Set Comparison Clarity:**
    *   While the generated code calculates `overlap_kbest_rfe`, it doesn't explicitly print the unique features to each set or compare with the top correlated features, which the ground truth does for a more comprehensive analysis of overlaps and differences.

7.  **Minor Issues:**
    *   Missing `try-except` block for file loading.
    *   `df.fillna(df.mean())` might fill `Potability` as well, though it's less critical. The ground truth is more precise by specifying the columns with missing values.
    *   The column names in the generated code's feature engineering (`pH`, `Total Dissolved Solids`, `Organic Carbon`, `Chlorine`, `Magnesium`) often contain spaces and are inconsistent with common Python naming conventions (`pH`, `Solids`, `Organic_carbon`, `Chloramines`) which could potentially lead to issues if not carefully handled. The ground truth uses the correct column names from the dataset specification.

The code provides a basic framework but needs significant refinement in feature engineering logic, visualization details, and adherence to best practices for feature selection to meet the ""expert code evaluator"" standard.",gemma-3-27b-it,LLM_judge_config
