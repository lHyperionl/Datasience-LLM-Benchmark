task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"import pandas as pd

df = pd.read_csv('winequality-red.csv', sep=';')

col_types = {col: 'numerical' if pd.api.types.is_numeric_dtype(df[col]) else 'categorical' for col in df.columns}
print(""Column Types:"", col_types)

num_cols = df.select_dtypes(include='number').columns
stats = df[num_cols].agg(['mean', 'median', 'std', 'min', 'max', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)])
stats = stats.rename(index={'<lambda_0>': '25%', '<lambda_1>': '75%'})
print(""Descriptive Statistics:\n""...",7,"The generated code correctly loads the dataset, identifies numerical and categorical columns, calculates descriptive statistics, and reports dataset dimensions. However, it implicitly assumes all numerical columns are not ordinal, which is not true for 'quality'. The descriptive statistics output format is less readable compared to ground truth's `describe()` method. The column type identification is less verbose and misses the pandas dtype information. It also hardcodes the separator to ';' which is common for this dataset, but the ground truth uses a more robust approach with error handling for file loading, which is a good practice for general solutions.

**Areas for Improvement:**
1.  **Error Handling:** Add a `try-except` block for `FileNotFoundError` as seen in the ground truth code to make the solution more robust.
2.  **Descriptive Statistics Output:** While functionally correct, the manual aggregation for descriptive statistics is less concise and readable than using `df.describe()`. The ground truth leverages `describe()` which inherently provides all required statistics (mean, median/50%, std, min, max, quartiles).
3.  **Column Type Identification Verbosity:** The ground truth code provides a more detailed output for column types, showing both the Pandas `dtype` and an interpreted ""Identified Type"". The generated code's dictionary output is less informative.
4.  **Ordinal Column Identification:** The hardcoding of `'quality'` as an ordinal column is specific to this dataset. While correct for this task, a more general solution would need a method to infer ordinality (which is typically complex and domain-specific, but the ground truth acknowledges it verbally). The ground truth's approach of identifying strictly non-numerical columns and then adding a note about 'quality' being often treated as ordinal is a good balance.
5.  **Separator:** The ground truth does not specify a separator, which is usually correct for standard CSV where it is a comma. While `winequality-red.csv` often uses a semicolon, explicitly mentioning `sep=';'` is good for this specific dataset but less general. The prompt didn't specify the separator.",o4_mini,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
df = pd.read_csv('video_games_sales.csv')
print(""First 10 rows:"")
print(df.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(""\nDataset shape:"")
print(df.shape)

# 3) List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# 4) Display the top 10 games by Global_Sales
print(""\nTop 10 games by Global_Sales:"")
print(df.sort_values('Global_Sales', asce...",7,"The generated code correctly performs all the requested tasks. It loads the data, displays the head, shows the shape, lists dtypes, finds top games by sales, lists unique values for Platform and Genre, calculates summary statistics for sales columns, and identifies missing values.

Areas for improvement to reach a higher score:
1.  **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` when loading the CSV and checks for the existence of columns (`Global_Sales`, `Platform`, `Genre`, sales columns) before attempting operations. The generated code assumes all files and columns exist, which can lead to runtime errors if the data is not as expected.
2.  **Output Clarity:** While the generated code prints information, the ground truth uses more descriptive print statements and separators (`=""*80`) to make the output more readable and clearly delineate each section of the analysis.
3.  **Specific Column Display for Top Games:** For the ""Top 10 games by Global_Sales"" section, the ground truth prints only relevant columns (`Name`, `Platform`, `Genre`, `Publisher`, `Global_Sales`), making the output cleaner and more focused. The generated code prints all columns, which can be overwhelming.
4.  **Filtering Missing Values:** The ground truth specifically filters to show only columns that *have* missing values, providing a more concise summary. The generated code shows `isnull().sum()` for all columns, even those with zero missing values.",o4_mini,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"import pandas as pd

df = pd.read_csv('video_games_sales.csv')
df['Year'] = pd.to_numeric(df['Year'], errors='coerce')
df['Year'] = df['Year'].fillna(df['Year'].median())
df = df.dropna(subset=['Publisher'])
df['Years_Since_Release'] = 2023 - df['Year']
df = df.drop_duplicates(subset=['Name', 'Platform'])
df = df[df['Global_Sales'] >= 0.1]
print(df.shape)
print(df.head())",8,"The generated code correctly implements most of the required data cleaning and preprocessing steps. It loads the data, handles missing 'Year' values, removes missing 'Publisher' values, creates the 'Years_Since_Release' column, removes duplicates, and filters low-selling games. It also displays the shape and head as requested.

Areas for improvement to achieve a higher score:
1.  **Robustness for 'Year' column**: While `pd.to_numeric` with `errors='coerce'` is good, explicitly converting the 'Year' column to an integer type *after* filling NaNs (as done in the ground truth) is more robust and ensures data consistency. The generated code leaves 'Year' as a float after `fillna`.
2.  **Robustness for 'Years_Since_Release'**: The ground truth ensures `Years_Since_Release` is non-negative, which is a good practice if there's a possibility of release years being after the current year (e.g., future releases or data errors). The generated code doesn't account for this.
3.  **Robustness for 'Global_Sales'**: The ground truth includes an `errors='coerce'` and `dropna` for `Global_Sales` before filtering, which makes the code more robust against non-numeric sales data. The generated code directly filters, assuming `Global_Sales` is already numeric.
4.  **Clarity/Information**: The ground truth provides print statements at various stages, showing the shape after each major operation. This is very helpful for debugging and understanding the data transformation process, which was implicitly requested by the ""display cleaned dataset's shape"" at the end, but the ground truth extends this for clarity at each step.
5.  **Error Handling**: The ground truth includes basic `try-except` blocks for `FileNotFoundError` and general exceptions, making the script more robust.",o4_mini,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('video_games_sales.csv')

# 1) Bar chart: Top 10 platforms by total global sales
platform_sales = df.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False).head(10)
plt.figure(figsize=(10,6))
platform_sales.plot(kind='bar', color='skyblue')
plt.title('Top 10 Gaming Platforms by Total Global Sales')
plt.xlabel('Platform')
plt.ylabel('Total Global Sales (Millions)')
plt.xticks(rotation=45...",7,"The generated code successfully creates all the requested visualizations with proper titles and labels, fulfilling the core requirements of the task. The plots are generally well-formatted.

However, there are several areas for improvement, especially regarding robustness and data handling:

1.  **Missing `numpy` import**: Although not explicitly used in the generated code, `numpy` is a common dependency for `pandas` operations in data science contexts and robust solutions often include it. The ground truth correctly includes it.
2.  **No Error Handling for File Loading**: The generated code directly tries to load the CSV without a `try-except` block for `FileNotFoundError`. This makes the code less robust compared to the ground truth, which gracefully handles the case where the CSV might be missing.
3.  **Lack of Data Cleaning/Preprocessing**: The ground truth code includes crucial preprocessing steps for `Year_of_Release` (converting to numeric, handling NaNs, converting to int) and ensuring all sales columns are numeric. The generated code relies on `dropna()` only for `Global_Sales` histogram and `Year_of_Release` line plot, but doesn't explicitly ensure the numeric type for all sales columns from the start, which could lead to errors if the CSV has non-numeric sales data. For `Year_of_Release`, simply using `astype(int)` without handling errors for potential non-numeric values or NaNs in the column before `dropna()` might cause issues.
4.  **Pie Chart Readability**: The ground truth includes logic to group smaller genres into an 'Other' category, which significantly improves the readability of the pie chart for datasets with many categories. The generated code plots all genres, which can lead to a cluttered and unreadable chart if there are too many unique genres.
5.  **Line Plot x-axis `Year_of_Release` Range**: The ground truth filters `Year_of_Release` to a reasonable range (1980-2020), which handles potential outliers or data entry errors in the year column. The generated code does not apply such filtering, potentially showing invalid years on the plot.
6.  **Minor Plotting Enhancements Missing**: The ground truth includes minor enhancements like `axis='equal'` for the pie chart to ensure it's circular, `grid` lines for bar/histogram/scatter/line plots, `s` for scatter marker size, and `range` limiting for histogram, `alpha` for scatter plot, and `ha='right'` for xticks, which generally improve visualization quality and informativeness. While not strictly required, they contribute to a more professional and readable chart.
7.  **`plt.ylabel('')` for Pie Chart**: While technically correct to remove the label, usually, a genre distribution chart doesn't typically need a y-axis label to begin with. `plt.ylabel('')` is not wrong, but doesn't add much value.",o4_mini,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
df = pd.read_csv('diabetes.csv')
print(""First 10 rows:"")
print(df.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(""\nDataset shape:"", df.shape)

# 3) List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# 4) Calculate the percentage of diabetic vs non-diabetic patients (Outcome column)
print(""\nDiabetic vs Non-diabetic percentage:"")
print(df['O...",7,"The generated code provides a functional solution to most of the requirements. It correctly loads the data, displays the head, shape, dtypes, missing values, and summary statistics. It also correctly calculates the percentages for the 'Outcome' column.

However, there are a few areas for improvement:
1.  **Robustness for file loading**: The ground truth includes error handling for `FileNotFoundError`, which is a good practice for tasks involving file I/O. The generated code lacks this.
2.  **`df.info()` vs `df.dtypes`**: While `df.dtypes` works for showing data types, `df.info()` (used in the ground truth) also provides non-null counts, which is more comprehensive and often preferred for initial data type inspection.
3.  **Specificity in zero-value identification**: The ground truth explicitly lists columns where zero values are *suspiciously* missing, recognizing that 'Pregnancies' and 'Outcome' can legitimately be zero. The generated code uses `(df == 0).sum()`, which will show zero counts for *all* columns, including 'Pregnancies' and 'Outcome', potentially misinterpreting valid zeros as missing data without further context. This is a subtle but important distinction in EDA for this particular dataset.
4.  **Clarity and readability**: The ground truth uses more descriptive print statements and section separators, which improves the readability of the output for an EDA task.
5.  **Non-essential import**: `numpy` is imported in the ground truth but not strictly used to achieve the exact requirements. The generated code correctly omits it.
6.  **Configuration**: The ground truth sets up constants for `DATASET_PATH` and `TARGET_COLUMN`, making the code more configurable and easier to modify for different datasets or column names. The generated code hardcodes these strings.

Overall, it's a good attempt, but the nuances of zero-value identification and error handling could be improved.",o4_mini,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# 1) Load the dataset
df = pd.read_csv('diabetes.csv')

# 2) Identify and replace zero values with NaN
cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[cols] = df[cols].replace(0, np.nan)

# 4) Fill missing values with median
for col in cols:
    df[col].fillna(df[col].median(), inplace=True)

# 5) Remove duplicate rows
df.drop_duplicates(inplace=True)

# 6) Create BMI_Cate...",7,"The generated code addresses most of the requirements but has a few omissions and potential issues that prevent it from being a perfect solution.

Correctness:
- Loading dataset: Correct.
- Identifying and replacing zeros: Correct for the specified columns.
- Handling missing values (median imputation): Correct.
- Removing duplicates: Correct.
- Creating BMI_Category: The logic is correct, but using `pd.cut` as in the ground truth is generally more efficient and idiomatic for binning.
- Scaling numerical features: This is where a significant issue lies. The code scales *all* numerical columns, including the `Outcome` variable (which is typically a target and should not be scaled) and potentially the newly created `BMI_Category` if `pd.cut` wasn't used and categories were represented numerically. The ground truth correctly excludes 'Outcome' and correctly handles `BMI_Category` as non-numerical if `pd.cut` is used to create categorical labels. If `BMI` was the only numerical column, it would scale it correctly. But generally, target variables should not be scaled with features for classification.
- Displaying shape and head: Correct, although it doesn't print the original shape which is a nice-to-have for comparison.

Code Quality:
- Imports: Correct.
- Structure: Generally good, follows the steps.
- Readability: Clear and concise.
- Error Handling: Lacks basic error handling for `FileNotFoundError` when loading the CSV, which could cause the script to crash. The ground truth includes this.
- Verbosity/Information: The generated code doesn't provide any print statements to indicate progress or confirm operations (e.g., ""Dataset loaded successfully"", ""Removed X duplicate rows"", ""Scaled numerical features""). The ground truth provides much better feedback.

Completeness:
- **Missing:** Explicitly identifying columns with zeros in *point 2* (the code directly replaces without a separate identification print). The problem statement asked to ""Identify columns with zero values that represent missing data"" which implies some form of output or check, rather than just directly replacing them. The generated code implicitly identifies them by hardcoding `cols`.
- **Incomplete/Incorrect:** Scaling of *all* numerical features, including the target 'Outcome', is incorrect for typical data science workflows. The ground truth specifically excludes 'Outcome'.
- **Minor omission:** No handling for if a specified column is *not* in the dataset (the `if col in df.columns:` check in the ground truth).
- **Minor omission:** No printing of original dataset shape for comparison, only the final shape.

Improvements for a higher score:
1.  **Exclude 'Outcome' from scaling:** Modify the numerical column selection to explicitly exclude the 'Outcome' column if it exists, as it's typically a target variable.
2.  **Add basic error handling for file loading:** Wrap `pd.read_csv` in a `try-except FileNotFoundError` block.
3.  **Add informative print statements:** Provide feedback to the user about each step completed, like ""Dataset loaded successfully,"" ""Removed X duplicate rows,"" etc., similar to the ground truth.
4.  Consider using `pd.cut` for `BMI_Category` for better performance and to ensure `BMI_Category` is treated as a categorical type if possible in subsequent steps.
5.  Add a check for column existence before attempting operations on them (e.g., `if col in df.columns:`).",o4_mini,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('diabetes.csv')

# 1) Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.tight_layout()
plt.show()

# 2) Histograms for all numerical features
features = df.columns.drop('Outcome')
for feature in features:
    plt.figure(figsize=(6, 4))
    sns.histplot(df[feature], bins=30, color='skyblue', edge...",7,"The generated code correctly addresses most of the task requirements by generating the specified visualizations. The imports are correct, and the structure is logical, creating each visualization type separately.

Areas for improvement to achieve a higher score include:
1.  **Readability/Efficiency for Histograms and Box Plots**: The generated code creates a new figure (and thus a new plot window/subplot) for *each* histogram and *each* box plot. The ground truth code efficiently uses `subplot` to group these similar plots into a single figure, which is a much better practice for presenting multiple related distributions or comparisons. This makes the output less cluttered and easier to digest.
2.  **Robustness**: The generated code assumes `diabetes.csv` is present. The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice for data loading.
3.  **Detailed Labels/Titles**: While titles and labels are present, some, like the `Outcome` labels in the box plots (`0=Non-diabetic, 1=Diabetic`), could be explicitly set using `labels` in the legend or by relabeling the axis ticks for clarity, similar to how the ground truth labels were handled.
4.  **Specific Scatter Plots**: The task asked for ""BMI vs Glucose and Age vs Pregnancies"". The generated code provides separate figures for each, which is acceptable, but the ground truth combined them into a single figure with two subplots, which is a more compact and often preferred way to present related scatter plots.
5.  **Pair Plot Features**: The ground truth explicitly selected `['Glucose', 'BMI', 'Age', 'Outcome']` for the pair plot. The generated code used `df, vars=['Glucose', 'BMI', 'Age', 'Outcome']`, which achieves the same result correctly, but `df[key_features]` (as in ground truth) is perhaps slightly more explicit about what's being passed.
6.  **Minor Formatting Details**: The ground truth used `fmt="".2f""` for the heatmap annotations to control decimal places, and `edgecolor='black'` for histograms (which the generated code also included), but the ground truth also used `kde=True` for histograms, which can provide more insight into distribution shape. `plt.tight_layout()` is used frequently in the generated code, which is good, but sometimes it doesn't need to be called after every single plot in a loop if a larger figure containing subplots is managed appropriately (as in histograms/box plots in ground truth).",o4_mini,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
df = pd.read_csv('world-happiness-report-2021.csv')
print(df.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(""Shape:"", df.shape)

# 3) List all column names and their data types
print(df.dtypes)

# 4) Display the unique regional indicators and count of countries per region
print(""Regions:"", df['Regional indicator'].unique())
print(""Countries per region:\n"", df['Regional indicator'].value_cou...",7,"The generated code correctly addresses most of the task requirements.

**Strengths:**
*   Correctly loads the dataset and displays the first 10 rows.
*   Shows the shape of the dataset accurately.
*   Lists column names and data types (though `df.info()` is often preferred for more comprehensive detail).
*   Correctly identifies unique regional indicators and counts countries per region using `value_counts()`.
*   Successfully identifies the top 10 happiest countries.
*   Correctly calculates summary statistics for the specified key happiness factors.
*   Identifies missing values correctly.
*   Properly uses `pandas` for data manipulation.

**Areas for Improvement to achieve a higher score:**
*   **Error Handling (File Not Found):** The generated code lacks a `try-except` block for `FileNotFoundError` when reading the CSV. If the file is not present, it will crash, whereas the ground truth handles this gracefully.
*   **Clarity and Readability of Output:** The ground truth includes clear print statements with headings and separators (`=""*70 + ""\n""`) for each task's output, making the overall output much more readable and organized. The generated code's output, while correct, is less structured and harder to follow quickly.
*   **Robustness of Column Selection:** The ground truth includes checks like `if 'Regional indicator' in df.columns:` before attempting to access columns. While the provided dataset likely contains these columns, robust code would check for their existence to prevent `KeyError` if the dataset structure differs slightly.
*   **`df.info()` vs. `df.dtypes`:** While `df.dtypes` is correct for listing data types, `df.info()` provides more comprehensive information, including memory usage and non-null counts, which is often more useful for initial EDA. The ground truth leverages `df.info()` for task 3.",o4_mini,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder

df = pd.read_csv('world-happiness-report-2021.csv')

num_cols = df.select_dtypes(include=[np.number]).columns
cat_cols = df.select_dtypes(include=['object', 'category']).columns
df[num_cols] = df[num_cols].apply(lambda x: x.fillna(x.mean()))
for col in cat_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

df['Happiness_Category'] = pd.cut(df['Ladder score'], bins=[-np.inf, 4, 6.5, np.inf],...",9,"The generated code is very good and largely addresses all requirements effectively.

**Strong Points:**
- **Correctness:** All major steps (loading, missing values, feature engineering, outlier removal, scaling, encoding, displaying) are implemented correctly.
- **Conciseness:** The missing value handling using `apply` with `lambda` for numerical columns is more concise than a loop.
- **Feature Engineering:** `pd.cut` for `Happiness_Category` and `pd.qcut` for `GDP_Category` are used appropriately and efficiently. `Well_being_Score` calculation is also correct.
- **Outlier Removal:** The IQR method is correctly applied using boolean indexing.
- **Scaling and Encoding:** `StandardScaler` and `LabelEncoder` are used correctly on the identified columns.
- **Imports:** All necessary imports are present.

**Areas for Improvement (Minor):**
1.  **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a robust practice for real-world scenarios. The generated code omits this.
2.  **Explicit Display Requirements:** The task asked for ""Display the cleaned dataset's shape and first 5 rows"", and the ground truth explicitly prints ""Shape of the cleaned dataset:"" and ""First 5 rows of the cleaned dataset:"". The generated code just prints `df.shape` and `df.head()` without descriptive messages, which is less user-friendly, although it does technically display the information.
3.  **Ambiguity in Categorical Selection:** `df.select_dtypes(include=['object', 'category'])` is robust. However, `pd.read_csv` usually loads string columns as `object` dtype; if a column were already a pandas `category` dtype, this selection would be fine. For the given dataset, `object` is sufficient. It's a minor point, but worth noting for broader applications.
4.  **Implicit Assumptions/Edge Cases:** The missing value handling for categorical columns uses a loop `for col in cat_cols: df[col] = df[col].fillna(df[col].mode()[0])`. This is correct. The numerical handling `df[num_cols] = df[num_cols].apply(lambda x: x.fillna(x.mean()))` is more compact. Both work.

Overall, the generated code is highly functional and accurate, demonstrating a strong understanding of the required data preprocessing steps. The minor points are mostly about robustness and presentation, not core functionality.",o4_mini,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# 1) Load dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# 2) World map visualization
fig = px.choropleth(
    df,
    locations='Country name',
    locationmode='country names',
    color='Ladder score',
    color_continuous_scale=px.colors.sequential.Plasma,
    title='World Happiness Score 2021'
)
fig.show()

# 3) H...",7,"The generated code successfully attempts all the requested visualizations.

Strengths:
- Successfully implemented all 7 types of visualizations as requested (map, bar chart, heatmap, scatter plots, box plots, radar chart).
- Used the correct libraries (pandas, matplotlib, seaborn, plotly).
- Most titles and labels are present, enhancing readability.

Areas for Improvement:
- **Renaming Columns for Clarity:** The ground truth renames 'Ladder score' to 'Happiness Score', and other factors, which makes the code more readable and consistent with common terminology for analysis. The generated code uses the original column names which are less intuitive in some plots.
- **Scatter Plots Formatting:** The ground truth combines the two scatter plots into a single figure with subplots, which is a more efficient use of space and allows for easier comparison. It also adds a `hue` based on 'Regional indicator' and a legend, providing richer insights. The generated code creates separate figures for each scatter plot and lacks the regional hue, making them less informative.
- **Radar Chart Normalization and Presentation:** The radar chart in the generated code uses raw values. For a radar chart comparing different factors that might have different scales (e.g., GDP vs. Generosity), normalization (as done in the ground truth using `MinMaxScaler`) is crucial to ensure fair comparison and accurate representation of the 'profile'. Without normalization, factors with larger absolute values will visually dominate the chart, even if their relative importance or variation among countries is not higher. The ground truth also uses `matplotlib` for the radar chart, which allows for more fine-grained control over aesthetics like filling the plot and custom tick labels, making it more visually appealing and informative.
- **General Plot Aesthetics:** While the generated code has basic titles and labels, the ground truth includes more advanced aesthetic touches like `sns.set_style`, `plt.rcParams`, `linewidths` and `linecolor` in heatmap, `bbox_to_anchor` for legends, and explicit `fontsize` which improve the overall presentation and readability of the plots.
- **Error Handling:** The ground truth includes basic error handling for file not found, which is a good practice for robust code.
- **Redundant Imports**: `import plotly.graph_objects as go` is imported but not explicitly used for the radar chart in the same way `go.Figure()` usually is. `px.choropleth` is used for the map. For the radar chart `go.Figure()` is used but the traces are added using `go.Scatterpolar`. This is implicitly using `go`.
- **Plotly Map Customization:** The ground truth includes `fig_map.update_layout` for `geo` settings which offers more control over the map's appearance (e.g., `projection_type`).",o4_mini,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd

# 1) Load the dataset and parse 'date' column as datetime
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2) Display the first 10 rows
print(""First 10 rows:"")
print(df.head(10))

# 3) Show the shape of the dataset
print(""\nShape (rows, columns):"", df.shape)

# 4) List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# 5) Total sum of 'total_cases' and 'total_deaths' grouped by 'continent'
print(...",7,"The generated code is largely correct and addresses most of the requirements.

**Strong Points:**
*   Properly loads the CSV and parses dates.
*   Correctly displays the first 10 rows, shape, and column data types.
*   Accurately calculates and displays the sum of 'total\_cases' and 'total\_deaths' grouped by 'continent'.
*   Correctly identifies and displays the top 5 locations based on the latest date for each, although it removes the 'date' column from the final output, which is a minor deviation from the ground truth's display but still correct in spirit.
*   Correctly calculates and displays summary statistics for 'new\_cases' and 'new\_deaths'.
*   Accurately counts and displays missing values.

**Areas for Improvement:**
*   **Error Handling:** The ground truth includes robust `try-except` blocks for file loading, which is a good practice for production-ready code. The generated code lacks this, making it less resilient to missing files or other loading issues.
*   **Output Clarity:** While the ground truth uses `df.info()` for displaying column names and data types, which provides more comprehensive information (non-null counts, memory usage) than just `df.dtypes`, the generated code's `df.dtypes` is still correct as per the prompt's request (""List all column names and their data types"").
*   **Robustness in Grouping/Aggregation:** The generated code directly performs aggregations (`groupby`, `nlargest`, `agg`) without explicitly handling potential `NaN` values in the relevant columns, e.g., `continent` or `total_cases`, in cases where the `dropna` or `pd.to_numeric(errors='coerce')` might be beneficial. While `NaN` values often implicitly get ignored by these operations, explicit handling (as shown in the ground truth for `continental_summary` or `top_5_locations`) makes the code more robust and predictable, especially if the dataset quality is inconsistent. The ground truth also explicitly checks for column existence before performing operations, which adds to robustness.
*   **Minor Display Discrepancy (Top 5 Locations):** The ground truth's display for the top 5 locations includes the 'date' column (`print(top_5_locations[['location', 'date', 'total_cases']])`), whereas the generated code only shows 'location' and 'total\_cases'. While the calculation is correct, displaying the date of the latest entry provides more context.

Overall, it's a solid attempt that performs the core tasks well but could benefit from improved error handling and robustness checks often found in more mature code.",o4_mini,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('winequality-red.csv', sep=';')

# 1) Average quality
mean_quality = df['quality'].mean()
print(f""Average quality: {mean_quality:.2f}"")

# 2) Average alcohol by quality
alcohol_by_quality = df.groupby('quality')['alcohol'].mean().reset_index()
print(alcohol_by_quality)

# 3) Bar chart
sns.set(style=""whitegrid"")
plt.figure(figsize=(8, 5))
sns.barplot(x='quality', y='alcohol', data=alc...",8,"The generated code correctly addresses all three requirements of the task: calculating the mean quality, grouping by quality to find average alcohol content, and generating a bar chart.

**Strong points:**
- Correctly loads the data.
- Accurately calculates and prints the average quality.
- Correctly groups data and calculates average alcohol by quality.
- Generates a relevant bar chart with appropriate labels.
- Uses proper imports (pandas, matplotlib, seaborn).

**Areas for improvement to achieve a higher score:**
- **Data loading robustness:** The generated code assumes the CSV is semicolon-separated (`sep=';'`). While this is often true for wine quality datasets, `winequality-red.csv` typically uses comma separation. The ground truth code correctly omits `sep=';'` which handles the default comma separator. This could lead to a `pandas.errors.ParserError` or incorrect parsing if the file is indeed comma-separated. A `try-except` block for `FileNotFoundError` as in the ground truth would also improve robustness.
- **Output formatting:** The ground truth code includes more descriptive print statements (e.g., ""1. Average 'quality' of red wines:"", ""2. Average 'alcohol' content for each 'quality' group:""). The generated code is a bit more concise here.
- **Plotting details:** The generated plot is good, but the ground truth adds more refinements:
    - A larger `figsize` for better readability.
    - More detailed `fontsize` control for title and labels.
    - Adding data labels directly on the bars for precise values, which is a very useful addition for data visualization.
- **Comments:** While the ground truth uses more extensive comments, the generated code's comments are minimal but sufficient for the task. The ground truth provides more context and step-by-step guidance, which is good for explaining the code.",o4_mini,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# 1) Load dataset
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2) Handle missing values
df['hosp_patients'] = df['hosp_patients'].fillna(0)
df['icu_patients'] = df['icu_patients'].fillna(0)
for col in ['positive_rate', 'reproduction_rate']:
    df[col] = df[col].fillna(df[col].mean())

# 3) Create mortality_rate
df['mortality_rate'] = (df['total_deaths'] / df['total_cases'] * 100) \
    .replace([np.inf, -np.inf], np.na...",9,"The generated code is highly correct and effectively implements all the specified task requirements.

**Strong Points:**
- **Correctness:** All tasks are handled accurately. Data loading, missing value imputation, new column creation with division by zero handling, and filtering are all implemented correctly.
- **Conciseness:** The `parse_dates` argument in `read_csv` is excellent for efficiency. Using a loop for mean imputation is also good.
- **Handling Division by Zero:** The chain `.replace([np.inf, -np.inf], np.nan).fillna(0)` is a very clean and effective way to handle both `inf` and `NaN` results from division by zero, which is slightly more robust than handling them separately as in the ground truth for cases where `0/0` might produce `NaN` directly.
- **Completeness:** All aspects of the task are covered.
- **Readability:** The code is well-structured and easy to understand.

**Minor Area for Improvement:**
- **Error Handling (File Not Found):** While not explicitly asked for, adding a `try-except` block for `FileNotFoundError` when loading the CSV, similar to the ground truth, would make the code more robust and user-friendly in a real-world scenario. However, for a benchmark evaluation, focusing solely on the core task logic, its omission is minor.
- **Clarity in `df.head()` output**: The instructions asked to ""Display the shape of the cleaned dataset and the first 5 rows with the new columns. Include proper imports and assume the CSV file is in the current directory."" The generated code outputs `df.shape` and then `df.head()`, which is perfectly fine. However, adding a small print statement before `df.head()` like ""First 5 rows of the cleaned dataset:"" would match the explicit output format requested in the ground truth for better user clarity, but this is a very minor aesthetic point and does not affect correctness.",o4_mini,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

sns.set(style='whitegrid')

# 1) Time series line plot of 'new_cases' for India
india_df = df[df['location'] == 'India']
plt.figure(figsize=(12, 6))
plt.plot(india_df['date'], india_df['new_cases'], color='tab:blue')
plt.title('Daily New COVID-19 Cases in India')
plt.xlabel('Date')
plt.ylabel('New Cases')
plt.xticks(rotation=45)...",7,"The generated code correctly addresses most of the plotting requirements and demonstrates good understanding of `matplotlib` and `seaborn`.

**Strengths:**
*   Correctly loads the data and parses the 'date' column.
*   Successfully generates the time series plot for India's new cases.
*   Correctly creates the scatter plot for GDP per capita vs. total vaccinations using the latest entry per location.
*   Generates a proper histogram for 'reproduction_rate' and a box plot for 'stringency_index' by continent.
*   Includes proper titles and labels for most charts.
*   Uses `sns.set(style='whitegrid')` for better aesthetics.

**Areas for Improvement (Why it didn't get a higher score):**
*   **Bar Chart for Total Deaths:** The calculation for `total_deaths` per continent in the bar chart is incorrect. It sums *all* 'total_deaths' entries for each continent, not the latest (and thus most representative) 'total_deaths' for each country within that continent. The ground truth correctly handles this by taking the latest `total_deaths` per *location* and then summing those by continent. This is a common data aggregation challenge for cumulative metrics.
*   **Missing Error Handling:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth and good practice.
*   **Minor Customization:** While functional, some plots could benefit from slightly better customization (e.g., in the scatter plot, adding `edgecolor='w'` and `s=100` are good but not explicitly requested, whereas using `population` for `size` in the scatter plot as seen in the ground truth would have added more insight and been a good advanced feature). The bar chart for total deaths by continent could benefit from sorting the bars, similar to the ground truth.
*   **Redundancy in Plotting Code:** The `plt.figure()` and `plt.show()` calls are repeated for each plot. While syntactically correct, for many plots in a single script, it often makes more sense to structure them as subplots or functions to improve readability and potentially performance if many figures are being created and displayed sequentially. However, for a direct instruction to ""create visualizations,"" individual plots are acceptable.

Overall, it's a strong attempt with a significant logical flaw in one of the data aggregations. If the bar chart had been computed correctly, the score would be higher.",o4_mini,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"import pandas as pd

df = pd.read_csv('clash_royale_cards.csv')

# 1) List columns and dtypes
print(df.columns.tolist())
print(df.dtypes)

# 2) Descriptive statistics for specified numerical columns
for col in ['elixirCost', 'Win Rate', 'Usage']:
    print(f'\nStatistics for {col}:')
    print('Mean:', df[col].mean())
    print('Median:', df[col].median())
    print('Std:', df[col].std())
    print('Min:', df[col].min())
    print('Max:', df[col].max())

# 3) Total number of cards
print('\nTotal...",7,"The generated code correctly addresses most of the requirements but has some shortcomings in terms of robustness and presentation.

**Correctness:**
- Loads the CSV correctly.
- Identifies column names and data types.
- Calculates descriptive statistics for the specified columns accurately.
- Reports the total number of cards correctly.

**Code quality:**
- Uses proper imports.
- The iteration for statistics calculation is a bit verbose compared to using `agg()`, which is more concise and idiomatic for multiple statistics across multiple columns in pandas.
- Lacks error handling for file not found, which is crucial for a robust script.

**Completeness:**
- All functional requirements are met: loading data, column info, descriptive statistics, and total rows.
- The output format for ""List all column names"" is just `df.columns.tolist()`, which presents them as a Python list. The ground truth iterates and prints them individually, which is a bit more readable for a human, though `df.columns.tolist()` is functionally equivalent.
- The descriptive statistics output is printed column by column, which can be less organized than a table generated by `agg()`.

**Areas for Improvement:**
1.  **Error Handling:** Implement `try-except` blocks for `pd.read_csv` to handle `FileNotFoundError` gracefully, as seen in the ground truth. This makes the code more robust.
2.  **Conciseness for Statistics:** Use `df[numerical_cols].agg(['mean', 'median', 'std', 'min', 'max'])` instead of looping and printing each statistic individually. This is more efficient and produces a cleaner, tabular output, which is generally preferred for presenting statistics.
3.  **Clarity of Output:** While the output is functional, adding header prints (e.g., ""--- Column Names and Data Types ---"") as in the ground truth improves readability for the user.",o4_mini,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"import pandas as pd

df = pd.read_csv(""clash_royale_cards.csv"")
filtered = df[df[""elixirCost""] <= 3]
result = filtered[[""Card"", ""Win Rate""]]
result_sorted = result.sort_values(by=""Win Rate"", ascending=False)
print(result_sorted)",9,"The generated code correctly performs all the required operations: loading the CSV, filtering by `elixirCost`, selecting the specified columns, and sorting by `Win Rate` in descending order. The code is concise and directly addresses the problem.

The only minor area for improvement is the lack of error handling for file not found or missing columns, which the ground truth includes. However, for a typical data science benchmark scenario where the file is assumed to exist and column names are known, this is often omitted for brevity. Given the prompt's focus on the core data manipulation, the generated code is excellent.",o4_mini,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('clash_royale_cards.csv')

plt.figure(figsize=(10, 6))
sns.histplot(df['elixirCost'], bins=range(df['elixirCost'].min(), df['elixirCost'].max() + 2), edgecolor='black')
plt.title('Distribution of Elixir Cost')
plt.xlabel('Elixir Cost')
plt.ylabel('Count')
plt.tight_layout()
plt.show()
```",9,"The generated code correctly loads the CSV, generates a histogram for `elixirCost`, and includes a title and labeled axes. This fulfills all primary requirements of the task.

**Strong Points:**
*   Uses `pandas` for data loading and `matplotlib`/`seaborn` for plotting as requested.
*   Correctly identifies and plots the `elixirCost` column.
*   Includes a title (`plt.title`) and axis labels (`plt.xlabel`, `plt.ylabel`) which are essential for plot readability.
*   The use of `sns.histplot` is good practice for modern data visualization.
*   `bins=range(df['elixirCost'].min(), df['elixirCost'].max() + 2)` is a decent approach to binning for discrete integer data, ensuring each distinct elixir cost has a bin.
*   `plt.tight_layout()` is a good addition for preventing labels from overlapping.

**Areas for Minor Improvement (to reach a perfect 10):**
*   **Error Handling:** The code lacks robust error handling for `FileNotFoundError` when reading the CSV or if the `elixirCost` column is missing, which is a good practice in real-world scenarios. The ground truth includes this.
*   **Plot Customization:**
    *   The y-axis label ""Count"" is functional but ""Number of Cards"" from the ground truth is slightly more descriptive in this context.
    *   The ground truth also sets `kde=False` explicitly (which is good for discrete data like elixir cost) and uses `color='skyblue'` and sets explicit `xticks` for better readability, which are minor but beneficial enhancements.
    *   The range for `bins` in the ground truth (`range(1, 12)`) is more fixed and clearer for typical elixir costs (1-10) compared to dynamic min/max, which might be less precise for all `elixirCost` distributions, though `range(df['elixirCost'].min(), df['elixirCost'].max() + 2)` is also acceptable. The ground truth also explicitly sets `bins` to ensure each integer value has its own bin if the min/max is within a fixed range.",o4_mini,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# Load dataset
df = pd.read_csv('iris.csv')

# 1) List columns and data types
print(""Columns:"", list(df.columns))
print(""Data types:\n"", df.dtypes)

# 2) Descriptive statistics for numerical columns
numerical_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
stats = df[numerical_cols].agg(['mean', 'median', 'std', 'min', 'max'])
print(""\nDescriptive statistics:\n"", stats)

# 3) Unique species count and frequency
unique_species_count = df['Sp...",9,"The generated code correctly addresses all requirements of the task: loading the CSV, listing column names and data types, calculating descriptive statistics for the specified numerical columns, and reporting unique species counts. The code is concise and uses appropriate pandas functions. It demonstrates a clear understanding of the task.

One minor improvement would be to include error handling for file not found, as seen in the ground truth code. While not explicitly asked for in the prompt (as it assumed the CSV is in the current directory), robust code typically includes such error handling. Also, the `print` statements could be slightly more descriptive or formatted for better readability, similar to the ground truth. However, these are minor points and do not detract significantly from the correctness or completeness of the solution for the given prompt.",o4_mini,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

df = pd.read_csv('iris.csv')
filtered = df[df['Species'] == 'Iris-setosa']
print(filtered[['SepalLengthCm', 'SepalWidthCm']])
```",8,"The generated code correctly performs all the core requirements of the task: loading the CSV, filtering by 'Species' == 'Iris-setosa', and selecting and displaying the specified columns. It uses proper imports and pandas functionality effectively.

However, the generated code lacks error handling for common issues like `FileNotFoundError` or `KeyError` if the CSV file or expected columns are missing, which is present in the ground truth. While the prompt asks for ""only the Python code without explanations,"" and often error handling isn't explicitly requested in such short code generation tasks, it is a significant aspect of robust code quality and completeness in a real-world scenario. The ground truth demonstrates robust error handling, which makes it superior for production use. For a benchmark, a lack of error handling, while functional, isn't ideal.",o4_mini,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('iris.csv')

colors = {'Iris-setosa': 'red', 'Iris-versicolor': 'green', 'Iris-virginica': 'blue'}

plt.figure(figsize=(8, 6))
for species, group in df.groupby('Species'):
    plt.scatter(group['SepalLengthCm'], group['SepalWidthCm'], label=species, color=colors[species])

plt.title('Sepal Length vs Sepal Width by Species')
plt.xlabel('SepalLengthCm')
plt.ylabel('SepalWidthCm')
plt.legend()
plt.show()
```",9,"The generated code correctly achieves all the core requirements of the task. It loads the CSV, creates a scatter plot of `SepalLengthCm` vs `SepalWidthCm`, colors the points by `Species`, and includes a title and axis labels, along with proper imports. The use of `groupby('Species')` is an elegant and efficient way to process the data for plotting different species with different colors.

Minor points for improvement (but not critical enough to lower the score significantly):
1.  **Error Handling**: The ground truth includes `try-except` blocks for `FileNotFoundError` and missing columns, which is good practice for robust code. The generated code lacks this.
2.  **Hardcoded Colors**: The generated code hardcodes a dictionary for `colors`. While effective for this specific dataset, a more general approach might be to let Matplotlib assign colors automatically or use a colormap if there were many more species. However, for 3 species, explicitly defining them is clear.
3.  **Comments**: The generated code is concise but could benefit from a few comments explaining the steps, similar to the ground truth.

Overall, it's a very good and efficient solution that directly addresses the problem statement.",o4_mini,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

# Load dataset
df = pd.read_csv('heart.csv')

# 1) List all column names and data types
print(""Columns and data types:"")
print(df.dtypes)

# 2) Descriptive statistics for numerical columns
num_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']
stats = df[num_cols].agg(['mean', 'median', 'std', 'min', 'max'])
print(""\nDescriptive statistics for numerical columns:"")
print(stats)

# 3) Count of occurrences in HeartDisease column
print(""\nHeartDisease value counts:"")
...",8,"The generated code correctly performs all the requested tasks: loading the CSV, printing column data types, calculating descriptive statistics for the specified numerical columns, and reporting value counts for 'HeartDisease'. The imports are correct, and the code is structured logically.

Strengths:
- Correctly uses pandas for loading the CSV and performing data analysis.
- Accurately identifies and displays column data types.
- Correctly calculates mean, median, std, min, and max for the specified numerical columns using `agg()`.
- Correctly uses `value_counts()` for the 'HeartDisease' column.

Areas for Improvement (to reach a perfect score):
- **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` when loading the CSV, which is a good practice for production-ready code. The generated code lacks this.
- **Output Formatting:** While the output is technically correct, the ground truth provides more descriptive print statements and clear headings for each section, making the output much more readable and user-friendly. For example, explicitly listing column names separately from their data types helps distinguish them.
- **Robustness for column existence:** The ground truth includes checks to ensure the specified numerical columns and the `HeartDisease` column actually exist in the DataFrame before attempting operations on them. This makes the code more robust against variations in the dataset.
- **Clarity in `df.dtypes` output:** The request asked to ""List all column names and identify the data type of each column."" `df.dtypes` does this, but the ground truth explicitly prints ""Column Names:"" and then iterates over them, which can be seen as a more direct fulfillment of ""List all column names"".",o4_mini,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"import pandas as pd

df = pd.read_csv('heart.csv')
filtered = df[(df['Age'] > 50) & (df['HeartDisease'] == 1)]
result = filtered[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]
print(result)",9,"The generated code correctly loads the CSV, filters the data based on the specified conditions (`Age` > 50 AND `HeartDisease` == 1), and selects the required columns (`Age`, `Sex`, `ChestPainType`, `Cholesterol`). It also properly displays the final result. All core requirements of the task are met. The code is concise and directly addresses the problem.

The only minor area for improvement, which doesn't directly impact the correctness for this specific task but is good practice in a production setting, would be to wrap the logic in a function to make it reusable and testable, and to include error handling for `FileNotFoundError`. However, given the prompt asked only for ""Python code without explanations"" and focused on the data manipulation, the current solution is excellent in fulfilling the core task requirements.",o4_mini,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"import pandas as pd

df = pd.read_csv('winequality-red.csv', sep=';')
filtered = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]
best = filtered.loc[filtered['quality'].idxmax()]
print(best)",9,"The generated code correctly implements all the core requirements of the task: filtering with compound conditions, finding the maximum quality wine in the filtered subset, and displaying its features. It uses appropriate pandas methods efficiently.

Strong points:
- Correctly uses boolean indexing for compound filtering.
- Correctly identifies the highest quality wine using `idxmax()` and `loc`.
- Displays all features as requested by implicitly printing the `Series`.

Areas for minor improvement:
- **Missing error handling:** The ground truth includes `try-except` blocks for `FileNotFoundError`, `KeyError`, and general exceptions, which is good practice for robust code. The generated code lacks this.
- **Assumed separator:** The generated code uses `sep=';'`. While `winequality-red.csv` typically uses semicolons, specifying `sep=','` or relying on pandas' default for CSV is more common if the separator isn't explicitly known to be a semicolon. For this specific dataset, semicolon is correct, but it's a detail worth noting for generalization.
- **Readability/Structure:** The ground truth uses a function `analyze_wine_quality` and defines `DATASET_PATH` as a constant, which improves code organization and reusability, especially for more complex scripts. The generated code is concise but less structured.

Given the prompt's focus on ""expert code evaluator for a data science benchmark"" and ""Correctness,"" ""Code quality,"" ""Completeness,"" the generated code delivers on the core data manipulation aspects very well. The lack of robustness (error handling) and slightly less structured approach (no function, no constants) are minor detractions in a pure ""does it solve the task efficiently"" context.",o4_mini,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('heart.csv')

# Histogram for Age
plt.figure(figsize=(8, 6))
sns.histplot(data=df, x='Age', bins=15, kde=False, color='skyblue')
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

# Bar chart for Sex
plt.figure(figsize=(6, 6))
sns.countplot(data=df, x='Sex', palette='pastel')
plt.title('Sex Distribution')
plt.xlabel('Sex')
plt.yl...",8,"The generated code correctly loads the dataset and generates both a histogram for `Age` and a bar chart for `Sex`, with appropriate titles and labels. This addresses the core requirements of the task.

**Strong points:**
*   Correctly uses `pandas` to load the CSV.
*   Correctly uses `seaborn` for plotting.
*   Generates a histogram for `Age` and a bar chart for `Sex`.
*   Includes titles and labeled axes for both plots.
*   Properly imports necessary libraries.

**Areas for improvement:**
1.  **Robustness (Error Handling):** The generated code lacks error handling for `FileNotFoundError` or `KeyError`, which is present in the ground truth. While not explicitly asked for in the prompt's requirements, good production-ready code often includes this.
2.  **Clarity for 'Sex' Column:** The ground truth code intelligently maps the numerical `Sex` column (0, 1) to descriptive labels ('Male', 'Female'). The generated code plots the raw numerical values, which is less intuitive for a bar chart representing 'Sex'. Although technically it shows the distribution, it's not as user-friendly or informative as the ground truth's approach.
3.  **Plot Refinements:** The ground truth includes `kde=True` for the histogram, which adds a density estimate, and `bins='auto'`, which can be more robust than a fixed `bins=15`. It also adds `grid` and `plt.tight_layout()` to both plots for better presentation, which are good practices. The generated code includes `plt.tight_layout()` but could benefit from other aesthetic refinements.

Overall, it's a solid attempt that fulfills the primary plotting requirements. The improvements would make it more robust, user-friendly, and visually polished, aligning more closely with best practices for data visualization.",o4_mini,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

# 1) Load dataset and list columns with dtypes
df = pd.read_csv(""spotify_2023.csv"")
print(""Columns and dtypes:"")
print(df.dtypes)

# 2) Convert streams to numeric, coerce errors, report NaNs
df[""streams""] = pd.to_numeric(df[""streams""], errors=""coerce"")
nan_count = df[""streams""].isna().sum()
print(f""\nNumber of NaNs in 'streams': {nan_count}"")
if nan_count > 0:
    print(""Rows with NaN in 'streams':"")
    print(df[df[""streams""].isna()])

# 3) Descriptive statistics ...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
*   Loads the CSV correctly.
*   Correctly identifies and prints column dtypes using `df.dtypes`.
*   Successfully converts the `streams` column to numeric, handling errors by coercing to NaN, and correctly reports the count of NaNs.
*   Calculates and reports all required descriptive statistics (mean, median, std, min, max) for all specified numerical columns.
*   Correctly counts and reports the total number of unique artists.

**Areas for improvement to reach a higher score:**
*   **Initial Column & Type Reporting:** While `print(df.dtypes)` shows the types, the ground truth's use of `df.info()` provides more comprehensive information (non-null counts, memory usage) which is generally preferred for data exploration and reporting. The task asked to ""list all column names and identify the data type of each,"" `df.info()` does this more completely.
*   **NaN Reporting Detail:** The generated code prints all rows with NaN in `streams` if `nan_count > 0`. For a large dataset, this could be excessive. The ground truth code limits this to `.head()`, which is a better practice for reporting samples without overwhelming the output.
*   **Error Handling for File Loading:** The generated code directly calls `pd.read_csv`, which would raise a `FileNotFoundError` if the CSV isn't present, crashing the script. The ground truth includes a `try-except` block to gracefully handle this, which is good practice for robust code.
*   **Descriptive Statistics Presentation:** While correct, printing each statistic separately in a loop is less concise and harder to read than using `df[num_cols].describe()` or `df[num_cols].describe().loc[['mean', '50%', 'std', 'min', 'max']]` as in the ground truth, which provides a clean, single-table output.
*   **Robustness for Column Existence:** The generated code assumes all specified columns (`num_cols`, `artist(s)_name`) exist. If one were missing (e.g., a typo in the column name), it would raise a `KeyError`. The ground truth includes checks (e.g., `if col in df.columns:` or `if 'artist(s)_name' in df.columns:`) to make the code more robust against missing columns, which is a good practice, especially when dealing with potentially varying datasets.",o4_mini,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"import pandas as pd

df = pd.read_csv('spotify_2023.csv')
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

filtered = df[df['released_year'] == 2023]
result = filtered[['track_name', 'artist(s)_name', 'streams']].sort_values(by='streams', ascending=False)

print(result)",9,"The generated code correctly addresses all the requirements of the task. It loads the CSV, filters by `released_year`, converts `streams` to numeric, selects the specified columns, and sorts the data in descending order by streams. The code is concise and readable.

Strong points:
- All task requirements are met.
- Proper use of `pd.to_numeric` with `errors='coerce'`.
- Correct filtering and column selection.
- Correct sorting with `ascending=False`.

Minor areas for improvement to reach a perfect score (though not strictly required given the prompt's simplicity):
- The ground truth includes error handling for `FileNotFoundError`, which is good practice for production code but not explicitly asked for by the prompt.
- The ground truth also explicitly drops NaNs in the 'streams' column before sorting, which can be useful depending on whether NaNs should appear at the end of the sorted list or be excluded. The generated code implicitly places NaNs at the end (pandas default behavior). For this specific problem, not dropping NaNs is acceptable.",o4_mini,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('spotify_2023.csv')

plt.figure(figsize=(8, 5))
plt.hist(df['bpm'], bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of BPM')
plt.xlabel('Beats Per Minute (BPM)')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8, 5))
plt.scatter(df['danceability_%'], df['energy_%'], alpha=0.7, color='green')
plt.title('Danceability vs Energy')
plt.xlabel('Danceability (%)')
plt.ylabel('Energy (%)')
plt.sh...",9,"The generated code correctly loads the CSV, creates a histogram for 'bpm', and a scatter plot for 'danceability_%' vs 'energy_%' with appropriate titles and axis labels. It addresses all the core requirements of the task.

**Strong points:**
*   Correctly uses `pandas` for CSV loading and `matplotlib.pyplot` for plotting.
*   Correctly generates a histogram for 'bpm' with a title and labeled axes.
*   Correctly generates a scatter plot for 'danceability_%' vs 'energy_%' with a title and labeled axes.
*   The code is concise and readable.

**Areas for minor improvement (to achieve a perfect 10):**
*   The `plt.show()` calls after each plot will display them one by one. Typically, you'd want to call `plt.show()` only once at the very end to display all generated figures simultaneously, or explicitly create `fig, ax` objects for more control, especially when generating multiple plots programmatically. The current approach works, but is less common for ""all plots at once"" display.
*   Error handling for file not found or missing columns (as seen in the ground truth) is absent. While not explicitly asked for, robust code would include it.
*   The `figsize` for both plots is `(8, 5)`, which is reasonable but could be omitted if the default is acceptable, or explicitly set separately if different sizes are desired. This is a minor stylistic point.",o4_mini,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd

# Load dataset
df = pd.read_csv('youtube_trendings.csv')

# 1) List all column names and their inferred data types
print(""Column names and data types:"")
print(df.dtypes)

# 2) Clean numeric columns
cols = ['Video views', 'Likes', 'Dislikes']
for col in cols:
    df[col] = pd.to_numeric(df[col].str.replace(',', '', regex=False), errors='coerce').fillna(0)

# 3) Descriptive statistics
print(""\nDescriptive statistics for cleaned columns:"")
print(df[cols].agg(['mean', ...",9,"The generated code is largely correct and addresses all requirements of the task.

**Strong points:**
- Correctly loads the CSV using pandas.
- Accurately identifies and lists column names and data types (via `df.dtypes`).
- Efficiently cleans the specified numeric columns (`Video views`, `Likes`, `Dislikes`) by removing commas, converting to numeric with `errors='coerce'`, and then filling NaNs with 0 in a concise loop.
- Calculates and prints the required descriptive statistics (mean, median, std, min, max) for the cleaned numeric columns using the `agg` function, which is a good practice.
- Correctly fills missing values in the 'Category' column with 'Unknown'.
- Accurately reports the count of unique values in the 'Category' column.
- Includes proper imports.

**Minor areas for improvement (to reach a perfect 10):**
- **Error Handling:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which makes the script more robust. The generated code assumes the file will always be present. While not explicitly asked for, robust error handling is good practice.
- **Output Clarity:** While all the necessary outputs are printed, the ground truth uses more descriptive print statements (e.g., ""--- Column Names and Inferred Data Types ---"", ""Statistics for 'col':"") which makes the output more user-friendly and easier to interpret, especially for someone running the code for the first time. The generated code's output for `df.dtypes` is concise but doesn't explicitly label column names vs. data types as much as `df.info()` or more verbosely formatted print statements.
- **`df.info()` vs `df.dtypes`**: The prompt asked to ""List all column names and their inferred data types."" While `df.dtypes` shows this, `df.info()` (used in the ground truth) provides more comprehensive information, including non-null counts, which is generally more informative. However, `df.dtypes` directly fulfills the literal request.

Overall, it's an excellent solution that performs all requested operations correctly and efficiently. The points for improvement are mostly about robustness and output formatting rather than core functionality, making the code highly effective for the task.",o4_mini,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

df = pd.read_csv('youtube_trendings.csv')

# Clean the 'Video views' column
df['Video views'] = (
    df['Video views']
    .astype(str)
    .str.replace(',', '', regex=False)
)
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce').fillna(0)

# Filter, select, and sort
filtered = df[df['Video views'] > 50_000_000]
result = filtered[['Video', 'Video views']].sort_values('Video views', ascending=False)

print(result)
```",10,"The generated code is excellent. It correctly implements all requirements: loading the dataset, cleaning the 'Video views' column (removing commas, converting to numeric, handling errors with NaNs, and filling NaNs with 0), filtering based on views, selecting the specified columns, and sorting the results in descending order. The code is concise, efficient, and well-structured, demonstrating good pandas practices. The single chain for `to_numeric` and `fillna` is a good optimization. The `try-except` block for `FileNotFoundError` is not explicitly required by the prompt, so its absence is not a detractor.",o4_mini,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('youtube_trendings.csv')
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)

plt.scatter(df['Video views'], df['Likes'])
plt.title('Video Views vs. Likes')
plt.xlabel('Video Views (cleaned)')
plt.ylabel('Likes (cleaned)')
plt.show()
```",9,"The generated code successfully addresses all requirements of the task. It loads the CSV, correctly cleans the 'Likes' and 'Video views' columns by removing commas, converting to numeric, and filling NaNs with 0. It then generates a scatter plot with the specified title and axis labels. The imports are proper.

The only minor area for improvement that prevents a perfect 10 is the lack of error handling for file loading (e.g., `FileNotFoundError`) and the assumption that the `Likes` and `Video views` columns will always be present and directly convertible to string for replacement. While this code works for the given problem context, robust production code would include such error handling as seen in the ground truth. However, for a benchmark evaluation focusing on the core task logic, it performs excellently.",o4_mini,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"import pandas as pd

df = pd.read_csv('mobiles_2025.csv')
df['Mobile Weight'] = pd.to_numeric(df['Mobile Weight'].str.replace('g','',regex=False), errors='coerce')
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB','',regex=False), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh','',regex=False), errors='coerce')
df['Launched Price (USA)'] = pd.to_numeric(
    df['Launched Price (USA)']
      .str.replace('USD ','',regex=False)
      .str.replace...",8,"The generated code correctly performs all the specified cleaning and analysis tasks. It successfully loads the data, cleans the four specified columns by removing units and converting them to numeric, handles conversion errors, prints the data types, calculates descriptive statistics for the relevant columns, and reports the unique count for 'Company Name'.

**Strong points:**
*   Concise and effective use of pandas for string cleaning and numeric conversion.
*   Correctly handles conversion errors using `errors='coerce'`.
*   All required outputs are generated.

**Areas for improvement (to reach a 9 or 10):**
*   **Robustness/Error Handling:** The code assumes the CSV file exists. Adding a `try-except` block for `pd.read_csv` would make it more robust against `FileNotFoundError`.
*   **Readability:** While the chained `.str.replace()` and `pd.to_numeric()` calls are efficient, explicitly adding `.str.strip()` to remove leading/trailing spaces after removing units (e.g., `'100 g'` becomes `'100 '` after `g` is removed) would make the cleaning more thorough, though the current `pd.to_numeric` would likely handle it. The ground truth includes this for better practice.
*   **Descriptive Statistics Output:** The ground truth specifically asks for mean, median, std, min, max. The generated code uses `describe()` which includes 25%, 75% quantiles and count, which is more than explicitly asked for but acceptable. The ground truth then extracts specific metrics which matches requirements more precisely.
*   **Structure:** For a production-level script, encapsulating the logic in a function and using `if __name__ == ""__main__"":` as in the ground truth would be better practice for reusability and clarity, although for a simple script, the direct approach is fine.
*   **Clarity of Output:** The output for descriptive statistics is a DataFrame, which is clear, but adding print statements explaining what each output section represents (e.g., ""Column Data Types:"", ""Descriptive Statistics:"") would improve user experience, as done in the ground truth.",o4_mini,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"import pandas as pd

df = pd.read_csv('mobiles_2025.csv')

df['Launched Price (India)'] = (
    df['Launched Price (India)']
    .str.replace('INR ', '', regex=False)
    .str.replace(',', '', regex=False)
)
df['Launched Price (India)'] = pd.to_numeric(df['Launched Price (India)'], errors='coerce')

df['RAM'] = df['RAM'].str.replace('GB', '', regex=False)
df['RAM'] = pd.to_numeric(df['RAM'], errors='coerce')

filtered = df[
    (df['Company Name'] == 'Apple') &
    (df['Launched Price (India)'] ...",10,"The generated code is highly correct and complete, addressing all requirements of the task. It successfully loads the dataset, cleans both the 'Launched Price (India)' and 'RAM' columns by removing specified strings and converting them to numeric, handling errors by coercing to NaN. It then correctly filters the dataset for 'Apple' company and prices under 100,000, selects the specified columns, and sorts the result by price in ascending order by default. The use of method chaining for cleaning the price column is efficient. Proper imports are included. The code is concise and effectively solves the problem.",o4_mini,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean RAM column
df['RAM'] = df['RAM'].str.replace('GB', '', regex=False)
df['RAM'] = pd.to_numeric(df['RAM'], errors='coerce')

# Clean Battery Capacity column
df['Battery Capacity'] = df['Battery Capacity'].str.replace('mAh', '', regex=False)
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'], errors='coerce')

# Plot
plt.figure(figsize=(10,6))
sns.scatte...",9,"The generated code successfully addresses almost all requirements of the task.

**Strong points:**
*   **Correct CSV Loading:** Loads the `mobiles_2025.csv` dataset correctly.
*   **Correct Cleaning of `RAM` and `Battery Capacity`:** Successfully removes 'GB' and 'mAh' and converts the columns to numeric, correctly using `errors='coerce'` to handle conversion errors by setting them to NaN.
*   **Correct Scatter Plot Generation:** Creates a scatter plot with `Battery Capacity` on the y-axis and `RAM` on the x-axis.
*   **Correct Coloring:** Colors the points by `Company Name` as required.
*   **Proper Plot Labels and Title:** Includes a title and labels for both axes.
*   **Efficient Imports:** Uses `pandas`, `matplotlib.pyplot`, and `seaborn` as expected.
*   **Leverages Seaborn:** Uses `sns.scatterplot` which is good for this kind of visualization and handling `hue`.

**Areas for minor improvement:**
*   **In-place modification of cleaned columns:** The generated code reassigns the cleaned data back to the original 'RAM' and 'Battery Capacity' columns (e.g., `df['RAM'] = df['RAM'].str.replace('GB', '', regex=False)`). While this works, it overwrites the original, potentially useful string data. The ground truth code creates new columns (`RAM_cleaned`, `Battery Capacity_cleaned`), which is generally safer and preserves the original data for inspection if needed. For this specific task, it's not a critical issue, but it's a good practice point.
*   **Error handling for file not found:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. The generated code does not, which means it would crash if the file isn't present. While the prompt implied the file would be present, robust code usually includes this.
*   **Whitespace stripping:** The ground truth includes `str.strip()` after removing 'GB' or 'mAh'. This accounts for cases like ""10 GB"" or ""10GB"" where there might be leading/trailing spaces after removal, ensuring `pd.to_numeric` works cleanly. The generated code omits this, but for well-formatted data, it might not be an issue.

Overall, the code is highly functional and meets the core requirements effectively. The minor points are more about robustness and best practices rather than direct failures of the task.",o4_mini,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd

df = pd.read_csv('titanic.csv')

print(df.head())
print(""Shape:"", df.shape)
print(df.dtypes)
print(f""Survival Rate: {df['Survived'].mean() * 100:.2f}%"")
print(df.isnull().sum())
```",7,"The generated code correctly performs all the requested tasks: loading the data, displaying the head, showing the shape, listing dtypes, calculating survival rate, and counting missing values. The calculations are accurate.

However, the code lacks the explicit print statements and labels that were present in the ground truth for improved readability and clarity, as requested by ""Provide only the Python code **without explanations.**"" which implies the output should be clear. For example, instead of just `print(df.head())`, providing ""1. First 5 rows of the dataset:"" before it makes the output more user-friendly. Also, the `try-except` block for file loading is missing, which makes the code less robust. The use of `.mean()` for the survival rate is a clever and concise way to calculate it, showcasing good use of pandas functionality.

To improve, add descriptive print statements for each output section and consider including error handling for file loading.",o4_mini,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

df = pd.read_csv('imdb_top_250.csv')

# 1) List all columns and data types
print(df.dtypes)

# 2) Descriptive statistics for Year, Rating, Runtime
for col in ['Year', 'Rating', 'Runtime']:
    print(f""{col} statistics:"")
    print(""  Mean:  "", df[col].mean())
    print(""  Median:"", df[col].median())
    print(""  Std:   "", df[col].std())
    print(""  Min:   "", df[col].min())
    print(""  Max:   "", df[col].max())

# 3) Total number of movies
print(""Total movies:"", df...",6,"The generated code addresses most of the task requirements but lacks robustness and detail in several areas.

**Strong Points:**
*   Correctly loads the CSV file.
*   Successfully prints column dtypes.
*   Calculates and prints mean, median, std, min, max for the specified columns.
*   Correctly reports the total number of movies.

**Areas for Improvement / Missing Aspects:**
*   **Robustness (File Handling):** The code does not include any error handling for `FileNotFoundError`. If `imdb_top_250.csv` is not present, the script will crash. The ground truth demonstrates robust `try-except` block for file loading.
*   **Runtime Column Preprocessing:** The most critical missing piece is the handling of the 'Runtime' column. In many datasets, 'Runtime' is stored as a string (e.g., '142 min'). The generated code assumes 'Runtime' is already numeric. If not, `df[col].mean()` and other statistical computations will fail for 'Runtime'. The ground truth explicitly handles this conversion.
*   **Output Format for Column Types:** While `df.dtypes` is correct, `df.info()` provides a more comprehensive overview including non-null counts, which is often more informative for data inspection.
*   **Descriptive Statistics Output Format:** The current output for descriptive statistics is verbose, printing each statistic separately for each column. Using `df[numerical_columns].describe()` as in the ground truth provides a much cleaner, more concise, and professional-looking summary table.
*   **Clarity and Readability:** The use of f-strings for printing statistics is good, but combining all statistics for multiple columns into a single `describe()` call (as shown in the ground truth) is generally preferred for data analysis reporting.
*   **Configuration:** The ground truth uses constants for file path and numerical columns, which improves maintainability.

Overall, the code provides a functional solution to the basic requirements but falls short in terms of error handling, data preprocessing (specifically for `Runtime`), and presenting results in a standard, clean format suitable for data analysis.",o4_mini,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

df = pd.read_csv('imdb_top_250.csv')
result = (
    df[df['Rating'] >= 9.0]
    [['Title', 'Rating']]
    .sort_values('Rating', ascending=False)
)
print(result)
```",9,"The generated code correctly addresses all the requirements of the task: it loads the CSV, filters by rating, selects the specified columns, and sorts the data in descending order by rating. The use of method chaining for operations is concise and idiomatic for pandas.

The only minor area for improvement is the lack of error handling (e.g., `FileNotFoundError`, `KeyError`) which is present in the ground truth. While not explicitly asked for by the prompt's requirements for ""proper imports and assume the CSV file is in the current directory,"" robust code in a real-world scenario would include such handling. However, given the specific constraints of the prompt, the generated code performs admirably.",o4_mini,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('imdb_top_250.csv')

plt.figure(figsize=(10, 6))
sns.histplot(df['Rating'], bins=10, kde=True)
plt.title('Distribution of IMDb Ratings')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='Runtime', y='Rating')
plt.title('IMDb Rating vs Runtime')
plt.xlabel('Runtime (minutes)')
plt.ylabel('Rating')
plt.show()
```",7,"The generated code correctly performs the requested plotting tasks (histogram for Rating, scatter plot for Rating vs Runtime) and includes appropriate titles and axis labels. Imports are correct.

However, there are two main areas for improvement that prevent a higher score:
1.  **Robustness to File Not Found:** The code does not include any error handling for `FileNotFoundError` when trying to load the CSV, which is bad practice for production-ready code and could lead to a hard crash if the file is missing or misspelled.
2.  **Handling of 'Runtime' Column:** The `Runtime` column in the `imdb_top_250.csv` dataset is typically in a string format (e.g., '120 min'). The generated code attempts to directly plot it without any preprocessing, which would lead to an error or an incorrect plot if `Runtime` is not numeric. The ground truth explicitly handles this by extracting the numerical part and converting it. This preprocessing is crucial for the scatter plot to function correctly and meaningfully. Without it, the code will likely fail or produce a misleading plot if `Runtime` is not already numeric (which it usually isn't in this dataset).",o4_mini,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load dataset
df = pd.read_csv('penguins.csv')

# 1) Report missing values per column
print(""Missing values per column:"")
print(df.isna().sum())

# 2) Drop rows with NA in specified numerical columns and report descriptive stats
numeric_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_num = df.dropna(subset=numeric_cols)
print(""\nDescriptive statistics for numerical columns after dropping rows with NA:"")
print(df_num[numeric_co...",7,"The generated code addresses most of the requirements but lacks robustness and detail in its reporting.

**Strengths:**
*   **Correctness:** The code correctly loads the data, calculates missing values for each column, drops NAs in the specified subset of columns, calculates the requested descriptive statistics on the filtered data, and counts unique values for 'species' and 'island'.
*   **Conciseness:** It's more concise than the ground truth while still performing the core operations.

**Areas for Improvement (Why not higher):**
*   **Robustness (Error Handling):** The most significant missing piece is the lack of error handling for `FileNotFoundError` when loading the CSV. The ground truth includes a robust `try-except` block, which is crucial for real-world applications.
*   **Reporting Detail:**
    *   **Missing Values:** While it prints `df.isna().sum()`, it doesn't explicitly filter to show *only* columns with missing values or provide a message if there are no missing values, as the ground truth does. The output for missing values is less user-friendly if all columns have NAs.
    *   **Descriptive Statistics:** It doesn't report the number of rows before and after filtering, which is a valuable piece of information for understanding the impact of the `dropna` operation.
    *   **Output Clarity:** The ground truth includes more descriptive print statements (e.g., ""--- Missing Values Report ---"", ""--- Descriptive Statistics for Filtered Numerical Columns ---"", ""--- Unique Values Report ---""). The generated code's print statements are less structured.
*   **Assumptions:** It directly accesses `df['species']` and `df['island']` without checking if these columns actually exist in the DataFrame, which could lead to a `KeyError` if the dataset structure changes. The ground truth adds checks for column existence.",o4_mini,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"import pandas as pd

df = pd.read_csv('penguins.csv')
filtered = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')]
result = filtered[['culmen_length_mm', 'culmen_depth_mm']].dropna()
print(result.head())",9,"The generated code correctly performs all the required operations: loading the CSV, filtering by species and island, selecting the specified columns, dropping rows with missing values, and displaying the first 5 rows. The code is concise and efficient.

The only minor improvements compared to the ground truth are:
1. The ground truth includes error handling for `FileNotFoundError` which is good practice for robustness, though not explicitly requested in the prompt.
2. The ground truth explicitly uses `subset` in `dropna`, which is slightly more robust if there were other columns in `selected_columns_df` whose `NaN`s should not trigger a drop. However, in this specific case, since `selected_columns_df` *only* contains `culmen_length_mm` and `culmen_depth_mm`, `dropna()` without `subset` has the same effect.
3. The print statement in the ground truth is more descriptive (""First 5 rows of the processed DataFrame:"").

Overall, the generated code fulfills the task requirements very well.",o4_mini,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv(""penguins.csv"")
df = df.dropna(subset=[""culmen_length_mm"", ""culmen_depth_mm"", ""species""])

sns.scatterplot(
    data=df,
    x=""culmen_length_mm"",
    y=""culmen_depth_mm"",
    hue=""species""
)

plt.title(""Culmen Length vs Culmen Depth by Species"")
plt.xlabel(""Culmen Length (mm)"")
plt.ylabel(""Culmen Depth (mm)"")
plt.show()
```",9,"The generated code correctly loads the CSV, filters out rows with NAs in the specified columns, and generates the scatter plot with the correct axes, hue, title, and labels. It uses proper imports and effectively addresses all requirements of the task.

The only minor area for improvement is that it does not include error handling for `FileNotFoundError` as the ground truth does. While not explicitly requested in the prompt, robust code often includes this. However, for the core task requirements, it is excellent.",o4_mini,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

# 1) Read the dataset
df = pd.read_csv('penguins.csv')

# 2) Fill NA values with column means
for col in ['culmen_length_mm', 'culmen_depth_mm']:
    mean_val = df[col].mean()
    df[col].fillna(mean_val, inplace=True)

# 3) Calculate and print statistics
stats = df[['culmen_length_mm', 'culmen_depth_mm']].agg(['mean', 'median', 'std'])
print(""Statistics for culmen_length_mm and culmen_depth_mm after cleaning:"")
print(stats)

# 4) Print total rows and columns
rows,...",8,"The generated code correctly addresses all the core requirements of the task. It successfully loads the dataset, fills NA values for the specified columns with their respective means, calculates and prints the required statistics (mean, median, standard deviation), and reports the dataset's dimensions.

Strong points:
- **Conciseness and effectiveness:** The code is very compact and uses pandas' built-in functions effectively for NA imputation and statistics calculation. The `agg` function is a clean way to get multiple statistics.
- **Correctness:** All calculations and data manipulations are performed correctly as per the task description.

Areas for improvement (to achieve a higher score):
- **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError`. The generated code lacks this, which makes it less robust in a real-world scenario where the file might be missing.
- **Output Clarity:** While the statistics are printed, the ground truth's output format is more verbose and user-friendly, explicitly labeling mean, median, and standrd deviation for each column. The `agg` output is correct but less granular in its presentation for an end-user.
- **Variable usage:** Hardcoding column names in the statistics printout could be slightly improved by iterating through the list of columns as done in the NA filling loop, which would make it more robust if the list of columns changed.

Overall, it's a very good solution that perfectly accomplishes the task requirements. The suggested improvements are mainly about robustness and output formatting rather than core functionality.",o4_mini,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv(""penguins.csv"")

# 2) Drop rows where `flipper_length_mm` is NA
df_clean = df.dropna(subset=[""flipper_length_mm""])

# 3) Histogram of the `flipper_length_mm` column
plt.figure(figsize=(8, 5))
sns.histplot(df_clean[""flipper_length_mm""], bins=20, kde=False)
plt.title(""Distribution of Flipper Length (mm)"")
plt.xlabel(""Flipper Length (mm)"")
plt.ylabel(""Count"")
plt.tight_layout(...",8,"The generated code correctly addresses all core requirements of the task. It successfully loads the dataset, handles NA values in `flipper_length_mm`, and generates both the histogram for flipper length and the bar chart for island counts with appropriate titles and labels.

**Strengths:**
*   **Correctness:** All answes are correct.
*   **Clarity:** The code is well-structured and easy to understand.
*   **Conciseness:** It gets the job done efficiently without unnecessary verbosity.
*   **Correct Libraries:** Uses `pandas`, `seaborn`, and `matplotlib.pyplot` as requested.
*   **Labels and Titles:** All plots have correct titles and axis labels.

**Areas for Improvement (to achieve a higher score):**
*   **Error Handling for File Loading:** The ground truth includes `try-except` blocks for `FileNotFoundError` when loading the CSV, which is a robust way to handle potential file issues. The generated code assumes the file will always be present.
*   **Consistency in `df_clean` usage:** The bar chart for `island` counts uses `df` instead of `df_clean`. While `island` might not have NAs in this specific dataset, using `df_clean` consistently after the NA drop is generally better practice to ensure all subsequent operations are on the cleaned data.
*   **Plot Customization:** The ground truth applies `sns.set_theme`, adds a `kde` to the histogram, and includes `grid` lines, `xticks`/`yticks` font sizes, and `plt.tight_layout()` for both plots, which enhance readability and presentation. The generated code is functional but less polished in terms of visualization aesthetics.
*   **Informative Print Statements:** The ground truth includes print statements that inform the user about the data loading status, NA counts, and changes in DataFrame shape, which can be very helpful for debugging and understanding data transformations.",o4_mini,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"import pandas as pd

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Filter to penguins from the 'Torgersen' island
torgersen_penguins = df[df['island'] == 'Torgersen']

# 3) Count and print the number of penguins for each species
species_counts = torgersen_penguins['species'].value_counts()
print(species_counts)",8,"The generated code correctly performs all the requested tasks: loading the dataset, filtering for 'Torgersen' island, and counting species. It is concise and directly addresses the prompt.

**Strong Points:**
- **Correctness:** The code correctly implements the logic for filtering and counting species.
- **Conciseness:** It achieves the task with minimal lines of code, which is good for simple scripts.
- **Readability:** The variable names are clear and the steps are logical.

**Areas for Improvement (reflected in the difference from a 9/10):**
- **Error Handling:** The ground truth code includes robust error handling for `FileNotFoundError` and checks for the existence of required columns (`island`, `species`). The generated code lacks these checks, which could lead to runtime errors if the CSV doesn't exist or is malformed.
- **Edge Cases:** The ground truth also handles the case where `Torgersen` island might not have any entries, providing a more informative message.
- **Clarity of Output:** While `print(species_counts)` works, the ground truth provides a more descriptive print statement for the results, which is better for user comprehension.

Overall, it's a very solid and functional solution given the direct requirements but could be improved with more robust error handling and user feedback.",o4_mini,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

# 1) Print the shape of the DataFrame
print(""Shape:"", df.shape)

#...",10,"The generated code perfectly addresses all requirements of the task. It correctly defines the DataFrame, accurately prints its shape, and correctly lists the data types of each column. The code is clean, well-structured, and directly implements the requested operations without any unnecessary complexity. It demonstrates a complete understanding of the pandas operations required.",o4_mini,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('titanic.csv')

# 1) Bar chart: survivors vs non-survivors
survival_counts = df['Survived'].value_counts().sort_index()
plt.figure(figsize=(6,4))
plt.bar(['Did Not Survive', 'Survived'], survival_counts, color=['red','green'])
plt.title('Survivors vs Non-Survivors')
plt.xlabel('Survival Status')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

# 2) Bar chart: distribution of passenger classes
pclass_cou...",7,"The generated code correctly creates four distinct plots with proper titles and labels for Survival Status, Passenger Class, Age Distribution, and Gender. It uses `pandas` and `matplotlib.pyplot` as requested.

However, there are a few areas for improvement:
1.  **Age Handling:** The task explicitly asks to ""handle missing values appropriately"" for age. The generated code uses `dropna()`, which removes rows with missing age values. While this *handles* missing values, it's not always the most appropriate method, especially if a large percentage of age data is missing, as it can lead to data loss. The ground truth fills missing age values with the median, which is a common and often preferred imputation method for numerical data.
2.  **Dataset Loading Robustness:** The generated code assumes `titanic.csv` is present. The ground truth includes a `try-except` block to handle `FileNotFoundError`, making it more robust.
3.  **Readability/Clarity:** For the first two plots, the generated code manually creates the x-axis labels (`['Did Not Survive', 'Survived']` and `['1st Class','2nd Class','3rd Class']`). While functional, using `index.map` or direct indexing on the DataFrame/Series to set labels (as in the ground truth for ""Survived"") can be more dynamic and less error-prone if the unique values change.
4.  **Minor Aesthetic Improvements:** The ground truth code includes `plt.xticks(rotation=0)` for bar charts and `plt.grid` which are good practices for readability and professional-looking plots. The generated code lacks these.",o4_mini,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]
}

df = pd.DataFrame(data)

avg_midterm = df['Midterm_Score'].mean()
print(f""Average Midterm Score: {...",9,"The generated code is highly correct and addresses all the requirements of the task. It successfully creates the DataFrame, calculates and prints the average midterm and final scores, and identifies the subject with the highest average final score. The code quality is good, using appropriate pandas functionalities.

One minor point for improvement, which keeps it from a perfect 10, is that the print statements for the average scores in the generated code are slightly less descriptive than in the ground truth (""Average Midterm Score:"" vs. ""Average Midterm Score for all students:""). Also, the ground truth explicitly prints the actual highest average score value for the subject, which isn't strictly asked for but adds clarity. However, these are minor stylistic differences and do not hinder the correctness or completeness of the solution.",o4_mini,LLM_judge_config
E51,easy,Time Series Analysis,,"import pandas as pd

data = {
    'Date': pd.to_datetime([
        '2024-01-01', '2024-01-02', '2024-01-03',
        '2024-01-04', '2024-01-05', '2024-01-08'
    ]),
    'Ticker': ['TECHCO'] * 6,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
    'Volume': [1000000, 1200000, 1100000, 1300000, 900000, 1050000]
}

df = pd.D...",9,"The generated code correctly performs all the required tasks: it creates the DataFrame with the 'Date' column as the index, calculates the 'Daily_Range' column, finds the date with the highest 'Close' price, and prints the updated DataFrame including the new column. The code quality is good, using concise pandas operations (`set_index`, `idxmax`).

The only minor point of improvement compared to the ground truth is the formatting of the printed highest 'Close' price date. The ground truth uses `strftime('%Y-%m-%d')` for a consistent and clear date format, whereas the generated code just prints the `Timestamp` object directly, which is less user-friendly for a simple date output. This is a very minor aesthetic difference and does not impact functionality.",o4_mini,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics']*7,
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST001']
}
df = pd.DataFrame(data)

# 1) Calculate total revenue for each transactio...",9,"The generated code correctly creates the DataFrame, calculates the 'Total_Revenue' column, identifies the product sold the most by quantity, and prints the head of the DataFrame with the new column. The code is well-structured and uses appropriate pandas functions.

One minor point for improvement:
- The print statement for the most sold product uses a slightly different phrasing than the ground truth. While functionally equivalent, adhering to the exact phrasing requested (if applicable) can sometimes be beneficial in specific benchmark scenarios.
- The `df.head()` call implicitly prints the first 5 rows, which matches the requirement. However, `df.head(5)` is more explicit and leaves no room for ambiguity, though the result is the same.",o4_mini,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 1) Calculate and print the average SessionDurationMinutes
avg_duration = df[""SessionDurationMinutes""]....",9,"The generated code is highly correct and complete, successfully addressing all requirements of the task. It correctly defines the DataFrame, calculates the average session duration, and counts users by device type using appropriate pandas methods (`.mean()` and `.value_counts()`). The code is well-structured and uses clear variable names.

The only minor area for improvement, preventing a perfect 10, is in the output formatting for the average session duration compared to the ground truth. The ground truth code formats the average to two decimal places and adds ""minutes"" for better readability and precision, and also includes a separator line, which enhances the overall presentation in a benchmark scenario. While the generated code's output is functionally correct, it lacks this minor refinement in presentation.",o4_mini,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""Timestamp"": [
        ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"",
        ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""
    ],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

df = pd.DataFrame(data)

max_temp = df[""TemperatureCelsius""].max()
min_humidity =...",9,"The generated code correctly defines the DataFrame and accurately finds and prints the maximum temperature and minimum humidity as required by the task. The code is clean, concise, and directly addresses all parts of the prompt.

Strong points:
- Correctly creates the pandas DataFrame with the specified data.
- Correctly uses `.max()` and `.min()` methods on the respective columns.
- Correctly prints the results.

Areas for minor improvement (not critical for correctness, but good practice):
- The `print` statements could be enhanced to match the exact output format of the ground truth code, which includes units (C and %) and more descriptive text (e.g., ""recorded across all sensors""). This is a minor aesthetic difference and doesn't impact functionality.",o4_mini,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}

df = pd.DataFrame(data)

# 1) Total number of Likes
total_likes = df[""Likes""].sum()
print(""Total Likes:"", total_likes)

# 2) PostType with highest average Shares
a...",10,"The generated code is correct, complete, and of good quality. It precisely defines the DataFrame, accurately calculates the total number of ""Likes"", and correctly identifies the ""PostType"" with the highest average number of ""Shares"". The variable names are clear, and the output format is readable. It perfectly addresses all requirements of the task.",o4_mini,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male""...",10,"The generated code is excellent. It correctly defines the DataFrame exactly as specified, calculates the average age using `.mean()`, and accurately counts patients by gender using `.value_counts()`. All requirements of the task are met, and the code is clean, well-structured, and includes the necessary imports. The output formats are also clear and appropriate.",o4_mini,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}

df = pd.DataFrame(data)

# 1) Print the title of the movie with the highest rating
print(df.loc[df['Ratin...",9,"The generated code correctly defines the DataFrame, identifies the title of the movie with the highest rating, and calculates the average release year. It adheres to all requirements of the task.

**Strong Points:**
- **Correctness:** Both tasks are solved accurately. `df['Rating'].idxmax()` is an efficient and correct way to find the index of the maximum rating, which is then used to retrieve the corresponding title. `df['Year'].mean()` correctly calculates the average year.
- **Conciseness:** The solution is very concise and uses built-in pandas functionalities effectively.
- **Readability:** Despite its conciseness, the code is easy to understand.

**Areas for Minor Improvement (why not a 10):**
- **Robustness for multiple highest ratings:** The ground truth code handles the edge case where multiple movies might have the same highest rating by identifying all of them and then selecting the first one. While `idxmax()` is good for finding the *first* occurrence of the max, if the task had implied listing *all* movies with the highest rating, the generated code would only pick one. For this specific prompt, ""the 'Title' of the movie"" implies one, so it's acceptable.
- **Output clarity:** The ground truth provides more descriptive print statements (e.g., ""The title of the movie with the highest rating is: 'The Dark Knight'""). The generated code simply prints the title and then the average year without labels, which is less user-friendly if the code were used in a standalone script without context. This is a minor point but affects the overall ""completeness"" in terms of user experience for the output.

Overall, it's an excellent and efficient solution.",o4_mini,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

print(""Total number of cars:"", len(df))
print(""Average PriceUSD:"", df[""PriceUSD...",9,"The generated code correctly defines the DataFrame, accurately calculates the total number of cars, and computes the average ""PriceUSD"". All requirements of the task are met. The code is concise and efficient.

Areas for minor improvement (to reach a perfect 10, align with the Ground Truth's verbosity for clarity and robustness):
1.  **Readability/Clarity of Output:** The ground truth uses f-strings for more descriptive and nicely formatted output (e.g., ""$[value]:.2f"" for currency and more descriptive labels like ""Total number of cars in the dataset:""). While the generated code's output is correct, it's slightly less user-friendly.
2.  **Explicit Variable Names (Optional but Good Practice):** The ground truth assigns the results to variables (`total_cars`, `average_price_usd`) before printing, which can sometimes improve readability and allow for easier reuse of computed values. However, for a simple print statement, direct calculation is perfectly acceptable.

Overall, it's a very good solution that fully solves the problem.",o4_mini,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}

df = pd.DataFrame(data)

# 1) Print the name of the Pokemon with the highest HP
print(df.l...",10,"The generated code correctly defines the DataFrame and accurately identifies and prints the names of the Pokemon with the highest HP and lowest Defense, fulfilling all requirements of the task. The code is concise and efficient, using `idxmax()` and `idxmin()` directly within the `loc` accessor, which is a standard and clean way to perform these operations in pandas. The structure is clear, and necessary imports are present.",o4_mini,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd

# Load dataset and identify missing values
df = pd.read_csv('titanic.csv')
print(df.isnull().sum())

# Fill missing Age values with median
df['Age'].fillna(df['Age'].median(), inplace=True)

# Fill missing Embarked values with mode
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# Drop Cabin column
df.drop('Cabin', axis=1, inplace=True)

# Extract Title from Name
df['Title'] = df['Name'].str.extract('([A-Za-z]+)\.', expand=False)

# Display cleaned d...",9,"The generated code is highly correct and addresses almost all requirements of the task.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly identifies and fills missing 'Age' values with the median.
*   Correctly identifies and fills missing 'Embarked' values with the mode.
*   Correctly drops the 'Cabin' column.
*   Correctly extracts the 'Title' from the 'Name' column using a concise regex with `str.extract`.
*   Correctly displays the cleaned dataset's shape and the first 5 rows.
*   Includes proper imports.

**Area for minor improvement (not critical but good practice):**
*   The ground truth included a `try-except FileNotFoundError` block for loading the CSV, which makes the code more robust. The generated code assumes the file exists, which is acceptable for a benchmark task but less robust for production.
*   The ground truth included standardizing less common titles (e.g., 'Mlle' to 'Miss', 'Capt' to 'Rare'). While the prompt didn't explicitly ask for this, it's a common and beneficial data cleaning step for the 'Title' feature and was present in the ground truth for a reason. The generated code *extracts* the title, but doesn't *standardize* it, which might lead to a highly granular and less useful 'Title' column for further analysis. This is a subtle point, but worth mentioning given the context of ""data cleaning"".",o4_mini,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}

df = pd.DataFrame(data)

# 1) Print the T...",9,"The generated code correctly defines the DataFrame, identifies the title of the book with the highest price, and calculates the total stock. Imports are correct, and the code structure is clean and concise. It addresses all requirements of the task. The only minor improvement could be to include more descriptive print statements for the output, similar to the ground truth, for better readability and user experience. This is a very good solution.",o4_mini,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"import pandas as pd

# 1) Load the dataset and display the first 10 rows
df = pd.read_csv('water_potability.csv')
print(df.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(""Shape:"", df.shape)

# 3) List all column names and their data types
print(""Data types:\n"", df.dtypes)

# 4) Calculate the percentage of potable vs non-potable water samples
percentages = df['Potability'].value_counts(normalize=True) * 100
print(""Potability percentages (%):\n"", percentages)

# 5) Display ...",7,"The generated code correctly performs most of the required tasks. It loads the dataset, displays the first 10 rows, shows the shape, lists data types, calculates potability percentages, counts missing values, and shows summary statistics.

However, there are several areas for improvement:
- **Lack of descriptive labels/headers:** The output for each section is very concise, sometimes just the raw output without any introductory text, making it harder for a user to understand what they are looking at. The ground truth provides clear print statements before each section (""--- 1. First 10 rows of the dataset ---"").
- **Error Handling:** The generated code lacks any error handling for `FileNotFoundError` or other potential issues when loading the CSV, which is a critical part of robust code, especially for an expert code evaluator.
- **`df.dtypes` vs `df.info()`:** While `df.dtypes` fulfills the request for listing column names and their data types, `df.info()` as used in the ground truth provides a more comprehensive overview, including non-null counts, which is generally preferred in EDA for a quick data quality check.
- **Missing Value Display:** The generated code prints `df.isnull().sum()` which includes columns with zero missing values. The ground truth `print(missing_values[missing_values > 0])` is more efficient and cleaner as it only shows columns that actually have missing data, and also explicitly states ""No missing values found in the dataset."" if applicable.
- **Potability Interpretation:** The ground truth adds a helpful interpretation for the 'Potability' column (1 = Potable, 0 = Non-Potable), which is good practice.

Overall, the code is functional but could be significantly improved in terms of user-friendliness, robustness, and detailed output, aligning it more closely with best practices for exploratory data analysis.",o4_mini,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"import pandas as pd

df = pd.read_csv('water_potability.csv')
missing_before = df.isnull().sum()
shape_before = df.shape

for col in df.select_dtypes(include='number').columns:
    df[col].fillna(df[col].mean(), inplace=True)

df.drop_duplicates(inplace=True)

ranges = {
    'ph': (0, 14),
    'Hardness': (0, 500),
    'Solids': (0, 20000),
    'Chloramines': (0, 10),
    'Sulfate': (0, 1000),
    'Conductivity': (0, 2000),
    'Organic_carbon': (0, 50),
    'Trihalomethanes': (0, 500),
    'Tur...",7,"The generated code correctly performs the basic data cleaning steps: loading, handling missing values, removing duplicates, and applying range checks. It also displays some before/after statistics and the head of the cleaned data.

However, it falls short in a few areas compared to the ground truth and the full requirements:

1.  **Missing Value Strategy Clarity**: While it uses `mean()` for numerical columns, it doesn't explicitly identify or print which columns had missing values before and after, nor does it confirm if all missing values were handled successfully (as the ground truth does by specifically printing `missing_values_before[missing_values_before > 0]` and `missing_values_after[missing_values_after > 0]`).
2.  **Range Validation**: The generated code defines specific upper bounds for `Hardness`, `Solids`, etc., which might be too restrictive or arbitrary without clear domain knowledge justification. The ground truth code correctly identifies that these parameters **cannot be negative** but doesn't impose arbitrary upper bounds, which is a safer approach for generic data cleaning unless specific thresholds are provided in the problem description. The ground truth's range validation for pH (0-14) is standard, but for other columns, it only enforces non-negativity, which is a more robust general approach if specific upper bounds are not given. The generated code also doesn't print how many rows were removed due to range violations, which is useful for understanding the impact of this step.
3.  **Before/After Summary**: Its summary is concise but less comprehensive. The ground truth provides detailed print statements for each step (e.g., ""Removed X duplicate rows"", ""Filled missing values in Y with its mean"", ""Removed Z rows due to out-of-reasonable-range values""), making the cleaning process transparent. The generated code only provides shape and missing value counts at the very end.
4.  **Error Handling**: The ground truth includes basic `FileNotFoundError` handling for the CSV file, which is good practice for robustness. The generated code lacks this.
5.  **Output Format**: The prompt asked for ""Summary showing before and after statistics (missing values count, dataset shape)"" and ""Display the first 5 rows"", and the ground truth provides clear labels. The generated code's summary `print(""Before shape:"", shape_before, ""Missing:"", missing_before.to_dict())` is less readable and lacks context compared to the ground truth's structured output.

To improve, the generated code should:
*   Make the ""before"" and ""after"" summaries more verbose and descriptive, indicating the impact of each cleaning step, similar to the ground truth.
*   Explicitly confirm that no missing values remain after imputation.
*   Reconsider the arbitrary upper bounds in the range validation; only enforcing non-negativity might be more appropriate if specific ranges are not explicitly stated as requirements.
*   Add error handling for file loading.",o4_mini,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('water_potability.csv')

# 1) Bar chart: potable vs non-potable
plt.figure(figsize=(6,4))
sns.countplot(x='Potability', data=df, palette='Set2')
plt.title('Distribution of Potable vs Non-Potable Water')
plt.xlabel('Potability')
plt.ylabel('Count')
plt.xticks([0,1], ['Non-Potable', 'Potable'])
plt.show()

# 2) Histograms for pH, Hardness, Chloramines
for feature in ['ph', 'Hardness', ...",7,"The generated code correctly addresses most of the requirements and produces the specified visualizations.

**Strong points:**
*   All four required visualizations are generated.
*   Proper imports are included.
*   Titles and axis labels are generally present for each plot.
*   Missing values are handled correctly for histograms by using `.dropna()`, and implicitly by seaborn for boxplots and heatmaps.

**Areas for improvement:**
1.  **Readability/Clarity of Histograms:** The histograms are generated in a loop, showing one plot at a time. While functional, it's generally better practice for presentation and comparison to plot multiple related histograms on a single figure using subplots (as in the ground truth). This reduces the number of pop-up windows and aids visual comparison.
2.  **Consistency in Potability Labels:** The bar chart and boxplot correctly use `Non-Potable` and `Potable` for x-axis labels. The ground truth also explicitly adds parenthetical explanations like `(0: Not Potable, 1: Potable)`, which can be helpful for clarity, though it's not a strict requirement to keep it.
3.  **Enhanced Plot Customization:** While basic formatting is there, the ground truth includes more advanced formatting like `fontsize`, `grid`, `tight_layout`, and `xticks` rotation for the heatmap, which contributes to more professional and readable plots. For example, `tight_layout()` is crucial to prevent titles/labels from overlapping in more complex plots or subplots.
4.  **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code assumes the file is always present.
5.  **KDE in Histograms:** The ground truth uses `kde=True` for histograms, which overlays a kernel density estimate and can be very useful for understanding the distribution's shape. The generated code sets `kde=False`. While not strictly wrong, `kde=True` usually enhances the visual analysis for distribution plots.

Overall, it's a good attempt, but a few refinements in presentation, code structure (for subplots), and robust error handling would elevate it to a higher score.",o4_mini,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression

df = pd.read_csv('winequality-red.csv')
eps = 1e-6
df['acid_ratio'] = df['fixed acidity'] / (df['volatile acidity'] + eps)
df['alcohol_sulfates'] = df['alcohol'] * df['sulphates']
df['pH2'] = df['pH'] ** 2
df['density_alcohol'] = df['density'] * df['alcohol']

X = df.drop('quality', axis=1)
y = df['quality']

rf = RandomForestRegressor(ra...",6,"The generated code correctly addresses several aspects of the task, including loading data, performing feature engineering, applying two different feature selection techniques (RFE and SelectKBest), and comparing the selected feature sets.

However, there are significant areas for improvement, preventing a higher score:

1.  **Feature Engineering Insufficiency and Quality:**
    *   The task explicitly asked for ""at least three *meaningful* new features (e.g., ratios, interactions, polynomial features)."" While ""acid_ratio"" and ""alcohol_sulfates"" are good, `pH^2` is a polynomial feature but its *meaningfulness* for wine quality is less immediately apparent without context. `density * alcohol` (a simple interaction) is okay but less creative than what the ground truth provided (density-alcohol *difference*). The ground truth provided clearly more meaningful and varied engineered features (e.g., free to total sulfur dioxide ratio) and more than the minimum of three, showing a deeper understanding of the domain or a more thorough approach.
    *   The generated code does not attempt to handle potential `NaN` or `inf` values that could arise from feature engineering (e.g., division by zero if `volatile acidity` is zero), which the ground truth gracefully handles.

2.  **Missing Feature Importance Extraction for RFE:** The request was to ""display the final selected features with their importance scores or rankings.""
    *   For RFE, `rfe_importances = rfe.estimator_.feature_importances_` attempts to get importances from the RFE's underlying estimator. However, `rfe.estimator_` is the *fitted estimator on the entire original dataset*, not just the *selected features*. To get meaningful importances *for the selected features* after RFE, the estimator needs to be re-fitted on the *subset* of features chosen by RFE, as demonstrated in the ground truth. The current `rfe_importances` might be misleading as it would technically be the importances of the original features *before* RFE if `rfe.estimator_` refers to the original estimator fit, or the importances of all features if the estimator was re-fit internally during the RFE process. The ground truth correctly shows how to extract the importances *of the final selected features* by retraining.
    *   The way it reports `rfe_imp_dict` ties importances to `rfe_selected`, which are *after* RFE, but the importances themselves are from the estimator that was likely trained on *all* features, or the RFE itself just provides a ranking. The ground truth approach is more robust for getting importances *specifically from the model on the selected features*.

3.  **Lack of Scaling:** While `RandomForestRegressor` and `f_regression` are less sensitive to feature scaling than, say, SVC or PCA, it's generally best practice to scale features when performing feature selection, especially if those features might later be used in models that *are* sensitive to scale (e.g., linear models, distance-based algorithms). The ground truth correctly incorporates `StandardScaler`. This wasn't explicitly asked for but demonstrates better overall ML pipeline practice.

4.  **Display Formatting and Information:** The print statements are functional but lack the detailed breakdown and explanatory text provided by the ground truth, particularly concerning the comparison and contrasting of the feature selection methods. The ground truth explains *why* the methods might differ, which adds significant value to the comparison aspect.

5.  **Robustness (File Handling):** The ground truth includes basic error handling for `FileNotFoundError`, which is good practice for robust scripts. The generated code assumes the file exists.

Overall, the generated code gets the core mechanics right but falls short on the depth of analysis, robustness, and best practices expected from an ""expert"" evaluation, particularly regarding the meaningfulness of engineered features, the correctness of feature importance extraction for RFE, and the detailed comparison.",o4_mini,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold, cross_val_predict, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings(""ignore"")...",7,"The generated code addresses most of the requirements, implementing multiple ensemble methods, cross-validation, hyperparameter tuning, and evaluation metrics. However, there are significant issues in the data preprocessing and feature importance sections that limit its score.

**Strong Points:**
*   Correctly implements Random Forest, Gradient Boosting, XGBoost, AdaBoost.
*   Includes Voting and Stacking ensembles.
*   Performs hyperparameter tuning for XGBoost using GridSearchCV.
*   Calculates MAE, MSE, RMSE, and R scores.
*   Generates residual plots.
*   Uses `cross_val_predict` for consistent evaluation across models.

**Areas for Improvement / Issues:**
*   **Preprocessing (Major Issue):**
    *   The `df.dropna(subset=['Publisher', 'Genre', 'Platform'], inplace=True)` line is problematic. It drops rows aggressively without proper handling of missing values, which is often not ideal for real-world datasets. The ground truth uses `SimpleImputer` within a `Pipeline` for robustness.
    *   `df = pd.get_dummies(df.drop(['Name'], axis=1), drop_first=True)`
        *   This does `get_dummies` on the entire DataFrame, including `Year_of_Release`, `Global_Sales`, etc., which are numerical. This is incorrect. One-hot encoding should only be applied to categorical features.
        *   It doesn't handle `Year_of_Release` as a categorical feature, but fills NaN with median and then dummies it. The ground truth correctly identifies numerical and categorical features and applies appropriate transformers.
        *   It completely misses the `User_Score` column mentioned in the task requirements, which often needs special handling (e.g., converting 'tbd' to NaN).
    *   The preprocessing is not integrated into a `Pipeline` for individual models. This means each model (`models['RF'].fit(X, y)`) relearns the preprocessing steps (like `get_dummies`) on the whole dataset, which is not how you integrate preprocessing with cross-validation in a robust way, especially when one-hot encoding might lead to different columns between train/test folds if not handled carefully (though `get_dummies` handles this implicitly by padding missing columns with zeros, it's less explicit than `OneHotEncoder(handle_unknown='ignore')`). This makes `cross_val_predict` potentially less reliable in how it handles new categories during CV if not explicitly piped.
    *   The `X` and `y` are created at the very beginning after a single preprocessing step on the full dataset. This means that features derived from the training set (e.g., categories discovered during one-hot encoding) are implicitly available to the test set during CV, which is data leakage. Proper CV should apply preprocessing separately to each fold's training data. The ground truth's `ColumnTransformer` within a `Pipeline` correctly solves this.

*   **Feature Importance:**
    *   The feature importance extraction is problematic because `pd.get_dummies` does not maintain a consistent order of columns or feature names if applied independently or in a non-pipelined way across folds. The `features` variable `df.drop('Global_Sales', axis=1).columns` is based on the full preprocessed `df`, not dynamically updated for each fold or through a pipeline.
    *   `model.fit(X, y)` is called on the entire dataset `X` and `y` *before* extracting importances. This is inconsistent with the cross-validation setup where models are trained on folds. For consistent feature importance, it should be extracted from a model fit on the full dataset, but also the feature names should correctly map to the processed features. The ground truth uses `get_feature_names_out` from the fitted `OneHotEncoder` within the preprocessor pipeline to get accurate feature names.
    *   The code plots feature importances using `features[idx]` where `features` is a Series of column names generated from `df.drop('Global_Sales', axis=1)`. This `features` will contain all dummy variables names. However, if the `get_dummies` created more features than `X` actually has, or if `X` was converted to `X.values` after `get_dummies`, then indexing `features[idx]` by integer indices `idx` might be off. It should be `features.iloc[idx]` if `features` refers to the `columns` attribute of a DataFrame or the `idx` are integer positions. If `X` has been converted to `NumPy` array `X.values`, then the relationship between `features` and `X` is lost unless `features` are explicitly ordered to match `X` dynamically. The ground truth addresses this by taking feature names *after* the `ColumnTransformer`.

*   **Completeness/Minor Issues:**
    *   While it implements tuning, it only tunes XGBoost, not the ""best individual model"" as requested.
    *   No titles or clear separation/explanation of steps in the print statements, unlike the ground truth.
    *   Residual plot for stacking only, not a broader analysis or choice of best model.
    *   The handling of the ""Rank"" column is completely missed.
    *   Imports are done throughout the code rather than at the top, hindering readability.

The most critical aspect is the incorrect and non-robust data preprocessing, especially the `get_dummies` application and the lack of a proper preprocessing pipeline within the cross-validation framework itself. This fundamentally impacts the correctness of the results and interpretation of feature importances.",o4_mini,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
import statsmodels.formula.api as smf
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_squared_error
import pymc3 as pm
import ruptures as rpt
from lifelines import KaplanMeierFitter, CoxPHFitter
...",6,"The generated code attempts to address all aspects of the task, which is commendable given the complexity. However, it falls short in several areas regarding correctness, completeness, and robustness, particularly concerning proper implementation of models, handling of categorical variables, interval calculations, and thorough time series cross-validation.

Here's a breakdown:

**Strengths:**
*   **Covers most requirements:** It includes ARIMA, seasonal decomposition, multivariate regression, regularization, Bayesian regression (PyMC3), change point detection, and survival analysis.
*   **Basic Data Loading & Aggregation:** Correctly loads and aggregates yearly sales.
*   **Initial ARIMA & Decomposition:** Attempts both, although the `period=1` for seasonal decomposition on yearly data is meaningless for typical seasonality.
*   **PyMC3 Integration:** Successfully sets up a basic PyMC3 model.
*   **Ruptures & Lifelines usage:** Correctly uses libraries for change point detection and survival analysis.

**Weaknesses & Areas for Improvement:**

1.  **Imports:** Missing `matplotlib.pyplot` and `seaborn` for plotting, which is essential for visualizing forecasts, decomposition, and survival curves. It also misses `StandardScaler` from `sklearn.preprocessing` which is crucial for polynomial regression with regularization and Bayesian regression.
2.  **ARIMA Modeling:**
    *   `seasonal_order=(1,1,1,12)` for yearly data is incorrect. Yearly data typically does not have a 12-month seasonality. `pmdarima.auto_arima` (used in ground truth) is better for automatically determining parameters.
    *   It correctly calculates `arima_pred` and `arima_ci`, but it does not visualize them.
3.  **Seasonal Decomposition:**
    *   `period=1` for `seasonal_decompose` on yearly data with `ts` (yearly sales) will not yield meaningful seasonal components. `period` should be a meaningful cycle, e.g., 5, 7, 10 years if such a cycle exists, or it should be omitted if no such cycle is suspected. The ground truth correctly addresses this.
    *   No plotting of the decomposition.
4.  **Multivariate Regression:**
    *   The formula `Global_Sales ~ Year * Genre + Year * Platform + Year * Publisher` with raw categorical data for `Genre`, `Platform`, `Publisher` from `df_reg` will likely lead to `patsy.dmatrices` failures or errors due to too many unique categories, making the model computationally infeasible or unstable. The ground truth explicitly handles this by:
        *   Filtering to top N categories for platforms/publishers.
        *   Using `pd.get_dummies` to one-hot encode.
        *   Carefully constructing the formula with `Q()` for interaction terms.
    *   It computes `reg_pred_frame` which contains confidence/prediction intervals, but doesn't explicitly display or use them significantly beyond calculation.
5.  **Polynomial Regression with Regularization:**
    *   **Crucial Missing Step:** It does *not* scale `Xp` before fitting Ridge and Lasso (`StandardScaler` is missing). Regularization models are highly sensitive to feature scaling. This is a major omission.
    *   `train_test_split` with `shuffle=False` is a step, but `TimeSeriesSplit` (as used in the ground truth) is more appropriate for time series validation.
    *   No explicit calculation or display of confidence/prediction intervals for these models (ground truth provides an approximation).
6.  **Bayesian Regression (PyMC3):**
    *   The model structure `beta_genre[genre_codes]` is correct for categorical effects.
    *   However, it uses `df_bayes['Year'].values` directly without scaling. Scaling of predictors (e.g., `Year`) is very important for MCMC sampling efficiency and convergence in PyMC3. The ground truth correctly scales year.
    *   It calculates `ppc_ci` but doesn't illustrate how these intervals would be used for prediction on new data, or how they compare to actual values.
    *   No plotting of results.
7.  **Change Point Detection:**
    *   `pen=10` is a fixed penalty. The ground truth uses a more robust penalty derived from the number of samples (`2 * np.log(n_samples_signal)`), which is adaptive.
    *   No plotting to visualize the detected change points on the time series.
8.  **Survival Analysis:**
    *   The definition of `duration`: `current_year - df_surv['Year']` is incorrect. `df_surv['Year']` typically refers to `Year_of_Release` from the original dataframe. If a game was released in 1990 and our data ends in 2016, its duration/longevity isn't `2016 - 1990` if it stopped being profitable in 1995. The ground truth's definition `Year_of_Release_last_observed - Year_of_Release_release + 1` is more accurate for ""game longevity"" based on the dataset's sales history.
    *   `cph.fit(df_surv[['duration','event','Year']], 'duration', 'event')`: Including `Year` (which is `Year_of_Release`) as a covariate is reasonable. However, the ground truth goes further:
        *   Aggregates attributes (`Genre`, `Platform`, `Global_Sales`) per game.
        *   Filters to top categories for `Genre` and `Platform`.
        *   One-hot encodes them for the Cox model.
        *   Plots the survival function.
9.  **Confidence & Prediction Intervals (Overall):** While point 9 states they're ""already computed,"" the implementation is sparse:
    *   ARIMA has them.
    *   Multivariate regression (OLS) has them (via `.summary_frame()`).
    *   Polynomial regression doesn't explicitly compute them (only `predictions`). Sklearn models don't directly provide PIs, so a strategy like residual-based approximation or bootstrapping (as hinted in GT) is needed.
    *   Bayesian regression has them from PPC.
    *   Survival analysis (Kaplan-Meier has CIs on estimate, Cox has CIs on coefficients).
    *   The overall integration and clear presentation of these intervals for *all* models is not fully realized. Explicitly showing and interpreting PIs/CIs for each model (e.g., plotting them or printing descriptive statistics) is expected.
10. **Time Series Cross-Validation:**
    *   Only implemented for ARIMA and only using `ts_vals` (aggregated yearly sales). The ground truth correctly applies TS-CV to other regression models (polynomial, multivariate) based on the full dataset using `TimeSeriesSplit`.
    *   The ARIMA TS-CV is very basic (`order=(1,1,1)` fixed). The GT re-fits the `pm.ARIMA` with the best order found earlier, which is more robust. It also demonstrates how to do rolling ARIMA `predict` across folds.

**Conclusion:**
The code provides a basic framework for most requested analyses but frequently lacks the detail, robustness, and statistical rigor needed for an ""expert code evaluator."" Many implementations are simplified, missing crucial steps like scaling, proper handling of categorical variables in complex models, and comprehensive confidence/prediction interval display and validation. The lack of plotting also significantly diminishes its utility for data science tasks.",o4_mini,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score
from sklearn.manifold import TSNE
import umap.umap_ as umap
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from m...",7,"The generated code provides a good attempt at the complex task, covering most of the requirements.

**Strong Points:**
*   **Feature Engineering:** Successfully creates 'Regional_Sales_Distribution' (though named differently), 'Market_Position', and 'Publisher_Portfolio_Size'.
*   **Multiple Clustering Algorithms:** Implements K-Means, DBSCAN, Hierarchical Clustering, and GMM.
*   **Dimensionality Reduction:** Uses t-SNE and UMAP and attempts visualization.
*   **Optimal Cluster Determination:** Implements silhouette analysis and a custom gap statistic function.
*   **Consensus Clustering:** Attempts to implement consensus clustering, which is a complex but important requirement.
*   **Cluster Profiling:** Generates numerical and categorical profiles and performs statistical significance tests (ANOVA, Chi-squared).
*   **Anomaly Detection:** Uses Isolation Forest.
*   **Association Rule Mining:** Implements Apriori and association rule generation.
*   **Recommendation System:** Provides a basic cluster-based recommendation system.
*   **Validation:** Includes internal metrics (silhouette, Davies-Bouldin, Calinski-Harabasz) and attempts external metrics (ARI, NMI).

**Areas for Improvement:**

1.  **Preprocessing and Feature Engineering Robustness:**
    *   The generated code uses `fillna(0)` after creating ratio features for regional sales, which can lead to `NaN` if `Total_Regional` is 0. The ground truth handles this with `apply` and a conditional check.
    *   Missing value handling for 'Year\_of\_Release', 'Genre', 'Platform', 'Publisher' is not explicitly shown, which could lead to errors or unexpected behavior if the dataset had NaNs in these columns.
    *   `LabelEncoder` was used for categorical features, which introduces an ordinal relationship that is generally not appropriate for clustering. One-Hot Encoding (as in the ground truth) is preferred. This is a significant issue for the quality of clustering.

2.  **Optimal Cluster Determination:**
    *   While the `gap_statistic` function is a great addition, the logic for `k_elbow` is flawed (`np.argmax(-np.diff(inertias))`) and doesn't correctly identify the elbow point. The `kneed` library used in the ground truth is more robust for this.

3.  **DBSCAN Parameter Tuning:**
    *   DBSCAN uses fixed `eps=0.5, min_samples=5`. The ground truth attempts to use `NearestNeighbors` and the k-distance graph, which is a more appropriate heuristic for parameter selection. Without proper tuning, DBSCAN often yields poor results (e.g., `-1` for most points).

4.  **Consensus Clustering Implementation:**
    *   The consensus clustering logic `coassoc = np.zeros((n,n))` initializes the matrix based on `n` (number of samples) but then uses the labels directly. This implies a hard consensus. It's also applying `affinity='precomputed'` on `1-coassoc` (distance matrix), which is correct. However, for a more robust consensus, the method used by the ground truth (counting co-occurrences of points falling into the same cluster across *different* algorithms) is more common and effective. The given code just checks if `labels[i] == labels[j]` for `labels_list`, but `labels_list` contains: `[kmeans_labels, hc_labels, gmm_labels]`. This implicitly means point `i` and point `j` must be in the same cluster for k-means, *and* in the same cluster for hierarchical, *and* in the same for GMM, which isn't the standard way to build a co-occurrence matrix for consensus. The ground truth sums up instances where they are in the same cluster *across algorithms*.

5.  **Visualization:**
    *   The visualization code for t-SNE and UMAP was commented out or missing (e.g., `plt.show()` and `plt.scatter` at the end), which violates the requirement to visualize results. The ground truth has robust visualization loops for all cluster types.

6.  **External Validation Metrics:**
    *   The attempt to use `adjusted_rand_score` and `normalized_mutual_info_score` with `le_genre.transform(df['Genre'])[mask]` as ground truth labels is incorrect. These metrics are used when you have *known* true labels for the clusters (e.g., if you knew games belonged to inherent ""segments"" already). Here, `Genre` is just one of the features, not a ground truth for market segments. True external validation would require a separate, labeled dataset.

7.  **Code Structure and Explanation:**
    *   Lacks the `print` statements that indicate progress and results, which are useful for understanding the flow of a complex analysis like this.
    *   No warning suppression.

In summary, the code provides a commendable attempt at a very challenging task, demonstrating knowledge of various required techniques. However, several critical details in preprocessing (LabelEncoding), optimal k selection, and the consensus clustering implementation deviates from best practices or are less robust compared to the ground truth, leading to potential inaccuracies in the results. The incomplete visualization and incorrect application of external validation metrics also reduce the score.",o4_mini,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoosti...",7,"The generated code provides a good attempt at implementing several advanced ensemble learning and model optimization techniques. It correctly identifies and uses many required libraries and concepts.

**Strong Points:**
- **Pipelining:** Uses `sklearn.pipeline.Pipeline` and `ColumnTransformer` effectively for preprocessing and model building, which is good practice.
- **Feature Engineering:** Basic feature engineering is present (`BMI_Age`, `Glucose_Age`, `BMI_cat`).
- **Ensemble Methods:** Implements various ensemble methods like Voting (hard/soft), Bagging, AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost.
- **Optuna Integration:** Successfully integrates Optuna for hyperparameter tuning.
- **Nested Cross-Validation:** Attempts to implement nested cross-validation, which is crucial for unbiased evaluation.
- **Model Interpretation (SHAP):** Includes SHAP for model interpretability.
- **Model Calibration:** Includes `CalibratedClassifierCV` and `brier_score_loss`.

**Areas for Improvement / Missing Aspects:**
1.  **Comprehensive Feature Engineering:** While some features are created, the ""comprehensive"" aspect is somewhat lacking compared to the ground truth (e.g., handling zero values as NaNs, `PolynomialFeatures` on all numeric at appropriate stage, custom ratios). The current preprocessing pipeline creates a `BMI_cat` which is then one-hot encoded, but the `PolynomialFeatures` are applied *before* the scaler in the ground truth, and covers interactions comprehensively. The generated code applies it after scaling within the `num_pipeline`, which might be suboptimal, and only for numeric features, not interactions with categorical ones (though `BMI_cat` is itself derived from a numeric feature). The ground truth handles imputed values explicitly, which is missing in the generated code, leading to potential issues with 0s in the raw data.
2.  **Nested Cross-Validation Implementation:** The nested CV implementation for Optuna is a bit convoluted. The `objective` function for Optuna already uses `cross_val_score` internally, which means the inner loop is essentially a CV within a CV. The outer loop then re-runs Optuna directly on each outer fold's training data, which is correct for nested CV. However, the `objective` function for Optuna should operate on `X_tr` and `y_tr` from the outer fold's split, *not* the global `X` and `y`. This is a critical logical flaw for proper nested CV. The ground truth separates the Optuna tuning to a single `train_test_split` for efficiency and then uses those best params in the nested CV, which is a common and valid approach. The generated code's approach for nested Optuna is potentially very slow and might not correctly reflect the unbiased performance if the `objective` was applied to `X` and `y` directly instead of `X_tr, y_tr`.
3.  **Handling of Categorical Features (CatBoost & Pipeline):** CatBoost is designed to handle categorical features directly. The generated code passes all data through a `preprocessor` that includes `OneHotEncoder` for `BMI_cat`. This converts the categorical feature to numerical, preventing CatBoost from leveraging its internal handling. While it works, it bypasses a key advantage of CatBoost. The ground truth correctly assumes all features become numerical after custom preprocessing/imputation for this dataset.
4.  **Custom Ensemble with Dynamic Weight Assignment:** The generated code attempts a dynamic ensemble, but it applies it only to `X_test` with weights derived from *all* models' individual `cross_val_score` `roc_auc` on the *full* dataset. This is problematic:
    *   Weights should ideally be derived from a validation set within a fold, not full dataset cross-validation (which introduces data leakage if the weights are then used on the test set of the same fold).
    *   The ""dynamic"" aspect is somewhat simplified. The ground truth's approach of obtaining AUCs on an inner validation split and using those to weight predictions on the test set is more robust.
    *   The `y_dyn` calculation `(probs.sum(axis=1) > 0.5).astype(int)` directly applies a threshold without checking if `probs.sum(axis=1)` is scaled correctly (it often isn't after weighted averaging of probabilities without re-normalization). This should typically be `roc_auc_score(y_test, probs.sum(axis=1))` to evaluate the combined probability, not the thresholded class.
5.  **Model Calibration and Reliability Analysis:** The generated code implements calibration but only for `best_xgb`. It prints the Brier score, but lacks plots (like reliability diagrams) that are essential for ""reliability analysis"". The ground truth provides detailed plots and compares calibrated vs. uncalibrated Brier scores. Also, `CalibratedClassifierCV(best_xgb, cv=3)` fits on `X_train, y_train` but then evaluates on `X_test` which is appropriate. However, for a truly integrated evaluation, the calibration step should generally be part of the pipeline or the nested CV structure to avoid optimistic evaluation of calibration performance.
6.  **Ensemble Diversity Analysis and Model Combination Strategies:**
    *   Diversity is calculated as `np.mean(preds[i] != preds[j])`, which is a simple disagreement metric. While okay, ""comprehensive"" diversity analysis typically involves correlation matrices of predicted probabilities (as in the ground truth) which gives a richer view, not just correlated class predictions.
    *   The meta-learner, beyond stacking, is missing. The ground truth shows how to train a meta-learner on out-of-fold predictions.
7.  **Plotting:** The current code doesn't generate plots for SHAP or calibration. `shap.summary_plot` and `shap.force_plot` have `show=False` by default, but `plt.show()` needs to be called to display them. Similarly for calibration curves.
8.  **Imports:** All necessary imports are correctly made, but `warnings` is not imported or used for suppression as in the ground truth, which often leads to verbose output regarding `use_label_encoder`.
9.  **Randomness:** While `random_state=0` is used for CV, it's not consistently used across all model initializations, which can affect reproducibility.
10. **Readability/Structure:** The ground truth organizes code into sections with comments (`1. Load and preprocess...`, `2. Implement advanced ensemble...`), making it very clear which part addresses which requirement. The generated code is more monolithic.

In summary, the generated code has many correct components but requires significant refinement in the logical flow, rigorous application of nested CV, more detailed implementation of diversity analysis, and enhanced visualization/plot displays to fully meet the ""comprehensive"" and ""expert"" level required by the prompt.",o4_mini,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"import numpy as np
import pandas as pd
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors
import pymc3 as pm
import arviz as az
from lifelines import CoxPHFitter
import statsmodels.api as sm
from statsmodels.formula.api import mixedlm
from statsmodels.sandbox.regression.gmm import IV2SLS

# 1) Load and preprocess
df = pd.read_csv('diabetes.csv')
imputer = I...",4,"The generated code attempts to address all parts of the prompt but falls short significantly in correctness and completeness for several complex modeling tasks.

**Critical Issues (Primary reasons for low score):**

1.  **Incorrect Data Preprocessing for Specific Tasks:**
    *   **Missing Value Handling:** The prompt specifically mentioned ""Identify columns where 0 might represent missing values (common in this dataset) like Glucose, BloodPressure, etc."" and replace them with `np.nan` *before* imputation. The generated code uses `IterativeImputer` directly after loading, which silently imputes these 0s as valid data, completely missing this crucial domain-specific pre-processing step. This fundamentally changes the data used for all subsequent models, leading to incorrect results.
    *   **Simulated Time-to-Event:** The ground truth carefully constructs a simulated `time` and `event` based on a risk score, ensuring the `event` is linked to the `Outcome` (diabetes status). The generated code simply uses `np.random.exponential` for `time` and copies `diabetes` to `event`, which is a meaningless simulation for typical survival analysis as `time` is completely random and `event` is just the outcome, not an event *at* a specific time. This makes the Cox PH model statistically invalid for the task.
    *   **Feature Scaling Alignment:** The ground truth scales features *before* using them in PyMC3 models, which is crucial for MCMC convergence. The generated code scales `df_imputed` into `df_scaled` but then immediately overwrites `df_scaled` with the addition of `time`, `event`, and `age_group` columns, making the `X` array derived from it incorrect (it still includes scaled columns but doesn't explicitly state that the features used for modeling, like `glucose`, `bmi`, `age` in mixed effects or other models, are the scaled ones).

2.  **PyMC3 Model Definitions and Usage:**
    *   **Hierarchical Model `X` and `y`:** The `X` in the hierarchical model setup is `df_scaled.drop(['diabetes','time','event','age_group'], axis=1).values`. This is likely to create issues as `df_scaled` contains the *scaled* version of the original columns, but then it drops `diabetes` (the target), `time`, `event`, `age_group` and attempts to use the *remaining* numerical values as `X`. This `X` doesn't align with the features explicitly mentioned in the ground truth for regression (e.g., `Pregnancies`, `Glucose`, `BMI`, etc.). The ground truth carefully selects features as `df_scaled['Pregnancies']` etc., ensuring correct column alignment and usage of scaled values.
    *   **Missing Informative Priors:** The prompt specifically required ""Informative priors based on medical knowledge"" for Bayesian logistic regression. The generated code uses `mu=0, sd=1` (standard normal) for all priors, which are weakly informative at best, failing this requirement.
    *   **Gaussian Process Likelihood:** The GP model uses `pm.Bernoulli(""y_obs3"", logit_p=f, observed=y)`. Gaussian Processes are typically for continuous outcomes (`pm.Normal` likelihood, like in ground truth), or for binary classification, they often wrap a Bernoulli likelihood with an inverse logit link (or use a dedicated GP classification library). While `logit_p=f` can technically work, it's less standard for continuous GPs and the prompt implied non-parametric modeling of *relationships between variables* (often continuous-continuous), not a binary classification. The ground truth modeled `Glucose` (continuous) vs `Age`.
    *   **WAIC/LOO Usage:** The generated code directly calls `pm.waic` and `pm.loo` on the trace and model, which is deprecated in newer ArviZ versions. The `az.waic` and `az.loo` with `return_inferencedata=True` and passing the `model` argument as done in the ground truth is the correct way.

3.  **Causal Inference:**
    *   **Propensity Score Matching:** The matching logic is rudimentary. `nn.fit(ps.reshape(-1,1))` and `_, idx = nn.kneighbors(ps.reshape(-1,1))` will match *every point to itself* or the closest point in the *entire dataset*, not explicitly matching treated to control groups as required for PSM. The `matched` dataframe will just be a reordering of the original `df_scaled` or a sub-sample, not a properly matched treated and control group balanced on confounders. The ground truth correctly separates treated and control and then matches controls to treated.
    *   **Instrumental Variable:** The choice `instrument=df_scaled[['age']]` is highly questionable. Age is a strong confounder for diabetes, not a valid instrument that only affects diabetes through BMI and not directly. The prompt specifically states that a valid IV ""affects the treatment but only affects the outcome *through* the treatment"". `IV2SLS` is used without proper variable selection or rationale (`exog` includes `bmi`, the endogenous variable, and `instrument` is `age`, a direct confounder and part of `exog` conceptually). The setup is statistically incorrect for estimating a causal effect via IV.

4.  **Mixed-Effects Model:** `statsmodels.formula.api.mixedlm` is used, but the task asked for a *Bayesian* mixed-effects model. The generated code's implementation is frequentist, missing the Bayesian aspect.

5.  **Lack of Interpretive Outputs/Visualizations:** While the prompt asks for code, the ground truth includes commented-out code for plotting PPCs and credible intervals, showing an understanding of model validation. The generated code only calls `az.summary` and `az.plot_ppc` without any context or specific plots, and the `ppc` calls are incorrect for the hierarchical and GP models in PyMC3 (they need the model object passed correctly).

**Minor Issues / Areas for Improvement:**

*   **Imports:** Some imports are missing (e.g., `matplotlib.pyplot`) or unnecessary (e.g., `statsmodels.sandbox.regression.gmm.IV2SLS` as it's not a standard, robust solution).
*   **Variable Naming Consistency:** `df_scaled['diabetes']` is used for the target, while the original dataset likely uses `Outcome`. Consistency would be better for clarity.
*   **Documentation:** Lack of comments explaining choices, especially for the complex simulation and causal inference parts.

In summary, while the generated code attempts to use the correct libraries for each section, the implementation details, especially around data handling, model specification (priors, likelihoods), and the statistical validity of the simulated data and causal inference setups, are largely incorrect or highly problematic, preventing it from producing meaningful or correct results for the given task.",o4_mini,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering, OPTICS
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import pdist, squareform
from sklearn.ensemble import IsolationForest
from sklearn.svm import ...",6,"The generated code attempts to address most of the requirements but falls short in several areas regarding completeness, correctness, and code quality, especially when compared to the comprehensive ground truth.

**Positives:**
*   **Structure:** The generated code follows the numbered structure somewhat, making it easy to see which requirement it's addressing.
*   **Basic Implementations:** It includes placeholders for most of the required models and techniques (clustering, dimensionality reduction, anomaly detection, ARM).
*   **Metabolic Syndrome Indicator:** Feature engineering for metabolic syndrome is present.
*   **Gap Statistic:** Attempts to implement the gap statistic for optimal K, which is a good effort, though the implementation itself is simplified.

**Areas for Improvement (Why it received a 6):**

1.  **Correctness/Robustness of Implementations:**
    *   **Optimal K for Clustering:** The `gap_statistic` implementation is very basic and might not be robust. It also only uses `opt_k_gap` for *all* algorithms that require a fixed K, which is not ideal (e.g., GMM, Hierarchical, Spectral often have different optimal Ks). The ground truth provides specific methods for each.
    *   **DBSCAN/OPTICS Parameters:** `eps` and `min_samples` for DBSCAN and OPTICS are hardcoded without proper parameter tuning/estimation methods (like the k-distance graph in GT).
    *   **Association Rule Mining:** `bin_df = df[features].apply(lambda x: x > x.median()).astype(int)` is a simplification that loses nuance. The ground truth's `pd.qcut` approach for multiple bins is superior for continuous data.
    *   **Time-Series Clustering:** The `if 'Time' in df.columns...` check is good, but the `tslearn` library is imported but not used, and the pivot operation for `ts` would require the `features` list to be applied to `values`, which might not produce the intended sparse time series in the `pivot` call. Also, the `fit` method of `TimeSeriesKMeans` for `Xts` directly instead of `Xts.reshape()` might lead to issues depending on how `Xts` is shaped. This section is conceptually present but the implementation is questionable given the data structure.
    *   **Consensus Clustering:** The `dist_mat = 1 - (co_mat / n_models)` is a reasonable approach to convert co-occurrence to distance, but the ground truth uses a more explicit method to ensure it's a proper distance matrix for `linkage`. The use of `t=0.5, criterion='distance'` for `fcluster` is a fixed threshold and doesn't involve finding an ""optimal"" number of clusters based on metrics like silhouette, as the ground truth does.
    *   **Phenotype Profiling:** While it groups by cluster and gets means/modes, it lacks the statistical significance testing (ANOVA/Kruskal-Wallis, Chi-squared) and visualization aspects covered extensively in the ground truth. The print statement for statistical tests only prints p-values without stating the test used or its implication.

2.  **Completeness & Robustness:**
    *   **Feature Engineering:** The GT includes more comprehensive metabolic syndrome indicators (`High_Glucose`, `High_BP`, `High_BMI`, `Metabolic_Syndrome_Score`, `Potential_Metabolic_Syndrome`) which are more aligned with clinical definitions than `ms_age` or a simplified binary threshold.
    *   **Error Handling/Warnings:** Lacks the `try-except` blocks for file loading or `warnings.filterwarnings` seen in the GT, which makes the code less robust.
    *   **Visualization:** Critically missing any `matplotlib`/`seaborn` imports or plotting code, which was a core implied requirement for visualizing dimensionality reduction embeddings and cluster profiles. The ground truth heavily emphasizes this.
    *   **Optimal Parameter Determination (beyond K-Means):** The ground truth provides detailed methods for optimizing parameters for GMM (BIC/AIC), Hierarchical (Silhouette), DBSCAN/OPTICS (k-distance graph), and Spectral (Silhouette). The generated code generally defaults to `opt_k_gap` or hardcoded values.
    *   **Risk Stratification:** While it defines `risk_level` function, it lacks the detailed textual clinical interpretation and summary of characteristics for each cluster provided by the ground truth.
    *   **Imports:** Missing `matplotlib`, `seaborn`, `warnings`, `scipy.stats` (specifically `kruskal`), and `scipy.spatial.distance` (ground truth used `scipy.cluster.hierarchy`). `tensorflow.keras.optimizers.Adam` is used in GT, but not imported in generated code. (Minor, but worth mention).

3.  **Code Quality:**
    *   **Imports:** Imports are present but not fully comprehensive for all tasks attempted.
    *   **Comments/Prints:** Less verbose and explanatory comments/print statements compared to the ground truth, making it harder to follow the execution and results.
    *   **Magic Numbers:** Several hardcoded values (e.g., `eps=0.5`, `min_samples=5` for DBSCAN, `contamination=0.05` for Isolation Forest) without clear derivation or justification.

To achieve a higher score, the generated code would need to:
*   Implement robust parameter tuning for *all* clustering algorithms, not just K-Means.
*   Add comprehensive visualization for dimensionality reduction and cluster profiling.
*   Enhance statistical testing for phenotyping with clear outputs and appropriate tests (e.g., Kruskal-Wallis, Chi-squared).
*   Improve the feature engineering for ARM to discretize continuous features into multiple bins.
*   Provide a more detailed and clinically interpretable risk stratification system.
*   Ensure the time series clustering section is fully runnable and robust.
*   Improve error handling and overall code robustness.",o4_mini,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import numpy as np
import pandas as pd
import scipy.stats as st
from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.model_selection import cross_validate
from sklearn.model_selection import BaseCrossValidator
from sklearn.ensemble import VotingRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model impo...",6,"The generated code attempts to address most of the requirements but falls short in several areas, particularly regarding the robustness and correctness of implementation for advanced techniques.

**Strong points:**
*   Includes most of the required imports and libraries.
*   Covers a good number of ensemble methods (Voting, Bagging, Boosting, Stacking).
*   Correctly implements Optuna for hyperparameter tuning.
*   Attempts SHAP and LIME interpretation.
*   Includes a section for custom ensemble with dynamic weights and blending.

**Areas for Improvement / Missing Features:**

1.  **Data Preprocessing and Feature Engineering:**
    *   The `X` and `y` definition is incomplete. It only selects a subset of features from the original task description's data loading (Ground Truth code dropped columns and retained others). This limits the comprehensiveness of preprocessing.
    *   The ""advanced feature engineering"" is very basic (multiplication, sqrt, log, inverse) and hardcoded. The ground truth used `ColumnTransformer` with `PolynomialFeatures` *after* imputation and scaling, which is a more robust approach.
    *   The handling of categorical features (`cluster`) is inconsistent. `KMeans` is applied to `X` (which includes numerical features) before `ColumnTransformer`, which might lead to data leakage if not done carefully. The ground truth handles all preprocessing (including categorical encoding) within `ColumnTransformer` *after* data splitting.
    *   `l gbr` syntax error.

2.  **Nested Cross-Validation:**
    *   The task explicitly asked for *nested* cross-validation for robust model evaluation, but the generated code only uses Optuna for hyperparameter tuning *without* wrapping it in an outer cross-validation loop. The ""Nested CV with Optuna for XGBoost"" section tunes XGBoost but doesn't then evaluate this tuning robustly using a nested CV as specified. The ground truth explicitly demonstrates a nested CV structure with an outer loop.

3.  **Custom Ensemble with Dynamic Weight Assignment:**
    *   The implementation of `dynamic_ensemble_predict` trains models on `X_train` *every time* `predict` is called, which is highly inefficient and incorrect for production. The intent is to use weights calculated from *out-of-fold* predictions, train the final models on the full training data, and then combine predictions.
    *   The `models` dictionary directly contains pipelines *or* raw estimators (like `Ridge()`), which creates inconsistency and potential issues when fitting and predicting.
    *   The approach for calculating dynamic weights (`scores` dictionary) is correct in concept (inverse RMSE from CV), but the subsequent prediction function is flawed.

4.  **Advanced Techniques: Blending and Multi-level Stacking:**
    *   **Blending:** The current blending implementation trains base models on `X_btrain` and predicts on `X_bval`. This is fine for *training* the meta-learner. However, for actual *prediction* on `X_test`, the base models should typically be re-trained on the full `X_train` or `X` (if X_bval is part of X_train), and then their predictions on `X_test` would be fed to the meta-learner (also trained on full base model predictions). The `blend_pred` function in the generated code only calls `predict` on the base models without referencing their fitting state.
    *   **Multi-level Stacking:** The `second_level` stacking setup trains `stack` and `best_xgb` and `lgb` as base estimators directly, but `stack` itself is already a complex ensemble. It's not clear how the ""levels"" are truly distinct in this setup in a robust OOF prediction manner, especially compared to the ground truth's OOF approach for Level 1 combined with a simple Level 2 meta-learner.

5.  **Model Calibration and Uncertainty Quantification:**
    *   The code only provides a very basic approach to uncertainty (`preds_ensemble.std`) and calibrates this uncertainty (not the predictions themselves) using `IsotonicRegression`. This is a weak implementation for ""model calibration"" and ""uncertainty quantification"" as typically understood in regression. The ground truth explores residual plots and quantile regression which are more standard for regression calibration/uncertainty.

6.  **Ensemble Diversity Analysis and Model Combination Optimization:**
    *   The diversity analysis is limited to pairwise prediction correlation after all models are fit on `X_train`, which is okay.
    *   The weight optimization `minimize` function is implemented, but it takes `preds_all` which are already predictions on `X_test`. Optimal weights *should* be derived from out-of-fold predictions on the *training set* to avoid leakage, then applied to predictions on the *test set*. The ground truth correctly uses `oof_predictions` derived from the training set.

7.  **Code Quality & Readability:**
    *   Inconsistent variable naming, e.g., `l gbr` (with a space) causes syntax errors.
    *   Lack of clear documentation (comments) for complex sections like `dynamic_ensemble_predict`.
    *   A significant portion of the code trains models on `X_train`/`y_train` directly (e.g., in section 11 ""Fit all"") without linking them to the preprocessing pipelines defined earlier. This suggests that the preprocessing (`preprocessor`) defined in the `base_learners` might not be consistently used for all models trained outside of `base_learners`. The ground truth wraps *all* models in pipelines that include preprocessing, ensuring consistency.

In summary, while the code attempts most of the requested functionalities, many implementations are either incomplete, less robust than required, or contain subtle logical flaws (e.g., data leakage concerns, inefficient re-training). The lack of truly nested cross-validation is a major omission for robust evaluation.",o4_mini,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors
import statsmodels.api as sm
import statsmodels.formula.api as smf
from linearmodels.iv import IV2SLS
import semopy
import pingouin as pg
import pymc3 as pm
import theano
import arviz as az

# 1) Load and preprocess
df = pd.read_csv('world-happine...",6,"The generated code attempts to address most of the requirements but often falls short in depth, correctness, or best practices for the specified advanced statistical modeling and causal inference tasks.

Here's a breakdown by criteria:

1.  **Correctness (5/10):**
    *   **Preprocessing:** `df['region_code'] = OneHotEncoder(drop='first').fit_transform(df[['Regional indicator']]).toarray()` directly adds a multi-column array to a single column, which is incorrect. It should result in multiple one-hot encoded columns. The `StandardScaler` is applied to all numerical columns, including the target ('Ladder score'), which is problematic for interpreting coefficients in models unless explicitly inverted or handled. This also applies *after* imputation, which is fine, but it changes the scale of the target variable for all subsequent models, but the PyMC3 models don't account for this scaling when setting priors or interpreting.
    *   **Hierarchical Bayesian Model:**
        *   `mu_a = pm.Normal('mu_a', mu=0, sigma=1)` and `sigma_a = pm.Exponential('sigma_a', 1)` are okay for priors, but less informative than what's typically expected for ""advanced"" models where some notion of scale might be available.
        *   `b_gdp` is the only fixed effect, making it a very simple incomplete model. The task explicitly asked for ""economic, social, and psychological factors,"" which implies multiple predictors. The provided model only includes `Logged GDP per capita`.
        *   `sigma` was implicitly named `eps` in the model, which is fine, but the naming consistency is low.
        *   The regional index is correctly factored, but the `mu` calculation only includes GDP and not the random intercepts. It should be `mu = a[region_idx] + b_gdp * df['Logged GDP per capita']`. (Corrected in my review of the line).
        *   The `sigma` for `pm.Normal` should be `sd` not `sigma` in PyMC3. This would cause a runtime error.
    *   **SEM:** Uses `semopy`, which is a valid library. The model description is very basic and only defines a few paths, not a comprehensive SEM. It doesn't explicitly link to the ""economic, social, and psychological factors"" pathways as requested.
    *   **IV:** Uses `linearmodels.iv.IV2SLS` correctly for the most part. The instrument `Generosity` for `Logged GDP per capita` is highly questionable in a real-world scenario (generosity is more likely an outcome of GDP or social factors, not an instrument for GDP affecting happiness). However, since the task asks to *perform* it and validating the instrument isn't the primary goal, syntactically it's correct.
    *   **PSM:**
        *   The `NearestNeighbors` fitting for PSM `nbrs = NearestNeighbors(n_neighbors=1).fit(psm_df.loc[psm_df['treat']==0,['pscore']])` needs to be `psm_df.loc[psm_df['treat']==0,['pscore']].values` as single-column dataframe is deprecated.
        *   The calculation of `psm_effect` by `matched.groupby('treat')['Ladder score'].mean().diff().iloc[-1]` is technically correct for ATE, but no confidence intervals or significance testing is performed as is standard for PSM results.
    *   **Bayesian Regression with Informative Priors:** `b1` and `b2` are specified but only for GDP and Social Support. The task implies broader predictors like the hierarchical model. Priors `mu=0.5, sigma=0.5` are informative, but the choice is minimal.
    *   **Gaussian Process:** The usage of `pm.gp.Marginal` is correct and efficient. However, it only models happiness against GDP, not complex relationships with multiple features.
    *   **Mediation Analysis:** `pingouin.mediation_analysis` is a good choice for mediation, but it's a frequentist approach, not Bayesian, which contradicts the prompt's strong emphasis on Bayesian methods.
    *   **Difference-in-Differences:** Simulation is simple. The formula `policy + C(year) + policy:C(year)` implies `year` is categorical, which is good. The `policy` variable definition is a bit arbitrary `((df['Regional indicator']=='Western Europe') & (df['year']>=2020)).astype(int)`. It works as a simulation.
    *   **Model Comparison:** `az.compare` is used correctly with appropriate models. `np.exp(comp['loo_diff'].iloc[1] - comp['loo_diff'].iloc[0])` for Bayes Factor is a misunderstanding. `az.compare` uses information criteria (LOO or WAIC); Bayes Factors are a different beast.
    *   **Credible Intervals, PPC, Sensitivity:**
        *   PPC is performed.
        *   `az.hdi(hier_trace)` gets CIs, but no explicit printing or further analysis of these.
        *   Sensitivity is done with `az.plot_forest`, which *can* be used for comparing parameter posteriors, but a proper ""sensitivity analysis"" as per the prompt often implies re-running models with varied prior choices and showing how results change. This is a very rough interpretation.

2.  **Code Quality (6/10):**
    *   **Imports:** Mostly correct, though `theano` is imported but not explicitly used in a way that couldn't be done otherwise (PyMC3 handles its Theano backend implicitly). `pingouin` and `linearmodels` are good additions for their respective tasks.
    *   **Structure:** Code is generally organized by task, which is good.
    *   **Readability:** Variable names are okay. Lack of comments makes it harder to follow assumptions and choices (e.g., why 'Generosity' for IV). No error handling for file not found.
    *   **Reproducibility:** `np.random.seed` or `pm.set_data_buffer_size` are missing. `target_accept` is good.
    *   **PyMC3 specifics:** Using `sigma` instead of `sd` for `Normal` distribution's standard deviation parameter is a syntax error in PyMC3: `pm.Normal('var', mu=0, sigma=1)` should be `pm.Normal('var', mu=0, sd=1)`. This is a critical error that would prevent the PyMC3 models from running.

3.  **Completeness (7/10):**
    *   **Loading/Preprocessing:** Addressed but with some issues mentioned above (ohe issue, target scaling). Missing direct handling of `Country_name` and `Regional_indicator` which become `object` types after imputation.
    *   **Hierarchical Bayesian Model:** Implemented, but too simple (only GDP as fixed effect).
    *   **SEM:** Implemented using `semopy`, but the model is very minimal and doesn't showcase complex causal pathways as suggested by the task (""analyze causal pathways between economic, social, and psychological factors"").
    *   **Causal Inference (IV and PSM):** Implemented, but the choice of IV is weak, and PSM lacks significance testing and robust matching (e.g., caliper, common support).
    *   **Bayesian Regression with Informative Priors:** Implemented, but again, uses only two predictors.
    *   **Gaussian Processes:** Implemented, but only for one predictor (GDP), not ""non-parametric modeling of complex relationships"" generally.
    *   **Mediation Analysis:** Implemented, but uses a frequentist `pingouin` package, not a Bayesian approach which is implicitly preferred given the overall context of the prompt.
    *   **Difference-in-Differences:** Simulated and implemented, which is fair given a cross-sectional dataset.
    *   **Bayesian Model Comparison (WAIC, LOO-CV, Bayes Factors):** WAIC/LOO-CV implemented. Bayes Factors are attempted but incorrectly derived from LOO-CV.
    *   **Credible Intervals, PPC, Sensitivity Analysis:** PPC and CIs are present. Sensitivity analysis is weak.

Overall, the code provides a basic implementation for most points, but often lacks the specific depth, correctness, or full Bayesian integration implied by ""advanced statistical modeling and causal inference"" and the details of the prompt. The PyMC3 `sigma` vs `sd` issue is a critical correctness flaw. The mishandling of one-hot encoding on `region_code` is also a significant error.",o4_mini,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE, Isomap, MDS, SpectralEmbedding
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklear...",6,"The generated code attempts to address many aspects of the complex task but falls short on correctness, completeness, and code structure compared to the ground truth.

**Correctness:**
*   **Data Preprocessing:** The generated code drops NA values (`df.dropna()`) which is a very naive approach and can lead to significant data loss. The ground truth uses `SimpleImputer` which is far more robust.
*   **Feature Engineering:** The feature engineering in the generated code is simplistic (`GDP_SS_ratio`, `Life_Freedom_sum`) compared to the more thoughtful choices and regional characteristic integration in the ground truth.
*   **Optimal K:** The `gap_statistic` function is incomplete and will not run as provided (missing `from sklearn.metrics import pairwise_distances` and incorrect reference data generation, no actual calculation of `Wk` and `logWk_b`).
*   **Consensus Clustering:** The `affinity='precomputed'` for `AgglomerativeClustering` requires a distance matrix, not a similarity matrix. The `1-cons_mat` attempts to convert it, but `affinity` also needs `linkage` method to be specified. The ground truth explicitly uses `linkage` then `fcluster` on `1-co_association_matrix`.
*   **Network Analysis:** The network analysis in the generated code calculates correlation (`np.corrcoef(X_feats.T)`) between *features*, not between *countries*. The task explicitly asks for ""relationships between countries based on happiness factors,"" which means calculating similarity between country feature vectors. The ground truth correctly uses `cosine_similarity(X)` between countries.
*   **Recommendation System:** The recommendation system is very basic and the `df.iloc[n].Country name` syntax is incorrect and will cause an error (should be `df.iloc[idx[i]].loc[:, 'Country name'].tolist()`). It does not incorporate policy insights or geopolitical validation meaningfully.

**Completeness:**
*   **Structure:** The generated code is a single script, not organized into a class structure. This makes it less modular, reusable, and harder to manage, especially for a complex task like this. The ground truth uses a comprehensive class.
*   **Optimal Clustering Parameters:** While it attempts `gap_statistic`, it lacks the full conceptual discussion and sophisticated implementation seen in the ground truth for ""stability analysis.""
*   **Detailed Country Profiles:** The profiling is very general (`profiles[name][c] = {'mean':means, 't':tvals, 'effect_size':es}`). The ground truth provides specific statistical significance testing (ANOVA) and effect size (eta-squared) *for each feature across clusters*, which is crucial for detailed profiles.
*   **Clustering Algorithms:** While it implements the algorithms, the hyperparameter tuning for DBSCAN and HDBSCAN is fixed/basic, whereas the ground truth shows a more robust approach to finding somewhat optimal parameters (even if simplification was needed for some).
*   **Visualization:** The generated code lacks explicit visualization for dimensionality reduction outcomes with clusters, and for anomaly detection results, which the ground truth includes and is vital for understanding.
*   **Policy Insights & Geopolitical Validation:** This section is completely missing, while the ground truth provides a detailed, qualitative analysis based on cluster profiles and suggests how to perform geopolitical validation.

**Code Quality:**
*   **Imports:** Imports are generally correct.
*   **Readability:** No comments or clear function separation makes it harder to follow than the ground truth's class-based, commented approach.
*   **Error Handling:** Lacks any error handling for various steps, unlike the ground truth which tries to gracefully handle scenarios like no valid clusters found.

Overall, the generated code provides a skeletal attempt but misses many nuances and robust implementations present in the ground truth, particularly in data preprocessing, quality of feature engineering, correct application of network analysis, detailed statistical profiling, and overall code structure and completeness of requirements.",o4_mini,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
import pmdarima as pm
from sklearn.metrics import mean_squared_error

# Load and prepare data
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
df = df[df['location'] == 'Germany'].set_index('date')
cols = ['new_cases', 'stringency_index', 'people_fully_vaccinated']
df[cols] = df[cols].interpolate(method='time').ffill()

# Train/test split
train, test =...",7,"The generated code provides a good attempt at the task, correctly implementing the core SARIMA model with exogenous variables and performing evaluation and plotting.

**Strengths:**
*   Correctly loads and filters data.
*   Handles time series specific operations like setting index and train/test split.
*   Successfully uses `pmdarima.auto_arima` to find optimal orders for SARIMA.
*   Correctly trains `SARIMAX` with exogenous variables and forecasts.
*   Evaluates using RMSE and plots the results.
*   Uses `enforce_stationarity=False` and `enforce_invertibility=False` which can be helpful for auto_arima selected non-stationary/non-invertible orders.

**Areas for Improvement (leading to a score of 7 instead of higher):**

1.  **Missing Value Handling:** The current missing value handling `df[cols].interpolate(method='time').ffill()` is a good start, but it's not robust enough for all scenarios (e.g., if a column is entirely NaN, or if there are leading NaNs). The ground truth's approach of `ffill().bfill().interpolate().ffill().bfill()` with checks is more comprehensive. This is a crucial aspect of time series data preparation.
2.  **`auto_arima` Parameters:** While `auto_arima` is used, crucial parameters like `start_p`, `start_q`, `max_p`, `max_q`, `test='adf'`, `start_P`, `start_Q`, `max_P`, `max_Q`, etc., are missing. These parameters guide the search space of `auto_arima` and are important for finding potentially better models and for controlling the computational cost. The ground truth provides a more robust set of parameters.
3.  **Error Handling/Robustness:** The code lacks robust error handling for file not found, or scenarios where data might be insufficient (e.g.,
    if `len(data)` < `test_days`). This makes the code less robust in real-world scenarios.
4.  **Forecast Index Alignment:** While the forecast does get an index, explicitly aligning it with `y_test.index` (as done in the ground truth: `forecast_values.index = y_test.index`) is a good practice to ensure consistency, especially if `get_forecast` doesn't always perfectly inherit the index.
5.  **Negative Forecasts:** The code does not handle negative forecasts (e.g., `forecast_values[forecast_values < 0] = 0`), which are not logical for 'new_cases' and can occur with SARIMA models.
6.  **Readability/Comments:** While not explicitly requested, adding comments, especially for critical sections like `auto_arima` parameters and missing value handling, would improve readability. Printing informative messages about the steps being performed is also helpful.

Overall, it's a solid implementation that correctly addresses the primary requirements. The missing value handling and `auto_arima` parameter selection could be more thorough to achieve a higher score and ensure robustness for various datasets.",o4_mini,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import scikit_posthocs as sp

# Load data
df = pd.read_csv('winequality-red.csv', sep=';')

# Define quality groups
bins = [-np.inf, 4, 6, np.inf]
labels = ['low', 'medium', 'high']
df['quality_label'] = pd.cut(df['quality'], bins=bins, labels=labels)

# Features to test
features = [c ...",7,"The generated code correctly addresses most of the requirements but has some significant issues.

**Strengths:**
*   **Correct Category Definition:** It correctly defines low, medium, and high quality wines.
*   **Appropriate Statistical Tests:** It correctly uses Shapiro-Wilk for normality, Levene's for homogeneity of variance, and switches between ANOVA and Kruskal-Wallis based on assumptions.
*   **Post-hoc Tests:** It correctly applies Tukey's HSD for ANOVA and uses `scikit_posthocs.posthoc_dunn` for Kruskal-Wallis, which is an excellent choice.
*   **Box Plots:** Generates box plots for significant features.
*   **Summary:** Provides a summary of significantly different features.

**Areas for Improvement (leading to a score of 7, not higher):**

1.  **Hardcoded Alpha Value for Normality/Homogeneity:** The code uses `0.05` directly (`stats.shapiro(g)[1] > 0.05` and `stats.levene(*groups)[1] > 0.05`) without calling it `ALPHA`, which is fine, but it assumes that a property should be *exactly* normal/homogeneous to proceed with parametric tests. In practice, researchers often allow for some violation, especially for larger sample sizes, or might use a slightly different alpha for these preliminary tests. A more robust approach, as in the ground truth, would be to store the p-values and clearly state the normality/homogeneity status.
2.  **Handling of Missing Values/Insufficient Data:** The ground truth meticulously checks if there's enough data in each group for statistical tests (e.g., minimum 3 for Shapiro-Wilk, 2 for pairwise comparisons). The generated code doesn't explicitly handle cases where a quality group might have too few samples for a test, which could lead to errors or unreliable results.
3.  **Error Handling for File Loading:** The ground truth includes `try-except` for `FileNotFoundError`, which is good practice. The generated code assumes the file will always be present.
4.  **Implicit `sep=';'` in `pd.read_csv`**: The `winequality-red.csv` typically uses commas as separators. The generated code uses `sep=';'`, which would likely lead to an error or incorrect data loading if the file uses commas. This is a critical bug. (Assuming the standard dataset, if the specific file happened to be semicolon-separated, this point would be moot for correctness, but it's not a safe general assumption).
5.  **Output Readability (Post-hoc Dunn's):** While using `scikit_posthocs` is excellent, printing the entire DataFrame for `posthoc_dunn` directly might be less readable than a more structured output, especially if comparisons are numerous. The ground truth's approach of enumerating comparisons and their corrected p-values is more verbose but clearer for specific conclusions.
6.  **Redundant Imports:** `statsmodels.api as sm` is imported but not used.
7.  **No `quality_order` for Plotting:** The ground truth explicitly defines `quality_order = ['low', 'medium', 'high']` and uses `pd.Categorical` to ensure `quality_label` is ordered correctly. While `sns.boxplot` might handle this correctly by default given the `labels` list, explicitly setting categorical order is more robust.
8.  **Lack of Intermediate Debugging Info:** The ground truth prints which test is being used (ANOVA/Kruskal-Wallis) for each feature along with their p-values, and whether it's significant or not. This provides valuable insights during execution. The generated code only summarizes the significant ones at the end.",o4_mini,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"import pandas as pd
import numpy as np
from datetime import timedelta
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score, f1_score
from xgboost import XGBClassifier
import shap

# 1) Load data
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
df = df.sort_values(['location', 'date'])

# 2) Compute 90th percentile threshold and target
thresholds = df[df['new_deaths'] > 0].groupby('location'...",7,"The generated code addresses most of the requirements but has several critical issues that prevent it from being a strong solution.

**Positive Aspects:**
*   **Target Variable Definition:** Correctly identifies the 90th percentile of `new_deaths` (excluding zeros) per `location` to define `high_daily_death_increase`.
*   **Feature Engineering:** Implements rolling averages/STDs and lag features as requested. Correctly calculates `days_since_first_case`.
*   **Missing Value Handling:** General `fillna(0)` is applied, which aligns with one of the prompt's suggestions, though could be more nuanced.
*   **Feature Selection:** Includes the specified features including the derived `vaccination_rate`.
*   **Encoding and Scaling:** Uses `StandardScaler` for numeric features and `pd.get_dummies` for one-hot encoding `continent`.
*   **Model Training and Tuning:** Uses `XGBoostClassifier` and `GridSearchCV` with appropriate parameters.
*   **Evaluation:** Calculates ROC-AUC and F1-score.
*   **SHAP Explanation:** Implements SHAP for feature importance.

**Areas for Improvement / Critical Issues:**

1.  **Missing Value Handling in Feature Engineering (Critical):**
    *   The `fillna(0, inplace=True)` is applied **globally** after all feature engineering, which is problematic. Rolling statistics and lagged features inherently produce `NaN`s at the beginning of each series (e.g., first 7 days for rolling, first 1/3/7 days for lags). These `NaN`s **should be filled per group (`location`)** to ensure data integrity within time series. A global fillna (especially with 0) can incorrectly fill strategic `NaN`s that represent ""no data available yet for this window/lag"" with `0`, potentially altering the meaning significantly. The ground truth used `ffill` then `fillna(0)` per group, which is a much more robust approach for time-series features. This is a common pitfall in time-series FE.
    *   The `days_since_first_case` calculation is problematic. It first maps `first_case_dates` (which will be `NaN` for locations with no `new_cases > 0`), then calculates differences. If a `first_case_date` is `NaN`, the `days_since_first_case` will become `NaT` (Not a Time) and then `NaN` after `.dt.days`. These `NaN`s are then filled with 0 by the global `fillna(0)`. This effectively sets ""days since first case"" to 0 for locations that never had a case, which might be misleading.
2.  **Time-Based Split (Critical):**
    *   The split `cutoff = data['date'].quantile(0.80)` is incorrect for a time-based split. A time-based split means you take the **last 20% of the *time points***, not the 80th percentile of *dates* present in the dataset. If many locations have recent data, the 80th percentile might include dates from earlier in the data. The correct method is to sort by date and then split by index, taking the last `X` percent of the *rows*. The ground truth correctly uses `df.sort_values(by='date')` and then splits by `int(len(df) * 0.8)`. This impacts the integrity of the evaluation.
3.  **Data Leakage in Scaling (Minor but important):**
    *   `X[numeric_cols] = scaler.fit_transform(X[numeric_cols])` performs `fit_transform` on the *entire dataset* (`X`) before the train-test split. This is a data leak because the scaling parameters (mean, std) are learned from the test data as well. `fit` should be applied only on `X_train` and `transform` applied on both `X_train` and `X_test`. Pipelines are designed to correctly handle this, but since the scaling is done manually, it's a common mistake.
4.  **One-Hot Encoding Integration (Minor):**
    *   One-hot encoding is done *before* the StandardScaler, meaning `pd.get_dummies` creates new columns. Then, the `numeric_cols` selection excludes these created dummy columns. While technically correct for this specific setup, it's generally better practice to use `ColumnTransformer` within a `Pipeline` (as in the ground truth) to manage various preprocessing steps concisely and prevent data leakage, especially when scaling and encoding are involved.
5.  **SHAP Plotting and Explanation:**
    *   The SHAP explanation part uses `shap_values = explainer.shap_values(X_train)`. While okay for global importance, for consistent evaluation/plotting post-preprocessing, it's generally better practice to pass `X_train_processed` (i.e., `X_train` *after* scaling and one-hot encoding) to `explainer.shap_values`. The ground truth handles this by getting feature names after preprocessing and transforming `X_train_df` with the preprocessor. Without this, the SHAP plot's X-axis labels for encoded features might be generic (e.g., 'continent_1', 'continent_2').
    *   The code handles the SHAP values correctly for binary classification (list case), but doesn't explicitly `import matplotlib.pyplot as plt` even though `shap.summary_plot` often uses it internally.

**Overall:**
The code attempts to solve all aspects of the problem and shows a good understanding of the typical ML workflow. However, the critical errors in handling missing values for engineered features and, more importantly, the incorrect time-based train-test split, undermine the validity and reliability of the model and its evaluation. Addressing these issues significantly would elevate the solution.",o4_mini,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.formula.api as smf

# 1) Load data and parse dates
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 1) Calculate vaccination percentage
df['vaccination_percentage'] = df['people_fully_vaccinated'] / df['population']

# 2) Handle missing values: interpolation then median imputation, by location
df = df.sort_values(['location', 'date'])
cols_to_impute = ['reproduction_rate', 'stringency_index', 'vaccination_...",6,"The generated code attempts to address all parts of the task but falls short on several critical points, particularly concerning missing value handling, exact model specification, and detailed interpretation.

**Strengths:**
*   **Correct Imports:** Essential libraries (pandas, numpy, statsmodels.formula.api) are correctly imported.
*   **Basic Data Loading:** Successfully loads the CSV and parses dates.
*   **Correct Model Formula for Fixed Effects:** Uses `stringency_index * vaccination_percentage` which correctly includes the main effects and their interaction.
*   **Attempts at Missing Value Handling:** Recognizes the need for imputation and uses `interpolate` and `median`.
*   **Reporting:** Prints fixed effects coefficients, standard errors, p-values, and some variance components.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling (Critical):**
    *   **Calculation of `vaccination_percentage`:** This is done *before* handling missing values in `people_fully_vaccinated` and `population`. If `population` is 0 or NaN, or if `people_fully_vaccinated` is NaN, `vaccination_percentage` will be incorrect or NaN from the start. The ground truth correctly imputes `population` and `people_fully_vaccinated` first, then calculates the percentage, and then caps it at 100%.
    *   **Imputation Logic:** The `fillna(x.median())` within the `transform` after `interpolate` is problematic. `x.median()` calculates the median of the *subset* of data for that group *before* interpolation. If `interpolate` can fill a value, `fillna` will not apply. If `interpolate` *cannot* fill a value (e.g., all nans in a group or leading/trailing nans), then `x.median()` might be `NaN` if the group itself has no non-NaN values for that column. A more robust approach, as in the ground truth, is to interpolate first, then fill any *remaining* NaNs (e.g., leading/trailing NaNs not covered by interpolation or groups entirely of NaNs) with a global median or a group's median *after* a first pass of interpolation. The ground truth uses `limit_direction='both'` for interpolation, which is crucial for time-series data.
    *   **Population and Vaccination Imputation:** The task explicitly asks to handle missing values for 'reproduction_rate', 'stringency_index', and 'vaccination_percentage'. The ground truth also correctly considers the inputs to `vaccination_percentage` (`population`, `people_fully_vaccinated`) for imputation. The generated code does not explicitly handle `population` or `people_fully_vaccinated` missing values.

2.  **Mixed-Effects Model Specification (Critical):**
    *   **Random Effects Structure:** The task explicitly states: ""Random effects: random intercepts for 'location' **nested within 'continent'**.""
        *   The generated code uses `groups=df[""location""]`. This specifies random intercepts for `location` only. It does not account for nesting within `continent`.
        *   The `vc_formula={""continent"": ""0 + C(continent)""}` attempts to add `continent` as a variance component, but in `mixedlm`, `vc_formula` is for *additional* random effects (e.g., random slopes or unstructured covariance for multiple random effects). For nested random intercepts (location within continent), the typical approach in `statsmodels` (and as correctly implemented in the ground truth) is to create a unique identifier for each (continent, location) combination and use that as the `groups` argument. The ground truth correctly uses `df['continent_location_id'] = df['continent'].astype(str) + '_' + df['location'].astype(str)` for this. The generated code's approach for random effects is incorrect for the specified nested structure.

3.  **Completeness & Interpretation:**
    *   **Interpretation of Fixed Effects:** The task asks for interpretation of significance and direction. The generated code only prints the coefficients, standard errors, and p-values, but lacks the detailed interpretation provided in the ground truth.
    *   **Reporting Variance Components:** The generated code correctly prints `result.cov_re.iloc[0, 0]` for the location random intercept variance and `result.vcomp` for the continent variance component (though the continent variance component is not correctly specified in the model). However, the prompt asks for ""variance components for the random effects"" which, for a random intercept model, typically refers to the variance of the random intercepts. The ground truth cleanly extracts and explains this single value (`random_intercept_variance`).
    *   **Model Assumptions Discussion:** The discussion is very brief and in comments, lacking the detail and structure of the fully-written discussion in the ground truth.

4.  **Robustness and Error Handling:**
    *   No `try-except` block for file loading.
    *   No warnings suppression for `statsmodel` convergence issues, which can be common.
    *   No clear messages for the execution flow.

In summary, while the core idea of using `mixedlm` is present, the specific implementation of the random effects structure and the robustness of missing value handling are significant inaccuracies that prevent it from fully meeting the task requirements. The output and interpretation are also less comprehensive.",o4_mini,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 2) Select features
X = df[['elixirCost', 'Win Rate', 'Usage']]

# 3) Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4) Determine optimal K using the elbow method
wcss = []
for k in range(2, 8):
    km = KMeans...",8,"The generated code correctly addresses almost all aspects of the task. It loads the dataset, selects, and standardizes the features, correctly implements the elbow method for K-selection (though it hardcodes K=3 without explicitly stating that this is an assumed optimal, which is fine for the task), applies K-Means, performs PCA, and visualizes the results.

Strong points:
- All required imports are present.
- The feature selection, standardization, K-Means application, and PCA steps are correctly implemented.
- The elbow method plot is generated, which is crucial for K selection.
- The final scatter plot with PCA-reduced features and cluster coloring is correctly produced.
- Use of `random_state` ensures reproducibility.
- Use of `n_init=10` is good practice and avoids a scikit-learn warning.

Areas for improvement (to reach a higher score):
- **Error Handling**: The ground truth includes robust error handling for `FileNotFoundError` and missing columns. The generated code lacks this, which makes it less robust in real-world scenarios.
- **Plot Labels/Information**: The final PCA plot is basic. The ground truth adds more informative axis labels including explained variance, a color bar, and a title that explicitly mentions the chosen K, which greatly enhances interpretability.
- **Chosen K Justification**: While the elbow plot is shown, the generated code directly picks `k = 3` without a comment indicating *why* this K was chosen based on the plot (even if it's a simple ""based on the elbow plot, K=3 was chosen""). The ground truth explicitly states this, guiding the user.
- **Print Statements**: The ground truth includes print statements to show progress and intermediate data shapes/contents, which is helpful for debugging and understanding the flow. The generated code is more concise but sacrifices some transparency.

Overall, it's a very solid implementation for the given task requirements. The missing error handling and less informative plots are the main reasons it doesn't achieve a perfect score compared to the robust and well-documented ground truth.",o4_mini,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error

# 1) Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 2) Select features and target
X_num = df[['elixirCost', 'Usage']]
y = df['Win Rate']

# 3) Preprocess the 'rarity' column
encoder = OneHotEncoder(drop='first', sparse=False)
rarity_en...",9,"The generated code correctly addresses all requirements of the task. It loads the data, correctly selects features and the target, preprocesses the `rarity` column using `OneHotEncoder` (which is a valid alternative to `pd.get_dummies` and explicitly requested in the prompt's import hint), splits the data, trains a `RandomForestRegressor` and evaluates it using R2 and MSE.

Strong points:
- Correctly loads the dataset.
- Correctly identifies and separates numerical and categorical features for processing.
- Successfully uses `OneHotEncoder` for `rarity` and correctly handles the `drop='first'` argument for multicollinearity.
- Correctly concatenates the processed features.
- Implements the train-test split with the specified `test_size` and `random_state`.
- Trains a `RandomForestRegressor` model as requested.
- Evaluates the model using both R-squared and MSE.
- Includes all necessary imports.

Areas for minor improvement (not critical enough to reduce score significantly):
- The `try-except` block for file loading in the ground truth is more robust. While not explicitly asked, it's good practice.
- The ground truth includes checks for missing columns and handles potential `NaN` values, which makes it more robust to real-world data issues. The generated code assumes clean data in the specified columns.
- The output printing for R2 and MSE could be slightly more descriptive, similar to the ground truth.",o4_mini,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy import stats

# Hypotheses:
# H0: The mean Win Rate is the same across all rarity categories.
# H1: At least one rarity category has a different mean Win Rate.

# 1) Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 2) Identify unique rarity categories
rarities = df['rarity'].unique()
print(""Rarity categories:"", rarities)

# 3) Prepare data for ANOVA
groups = [df[df['rarity'] == r]['Win Rate'] for r in rarities]

# 4) Perform one-way ANOVA
f_...",7,"The generated code correctly performs the ANOVA test and interprets the results. It loads the data, identifies unique categories, and properly uses `scipy.stats.f_oneway`. The hypotheses are stated, and the F-statistic and p-value are printed, followed by a correct interpretation based on the significance level.

However, there are a few areas for improvement, primarily concerning robustness and clarity:
1.  **Error Handling for File Loading:** The generated code lacks error handling for `FileNotFoundError` or other exceptions when loading the CSV. The ground truth includes a `try-except` block, which is crucial for real-world applications.
2.  **Data Cleaning/Validation:** The generated code assumes 'Win Rate' is always numeric and doesn't handle potential non-numeric values or NaNs in 'Win Rate' or 'rarity'. The ground truth explicitly converts 'Win Rate' to numeric with `errors='coerce'` and drops NaNs, which is more robust.
3.  **Group Validity for ANOVA:** The generated code creates groups using `df[df['rarity'] == r]['Win Rate']`. While this works for the general case, it doesn't explicitly check if each group has enough observations (e.g., at least two) required for a meaningful ANOVA, or if there are at least two distinct `rarity` groups to compare. The ground truth includes checks for empty dataframes and groups with less than two observations, adding to its robustness.
4.  **Output Clarity:** The output for hypotheses is not printed, and the interpretation statement is a bit terse compared to the ground truth which provides more descriptive conclusions (""Conclusion: There is a statistically significant difference..."").",o4_mini,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# 1) Load the dataset
df = pd.read_csv(""iris.csv"")

# 2) Define features and target
X = df[[""SepalLengthCm"", ""SepalWidthCm"", ""PetalLengthCm"", ""PetalWidth...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task. It successfully loads the data, defines features and target, encodes the target, splits the data, trains the specified models, calculates and prints all required metrics, and identifies the best model based on F1-score.

Strong points:
- All required libraries are imported correctly.
- Data loading and feature/target selection are accurate.
- Target encoding is correctly implemented.
- Data splitting uses the correct `test_size` and `random_state`.
- All three specified classifiers are used with their correct parameters (Logistic Regression default, SVC linear kernel with `random_state`, RandomForest with `n_estimators` and `random_state`).
- Performance metrics (accuracy, precision, recall, F1-score) are correctly calculated using `macro` averaging as requested.
- The output clearly states the performance for each model.
- The best performing model is correctly identified based on the highest F1-score.

Minor improvements:
- The ground truth code included `stratify=y_encoded` in `train_test_split`, which is good practice for classification tasks to ensure class distribution is preserved in splits, especially for imbalanced datasets. While not explicitly asked for in the prompt, it's a beneficial addition for robustness. The generated code omitted this.
- The ground truth code also added `max_iter=200` to `LogisticRegression` to prevent potential convergence warnings, which can sometimes occur with default settings. While not strictly necessary for this dataset, it's a good practice.
- The ground truth included basic error handling for `FileNotFoundError`, which is also a good practice for robust code.

Overall, the generated code is excellent and meets the task requirements effectively. The omitted `stratify` parameter is the primary reason for not giving a perfect 10, as it's a crucial consideration in classification data splitting.",o4_mini,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. Load dataset
df = pd.read_csv('iris.csv')

# 2. Select features
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# 3. Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto',...",9,"The generated code is excellent and correctly addresses almost all requirements of the task.

**Strong Points:**
*   **Correctness:** All steps (loading data, feature selection, standardization, K-Means, PCA, plotting) are performed correctly.
*   **K-Means Parameters:** `n_init='auto'` and `random_state=42` are correctly applied for K-Means.
*   **PCA Components:** PCA is correctly reduced to 2 components.
*   **Plotting:** Both required scatter plots are generated, with points colored by K-Means labels and true species labels, respectively. Titles are also added.
*   **Imports:** All necessary libraries are correctly imported.
*   **Readability:** The code is well-structured and easy to follow.

**Areas for Minor Improvement (for a perfect 10):**
*   **Missing `plt.show()` after the first plot:** While `plt.show()` at the very end will display all open figures, it's generally good practice to call `plt.show()` after each `plt.figure()` block if you want to display them sequentially or ensure they pop up immediately. In this case, since there are two figures, it implicitly shows both at the end. However, adding `plt.show()` after the first plot would make it more explicit that the first plot is distinct. For this benchmark, it's a very minor point and not a functional error.
*   **Plot Enhancements (Optional but in Ground Truth):** The ground truth included `edgecolor='k'` and `s=50` for the scatter points for better visual clarity, and `plt.grid(True)` for better data presentation. It also added colorbars which are useful for interpreting the `cmap='viridis'` values directly, especially for the K-Means clusters and to map numerical species labels back to their names. While these are not strictly *required* by the prompt, they are good practices for data visualization.

Overall, the generated code performs the core task exceptionally well. The minor points are mostly about visual enhancements and explicit plot management rather than functional correctness.",o4_mini,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

df = pd.read_csv('iris.csv')
df['PetalArea'] = df['PetalLengthCm'] * df['PetalWidthCm']
df['SepalArea'] = df['SepalLengthCm'] * df['SepalWidthCm']

X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'PetalArea', 'SepalArea']]
le = LabelEncoder()
y = le.fit_transform(df['Species'])

model = GradientBo...",9,"The generated code correctly addresses all requirements of the task. It loads the data, creates the specified features, defines `X` and `y` correctly (including encoding `y`), initializes and trains the Gradient Boosting Classifier, and performs 5-fold cross-validation, reporting the mean accuracy. All necessary imports are present.

The only minor area for improvement that prevents a perfect score is the output clarity. The prompt asked to ""Report the mean cross-validated accuracy score"" and the generated code just prints the float value of the mean. While technically correct, the ground truth provides a more user-friendly output with a descriptive label for the printed value, as well as showing the individual fold scores. This is a very minor aesthetic point and doesn't affect the core functionality or correctness of the solution.",o4_mini,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# 1. Load the dataset
data = pd.read_csv('heart.csv')

# 2. Define features and target
feature_cols = ['Age', 'Sex', ...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task. It successfully loads the data, defines features and target, performs correct preprocessing (one-hot encoding and scaling) using `ColumnTransformer` and `Pipeline`, splits the data appropriately, trains a `RandomForestClassifier`, and evaluates it using all specified metrics. The variable naming is clear, and the code structure is logical.

One minor point for improvement, which is not a functional error but a best practice, is the omission of `stratify=y` in the `train_test_split` function. While the task prompt didn't explicitly require it, it's generally good practice for classification tasks, especially with imbalanced datasets, to ensure the class distribution is maintained in the training and testing sets. The ground truth includes this. Second, the ground truth includes more robust error handling for `FileNotFoundError` for the dataset, but this was not explicitly asked for by the prompt, so it's not a deduction here.",o4_mini,LLM_judge_config
H29,hard,Clustering,heart.csv,"import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load dataset
df = pd.read_csv('heart.csv')

# 2) Select numerical features
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']
X = df[features]

# 3) Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4) Determine K using elbo...",8,"The generated code correctly addresses most of the task requirements. It loads the dataset, selects and standardizes features, calculates WCSS and silhouette scores for K-means to determine an optimal K, applies K-means, performs PCA, and visualizes the clusters.

Areas for improvement:
- **K selection justification**: While the code calculates silhouette scores and plots WCSS, it automatically selects `best_k` based *only* on the highest silhouette score. The prompt specifically asks to ""Use the elbow method (plotting Within-Cluster Sum of Squares - WCSS) and silhouette scores to justify your choice of K."" A more robust approach, often involving manual inspection and human judgment, considers both plots. The ground truth explicitly states, ""Based on the plots, choose an appropriate K."" and makes an explicit choice based on typical patterns. While automating based on silhouette is a valid strategy, it deviates slightly from the nuanced justification expected by the prompt.
- **Plotting**: The WCSS and Silhouette plots are generated separately, making it harder to compare them side-by-side as one might do for comprehensive K selection. The generated code only plots WCSS. The prompt asked for both.
- **Error Handling**: The ground truth includes basic error handling for `FileNotFoundError` for the CSV, which is a good practice not present in the generated code.
- **Plot Labels/Titles**: The `plt.figure()` calls lack specific figure titles for better organization when multiple plots are generated. The PCA plot itself is good but doesn't explicitly state the chosen K in the title.
- **Completeness of Silhouette Plot**: The generated code calculates silhouette scores and prints them, but it *does not plot* the silhouette scores as the ground truth does and as implied by ""plotting... silhouette scores to justify your choice of K."" in the prompt. This is a significant omission.
- **`n_init='auto'`**: The generated code correctly uses `n_init='auto'`, which is good.",o4_mini,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report

# Load data
df = pd.read_csv('winequality-red.csv', sep=';')

# Create binary target
df['quality_label'] = np.where(df['quality'] >= 7, 1, 0)
df = df.drop(columns=['quality'])

# ...",8,"The generated code largely fulfills the requirements of the task.

**Strong Points:**
*   **Correct Binary Target:** The code correctly redefines the target variable (`quality_label`) based on the `quality` column.
*   **Data Preprocessing:** It includes both imputation (though `SimpleImputer` with 'mean' strategy might be overkill for this dataset as there are no missing values based on problem knowledge, the implementation is correct) and scaling using `StandardScaler`.
*   **Train/Test Split:** The data is split correctly with `test_size=0.2` and `stratify=y` and `random_state`.
*   **Model Training:** A `LogisticRegression` model is initialized and trained.
*   **Evaluation:** Accuracy, F1-score, and `classification_report` are correctly calculated and printed.
*   **Feature Importance:** It correctly extracts and reports feature importance based on coefficients.

**Areas for Improvement (to reach a higher score):**
*   **Dataset Loading:** The generated code uses `sep=';'` for `pd.read_csv`. While `winequality-red.csv` is typically comma-separated, some versions or local setups might use semicolons. The ground truth code assumes comma-separated (default) and includes error handling for `FileNotFoundError`. The generated code lacks explicit error handling for file loading.
*   **Missing Value Handling (Implicit):** While `SimpleImputer` is used, the prompt specifies ""handle missing values if any."" The ground truth explicitly checks `if X.isnull().sum().sum() > 0:` which is a more robust way to demonstrate handling _only if necessary_ rather than unconditionally applying an imputer. For this dataset, there are no missing values, so the imputation step is technically unnecessary, but the `SimpleImputer` implementation is correct if they were present.
*   **Feature Importance Output:** The output for feature importance is good, but the ground truth `DataFrame` approach is slightly more organized and allows for easier inspection of both raw and absolute coefficients, which is a good practice. The generated code sorts correctly by `abs(x[1])` and prints, but it's a list of tuples which is less structured than a DataFrame.
*   **Redundant Import:** `numpy` is imported but not strictly necessary for the core logic implemented in the generated code (e.g., `np.where` implicitly used if not defined by pandas `astype(int)`). The ground truth uses it for `np.abs` in the feature importance calculation, making its import more justified.

Overall, the code is highly functional and addresses almost all requirements effectively. The points for improvement are minor details in implementation and robustness.",o4_mini,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.linear_model import LogisticRegression

# 1) Load dataset
df = pd.read_csv('heart.csv')

# 2) Box plot of Oldpeak by HeartDisease
plt.figure(figsize=(8,6))
sns.boxplot(x='HeartDisease', y='Oldpeak', data=df)
plt.xlabel('HeartDisease')
plt.ylabel('Oldpeak')
plt.title('Oldpeak Distribution by HeartDisease')
plt.show()

# 3) Logistic regression with Oldpeak
X = df[['Oldpeak']]
y = df[...",7,"The generated code correctly addresses most of the requirements.

**Strong points:**
*   Correctly loads the dataset.
*   Successfully generates the box plot.
*   Performs logistic regression and prints the coefficient and odds ratio.
*   Correctly discretizes `Oldpeak` into three quantile bins using `pd.qcut`.
*   Correctly calculates and prints the proportion of `HeartDisease=1` within each bin.

**Areas for improvement to achieve a higher score:**
1.  **Interpretation of Odds Ratio:** The prompt specifically asked to ""interpret its meaning in the context of odds ratios."" The generated code prints the odds ratio but does not provide any textual interpretation of what that number signifies in terms of the likelihood of heart disease with increasing Oldpeak. This was a direct request in the prompt.
2.  **Robustness (Error Handling/Data Quality Checks):** The ground truth code includes `try-except` for file loading and checks for missing values, making it more robust. While not explicitly asked for, best practices in data science often involve such checks.
3.  **Visualization Enhancements:** The box plot generated is functional, but lacks informative labels (`xticks` labels for HeartDisease 0/1 are missing), a common best practice for plots.
4.  **Completeness of Output:** The ground truth provides more verbose output to confirm each step (e.g., ""Dataset loaded successfully,"" ""Box plot displayed,"" etc.) and also visualizes the proportion of HeartDisease by Oldpeak bin, which enhances clarity and understanding, even though the prompt only explicitly asked to ""display"" the proportion, not necessarily visualize it.
5.  **Documentation/Comments:** While the prompt requested only code, more descriptive comments would improve readability, especially for a benchmark task.

The primary reason for not giving a higher score is the missing interpretation of the odds ratio, which was a clear requirement in step 3 of the task description.",o4_mini,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

# 1) Load data
df = pd.read_csv('spotify_2023.csv')

# 2) Convert streams to numeric, drop NaNs
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams'])

# 3) Create binary target
thresho...",9,"The generated code correctly addresses almost all aspects of the task.

**Strong points:**
- Correctly loads data.
- Converts `streams` to numeric and handles missing values.
- Accurately creates the `is_popular` target variable using the 75th percentile.
- Correctly selects the specified numerical features.
- Implements one-hot encoding for `key` and `mode` and combines them with numerical features.
- Correctly splits data into training and testing sets with the specified `random_state`.
- Trains a `LogisticRegression` model with `max_iter=1000`.
- Calculates and prints F1-score (macro), ROC AUC score, and the confusion matrix.
- Imports are proper and necessary for the task.

**Areas for minor improvement:**
- The `train_test_split` does not use `stratify=y`. While not explicitly requested, stratifying by the target variable (`is_popular`) is a best practice, especially with potentially imbalanced datasets, to ensure that both training and testing sets have a similar distribution of the target class. The ground truth code includes this for better generalization.
- The `try-except` block for file loading is missing, which makes the code less robust to missing files. However, the task statement does not explicitly require error handling beyond the `streams` column.
- The `random_state` is set for `train_test_split` but not for `LogisticRegression`. While `LogisticRegression` defaults to `None` for `random_state` for solvers that use random numbers, setting it for both is good practice for full reproducibility. The ground truth code includes `random_state` in the model definition.",o4_mini,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load data
df = pd.read_csv('spotify_2023.csv')

# 2) Select audio features and drop rows with NaNs
features = [
    'danceability_%', 'energy_%', 'valence_%', 'acousticness_%',
    'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm'
]
df_selected = df[features].dropna().copy()

# 3) Standardize feature...",9,"The generated code correctly performs all steps of the task: loading data, selecting features, handling NaNs, standardizing, applying K-Means, performing PCA, plotting, and calculating cluster means.

**Strong points:**
*   All required libraries are imported correctly.
*   The data loading, feature selection, and NaN handling are concise and effective.
*   Standardization, K-Means clustering (with correct parameters), and PCA (with correct parameters) are implemented as specified.
*   The PCA plot is correctly generated with clusters colored.
*   The calculation and display of mean original audio features per cluster are correct.

**Areas for minor improvement:**
*   **Missing `try-except` for file loading:** The ground truth includes a `try-except FileNotFoundError` block for `pd.read_csv`, which is good practice for robustness. The generated code assumes the file will always be present. While not strictly part of the core data science logic, it improves user experience for a practical script.
*   **`df_selected.dropna().copy()` vs. `df[features].copy()`, then `dropna()`:** The generated code applies `dropna()` immediately after selecting features: `df[features].dropna().copy()`. This is functionally correct and concise. The ground truth separates it: `df[audio_features].copy()` then `df_features.dropna(inplace=True)`. The ground truth's approach provides intermediate print statements about dropped rows, which is helpful for debugging or understanding data cleanliness, but not strictly required by the prompt. Both are valid.
*   **Clarity on original features for analysis**: The ground truth explicitly creates `df_original_for_analysis = df_features.copy()` *before* scaling, ensuring that the original unstandardized values are preserved for the final mean calculation. The generated code relies on `df_selected.dropna().copy()` which also correctly preserves the unstandardized data. The distinction is subtle but the ground truth's approach might be slightly clearer in intent for complex pipelines.
*   **Plotting `cmap`**: The generated code uses `'tab10'`, which is a good categorical colormap, but `'viridis'` (used in ground truth) is also a suitable choice. This is a stylistic preference and doesn't impact correctness.

Overall, the generated code is very good and directly addresses all the requirements of the prompt accurately. The minor points are mostly about robustness or slightly clearer structuring, not functional correctness.",o4_mini,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. Load data
df = pd.read_csv('spotify_2023.csv')

# 2. Convert 'streams' to numeric, coerce errors to NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# 3. Drop rows with NaN 'streams' and filter by streams > 10,000,000 and release year between 2010 and 2023
df = df.dropna(subset=['streams'])
df_filtered = df[(df['streams'] > 10_000_000) & (df['released_year'].between(2010, 2023))]

# 4. Group the filtered data b...",7,"The generated code correctly performs all the data loading, cleaning, filtering, grouping, and aggregation steps as required by the task. It successfully converts 'streams' to numeric, filters by streams and year, and calculates the average audio features. The main area for improvement is how the plots are displayed. The requirement was to ""Display all three plots,"" which usually implies them being shown together (e.g., in subplots) rather than as three separate `plt.figure()` calls, each of which creates a new, independent window if run in a desktop environment, or three separate plots embedded one after another if run in a notebook. While technically displaying all three, it's not the most user-friendly or standard way to present multiple related plots. The `plt.tight_layout()` is called *after* all `plt.figure()` calls, which means it will only apply to the *last* created figure. It should be applied to the specific figure (or subplots) it's intended for, typically before `plt.show()`. The `plt.show()` is also only called once at the end, but since new figures were created, each figure would ideally have its own `plt.show()` if they are truly meant to be separate, or they should be combined into one figure with subplots. The lack of `try-except` for file loading is also a minor robustness issue.",o4_mini,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder

df = pd.read_csv('youtube_trendings.csv')

for col in ['Likes', 'Dislikes', 'Video views']:
    df[col] = df[col].astype(str).str.replace(',', '', regex=False)
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

df['published'] = pd.to_numeric(df[...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being fully correct and robust.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly handles comma removal, conversion to numeric, and NaN filling with 0 for 'Likes', 'Dislikes', and 'Video views'.
*   Correctly fills missing 'Category' values with 'Unknown' and applies one-hot encoding.
*   Correctly compiles `X` and `y`.
*   Correctly splits data into training/testing sets with `random_state=42`.
*   Correctly trains a Linear Regression model.
*   Correctly calculates and prints R2 and MAE.
*   Uses proper imports.

**Weaknesses and Areas for Improvement:**

1.  **Handling of 'published' column:** The prompt specifically asks to treat the `published` column (`year`) as a numeric feature. The generated code attempts `df['published'] = pd.to_numeric(df['published'], errors='coerce')`. However, the 'published' column in the `youtube_trendings.csv` dataset typically contains a full datetime string (e.g., '2020-08-11 12:00:00 UTC'). Attempting to convert the entire string directly to numeric with `pd.to_numeric` will result in NaNs for all valid entries, effectively making the 'published' feature useless, as the prompt specifies to use the *year*. The ground truth extracts the year correctly. This is a critical logical error regarding a specified feature.
2.  **`OneHotEncoder` usage:** While `OneHotEncoder` is used, the `sparse=False` argument is deprecated and should be replaced by `sparse_output=False`. Although `handle_unknown='ignore'` is good practice, it's not strictly necessary for this specific problem if 'Unknown' is the only new category. The ground truth's use of `pd.get_dummies` is simpler and more direct for this scenario as it directly integrates into a DataFrame without extra steps for column naming and indexing.
3.  **No error handling for `FileNotFoundError`:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.
4.  **Implicit type casting:** While `pd.to_numeric` with `fillna(0)` works for the cleaning step, the ground truth explicitly converts the cleaned 'Likes', 'Dislikes', and 'Video views' to `int` types, which is more robust and clearer. The generated code leaves them as float if any non-numeric values were coerced.
5.  **Lack of explicit column type warnings/checks:** The ground truth adds warnings if expected columns (like 'Category', 'published') are not found. While not explicitly asked for, it makes the code more robust. This is a minor point but contributes to completeness.

The most significant issue is the incorrect handling of the `published` column, which directly impacts the features `X`. This is a non-trivial bug that would lead to incorrect model behavior for that specific feature.",o4_mini,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv(""youtube_trendings.csv"")

# 2) Clean 'Video views' column
df['Video views'] = (
    pd.to_numeric(
        df['Video views'].str.replace(',', '', regex=False),
        errors='coerce'
    )
    .fillna(0)
)

# 3) Ensure 'published' column is numeric/integer
df['published'] = (
    pd.to_numeric(df['published'], errors='coerce')
    .fillna(0)
    .astype(int)
)

# 4) Group by published year
grou...",7,"The generated code addresses most of the requirements but has a critical flaw in handling the 'published' column.

**Strengths:**
- Correctly loads the dataset.
- Correctly cleans the 'Video views' column by removing commas, converting to numeric, and filling NaNs with 0.
- Correctly groups the data by year.
- Calculates both the average video views and the total number of unique videos per year as required.
- Creates two subplots with appropriate titles and axis labels.
- Uses `pd.to_numeric` with `errors='coerce'` and `fillna(0)` for robust data type conversion.

**Areas for Improvement / Issues:**
- **Critical: Handling of 'published' column:** The prompt states ""Ensure the `published` column (year) is treated as a numeric or integer type."" The ground truth code correctly interprets this as needing to extract the year from a date string (e.g., 'publishedAt' or similar, which is common in such datasets). The generated code assumes `df['published']` *is* the year itself and directly tries to convert it to numeric. If 'published' contains full date strings (e.g., ""YYYY-MM-DD HH:MM:SS""), this conversion will fail or produce incorrect results (e.g., all zeros if `errors='coerce'` handles entire non-numeric strings as NaNs). This is a significant functional error if the 'published' column is not already in a simple year format.
- **Robustness:** The generated code does not include any error handling for file loading (`FileNotFoundError`), which is good practice for real-world scenarios.
- **Clarity on `published` column:** While the prompt used ""published year"", typical datasets have a more detailed 'publishedAt' or similar column from which the year needs to be extracted. The generated code's direct conversion of `df['published']` is less robust if the column isn't already just the year. The ground truth correctly assumes a datetime column.
- **Minor: Plot aesthetics:** The plots are functional, but lack some of the polish of the ground truth (e.g., grids, specific colors, handling x-axis ticks for years).",o4_mini,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

df = pd.read_csv('youtube_trendings.csv', parse_dates=['published'])
df['title_word_count'] = df['Video'].str.split().str.len()
df['views_clean'] = pd.to_numeric(df['Video views'].str.replace(',', '', regex=False), errors='coerce').fillna(0)
print(df['title_word_count'].corr(df['views_clean']))

df_23_24 = ...",8,"The generated code correctly addresses most aspects of the task.

**Strong Points:**
- **Correct Data Loading and Initial Setup:** Correctly loads the CSV and downloads NLTK stopwords.
- **Efficient Word Count Calculation:** Uses `str.split().str.len()` which is a concise and efficient way to calculate word counts.
- **Correct View Cleaning:** Accurately removes commas, converts to numeric, and fills NaNs for `Video views`.
- **Correct Correlation Calculation:** Calculates and prints the correlation as requested.
- **Correct Filtering by Year:** Accurately filters the DataFrame for 2023 and 2024.
- **Correct Word Processing for Common Words:** Lowers case, removes stopwords and punctuation, and uses `Counter` to find the most common words.

**Areas for Improvement:**
- **Implicit Error Handling:** The ground truth includes `try-except` blocks for file loading and `nltk.download` which makes it more robust. The generated code implicitly assumes the file exists and stopwords are downloaded.
- **Punctuation Removal in `re.findall`:** While `\b\w+\b` will extract words, it doesn't explicitly *remove* punctuation from within words or if punctuation is attached. For example, ""title."" would extract ""title"". The ground truth uses `re.sub(r'[^\w\s]', '', title)` which is generally more robust for removing all punctuation. The `isalpha()` check in the ground truth also ensures purely alphabetic words, removing potential numbers that `\w` might include.
- **Output Format:** The output format for correlation and the most common words does not exactly match the detailed requirements (e.g., specific print statements for correlation, formatted list for common words). While the data is there, the presentation is less user-friendly. `print(df['title_word_count'].corr(df['views_clean']))` just prints a float, without context. Similarly, printing the raw `Counter` output is less readable than the formatted loop in the ground truth.
- **Clarity of Variable Names:** While `df_23_24` is clear, the new column for cleaned views is named `views_clean` which is reasonable, but `Video Title` is used for the word count in the ground truth but then later just `Video`. Sticking to the original column names as much as possible for clarity, or explicitly creating new columns with very clear names is good practice. The original `Video views` column is effectively overwritten with the cleaned data in the ground truth; the generated code creates a new column `views_clean`. Both are valid, but consistency in an application development context would be important.

Overall, the code is highly functional and correctly implements the core logic. Improvements mainly revolve around robustness, explicit formatting of output, and slightly more comprehensive text cleaning in the word processing step.",o4_mini,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score

# 1) Load data
df = pd.read_csv('mobiles_2025.csv')

# 2) Clean `Launched Price (USA)`
df['Launched Price (USA)'] = (
    df['Launched Price (USA)']
    .str.replace('USD ', '', regex=False)
    .str.replace(',', '', regex=False)
)
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'], ...",9,"The generated code is largely correct and complete, successfully addressing almost all requirements of the task.

**Strong points:**
*   **Correct Data Loading and Target Creation:** Handles cleaning the 'Launched Price (USA)' column, converting it to numeric, and correctly creating the `is_expensive_usa` binary target, including the `NaN` handling for the target.
*   **Effective Feature Cleaning:** All specified numerical feature cleaning (Mobile Weight, RAM, Battery Capacity, Screen Size) is done correctly.
*   **Good Camera MP Extraction:** The regex for `Front Camera` and `Back Camera` extraction is well-implemented and correctly handles `NaN` values by filling with 0.
*   **Proper One-Hot Encoding:** Categorical features `Company Name` and `Processor` are correctly one-hot encoded using `pd.get_dummies`.
*   **Correct Feature and Target Definition:** `X` and `y` are defined using all required features and the target.
*   **NaN Handling in Features/Target:** Drops rows where `X` has NaNs, which implicitly handles `y` as well because `mask` applied to `X` is then applied to `y`. This is functionally correct.
*   **Correct Model Training and Evaluation:** Uses `train_test_split` with `random_state`, trains `RandomForestClassifier` with specified `n_estimators` and `random_state`, and evaluates using F1-score (macro) and ROC AUC score.
*   **Proper Imports:** All necessary libraries are imported.
*   **Concise Code:** The code is quite compact and readable.

**Areas for minor improvement:**
*   **Target NaN Handling:** The `is_expensive_usa` NaN creation using `np.where` is `(df['Launched Price (USA)'] > 1000).astype(int)` inside `np.where` means that if `Launched Price (USA)` is `NaN`, `(df['Launched Price (USA)'] > 1000)` will directly result in `False` rather than `NaN` for `df[nan > 1000]` operation. To be perfectly aligned with the ground truth's nuanced `NaN` handling for the target (where if the original price was unparseable, the target also becomes NaN), the ground truth's lambda function `(0 if pd.notna(x) else np.nan)` explicitly sets 0 if `x` is not NaN but not > 1000, and NaN if `x` is NaN. The generated code's `np.where(df['Launched Price (USA)'].isna(), np.nan, (df['Launched Price (USA)'] > 1000).astype(int))` simplifies this to: if `Launched Price (USA)` is NaN, `is_expensive_usa` is NaN. Otherwise, it's 0 or 1. This is a very subtle difference. The ground truth might be slightly more robust if the intention was that *only* the original parseable values lead to a 0 or 1. However, for most practical purposes, the generated code's implementation is fine and achieves the same outcome of dropping rows with `NaN` target.
*   **Missing `random_state` in `train_test_split` `stratify` parameter:** The task implicitly expects stratified splitting (common for classification, and done in ground truth) but doesn't explicitly state `stratify=y`. The generated code is missing `stratify=y`, which could lead to minor differences in train/test set class distribution if the dataset is imbalanced. This isn't a critical error, but it's a best practice in classification.

Overall, the generated code is highly functional and correctly implements the core logic. The minor improvements focus on robustness and alignment with best practices, not on fundamental errors.",o4_mini,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"import pandas as pd
import numpy as np
import re

df = pd.read_csv('mobiles_2025.csv')

def clean_price(price):
    if pd.isna(price):
        return np.nan
    s = str(price)
    s = re.sub(r'(PKR|INR|CNY|USD|AED)', '', s)
    s = s.replace(',', '').strip()
    try:
        return float(s)
    except:
        return np.nan

price_cols = [
    'Launched Price (Pakistan)',
    'Launched Price (India)',
    'Launched Price (China)',
    'Launched Price (USA)',
    'Launched Price (Dubai)'
]

for c...",8,"The generated code correctly addresses most of the requirements.

**Strong Points:**
- **Correct Data Loading:** Successfully loads the `mobiles_2025.csv` dataset.
- **Effective Price Cleaning:** The `clean_price` function correctly removes specified currency symbols using `re.sub` and commas, then converts the string to a float, handling `NaN` values and conversion errors. This is a robust approach.
- **Correct Column Application:** Applies the cleaning function to all five specified price columns.
- **Average Global Price Calculation:** Correctly calculates the `Average Global Price` by taking the mean across the specified price columns.
- **Top 5 Identification:** Uses `nlargest` which is an efficient and correct way to identify the top 5 models by `Average Global Price`.
- **Correct Display:** Displays the `Model Name` and `Average Global Price` for the top 5.

**Areas for Improvement:**
- **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this, making it less robust in unexpected file system scenarios.
- **`skipna` in `mean`:** The generated code calculates `df[price_cols].mean(axis=1)`. By default, `pd.DataFrame.mean()` with `skipna=True` (which is the default behavior), correctly ignores `NaN` values when calculating the mean. This implicitly meets the requirement ""This average should only consider countries for which price data is available (not NaN)"". However, explicitly mentioning `skipna=True` (as in the ground truth) would make the intention clearer.
- **Missing Column Warnings:** The ground truth includes a `Warning` message if a `price_column` is not found. While not explicitly asked for, it's a good practice for robustness.
- **Docstrings and Comments:** The ground truth includes a detailed docstring for the `clean_price` function and comments explaining each step, which improves readability and maintainability.
- **Robustness in `clean_price`:** The ground truth `clean_price` checks `if pd.isna(price_str): return np.nan` at the very beginning and also `price_str = str(price_str)` to ensure string operations. While the generated code's `if pd.isna(price): return np.nan` and `s = str(price)` achieve the same, the ground truth's order and `try-except ValueError` vs bare `except` are slightly more precise.

Overall, the generated code is highly functional and correctly solves the core problem. The improvements are mainly about robustness, explicit intent, and code readability/best practices rather than core functionality.",o4_mini,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# 2) Clean columns
df['RAM_clean'] = pd.to_numeric(df['RAM'].str.replace('GB', '', regex=False), errors='coerce')
df['Battery_clean'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', '', regex=False), errors='coerce')
df['Price_clean'] = pd.to_numeric(
    df['Launched Price (USA)']
      .str.replace('USD ', '', regex=False)
      .str.replace(',', '', regex=False)...",8,"The generated code correctly addresses most of the requirements. It loads the dataset, cleans the specified columns (`RAM`, `Battery Capacity`, `Launched Price (USA)`) by removing units and handling conversion errors to `NaN`, and then correctly drops rows with `NaN` in these cleaned columns. It filters the data for 'Apple' products and then iteratively generates box plots for each `Launched Year` for `RAM`, `Battery Capacity`, and `Price`. The plots have appropriate titles and y-axis labels.

Areas for improvement:
1.  **Robustness in Cleaning:** The ground truth handled `Launched Year` cleaning and conversion to integer more explicitly and robustly (handling potential NaNs there before dropping). While the generated code might implicitly handle some `Launched Year` issues if they cause problems during `groupby`, explicit handling is better.
2.  **Slightly Less Explicit NaN Handling:** The generated code performs the `dropna` on the *newly created* cleaned columns. While this is valid, the ground truth explicitly uses `df.dropna(subset=columns_to_check_nan, inplace=True)` on the original dataframe, which is a slightly more common pattern. Both achieve the same result here.
3.  **Use of `seaborn`:** The ground truth used `seaborn` for plotting, which was implicitly suggested by the prompt (or at least `matplotlib.pyplot` was, and `seaborn` often produces aesthetically better plots with less code). While basic `matplotlib.pyplot` `boxplot` is used, `seaborn.boxplot` often results in cleaner visuals.
4.  **Error Handling for File Loading:** The ground truth included a `try-except` block for `FileNotFoundError`, which is good practice for robust code. The generated code assumes the file exists.
5.  **Plot Aesthetics:** While the basic functionality is there, the ground truth uses `plt.style.use('seaborn-v0_8-darkgrid')` and explicitly sets `width` for the boxplots, making them more visually appealing and professional. The generated code's plots are functional but could be improved aesthetically.",o4_mini,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import numpy as np
import pandas as pd
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, FunctionTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.compose import ColumnTransformer
from sklearn.model_selection impo...",6,"The generated code attempts to address most requirements but falls short in comprehensiveness and adherence to specified advanced techniques in several key areas.

**Strong points:**
*   **Pipeline usage:** Correctly uses `Pipeline` and `ColumnTransformer` for preprocessing and model building.
*   **Multiple Base Models:** Implements Random Forest, Gradient Boosting (XGBoost), SVM, and MLP, with hyperparameter tuning.
*   **Ensemble Methods:** Includes `VotingClassifier` and `StackingClassifier`.
*   **Evaluation:** Performs cross-validation and attempts learning/validation curves.
*   **Model Interpretability:** Uses SHAP (though `KernelExplainer` might be computationally slow for large datasets and could be replaced by `TreeExplainer` for tree models).
*   **Statistical Tests:** Includes `ttest_rel` for comparing models.

**Areas for Improvement / Missing Aspects:**

1.  **Data Preprocessing & Feature Engineering:**
    *   **Advanced Feature Engineering (Name):** The ground truth provides a `FeatureEngineerTransformer` which extracts 'Title', 'Is_Married', 'Name_Length', 'FamilySize', 'IsAlone', 'Fare_Per_Person', and processes 'Cabin' into 'Deck'. The generated code's `extract_name_features` is much simpler, only extracting `title`, `name_len`, and `name_word_count`. It doesn't use `Cabin` or `Ticket` (drops them early) and misses `FamilySize`, `IsAlone`, `Fare_Per_Person`.
    *   **Text Mining (Names) - Specificity:** The prompt asked for ""text mining from names"" which implies more advanced techniques than just extracting title length/word count. While `TfidfVectorizer` is included in the name pipeline, it's applied after `FunctionTransformer(lambda df: TfidfVectorizer(min_df=5).fit_transform(df['Name']).toarray())` (and later PCA), but `df['Name']` is likely just the single 'Name' column which would not be formatted correctly for TF-IDF directly via `FunctionTransformer` within `ColumnTransformer`. `df` inside `extract_name_features` would be a dataframe, but `df` inside the `lambda` for TF-IDF would be the Series representing the 'Name' column for that step, requiring `.loc` or similar to access its values, and even then, `TfidfVectorizer` needs raw text rather than a single `df` (it should operate on the Name column's content to get new features). The implementation for `tfidf` within the `name_pipeline` is likely incorrect as `FunctionTransformer` expects input that it then transforms, not to initialize `TfidfVectorizer` within it as a one-off. It should be outside, as part of the overall pipeline steps, or the data passed to it needs to be correct.
    *   **Multiple Imputation Strategies:** The ground truth used `IterativeImputer` for numerical features and `SimpleImputer` for categorical. The generated code also does this, which is good. However, the ground truth explicitly states ""multiple imputation strategies"" referring to different approaches (e.g., MICE/IterativeImputer vs. simpler methods) for *different columns*, which is covered, but also potentially implies comparing the impact of *different* complex imputation methods, which isn't done. The prompt could be interpreted as comparing the impact of `IterativeImputer` vs. `SimpleImputer(mean/median)` for numerical fields, for example.

2.  **Feature Selection:**
    *   **Comparison of Impact:** The generated code extracts the scores for different feature selection methods (`fs_results`), but it doesn't explicitly compare their impact or evaluate models *trained with* selected features against models trained without, as explicitly asked in the prompt (""compare their impact""). The ground truth rebuilds/retrains a model (Random Forest in this case) on the reduced feature sets and compares its performance directly. The generated code calculates `mean(cvres['test_score'])` but doesn't store the actual selected features or visibly compare the scores to the full-feature models.
    *   **Method Choice:** Using `chi2` with `SelectKBest` is appropriate for categorical features but less so directly on the (potentially) numerical features produced from `PolynomialFeatures` without proper preparation. The ground truth used `f_classif` which is suitable for numerical features with a categorical target.
    *   **RFE:** `RFE` is used with a `RandomForestClassifier` but `n_features_to_select` is hardcoded to 10. The prompt ""using multiple techniques and compare their impact"" suggests exploring the optimal number of features or seeing the effect of different feature counts.

3.  **Hyperparameter Tuning:**
    *   **Completeness:** While `GridSearchCV` and `RandomizedSearchCV` are used, the search spaces are relatively small, especially for `MLPClassifier` and `GradientBoosting` (XGBoost). The ground truth has more comprehensive parameter ranges and `n_iter` for `RandomizedSearchCV`.

4.  **Model Evaluation and Interpretability:**
    *   **Learning/Validation Curves:** While plotted, the plots are generic and lack proper labels, titles, and legends, making them difficult to interpret compared to the ground truth's detailed plots.
    *   **SHAP/LIME:** The prompt specifically required SHAP *or* LIME. The generated code implements SHAP (`KernelExplainer`), but `KernelExplainer` is generally very slow for many samples. For tree-based models (like the XGBoost or RandomForest used in the stacking ensemble), `shap.TreeExplainer` is much more efficient and preferred. Moreover, the generated code doesn't produce the actual SHAP plots, just calls `shap.summary_plot` without `plt.show()`. Also, the `KernelExplainer` for the entire `stack` model might be problematic as `predict_proba` might not be directly explainable by `KernelExplainer` without careful data preprocessing. LIME is completely missing.
    *   **Performance Metrics:** While `cross_validate` used multiple metrics, the final `print` statement just outputs `metrics` which is a dictionary. A more structured or visual comparison of all model performance metrics would be better, similar to the ground truth's approach of logging and comparing test set performance.

5.  **Code Structure and Readability:**
    *   **Custom Transformer:** The ground truth implemented a detailed `FeatureEngineerTransformer` as a custom scikit-learn compatible transformer, which is good practice. The generated code uses `FunctionTransformer` with lambda functions, which can become less readable for complex operations.
    *   **Imports:** The generated code throws `warnings.filterwarnings('ignore')` at the beginning, but no actual `warnings` import.
    *   **Missing `lime` import/usage** - LIME is not imported or used, although it was requested.

Overall, the generated code has a good skeleton but needs significant work on the details of preprocessing, feature engineering complexity, thorough evaluation (especially feature selection comparison), and interpretability tool implementation.",o4_mini,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

df = pd.read_csv('imdb_top_250.csv')
X = df[['Year', 'Runtime']]
y = df['Rating']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y...",7,"The generated code correctly performs all the core tasks: loading data, defining features and target, splitting data, training a Linear Regression model, and evaluating it with R2 and MAE. The imports are correct and the structure is logical.

However, the code is missing crucial data cleaning and preprocessing steps essential for real-world datasets, especially the 'Runtime' column which is often stored as strings (e.g., 'X min'). The ground truth includes robust error handling for file loading and missing columns, and conversion of 'Runtime' to numeric, which the generated code lacks. Without this, the model might fail or produce incorrect results if the 'Runtime' column is not purely numeric or if the file is missing/corrupted.

**Areas for improvement:**
1.  **Data Cleaning:** Implement steps to convert the 'Runtime' column to a numeric format (e.g., remove ' min' and convert to int/float).
2.  **Error Handling:** Add `try-except` blocks for file loading and checks for missing columns.
3.  **Data Type Conversion:** Explicitly convert `Year` and `Rating` to numeric types, handling potential non-numeric values.
4.  **Handling Missing Values:** Add `dropna()` or other imputation strategies after data cleaning.
5.  **Output Clarity:** The ground truth provides more descriptive print statements, which is good practice for making the code's execution flow clear.",o4_mini,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 2) Select features
X = df[['Rating', 'Runtime']]

# 3) Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4) Apply K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
labels = kmeans.fit_predict(X_scaled)
df['cluster'] = labels
...",6,"The generated code correctly implements the core K-Means clustering steps: data loading, feature selection, standardization, K-Means application, and visualization. It also calculates and prints the mean values per cluster.

However, it critically misses necessary data cleaning for the 'Runtime' column, which is usually stored as a string like '142 min' in this dataset. Without this cleaning, the `pd.to_numeric` conversion of 'Runtime' and `df.dropna` will likely fail or drop a significant portion of the data, leading to an incorrect or incomplete analysis. The ground truth code explicitly handles this common scenario.

Additionally, the plot lacks a color bar and a more descriptive title and axis labels, which would improve its interpretability. The method for printing the cluster means is also a bit verbose compared to a simple `groupby().mean()`.

To improve, the code needs robust data cleaning for the 'Runtime' column. Adding a color bar to the plot and using `df.groupby().mean()` for cluster statistics would also enhance completeness and code quality.",o4_mini,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter

# Ensure NLTK resources are available
nltk.download('punkt')
nltk.download('stopwords')

# 1) Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 2) Create title_length feature
df['title_length'] = df['Title'].str.len()

# 3) Calculate and print Pearson correlation between title_length and Rating
corr = df['title_length'].corr(df[...",7,"The generated code addresses most of the requirements correctly and concisely.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly calculates `title_length`.
*   Correctly calculates and prints the Pearson correlation.
*   The preprocessing steps (lowercase, remove punctuation, tokenize) are chained efficiently using method chaining.
*   Correctly removes stopwords.
*   Correctly finds and prints the 10 most common words.
*   Handles NLTK downloads (though not inside a try-except, which is safer for production and less noisy during repeated runs if already downloaded).

**Areas for improvement to achieve a higher score:**
*   **Correlation Calculation Robustness:** It directly calculates the correlation without `pd.to_numeric` on the 'Rating' column or `dropna` on relevant columns. If 'Rating' contains non-numeric values or NaNs, this could raise an error or produce an incorrect correlation. The prompt implies numerical calculation, so validating the 'Rating' column type and handling potential NaNs is crucial for robustness. The ground truth correctly addresses this.
*   **Punctuation Removal Regex:** While `str.replace(r'[^0-9a-zA-Z\s]', '', regex=True)` is generally good for removing non-alphanumeric/non-whitespace characters, it might remove legitimate (though less common) title characters if not strictly adhering to ""remove characters that are not alphanumeric or whitespace"". The ground truth uses `re` module directly and an `apply` function, which allows for more control and handles potential `NaN` values in the 'Title' column before processing.
*   **Output Formatting:** The output for the 10 most common words is `word freq` whereas the ground truth specifies `print(f""'word': {count}"")`, which is slightly more aligned with clarity for presentation.
*   **NLTK Downloads:** While `nltk.download` is present, wrapping it in `try-except LookupError` like the ground truth prevents repeated download attempts or errors if resources aren't found. This is a best practice for `nltk` usage.",o4_mini,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Define features and target
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
X = df[features]
y = df['species']
...",7,"The generated code addresses most of the requirements. It successfully loads the data, defines features and target, imputes numerical and categorical features, performs one-hot encoding and scaling, trains a RandomForestClassifier, and evaluates it using cross-validation.

However, there's a significant issue regarding the handling of missing values in the target `y` and how the index is managed during the imputation process, especially when dropping rows.

1.  **Handling missing target values:** The prompt doesn't explicitly mention handling missing values in `y`, but good practice dictates that rows with missing target values cannot be used for supervised learning. The ground truth correctly addresses this by dropping such rows early. The generated code does not explicitly handle `y`'s missing values. If there were `NaN` in `species`, it would cause an error during `LabelEncoder().fit_transform(y)`.

2.  **Index management during imputation and row dropping:**
    *   When imputing numerical features using `SimpleImputer`, the `index` is passed from the original `X`. Same for `X_cat`. This is good.
    *   However, when rows are dropped due to `island` being `NaN` (`mask = X_cat['island'].isna()`), `X_num`, `X_cat`, and `y` are filtered. While this correctly aligns them, it creates a discrepancy with the original `X` pandas Dataframe, potentially leading to issues if subsequent steps relied on original indexing from `X`. More importantly, the `X_num.index` and `X_cat.index` will now have gaps (non-contiguous indices).
    *   When `pd.concat` is used to combine `X_num_scaled` and `X_cat_ohe`, if their indices are not perfectly aligned (e.g., if one was dropped rows based on a different condition or a different number of rows), `pd.concat` might introduce `NaN` values or misalign data if not handled carefully. In this specific case, since both `X_num_scaled` and `X_cat_ohe` are derived from the same filtered `X_num` and `X_cat` (which had their indices aligned through `mask`), they should align correctly. But it's a potential pitfall of this index-preserving approach for concatenation.

The ground truth's approach of always resetting the index after dropping rows (`reset_index(drop=True, inplace=True)`) is generally safer and ensures contiguous indexing, preventing potential future alignment issues, especially for `concat` operations or direct indexing.

Small improvements:
*   The `handle_unknown='ignore'` in `OneHotEncoder` is good practice but not strictly necessary for this dataset if all categories are present during fit.
*   The use of `SimpleImputer` is a valid and robust way to handle imputation compared to manual `fillna`.

Overall, a good attempt but the index management and lack of explicit target NA handling slightly detract from a perfect score.",o4_mini,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Select the features
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']

# 3) Drop rows with missing values in these features
df_clean = df.dropna(subset=features)

# 4) Standardize these features
...",10,"The generated code successfully addresses all requirements of the task. It correctly loads the dataset, selects the specified features, handles missing values (NA), standardizes the data, applies K-Means clustering with the correct parameters (K=3, n_init='auto', random_state=42), performs PCA to 2 components, and generates both required scatter plots with appropriate coloring (K-Means clusters and original species labels) and titles. The code is well-structured and uses necessary libraries. It successfully reproduces the core functionality of the ground truth code. One minor improvement could be to include error handling for `FileNotFoundError` as in the ground truth, but this wasn't explicitly requested as a requirement for the task itself. The `reset_index(drop=True)` for `original_species_labels` in the ground truth is a slightly more robust way to ensure alignment when `.values` is not used, but `df_clean['species'].values` achieves the same result here effectively.",o4_mini,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# 1) Load the dataset
df = pd.read_csv(""penguins.csv"")

# 2) Impute missing values
for col in [""culmen_length_mm"", ""culmen_depth_mm"", ""flipper_length_mm"", ""body_mass_g""]:
    df[col].fillna(df[col].mean(), inplace=True)
df[""sex""].fillna(df[""sex""].mode()[0], inplace=True)
df.dropna(subset=[""island""], in...",7,"The generated code correctly addresses most of the task requirements.

**Strong Points:**
*   Correctly loads the dataset.
*   Handles numerical imputation with the mean and 'sex' imputation with the mode.
*   Correctly drops rows where 'island' is NA.
*   One-hot encodes 'island' and 'sex' correctly.
*   Concatenates features `X` and label encodes `y`.
*   Trains the `GradientBoostingClassifier` with the specified parameters.
*   Extracts and prints feature importances appropriately.
*   Generates box plots for `flipper_length_mm` per `species` grouped by `sex`, with titles and labels.

**Areas for Improvement (Why it lost points):**
*   **Order of Imputation:** The ground truth first drops `island` NAs, then imputes others. This is important because rows with `island` NAs would be dropped, which could affect the mean/mode calculation for other columns if those dropped rows also had missing values in other columns. The generated code imputes numerical and `sex` first, then drops `island` NAs. While the difference might be negligible for this specific dataset, it's a logical inconsistency in the imputation order compared to the ground truth's more careful approach.
*   **Feature Importance Output Format:** While correctly printing feature names and importances, the output format is a simple loop, not a sorted DataFrame like the ground truth, which is generally more readable and useful for analysis. The task asked to ""print the feature importances... associating them with their original feature names,"" and while technically met, the ground truth's DataFrame output is a higher quality presentation that is often expected.
*   **Single Plot for Box Plots:** The task asked to ""For each `species`, create box plots to visualize the distribution of `flipper_length_mm`...grouped by the imputed `sex` column."" The generated code creates *separate* plots for each species, which is a valid interpretation, but the ground truth's single faceted plot using `hue='sex'` is often more efficient and provides a better comparative overview across species in one glance. The ground truth also uses a `palette` for better visualization.
*   **Plot Styling/Consistency:** The ground truth includes `sns.set_theme(style=""whitegrid"")` for consistent plot styling, which is good practice for data visualization tasks but missing in the generated code.",o4_mini,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer

# 1) Load the dataset
df = pd.read_csv(""penguins.csv"")

# 2) Impute numerical columns with KNNImputer
num_cols = [""culmen_length_mm"", ""culmen_depth_mm"", ""flipper_length_mm"", ""body_mass_g""]
imputer = KNNImputer(n_neighbors=3)
df[num_cols] = imputer.fit_transform(df[num_cols])

# 3) Impute 'sex' with mode
mode_sex = df[""sex""].mode()[0]
df[""sex""] = df[""sex""].fillna(mode_sex)

# 4) Print remaining NA counts per column
print(df.isna...",9,"The generated code correctly performs all the required steps: loading the dataset, applying KNN imputation to the specified numerical columns, imputing the 'sex' column with its mode, and verifying remaining NA values. The code is concise and directly addresses the task requirements.

**Strong points:**
*   Correctly loads the CSV file.
*   Correctly identifies and applies KNN imputation to the specified numerical columns with `n_neighbors=3`.
*   Correctly imputes the 'sex' column with its mode.
*   Successfully prints the remaining NA counts for verification.
*   Uses `inplace=True` or reassignment for updates, which is good practice.

**Areas for minor improvement (but not critical for this task):**
*   **Error Handling:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is robust. The generated code assumes the file exists. For a production-ready script, this would be a valuable addition.
*   **Verification Clarity:** While `df.isna().sum()` provides the necessary verification, the ground truth's additional print statements specifically for the imputed columns could be seen as slightly more explicit in showing that the task was met. However, the current output is sufficient.
*   **Comments/Prints:** Adding a few print statements to indicate which step is being performed (e.g., ""Applying KNN imputation..."") as seen in the ground truth can enhance readability and debugging, especially for more complex scripts.

Overall, it's a very good solution that fully satisfies the core requirements of the task. The minor points are more about robustness and user experience than correctness of the core logic.",o4_mini,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"import pandas as pd
from scipy.stats import ttest_ind

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Filter for Adelie and Gentoo species
df = df[df['species'].isin(['Adelie', 'Gentoo'])]

# 3) Drop rows with NA in culmen_length_mm
df = df.dropna(subset=['culmen_length_mm'])

# Separate samples
adelie = df[df['species'] == 'Adelie']['culmen_length_mm']
gentoo = df[df['species'] == 'Gentoo']['culmen_length_mm']

# 4) Perform independent two-sample t-test
t_stat, p_val = ttest_ind(a...",9,"The generated code successfully completes almost all aspects of the task.

**Strong points:**
*   **Correctness:** Loads data, filters correctly, handles NAs, and performs the t-test accurately. The interpretation of the p-value is also correct.
*   **Conciseness:** The code is direct and to the point, achieving the task with minimal lines.
*   **Readability:** Variable names are clear, and the comments align with the task requirements.

**Areas for improvement (to achieve a perfect 10):**
*   **Error Handling:** The code does not include any error handling for `FileNotFoundError` if `penguins.csv` is not present, which is present in the ground truth. This makes the code less robust for real-world scenarios.
*   **Function Encapsulation:** While not explicitly required for a short script, wrapping the logic in a function (as done in the ground truth) promotes reusability and modularity, which is generally good practice for data science tasks.
*   **`equal_var` parameter:** The generated code uses `equal_var=False` for the `ttest_ind`. While often a safer default for many t-tests (Welch's t-test), the prompt specified ""an independent two-sample t-test"", and commonly, the Student's t-test (which assumes equal variances, `equal_var=True`) is the initial go-to unless there's a strong reason to assume unequal variances. The ground truth uses `equal_var=True` and comments on it. For this specific problem (interpreting a simple t-test without an explicit instruction on variance equality), either is acceptable, but being explicit or matching common conventions can be beneficial. However, the task did not specify *which* variant of the t-test, so `equal_var=False` is not strictly incorrect, just a different assumption from the ground truth's default. Given the context, assuming `equal_var=True` might be implied by ""independent two-sample t-test"" without further qualification.
*   **Verbosity in Output:** The ground truth includes more verbose output (e.g., initial and final dataframe shapes, number of rows dropped, mean culmen lengths) which aids in debugging and understanding the data processing steps. The generated code is more minimal in its output. While the prompt asks for specific outputs (t-stat, p-value, conclusion), the additional print statements of the ground truth add value.",o4_mini,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Define features and target
features = ['culmen_len...",10,"The generated code flawlessly addresses all requirements of the task. It correctly loads the data, defines features and target, performs all specified preprocessing steps (median imputation for numerical, mode imputation for 'sex', one-hot encoding for 'island' and 'sex', and label encoding for 'species' target). The data splitting is done with correct stratification and `random_state`. The RandomForestClassifier is built and trained within a `Pipeline`, which is excellent for combining preprocessing and modeling. Finally, it correctly evaluates the model by printing the accuracy score and a comprehensive classification report with correct class names. The code is well-structured, uses standard libraries, and is highly readable.",o4_mini,LLM_judge_config
H49,hard,Statistical Analysis,,"import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

# 1) Correlation between PerformanceScore and YearsExperience
corr...",10,"The generated code correctly defines the DataFrame, accurately calculates both specified Pearson correlation coefficients, prints them as required, and precisely determines and prints which feature has a stronger absolute correlation with ""PerformanceScore"". The code is clean, well-structured, and directly addresses all parts of the task. The use of f-strings for printing and `abs()` for comparing absolute correlations demonstrates good Python practice.",o4_mini,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test, multivariate_logrank_test
import scipy.stats as stats
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.neighbors import NearestNeighbors
import pymc3 as pm

# Load and preprocess
df = pd.read_csv('titanic.csv')
df['Age'].fillna(df['Age'].median(), inplace=True)
df['...",6,"The generated code attempts to cover most of the requirements but falls short in crucial aspects, particularly regarding the interpretation of time-to-event for the Titanic dataset, the rigor of statistical methods, and the depth of model validation.

**Strengths:**
*   Correctly uses `lifelines` for Kaplan-Meier and CoxPH.
*   Includes a basic implementation of propensity score matching.
*   Attempts bootstrap resampling and Bayesian analysis.
*   Handles basic data preprocessing (imputation, one-hot encoding).
*   Correctly uses PyMC3 for Bayesian modeling.

**Weaknesses and Areas for Improvement:**

1.  **Kaplan-Meier and CoxPH ""Time-to-Event"" Interpretation:** The most significant flaw is the understanding of 'time-to-event' for the Titanic dataset. The prompt specifically states to treat ""voyage as time-to-event data (with survival as event indicator)"". The ground truth correctly interprets this as a fixed time `T=1` for all, and the event `E` being death (survival=0). The generated code uses 'Age' as `duration_col` and 'Survived' as `event_col`. This is conceptually incorrect, as 'Age' is a demographic characteristic at the start of the voyage, not the duration of exposure to risk *during the voyage*. This fundamentally misinterprets the survival analysis task for this specific dataset. This makes the KM plots and CoxPH results misleading.

2.  **Advanced Statistical Tests & Multivariate Analysis:**
    *   The prompt asked for ""multivariate analysis, interaction effects testing, and model diagnostics."" While an interaction term (`Sex_Pclass`) is introduced for CoxPH, the multivariate analysis aspect is not fully explored beyond basic Cox regression.
    *   The model diagnostics for CoxPH are limited to `check_assumptions()`, which is insufficient when `T` is constant or misdefined.
    *   The prompt asked for interaction effects testing in the context of advanced statistical tests; the generated code only adds an interaction term and re-runs Cox, but doesn't explicitly *test* its significance or compare models using, for example, likelihood ratio tests or AIC/BIC (though this might be harder to do directly with `lifelines` `CoxPHFitter`).

3.  **Propensity Score Matching (PSM) Rigor:**
    *   The PSM implementation is very basic. It performs matching but lacks critical components for robust PSM:
        *   **Balance Check:** No quantitative check for covariate balance (e.g., standardized mean differences (SMD)) after matching is performed. This is crucial to validate the matching process.
        *   **ATE Calculation:** The ATE calculation on `Survived` is correct, but it's often more informative to report odds ratios or risk differences.
        *   **Treatment Definition:** The treatment `Pclass==1` vs `others` might lead to less clear interpretation than `Pclass==1` vs `Pclass==3` as suggested in the ground truth, particularly if `Pclass==2` is also heterogeneous.

4.  **Bootstrap Resampling:**
    *   The bootstrap for Cox coefficients is implemented, but the interpretation of the results (printing intervals) is basic. The ground truth provides slightly more informative output (mean and CI).
    *   It only bootstraps the *coefficients*, not other metrics like C-index.

5.  **Bayesian Analysis:**
    *   The Bayesian model uses PyMC3 correctly for a logistic regression (which is a suitable approach for 'Survived' as a binary outcome).
    *   However, it doesn't align with the *survival analysis* task. The prompt asked for ""probabilistic modeling of survival factors"", which often implies a Bayesian Cox model or similar, not just a generic logistic regression. While a Bayesian logistic regression is an advanced statistical model, it deviates from the survival analysis focus in the context of the prompt.
    *   It lacks posterior plots (`plot_trace`, `plot_posterior`) which are standard for presenting Bayesian results.

6.  **Model Validation and Sensitivity Analysis:**
    *   The sensitivity analysis is quite superficial. It only adds noise to 'Age' and re-runs Cox, which is a very specific and potentially not very informative sensitivity.
    *   It completely misses other crucial sensitivity analyses like imputation strategy sensitivity (median vs. mean imputation for Age), prior sensitivity for Bayesian models, or model specification sensitivity (different feature sets for Cox models), all of which were requested and implemented in the ground truth.
    *   The heatmap for correlations is a good starting point for model diagnostics but not a comprehensive validation of model assumptions or sensitivity analysis on its own.

7.  **Code Structure and Readability:**
    *   The code is a bit monolithic; breaking it down into named functions would improve readability and maintainability.
    *   Variable names are mostly clear, but some sections are quite dense.

To improve, the generated code needs to fundamentally re-evaluate the ""time-to-event"" for Titanic data, deepen the statistical rigor for PSM (balance checks), expand on model diagnostics and sensitivity analysis, and ensure the Bayesian analysis aligns more directly with survival modeling if possible, or at least provides a more complete output.",o4_mini,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# 1. Create the DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
    'Final_Score': [90, 88, 82, 92, 8...",9,"The generated code is very good and largely fulfills all requirements.

**Strengths:**
*   **Correctness:** It correctly creates the DataFrame, calculates the Pearson correlation, generates the scatter plot with correct coloring by subject, and assigns the grades accurately. The `value_counts()` method with `sort_index()` correctly displays the grade distribution.
*   **Code Quality:** Imports are correct and necessary. The code is well-structured and readable. Using `seaborn.scatterplot` with `hue` is an excellent choice for plotting the data by subject. The use of `df.apply` for grade assignment is a valid and readable approach.
*   **Completeness:** All major requirements are met: DataFrame creation, correlation calculation and printing, scatter plot generation with all specified visual elements (title, labels, legend, coloring), grade assignment, and printing grade distribution.

**Minor Area for Improvement:**
*   **Correlation Formatting:** The correlation coefficient is printed with 3 decimal places (`coef:.3f`), whereas the ground truth uses 4 (`coef:.4f`). While this is a minor difference, consistency with benchmarks is ideal.
*   **`numpy` Import:** The ground truth imported `numpy` for `np.select` for grade assignment. While the generated code used a function with `apply`, which is perfectly valid, if `np.select` was intended (as hinted by the ground truth), `numpy` would be needed. However, given the actual implementation, it's not strictly missing.

Overall, this is a highly competent solution that effectively addresses the prompt. The minor points are aesthetic or alternative implementations rather than errors.",o4_mini,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {
    'Date': pd.to_datetime([
        '2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05',
        '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12'
    ]),
    'Ticker': ['TECHCO'] * 10,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0],
    'Low': [149.5, 150.5...",10,"The generated code is excellent. It correctly addresses all requirements of the task: it properly creates the DataFrame and sets the index, calculates daily returns and both 3-day and 5-day simple moving averages. The plotting section is well-implemented with all necessary labels, title, and a legend. Finally, it correctly identifies and prints the bullish crossover dates using a robust logic for detecting crossovers. The code is clean, well-structured, and includes all necessary imports.",o4_mini,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"import pandas as pd
import matplotlib.pyplot as plt

data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics'] * 7,
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST001']
}

df = pd.DataFrame(data)
df['Revenue'] = df['Quantity'] * df['Price_USD'...",9,"The generated code correctly addresses all parts of the task.

**Strong points:**
- All required libraries (`pandas`, `matplotlib.pyplot`) are correctly imported.
- The DataFrame is created accurately with the specified data.
- The 'Revenue' column calculation is correct.
- Total sales revenue per product is calculated and grouped correctly.
- The horizontal bar chart is generated as requested, with correct labels and title. `plt.tight_layout()` is a good addition for plot presentation.
- The average order value is calculated correctly using `df['Revenue'].sum() / df['OrderID'].nunique()`.
- Identification of customers with more than one purchase is correct, using `value_counts()` and filtering.

**Minor areas for improvement (to reach a 10):**
- **Output clarity:** The GROUND TRUTH code includes more descriptive print statements and formatting (like separate print sections, f-string formatting for currency, and delimiters `""=""*40`) which significantly improves the readability of the output for the different analysis steps. The generated code's print statements are functional but less verbose and formatted. For instance, `print(f""Average order value: ${avg_order_value:.2f}"")` is good, but adding a header like `--- Average Order Value ---` would match the quality of the GROUND TRUTH.
- **Plot details:** While the plot fulfills the requirements, the GROUND TRUTH code adds `plt.figure(figsize=(10, 6))` for better sizing and `plt.grid(axis='x', linestyle='--', alpha=0.7)` for better readability, which are good practices for professional plots.
- **Explicit variable names:** `rev_by_prod` and `cust_counts` are clear, but variables like `multi_customers` could be `repeat_customers` for consistency with task phrasing, though this is very minor.

Overall, it's a very good solution that perfectly fulfills the core requirements of the task. The areas for improvement are mostly about presentation and robustness, not correctness.",o4_mini,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd
import numpy as np

# 1) Define the original DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 2) Map DeviceType to numeric
df['DeviceType_Numeric'] = df['DeviceType'...",10,"The generated code is excellent. It correctly defines the initial DataFrame, accurately creates the `DeviceType_Numeric` feature by mapping the string values to integers, and correctly calculates `PagesPerMinute` while robustly handling potential division by zero by replacing NaNs and Infs with 0. Finally, it selects the specified feature columns and prints the head of the resulting DataFrame, fulfilling all requirements of the task. The code is clean, concise, and directly addresses all aspects of the prompt. It imports necessary libraries, and the structure is logical, following the steps outlined in the problem description.",o4_mini,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""Timestamp"": [
        ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"",
        ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""
    ],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 3...",9,"The generated code correctly defines the DataFrame, calculates the mean and standard deviation for temperature, identifies outliers based on the 1.5xSTD threshold, and prints the specified details for each outlier.

**Strong points:**
*   **Correct DataFrame definition:** Accurately initializes the pandas DataFrame with the provided data.
*   **Correct timestamp conversion:** Converts the 'Timestamp' column to datetime objects, which is good practice.
*   **Accurate outlier calculation:** Correctly calculates the mean and standard deviation, and identifies outliers using the absolute difference from the mean compared to the `1.5 * std_temp` threshold. This is mathematically equivalent to checking against `upper_bound` and `lower_bound`.
*   **Correct outlier printing:** Successfully iterates through the identified outliers and prints the 'SensorID', 'Timestamp', and 'TemperatureCelsius' as requested.

**Areas for minor improvement:**
*   **Output formatting for timestamp:** The output `Timestamp: {row['Timestamp']}` will print the full datetime object, including microseconds and timezone if any, which might be less readable than a formatted string (e.g., `strftime('%Y-%m-%d %H:%M')`) as in the ground truth.
*   **Output formatting for temperature:** The temperature is printed without a specific decimal precision or unit (e.g., ""C""). Adding this would improve readability and align with the ground truth's attention to detail.
*   **Missing ""No outliers found"" message:** If no outliers were found, the code would simply print nothing. The ground truth includes a helpful message in this scenario. This is a minor omission but enhances user experience.
*   **Redundant import `datetime`:** `import datetime` is present, but `pd.to_datetime` handles the conversion directly, so `datetime` module is not explicitly used elsewhere. While not harmful, it's slightly unnecessary. (Self-correction: Upon re-checking, the `datetime` import was in my internal review template, not in the provided generated code. The generated code submitted for evaluation did *not* have this unnecessary import. This point is therefore irrelevant for *this* specific generated code.)",o4_mini,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Step 0: create the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comme...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the `EngagementScore` as specified, and then efficiently identifies the `PostType` with the highest average `EngagementScore` for each `Platform`. The use of `groupby('Platform')['EngagementScore'].idxmax()` is a concise and effective way to achieve the desired result, showcasing good pandas idiom. The output format is also correct and clearly presents the required information. The code is well-structured and easy to understand.",o4_mini,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

# 1. Define the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([
        ""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"",
        ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""
    ]),
    ""DischargeDate"": pd.to_datetime([
        ""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"",
        ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""
    ]),
    ""...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the ""LengthOfStay"" column, aggregates the data to find the average length of stay and patient count per diagnosis, prints this information, and correctly identifies the diagnosis with the longest average length of stay. The code is clean, concise, and directly addresses all parts of the task. The use of `groupby().agg()` is efficient and idiomatic pandas. The f-string formatting for printing is also well done. Everything is accurate and complete.",o4_mini,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",9,"The generated code correctly defines the DataFrame, calculates the ""WeightedRating"" precisely as specified, adds it as a new column, and accurately identifies and prints the title and weighted rating of the movie with the highest ""WeightedRating"". The use of `idxmax()` to find the highest weighted rating and then `loc` for selection is efficient and idiomatic pandas. The code is concise and directly addresses all parts of the problem.

The only minor point for improvement, which is not critical for correctness but good practice, would be to handle the edge case where `total_votes` could be zero, which the ground truth code includes. Although for this specific dataset `total_votes` will never be zero, robust code often considers such possibilities. Additionally, the print format for the final output could be slightly more explicit (e.g., ""Title: X, WeightedRating: Y"") instead of just printing the series values directly. However, these are minor and don't detract significantly from the solution's correctness or quality for this specific task.",o4_mini,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 10000, 8000]
}

df = p...",10,"The generated code is excellent. It correctly defines the DataFrame as specified, precisely calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"" using the `groupby` and `agg` methods, and creates a new DataFrame `grouped_df` for these averages. The use of `.reset_index()` is a good practice to ensure 'Brand' remains a column rather than becoming the index, which often leads to more readable output for grouped summaries. Finally, it correctly prints the resulting grouped DataFrame. All requirements of the task are met perfectly.",o4_mini,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 8...",10,"The generated code is excellent. It correctly defines the DataFrame, adds the 'TotalStats' column, identifies and prints the Pokemon with the highest total stats, and calculates and prints the average speed for each 'Type1'. The code is clean, concise, and uses appropriate pandas functionalities. All requirements of the task are met accurately, and the output format of the print statements is clear.
Specifically:
1.  **DataFrame definition:** Perfectly matches the task description.
2.  **`TotalStats` column:** Correctly calculates the sum of the specified columns using `pd.DataFrame.sum(axis=1)`, which is a robust way to do so.
3.  **Highest `TotalStats` Pokemon:** Correctly uses `idxmax()` and `.loc` to find the name of the Pokemon with the highest total stats. The f-string for printing is clear.
4.  **Average Speed per `Type1`:** Correctly uses `groupby()` and `mean()` to calculate the average speed for each type. The loop to print the results is also well-implemented and readable.",o4_mini,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score, adjusted_rand_score, mutual_info_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE...",6,"The generated code attempts to address most of the requirements but falls short in several areas, particularly regarding advanced feature engineering, comprehensive cluster validation, and detailed cluster analysis/visualization as requested by the prompt.

**Strengths:**
*   **Basic Feature Engineering:** Includes `FamilySize`, `TicketCount`, `FarePerPerson`, `NameLen`, `Title`, and `Deck`, which are good starting points.
*   **Multiple Clustering Algorithms:** Uses K-Means, Agglomerative, DBSCAN, and GMM.
*   **Dimensionality Reduction:** Implements PCA, t-SNE, and UMAP.
*   **Some Validation Metrics:** Includes Silhouette, Gap Statistic, and attempts at ARI for stability.
*   **Consensus Clustering:** Attempts a basic form of consensus clustering.
*   **Basic Visualizations:** Provides scatter plots for DR, countplot for survival, and a heatmap for feature means.

**Weaknesses and Areas for Improvement:**

1.  **Correctness & Completeness of Feature Engineering:**
    *   **Advanced Features:** The prompt specifically asked for ""advanced features including social network analysis (family groups, ticket sharing), socioeconomic indicators, and text-based features from names and tickets."" While `TicketCount` and `FamilySize` touch on this, the ground truth's `TicketGroupSize`, `FarePerPerson` (correctly handling division by zero), `Title` (with proper grouping of rare titles), `TicketPrefix`, `CabinDeck`, `HasCabin`, and interaction features (`Age_Class`, `Fare_Class`, `FarePerPerson_Class`) are far more comprehensive and robust. The generated code's `Title` and `Deck` processing is overly simplified.
    *   **Imputation:** The generated code imputes Age and Fare *before* some feature engineering (like FarePerPerson which uses FamilySize, but not TicketGroupSize from the ground truth), which is okay, but the ground truth's approach of imputing Fare *before* `FarePerPerson` is more robust if `TicketGroupSize` was correctly implemented.
    *   **One-Hot Encoding:** The generated code uses `LabelEncoder` for categorical features. While this is simple, it implies ordinality which is incorrect for features like 'Sex', 'Embarked', 'Title', 'Deck', and 'Pclass'. The prompt asked for proper preprocessing, and `OneHotEncoder` (as in the ground truth) is essential for such features prior to scaling and clustering. This is a critical error as it can heavily bias clustering results.

2.  **Preprocessing & Scaling:**
    *   The generated code uses `LabelEncoder` for categorical features and then scales them directly. This is fundamentally flawed as `LabelEncoder` introduces arbitrary numerical relationships (e.g., Cat A=0, Cat B=1, Cat C=2 implies B is ""between"" A and C, and C is ""twice"" A, which is nonsensical for nominal categories). `OneHotEncoder` followed by `StandardScaler` (as in the ground truth) is the correct approach.

3.  **Optimal Number of Clusters:**
    *   **Metrics:** It mentions Elbow (implicitly via inertia), Silhouette, and Gap Statistic, which are good. However, it only plots Gap Statistic and Silhouette, and doesn't explicitly choose or justify an ""optimal k"" based on these plots for the subsequent clustering. The ground truth shows K-Means inertia (equivalent to elbow), Silhouette, Calinski-Harabasz, and Davies-Bouldin metrics for *all* applicable clustering algorithms (K-Means, GMM, Hierarchical), providing a much more comprehensive view.
    *   **Fixed K:** It hardcodes `n_clusters=4` for all algorithms without explicitly using the validation metrics to determine this number, making the validation section less impactful.

4.  **DBSCAN:**
    *   The generated code uses hardcoded `eps` and `min_samples` without any mention of strategy for selection. DBSCAN sensitivity to these parameters means careful tuning or explanation is required in a comprehensive clustering task. The ground truth code briefly acknowledges this.

5.  **Cluster Stability and Consensus Clustering:**
    *   **Stability:** The generated code calculates ARI between two K-Means runs (with `i` and `i+5` random states). This is a start, but the ground truth's approach of consistently comparing multiple runs against a baseline (e.g., the first run) and then calculating mean/std ARI provides a more robust measure of stability.
    *   **Consensus Clustering:** The implementation is a reasonable attempt using a co-association matrix and hierarchical clustering. However, the subsequent analysis *only* focuses on this consensus cluster and neglects the individual clustering algorithm results (K-Means, GMM, Agglo, DBSCAN) and their specific characteristics, which was a core requirement. The ground truth analyzes *each* clustering algorithm separately, then demonstrates how external validation can be applied.

6.  **Analysis of Cluster Characteristics and Survival Patterns:**
    *   The prompt asked for ""Analysis of cluster characteristics and survival patterns within each segment."" The generated code *only* does this for the `ConsensusCluster`. The ground truth dedicates a significant section to analyzing *each* clustering algorithm (`KMeans`, `Hierarchical`, `GMM`, `DBSCAN`) separately, presenting:
        *   Survival rates per cluster.
        *   Mean/mode of key *original* features for each cluster profile.
        *   Heatmap of *scaled* feature means.
    *   The generated code's `cluster_summary` heatmap uses `mean()` directly on some nominally encoded features (like Sex, Embarked, Title, Deck) which is not meaningful after `LabelEncoder`. This is a direct consequence of using `LabelEncoder`.

7.  **Comprehensive Visualizations:**
    *   The generated code's visualizations are basic. It shows DR plots, survival by consensus cluster, and a heatmap of feature means. The ground truth provides a richer set of visualizations for *each* clustering algorithm:
        *   Cluster distributions on PCA, t-SNE, UMAP.
        *   Survival rates per cluster (barplot).
        *   Feature distributions (violin plots for Age, Fare, Sex, Pclass), which are very insightful for understanding segments.
        *   Heatmap of scaled feature means for each cluster provides a clear profile comparison.

8.  **External Validation:**
    *   The prompt asked for ""Validation clustering results using external criteria and survival outcomes."" The generated code attempts this with `adjusted_rand_score(df['Pclass'], consensus_labels)` and `mutual_info_score(df['Survived'], consensus_labels)`. While using Pclass and Survived as external criteria is valid, the interpretation of `ARI` with `Pclass` as ""ground truth"" for clusters is problematic unless Pclass is strictly considered a known, ideal segmentation (which it usually isn't for an unsupervised task). The ground truth instead focuses on analyzing the *survival distribution* within each cluster, which is a more direct application of external validation using the 'Survived' outcome. It doesn't claim to calculate direct ARI to `Survived` as `Survived` is binary not a clustering.

In summary, the generated code provides a skeletal solution that hits most keywords but lacks the depth, robustness, and correctness in implementation details (especially preprocessing and feature engineering) and comprehensive analysis/visualization that the prompt specifically requested. The `LabelEncoder` usage is a critical flaw.",o4_mini,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [
        ""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"",
        ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""
    ],
    ""Author"": [
        ""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"",
        ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""
    ],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopia...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the book count and average price per author, prints this information clearly, and accurately identifies the author with the highest total stock. The use of `.agg()` for multiple aggregations and `.round(2)` for price formatting are good practices. The output formatting is also clean and easy to read. All requirements of the task are fully met.",o4_mini,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"import warnings
warnings.filterwarnings(""ignore"")

import numpy as np
import pandas as pd
from scipy import stats
from scipy.stats import ttest_rel
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.feature_selection import VarianceThreshold
from sklearn.pipeline import Pipeline
from sklearn.compose import Column...",4,"The generated code attempts to address many requirements but largely fails in implementing them correctly or comprehensively, leading to significant functional and conceptual errors.

**Primary Reasons for Failure:**

1.  **Fundamental Preprocessing Error (X_z and preprocessor):** The code applies `z-score winsorization` by setting outliers to `np.nan` *before* the `IterativeImputer` is used. However, the `ColumnTransformer` applies `IterativeImputer` to `num_features` (which are copied from `X.columns.tolist()`) *within the pipeline*. This means the `z-score` winsorization that introduces `np.nan` values is done *outside* the pipeline on a standalone `X_z` DataFrame, and then `X_z` is passed to `preprocessor.fit_transform`. This `X_z` already has `NaN`s, but the `ColumnTransformer` is incorrectly initialized with `num_features` for *all* steps, including scaling and `VarianceThreshold`. This approach of applying some preprocessing (winsorization) *before* the `ColumnTransformer` fits and transforms introduces a disconnect and potential data leakage, especially if `X_z` is derived from `X` and not just `X_train`. The features for `PolynomialFeatures`, `StandardScaler`, and `VarianceThreshold` are incorrect because they operate on `num_features`, but the actual data passed (`X_z`) already has modified values from the initial `z-score` winsorization. The `ColumnTransformer` applies the `IterativeImputer` *again* to *all* numerical features, which duplicates the imputation step relative to the `X_z` modification, or more likely, handles the `NaN`s introduced by an *external* operation. The design is convoluted and potentially incorrect.
2.  **Incorrect Feature Naming and SHAP/LIME Application:** After `preprocessor.fit_transform(X_z)`, `X_proc` is a NumPy array. The subsequent `X_train` and `X_test` are also NumPy arrays. SHAP and LIME require feature names for meaningful interpretation, but the generated code passes raw NumPy arrays (`X_train`, `X_test`, `X_test[i]`) without feature names to `TreeExplainer` and `LimeTabularExplainer`. This will result in generic feature names (e.g., 'f0', 'f1', ...) or errors, rendering interpretability plots useless. The ground truth correctly converts processed arrays back to DataFrames with feature names.
3.  **Flawed Blending Implementation:** The blending implementation is conceptually incorrect. It trains all base models on `X_train` and then directly passes `best_estimators[n].predict_proba(X_train)` to a Logistic Regression meta-learner. This is classic data leakage. Blending requires splitting the training data into two parts: one to train the base models, and the other to train the meta-learner on the base models' predictions from the *first* part. The generated code predicts on `X_train` using models trained on `X_train`, leading to overly optimistic meta-learner performance and violating the principle of not using the same data for training and meta-training. The `blend_predict_proba` function is then directly passed to `cross_val_score`, which expects an estimator. This is a hacky and likely problematic way to evaluate the blending model within `cross_val_score`. The `cross_val_score` for blending is also ambiguous; it should evaluate the entire blending process, not just the meta-learner. Blending is typically evaluated on a held-out test set or via a more complex cross-validation setup, not directly with `cross_val_score` on a function.
4.  **Incomplete/Incorrect `cross_val_score` for Ensembles:** The `cross_val_score` for `stack` and `vote` uses the full `X_train` and `y_train`. While this evaluates the ensemble method using internal CV (for Stacking) or direct fitting, it doesn't align with the comprehensive evaluation using stratified k-fold *across the entire process* or the learning curve generation, which should be based on a consistent CV split.
5.  **Validation Curve Input Errors:** The validation curve uses `hasattr(est, 'n_estimators')` and `param_name='n_estimators'`. Not all models have `n_estimators` (e.g., SVM, MLP). Even for models that do, the `param_range` of `[10,50,100,200]` is hardcoded, which might not be appropriate for all models or the best parameters found during hyperparameter tuning. The plot logic for `param_range` is also questionable.
6.  **Bootstrap Sampling for CI:** The bootstrap sampling takes random choices from `X_test` and `y_test.values`. While `random_state` is typically used for reproducibility, `np.random.choice` without setting a seed will produce different results every time the script is run in isolation. More importantly, it directly uses `X_test` from the initial `train_test_split` without ensuring the transformation pipeline is applied correctly *within* the bootstrap loop if `X_test` wasn't already processed. `X_test` *is* processed in this code, so that is okay, but `np.percentile(bs, [2.5,97.5])` calculates the CI for the *test set performance* and not necessarily the *generalization performance* if the base models were not refitted on bootstrap samples. The ground truth re-trains models on train folds and evaluates on validation folds within CV for CI calculation, which is a more robust approach.
7.  **Plotting and Dashboard Issues:** The performance metrics dashboard calculation is based on `results` which are `cross_val_score` outputs (mean ROC AUC scores) but then attempts to add bootstrap CIs from `bootstrap_scores` which are calculated on the `X_test`. This mixes evaluation methodologies inappropriately. The error bar for `std_auc` in the dashboard plot is correct for `cross_val_score` std dev, but the task asked for CI plots, which the bootstrap method aims to provide. The blending evaluation in the dashboard is also implicitly messy due to the `cross_val_score` handling.
8.  **Model Calibration:** `clf = CalibratedClassifierCV(est, cv=cv)` trains the calibrated model using cross-validation on `X_train` (the training data). Then it uses `clf.predict_proba(X_test)` on the test set. This is generally fine, but the plotting code simply iterates over `best_estimators`, which includes the `blend` function that `CalibratedClassifierCV` cannot wrap. The `if name=='blend': continue` skips blending, but other models like `stack` and `vote` are included, which might not be appropriate for direct calibration as they are already ensembles.
9.  **Missing Requirements:**
    *   **Advanced Feature Engineering:** While `PolynomialFeatures` and `VarianceThreshold` are used, there's no explicit advanced feature engineering like creating domain-specific features or explicit interaction terms beyond what `PolynomialFeatures` does generally. The task specifically asked for ""advanced feature engineering including polynomial and interaction features.""
    *   **Multiple Imputation Strategies (Applied):** The code *defines* multiple imputers but only uses `IterativeImputer` in the pipeline. It doesn't implement a strategy where multiple imputations are used and models are trained on each, then results combined, which is what ""multiple imputation strategies"" typically implies (e.g., MICE). The `IterativeImputer` *is* a multiple imputation technique by definition, but implies iterative rather than multiple strategies applied sequentially. The prompt indicates ""multiple imputation strategies"" as in *different approaches*, not necessarily MICE. The GT satisfies this by calling out MICE explicitly, which is the iterative imputer.
    *   **Bootstrap Sampling for Confidence Intervals (Correctness):** Has issues as noted.
    *   **Model Comparison Dashboard with Prediction Confidence Intervals:** The code doesn't explicitly compute or visualize *prediction* confidence intervals for individual predictions. The bootstrap CI is for *metric* confidence intervals, not *prediction* confidence intervals.
    *   **Brier Score for Calibration:** While calibration plots are generated, the quantitative Brier score, which is a key metric for calibration, is not reported for calibrated vs. uncalibrated models (as demonstrated in the ground truth).
    *   **LIME Visualization:** LIME explanations are saved to HTML files and not visualized directly alongside other plots.

In summary, the generated code contains numerous fundamental errors in its implementation of advanced techniques, especially in preprocessing, ensemble training (blending), and the application of interpretability and evaluation tools. While it features many required modules and function calls, the overall correctness and robustness are severely lacking.",o4_mini,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import KNNImputer, IterativeImputer
from statsmodels.imputation.mice import MICEData
import statsmodels.api as sm
from sklearn.linear_model import LogisticRegression
from pygam import LogisticGAM, s
import pymc3 as pm
from scipy import stats
from statsmodels.multivariate.manova import MANOVA
from sklearn.utils import resample
from sklearn.ensemble import IsolationForest
from sklearn.n...",6,"The generated code attempts to address all aspects of the prompt, showing an understanding of the requirements. However, it falls short in several key areas of implementation and comprehensiveness, particularly in the advanced statistical models and diagnostics, and lacks user-friendliness and proper output.

Here's a breakdown:

**Strengths:**
*   **Imports:** Most necessary imports are present.
*   **Imputation Methods:** Implemented MICE (using `statsmodels.imputation.mice`), IterativeImputer, and KNNImputer, and performs a basic comparison.
*   **Model Variety:** Includes Logistic Regression (Scikit-learn and Statsmodels), GAM, and PyMC3 Bayesian Logistic Regression.
*   **Hypothesis Testing:** Includes MANOVA, a permutation test, and bootstrap confidence intervals.
*   **Anomaly Detection:** Implements Isolation Forest, LOF, and One-Class SVM.
*   **Dimensionality Reduction:** Includes PCA and Factor Analysis.
*   **Robust Methods:** Includes RLM.
*   **Diagnostics:** Attempts Q-Q plots, residuals vs. fitted, influence measures, and a basic diagnostic dashboard.

**Weaknesses and Areas for Improvement:**

1.  **Readability/User Experience:**
    *   **No print statements:** The prompt specifically asks for no explanations, but to clearly identify which part of the code is executing a requirement, a simple print statement (e.g., ""--- Performing KNN Imputation ---"") would be standard practice for a complex script like this. The generated code runs silently without indicating progress.
    *   **No comments:** Lacks comments to explain complex sections or choices, making it harder to follow.
2.  **Missing Data Imputation:**
    *   **MICE Implementation:** The MICE implementation using `MICEData` is not fully robust for comparing effectiveness directly. It implies a single imputation model rather than comparing across different MICE models. The ground truth shows more robust application (IterativeImputer is effectively MICE-like). No actual ""comparison of effectiveness"" is presented beyond basic descriptive statistics.
    *   The `df_cc = df.dropna()` line discards data, which defeats the purpose of imputation evaluation. The comparison should ideally be on the imputed datasets' characteristics without dropping original rows.
3.  **Statistical Modeling:**
    *   **Logistic Regression with Regularization:** While present, `skl_l1.fit(X, y)` and `skl_l2.fit(X, y)` are trained on the full dataset, not split train/test sets, which is crucial for evaluating performance and preventing overfitting. No performance metrics (ROC AUC, classification report) are shown for these models.
    *   **GAM:** While the `gridsearch` method is used, no output or performance metrics (like ROC AUC or classification report) are displayed. Plotting partial dependencies, a key feature of GAMs, is missing.
    *   **PyMC3 Bayesian Logistic Regression:** The implementation is mostly correct, but it trains on the full `X` and `y` without a train/test split. It also doesn't perform posterior predictive checks or calculate performance metrics (like ROC AUC) on a held-out test set, which should be the standard for evaluating models, including Bayesian ones. The `progressbar=False` hides potentially useful information during sampling.
    *   **Robust Statistical Methods:** The RLM implementation is sparse. It's applied without context (e.g., modeling what dependent variable? Against what independent variables?). The prompt generally asks for handling outliers/non-normal distributions in the context of the main task (potability prediction), not just a standalone regression task on arbitrary variables.
4.  **Hypothesis Testing:**
    *   **MANOVA:** Uses a hardcoded subset of features (`Hardness + Solids + Chloramines`). A more comprehensive solution would either use all features or explain the feature selection strategy. The `mv_test()` output is just printed, with no interpretation guidance. The ground truth provides helpful interpretation.
    *   **Permutation Tests/Bootstrap CI:** The implementations are functional, but print only the result without any explanatory text or proper visualization to accompany them, making them less useful.
5.  **Anomaly Detection:**
    *   Models are fitted and predictions are stored in the DataFrame, but no summary or actual *identification* of anomalies (e.g., printing the indices or count of detected anomalies) is made explicit. No visualization of anomaly scores or identified outliers is presented.
6.  **Model Diagnostics:**
    *   **Lilliefors test:** Applying Lilliefors test to `logit.resid_response` (deviance or Pearson residuals) for a logistic regression model is generally inappropriate. Logistic regression residuals are not expected to be normally distributed since the response is Bernoulli. This indicates a misunderstanding of GLM diagnostics. The Q-Q plot of deviance residuals is also shown, which is more common, but interpreting its ""normality"" for a binary outcome is tricky.
    *   **Completeness:** Lacks influence measures beyond just showing the `influence_plot`. Doesn't discuss Cook's distance threshold or interpretation.
    *   **Goodness-of-Fit:** Beyond the inappropriate Lilliefors test, it doesn't provide standard goodness-of-fit tests for logistic regression (e.g., Hosmer-Lemeshow or Pearson chi-square). The `logit.summary()` output is not displayed.
7.  **Visualizations:**
    *   Only basic diagnostic plots are present. Lacks ""advanced statistical visualizations"" like ROC curves for models, leverage plots (though `influence_plot` covers some aspects), or comprehensive diagnostic dashboards beyond the single `fig2`. The dashboard itself has a very generic plot of `Bayes Alpha Trace` which isn't very informative without context. The prompt specifically asked for Q-Q plots, leverage plots, and diagnostic dashboards.
    *   No pairplots, correlation heatmaps, or boxplots comparing features by Potability, which are crucial for initial data understanding and advanced statistical visualization.

Overall, the code demonstrates an awareness of the components required but often implements them in a basic, incomplete, or sometimes incorrect manner without robust evaluation, comprehensive outputs, or proper interpretation. It serves as a good starting point but needs significant refinement to meet the ""expert"" level implied by the prompt.",o4_mini,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.manifold import TSNE
import umap.umap_ as umap
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, pairwise_di...",7,"The generated code provides a good attempt at addressing most of the requirements but falls short in a few key areas, particularly regarding the depth of implementation for certain advanced techniques and the overall user experience of the visualizations.

**Strong Points:**
*   **Feature Engineering:** Includes water quality indices, chemical balance ratios, and contamination risk scores, which is good.
*   **Multiple Clustering Algorithms:** Implements K-Means, Hierarchical, DBSCAN, GMM, and Spectral Clustering.
*   **Dimensionality Reduction:** Covers PCA, t-SNE, UMAP, and Factor Analysis.
*   **Optimal Cluster Determination:** Attempts multiple metrics (silhouette, Davies-Bouldin, Calinski-Harabasz, Elbow, Gap Statistic - though the Gap Statistic implementation is simplified).
*   **Cluster Stability Analysis:** Includes a basic bootstrap resampling approach for stability.
*   **Anomaly Detection:** Implements Isolation Forest and Local Outlier Factor.
*   **Visualization:** Generates 2D/3D scatter plots and radar charts using Plotly.

**Areas for Improvement / Missing Aspects:**
*   **Optimal Cluster Determination - Gap Statistic:** The `gap_statistic` function is implemented but its results (the `gaps` variable) are not used in the final choice of `optimal_k` or even plotted alongside other metrics for visual inspection (which the ground truth does). This makes its inclusion less impactful.
*   **Optimal Cluster Determination - Consolidated Approach:** The code calculates metrics for `k_range` and then directly picks `n_clusters=3` for all algorithms. While `optimal_k=3` was mentioned in ground truth also as a *choice for demonstration*, in a real scenario, the plots (which are *not* generated by the provided code for silhouette, DB, CH, elbow) would be inspected to make that decision dynamically. The generated code calculates the metrics into `metrics_df` but doesn't plot them, making visual inspection impossible.
*   **Cluster Stability Analysis - Consensus Clustering:** The prompt explicitly asked for ""consensus clustering"" in addition to bootstrap resampling. The generated code only implements a form of bootstrap resampling by comparing ARI scores between successive runs on resampled data. It does not implement a matrix-based consensus clustering approach (co-occurrence matrix, hierarchical clustering of the similarity matrix) as demonstrated in the ground truth, which is a more robust method for stability.
*   **Visualization Dashboard:**
    *   While Plotly plots are generated, they are saved as HTML files rather than displayed directly (`fig.show()`). This makes the ""dashboard"" aspect less interactive and immediate compared to what `fig.show()` provides in a typical interactive environment (like a Jupyter notebook).
    *   The radar charts are generated one *per cluster and algorithm*, leading to many individual HTML files, which is not ideal for a ""dashboard"" view where comparisons between clusters are central. The ground truth provides a single radar chart overlaying all cluster profiles for easy comparison.
    *   Missing potability rate per cluster bar chart.
*   **Feature Engineering Depth:** While it includes some indices, the WQI calculation is a very basic sum/average. The ground truth includes more nuanced concepts like pH deviation from ideal, hardness quality, and also normalizes certain features before combining them, creating a more sophisticated WQI. The generated code also does not explicitly state that temporal patterns are ""not applicable"" as the ground truth does, which is a minor point but shows attention to detail regarding prompt constraints.
*   **DBSCAN Parameter Tuning:** The ground truth includes a k-distance graph plot to help estimate `eps` for DBSCAN, which is crucial. The generated code simply uses a fixed `eps=0.5, min_samples=5`, which is unlikely to be optimal without prior analysis.
*   **Factor Analysis Use:** Factor analysis is applied, but its results are not explicitly used in any further analysis or visualization.
*   **Domain Knowledge Validation:** The section on external validation is commented out rather than implemented or explicitly discussed as part of the analysis. The ground truth explicitly lists WHO guidelines and then refers to the `cluster_profiles_df` for comparison, serving as a direct validation step.
*   **Code Structure and Comments:** Some parts could benefit from more detailed comments, especially around the logic for advanced techniques like gap statistic and stability analysis.

Overall, the code demonstrates a solid understanding of the individual components requested, but the implementation sometimes lacks the depth and integration expected for a truly ""comprehensive"" analysis and a fully interactive ""dashboard.""",o4_mini,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('winequality-red.csv')

# Correlations with quality
corr = df.corr()['quality'].drop('quality')
top_pos = corr.sort_values(ascending=False).head(3)
top_neg = corr.sort_values().head(3)

print(""Top 3 positively correlated features with quality:"")
print(top_pos)
print(""\nTop 3 negatively correlated features with quality:"")
print(top_neg)

# Correlation matrix heatmap
plt.figure(figsize=(12,10))
sn...",8,"The generated code correctly addresses most of the task requirements. It loads the data, calculates correlations, identifies top positive and negative features, generates a correlation heatmap, and creates scatter plots with regression lines for the most correlated features.

Strong points:
- Correctly uses pandas for data loading and correlation calculation.
- Efficiently identifies top positive and negative correlations using `sort_values` and `head`/`tail`.
- Properly uses seaborn for heatmap and `regplot` for scatter plots with regression lines, which is a very concise way to achieve that.
- Includes appropriate titles for plots.

Areas for improvement to reach a perfect score:
- **Missing error handling for file loading:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice for robust code. The generated code lacks this.
- **Explicit R-squared display on scatter plots:** While `sns.regplot` draws a regression line, it doesn't automatically display the R-squared value, which was implicitly part of the ground truth's detailed linregress plot. The ground truth explicitly calculates and displays R-squared, which adds more analytical value to the plots.
- **Plot customization:** The ground truth includes more detailed plot customizations (e.g., DPI, grid, legends, custom x-axis for regression line calculation with numpy) which make the plots more polished and informative, even though the `regplot` handles the line automatically. However, the task prompt didn't explicitly ask for these specific plot customizations, so it's a minor point.
- **Slightly less verbose output for correlations:** The ground truth prints the full `quality_correlations` series before picking the top 3, which is a good intermediate step for debugging/understanding. However, the generated code directly picks the top 3, which is also acceptable given the task.",o4_mini,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import chi2_contingency, mannwhitneyu

# 1. Load and clean
df = pd.read_csv('video_games_sales.csv')
df = df.dropna()

# 2. T-test: Nintendo vs Others
nintendo = df[df['Publisher']=='Nintendo']['Global_Sales']
others = df[df['Publisher']!='Nintendo']['Global_Sales']
t1, p1 = stats.ttest_ind(nintendo, others, equal_var=False)

# 3. T-test: Action vs Sports
action = df[df['Genre']=='Action']['Global_Sales']
s...",7,"The generated code correctly identifies and uses the necessary statistical tests for the given tasks. It performs t-tests, a chi-square test, and a Mann-Whitney U test, and correctly calculates p-values for interpretation. The data loading and basic cleaning are present.

However, there are several areas for improvement:
1.  **Data Cleaning:** The `df.dropna()` call at the beginning is too aggressive. The task specifically requires cleaning *missing values*, implying careful handling rather than just dropping all rows with any missing value. For example, `Year_of_Release` needs to be converted to `int` *after* dropping its NaNs, and `Publisher`/`Genre`/`Platform` NaNs should also be handled carefully. The ground truth demonstrates targeted dropping based on importance and specific type conversion for `Year_of_Release`. The generated code's blanket `dropna()` might remove too much valid data or cause errors if `Year_of_Release` has NaNs before conversion.
2.  **Interpretation Detail:** While the code provides basic ""Significant"" or ""Not significant"" interpretations, the ground truth offers more verbose and context-specific interpretations, explicitly stating the null and alternative hypotheses and what ""significant"" means in the context of each test. This is crucial for a data scientist interpreting results.
3.  **Robustness:** The generated code lacks checks for empty dataframes or groups before performing statistical tests (e.g., `if len(nintendo) < 2:` or `if ct.empty:`). This can lead to errors if certain filters result in insufficient data for a test.
4.  **Specific T-test Alternative:** For the Nintendo vs. Others test, the task states ""significantly *higher* global sales"". This implies a one-sided t-test (`alternative='greater'`), which the generated code doesn't specify (it defaults to two-sided). The ground truth correctly uses `alternative='greater'`.
5.  **Output Clarity:** The ground truth code uses f-strings with descriptions for each test, making the output much clearer than just ""1) Nintendo vs Others t-test p-value = ..."".
6.  **Error Handling (File Not Found):** The generated code does not include error handling for `FileNotFoundError` when reading the CSV, which is a good practice as shown in the ground truth.",o4_mini,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt

# 1) Load dataset and handle missing values
df = pd.read_csv('video_games_sales.csv')
df.dropna(inplace=True)

# 2) Create binary target variable
df['High_Sales'] =...",7,"The generated code generally addresses most requirements of the task.

**Strong Points:**
- Correctly loads the dataset.
- Creates the 'High_Sales' target variable as specified.
- Correctly uses LabelEncoder for categorical variables.
- Creates 'Total_Regional_Sales' and 'Years_Since_Release' features.
- Splits data into training and testing sets.
- Trains a Random Forest Classifier with 100 estimators.
- Evaluates the model using accuracy, precision, recall, F1-score, and confusion matrix.
- Displays feature importance.
- Includes proper imports.

**Areas for Improvement / Missing Aspects:**
-   **Missing Value Handling (Critical):** The generated code `df.dropna(inplace=True)` drops rows with *any* NaN. This is a very aggressive approach and could lead to significant data loss if many columns have scattered NaNs. The ground truth code specifically handles NaNs related to 'Global_Sales' (for target creation), 'Year', 'Publisher', and regional sales which are crucial for feature engineering. Simply dropping all NaNs globally without specifying which columns to prioritize or considering imputation for non-critical features is less robust and could discard valuable data.
-   **Feature Selection for Model (Minor Inaccuracy):** The ground truth includes all regional sales (`NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`) *in addition to* `Total_Regional_Sales` as features. The generated code only includes `Total_Regional_Sales` and omits the individual regional sales. While `Total_Regional_Sales` is derived from them, sometimes the individual components can still hold unique predictive power. The prompt implies using all relevant features (`NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`) as part of the model. This is a subtle difference but important in feature engineering.
-   **No Stratification in `train_test_split` (Minor):** The ground truth uses `stratify=y` when splitting the data. This is crucial for classification tasks, especially if the target variable is imbalanced (which 'High_Sales' likely is). Stratification ensures that the proportion of target classes is maintained in both training and testing sets, leading to more reliable model evaluation. The generated code omits this.
-   **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.
-   **Clarity/Verbosity:** While the prompt asked for just code, the ground truth includes comments and print statements that clarify the steps and execution flow, which is beneficial for understanding. The generated code is more concise but could benefit from a few more comments.
-   **`n_jobs=-1` for RandomForestClassifier:** The ground truth uses `n_jobs=-1` to utilize all available CPU cores, which can speed up training. The generated code doesn't include this optimization.

The primary reason for not scoring higher is the simplified and potentially problematic missing value handling and the omission of relevant features (individual regional sales) in the model training. The lack of stratification is also a notable point for improvement in classification tasks.",o4_mini,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, PolynomialFeatures

# 1) Load and handle missing values
df = pd.read_csv('video_games_sales.csv')
num_cols = df.select_dtypes(include=[np.number]).columns
cat_cols = df.select_dtypes(include=['object']).columns
df[num_cols] = df[num_cols].fillna(df[num_cols].median())
df[cat_cols] = df[cat_cols].fillna(df[cat_cols].mo...",7,"The generated code addresses most of the requirements and demonstrates a good understanding of the requested operations.

**Strengths:**
*   **Correctness in Core Tasks:** It correctly performs most of the feature engineering, correlation analysis, PCA, polynomial features, and interaction features.
*   **Missing Value Handling:** The approach for handling missing numerical and categorical values is sensible (median for numerical, mode for categorical).
*   **Feature Engineering:** All five requested engineered features are created, and generally, the calculations are correct.
*   **Correlation & PCA:** These sections are well-implemented and use standard libraries effectively.
*   **Polynomial Features:** Correctly applies polynomial features to the specified columns.

**Areas for Improvement:**

1.  **Missing Value Handling (Refinement):**
    *   `replace(0, np.nan)` for `EU_Sales` denominator in `Sales_Ratio_NA_EU` is better than not handling zero division, but `+ 1e-6` is a more robust way to prevent division by zero in this context as `NaN`s would propagate.
    *   The missing value handling (median/mode imputation) should ideally use `sklearn.impute.SimpleImputer` for consistency and better practice, especially if this code were part of a larger pipeline. The current approach is functional but less aligned with typical scikit-learn workflows for robustness.

2.  **Outlier Handling (Critical Flaw):**
    *   The ground truth code **caps** outliers, meaning values outside the IQR bounds are replaced with the bounds.
    *   The generated code **removes** rows containing outliers. This is a significant difference from the requested task (""handle outliers using IQR method"") which typically implies capping or transformation rather than outright removal, especially when not specified (removal can lead to significant data loss). This completely changes the dataset's shape for subsequent analysis, which is not what 'handle' usually implies in this context unless specified to remove. The message ""Identify and handle outliers"" in the prompt implies capping or transformation to preserve data.

3.  **Interaction Features (Implementation Detail):**
    *   The `multiply(genre_dummies.values)` approach for `Year_x_Genre` interaction features is a bit unconventional and could lead to issues if `df.reset_index(drop=True)` causes misalignment if the original `df` had non-default indices.
    *   A more standard and robust way would be to iterate through the columns of `genre_dummies` and multiply `df['Year']` with each column directly, adding them to the main DataFrame, similar to the ground truth.
    *   The generated code creates new columns prefixed with 'Year_' and then concatenates. The ground truth directly adds the interaction term to the original dataframe within the loop. The `pd.concat` for `year_genre_inter` also adds a lot of redundant columns (`Year_Genre_Action`, `Year_Genre_Adventure`, etc.) that are essentially `Year` multiplied by `0` or `1` for each original genre. This is conceptually correct but less efficient.

4.  **PCA Data Preparation:** The ground truth explicitly removes 'Rank' from PCA features, which is good practice as 'Rank' is often an identifier rather than a true numerical feature for dimensionality reduction. The generated code includes all numerical columns, which might not be ideal.

5.  **Robustness and Error Handling:** The ground truth includes `try-except` for file loading and handles `warnngs` which improves robustness and user experience. The generated code lacks these.

To score higher, the outlier handling should be changed to capping, and the interaction feature creation could be made more direct.",o4_mini,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# 1. Load and clean data
df = pd.read_csv('diabetes.csv')
cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[cols] = df[cols].replace(0, np.nan)
for c in cols:
    df[c].fillna(df[c].median(), inplace=True)

# 2. T-test: Glucose in diabetic vs non-diabetic
g0 = df[df['Outcome']==0]['Glucose']
g1 = df[df['Outcome']==1]['Glucose']
t_stat, p_t = stats.ttest_ind(g1, g0, equal_var=False)
print(f""T-test gl...",7,"The generated code correctly implements all the requested statistical tests and data cleaning steps. It handles the median imputation and performs the t-test, Mann-Whitney U test, chi-square test, Pearson correlation, and ANOVA as specified. The interpretation of results is present, although it's very concise, just indicating ""significant"" or ""not significant"".

Areas for improvement to achieve a higher score:
1.  **Interpretation Detail:** The ground truth provides much more detailed interpretation for each test, explaining what rejecting or failing to reject the null hypothesis implies, and even in the case of correlation, indicating the direction. The generated code's interpretation is minimal (""significant"" or ""not significant"").
2.  **Robustness (File Handling):** The ground truth includes a `try-except` block for loading the CSV, which is good practice for robust code. The generated code lacks this.
3.  **Readability/Output:** The ground truth includes more descriptive print statements (e.g., specific means/medians for groups, chi-square statistic, degrees of freedom, correlation coefficient) which makes the output more informative and easier to follow, especially for someone reviewing the statistical analysis. The generated code only prints p-values and a boolean interpretation.
4.  **Clarity in Imputation:** The ground truth explicitly lists `cols_to_impute` and explains why `Pregnancies` and `Outcome` are excluded from 0-value treatment, which adds clarity. While the generated code gets it right, the explicit mention improves clarity.
5.  **Consistency in `pd.cut` `right` parameter:** The ground truth uses `right=True` for age groups and `right=False` for BMI categories, which aligns with common conventions for those types of bins (e.g., age groups typically `(a, b]`, while BMI categories might be `[a, b)`. The generated code consistently uses `right=False` for both, which is okay but less precise for the age groups, where `(0, 30]` might be more intuitive for ""under 30"". This is a minor point but demonstrates closer attention to detail in the ground truth.",o4_mini,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curv...",6,"The generated code attempts to address all requirements but falls short in several areas related to correctness, completeness, and code quality.

**Correctness & Completeness Issues:**

1.  **Missing Value Imputation Logic:** The ground truth correctly identifies `0` in specific columns as missing values and replaces them with the median *of the non-zero values*. The generated code uses `df[df[col] != 0][col].median()`, which is a correct approach for getting the median of non-zero values. However, the ground truth explicitly replaces `0`s with `NaN` first and then uses `fillna(median_val)` which is semantically clearer for ""imputation"". Both approaches achieve the same outcome for this specific task.
2.  **Feature Scaling & Encoding Pipeline:** This is a major functional gap.
    *   The ground truth code uses `ColumnTransformer` with `StandardScaler` for numerical features and `OneHotEncoder` for categorical/engineered features (`Age_group`, `Insulin_level`). This is the correct, robust way to handle mixed data types and prevent data leakage by fitting transformers only on training data.
    *   The generated code performs `StandardScaler` globally on `X_train` and `X_test` *after* one-hot encoding. This is problematic:
        *   It scales one-hot encoded features, which is incorrect and can harm model performance.
        *   It lacks proper integration into a pipeline, making it harder to ensure only training data is used for fitting transformers during cross-validation.
    *   The `ColumnTransformer` is entirely missing from the generated code, which is a critical piece for a robust data science pipeline.
3.  **Cross-Validation & Model Training Cycle:** The generated code's loop for `cv_scores` is inconsistent. It conditionally scales `X_train` for 'LogisticRegression' and 'SVM' but doesn't for tree-based models, which is unusual. While tree models are not always sensitive to scaling, it's best practice either to scale all numerical features (which the ground truth does via `ColumnTransformer` and then passes preprocessed data to models) or to explicitly handle when models should receive scaled/unscaled data. The current implementation creates a potential for error and inconsistency, especially given the lack of a proper preprocessing pipeline.
4.  **Hyperparameter Tuning `best_name` Logic:**
    *   The generated code selects the `best_name` based solely on `accuracy` from direct `cross_val_score` calls, which might not be the best strategy for selecting the ""best model"" for tuning (e.g., ROC-AUC might be preferred for imbalanced datasets). The ground truth specifically chooses Random Forest based on general performance/flexibility and tunes using `roc_auc`.
    *   The `param_grid` selection is very basic and limited compared to the ground truth's comprehensive search space.
    *   The `GridSearchCV` is applied only to the ""best model"" found. The task explicitly asks to ""Perform hyperparameter tuning for the best model,"" which implies refining one model. However, the ground truth performs comprehensive evaluation *before* tuning (as a comparison point) and then tunes the selected ""best"" model, re-evaluating it. The generated code's flow is a bit messy in combining these steps.
5.  **Evaluation Metrics Display and Overall Reporting:**
    *   The ground truth provides a clear split of ""Test Set Evaluation Results (Before Hyperparameter Tuning)"" and ""Final Test Set Evaluation Results (Including Tuned Random Forest)"", along with `classification_report` output, which is more comprehensive.
    *   The generated code prints metrics on a single line for each model inside a loop, which is less readable and harder to compare. It doesn't print classification reports.
6.  **Visualization:**
    *   The confusion matrices are plotted but lack `seaborn.heatmap` for better aesthetics and labeling (axis labels, tick labels). `imshow` is used directly, which is less user-friendly for confusion matrices.
    *   ROC curves are plotted, but again, with a basic `matplotlib` plot rather than `RocCurveDisplay` or a more polished `matplotlib/seaborn` approach.
    *   The visualizations are created within the evaluation loop, meaning multiple plots open unnecessarily. The ground truth separates plotting into dedicated sections after all evaluations are complete, which is better practice.
7.  **Feature Importance:**
    *   The generated code calculates and prints feature importance for ""RandomForest"" and ""GradientBoosting"". However, it passes `X_train` (unscaled and with original categorical features) back to `m.fit(X_train, y_train)` for importance calculation, even if the model was trained with scaled data during general evaluation or tuning. This is inconsistent and incorrect when scaling was applied.
    *   The generated code prints the `pd.Series` directly (`print(name, imp)`). The ground truth provides visualization (barplot) and then prints the top features, which is much better.

**Code Quality & Structure Issues:**

1.  **Imports:** Missing `ColumnTransformer` and `OneHotEncoder`, `seaborn`, `ConfusionMatrixDisplay`, `RocCurveDisplay`. The generated code uses basic matplotlib for plotting which is less effective for these specific needs.
2.  **Readability/Clarity:** The conditional scaling logic within the loop (`Xt = ...`, `Xv = ...`) is confusing and indicative of the missing `Pipeline`/`ColumnTransformer` structure.
3.  **Reproducibility:** A `random_state` is set for `train_test_split` and some models, but `np.random.seed(42)` is missing for overall run reproducibility.
4.  **Error Handling:** No `try-except` for file loading, which the ground truth includes.
5.  **Comments & Print Statements:** Less verbose and clear explanations in print statements compared to the ground truth, which aids in understanding the execution flow.

In summary, the generated code outlines the core steps but lacks the robustness, best practices, and completeness of a production-quality data science script, primarily due to the mishandling of preprocessing, the inconsistent approach to cross-validation & tuning, and less polished visualization/reporting.",o4_mini,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load and preprocess
df = pd.read_csv('diabetes.csv')
cols_...",7,"The generated code addresses most of the requirements of the task.

**Strong Points:**
*   Successfully loads and preprocesses the data by handling missing values (replacing 0s with NaN and then imputing with median).
*   Correctly implements the specified engineered features: `Metabolic_Score`, `Risk_Factor_Count` (with some difference in thresholds), `Glucose_per_Age`, `BMI_BloodPressure_interaction`, and `Pedigree_Age_product`.
*   Applies polynomial features of degree 2.
*   Performs both StandardScaler and MinMaxScaler.
*   Applies PCA and correctly prints explained variance ratios.
*   Implements SelectKBest and RFE for feature selection.
*   Applies SMOTE for handling class imbalance.
*   Evaluates feature importance using Random Forest and visualizes the top features.

**Areas for Improvement / Missing Aspects:**

*   **Polynomial Features Logic:** The polynomial feature implementation is slightly off. It applies polynomial features to *all* numerical columns (excluding 'Outcome') then concatenates by dropping *all* original numerical columns. The `df.drop(columns=numeric_cols)` will remove the *original* versions of the features that are now in `poly_arr` but also removes other numerical features that were not intended for polynomial transformation. This leads to information loss and incorrect feature set. The ground truth correctly applies polynomial features to a *selected subset* of numerical columns and then integrates them.
*   **Correlation Filtering Placement:** The correlation-based feature filtering is performed *before* scaling. It should ideally be performed after scaling, as scaling can sometimes affect correlation values slightly (though usually not dramatically). More importantly, the correlation filtering is applied to the *entire* dataframe (including polynomial features) but *before* splitting X and y, and importantly, before scaling. The ground truth performs this step after scaling, which is generally a better practice to ensure all numerical features are on a comparable scale.
*   **PCA Input:** PCA is applied to `X_std`, which is correct. However, subsequent feature selection methods (SelectKBest and RFE) are also applied to `X_std`. The correlation filtering step effectively modifies the feature set (by removing columns) *before* PCA and scaling. This makes the flow a bit confusing. The ground truth wisely applies correlation filtering **after** initial scaling. It then performs PCA on this filtered set. The feature selection methods (SelectKBest, RFE) are also applied to this correlation-filtered and scaled `X` (before PCA). This ensures a consistent feature set for these operations.
*   **Risk Factor Thresholds:** The `Risk_Factor_Count` thresholds in the generated code are slightly different from the ground truth (e.g., BloodPressure > 80 vs > 90, Insulin > 150 vs not in GT). This is a minor point but something to note for exact replication.
*   **Code Structure and Readability:** The ground truth code has numbered sections with print statements, which greatly enhances readability and understanding of the flow. The generated code lacks these informative comments and structured sections.
*   **Imports:** The generated code is missing `SimpleImputer` for handling missing values and `LogisticRegression` for RFE (if used as an estimator). It implicitly uses `df.fillna(df.median())` which is acceptable but less explicit than `SimpleImputer`.
*   **PCA `random_state`:** `random_state` for PCA is not a valid parameter. PCA is a deterministic algorithm.
*   **SMOTE Input:** SMOTE is applied to `X_std` directly. Since the correlation filtering occurred *before* scaling, `X_std` actually contains the *removed* columns. The code needs to ensure SMOTE is applied to the final, processed feature set after all transformations and selections.

Overall, the generated code demonstrates a good understanding of the required steps but has some logical flaws in the order and application of transformations (especially polynomial features and correlation filtering) that would lead to a different, potentially incorrect, final feature set.",o4_mini,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load dataset and handle missing values
df = pd.read_csv('world-happiness-report-2021.csv')
df = df.dropna()

alpha = 0.05

# 1) Western Europe vs Others t-test on happiness score
we = df[df['Regional indicator'] == 'Western Europe']['Ladder score']
others = df[df['Regional indicator'] != 'Western Europe']['Ladder score']
t1, p1 = stats.ttest_ind(we, others, equal_var=False)
print(f""1) Western Europe vs Others t-test p-va...",5,"The generated code has significant deficiencies in meeting the prompt's requirements, specifically regarding output and interpretation.

**Correctness:**
*   **Data Handling:** `df.dropna()` without specifying `subset` columns is a blunt approach and might drop too many rows unnecessarily, affecting statistical power. The ground truth carefully selects critical columns.
*   **T-test Interpretation:** While it correctly calculates p-values, it lacks the detailed interpretation of *direction* (e.g., ""Western European countries have significantly HIGHER happiness scores""). This is a critical part of hypothesis testing interpretation.
*   **Mann-Whitney U Test:** The implementation for the top and bottom 25% is incorrect. The prompt asks for ""top 25% happiest"" and ""bottom 25% happiest"". The generated code uses `>= q75` and `<= q25`, which correctly identifies the data for the groups, but the interpretation of *which* group is higher or lower is missing from the output. The ground truth sorts the DataFrame and explicitly defines the head/tail for clarity and robustness.
*   **ANOVA Interpretation:** It correctly states significance but fails to mention the need for post-hoc tests to identify *where* the difference lies, which is a standard and crucial part of ANOVA interpretation.
*   **Correlation Interpretation:** It prints 'r' but does not specify if the correlation is positive or negative in the ""significant"" message, which is a key part of interpreting correlation results.
*   **Chi-square Interpretation:** It lacks the interpretation of whether the variables are associated or independent, which is the core output of a chi-square test. Also, it doesn't print degrees of freedom or the chi-square statistic itself.
*   **Freedom/Generosity T-test:** Similar to the first t-test, it only indicates significance but not the direction (e.g., ""higher"" or ""lower generosity"").

**Code Quality:**
*   **Readability:** The output is very terse (e.g., `-> Significant`). The prompt explicitly asks for ""display p-values and interpret results with significance level =0.05"". The generated code's interpretations are minimal and lack the required detail.
*   **Output Formatting:** Missing clear headings for each test and detailed interpretive sentences as requested. The ground truth provides excellent structured output.

**Completeness:**
*   **Missing Interpretations:** This is the most significant drawback. The prompt explicitly states ""Display p-values and interpret results with significance level =0.05."" The generated code only provides a binary ""Significant"" or ""Not significant"" without explaining the implications of the finding (e.g., ""Western European countries have significantly higher happiness scores...""). This fails to meet the ""interpretation"" requirement.
*   **No ""Error: FileNotFoundError"" Handling:** The ground truth includes robust error handling for missing files, which is good practice for runnable scripts.
*   **No Initial Data Info:** The ground truth prints initial/final row counts after NaN handling, which is helpful for transparency.
*   **No explanation of `alpha`:** The `alpha` value is set but never explicitly printed to the console as the chosen significance level, which the ground truth does.

The generated code performs the statistical tests correctly for the most part but falls short on the crucial ""interpretation"" aspect of the prompt and overall output quality/completeness.",o4_mini,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings('ignore')

# 1) Load and preprocess
df = pd.read_csv...",7,"The generated code addresses most of the task requirements but has several significant issues that prevent it from being a fully correct and complete solution as per the prompt's specifications:

1.  **Preprocessing Issues (Critical):**
    *   **Incorrect Feature Names:** The prompt specifically mentions ""Logged GDP per capita"", ""Freedom to make life choices"", and ""Perceptions of corruption"" as they appear in the dataset, along with ""Social support"", ""Healthy life expectancy"", and ""Generosity"". The generated code uses simplified names like 'GDP per capita', 'Freedom', and 'Perceptions of corruption', which are *not* the exact column names in the provided `world-happiness-report-2021.csv` dataset. This will cause `KeyError` or result in the model being trained on only a subset of the intended features, or mismatched data. The ground truth explicitly handles this by using the correct, full column names.
    *   **Missing Value Handling:** The generated code uses `dropna` on a hardcoded list of feature names that are incorrect. This is directly tied to the point above.
    *   **Categorical Variable Encoding:** The prompt *doesn't* specify using 'Regional indicator' as a feature, nor does it ask for encoding any categorical variables *among the specified features*. The ground truth notes that the specified features are all numerical. The generated code incorrectly includes 'Regional indicator' in the features and performs one-hot encoding on it. While this isn't necessarily ""wrong"" in a general ML sense, it deviates from the specific set of features requested in the prompt. If 'Regional indicator' was intended to be used, it should have been explicitly listed as a feature in the prompt.

2.  **Hyperparameter Tuning:** While it performs `GridSearchCV`, the `param_grid` is very limited compared to what might be considered robust tuning. The prompt requires tuning for ""the *best* model"", implying the selection (or performance-based decision) of which model to tune, which the generated code hardcodes to Gradient Boosting without checking which one performed best initially. The ground truth does select based on initial performance.

3.  **Evaluation Metrics:** All specified metrics (MAE, MSE, RMSE, R) are calculated and displayed, which is good.

4.  **Visualizations:** Residual plots and feature importance plots are generated, meeting this requirement. `sns.residplot` is a good compact way to plot residuals.

5.  **Cross-Validation:** Cross-validation is implemented using `cross_val_score`, but it's applied to *all* models again, not just the best or tuned model, which is less efficient and not strictly what the prompt asked for ""to assess model stability"" (implying for the chosen good model). Also, it only shows R2 scores, not RMSE/MSE which were also requested for cross-validation in the ground truth for comprehensive assessment.

6.  **Prediction for New Data:** The structure for hypothetical countries is good, but the feature names for hypothetical data suffer from the same issue as the original feature names  they don't match the actual dataset's column names, leading to potential `KeyError` or incorrect predictions if the `X` columns were correctly mapped. The inclusion of `region_cols` set to 0 for hypothetical countries is an attempt to handle the OHE, but again, if 'Regional indicator' wasn't meant to be a feature, this is superfluous.

7.  **Code Quality/Clarity:**
    *   Imports are mostly appropriate.
    *   Magic numbers for feature columns are not ideal, though the ground truth does use a list, which is clearer.
    *   `warnings.filterwarnings('ignore')` is generally discouraged as it can hide important warnings.
    *   The `df = df.drop('Country name', axis=1)` line is a reasonable preprocessing step, but not explicitly requested by the prompt for the feature selection, though `Country name` is not used anyway.

In summary, the most significant flaw is the incorrect identification and usage of feature column names from the dataset, which would cause runtime errors or produce models based on incorrect data, failing the core task. The additional inclusion of 'Regional indicator' as a feature also deviates from the prompt's specific request.",o4_mini,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, mutual_info_regression, RFE
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load and preprocess
df = pd.read_csv('world-happiness-report-2021.csv')
df = df.dropna()

# ...",7,"The generated code addresses most of the task requirements, demonstrating a good understanding of feature engineering and dimensionality reduction techniques.

**Strong Points:**
*   **Feature Engineering:** Successfully created all requested engineered features (`Happiness_Efficiency`, `Social_Wellness_Index`, `Governance_Score`, `Regional_Happiness_Rank`, `GDP_vs_Regional_Average`).
*   **Polynomial Features:** Correctly applied polynomial features of degree 2 to the specified key happiness factors.
*   **Scaling:** Implemented all three specified scaling methods (StandardScaler, MinMaxScaler, RobustScaler).
*   **Dimensionality Reduction:** Applied PCA and t-SNE for dimensionality reduction and included basic visualizations.
*   **Feature Selection:** Used `SelectKBest` (with `mutual_info_regression`) and `RFE` for feature selection.
*   **Interaction Features:** Created the requested interaction features.
*   **Feature Importance:** Calculated and visualized Random Forest feature importances.
*   **Code Structure:** Generally well-structured with comments indicating the different steps.

**Areas for Improvement to achieve a higher score:**

1.  **Preprocessing Robustness:** The ground truth includes a `try-except` block for file loading and handles potential `NaN` values in 'Perceptions of corruption' specifically before general `dropna()`. The generated code uses a simple `df.dropna()`, which might not be robust enough for real-world datasets and could lead to data loss if `NaN`s are strategically located. It also doesn't explicitly handle `inf`/`-inf` values that can arise from division in engineered features, which the ground truth addresses.
2.  **`SelectKBest` usage:** The `SelectKBest` for `f_regression` was missed. Only `mutual_info_regression` was used.
3.  **Visualization Detail for DR:** While PCA and t-SNE are visualized, the target variable (`Ladder score`) isn't used for coloring the points, which is a key aspect of visualization to understand how the reduced dimensions relate to the target. The ground truth uses `hue=y` for this purpose.
4.  **Clarity for Scaling and Subsequent Steps:** The generated code scales the data but then doesn't explicitly state which scaled version (e.g., `scaled['standard']`) is used for subsequent steps like PCA, t-SNE, and feature selection (though it implies `scaled['standard']` for PCA). The ground truth is clearer by assigning `X_scaled = df_scaled_standard.copy()`.
5.  **`RFE` Base Estimator:** The ground truth specifies `LinearRegression` as the estimator for RFE, which is a common choice for interpretability. The generated code uses `RandomForestRegressor`, which is also valid but less common for illustrating RFE without a specific model goal in mind, and can be computationally heavier for just RFE. The prompt asked for ""using Recursive Feature Elimination"", so a basic estimator is generally preferred for this demonstration.
6.  **Correlation Filtering:** The ground truth's correlation filtering explicitly states features to be dropped, which adds transparency. The generated code drops them but doesn't print the list, which makes it harder to trace. Also, the prompt requested correlation-based feature filtering, implying the use of a threshold (which was set at 0.9 in the ground truth) to identify features to drop, ensuring redundancy reduction, which was correctly implemented.
7.  **Mutual Information Scoring Display:** While mutual information was used for `SelectKBest`, the prompt also asked for ""mutual information scoring"" (as a separate point), which the ground truth explicitly shows as a sorted list of all features with their scores. The generated code gets the scores but only prints the series without an illustrative `head()`.
8.  **Imports:** Some imports included in the ground truth for best practice (e.g., `LinearRegression`, `train_test_split`) are missing, even if not strictly used in all parts for the final output. The `f_regression` import was also missing.",o4_mini,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# 1) Load and filter
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
usa = df[df['location'] == 'USA']

# 2) Set date index
usa = usa.set_index('date')

# 3) Resample weekly
weekly = usa['new_cases'].resample('W').sum()

# 4) Decompose
decomp = season...",7,"The generated code addresses most of the requirements of the task correctly. It loads, filters, resamples, decomposes, and plots the time series appropriately. It also conducts the ADF test and applies differencing if needed, followed by ACF/PACF plots.

However, there are several areas for improvement:

1.  **Error Handling/Robustness**: The provided code lacks any error handling (e.g., for `FileNotFoundError` for the CSV, or `KeyError` if 'location' or 'new_cases' columns are missing, or if 'USA' does not exist in the data). The ground truth includes robust error handling, which is crucial for production-ready code.
2.  **Missing `warnings` import/handling**: The ground truth correctly suppresses `UserWarning` from matplotlib which can occur with `statsmodels` plots. The generated code does not.
3.  **Noisy Plotting**: The `decomp.plot()` method already creates a figure and axes. Calling `plt.figure()` *before* it will result in an empty figure being displayed in addition to the decomposition plot, which is redundant or confusing. The `suptitle` and `rect` adjustments present in the ground truth for better plot layout are also missing.
4.  **ADF Test Output**: The p-value interpretation and critical values are not printed as explicitly as in the ground truth, which makes the output less informative regarding stationarity conclusions.
5.  **`seasonal_decompose` `period` parameter**: While `period=52` is a reasonable first guess, the code doesn't handle cases where the series length is less than `2 * period`, which might lead to errors or warnings for `seasonal_decompose`. The ground truth includes a check and warning for this scenario.
6.  **ACF/PACF Lags**: The generated code uses a fixed `lags=20`. While often sufficient, the ground truth uses a more dynamic calculation (`min(50, len(stationary_series) // 2 - 1)`) to ensure the lags are appropriate for the length of the series, preventing potential issues with very short series.
7.  **Readability/Comments**: While the code is functional, it lacks descriptive comments for each step, which could improve readability. The ground truth code is encapsulated in a function and includes docstrings and more verbose comments.

Overall, it's a good attempt that covers the core logic but could be significantly improved in terms of robustness, error handling, and general best practices for production or sharable code.",o4_mini,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Load data
df = pd.read_csv('winequality-red.csv', sep=';')

# Categorize pH groups
df['pH_group'] = pd.cut(df['pH'],
                        bins=[-np.inf, 3.2, 3.4, np.inf],
                        labels=['Low', 'Medium', 'High'])

# Calculate average quality per group
means = df.groupby('pH_group')['quality'].mean()

# ANOVA test
groups = ['Low', 'Medium', 'High']
values = [df[df['pH_group'] == g...",7,"The generated code correctly addresses most of the requirements. It categorizes pH groups, calculates mean quality, performs an ANOVA test, and creates a visualization with error bars.

However, there are a few areas for improvement:

1.  **Confidence Interval Calculation:** The computation of confidence intervals (`cis = []` and the loop that follows) is incorrect. `stats.sem(q)` correctly gives the standard error of the mean, but `stats.t.ppf` is used to get the critical t-value. To calculate the 95% confidence interval, one typically uses `stats.t.interval` or manually calculates `mean +/- (t_critical * SEM)`. The current code calculates `h` which seems to be the margin of error, but then directly uses `cis` (which contains `h` values for each group) as `yerr` in `plt.bar`. This implies the bar plot is showing `mean +/- h`, but `h` itself is just the half-width of the CI, not the actual CI bounds or error bar lengths. The ground truth code correctly calculates `ci_lower` and `ci_upper` and then uses `mean - ci_lower` for the error bar length, which is more robust for asymmetric CIs, although for standard t-distribution CIs around a mean they often appear symmetric. The ground truth also correctly ensures error bars are relative to the mean, which is crucial for `yerr`.

2.  **Missing `include_lowest` and `right` in `pd.cut`:** The ground truth uses `include_lowest=True` and `right=False` for `pd.cut`. While `pd.cut`'s default `right=True` (meaning intervals are `(a, b]`) might work, for the specified ranges (e.g., ""3.2 to 3.4""), `[lower, upper)` intervals (`right=False`) are typically more intuitive (e.g. 3.2 is included in the medium bin). This is a minor subtlety but can affect edge cases.

3.  **Error Handling for File Loading:** The generated code assumes the file will always be present and directly reads it. The ground truth includes a `try-except FileNotFoundError` block, which is good practice for robustness.

4.  **No explicit reporting of Confidence Intervals:** While they are used in the plot, the ground truth code explicitly prints the calculated confidence intervals, which is useful for direct inspection.

5.  **Hardcoded separator:** `sep=';'` is used when reading the CSV, which might not always be the case for `winequality-red.csv` (often comma-separated). The ground truth assumes comma-separated (default for `pd.read_csv`).

To get a higher score, the main improvements would be in the confidence interval calculation and the explicit printing of CI bounds, along with basic error handling.",o4_mini,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# 1. Load dataset
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2. Select features and target
features = ['new_cases', 'h...",8,"The generated code largely addresses all requirements of the task.

**Strong Points:**
*   **Correctness:** The code correctly loads data, selects features, handles missing values (though with a slight deviation in approach from the ground truth, it achieves the desired outcome), encodes categorical features, scales numerical features, splits data, trains both Linear Regression and Random Forest models, and evaluates them using the specified metrics.
*   **Completeness:** All steps outlined in the task description are implemented.
*   **Code Quality:** Imports are proper, and the code is structured logically with comments indicating the steps.

**Areas for Improvement (Why not a 10):**
*   **Imputation Strategy:** The generated code uses `SimpleImputer` on `df[features]` and `df[[target]]` separately. While this works, it does not respect the original DataFrame structure, and `X_num` immediately becomes a NumPy array, losing column names which could potentially be an issue if not handled carefully (though `pd.DataFrame(imputer_X.fit_transform(df[features]), columns=features)` mitigates this). The ground truth's approach of imputing directly on a pre-selected `df_selected` DataFrame in-place is often more robust for maintaining DataFrame integrity and context, especially if there were future operations referencing the original column names. Also, `SimpleImputer` implicitly transforms the data into a NumPy array, which means creating `X_num` and `y` from the imputed arrays then converting `X_num` back to a DataFrame. The ground truth's method using `df_selected[col].fillna(median_val, inplace=True)` is more ""pandas-native"" for this specific task and avoids unnecessary type conversions.
*   **Scaling Order:** The generated code scales `X` *before* the `train_test_split`. This is a common mistake and leads to data leakage. The `StandardScaler` is fitted on the *entire* dataset `X`, meaning information from the test set is used to determine the scaling parameters (mean and standard deviation). The correct practice, as demonstrated in the ground truth, is to fit the scaler *only* on the training data and then transform both training and testing data. This ensures that the model's performance on the test set is a true reflection of its generalization ability. This is the most significant flaw.
*   **""continent"" Feature Inclusion:** The problem statement explicitly asks to ""Encode the 'continent' column using one-hot encoding and add it to the features."" The generated code correctly does this (`X = pd.concat([X_num, continent_dummies], axis=1)`), but the initial `features` list does not include `continent`. While `X_num` uses the initial `features` list and then `continent_dummies` are concatenated, it implicitly adds continent, so the logic is correct in the end. However, for clarity and strict adherence to the request, including 'continent' in the initial `features` list might be slightly better, even if it's handled separately for imputation. The ground truth handles this more explicitly by including `continent` in `features` and then excluding it from median imputation, which is a cleaner separation of concerns.",o4_mini,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"import pandas as pd
import numpy as np
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols

# 1) Load data and impute missing values
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
cols = ['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']
for col in cols:
    df[col].fillna(df[col].median(), inplace=True)

# 2) ANOVA: reproduction_rate across continents
df_cont = df[df['continent'].isin(...",7,"The generated code correctly implements the core statistical tests (ANOVA, Pearson, T-test) and handles the data manipulation steps reasonably well.

**Strong Points:**
*   Correctly loads data and performs median imputation.
*   Correctly identifies the continents for ANOVA.
*   Correctly calculates the death rate ratio and the 75th percentile for the t-test.
*   Uses appropriate statistical functions for each test.
*   Clearly states null hypotheses and conclusions based on p-values.

**Areas for Improvement:**
*   **Missing Value Handling in T-test:** The `nan_policy='omit'` in `ttest_ind` for the t-test handles NaNs by dropping them. While this is acceptable for the t-test, the task explicitly asked for median imputation for 'median_age' *prior* to this step. The initial imputation handles this generally, but ensuring all data used in the comparison (especially median\_age for high/low death rate countries after the sorting/grouping) is handled is important. The ground truth explicitly drops NaNs again on 'median\_age' before passing to t-test, which is a safer practice.
*   **Robustness of ANOVA:** The generated code uses `statsmodels.formula.api.ols` and `sm.stats.anova_lm`. While this is a valid way to perform ANOVA, it's slightly more complex than `scipy.stats.f_oneway` and might be overkill for a simple one-way ANOVA with predefined groups. More importantly, it doesn't explicitly check for groups having data, which `f_oneway` in the ground truth handles gracefully. If one of the continents (`C(continent)`) had no data or all NaNs for `reproduction_rate`, `ols` might throw an error or produce unexpected results.
*   **Clarity in Output:** While conclusions are stated, the output is a bit terse. Adding context like the calculated F-statistic or correlation coefficient would make the output more informative.
*   **Error Handling:** The code lacks error handling for file not found (though the benchmark setup usually provides the file, in a real-world scenario it's crucial).
*   **Redundant Import:** `statsmodels.api as sm` and `statsmodels.formula.api as ols` are imported, but only `ols` is strictly used by itself, the `sm.stats.anova_lm` part is fine. It's a minor point.
*   **Task Misinterpretation in T-test group names:** The task mentions 'high_death_rate_country' and 'others'. The generated code uses `group_high` and `group_low`. While logically `group_low` represents 'others' in this context (countries not above the 75th percentile), explicitly mentioning 'others' in the code or comments would align perfectly with the prompt. Also, it uses 'location' for groupby which implies country. Using 'country' might align better with variable names from the dataset.

Overall, the code is functional and largely correct, but could be improved in terms of robustness, error handling, and output clarity to achieve a higher score.",o4_mini,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('clash_royale_cards.csv')
cols = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
df_selected = df[cols]
corr = df_selected.corr()

plt.figure(figsize=(8, 6))
sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()
```",9,"The generated code successfully loads the dataset, selects the specified numerical columns, calculates their pairwise correlation matrix, and visualizes it as a heatmap, fulfilling all the core requirements of the task. The imports are correct, and the structure is straightforward and functional.

Minor improvements could include:
1. Adding error handling for `FileNotFoundError` in `pd.read_csv`.
2. Making the `DATASET_PATH` and `NUMERICAL_COLUMNS` explicit constants for better readability and maintainability, similar to the ground truth.
3. Slightly more detailed `plt.title` for the heatmap.
4. While not strictly required by the prompt, adding `fmt="".2f""` to the `sns.heatmap` for consistent annotation formatting (as in the ground truth) is good practice.

Overall, it's a very good and concise solution.",o4_mini,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"import pandas as pd

df = pd.read_csv('clash_royale_cards.csv')

avg_stats = df.groupby('rarity')[['elixirCost', 'Win Rate', 'Usage']].mean()

print(avg_stats)",10,"The generated code is functionally identical to the ground truth code and correctly addresses all aspects of the task. It properly loads the CSV, groups by `rarity`, calculates the average `elixirCost`, `Win Rate`, and `Usage`, and displays the results. The code is concise and efficient.",o4_mini,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

df = pd.read_csv('clash_royale_cards.csv')
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)

X = df[['elixirCost', 'Usage', 'rarity']]
y = df['is_high_win_rate']

encoder = OneHotEncoder(sparse=False)
rarity_encoded = encoder.fit_transform(X[['rarity']])
rarity_cols = encoder.get_feature_names_out(['rarity'])
rarity_df = pd.DataFrame(rarity_encoded, column...",9,"The generated code correctly performs all the required steps: loading the data, creating the target column, selecting features, one-hot encoding `rarity`, and splitting the data. The use of `OneHotEncoder` directly is a valid approach, although `pd.get_dummies` is often simpler for this specific case.

**Strong points:**
- All requirements are met.
- Proper imports are included.
- The `OneHotEncoder` is used correctly with appropriate steps to get feature names and concatenate back to the DataFrame.
- The train/test split parameters are correct.

**Areas for minor improvement (why it's not a perfect 10):**
- The ground truth code included error handling for `FileNotFoundError`. While not strictly required by the prompt's output format, it's a good practice for robust code.
- The `OneHotEncoder` approach, while correct, is slightly more verbose than `pd.get_dummies` for simple one-hot encoding within a DataFrame, and the prompt implies minimal code. However, it's a perfectly valid and often preferred method in larger pipelines.
- The initial `X = df[['elixirCost', 'Usage', 'rarity']]` line doesn't use `.copy()`, which could lead to `SettingWithCopyWarning` if subsequent modifications were made directly to `X` that pandas thought might affect `df`. For this specific code, it doesn't cause functional issues, but `.copy()` is a good habit.",o4_mini,LLM_judge_config
M25,medium,Classification,iris.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier

df = pd.read_csv(""iris.csv"")
X = df[[""SepalLengthCm"", ""SepalWidthCm"", ""PetalLengthCm"", ""PetalWidthCm""]]
y = df[""Species""]
le = LabelEncoder()
y = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_tra...",9,"The generated code correctly performs all the steps outlined in the task: loading data, defining features and target, encoding the target, splitting the data, training a KNN classifier, and reporting accuracy.

Strong points:
- All required imports are present and correctly used.
- Data loading and feature/target selection are accurate.
- Label encoding is correctly applied.
- Data splitting parameters (test_size, random_state) match the requirements.
- KNN classifier initialization (n_neighbors) is correct.
- Training and prediction are correctly performed.
- Accuracy calculation using `knn.score()` is a valid and concise way to get the accuracy.

Areas for minor improvement (not critical errors, but good practice):
- The ground truth code includes `stratify=y_encoded` in `train_test_split`. While not explicitly requested in the prompt, it's generally good practice for classification tasks, especially with imbalanced datasets, to ensure that class proportions are maintained in the train and test splits. The generated code omits this.
- The ground truth code includes error handling for `FileNotFoundError` when loading the CSV, which makes it more robust. The generated code does not include this, but it wasn't a strict requirement in the prompt.
- The output format for accuracy is slightly different (e.g., formatting to 4 decimal places in the ground truth), but the core value is the same.",o4_mini,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy import stats

# Null hypothesis: The mean PetalLengthCm of Iris-versicolor equals that of Iris-virginica
# Alternative hypothesis: The mean PetalLengthCm of Iris-versicolor is different from that of Iris-virginica

# 1) Load the dataset
df = pd.read_csv('iris.csv')

# 2) Filter the data for the two species
versicolor = df[df['Species'] == 'Iris-versicolor']['PetalLengthCm']
virginica = df[df['Species'] == 'Iris-virginica']['PetalLengthCm']

# 3) Perform a...",7,"The generated code correctly performs the core task of loading data, filtering, and conducting a t-test. It also correctly interprets the p-value against the given alpha. However, it falls short in completeness and explicitly stating the hypotheses as requested.

**Strengths:**
- Correctly uses pandas to load the dataset.
- Correctly filters the data for the two specified species and the PetalLengthCm column.
- Correctly uses `scipy.stats.ttest_ind` to perform the t-test.
- Prints the t-statistic and p-value with appropriate formatting.
- Correctly interprets the p-value based on alpha = 0.05.

**Areas for Improvement (to achieve a higher score):**
- **Hypotheses Statement (Task Requirement 4):** The task explicitly asked to ""Clearly state the null and alternative hypotheses for this test."" The generated code includes them as a comment block, which is a good effort but does not fulfill the spirit of ""clearly state"" as an output. The ground truth prints these hypotheses to the console, which is a more complete fulfillment of the requirement.
- **Error Handling (Implicit Requirement for Robustness):** The ground truth includes basic error handling for `FileNotFoundError` and empty groups, which makes the code more robust. While not explicitly mentioned as a scoring criterion, in an ""expert"" evaluation context, this is a sign of higher quality production-ready code.
- **Clarity of Output:** The output formatting could be improved for better readability, similar to the ground truth which uses descriptive print statements and separators.",o4_mini,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('iris.csv')

features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
for feature in features:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='Species', y=feature, data=df)
    plt.title(f'Box Plot of {feature} by Species')
    plt.xlabel('Species')
    plt.ylabel(feature)
    plt.tight_layout()
    plt.show()
```",9,"The generated code successfully loads the iris dataset and generates four box plots, one for each numerical feature, grouped by `Species`. Each plot has an appropriate title, and the code uses common libraries (`pandas`, `matplotlib`, `seaborn`) correctly.

**Strengths:**
- **Correctness:** The code correctly performs all the requested tasks.
- **Completeness:** It addresses all the specified requirements: loading data, generating 4 box plots, grouping by Species, and adding titles.
- **Conciseness:** The code is quite concise while remaining readable.
- **Appropriate Libraries:** Uses `pandas` for data handling, `matplotlib.pyplot` and `seaborn` for plotting effectively.

**Areas for Improvement (Minor):**
- **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions, which is good practice for robust code, especially when dealing with file I/O. The generated code lacks this.
- **Readability/Configuration:** The ground truth separates configurations (like `NUMERICAL_FEATURES`, `GROUPING_COLUMN`) into constants, which can improve readability and maintainability for larger projects. While not strictly necessary for this small task, it's a good habit.
- **Plot Customization:** The ground truth adds minor aesthetic improvements like adding a grid and adjusting font sizes for titles and labels, which makes the plots slightly more polished.

Despite these minor points, the generated code fully delivers on the core requirements of the task.",o4_mini,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

df = pd.read_csv('heart.csv')
X = df[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']]
y = df['HeartDisease']

preprocessor = ColumnTransformer(
    transformers=[
        ('onehot', OneHotEncoder(sparse=False), ['Sex', 'ChestPainType'])
    ],
    r...",8,"The generated code correctly addresses most of the requirements. It loads the data, selects the specified features and target, performs one-hot encoding on `Sex` and `ChestPainType` using `ColumnTransformer`, splits the data with the correct proportions and `random_state`, trains a Logistic Regression model, and reports the accuracy.

**Strong points:**
- Correctly identifies and preprocesses categorical features.
- Uses `ColumnTransformer` for robust preprocessing.
- Correctly splits the data as per specifications.
- Utilizes the `LogisticRegression` model.
- Uses `model.score()` which is a concise way to get accuracy for classification tasks, though `accuracy_score` is often preferred for explicit metric calculation.

**Areas for improvement to achieve a higher score:**
- **Missing `Pipeline`:** The ground truth uses `Pipeline` which is a good practice for combining preprocessing and modeling steps. This ensures that the preprocessing steps are consistently applied to new data (e.g., during prediction). While the generated code works by applying `fit_transform` before `train_test_split`, it breaks the flow if, for example, `X_test` needs to be transformed later for prediction on new, unseen data, or if you want to save and load the entire model. A pipeline is more robust for deployment and model management.
- **`sparse=False` in `OneHotEncoder`:** While `sparse=False` is used,
  the default for `OneHotEncoder` changed to `sparse_output=True` in scikit-learn 1.2. The use of `sparse=False` (or `sparse_output=False` in newer versions) is generally good practice when working with `ColumnTransformer` and subsequent models that expect dense input. It's a common explicit setting, but it's not strictly required by the prompt, as the code works without it.
- **Explicit `accuracy_score` import/usage:** While `model.score()` works, explicitly importing `accuracy_score` from `sklearn.metrics` and using it is generally considered better practice in data science benchmarks for clarity and for showcasing that the user knows how to calculate a specific metric.
- **`max_iter` for `LogisticRegression`:** The prompt didn't specify `max_iter`, but the ground truth sets `max_iter=1000` to prevent potential convergence warnings, which is a good practical improvement often needed for Logistic Regression. The generated code uses default parameters, which is also technically correct per the prompt.

Overall, it's a solid solution that correctly fulfills the task requirements and is highly functional. The lack of a `Pipeline` is the most significant difference from best practices demonstrated in the ground truth.",o4_mini,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Null hypothesis (H0): mean Cholesterol for males = mean Cholesterol for females
# Alternative hypothesis (H1): mean Cholesterol for males != mean Cholesterol for females

# 1) Load the dataset
df = pd.read_csv('heart.csv')

# 2) Create two groups based on Sex
male_chol = df[df['Sex'] == 'M']['Cholesterol']
female_chol = df[df['Sex'] == 'F']['Cholesterol']

# 3) Perform independent two-sample t-test
t_stat, p_val = ttest_ind(male_...",7,"The generated code correctly performs the core task of loading data, separating groups, and running a t-test. It also correctly states the hypotheses and interprets the p-value.

However, it lacks some robustness and completeness compared to the ground truth:
- **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions when loading the CSV, making it more robust. The generated code does not handle this.
- **Hypotheses Stating:** While the hypotheses are commented, the ground truth explicitly prints them out as part of the output, which was a specific requirement (""4) Clearly state the null and alternative hypotheses for this test."").
- **Input Validation:** The ground truth checks for the existence of required columns and sufficient data in each group before performing the t-test, which adds to its robustness.
- **Function Encapsulation:** The ground truth wraps the logic in a function `perform_cholesterol_t_test`, making it reusable and more modular, which is good practice for more complex scripts.
- **Handling NaNs:** Both codes correctly handle NaNs (`nan_policy='omit'` in generated, `.dropna()` in ground truth), but the ground truth explicitly shows the number of samples after dropping NaNs, which can be helpful for debugging or understanding the data used.
- **Clarity of Output:** The ground truth provides more verbose and clear output, explaining the interpretation explicitly with the p-value and alpha values.

Overall, it's a functional solution that meets most requirements but could be improved significantly by adding error handling, explicitly printing hypotheses, and encapsulating the logic in a function for better reusability and robustness.",o4_mini,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('winequality-red.csv')

# Columns to check
cols = ['volatile acidity', 'residual sugar']

# Identify outliers using IQR
outlier_counts = {}
bounds = {}

for col in cols:
    q1 = df[col].quantile(0.25)
    q3 = df[col].quantile(0.75)
    iqr = q3 - q1
    lower = q1 - 1.5 * iqr
    upper = q3 + 1.5 * iqr
    bounds[col] = (lower, upper)
    outlier_counts[col] = ((df[col] < lower) | (df[col] > upper)).sum()

# Report ...",9,"The generated code correctly implements all the core requirements of the task: identifying outliers using IQR, reporting their count, winsorizing the outliers, and comparing statistics before and after treatment.

**Strong Points:**
- Correctly calculates Q1, Q3, IQR, and the 1.5*IQR bounds.
- Accurately identifies and counts outliers.
- Effectively uses `clip()` for winsorization, which is a concise and idiomatic pandas way to handle capping.
- Compares mean and standard deviation before and after treatment for the specified columns.
- Uses appropriate imports.

**Minor Improvements/Observations:**
- **Robustness (File Not Found):** The ground truth includes a `try-except FileNotFoundError` block, which makes the script more robust if the CSV file is not present. The generated code assumes the file exists, which is acceptable for a benchmark task but less robust for production code.
- **Output Clarity/Formatting:** The output for comparing statistics could be slightly clearer, perhaps by labeling ""Before"" and ""After"" explicitly rather than combining within one string. However, the current output is understandable and provides the required information.
- **Explicit Steps:** The ground truth breaks down the process with print statements for each step (e.g., ""--- 1. Load the dataset ---""), which wasn't explicitly asked for but does improve readability and user understanding. The generated code is more concise, which is also a valid approach.

Overall, the generated code is highly effective and directly addresses the problem statement. The minor areas for improvement relate more to robustness and output verbosity rather than core correctness or functionality.",o4_mini,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('heart.csv')

# Create grouped bar chart
plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='ChestPainType', hue='HeartDisease', palette='Set1')

# Customize plot
plt.title('Heart Disease Counts by Chest Pain Type')
plt.xlabel('Chest Pain Type')
plt.ylabel('Count')
plt.legend(title='HeartDisease', labels=['No (0)', 'Yes (1)'])

# Show plot
plt.tight_layout()
plt.show()
```",8,"The generated code correctly loads the dataset and creates a grouped bar chart as requested. It uses `seaborn.countplot` effectively to display the counts of `HeartDisease` for each `ChestPainType`, which is the core requirement. The plot has a title, labeled axes, and a legend, satisfying the prompt's requirements.

Areas for improvement:
1.  **Robustness**: The provided code directly attempts to load the CSV without error handling for `FileNotFoundError` or other potential issues, which is present in the ground truth.
2.  **Code Structure**: The ground truth encapsulates the functionality within a function, making it more modular and reusable. The generated code is more of a script.
3.  **Legend Labels**: While `labels=['No (0)', 'Yes (1)']` in the legend is a good attempt, directly mapping integer values (0 and 1) to more descriptive labels (e.g., 'No Heart Disease', 'Heart Disease') as done in the ground truth is generally clearer and more flexible for categorical data.
4.  **No `if __name__ == '__main__':` block**: The ground truth wraps execution in a `if __name__ == '__main__':` block, which is good practice for Python scripts.
5.  **No explicit `ChestPainType` order**: The ground truth explicitly defines the order of `ChestPainType` categories, ensuring consistent plotting even if data order changes. For this specific task it might not be critical, but it's a good practice for categorical plots.",o4_mini,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"import pandas as pd

df = pd.read_csv('spotify_2023.csv')

df['in_shazam_charts'] = df['in_shazam_charts'].str.replace(',', '', regex=False)
df['in_shazam_charts'] = pd.to_numeric(df['in_shazam_charts'], errors='coerce')
nan_count = df['in_shazam_charts'].isna().sum()
print(f""NaNs in in_shazam_charts: {nan_count}"")

grouped = df.groupby(['key', 'mode'])[['danceability_%', 'valence_%']].mean()
print(grouped)",9,"The generated code is highly correct and complete, addressing almost all requirements.

**Strong Points:**
- **Correct CSV Loading:** Successfully loads the `spotify_2023.csv` dataset.
- **Correct Cleaning and Conversion:** Properly removes commas from `in_shazam_charts` and converts it to numeric, coercing errors to NaN.
- **Correct NaN Reporting:** Accurately reports the count of NaNs in the cleaned column.
- **Correct Grouping and Aggregation:** Correctly groups the data by `key` and `mode` and calculates the average `danceability_%` and `valence_%`.
- **Concise and Readable:** The code is well-structured and easy to understand.
- **Proper Imports:** Includes the necessary `pandas` import.

**Minor Improvements:**
- **Robustness for `in_shazam_charts` cleaning:** The ground truth code `df['in_shazam_charts'].astype(str).str.replace(',', '')` is slightly more robust as it explicitly converts the column to string first before `str.replace()`. While in many cases pandas might handle this implicitly, explicit conversion ensures the `.str` accessor always works as expected, particularly if the column has a mixed data type or is initially considered numeric. The generated code implicitly relies on `str.replace` working on the current dtype which might fail if the column is an integer type.
- **Output Clarity:** The ground truth provides more descriptive print statements (e.g., ""Average 'danceability_%' and 'valence_%' by 'key' and 'mode':""). While not strictly required by the prompt, it improves user experience.
- **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice for loading files. The prompt didn't explicitly ask for this, but it's a valuable addition for production-ready code.

Overall, the generated code performs admirably and meets the core requirements of the task. The issues are minor best practices rather than functional errors.",o4_mini,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

df = pd.read_csv('spotify_2023.csv')
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams', 'in_spotify_playlists', 'energy_%'])

X = df[['in_spotify_playlists', 'energy_%']]
y = df['streams']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_stat...",9,"The generated code correctly implements all specified requirements: loading the dataset, converting `streams` to numeric and handling NaNs in relevant columns, selecting features and target, splitting data, training a Linear Regression model, and evaluating it with R-squared and MSE. The imports are correct, and the code is structured logically.

The only minor area for improvement that prevents a perfect 10 is the lack of error handling for `FileNotFoundError` when loading the CSV, which was present in the ground truth. While not explicitly asked in the prompt, it's good practice for robust code, especially for an expert code evaluator. Also, the output messages are minimal compared to the ground truth, but the prompt only asked to ""report"" the R2 and MSE, which it does.",o4_mini,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"import pandas as pd

df = pd.read_csv('spotify_2023.csv')
df['release_date'] = pd.to_datetime(df[['released_year', 'released_month', 'released_day']], errors='coerce')
ref_date = pd.to_datetime('2023-12-31')
df['days_since_release'] = (ref_date - df['release_date']).dt.days.astype('Int64')

print(df[['track_name', 'release_date', 'days_since_release']].head(5))",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, efficiently creates the `release_date` column by combining year, month, and day and correctly uses `errors='coerce'` to handle invalid dates. It then accurately calculates `days_since_release` using the specified fixed date and converts it to the correct integer type (`Int64`). Finally, it displays the required columns for the first 5 tracks.

The strong points are its conciseness and efficiency in achieving the desired outcome. The use of `df[['released_year', 'released_month', 'released_day']]` directly in `pd.to_datetime` is a clean way to construct the date.

One minor point for improvement, not strictly a correctness issue but a best practice: the ground truth includes robust error handling for file loading, which is good practice for real-world scenarios. The generated code assumes the file will always be present, which is acceptable for a benchmark but worth noting for robust applications. Additionally, the ground truth explicitly mentions ""proper imports"" and ""display only the Python code without explanations"", and the generated code correctly follows this.",o4_mini,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

df = pd.read_csv('youtube_trendings.csv')

for col in ['Video views', 'Likes']:
    df[col] = (
        df[col]
        .astype(str)
        .str.replace(',', '', regex=False)
        .pipe(pd.to_numeric, errors='coerce')
        .fillna(0)
    )

df['Category'] = df['Category'].fillna('Unknown')

result = df.groupby('Category')[['Video views', 'Likes']].sum()
print(result)
```",9,"The generated code correctly implements all the requirements of the task. It properly loads the CSV, cleans the 'Video views' and 'Likes' columns by removing commas, converting them to numeric types (treating errors as NaN), and filling NaNs with 0. It also correctly fills missing 'Category' values with 'Unknown'. Finally, it groups the data by 'Category' and calculates the sum of cleaned 'Video views' and 'Likes'.

The use of `pipe(pd.to_numeric, errors='coerce')` is a concise and effective way to apply the numeric conversion. The code is clean, readable, and directly addresses the problem.

The only minor area for improvement that prevents a perfect 10 is the lack of error handling for file not found or if expected columns are missing, which the ground truth includes, but these were not explicitly requested as part of the core task logic. For a competitive programming scenario or a typical data science task where the data structure is known, the submitted solution is very robust.",o4_mini,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# 2) Clean 'Likes' column
df['Likes'] = df['Likes'].str.replace(',', '', regex=False)
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce').fillna(0)

# 3) Fill missing 'Category' values
df['Category'] = df['Category'].fillna('Unknown')

# 4) Create two groups: Music and Sports
likes_music = df.loc[df['Category'] == 'Music', 'Likes']
likes_sports = df.loc[df['Category'] == ...",8,"The generated code correctly performs all the core steps required by the task: loading the dataset, cleaning the 'Likes' column, filling missing 'Category' values, creating the two groups, performing the t-test, stating hypotheses, printing results, and interpreting them.

Strengths:
- **Correctness**: The core logic for data cleaning, group creation, and t-test execution is correct.
- **Conciseness**: The code is more concise than the ground truth while still achieving the objectives.
- **Hypotheses**: The hypotheses are correctly stated.
- **Interpretation**: The interpretation of the p-value against alpha is correct.

Areas for improvement to reach a higher score:
- **Robustness (File Loading)**: The ground truth includes a `try-except FileNotFoundError` block, which makes the code more robust if the CSV file is not present. The generated code assumes the file exists.
- **Clarity in Cleaning (`Likes` column)**: The ground truth creates a new column `Likes_cleaned`, which is good practice as it preserves the original 'Likes' column (if needed) and makes the cleaned data explicit. The generated code overwrites the original 'Likes' column. While acceptable, creating a new column is often preferred for data manipulation traceability.
- **T-test `equal_var` parameter**: The ground truth explicitly sets `equal_var=False` for `ttest_ind`, performing Welch's t-test which is generally safer when homogeneity of variance cannot be assumed. The generated code uses the default `equal_var=True`, which assumes equal variances. While often sufficient, the prompt implies a robust analysis, and Welch's t-test is more robust to violations of this assumption. This was a subtle but important detail.
- **Output Verbosity**: The ground truth provides more verbose output messages (`Dataset loaded successfully.`, `'Likes' column cleaned...`, etc.), which is helpful for debugging and understanding script progression, especially in a benchmark evaluation context. The generated code is more minimal in its output.
- **Edge Cases for T-test**: The ground truth includes a check for `len(music_likes) < 2 or len(sports_likes) < 2` to prevent errors if there isn't enough data for a t-test in one or both groups. While perhaps not strictly required by the prompt, it adds robustness.",o4_mini,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean Likes and Dislikes columns
df['Likes'] = pd.to_numeric(df['Likes'].astype(str).str.replace(',', ''), errors='coerce').fillna(0)
df['Dislikes'] = pd.to_numeric(df['Dislikes'].astype(str).str.replace(',', ''), errors='coerce').fillna(0)

# Feature engineering
df['TotalEngagement'] = df['Likes'] + df['Dislikes']
df['LikeDislikeRatio'] = df['Likes'] / (df['Dislikes'] + 1)

# Display results
print(df[...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, performs the necessary cleaning on 'Likes' and 'Dislikes' columns (removing commas, converting to numeric, filling NaNs with 0), creates the `TotalEngagement` and `LikeDislikeRatio` columns, and displays the required output. The chaining of operations for cleaning 'Likes' and 'Dislikes' is efficient and good practice.

The only minor area for improvement that prevents a perfect 10 is the lack of robust error handling for file loading (e.g., `FileNotFoundError`), which was present in the ground truth. While not explicitly asked for in the prompt's requirements for the output, it's a good practice for production-ready code. However, given the prompt's focus on the feature engineering steps, the core task is perfectly executed.",o4_mini,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd

# Load dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean 'Launched Price (USA)' column
df['Launched Price (USA)'] = (
    df['Launched Price (USA)']
    .str.replace('USD ', '')
    .str.replace(',', '')
)
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'], errors='coerce')

# Clean 'Screen Size' column
df['Screen Size'] = df['Screen Size'].str.replace(' inches', '')
df['Screen Size'] = pd.to_numeric(df['Screen Size'], errors='coerce')

# Gro...",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans both the `Launched Price (USA)` and `Screen Size` columns by removing specified strings and converting them to numeric, handling errors by coercing to NaN. It then correctly groups the data by `Company Name` and `Launched Year` and calculates the average of the cleaned columns. The final aggregated data is displayed as requested.

The main difference from the ground truth is that the generated code reuses the original column names for the cleaned numeric data, which is acceptable and often more concise. The ground truth creates new columns (`Cleaned Price (USD)` and `Cleaned Screen Size (inches)`). Both approaches are valid. The generated code is also slightly more concise in its chaining of string operations and lacks explicit `try-except` blocks for file loading, but the prompt only asked for ""proper imports"" and ""assume the CSV file is in the current directory,"" which it does.
The use of `regex=False` in `str.replace` in the ground truth is a good practice for literal string replacements, which is missing in the generated code but doesn't cause functional issues here as the patterns are simple literal strings. Overall, it's a very good solution.",o4_mini,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"import pandas as pd
import re

def clean_camera(val):
    if pd.isnull(val):
        return 0
    m = re.search(r'(\d+)', str(val))
    return int(m.group(1)) if m else 0

df = pd.read_csv('mobiles_2025.csv')
df['Cleaned Front Camera'] = df['Front Camera'].apply(clean_camera)
df['Cleaned Back Camera'] = df['Back Camera'].apply(clean_camera)
df['Total Camera MP'] = df['Cleaned Front Camera'] + df['Cleaned Back Camera']
print(df[['Model Name', 'Cleaned Front Camera', 'Cleaned Back Camera', 'Total ...",9,"The generated code successfully completes all aspects of the task: it loads the dataset, correctly defines and applies the camera cleaning function (including handling `NaN` and `None` values and extracting the first digit sequence), calculates the `Total Camera MP`, and displays the requested columns for the first 10 rows. The imports are correct, and the structure is clean and concise.

The only minor area for improvement is the lack of a `try-except` block for file loading and print statements for user feedback, which were present in the ground truth for robustness and clarity. However, these were not explicitly requested as requirements in the prompt, so their absence does not detract significantly from the core functionality.

Strong points:
- Correct and efficient use of pandas for data loading and column application.
- Accurate regex for extracting the first numerical value.
- Correct handling of missing or non-matching values (returning 0).
- Successfully creates the new derived feature.
- Displays the output as requested.",o4_mini,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# 1) Load dataset
df = pd.read_csv('mobiles_2025.csv')

# 2) Clean columns
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', '', regex=False), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', '', regex=False), errors='coerce')
df['Screen Size'] = pd.to_nu...",10,"The generated code correctly addresses all requirements of the task. It loads the dataset, cleans the specified columns correctly by removing text and converting to numeric (handling errors by coercing to NaN and then dropping rows with NaNs in those columns), defines features and target, splits the data, trains a Linear Regression model, and reports R-squared and MAE. The imports are correct, and the code structure is clear and concise. It provides a complete and accurate solution to the problem. The specific cleaning steps using `.str.replace` followed by `pd.to_numeric` with `errors='coerce'` and then `dropna(subset=...)` is exactly what was required.",o4_mini,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"import pandas as pd
import numpy as np
from scipy import stats

df = pd.read_csv('titanic.csv')
alpha = 0.05

# Chi-square test
ct = pd.crosstab(df['Pclass'], df['Survived'])
chi2, p_chi, dof, expected = stats.chi2_contingency(ct)
n = ct.values.sum()
r, k = ct.shape
cramers_v = np.sqrt(chi2 / (n * (min(r, k) - 1)))
chi_conclusion = 'reject H0' if p_chi < alpha else 'fail to reject H0'
print(f""Chi-square: chi2={chi2:.4f}, p={p_chi:.4f}, Cramr's V={cramers_v:.4f}, {chi_conclusion}"")

# t-test for...",8,"The generated code correctly performs the chi-square test, t-test, and ANOVA, including the calculation of effect sizes (Cramer's V, Cohen's d, Eta-squared) and interpretation based on the p-value and alpha level. This addresses the core requirements of the prompt.

**Strong points:**
*   Correct application of statistical tests.
*   Accurate calculation of effect sizes.
*   Clear interpretation of results (reject/fail to reject H0).
*   Correct handling of missing 'Age' data for the t-test and 'Fare' for ANOVA.
*   Appropriate use of `loc` for filtering.
*   No extraneous explanations as requested.

**Areas for improvement (to achieve a higher score):**
*   **Error Handling for File Loading:** While the generated code assumes `titanic.csv` is present, the ground truth includes robust error handling for `FileNotFoundError`. This is a good practice for general benchmark tasks.
*   **Verbosity and Clarity of Output:** The ground truth provides much more detailed output, including intermediate calculations (like contingency tables, mean ages/fares for groups), specific hypotheses for each test, what the effect sizes mean, and deeper conclusions (e.g., ""statistically significant association""). The generated code's output is very concise, which might be sufficient depending on the exact interpretation of ""display p-values, test statistics, and conclusions,"" but the ground truth offers a more comprehensive interpretive analysis expected in a statistical report.
*   **T-test `equal_var` Parameter:** The generated code hardcodes `equal_var=False` for the t-test, implying a Welch's t-test. While often a safer choice, the ground truth code dynamically determines this based on Levene's test for equality of variances, which is a more statistically rigorous approach if assumptions are to be checked.
*   **Cohen's d Calculation:** The `pooled_std` calculation for Cohen's d in the generated code uses `(n1 - 1)*s1**2 + (n2 - 1)*s2**2) / (n1 + n2 - 2)`. This is the correct formula for pooled standard deviation when `equal_var=True`. However, if `equal_var=False` (as set in the `ttest_ind` call), the pooled standard deviation calculation for Cohen's d should typically use the standard deviation of each group directly (e.g., `np.sqrt((s1**2 + s2**2) / 2)` or a more complex pooled variance for unequal n, or simply define Cohen's d without pooling for Welch's test context), or state an alternative effect size like Glass's delta. The current calculation for `pooled_std` might be inconsistent with the `equal_var=False` setting of `ttest_ind`.
*   **Cramer's V Denominator:** The Cramer's V calculation `min(r, k) - 1` should be `min(r-1, k-1)`. The `min(r, k) - 1` can be incorrect if `r` or `k` is 1 (resulting in 0 or negative denominator). The ground truth `min_dim = min(contingency_table.shape) - 1` is correct assuming shape always returns `(rows, cols)`. In Python, `min(r, k)` returns the minimum of the two dimensions. If `r=2, k=3`, then `min(2,3)-1 = 1`. If `r=2, k=2`, `min(2,2)-1 = 1`. This looks okay; I misread the `min` in `min(r,k)-1` as being different from the ground truth. It is equivalent. However, the ground truth adds `if (n * min_dim) > 0 else 0` to prevent division by zero, which is good practice.

Overall, it's a solid solution achieving most requirements efficiently. The improvements mainly relate to statistical rigor (Levene's test, Cohen's d consistency) and comprehensive reporting.",o4_mini,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('imdb_top_250.csv')
grouped = df.groupby('Year')[['Rating','Runtime']].mean().reset_index()

plt.figure(figsize=(10,6))
plt.plot(grouped['Year'], grouped['Rating'], marker='o')
plt.title('Average IMDb Rating by Year')
plt.xlabel('Year')
plt.ylabel('Average Rating')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```",7,"The generated code successfully loads the data, groups it by year, calculates the average rating and runtime, and generates a line plot for the average rating over the years. This covers the core requirements.

However, there are a few areas for improvement:
1.  **Robustness for `Year` and `Runtime` columns**: The ground truth code includes cleaning steps for `Year` (if it contains non-numeric characters) and `Runtime` (to extract numeric values and convert to int). The generated code assumes these columns are directly parsable, which might not always be the case given the nature of CSV data. This is a common issue in real-world data and the ground truth handles it well.
2.  **`Runtime` plotting**: While the `Runtime` average is calculated, it's not visualized as requested by the prompt (plot showing *trend of average Rating* implied only Rating, but the instruction to also calculate Runtime was clearly made). The ground truth implicitly understood that only the 'Rating' plot was explicitly requested, but it also robustly calculates both.
3.  **Encapsulation/Modularity**: The ground truth code encapsulates the logic within a function (`analyze_imdb_data`), which is good practice for reusability and organization. The generated code is a script.
4.  **Error Handling**: The ground truth includes `try-except` blocks for `FileNotFoundError`, `KeyError`, and general `Exception`, making it more robust. The generated code lacks explicit error handling.

Overall, it's a good attempt that meets the basic requirements, but lacks the robustness and best practices seen in the ground truth.",o4_mini,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 2) Create two groups based on Year
group_pre_1990 = df[df['Year'] < 1990]['Rating']
group_post_1990 = df[df['Year'] >= 1990]['Rating']

# 4) Hypotheses
# H0: _pre1990 = _post1990
# H1: _pre1990 != _post1990

# 3) Perform independent two-sample t-test (Welchs t-test)
t_stat, p_value = stats.ttest_ind(group_pre_1990, group_post_1990, equal_var=False)

# 5) Print t-statistic and...",7,"The generated code correctly performs the core task of loading data, grouping, and conducting a t-test, and interpreting the results. It addresses most of the requirements. However, there are several areas for improvement to meet the ""expert code evaluator"" standards:

**Strong Points:**
*   Correctly loads the dataset.
*   Correctly creates the two groups based on the `Year` column.
*   Correctly performs an independent two-sample t-test using `scipy.stats.ttest_ind`.
*   Correctly prints the t-statistic and p-value.
*   Correctly interprets the p-value against the alpha level and states whether to reject the null hypothesis.

**Areas for Improvement / Missing Aspects:**
*   **Hypotheses Statement (Task 4):** The code only states the hypotheses symbolically (H0: _pre1990 = _post1990, H1: _pre1990 != _post1990). The task explicitly asked to ""Clearly state the null and alternative hypotheses for this test,"" which implies a more descriptive, human-readable statement as provided in the ground truth.
*   **Error Handling for File Loading:** The ground truth includes a `try-except FileNotFoundError` block for `pd.read_csv`, which is good practice for robust code, especially when assuming a file is in the current directory. The generated code lacks this.
*   **Data Type Conversion and Error Handling:** The ground truth includes explicit `pd.to_numeric` conversions for 'Year' and 'Rating' with `errors='coerce'` and subsequent `dropna`. This makes the code more robust against potential issues with data types in the CSV (e.g., if 'Year' is read as an object due to non-numeric characters or if 'Rating' has issues). The generated code assumes 'Year' and 'Rating' are already in suitable numeric formats, which might fail if the CSV isn't perfectly clean.
*   **Explicit Interpretation of Rejection Decision:** While it states ""Reject the null hypothesis,"" it could elaborate slightly more on what that means in the context of the problem, similar to the ground truth's ""Conclusion: There is a statistically significant difference..."".
*   **`equal_var` Parameter:** The generated code uses `equal_var=False` (Welch's t-test), which is often a safer default. However, the task instruction for the t-test section only says ""Perform an independent two-sample t-test."" The ground truth chose `equal_var=True` and added a comment explaining the implication. It's not strictly incorrect for `equal_var=False`, but it should either be justified or the task should specify which variant. For general ""independent two-sample t-test"" when not specified, `equal_var=True` is typically what's implied for a standard test, though Welch's is statistically more robust without the assumption.
*   **Clarity on Numbering:** While it comments with numbering (e.g., ""# 1) Load the dataset""), the order is slightly off, with hypotheses stated before the t-test (4 before 3). The ground truth correctly numbered and executed in order. This is minor but affects flow.",o4_mini,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

df = pd.read_csv('imdb_top_250.csv')
df['Year'] = df['Year'].astype(int)
df['Decade'] = (df['Year'] // 10 * 10).astype(str) + 's'
print(df[['Title', 'Year', 'Decade']].head(10))
```",9,"The generated code correctly performs all requested operations: loading the CSV, creating the `Decade` column by categorizing the `Year` column, and displaying the `Title`, `Year`, and `Decade` for the first 10 movies. The `astype(int)` directly on the 'Year' column is a common and usually safe assumption for this dataset, though `pd.to_numeric` with `errors='coerce'` as in the ground truth is more robust for general cases where year data might be problematic. The use of floor division `//` for calculating the decade is elegant and correct. The output format is also as requested. The only minor point is the lack of explicit error handling for file not found or missing columns, which the ground truth includes, but for a direct task and assuming correct input, the generated code is highly effective and concise.",o4_mini,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"import pandas as pd

df = pd.read_csv('penguins.csv')

num_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in num_cols:
    df[col].fillna(df[col].mean(), inplace=True)

df['sex'].fillna(df['sex'].mode()[0], inplace=True)

result = df.groupby(['species', 'sex'])['body_mass_g'].mean()
print(result)",9,"The generated code correctly implements all specified requirements: it loads the dataset, imputes missing numerical values using the mean, imputes missing categorical 'sex' values using the mode, groups by 'species' and 'sex', and calculates the average 'body_mass_g' for each group. The code is concise and efficient.

One minor point for improvement, though not critical for correctness here, is to add error handling for file not found or missing columns, similar to the ground truth. This makes the code more robust. However, for a data science benchmark focusing on the core task logic, this is a strong solution.",o4_mini,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Filter for Adelie and Gentoo
df_filtered = df[df['species'].isin(['Adelie', 'Gentoo'])]

# 3) Remove rows with missing flipper_length_mm or species
df_filtered = df_filtered.dropna(subset=['flipper_length_mm', 'species'])

# Prepare groups
adelie = df_filtered[df_filtered['species'] == 'Adelie']['flipper_length_mm']
gentoo = df_filtered[df_filtered['species'] == 'Gentoo']['flipper_...",9,"The generated code is highly correct and directly addresses almost all aspects of the task.

**Strong Points:**
*   **Correctness:** Loads data, filters correctly, handles NAs, performs the t-test with `equal_var=False` (a good robust choice), states hypotheses, and correctly interprets the p-value.
*   **Conciseness:** The code is clean and to the point, solving the problem efficiently.
*   **Imports:** Proper imports are included.
*   **Output Formatting:** The t-statistic and p-value are nicely formatted.

**Areas for Minor Improvement (for a perfect score):**
*   **Error Handling:** The ground truth includes `try-except` blocks for file loading, which is good practice for robust code, especially in a production-like environment. The generated code lacks this.
*   **Detail in Hypotheses:** While the hypotheses are stated, the ground truth's phrasing is a bit more formal and explicit about what ""no difference"" or ""difference"" means in terms of mean flipper length for Adelie and Gentoo penguins.
*   **Explicit Conclusion:** In the interpretation, explicitly stating *what* the rejection or failure to reject the null hypothesis implies (e.g., ""There is a statistically significant difference..."") would make the conclusion even clearer, as done in the ground truth.
*   **Edge Case for Empty Groups:** The ground truth checks if `adelie_flipper_lengths` or `gentoo_flipper_lengths` are empty or have fewer than 2 data points after filtering and NA removal. While unlikely with this dataset, it's a good practice for general robustness.

Overall, an excellent solution that fulfills the core requirements effectively.",o4_mini,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load and clean data
df = pd.read_csv('penguins.csv')
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df = df[features + ['species']].dropna()

# Prepare features and target
X = df[features]
y = LabelEncoder().fit_transform(df['specie...",9,"The generated code correctly addresses all requirements of the task: it loads the dataset, selects the specified features and target, correctly drops rows with NA values in the relevant columns, encodes the `species` target variable, splits the data (80/20 with `random_state=42`), trains a Logistic Regression model with `max_iter=1000`, and reports the accuracy. The imports are correct and standard.

The only minor area for improvement that prevents a perfect 10 is that the error handling for `FileNotFoundError` is missing, which was present in the ground truth for robustness. While the prompt did not explicitly ask for error handling, it's a good practice for real-world scenarios. Additionally, the dropping of NAs in `df = df[features + ['species']].dropna()` is concise but less explicit about *which* columns are checked for NA, potentially leading to confusion if one only wants to drop NAs in those specific feature/target columns and not implicitly in other columns that might have been selected earlier in `df`. However, given the preceding `df = pd.read_csv('penguins.csv')` and immediate filtering, this is effectively correct for the problem scope. The output is a single accuracy score, which is sufficient.",o4_mini,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv(""penguins.csv"")

# 2) Drop rows where body_mass_g or species is NA
df_clean = df.dropna(subset=[""body_mass_g"", ""species""])

# 3) For each species, calculate the average body_mass_g
avg_body_mass = (
    df_clean
    .groupby(""species"")[""body_mass_g""]
    .mean()
    .reset_index()
)

# 4) Create a bar plot showing the average body_mass_g for each species
plt.figure(figsize=...",9,"The generated code is highly correct and complete, addressing all requirements of the task.

**Strong points:**
*   **Correctness:** It correctly loads the dataset, drops NAs in the specified columns, calculates the average body mass per species, and generates a bar plot with appropriate labels and title.
*   **Conciseness:** The code is clean, readable, and directly implements the stated requirements without unnecessary complexity.
*   **Proper Libraries:** It correctly uses pandas for data manipulation and seaborn/matplotlib for plotting.
*   **Specific NA handling:** It correctly uses `dropna(subset=[...])` to target specific columns for NA removal, as required.
*   **Clear Plotting:** The plot setup, `sns.barplot`, title, and axis labels are all correct and effective.

**Minor points for improvement (not critical enough to reduce score significantly):**
*   **Error Handling for `FileNotFoundError`:** The ground truth code includes robust `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for production-ready code. The generated code assumes the file will always be present, which is acceptable for a direct solution to a benchmark, but less robust.
*   **Clarity of variable names for plot Y-axis:** While `body_mass_g` is the column, calling the Y-axis label ""Average Body Mass (g)"" is more descriptive than just ""body_mass_g"". The generated code does use ""Average Body Mass (g)"", which is good, but the `y` argument in `sns.barplot` still refers to the original column name. This is a minor stylistic point, however.
*   **Output/Intermediate Prints:** The ground truth includes `print` statements to show intermediate steps (e.g., initial/final shape, NA counts, calculated averages), which can be very helpful for debugging and understanding data transformations. The generated code omits these. For a benchmark answer, direct solution without verbose output can sometimes be preferred, but for a learning environment, intermediate feedback is valuable.

Overall, an excellent submission that fully meets the core requirements.",o4_mini,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Drop rows with NA in specified columns
cols_to_check = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df = df.dropna(subset=cols_to_check)

# 3) Create a new feature culmen_area
df['culmen_area'] = df['culmen_length_mm'] * df['culmen_depth_mm']

# 4) Calculate the Pearson correlation matrix
features = ['culmen_length_mm', 'culmen_dep...",9,"The generated code correctly performs all the required steps: loading the dataset, dropping NA values, creating the `culmen_area` feature, calculating the Pearson correlation matrix for the specified columns, and visualizing it as an annotated heatmap with the correct title. The imports are correct, and the code structure is logical and easy to follow.

Strengths:
- **Correctness**: All task requirements are met accurately.
- **Clarity**: The code is straightforward and easy to understand.
- **Completeness**: All five points of the task are addressed.
- **Visualization**: The heatmap is correctly generated with annotations and a title.

Areas for minor improvement (not critical for functionality but good practice):
- **Error Handling**: The generated code lacks error handling for `FileNotFoundError` when loading the CSV, unlike the ground truth. This is a good practice for robust code.
- **Defensive Programming**: The ground truth uses `.copy()` after `dropna()` to avoid `SettingWithCopyWarning`, and also checks if the DataFrame becomes empty after dropping NAs. While not strictly necessary for this specific dataset given its structure, these are good practices for more general utility.
- **Plotting Aesthetics**: The ground truth adds `'right'` alignment for x-axis labels and `rotation=0` for y-axis labels, which can sometimes improve readability depending on label length. `vmin` and `vmax` are good additions by the generated code to ensure consistent color mapping for correlation matrices.",o4_mini,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Select features and target
feature_cols = [
    'culmen_length_mm',
    'culmen_depth_mm',
    'flipper_length_mm',
    'body_mass_g',
    'island',
    'sex'
]
X = df[feature_cols].copy()
y = df['species'].copy()

# 3) Impute numerical features with mean
num_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
X[num_cols] = X[num_cols].fillna(X[num_cols].mean())

# 4) Impute...",9,"The generated code correctly implements all the requested steps for preparing the dataset. It accurately loads the data, selects features, performs mean imputation for numerical columns, handles NA imputation and mapping for the 'sex' column as specified, and applies one-hot encoding to 'island'. The final output of the shape and head of the feature matrix is also correct.

Strong points:
- Concise and efficient code.
- Correct use of `fillna` for both numerical and categorical imputation.
- Proper application of `map` for the 'sex' column.
- Correct usage of `pd.get_dummies` for one-hot encoding.
- Adherence to all task requirements.

Areas for minor improvement (not critical but good practice):
- The Ground Truth code includes error handling for `FileNotFoundError`, which is good practice for robustness in real-world scenarios, though not explicitly required by the prompt's steps.
- The Ground Truth also separates `X` and `y` explicitly after all processing, which aligns more with typical machine learning workflows where `X` is the feature matrix and `y` is the target vector, and `X.head()` shows only the features. The generated code concatenates `X` and `y` earlier by taking a copy of the target, but then it prints `X.head()` which is just the features, so it correctly reports features.
- Including comments for each step (1), (2), etc., as done in the beginning of the generated code, is helpful for readability.

Overall, the generated code is highly effective and meets all technical requirements of the task.",o4_mini,LLM_judge_config
M49,medium,Customer Segmentation,,"import pandas as pd

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics'] * 7,
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST001']
}
df = pd.DataFrame(data)

# 1) Calculate total spending per customer
df['TotalPric...",10,"The generated code is excellent. It correctly creates the DataFrame, calculates the total spending for each customer, identifies and prints the top 3 customers by spending in descending order using `sort_values` and `head`, and calculates and prints the total quantity sold per category. The code is clean, efficient, and uses appropriate pandas functionalities. All requirements of the task are met.",o4_mini,LLM_judge_config
M5,medium,Classification,titanic.csv,"import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
import matplotlib.pyplot as plt

# Load and preprocess data
df = pd.read_csv('titanic.csv')
df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)
df['A...",7,"The generated code addresses most of the requirements but has significant issues in its data preprocessing and feature scaling, which impacts the overall correctness and robustness.

**Strengths:**
*   **Correct Model Training and Evaluation:** Successfully trains a logistic regression model and calculates all requested evaluation metrics (accuracy, precision, recall, F1, ROC-AUC).
*   **Visualization:** Generates a confusion matrix and ROC curve as requested.
*   **Coefficient Interpretation:** Attempts to interpret coefficients, though the method for obtaining feature names after scaling was flawed.
*   **Cross-Validation:** Performs basic cross-validation for accuracy.

**Areas for Improvement (leading to the score):**

1.  **Data Preprocessing Pipeline (Critical Missing Piece):** The most significant drawback is the lack of a robust preprocessing pipeline using `ColumnTransformer` and `Pipeline` from `sklearn`.
    *   **Feature Scaling before One-Hot Encoding:** The generated code scales numerical features (`StandardScaler`) *before* `pd.get_dummies` is used for categorical features. This means the `Pclass` column, which is an integer type, gets scaled, which is incorrect as `Pclass` should be treated as a categorical variable for one-hot encoding.
    *   **Scaling Entire `X`:** `X[numeric_cols] = scaler.fit_transform(X[numeric_cols])` scales features *before* the train-test split. This is data leakage, as the scaler learns from the test set data. The scaler should be fitted only on the training data.
    *   **Inconsistent Preprocessing:** The preprocessing (scaling and one-hot encoding) is done on the entire dataset `X` *before* the train-test split. This means the statistical properties (mean/std for scaling, categories for one-hot encoding) are learned from the entire dataset, leading to data leakage. The preprocessing steps should be part of a `Pipeline` applied *after* the split, fitted only on `X_train`.

2.  **Handling `Pclass`:** The ground truth code correctly identifies `Pclass` as categorical and treats it as such. The generated code includes `Pclass` in `numeric_cols` and scales it, then later One-Hot Encodes it using `pd.get_dummies` (which isn't ideal because it might be scaled first then encoded), which is conceptually incorrect for this dataset. `Pclass` should be one-hot encoded, not scaled as a numerical feature.

3.  **Coefficient Interpretation for Categorical Features:** The ground truth code correctly extracts feature names after one-hot encoding through the pipeline, providing proper mapping for interpretation. The generated code uses `X.columns`, which do not reflect the one-hot encoded columns (e.g., `Sex_male`, `Embarked_Q`, etc.) accurately if `drop_first=True` is used and the column names from `get_dummies` are mixed with original numerical columns. This makes the `coef_df` less informative for the one-hot encoded features.

4.  **Cross-Validation Metrics:** The generated code only shows cross-validation for 'accuracy'. The prompt requested evaluation with multiple metrics in cross-validation. The ground truth used `cross_validate` to get multiple metrics, which is better.

5.  **Lack of `seaborn` and `ConfusionMatrixDisplay`/`RocCurveDisplay`:** While not strictly failing, `ConfusionMatrixDisplay` and `RocCurveDisplay` from `sklearn.metrics` provide a more streamlined and visually appealing way to generate these plots compared to manual `plt.matshow` and `plt.plot(fpr, tpr)`. The prompt mentioned `matplotlib` specifically, but these are good additions for clarity.

The code's preprocessing approach makes it less robust and potentially leads to inflated performance metrics due to data leakage. The ground truth's use of `ColumnTransformer` and `Pipeline` is the standard and correct way to handle such preprocessing steps in `sklearn`.",o4_mini,LLM_judge_config
M50,medium,Time Series Analysis,,"import pandas as pd

data = {
    'Date': pd.to_datetime([
        '2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04',
        '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10'
    ]),
    'Ticker': ['TECHCO'] * 8,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0, 157.0, 154....",9,"The generated code is almost entirely correct and demonstrates a strong understanding of pandas for this task.

**Strong points:**
*   **Correct DataFrame Creation:** The DataFrame is correctly initialized with the provided data and 'Date' is set as the index.
*   **Correct SMA Calculation:** The 3-day Simple Moving Average for 'Close' is calculated correctly using `rolling(window=3).mean()`, and it correctly produces NaNs for the initial days, matching the requirement.
*   **Correct Volume Percentage Change Calculation:** The `pct_change()` method is correctly used for 'Volume', and it properly results in a NaN for the first day.
*   **Necessary Imports:** `pandas` is imported.
*   **Printing Result:** The final DataFrame with the new columns is printed.

**Area for improvement:**
1.  **`Volume_Pct_Change` Multiplier:** The ground truth code calculates `pct_change()` which returns a decimal representing the percentage change (e.g., 0.20 for a 20% increase). The generated code multiplies this by `100`. While this might be a common way to display percentages, the prompt asked for ""percentage change"" without explicitly stating it should be multiplied by 100 or formatted as a string with a '%' sign. In data analysis, `pct_change()` typically refers to the decimal value. Therefore, removing the `* 100` would make it a direct match to the standard interpretation of `pct_change()` in pandas and the likely intention of ""percentage change"" in a data science context, aligning more closely with the ground truth's output format.

This minor difference in the `Volume_Pct_Change` column calculation prevents a perfect score, as it alters the *value* stored in the column slightly from what a direct interpretation of ""percentage change"" via `pct_change()` would yield.",o4_mini,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]
}
df = pd.DataF...",9,"The generated code correctly performs all the required tasks: it creates the DataFrame, calculates the 'Score_Improvement' column, calculates and prints the average improvement per subject, and identifies students meeting the specified score and attendance criteria. The code is well-structured and uses appropriate pandas functionalities.

Minor points for improvement (why not a 10):
*   **Output Formatting for Student IDs:** The ground truth code iterates through the `StudentID` series to print each ID on a new line, which is a bit cleaner for readability than `print(qualified.tolist())` which prints a list representation. While `tolist()` is a valid way to get the student IDs, the ground truth's output is slightly more user-friendly for this specific request.
*   **Clarity of Print Statements:** The ground truth includes more descriptive print statements (e.g., ""Original DataFrame:"", ""DataFrame after adding 'Score_Improvement' column:""), which aren't strictly required by the prompt but enhance readability and understanding of the output. The generated code's print statements are concise, but less verbose.

Overall, it's a very good solution that successfully addresses all requirements.",o4_mini,LLM_judge_config
M52,medium,Data Manipulation,,"import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

tech_high_perf = df[(df[""Department""] == ""Tech"") & (df[""Performanc...",9,"The generated code correctly defines the DataFrame, filters it according to the specified criteria, and accurately calculates the average number of ""ProjectsCompleted"" for the filtered subset. All requirements are met. The code is concise and efficient.

One point for minor improvement would be to add a check for an empty `tech_high_perf` DataFrame before calculating the mean, similar to the ground truth. While for this specific dataset it's not strictly necessary (as there will always be matching rows), it's a good practice for more robust code that might encounter different datasets. Also, the output format could be slightly more descriptive, though it provides the correct value.",o4_mini,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate conversion rates
desktop = df[df[""DeviceType""] == ""Desktop""]
mobile = df[df[""DeviceType""] ==...",9,"The generated code correctly defines the DataFrame, calculates the conversion rates for Desktop and Mobile users, and prints them clearly labeled. The calculations are accurate, and the code is concise and readable. It fulfills all the requirements of the task.

One minor improvement, though not strictly required by the prompt, would be to handle the edge case where `desktop` or `mobile` DataFrames might be empty. In this specific dataset, they are not, so `mean()` will not raise an error. However, in a more general scenario, an empty DataFrame would result in `NaN` for `mean()`, which might not always be desired and could be handled (e.g., by setting the rate to 0 or printing a warning). The ground truth code handles this gracefully with `if not desktop_users_df.empty:`, which makes it more robust for varied input data, but for the given task and data, the generated code performs perfectly.",o4_mini,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

df = pd.DataFrame(data)

# Calculate and print average AirQualityIndex per sensor
aqi_means = df.groupb...",10,"The generated code is correct, complete, and of high quality.

**Strong Points:**
- **Correctness:** The code accurately defines the DataFrame as specified and correctly calculates the average ""AirQualityIndex"" for each unique ""SensorID"". The `groupby(""SensorID"")[""AirQualityIndex""].mean()` operation is the canonical and most efficient way to achieve this in pandas.
- **Completeness:** Both requirements of the task (DataFrame creation and calculation/printing of average AQI per sensor) are fully met.
- **Clarity of Output:** The output format `Sensor {sensor_id} average AirQualityIndex: {avg_aqi:.2f}` clearly indicates which average belongs to which sensor, fulfilling the output requirement. The use of `.2f` for formatting is a good touch for readability.
- **Code Quality:** The code is concise, easy to read, and follows standard Python and pandas conventions. Necessary imports are present.

No improvements are needed for this specific task.",o4_mini,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}

df = pd.DataFrame(data)

# 1) Create EngagementScore
df[""EngagementScore""] = df[""Likes""] + df[""Shares""] * 2 + ...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the ""EngagementScore"" using the specified formula, filters the DataFrame to include only Instagram posts, and correctly calculates and prints the average ""EngagementScore"" for those posts. The code is clean, concise, and directly addresses all requirements of the task. It demonstrates a clear understanding of pandas operations. The naming of variables (e.g., `instagram_df`) is appropriate and enhances readability. There are no unnecessary comments or print statements, making it very performant for a benchmark.",o4_mini,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the ""LengthOfStay"" column accurately, filters the DataFrame for ""Flu"" patients, and then correctly calculates and prints the average ""LengthOfStay"" for these patients. The code is clean, well-structured, and directly addresses all parts of the prompt's requirements. It demonstrates a solid understanding of pandas operations for data manipulation and analysis.",o4_mini,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}
df = pd.DataFrame(data)

# 1) Filter for Sci-Fi movie...",9,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates their average rating. It adheres to all the requirements of the task. The code is clean, concise, and directly addresses the prompt.

One minor point for improvement, though not strictly required by the prompt, could be to handle the edge case where no 'Sci-Fi' movies are present in the DataFrame. In such a scenario, `sci_fi_df[""Rating""].mean()` would return `NaN`, which might not always be the desired behavior for reporting an average. The Ground Truth code includes a check for `sci_fi_movies_df.empty` which is a good practice for robustness, although for this specific dataset and task, it doesn't impact the outcome. However, given the prompt's simplicity, the generated code is excellent.",o4_mini,LLM_judge_config
M58,medium,Data Manipulation,,"import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

toyota_df = df[df[""Brand""] == ""Toyota""]
average_mileage = toyota_df[""Mileage""]....",10,"The generated code is entirely correct and effectively addresses all specified requirements. It accurately defines the DataFrame, filters for 'Toyota' brand cars, and computes the average mileage for them, printing the result as requested. The code is concise, well-structured, and uses standard pandas operations efficiently. It successfully imports `pandas`, creates the DataFrame as specified, correctly filters by brand using boolean indexing, and then calculates the mean of the 'Mileage' column for the filtered DataFrame. The output format is also clear and informative.",o4_mini,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 80...",9,"The generated code correctly creates the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat. All functional requirements are met. The code is concise and demonstrates a good understanding of pandas operations.

The only minor area for improvement, which is purely cosmetic and not functionally critical for this specific task, is the lack of a more descriptive `print` statement for the output. The ground truth code includes a formatted string that clearly explains what the printed number represents. This makes the output more user-friendly. Additionally, the ground truth code includes an edge case check for an empty filtered DataFrame (`if not fire_pokemon_df.empty:`). While not strictly necessary for this dataset (as Fire-type Pokemon exist), it's a good practice for robust code. However, given the explicit dataset provided, its omission doesn't break correctness.",o4_mini,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Load and preprocess
df = pd.read_csv('titanic.csv')
df['AgeGroup'] = pd.cut(df['Age'], bins=[0,12,18,35,60,100], labels=['Child','Teen','Adult','Middle','Senior'])

# 1) Multi-panel survival rates
fig1, axes = plt.subplots(1, 3, figsize=(18, 5))
sns.barplot(x='Pclass', y='Survived', data=df, ax=axes[0], palette='muted').set_title('Survival Rate by Class')
sns.barp...",7,"The generated code addresses most of the requirements and successfully generates the specified visualizations. The overall structure is logical. However, there are several areas for improvement in terms of robustness, clarity, and completeness as per the ground truth's level of detail and best practices for data visualization.

**Areas for Improvement (to reach a higher score):**

*   **Data Preprocessing and Robustness:**
    *   **Missing Values:** The generated code directly uses `df.read_csv` and `pd.cut` without handling missing values in 'Age' or 'Fare'. The ground truth explicitly handles `Age` and `Fare` NaN values with `median()`, which is a more robust approach.
    *   **Error Handling:** The ground truth includes `try-except FileNotFoundError`, which is good practice for loading data.
    *   **Categorical Conversion:** The ground truth meticulously converts `Pclass` and `Survived` into descriptive categorical types (`Pclass_Cat`, `Survived_Cat`) and sets specific orders, which significantly improves readability and consistency in plots. The generated code uses raw `Pclass` (1, 2, 3) and `Survived` (0, 1) which are less intuitive on plot labels.

*   **Visualization Details and Storytelling:**
    *   **Plot Titles and Labels:** While titles are present, the ground truth often provides more descriptive and clearer titles (e.g., ""Survival Rates by Key Demographic Groups"" vs. ""Survival Rate by Class"").
    *   **Y-axis Limits:** The ground truth explicitly sets `ylim(0, 1)` for survival rate plots, ensuring consistent comparison. The generated code omits this.
    *   **Interactive Scatter Plot:**
        *   The ground truth uses `symbol='Survived_Str'` where `Survived_Str` is a descriptive category ('Perished', 'Survived'), making the legend and plot much clearer. The generated code uses `symbol='Survived'` (0, 1), which is less informative.
        *   The ground truth adds `hover_name='Name'` and custom `hover_data`, enriching the interactivity. The generated code only adds `Name` and `Sex`.
        *   The ground truth uses `color_discrete_map` for custom colors and `category_orders` for clarity.
    *   **Dashboard-Style Figure:**
        *   The generated dashboard reuses individual plots, which is good. However, it lacks the ""Insights"" text panel which was a key requirement for ""storytelling"" and ""proper annotations and insights"" specified in the prompt. The generated code only includes a single `fig.text` at the bottom which is a basic annotation, not a structured insights panel.
        *   The ground truth uses `gridspec` effectively to create a more complex and tailored layout with a dedicated text panel, showcasing a higher level of customization for dashboard design.
        *   The ground truth includes additional plots (KDE for Age and Histplot for Fare) in the dashboard, providing more comprehensive insights than just replicating the previous plots.

*   **Code Quality and Clarity:**
    *   **Comments:** The ground truth is heavily commented, explaining each step, which greatly enhances readability and maintainability. The generated code is sparse on comments.
    *   **Variable Naming:** The ground truth uses more descriptive variable names (e.g., `fig_interactive`, `fig_dashboard`).
    *   **Duplication:** The generated code re-calculates `pivot` for the heatmap in the dashboard, whereas it could have reused the one from the individual heatmap section. (Minor point)

Overall, the generated code fulfills the core visualization requirements but falls short on the ""advanced data visualization and storytelling"" aspect, particularly due to the lack of detailed preprocessing, explicit plot enhancements for clarity and consistency, and the depth of insights/annotations in the dashboard compared to the ground truth.",o4_mini,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.2...",9,"The generated code correctly defines the DataFrame, filters for the 'Dystopian' genre, and accurately calculates the average stock for that genre, fulfilling all requirements of the task. The code is clean, concise, and uses appropriate pandas operations. It directly addresses the problem statement without unnecessary verbosity.

The only minor area for improvement, which is not critical for correctness but good practice, would be to handle the edge case where no Dystopian books might be found (though in this specific dataset, there are). The ground truth includes an `if not dystopian_books_df.empty:` check, which is a robust way to prevent potential errors or misleading output in more general scenarios. However, for the given static dataset, the generated code is perfectly functional and correct.",o4_mini,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load data
df = pd.read_csv('water_potability.csv')

# Parameters
alpha = 0.05
features = [c for c in df.columns if c.lower() != 'potability']
group0 = df[df.Potability == 0]
group1 = df[df.Potability == 1]

# 1) t-tests and Cohen's d
print(""T-Tests and Effect Sizes (Cohen's d):"")
for feat in features:
    x0 = group0[feat].dropna()
    x1 = group1[feat].dropna()
    t_stat, p_val = stats.ttest_ind(x1, x0, equal_var=False...",6,"The generated code correctly performs the requested statistical tests (t-tests, chi-square, Shapiro-Wilk) and calculates Cohen's d. It also includes basic interpretations of the p-values.

However, there are several significant areas for improvement to achieve a higher score and meet the robustness and completeness of the ground truth:

1.  **Missing Value Handling:** This is a critical omission. The ground truth explicitly addresses and imputes missing values (using `fillna` with the mean), which is crucial for statistical analysis in real-world datasets. The generated code uses `.dropna()` for each test, which might lead to different sample sizes for different tests/features or discard too much data, potentially biasing results. Imputation ensures a consistent dataset for analysis.
2.  **Dataset Loading Robustness:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, making the code more robust. The generated code lacks this.
3.  **Completeness of Interpretation:** While the generated code prints a ""significant"" or ""not significant"" message, the ground truth provides a more detailed explanation of the null hypothesis and what rejection/failure to reject implies.
4.  **Cohen's d Edge Cases:** The ground truth includes robust handling for calculating Cohen's d, specifically addressing cases where the pooled standard deviation might be zero (e.g., if all values are identical within groups, leading to `s0`/`s1` being 0) or if sample sizes are too small, preventing division by zero errors or `NaN` outputs. The generated code has `if pooled_sd > 0 else np.nan` but doesn't handle the case where `n0+n1-2` might be zero or negative, which could lead to errors if `n0` or `n1` is very small. Also, the interpretation of Cohen's d is very basic (just the value) in the generated code, whereas the ground truth classifies it (trivial, small, medium, large).
5.  **Shapiro-Wilk Sample Size Warning:** The ground truth includes a useful warning about Shapiro-Wilk's limitations for very large sample sizes (e.g., >5000), suggesting alternative tests. This adds a layer of statistical best practice that the generated code misses.
6.  **Redundant `dropna()`:** While not critically incorrect for the tests themselves, the need for `.dropna()` for each test in the generated code is a symptom of not having a central missing value handling strategy.
7.  **Output Clarity:** The ground truth organizes the output more cleanly with clear headings for each section and consistent formatting, making the results easier to read and understand.
8.  **Variable Naming:** `group0` and `group1` are not as descriptive as `potable_df` and `non_potable_df`.
9.  **Redundant `features` list:** The `features` list is created by excluding 'potability' but then `df.columns.drop('Potability')` or similar could be used directly in loops. Creating a separate list is fine, but the naming and method could be clearer.",o4_mini,LLM_judge_config
M8,medium,Classification,water_potability.csv,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn import metrics

# 1) Load and preprocess
df = pd.read_csv('water_potability.csv')
df = df.fillna(df.mean())
X = df.drop('Potability', axis=1)
y = df['Potabili...",6,"The generated code addresses most of the requirements but has several critical issues and areas for improvement, particularly regarding data preprocessing and the implementation of cross-validation.

**Correctness:**
*   **Missing Value Handling (Incorrect):** The generated code `df = df.fillna(df.mean())` imputes missing values *before* the train-test split. This leads to *data leakage* because information from the test set's distribution (mean) is used to impute missing values in the training set, and vice versa. The ground truth correctly applies imputation *after* the split using `fit_transform` on the training data and `transform` on the test data.
*   **Feature Scaling (Incorrect):** Similar to imputation, feature scaling is applied *after* the train-test split but *before* imputation in the generated code, which is illogical as `fit_transform` and `transform` should be applied to the preprocessed data (post-imputation). The order is also incorrect within the cross-validation loop.
*   **Cross-Validation (Incorrect Pipeline):** The crucial mistake in cross-validation is performing `scaler.transform(X)` directly on the entire `X` dataset *outside* any `Pipeline`. This means the `scaler` was fitted on `X_train` only. When `cross_val_score` is called with `scaler.transform(X)`, it scales the *entire* dataset using means/stds calculated only from the training set, which is not how proper cross-validation with preprocessing should work. Preprocessing (imputation and scaling) should always be done *inside* the cross-validation folds using a `Pipeline` to prevent data leakage and ensure fair evaluation. The ground truth correctly introduces a `Pipeline` for CV.
*   **ROC Curve Plotting:** The generated code creates separate figures for each ROC curve, which is not ideal for comparison as requested by the prompt. The ground truth code correctly plots all ROC curves on a single subplot.
*   **Confusion Matrix Display:** The `imshow` plotting of the confusion matrix is basic and lacks the annotations and better visual presentation (e.g., using `ConfusionMatrixDisplay` or `seaborn.heatmap`) that the ground truth provides.

**Code Quality:**
*   **Imports:** Uses `sklearn import metrics` which bundles many metrics. While functional, direct imports like `accuracy_score`, `precision_score`, etc., are generally clearer and more explicit.
*   **Structure:** The flow is somewhat less organized compared to the ground truth, which clearly labels and separates sections based on the task prompt's requirements.
*   **Variables:** Variable names are generally okay.
*   **Redundancy in CV Loop:** The `scores = cross_val_score(model, scaler.transform(X), y, cv=cv, scoring='roc_auc')` is within `results.items()`, which means `model` is already trained on the test/train split. `cross_val_score` expects a fresh model (or a pipeline to instantiate it). This is less efficient and conceptually problematic without a pipeline, as it reuses the `model` object potentially, rather than re-training it in each CV fold.

**Completeness:**
*   **Handling Missing Values:** It handles missing values but incorrectly (data leakage).
*   **Feature Scaling:** Done, but also incorrectly (data leakage and pre-`fit_transform` context).
*   **Train-Test Split:** Done.
*   **Multiple Classification Algorithms:** Implemented.
*   **Evaluation Metrics:** All requested metrics are calculated.
*   **Confusion Matrices & ROC Curves:** Generated, but plotting could be improved (single ROC plot for comparison).
*   **K-fold Cross-Validation:** Performed, but fundamentally flawed due to data leakage.
*   **Compare Model Performances:** Done.
*   **Select Best Model:** Done, but based on a flawed CV strategy.
*   **Feature Importance:** Implemented correctly for Random Forest and Logistic Regression (coefficients). SVC correctly identified as not having direct feature importance.
*   **Error Handling:** Lacks file not found error handling for the CSV.
*   **Stratification:** Does not perform stratified train-test split or stratified k-fold cross-validation, which is crucial for binary classification with potentially imbalanced datasets to maintain class proportions. The ground truth uses `stratify=y` and `StratifiedKFold`.",o4_mini,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier

# Load and preprocess
df = pd.read_csv('water_potability.csv')
df = df.fillna(df.mean())

# Manual feature engineering
df['ph_chloramines'] = df['ph'] / (df['Chloramines'] + 1e-6)
df['hardness_solids'] = df['Hardness'] / (df[...",6,"The generated code addresses most of the task requirements but has several areas for improvement, particularly regarding the comprehensiveness of feature engineering, handling of missing values, and the rigor of feature selection setup.

**Strengths:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Basic Feature Engineering:** Creates some ratio and interaction terms, and attempts polynomial features.
*   **Correlation Analysis:** Correctly calculates and identifies top correlated features.
*   **Feature Selection:** Implements both `SelectKBest` and `RFE` as requested.
*   **Visualizations:** Generates all three requested visualization types (heatmap, feature importance, pairplot).
*   **Comparison of Feature Sets:** Compares the overlap between selected features.

**Areas for Improvement:**

1.  **Missing Value Handling:** The ground truth correctly identifies specific columns (`ph`, `Sulfate`, `Trihalomethanes`) that have missing values and imputes only those. The generated code uses `df.fillna(df.mean())` which imputes *all* numerical columns, including 'Potability', which is the target variable and should not be imputed this way. While `Potability` might not have NaNs in the given dataset, it's a less robust approach.
2.  **Feature Engineering Completeness and Meaningfulness:**
    *   **Ratios:** Only two ratios (`ph_chloramines`, `hardness_solids`) are created, while the prompt implicitly asks for more comprehensive creation of ""ratios between chemical parameters (e.g., pH/Chloramines, Hardness/Solids)"". The ground truth creates several more.
    *   **Interaction Terms:** Only two interaction terms (`ph_hardness`, `sulfate_conductivity`) are created. The prompt asks for ""interaction terms"" more broadly. The ground truth provides more examples.
    *   **Polynomial Features:**
        *   The generated code applies polynomial features to *all* numerical columns except 'Potability'. This can lead to a very high dimensionality and potentially less meaningful features. The ground truth wisely selects a subset of original features for polynomial expansion to avoid excessive dimensionality, which is a better practice.
        *   The way `poly_df` and `new_cols` are concatenated might lead to duplicate columns if any generated polynomial features have the same name as existing columns (e.g., if a feature like 'ph' is generated again through 1st degree polynomial, though `include_bias=False` and `interaction_only=False` prevents this common issue, it's still less explicit than the ground truth's check if `col not in X_fe.columns`).
        *   The ground truth also correctly handles `get_feature_names_out` for `PolynomialFeatures`, ensuring column names are correctly assigned when adding to the DataFrame.
3.  **Feature Selection Rigor:**
    *   **Scaling:** The generated code does *not* scale the features before applying `SelectKBest` or `RFE`. This is a significant omission, as tree-based models like RandomForest are less sensitive to scaling, but `SelectKBest` (especially with `f_classif`) and generally for good machine learning practice, scaling is crucial. The ground truth correctly applies `StandardScaler`.
    *   **`k` value for `SelectKBest` and `n_features_to_select` for `RFE`:** Both are set to 5. While not strictly wrong, the prompt implies ""optimal features"" and selecting only 5 from a potentially large set of engineered features might be too restrictive, especially without further justification or cross-validation. The ground truth chooses 15, which is a more reasonable starting point for exploration.
4.  **Clarity and Structure:** The ground truth code is more structured with sections (`1. Load Data`, `2. Feature Engineering`, etc.) and comments, making it easier to read and understand.
5.  **Robustness:** The ground truth includes a `try-except` block for loading the CSV, which is good practice for robustness.
6.  **Minor Details:**
    *   The pairwise scatter plots in the generated code use `df` directly, which has the engineered features, but the `vars` parameter includes `list(top5.index)`, which refers to the original DataFrame's correlation, and might miss new engineered features if they become highly correlated. The ground truth ensures the `pairplot` uses the dataframe with engineered features.
    *   The variable `num_cols` in the generated code includes the target variable 'Potability' initially before dropping it, which is not ideal for the `PolynomialFeatures` transformation. The ground truth separates X and y earlier.",o4_mini,LLM_judge_config
