task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1) List all column names and data types
print(df.dtypes)

# 2) Calculate descriptive statistics for numerical columns
print(df.describe())

# 3) Total number of rows and columns
print(""Number of rows:"", df.shape[0])
print(""Number of columns:"", df.shape[1])

# 4) Identify categorical or ordinal columns
print(""Columns that appear to be categorical or ordinal:"")
print("" - Type"")
for column in df.columns:
    if df[co...",6,"The generated code correctly loads the dataset, lists column data types (implicitly via `df.dtypes`), calculates descriptive statistics for numerical columns, and reports dataset dimensions. However, it falls short on identifying categorical/ordinal columns correctly. It only checks for `object` dtype, which is insufficient, especially for a dataset like `winequality-red.csv` where all columns are numerical but 'quality' is often treated as ordinal. It does not provide the `identified_type` for each column as requested in the task description. The code quality is acceptable, but it lacks the robustness of error handling for file not found. It misses the specific ""mean, median, standard deviation, min, max, and quartiles"" explicitly by just calling `describe()` without highlighting specific values for each column. The ground truth shows a more robust and explicit handling for data types and descriptive statistics.",gpt-3.5-turbo,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"import pandas as pd

# Load the dataset and display the first 10 rows
df = pd.read_csv('video_games_sales.csv')
print(df.head(10))

# Show the shape of the dataset
print(df.shape)

# List all column names and their data types
print(df.dtypes)

# Display the top 10 games by Global_Sales
top_10_games = df.sort_values('Global_Sales', ascending=False).head(10)
print(top_10_games)

# Show the unique values in the Platform and Genre columns
print(df['Platform'].unique())
print(df['Genre'].unique())

#...",7,"The generated code correctly performs most of the requested EDA tasks. It successfully loads the data, displays the head, shows the shape, lists data types, identifies top 10 games by sales, lists unique values for Platform and Genre, provides summary statistics for sales columns, and shows missing values.

However, it lacks robust error handling (e.g., for FileNotFoundError or missing columns), which is present in the ground truth and is good practice for real-world scenarios. The output formatting is also very minimal compared to the ground truth, which includes descriptive print statements, separators, and specific column displays for the top 10 games, making the output much clearer and more readable for a user.

Specific areas for improvement:
1.  **Error Handling**: Add `try-except` blocks for file loading and checks for column existence before attempting operations on them.
2.  **Output Clarity**: Incorporate descriptive print statements and separators to clearly label each section of the output, as demonstrated in the ground truth.
3.  **Display Specificity**: When showing the top 10 games, it's better to show only the relevant columns (e.g., Name, Platform, Genre, Publisher, Global_Sales) rather than the entire DataFrame row, which can be verbose.
4.  **Completeness of Missing Values**: While `df.isnull().sum()` is correct, filtering to show only columns with *actual* missing values (i.e., sum > 0) is often more informative, as the ground truth does.",gpt-3.5-turbo,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"import pandas as pd

# Load dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values in Year column
df['Year'].fillna(df['Year'].median(), inplace=True)

# Remove games with missing Publisher information
df = df.dropna(subset=['Publisher'])

# Create Years_Since_Release column
current_year = 2023
df['Years_Since_Release'] = current_year - df['Year']

# Remove duplicate games
df = df.drop_duplicates(subset=['Name', 'Platform'])

# Filter out games with Global_Sales less than 0.1...",7,"The generated code correctly addresses most of the requirements. It loads the dataset, handles missing 'Year' median, removes missing 'Publisher' data, creates 'Years_Since_Release', removes duplicates, and filters by 'Global_Sales'. The display of shape and head is also correct.

However, there are a few areas for improvement to reach a higher score:
1.  **Robustness for 'Year' column**: The ground truth code correctly converts the 'Year' column to numeric with `errors='coerce'` before calculating the median and filling NaNs, and then converts it to `int`. The generated code assumes 'Year' is already numeric, which might lead to errors if the column contains non-numeric strings.
2.  **Robustness for 'Global_Sales'**: Similar to 'Year', 'Global_Sales' might contain non-numeric data. The ground truth explicitly converts it to numeric using `errors='coerce'` and then drops NaNs before filtering. The generated code directly assumes it's numeric, which could cause a `TypeError` if a non-numeric value exists in the `Global_Sales` column during comparison.
3.  **Handling negative 'Years_Since_Release'**: While less common, if a 'Year' value is beyond 2023, `current_year - df['Year']` could result in negative `Years_Since_Release`. The ground truth explicitly handles this by ensuring values are non-negative.
4.  **Clarity and readability**: The ground truth code includes print statements at each major step, which helps in debugging and understanding the data transformation process. It also defines constants, which is good practice. While not strictly required by the prompt, these improve code quality.
5.  **Error Handling**: The ground truth includes a `try-except` block for `FileNotFoundError` and general exceptions, making the code more robust.",gpt-3.5-turbo,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Create a bar chart showing the top 10 gaming platforms by total global sales
platform_sales = df.groupby('Platform')['Global_Sales'].sum().nlargest(10)
platform_sales.plot(kind='bar', figsize=(10,5))
plt.title('Top 10 Gaming Platforms by Total Global Sales')
plt.xlabel('Platform')
plt.ylabel('Total Global Sales')
plt.show()

# Generate a pie chart displaying the distribu...",6,"The generated code addresses most of the visualization requirements, creating bar, pie, histogram, scatter, and line plots. The plots are generally correct in their basic functionality and include titles and labels. However, there are several areas where the code quality and robustness could be significantly improved to match the ground truth:

1.  **Missing Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is crucial for robust code. The generated code lacks this.
2.  **Lack of Data Cleaning/Preprocessing:** The ground truth includes essential steps for handling potential `NaN` values and ensuring correct data types for sales columns and `Year_of_Release`. For example, `Year_of_Release` often needs to be parsed as numeric and potentially cleaned (e.g., removing NaNs or out-of-range values) for accurate time-series plotting. The generated code directly uses the columns without any form of data cleaning, which could lead to errors or inaccurate plots if the original data is messy (e.g., `Year` column is not purely numeric or has missing values).
3.  **Incorrect Column Name for Line Chart:** The line plot uses `df.groupby('Year')['Name'].count()`. The correct column for the year of release in the ground truth dataset is `Year_of_Release`. This is a significant correctness issue.
4.  **Matplotlib Customization/Formatting:** While the code uses `plt.show()` for each plot, it doesn't explicitly create `figure` and `axes` objects using `plt.figure(figsize=...)` and `fig, ax = plt.subplots()` for better control over plot sizes and layouts. Some plots could benefit from `plt.tight_layout()` to prevent labels/titles from overlapping.
5.  **Minor Details:** The axis labels for the bar chart omit ""Millions"" which is less informative. The histogram's bin count is a bit low (`bins=20`) compared to the ground truth's `bins=50` or the idea of using quantiles to get a better distribution. Scatter plot marker size `s` and color are not specified.

Overall, the code provides a functional solution but lacks the robustness, thoroughness, and attention to detail present in the ground truth, particularly concerning error handling and data preparation.",gpt-3.5-turbo,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""diabetes.csv"")

# Display the first 10 rows
print(df.head(10))

# Show the shape of the dataset
print(""Shape of the dataset:"", df.shape)

# List all column names and data types
print(""Column names and data types:"")
print(df.dtypes)

# Calculate percentage of diabetic vs non-diabetic patients
print(""Percentage of diabetic vs non-diabetic patients:"")
print(df['Outcome'].value_counts(normalize=True) * 100)

# Display count of missi...",7,"The generated code correctly addresses most of the requirements but lacks robustness and completeness in a few areas compared to the ground truth.

**Strong Points:**
*   Correctly loads the CSV and displays the first 10 rows.
*   Correctly shows the shape of the dataset.
*   Correctly lists column names and data types (though `df.info()` is more comprehensive).
*   Correctly calculates percentages for the 'Outcome' column.
*   Correctly displays missing value counts.
*   Correctly shows summary statistics.

**Areas for Improvement (Why not higher than 7):**

*   **Robustness (File Loading):** The generated code does not include any error handling for `FileNotFoundError` or other exceptions when loading the dataset, unlike the ground truth. This is a critical omission for real-world applications.
*   **Completeness (Column Data Types):** While `df.dtypes` works, `df.info()` as used in the ground truth provides a more complete picture, including non-null counts, which is useful for EDA.
*   **Completeness (Identifying Zero Values):** The current implementation for identifying zero values (`df.columns[(df == 0).any()]`) is overly broad. It merely tells you which *columns* contain at least one zero. It doesn't tell you *how many* zeros are in specific columns that are *known* to have zeroes signifying missing data (e.g., Glucose, BloodPressure). The ground truth correctly identifies specific columns for this check and provides a count and percentage, which is a much more valuable insight for EDA on this specific dataset. The `(df == 0).any()` simply lists the columns that *might* contain zeros, which is less useful than quantifying the instances of 0 for specific columns, especially since for 'Pregnancies' and 'Outcome', 0 is a valid value.
*   **Readability/Presentation:** The output is a bit less structured than the ground truth, which uses clear headers and separators to distinguish between different sections of the output. This is a minor point but contributes to overall code quality for presentation.
*   **Imports:** `numpy` is not imported, although not strictly needed for this version of the code, it's often a good practice to include it for data science tasks.

The key differences are in the error handling and the more nuanced approach to identifying and presenting zero values as potential missing data, which is a common and important detail for this specific dataset.",gpt-3.5-turbo,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Replace zero values in certain columns with NaN
df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.nan)

# Handle missing values by filling them with the median of each respective column
df.fillna(df.median(), inplace=True)

# Remove duplicate rows
df.drop_duplica...",6,"The generated code addresses most of the requirements but has several critical issues that make it less robust and potentially incorrect for the given task.

**Correctness:**
- **Missing `numpy` import:** The generated code uses `np.nan` but doesn't import `numpy`, which will lead to a `NameError`.
- **Incorrect NaN replacement:** The line `df[['Glucose', ..., 'BMI']].replace(0, np.nan)` works for replacing zeros with NaNs. However, the subsequent `df.fillna(df.median(), inplace=True)` will fill *all* missing values in *all* columns with their respective medians, not just the ones where zeros were replaced, and crucially, it will fill NaNs in columns that weren't specified for zero replacement (e.g., `Pregnancies`). The task specifically states to replace zeros in ""Glucose, BloodPressure, SkinThickness, Insulin, BMI"" and then handle missing values in ""each respective column"" (implying only those).
- **Scaling includes 'Outcome':** The hardcoded list of columns for scaling includes 'Pregnancies', 'Glucose', etc., but implicitly excludes 'Outcome'. While this is good that it excludes 'Outcome', it is less robust than dynamically selecting numerical columns and then explicitly removing 'Outcome' if it's there. The hardcoded list is fragile if the dataset columns change.
- **Error Handling:** Lacks file not found error handling for `pd.read_csv`.

**Code Quality:**
- **Readability:** The code is concise but lacks comments and logical separation for each step of the task, unlike the ground truth.
- **Robustness:** As mentioned, hardcoding column names for scaling and not handling cases where columns might be missing makes it less robust.
- **Imports:** Missing `import numpy as np`.

**Completeness:**
- **Missing `numpy` import:** Fails this.
- **Zero-to-NaN handling:** Partially correct, but the median filling step is too broad.
- **Output:** Only prints shape and head. The task asked for displaying the shape *and* original shape for comparison, along with first 5 rows, and also explicitly requested displaying ""cleaned dataset's shape and first 5 rows"", not just printing them raw. It also didn't include the print statements for success/warning messages which the ground truth included.

To improve, the code should:
1. Add `import numpy as np`.
2. Refine the `fillna` step to only apply to the specific columns where zeros were replaced, or iterate through those columns for median imputation.
3. Make the scaling column selection more dynamic and explicitly exclude the target variable ('Outcome').
4. Add basic error handling for file loading.
5. Add descriptive print statements as requested.",gpt-3.5-turbo,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Create a correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Diabetes Dataset')
plt.show()

# Generate histograms for all numerical features
df.hist(figsize=(12, 10))
plt.suptitle('Histograms of Diabetes Dataset Features', y=1.03)
plt.show()

# Create box plots comparing each feature bet...",7,"The generated code creates most of the requested visualizations and demonstrates a good understanding of using `matplotlib` and `seaborn`.

**Strengths:**
- Successfully loads the dataset.
- Generates a correlation heatmap.
- Creates histograms for all features.
- Produces box plots for each feature by outcome.
- Plots a pie chart for outcome distribution.
- Generates the two specified scatter plots.
- Creates a pair plot for the specified key features.
- Uses proper imports.

**Areas for Improvement:**
- **Plot Formatting and Readability:**
    - **Correlation Heatmap:** The ground truth excludes the 'Outcome' column from the correlation matrix before plotting, which is often done when comparing features. The generated code includes 'Outcome' in the correlation, which isn't wrong but might be less insightful for feature-to-feature correlation.
    - **Histograms:** The generated code uses `df.hist()`, which is simpler but less customizable than `sns.histplot`. The ground truth creates a grid of subplots using `sns.histplot` with KDE for better visualization and consistent styling, along with custom titles and labels for each subplot, which is missing in the generated code. The generated code shows all histograms in a single figure, which can be quite dense.
    - **Box Plots:** The generated code creates a *new figure* for *each* box plot, leading to many individual windows/plots instead of a single figure with subplots, like the ground truth. This is inefficient and makes comparisons harder. The labels for the x-axis (`Outcome` vs detailed `0: Non-Diabetic, 1: Diabetic`) are less informative in the generated code.
    - **Scatter Plots:** The generated code only sets the main plot titles (`BMI vs Glucose`, `Age vs Pregnancies`) but misses proper axis labels and legends with descriptive labels (e.g., 'Non-Diabetic', 'Diabetic') which are present in the ground truth for better clarity. It also doesn't use `plt.tight_layout()` for optimal spacing.
    - **Pie Chart:** The generated code accurately plots the pie chart, but the ground truth adds `explode` and custom `labels` for better presentation.
    - **Pair Plot:** The pair plot is correctly generated, but the ground truth adds `diag_kind='kde'` for density plots on the diagonal, which is a common and useful addition for pair plots.

- **Completeness/Detail:**
    - The ground truth includes more detailed print statements for loading data, which is good practice for data validation.
    - The ground truth explicitly defines `numerical_cols` to exclude 'Outcome' from feature-based plots (heatmap, histograms, box plots), which is a more robust approach than simply `df.columns[:-1]` or `df.corr()` as it makes the intent clearer and prevents plotting the 'Outcome' column with features when it's the target.
    - The ground truth applies `sns.set_style(""whitegrid"")` for consistent and clean aesthetics.
    - Error handling for `FileNotFoundError` is present in the ground truth, making it more robust.

Overall, the generated code fulfills the basic requirements but lacks the polishing, consistency in formatting, and robustness of the ground truth. Improving the visualization aesthetics, using subplots more efficiently, and adding more descriptive labels would significantly enhance its quality.",gpt-3.5-turbo,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv(""world-happiness-report-2021.csv"")

# Display the first 10 rows
print(df.head(10))

# Show the shape of the dataset
print(""Shape of the dataset:"", df.shape)

# List all column names and their data types
print(""Column names and data types:"")
print(df.dtypes)

# Display the unique regional indicators and count of countries per region
print(""Unique regional indicators and count of countries per region:"")
print(df['Regional indicator'].value_c...",7,"The generated code correctly performs most of the requested tasks.

**Strengths:**
- Correctly loads the dataset.
- Accurately displays the first 10 rows, dataset shape, unique regional indicators, top 10 happiest countries, and missing values.
- Calculates summary statistics for the specified columns.

**Areas for Improvement to achieve a higher score:**
- **Robustness:** The generated code lacks error handling for `FileNotFoundError`. If the CSV file is not present, the script will crash. The ground truth includes a `try-except` block for this.
- **Output Clarity:** The output formatting (e.g., using separators `=""*70""` and explicit step numbers) is missing, which makes the output less readable compared to the ground truth.
- **Column Existence Checks:** The generated code assumes all specified columns (like 'Regional indicator', 'Ladder score', etc.) always exist. If a column name is different or missing, it will throw a `KeyError`. The ground truth includes checks for column existence before attempting operations.
- **`df.info()` vs `df.dtypes`:** For Task 3, `df.info()` from the ground truth provides a more comprehensive overview of column types, including non-null counts, which is generally more useful for EDA than just `df.dtypes`.
- **Clarity in `print` statements:** While the generated code has some print statements, it could benefit from more descriptive labels (e.g., ""1. First 10 rows of the dataset:"") as in the ground truth for better organization and readability of the output.",gpt-3.5-turbo,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder

# Load the dataset
data = pd.read_csv('world-happiness-report-2021.csv')

# Handling missing values
data['Social support'].fillna(data['Social support'].mean(), inplace=True)
data['Healthy life expectancy'].fillna(data['Healthy life expectancy'].mean(), inplace=True)
data['Freedom to make life choices'].fillna(data['Freedom to make life choices'].mean(), inplace=True)
data['Percep...",6,"The generated code addresses most of the requirements but has several shortcomings regarding robustness, completeness, and correctness.

**Strengths:**
*   Correctly loads the dataset.
*   Successfully creates 'Happiness_Category' and 'GDP_Category' using appropriate pandas functions (`pd.cut`, `pd.qcut`).
*   Calculates 'Well_being_Score' correctly.
*   Applies IQR method for outlier removal (though the implementation has a minor flaw).
*   Correctly standardizes a subset of numerical features.
*   Correctly uses LabelEncoder for 'Regional indicator'.
*   Displays the final shape and head of the DataFrame.

**Areas for Improvement/Weaknesses:**

1.  **Missing Value Handling (Critical Flaw):** This is the most significant issue.
    *   The ground truth identifies and fills all numerical columns with the mean and all categorical columns with the mode.
    *   The generated code *only* handles specific columns (`Social support`, `Healthy life expectancy`, `Freedom to make life choices`, `Perceptions of corruption`). It completely misses other numerical columns like `Ladder score`, `Logged GDP per capita`, `Generosity`, `Dystopia + residual` for mean imputation, and doesn't generalize for any other potential categorical missing values. This makes the missing value handling **incomplete and brittle**.
    *   `Perceptions of corruption` is a numerical column in the dataset, but the generated code fills it with the `mode()` instead of the `mean()`. This is an **incorrect application** of the rule.

2.  **Outlier Removal (Minor Flaw):**
    *   The line `data = data[~((data['Ladder score'] < (Q1 - 1.5 * IQR)) | (data['Ladder score'] > (Q3 + 1.5 * IQR))]` is technically correct for filtering, but the `~` (NOT operator) is applied globally to the boolean series, which can sometimes be less intuitive or harder to read than `(data['Ladder score'] >= lower_bound) & (data['Ladder score'] <= upper_bound)`. More importantly, it removes all rows with outliers, which is what's asked, but doesn't explicitly define `lower_bound` and `upper_bound` variables for clarity like the ground truth does.

3.  **Standardization (Incomplete):**
    *   The ground truth standardizes *all* numerical features.
    *   The generated code only standardizes a specific list of columns (`Logged GDP per capita`, `Social support`, `Healthy life expectancy`, `Freedom to make life choices`). It misses `Ladder score`, `Generosity`, `Perceptions of corruption`, `Dystopia + residual`, and the newly created `Well_being_Score`. This fails the requirement to ""Standardize numerical features using StandardScaler"" comprehensively.

4.  **Implicit Assumptions/Robustness:**
    *   The ground truth includes `try-except` for file loading, making it more robust. The generated code does not.
    *   The ground truth dynamically identifies numerical/categorical columns for missing value handling, which is much more robust to schema changes than hardcoding column names.

5.  **Small Accuracy Issue for Well-being Score:** While the average calculation for `Well_being_Score` is mathematically correct, calculating it based on `mean(axis=1)` on a DataFrame subset, as in the ground truth, is generally more robust and concise for multiple columns. The explicit sum and division works but is less ""Pandas idiomatic"" for an average across columns.

To improve, the generated code needs to generalize its missing value handling and standardization to *all* relevant numerical and categorical columns, and ensure the correct imputation method (mean vs. mode) is applied based on data type.",gpt-3.5-turbo,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import plotly.express as px
import folium
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# World map showing happiness scores by country
fig = px.choropleth(df, locations=""Country name"", locationmode=""country names"", color=""Ladder score"",
                    hover_name=""Country name"", title=""Happiness Score by Country"")
fig.show()

# Horizontal bar chart showing average happiness scores ...",6,"The generated code addresses most of the requirements but has several shortcomings in terms of detail, quality, and complete adherence to the specifications.

**Strengths:**
*   Successfully loads the dataset.
*   Generates the required visualization types: map, bar chart, heatmap, scatter plots, box plots, and attempts a radar chart.
*   Uses a mix of Plotly, Seaborn, and Matplotlib as requested.

**Weaknesses and Areas for Improvement:**

1.  **Column Renaming and Consistency:** The ground truth renames columns for readability, which is good practice. The generated code uses original column names ('Ladder score', 'Logged GDP per capita', etc.), making the plots slightly less intuitive.
2.  **World Map (Plotly):**
    *   The generated code's map is basic. It misses `color_continuous_scale` for better visual appeal.
    *   It doesn't include the `update_layout` for `geo` settings which improves the map's appearance (e.g., hiding frame/coastlines).
3.  **Horizontal Bar Chart (Plotly vs. Matplotlib/Seaborn):** The ground truth uses Matplotlib/Seaborn for this, which was specified. The generated code uses Plotly. While Plotly is good, adhering to the library specification per chart type (where optional) is better. The ground truth also explicitly sorts the bars, which improves readability.
4.  **Correlation Heatmap:**
    *   The generated code computes correlation for *all* columns, including non-numerical ones (which will result in NaNs or errors for some data types) and irrelevant identifiers. The ground truth explicitly selects relevant numerical columns, which is crucial for a meaningful correlation matrix.
    *   It lacks proper formatting for `annot` (e.g., `fmt="".2f""`) and aesthetic touches like `linewidths` and `linecolor` from the ground truth.
    *   No rotation for x-axis labels, which can lead to overlapping labels if there are many factors.
5.  **Scatter Plots:**
    *   The scatter plots are very basic (`sns.scatterplot`). The ground truth adds `hue` by `Regional indicator`, `s` (size), `alpha` (transparency), a specific `palette`, and `grid(True)`, which significantly enhances the plots' informational content and aesthetics.
    *   Legends are missing or not optimally placed.
    *   No `plt.tight_layout()` for optimal spacing.
6.  **Box Plots (Plotly vs. Matplotlib/Seaborn):** Similar to the bar chart, the ground truth uses Matplotlib/Seaborn and rotates x-tick labels for readability, while the generated code uses Plotly. While Plotly is fine, adhering to specified libraries or implementing better visuals (like rotated labels) would be better.
7.  **Radar Chart (Major Issue):**
    *   This is the biggest deviation/error. The generated code uses `px.line_polar(top5_countries, r=top5_countries.iloc[:, 6:], theta=top5_countries.columns[6:], line_close=True)`. This approach is fundamentally flawed for a radar chart based on multiple features for each country.
        *   `r` should be the values for each feature.
        *   `theta` should be the feature names.
        *   To plot multiple countries, each needs its own set of `r` values.
    *   The standard way to do a radar chart involves normalizing the data across features (like the ground truth with `MinMaxScaler`), plotting each country iteratively on a polar subplot, and manually setting angles and labels. The `plotly.express` `line_polar` is not designed for this type of multi-profile comparison directly without significant data restructuring that wasn't performed here.
    *   The generated radar chart will likely be incorrect or uninterpretable. The ground truth's implementation of the radar chart is significantly more robust and correct.
8.  **Overall Code Quality:**
    *   Lack of `try-except` for file loading.
    *   Missing `plt.rcParams` settings for default plot size and DPI.
    *   Missing `sns.set_style(""whitegrid"")`.
    *   No print statements for progress tracking (minor, but adds to user experience).
    *   Less robust handling of subplot legends (`bbox_to_anchor`).
    *   Lack of `plt.tight_layout()` in many Matplotlib plots.

The radar chart's incorrect implementation is a significant drawback given its complexity and uniqueness among the required visualizations. The other plots are present but lack the polished detail and informativeness of the ground truth.",gpt-3.5-turbo,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd

# 1) Load the dataset and parse the 'date' column as datetime objects
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2) Display the first 10 rows
print(df.head(10))

# 3) Show the shape of the dataset (rows and columns)
print(""Shape of the dataset:"", df.shape)

# 4) List all column names and their data types
print(""Column names and data types:"")
print(df.dtypes)

# 5) Calculate and display the total sum of 'total_cases' and 'total_deaths' g...",6,"The generated code addresses most of the requirements, but some parts are incomplete or lack robustness compared to the ground truth.

**Strengths:**
*   Correctly loads the CSV and parses dates.
*   Correctly displays the first 10 rows.
*   Correctly displays the shape of the dataset.
*   Correctly calculates and displays continental sums for total cases and deaths.
*   Correctly calculates and displays missing values.
*   Uses `describe()` for summary statistics, which is a concise way to get mean, min, max, std, etc.

**Areas for Improvement / Missing Aspects:**
*   **Missing Print Statements/Context:** For almost every output, there are no informative print statements (e.g., ""--- First 10 rows of the dataset ---"", ""--- Shape of the dataset (rows, columns) ---""). This makes the output less readable and less aligned with the ground truth's presentation.
*   **Column Names and Data Types (4):** The generated code uses `df.dtypes` which only shows data types. The ground truth uses `df.info()` which is more comprehensive, showing column names, non-null counts, and data types, aligning better with the ""list all column names and their data types"" requirement. `df.dtypes` is technically correct for *data types*, but `info()` is better for *listing columns and types*.
*   **Top 5 Locations (6):** This is the most significant correctness issue. The generated code calculates the top 5 locations based on the *overall latest date in the entire dataset*, not the *latest date for each location*. This is a crucial distinction in time-series data like this. The ground truth correctly handles this by sorting and then dropping duplicates based on `location` `keep='last'`.
*   **Summary Statistics (7):** While `describe()` is fine, the request specifically asked for ""mean, median, min, max, std"". `describe()` also provides count, 25%, 50% (median), and 75% quartiles. This is a minor point, but generating precisely what was asked for is generally better. The ground truth explicitly used `agg()` for this. Also, the ground truth made the columns numeric first and dropped NaNs which is a more robust approach.
*   **Error Handling:** The ground truth includes basic error handling for file not found and other exceptions, which is good practice for production code. The generated code lacks this.
*   **Robustness/Safety Checks:** The ground truth includes checks like `if 'continent' in df.columns` before performing operations, making the code more robust against missing columns. The generated code does not include these checks.",gpt-3.5-turbo,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Read the CSV file
data = pd.read_csv('winequality-red.csv')

# 1) Calculate and report the average 'quality' of red wines
average_quality = data['quality'].mean()
print(""Average Quality: "", average_quality)

# 2) Group wines by 'quality' score and calculate average 'alcohol' content for each quality group
quality_alcohol = data.groupby('quality')['alcohol'].mean()

# 3) Create a bar chart visualizing the relat...",8,"The generated code correctly addresses all three requirements of the task: calculating the mean quality, grouping by quality to find the average alcohol content, and creating a bar chart to visualize this relationship. The imports are proper, and the code structure is clear.

Areas for improvement (to reach a 9 or 10):
1.  **Readability/Presentation of Output:** The printed output for the mean quality is concise but doesn't mirror the Ground Truth's more descriptive labeling (e.g., ""1. Average 'quality' of red wines: ...""). Similarly, the grouped 'alcohol' content is not explicitly printed to the console, which the Ground Truth includes for direct user feedback. While not strictly required for the visualization, it enhances the overall analytical output.
2.  **Robustness:** The generated code lacks error handling for `FileNotFoundError` when reading the CSV, which the Ground Truth includes. This makes the Ground Truth more robust.
3.  **Visualization Aesthetics:** While the bar chart is functionally correct, the Ground Truth's visualization includes more aesthetic refinements: a specific figure size, a `palette`, adjusting font sizes for title and labels, and importantly, adding data labels on top of the bars for precise values. These details significantly improve the clarity and professional appearance of the plot.
4.  **Commenting:** The Ground Truth provides more explanatory comments for each step, which improves code understanding for others.",gpt-3.5-turbo,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd

# Load the dataset
data = pd.read_csv('synthetic_covid19_data.csv')

# Parse the 'date' column as datetime objects
data['date'] = pd.to_datetime(data['date'])

# Fill missing values
data['hosp_patients'].fillna(0, inplace=True)
data['icu_patients'].fillna(0, inplace=True)
data['positive_rate'].fillna(data['positive_rate'].mean(), inplace=True)
data['reproduction_rate'].fillna(data['reproduction_rate'].mean(), inplace=True)

# Calculate new columns
data['mortality_r...",7,"The generated code addresses most of the requirements but has a few omissions and minor inaccuracies that prevent it from being perfect.

**Strengths:**
- Correctly loads the data and parses the 'date' column.
- Handles missing values for 'hosp_patients', 'icu_patients', 'positive_rate', and 'reproduction_rate' as per instructions.
- Creates 'mortality_rate' and 'vaccination_rate_per_capita' columns with correct formulas and handles division by zero by replacing `inf` and `NaN` with 0.
- Filters the dataset to include only records from 2021.
- Displays the shape and head of the cleaned data.

**Areas for Improvement:**
- **Missing `numpy` import:** The code uses `np.inf` and `np.nan` but doesn't import `numpy` which would cause a `NameError`. The ground truth includes `import numpy as np`.
- **Handling of `NaN` for new column calculations:** The `replace([np.inf, -np.inf, np.nan], 0, inplace=True)` handles NaNs *after* they might have been generated. It's more robust and sometimes necessary to ensure the columns involved in calculations are numeric *before* the division, especially if not all original columns are purely numeric (e.g., if `total_deaths` or `total_cases` contained non-numeric strings). The ground truth explicitly converts columns to numeric using `pd.to_numeric(..., errors='coerce')` which is a more robust approach. While the `replace` might catch `NaN` results from, say, division by zero, it might not catch `NaN`s propagated from non-numeric source columns.
- **Error handling for file not found:** The generated code does not include any error handling for `FileNotFoundError` when trying to load the CSV, which is a good practice for robust code.
- **Clarity of output messages:** The ground truth includes more descriptive print statements for the shape and head, which is helpful for understanding the output.
- **Use of `.copy()` for filtered DataFrame:** When filtering a DataFrame (e.g., `data = data[data['date'].dt.year == 2021]`), it's generally good practice to explicitly use `.copy()` to avoid `SettingWithCopyWarning` if subsequent operations modify the filtered DataFrame. The ground truth does this (`df_2021 = df[df['date'].dt.year == 2021].copy()`). While not strictly a correctness issue in this specific case, it's a best practice.",gpt-3.5-turbo,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Parse the 'date' column
df['date'] = pd.to_datetime(df['date'])

# Time series line plot of 'new_cases' for 'India'
india_data = df[df['location'] == 'India']
plt.figure(figsize=(12, 6))
plt.plot(india_data['date'], india_data['new_cases'], marker='o')
plt.title('New Cases of COVID-19 in India')
plt.xlabel('Date')
plt.ylabel('New Cases')
plt.gr...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being excellent.

**Correctness & Completeness Issues:**

1.  **Time Series Plot for India:** The use of `marker='o'` on a line plot makes it less ideal for showing continuous trends over time, especially with a large number of data points. `sns.lineplot` (as in Ground Truth) is generally preferred for time series as it handles aesthetics and missing data more gracefully.
2.  **Bar Chart for Total Deaths by Continent - CRITICAL FLAW:** The generated code calculates `total_deaths_continent = df.groupby('continent')['total_deaths'].sum()`. This is incorrect. `total_deaths` is a cumulative metric. Summing it directly across all entries for each continent will lead to an extremely inflated and misleading number because each day's `total_deaths` includes previous days' deaths. The correct approach, as in the Ground Truth, is to take the *latest* `total_deaths` value *per location* and then sum these latest values by continent. This is a significant data aggregation error.
3.  **Scatter Plot for GDP per Capita vs. Total Vaccinations - CRITICAL FLAW:** The generated code uses `latest_data = df[df['date'] == df['date'].max()]`. This only selects data for *one specific day* (the absolute last date in the dataset). The requirement was ""using the latest entry for each location"". This means the *latest* `gdp_per_capita` and `total_vaccinations` value for *each individual country/location*, not just a single global latest date. This leads to very few data points, potentially incorrect ones, and misses the underlying relationship across all locations.
4.  **Scatter Plot - Missing Features:** The generated code doesn't use `hue` or `size` based on `continent` or `population` respectively, which were implicit in the prompt's reference to a richer scatter plot (though not explicitly stated, good to include for insightful plots). It also lacks `dropna` for the relevant columns, which can cause issues or misleading plots if data is missing.
5.  **Lack of `tight_layout()` and `plt.show()` After Each Plot:** While `plt.show()` is present, `plt.tight_layout()` is crucial for preventing labels/titles from overlapping, especially in plots with rotations. The current `plt.show()` structure means plots appear one by one, which is fine, but `tight_layout` would improve presentation.
6.  **Error Handling:** The generated code does not include any error handling for file not found or data parsing issues, unlike the robust Ground Truth.

**Code Quality Issues:**

1.  **Readability/Clarity:** While functional, the plotting code uses `plt.plot` and `df.plot(kind='bar')` for some charts. While not strictly ""wrong,"" `seaborn` provides more visually appealing and often more convenient functions (`sns.lineplot`, `sns.barplot`) that align better with general data science best practices for visualizations and are explicitly part of the prompt's suggested libraries.
2.  **Aesthetics:** The basic `matplotlib` plots lack the professional styling and specific color palettes (`viridis`, `pastel`) that `seaborn` offers (`sns.set_theme(style=""whitegrid"")`). The prompt mentioned using seaborn for plotting.
3.  **Redundancy in `plt.show()`:** Having `plt.show()` after each plot is good for individual display, but sometimes for a set of plots, one might organize them into subplots or just have one `plt.show()` at the end if wanting them to appear sequentially or in specific windows. However, for a script designed to show distinct plots, it's acceptable.

To improve, the generated code needs to correct the data aggregation logic for total deaths and the scatter plot, and incorporate more professional visualization practices from Seaborn.",gpt-3.5-turbo,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 1) List column names and data types
print(df.dtypes)

# 2) Calculate descriptive statistics for numerical columns
print(df[['elixirCost', 'Win Rate', 'Usage']].describe())

# 3) Total number of cards
print(len(df))",7,"The generated code correctly loads the dataset, identifies data types, calculates descriptive statistics for the specified columns, and reports the total number of rows. It covers the core requirements.

However, there are several areas for improvement:
- **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and other exceptions, which is crucial for real-world applications. The generated code lacks this.
- **Output Clarity:** While the ground truth explicitly prints column names and data types separately, the generated code uses `df.dtypes` which is concise but doesn't explicitly 'list' them as distinct items. Similarly, the descriptive statistics are presented via `.describe()`, which provides more statistics than requested (e.g., count, 25%, 50%, 75% quartiles) and not in the clearly formatted way the ground truth achieves by using `.agg()` for specific statistics. The task specifically asked for `mean`, `median`, `standard deviation`, `min`, `max`.
- **Readability/Structure:** The ground truth provides more verbose print statements for clarity which are helpful for the user to understand what output corresponds to which task. This aligns better with a ""report"" format.
- **Completeness of Statistics:** The ground truth explicitly calculates only the requested statistics, whereas `.describe()` calculates additional ones. This is a minor point, but adherence to specific requirements is important.

Overall, it's a functional solution that addresses the main technical points, but could be improved in terms of robustness, output presentation, and precise fulfillment of output requirements.",gpt-3.5-turbo,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Filter cards with elixirCost <= 3
filtered_df = df[df['elixirCost'] <= 3]

# Select and display 'Card' and 'Win Rate' columns
result = filtered_df[['Card', 'Win Rate']]

# Sort the data by 'Win Rate' in descending order
result = result.sort_values(by='Win Rate', ascending=False)

print(result)",9,"The generated code correctly implements all the requirements of the task: it loads the CSV, filters by `elixirCost` <= 3, selects the `Card` and `Win Rate` columns, and sorts the result by `Win Rate` descending. The code is concise and easy to understand.

Areas for minor improvement (not critical for current task):
- Does not include error handling for `FileNotFoundError` or other `pd.read_csv` issues.
- Does not explicitly check if the required columns (`elixirCost`, `Card`, `Win Rate`) exist before attempting operations on them, which could lead to `KeyError` if the CSV structure changes.
These are considerations for robust production code but do not detract from its correctness for this specific benchmark task.",gpt-3.5-turbo,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Generate histogram
plt.hist(df['elixirCost'], bins=10, color='skyblue', edgecolor='black')
plt.title('Distribution of Elixir Cost in Clash Royale Cards')
plt.xlabel('Elixir Cost')
plt.ylabel('Frequency')
plt.show()
```",8,"The generated code correctly loads the CSV, creates a histogram of `elixirCost`, and includes a title and labeled axes as required. It successfully fulfills the core requirements of the task.

**Strong points:**
- Correctly uses `pandas.read_csv` to load the data.
- Correctly uses `matplotlib.pyplot.hist` to generate the histogram.
- Adds a title and labels to the axes.

**Areas for improvement to achieve a higher score:**
- **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and other potential issues when loading the CSV, and also checks for the existence of the `elixirCost` column. The generated code lacks these checks, which could lead to crashes if the file is missing or the column name is incorrect.
- **Plotting Library Choice:** While matplotlib is used and fulfills the task, the prompt specifically mentioned ""Use matplotlib or seaborn for plotting."" The ground truth leverages seaborn's `histplot` which often produces more aesthetically pleasing plots by default and can be a common practice in data science for visualization.
- **Histogram Bins:** The ground truth uses `bins=range(1, 12)` which creates precise bins for each integer elixir cost from 1 to 10, which is very appropriate for discrete data like elixir costs. The generated code uses `bins=10`, which defaults to 10 equal-width bins over the data's range, which might not align perfectly with discrete integer values.
- **Plot Readability Enhancements:** The ground truth adds more visual enhancements like setting `figsize`, explicitly setting `xticks` for integer costs, adding a grid, and using `kde=False` (though this is implicitly handled by `plt.hist` unless a KDE specific function is called). These small details contribute to a more professional and readable plot.
- **Print Statements:** The ground truth includes print statements for confirmation of file loading and error messages, which are helpful for debugging and user feedback.",gpt-3.5-turbo,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# Load the iris dataset
iris = pd.read_csv('iris.csv')

# 1) List all column names and identify the data type of each column
print(iris.dtypes)

# 2) Calculate descriptive statistics for numerical columns
numerical_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
print(iris[numerical_cols].describe())

# 3) Report the number of unique species and their counts
print(iris['Species'].value_counts())
```",7,"The generated code correctly performs the three core tasks: loading the data, identifying data types and column names (implicitly through `dtypes`), calculating descriptive statistics for the specified numerical columns, and reporting unique species counts.

Strong points:
- Correctly uses `pandas.read_csv` to load the dataset.
- `iris.dtypes` correctly lists column names and their data types.
- `iris[numerical_cols].describe()` efficiently calculates the required descriptive statistics (mean, std, min, max, cuartiles, and count).
- `iris['Species'].value_counts()` accurately provides the unique species and their counts.
- Proper import of pandas.

Areas for improvement:
- **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and other exceptions during file loading. The generated code lacks this, which makes it less robust in a real-world scenario where the file might be missing.
- **Explicit Column Names Listing:** While `df.dtypes` shows column names, the first requirement specifically asks to ""List all column names *and* identify the data type of each column."" The ground truth explicitly prints `df.columns.tolist()` in addition to `df.dtypes`, which is clearer for the first part of the requirement.
- **Specific Statistics for Numerical Columns:** The prompt specifically asked for ""mean, median, standard deviation, min, max."" While `describe()` provides these (and more), the ground truth explicitly uses `.agg(['mean', 'median', 'std', 'min', 'max'])` to adhere precisely to the requested statistics, making the output more targeted. For example, `median` is not directly shown by `describe()` but as the 50% quartile. If `median` was explicitly requested, `describe()` doesn't show it as ""median"".
- **Clarity and Formatting:** The ground truth includes print statements that clearly label each section of the output, making it much more readable and user-friendly. The generated code simply prints the outputs without much context. For instance, just printing `iris.dtypes` doesn't explicitly state ""Column Names and Data Types:"". Adding headers would improve readability significantly.
- **Completeness of descriptive statistics:** `describe()` provides median as the 50% percentile, but does not explicitly label it as ""median"" which was one of the requested statistics.

Overall, the code is functional and produces the expected data, but it could be improved in terms of robustness, clarity of output, and precise adherence to the requested statistics.",gpt-3.5-turbo,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"import pandas as pd

iris_data = pd.read_csv('iris.csv')
setosa_data = iris_data[iris_data['Species'] == 'Iris-setosa']
setosa_data[['SepalLengthCm', 'SepalWidthCm']]",7,"The generated code correctly performs the data loading, filtering, and column selection steps. It loads `iris.csv`, filters for 'Iris-setosa' species, and selects 'SepalLengthCm' and 'SepalWidthCm' columns. However, it fails to explicitly display the final DataFrame by using `print()`, which is a key requirement (""select and display only the `SepalLengthCm` and `SepalWidthCm` columns""). In an interactive environment (like a Jupyter notebook), the last line would implicitly display the DataFrame, but in a standard Python script, it would not output anything. It also lacks error handling (e.g., `FileNotFoundError`), which is present in the ground truth and generally good practice for robust code. Adding a `print()` statement for the final selection and basic error handling would significantly improve its completeness and robustness.",gpt-3.5-turbo,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

iris = pd.read_csv('iris.csv')

plt.scatter(iris['SepalLengthCm'], iris['SepalWidthCm'], c=pd.factorize(iris['Species'])[0])
plt.title('Sepal Length vs Sepal Width')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.show()
```",6,"The generated code correctly loads the dataset and creates a scatter plot with the specified axes and title. It also attempts to color the points by species using `pd.factorize`.

However, the coloring by `pd.factorize` is suboptimal for plotting as it assigns arbitrary numerical values to categories, leading to a default colormap that may not be distinct or aesthetically pleasing for categorical data, and crucially, it does not provide a legend to identify which color corresponds to which species. This fails to fully address the requirement of ""Color the points in the scatter plot according to the Species column"" in a clearly interpretable manner.

Specifically, the ground truth code iterates through each species and plots them individually, assigning a unique color and label for the legend, which is a much better way to visualize categorical data on a scatter plot.

To improve, the generated code should:
1. Iterate through unique species to assign distinct colors and add a legend.
2. Use specific color maps or a manual color assignment if `pd.factorize` is used for numerical mapping.
3. The axis labels provided are slightly different from the ground truth (`Sepal Length (cm)` vs `SepalLengthCm`). While not incorrect, consistency with the column names is often preferred.",gpt-3.5-turbo,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"import pandas as pd

# Load the dataset
heart_data = pd.read_csv('heart.csv')

# 1) List all column names and identify the data type of each column
print(heart_data.dtypes)

# 2) Calculate basic descriptive statistics for numerical columns
print(heart_data[['Age', 'RestingBP', 'Cholesterol', 'MaxHR']].describe())

# 3) Count occurrences for each value in the HeartDisease column
print(heart_data['HeartDisease'].value_counts())",6,"The generated code correctly identifies column types, calculates descriptive statistics for the specified columns, and reports value counts for 'HeartDisease'. However, it lacks robustness and completeness compared to the ground truth.

**Areas for Improvement:**
*   **Column Names Listing:** The task explicitly asked to ""List all column names"". The generated code only prints `df.dtypes`, which shows data types but doesn't explicitly list the column names separately as requested.
*   **Descriptive Statistics Detail:** While `describe()` generates all statistics, the ground truth specifically asked for ""mean, median, standard deviation, min, max"". The generated code outputs all statistics from `describe()`, which is good, but doesn't filter specifically for those, and the median is represented as '50%'. The ground truth explicitly renames '50%' to 'median' for clarity.
*   **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions when loading the CSV, making the script more robust. The generated code has no error handling.
*   **Clarity and Readability:** The ground truth includes print statements that clearly label each section of the output, making it much easier to understand what information is being presented. The generated code just prints outputs one after another without explanatory headers.
*   **Missing Column Handling:** The ground truth checks if specified numerical columns actually exist before trying to calculate statistics, printing a warning if some are missing. The generated code would raise a KeyError if one of 'Age', 'RestingBP', 'Cholesterol', or 'MaxHR' were missing. Similarly for 'HeartDisease'.",gpt-3.5-turbo,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"import pandas as pd

# Load the heart.csv dataset
heart_data = pd.read_csv('heart.csv')

# Filter the dataset
filtered_data = heart_data[(heart_data['Age'] > 50) & (heart_data['HeartDisease'] == 1)]

# Select and display specific columns
selected_columns = filtered_data[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]
print(selected_columns)",10,"The generated code correctly loads the `heart.csv` dataset, filters it based on the specified age and HeartDisease criteria, and then selects and displays precisely the required columns. It includes the necessary import (`pandas`). All requirements of the task are fully met. The code is concise and directly addresses the problem.",gpt-3.5-turbo,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"import pandas as pd

df = pd.read_csv('winequality-red.csv')

filtered_df = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]

highest_quality_wine = filtered_df[filtered_df['quality'] == filtered_df['quality'].max()]

print(highest_quality_wine)",7,"The generated code correctly performs the initial filtering and attempts to find the highest quality wine. However, there's a subtle but important issue in identifying ""the wine"" with the highest quality.

**Strong Points:**
*   Correctly imports pandas.
*   Correctly loads the CSV file.
*   Correctly applies the compound filtering condition (`alcohol > 12` AND `pH < 3.3`).

**Areas for Improvement:**
*   **Identifying the Single Best Wine:** The line `highest_quality_wine = filtered_df[filtered_df['quality'] == filtered_df['quality'].max()]` will return *all* wines that have the maximum quality score in the filtered subset. The task specifically asks to ""identify the wine"" (singular) and ""display all features of this specific wine"". This can be problematic if multiple wines share the highest quality score, as it would print a DataFrame with multiple rows instead of a single wine's features. A more robust approach, like using `idxmax()` on the 'quality' column of the filtered DataFrame and then `.loc` to retrieve that specific row, ensures only one wine (the first one encountered if ties exist) is returned.
*   **Robustness/Edge Cases:** The code doesn't handle the edge case where `filtered_df` might be empty (i.e., no wines meet the criteria). In such a scenario, `filtered_df['quality'].max()` would likely raise an error or return `NaN`, and the subsequent indexing would fail. The ground truth includes a check for an empty filtered DataFrame.
*   **Readability/Clarity:** While functional, encapsulating the logic within a function and adding comments, as done in the ground truth, improves code organization, reusability, and understanding, especially for more complex tasks. The task asked for ""proper imports"" and implied a complete script, and while basic functions are proper, a more structured approach is generally better.
*   **Output Format:** The task asked to ""Display all features of this specific wine."" If `highest_quality_wine` contains multiple rows, it wouldn't be ""this specific wine."" If it contained a single row but was still a DataFrame, it might not print as clearly as suggested by `to_string()` for a single Series representation.",gpt-3.5-turbo,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
heart_data = pd.read_csv('heart.csv')

# Generate histogram for Age column
plt.figure(figsize=(10, 6))
plt.hist(heart_data['Age'], bins=10, color='skyblue', edgecolor='black')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Generate bar chart for Sex column
plt.figure(figsize=(6, 4))
heart_data['Sex'].value_counts().plot(kind='bar', color='salmon')
plt.title('Distribution of...",7,"The generated code correctly loads the dataset and generates both a histogram for 'Age' and a bar chart for 'Sex' with titles and labeled axes, fulfilling the core requirements. However, it uses `matplotlib.pyplot.hist` directly for the histogram, which is less visually appealing and lacks the KDE plot compared to the `sns.histplot` used in the ground truth. For the bar chart, it directly plots using `value_counts().plot(kind='bar')`, which works but is less explicit and customizable than `sns.countplot`. It also doesn't handle the mapping of `0` and `1` in the 'Sex' column to 'Female' and 'Male' as done in the ground truth, which would improve readability of the bar chart. Error handling (e.g., `FileNotFoundError`) is also missing. Including `seaborn` as per the ground truth would have allowed for more sophisticated and visually richer plots.",gpt-3.5-turbo,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 1) List column names and data types
print(df.dtypes)

# 2) Convert 'streams' column to numeric and handle potential errors
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
print(""Number of NaNs in 'streams' column:"", df['streams'].isnull().sum())

# 3) Calculate basic descriptive statistics
numerical_cols = ['streams', 'bpm', 'danceability_%', 'valence_%', 'energy_%']
print(df[numerical_cols].d...",7,"The generated code correctly addresses most of the task requirements.

**Strong Points:**
- Correctly loads the CSV file.
- Correctly converts the `streams` column to numeric with `errors='coerce'` and reports the NaN count.
- Calculates and prints descriptive statistics for the specified numerical columns.
- Correctly counts and reports unique artists.
- Uses proper `pandas` functions for each step.

**Areas for Improvement to achieve a higher score:**
- **Detail Missing for Column Names/Data Types (Requirement 1):** While `print(df.dtypes)` shows data types, the request was to ""List all column names and identify the data type of each."" The ground truth's `df.info()` provides a more comprehensive and readable output, including `non-null` counts, which is generally preferred when listing column info from a DataFrame. `df.dtypes` is technically correct for data types but less descriptive.
- **Reporting of Stats (Requirement 3):** The requested output was to report specific statistics (mean, median, std, min, max). `df[numerical_cols].describe()` provides these along with `count` and 25%/75% quartiles. While it includes the requested stats, a slightly more targeted output (like the ground truth example) could be provided if only those specific stats were strictly needed, but `describe()` is a common and acceptable way to get basic stats.
- **Robustness (File Loading):** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth included. While not explicitly asked for in the prompt's requirements, it's good practice for robust code and could lead to silent failures or traceback errors in a real-world scenario if the file is missing.
- **Output Clarity:** The ground truth code used `print` statements with headings to clearly indicate which part of the output corresponds to which task, making the output much easier to read and understand. The generated code lacks these descriptive print statements.",gpt-3.5-turbo,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"import pandas as pd

df = pd.read_csv('spotify_2023.csv')
df = df[df['released_year'] == 2023]
df = df[['track_name', 'artist(s)_name', 'streams']]
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.sort_values(by='streams', ascending=False)

print(df)",8,"The generated code correctly achieves all the core requirements of the task: loading the CSV, filtering by `released_year` == 2023, selecting the specified columns, ensuring `streams` is numeric, and sorting in descending order. The `pd.to_numeric` with `errors='coerce'` is correctly applied.

One minor point for improvement, compared to the ground truth, is the order of operations for `pd.to_numeric`. While the current order works, applying `pd.to_numeric` *before* filtering and selecting columns might be slightly more robust or prevent potential `SettingWithCopyWarning` if more complex operations were involved on the filtered DataFrame. However, for this specific simple chain of operations, it doesn't cause an issue.

The ground truth also includes error handling for file not found, which is good practice in production code but not explicitly requested by the prompt's functional requirements. The generated code is concise and directly addresses the problem's core logic.",gpt-3.5-turbo,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Generate a histogram
plt.hist(df['bpm'], bins=20, color='skyblue', edgecolor='black')
plt.xlabel('BPM')
plt.ylabel('Frequency')
plt.title('Distribution of BPM in Spotify 2023')
plt.show()

# Create a scatter plot
plt.scatter(df['danceability_%'], df['energy_%'], color='coral')
plt.xlabel('Danceability (%)')
plt.ylabel('Energy (%)')
plt.title('Relation between Danceability and...",8,"The generated code correctly performs all the required tasks: loading the CSV, generating a histogram for `bpm`, and a scatter plot for `danceability_%` vs `energy_%`. Both plots have appropriate titles and axis labels. The imports are correct.

Areas for minor improvement:
1. The `plt.show()` calls after each plot will display them one by one. For better user experience, it's often preferred to display all plots at once. This can be achieved by creating new figure objects for each plot (`plt.figure()`) and then calling `plt.show()` only once at the end.
2. Error handling for file loading or missing columns could be added for robustness, though not explicitly asked for in the prompt.
3. The prompt implied the CSV file is in the current directory and the code correctly assumes this.

Overall, it's a very good solution that meets the core requirements.",gpt-3.5-turbo,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""youtube_trendings.csv"")

# Display column names and their inferred data types
print(df.dtypes)

# Remove commas from numeric columns and convert to numeric types
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)
df['Dislikes'] = pd.to_numeric(df['Dislikes'].str.replace(',', ''), errors='coer...",8,"The generated code correctly addresses most of the task requirements.

**Strong points:**
*   Successfully loads the CSV dataset.
*   Correctly removes commas from the specified numeric columns (`Video views`, `Likes`, `Dislikes`), converts them to numeric types (float), handles conversion errors by coercing to NaN (`errors='coerce'`), and fills these NaNs with 0.
*   Correctly calculates and reports basic descriptive statistics for the cleaned numeric columns, although it uses `describe()` which provides more than just the requested mean, median, std, min, max (but includes them).
*   Correctly fills missing values in the `Category` column with 'Unknown'.
*   Correctly reports the count of unique values in the `Category` column.
*   Includes proper imports.

**Areas for improvement (to reach a 9 or 10):**
*   **Listing Column Names & Data Types:** The requirement was to ""List all column names and their inferred data types."" The generated code uses `print(df.dtypes)`, which is sufficient for displaying data types but doesn't explicitly list column names in a separate, clear way as `df.info()` would (which also provides non-null counts, memory usage, etc., which is generally more informative for this type of task). While `df.dtypes` implicitly shows column names alongside their types, the prompt's wording suggests a clear listing.
*   **Error Handling for File Loading:** The ground truth code includes robust error handling for `FileNotFoundError` when loading the CSV, which is good practice. The generated code assumes the file will always be present, which could lead to a crash if the file is missing.
*   **Descriptive Statistics Output Format:** While `describe()` works, it produces `count`, `25%`, `50%` (median), `75%` in addition to the requested mean, std, min, max. The prompt specifically asked for ""mean, median, std, min, max"". Printing `views_stats.loc[['mean', '50%', 'std', 'min', 'max']]` or similar could make the output more precise to the prompt's request.
*   **Clarity of Output:** The output for the calculated statistics is a `pandas.Series` representation (`views_stats`). While correct, explicitly printing ""Mean:"", ""Median:"", etc., with formatted numbers (like the ground truth) would make the output more user-friendly and directly match the ""reporting"" aspect of the requirement.",gpt-3.5-turbo,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Video views' column
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)

# Filter videos with views greater than 50,000,000
filtered_df = df[df['Video views'] > 50000000]

# Select and display 'Video' and 'Video views' columns
result = filtered_df[['Video', 'Video views']]

# Sort by 'Video views' in descending order
result = result.sort_values(by='Video vi...",10,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans the 'Video views' column by removing commas, converting to numeric, and filling NaNs with 0. It then filters the data based on the view count, selects the required columns, and sorts the result in descending order. The code is concise and efficient, demonstrating good pandas usage.",gpt-3.5-turbo,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the Likes column
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)

# Clean the Video views column
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)

# Create scatter plot
plt.scatter(df['Video views'], df['Likes'])
plt.title('Video Views vs. Likes')
plt.xlabel('Video Views (c...",9,"The generated code correctly performs all the required tasks: it loads the CSV, cleans the 'Likes' and 'Video views' columns by removing commas, converting to numeric, and filling NaNs with 0, and then generates a scatter plot with the specified title and axis labels. The code is concise and efficient.

One minor point for improvement, although not explicitly stated as a hard requirement in the prompt, is error handling for `pd.read_csv`, which would make the code more robust (e.g., if the file isn't found). Additionally, `pyplot.show()` should be called last before the program ends. The current placement is fine. Overall, it's a very good solution.",gpt-3.5-turbo,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"import pandas as pd

df = pd.read_csv('mobiles_2025.csv')

df['Mobile Weight'] = pd.to_numeric(df['Mobile Weight'].str.replace('g', ''), errors='coerce')
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''), errors='coerce')

print(df.dtypes)

print(df[['Mobi...",8,"The generated code correctly performs all the cleaning, data type conversion, and analysis tasks specified in the prompt.

**Strong points:**
*   Successfully loads the CSV file.
*   Correctly cleans `Mobile Weight`, `RAM`, `Battery Capacity`, and `Launched Price (USA)` columns, removing specified characters and converting to numeric with error handling (`errors='coerce'`).
*   Successfully prints the data types for all columns after cleaning.
*   Correctly calculates and prints descriptive statistics for the specified numeric columns.
*   Accurately counts and prints the number of unique values in the `Company Name` column.
*   Concise and readable code.

**Areas for improvement (to reach a perfect 10):**
*   **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.
*   **Output Formatting:** The prompt asked for ""reporting"" the statistics and company name count. While the code prints the raw output of `describe()` and `nunique()`, a slightly more user-friendly output with labels (similar to the ground truth's print statements) would improve readability for the end-user.
*   **Minor Cleaning Detail:** For `Mobile Weight`, `RAM`, and `Battery Capacity`, the ground truth included `.str.strip()` after removing the unit to handle potential whitespace, which is a good defensive coding practice, though it might not always be strictly necessary depending on the dataset. The generated code did not include this.",gpt-3.5-turbo,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"import pandas as pd

df = pd.read_csv('mobiles_2025.csv')

df['Launched Price (India)'] = pd.to_numeric(df['Launched Price (India)'].str.replace('INR ', '').str.replace(',', ''), errors='coerce')
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')

filtered_df = df[(df['Company Name'] == 'Apple') & (df['Launched Price (India)'] < 100000)]

result = filtered_df[['Model Name', 'RAM', 'Launched Price (India)']].sort_values(by='Launched Price (India)', ascending=True)

print(...",9,"The generated code correctly performs all requested operations: loading the CSV, cleaning both 'Launched Price (India)' and 'RAM' columns (including error handling for conversions), filtering by company and price, selecting the specified columns, and sorting the final output. The code is concise and directly addresses the prompt.

Minor improvements could include:
- Adding `regex=False` to `str.replace()` calls for clarity and performance, especially when the pattern is a literal string.
- Explicitly handling `FileNotFoundError` or other potential `pd.read_csv` errors, though the task description for the generated code didn't specifically ask for robust error handling beyond data cleaning.
- Using `.copy()` after filtering to prevent potential `SettingWithCopyWarning` in more complex scenarios, though for this specific task, it might not manifest as an issue.
- The ground truth code includes helpful print statements for debugging and understanding the flow, which are good practices but were not explicitly required by the prompt for the ""generated code"".

Overall, it's a very good solution that directly answers the prompt's requirements.",gpt-3.5-turbo,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the RAM column
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')

# Clean the Battery Capacity column
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')

# Create scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(df['RAM'], df['Battery Capacity'], c=pd.factorize(df['Company Name'])[0])
plt....",7,"The generated code correctly loads the dataset, cleans the `RAM` and `Battery Capacity` columns by removing units and converting to numeric, and generates a scatter plot with appropriate axis labels and title. It successfully handles conversion errors by coercing to NaN using `errors='coerce'`.

However, there are a few areas for improvement:
1.  **Coloring by Company Name**: The task specifically asked to ""Color the points in the scatter plot according to the `Company Name` column."" The generated code uses `c=pd.factorize(df['Company Name'])[0]`. While this does assign different colors based on company name, it uses matplotlib's default color cycling which often results in a limited set of colors and no legend for what each color represents. The ground truth uses `seaborn.scatterplot` with the `hue` parameter, which automatically handles categorical coloring and generates a legend, making the plot much more interpretable.
2.  **Robustness**: The ground truth includes a `try-except` block for `FileNotFoundError` during CSV loading, which is good practice for robust code. The generated code assumes the file will always be present.
3.  **Clarity of cleaning**: The generated code cleans the columns in-place: `df['RAM'] = pd.to_numeric(...)`. While functional, creating new `_cleaned` columns (as in the ground truth) can sometimes be preferable to preserve the original data, especially in a data cleaning pipeline. This is a minor point but relates to good data handling practices.
4.  **Whitespace/strip**: The ground truth explicitly uses `.str.strip()` after removing 'GB' or 'mAh' to handle potential leading/trailing whitespace, which makes the conversion more robust. The generated code omits this.",gpt-3.5-turbo,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"import pandas as pd

# 1) Load the dataset and display the first 5 rows
df = pd.read_csv(""titanic.csv"")
print(df.head())

# 2) Show the shape of the dataset (rows and columns)
print(""Shape of the dataset:"", df.shape)

# 3) List all column names and their data types
print(""Column names and data types:"")
print(df.dtypes)

# 4) Calculate the survival rate (percentage of passengers who survived)
survival_rate = df[""Survived""].mean() * 100
print(""Survival rate: {:.2f}%"".format(survival_rate))

# 5) S...",9,"The generated code correctly performs all the required tasks: loading data, displaying the head, showing shape, listing dtypes, calculating survival rate, and counting missing values.

Strengths:
- Correctly uses `pandas` for all data manipulations.
- The survival rate calculation `df[""Survived""].mean() * 100` is a very concise and efficient way to achieve the desired result, leveraging the fact that `mean()` works directly on binary (0/1) columns as proportions.
- All tasks are completed as requested.
- Clear and concise code.

Areas for Improvement (minor):
- The `df.dtypes` output for column names and data types is correct but `df.info()` from the ground truth provides a more comprehensive overview including non-null counts and memory usage, which is often preferred for initial EDA. However, `df.dtypes` *does* fulfill the specific requirement of ""List all column names and their data types.""
- No robust error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth. While not explicitly asked for, it's good practice.
- The output formatting could be slightly more explicit (e.g., adding labels like ""1. First 5 rows..."" as in the ground truth) for better readability, but this is a stylistic preference and doesn't impact correctness.",gpt-3.5-turbo,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 1) List all column names and their data types
print(df.dtypes)

# 2) Calculate and report basic descriptive statistics
numerical_columns = ['Year', 'Rating', 'Runtime']
statistics = df[numerical_columns].describe()
print(statistics)

# 3) Report the total number of movies in the dataset
total_movies = df.shape[0]
print(""Total number of movies:"", total_movies)
```",6,"The generated code correctly loads the dataset, lists column types, and reports the total number of movies. It also attempts to get descriptive statistics.

However, there are several key areas for improvement that prevent a higher score:

1.  **Robustness (File Not Found):** The code lacks error handling for `FileNotFoundError`. If `imdb_top_250.csv` is not present, the script will crash. The ground truth includes a `try-except` block for this, which is crucial for production-ready code.
2.  **Descriptive Statistics Specificity:** The `describe()` method outputs more statistics than requested (e.g., count, 25%, 75%). The task specifically asked for mean, median, standard deviation, min, and max. The ground truth demonstrates how to select only these specific statistics.
3.  **Median (50th percentile):** While `describe()` outputs the 50th percentile, it doesn't explicitly name it ""median"". The ground truth renames '50%' to 'median' for clarity, which aligns better with the requested output.
4.  **Runtime Preprocessing:** The `Runtime` column in the `imdb_top_250.csv` dataset is typically in a string format like ""142 min"". The generated code directly calls `describe()` on it, which will likely result in `NaN` or incorrect statistics for `Runtime` if it's not pre-processed into a numeric type. The ground truth specifically handles this conversion. This is a significant omission for a dataset with this common characteristic.
5.  **Output Readability:** While `df.dtypes` works, `df.info()` as used in the ground truth often provides a more comprehensive and readable overview, including non-null counts, which is useful for data quality assessment.
6.  **Clarity of Output:** The output for descriptive statistics could be more clearly labeled, similar to the ground truth.

To improve, the code should:
*   Add error handling for file loading.
*   Pre-process the 'Runtime' column to convert it to a numeric type before calculating statistics.
*   Filter the `describe()` output to only include the requested statistics (mean, median, std, min, max).
*   Consider using `df.info()` for column types instead of `df.dtypes` for better output.",gpt-3.5-turbo,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

data = pd.read_csv('imdb_top_250.csv')
filtered_data = data[data['Rating'] >= 9.0]
result = filtered_data[['Title', 'Rating']].sort_values(by='Rating', ascending=False)
print(result)
```",9,"The generated code correctly addresses all the requirements of the task: it imports pandas, loads the CSV, filters by Rating >= 9.0, selects 'Title' and 'Rating' columns, and sorts the result by 'Rating' in descending order. The code is concise and directly implements the required steps.

However, the provided ground truth code includes robust error handling for `FileNotFoundError`, `KeyError`, and general exceptions, which is a good practice for production-ready code or more reliable scripts. The generated code lacks this error handling. While not explicitly asked for in the prompt (""Provide only the Python code without explanations.""), robust error handling is generally considered part of ""proper imports, structure, etc."" for production code. For a task focused purely on the logical steps of data manipulation, the generated code is excellent. The absence of error handling is the only reason it doesn't receive a perfect 10, as it makes the code less robust in a real-world scenario where the file might be missing or columns might be misnamed.",gpt-3.5-turbo,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create a histogram
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Rating', bins=10, kde=True)
plt.title('Distribution of Ratings in IMDb Top 250 Movies')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# Create a scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='Runtime', y='Rating')
plt.title('Relationship between Rating and Runt...",8,"The generated code correctly addresses the core requirements of loading the data, creating a histogram for `Rating`, and a scatter plot for `Rating` vs. `Runtime`. Both plots have appropriate titles and axis labels, and necessary libraries are imported.

The main area for improvement that prevents a higher score is the lack of robust error handling for file not found and, more critically, the absence of data preprocessing for the `Runtime` column. The `Runtime` column in the `imdb_top_250.csv` dataset typically contains strings like ""120 min"", which need to be converted to numerical format for proper plotting. The generated code assumes `Runtime` is already numeric, which would cause an error or incorrect plot if the raw string format is used. The ground truth code handles this robustly. Adding error handling for file loading and robust processing for the `Runtime` column would significantly improve the solution's quality and reliability.",gpt-3.5-turbo,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Report number of missing values for each column
print(df.isnull().sum())

# Drop rows with missing values in the specified numerical columns
df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'], inplace=True)

# Calculate and report descriptive statistics for the numerical columns
print(df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']].describe())

# C...",6,"The generated code addresses most of the task requirements but lacks some crucial details and robustness present in the ground truth.

**Strengths:**
*   Correctly loads the CSV using pandas.
*   Reports NA counts per column (though the output format is less clear than the ground truth).
*   Correctly uses `dropna` with `subset` to filter rows based on NAs in the four numerical columns.
*   Calculates descriptive statistics for the specified columns on the filtered data using `.describe()`.
*   Correctly counts unique values for `species` and `island` using `nunique()`.

**Areas for Improvement / Missing Features:**

1.  **Reporting NA Counts:** The generated code simply prints `df.isnull().sum()`. While this technically reports the counts, it doesn't filter out columns with zero missing values, nor does it provide a clear heading or context as the ground truth does. The output format is less user-friendly specifically for reporting *missing values*.
2.  **Descriptive Statistics Output**: Using `.describe()` is a good shortcut for descriptive statistics, but the task specifically requested (mean, median, standard deviation, min, max), and `.describe()` also includes count, 25%, 50% (median), and 75% percentiles. While it contains the requested items, it's not as precise as the `.agg()` method used in the ground truth if only those specific metrics were desired.
3.  **Robustness (Error Handling/File Not Found):** The generated code does not include any error handling for `FileNotFoundError` when loading the CSV, which is a critical consideration for a production script. The ground truth demonstrates robust error handling.
4.  **Clarity and Readability:** The ground truth code includes print statements with headers and separators to make the output very clear and readable, indicating exactly what information is being presented at each step. The generated code lacks this level of presentation, making the output less interpretable for a user.
5.  **Intermediate State Reporting:** The ground truth clearly reports the number of rows before and after the `dropna` operation, which is very helpful for understanding the impact of the filtering. The generated code skips this.
6.  **`inplace=True` usage:** While `inplace=True` works, it's generally discouraged in modern pandas workflows. Assigning the result back to `df` (e.g., `df = df.dropna(...)`) is often preferred for clarity and to avoid unexpected side effects, though for this specific script, it doesn't cause an issue.

To get a higher score, the generated code needs to match the ground truth's level of robustness, output clarity, and precise adherence to requested statistics.",gpt-3.5-turbo,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"import pandas as pd

penguins = pd.read_csv('penguins.csv')
filtered_penguins = penguins[(penguins['species'] == 'Adelie') & (penguins['island'] == 'Torgersen')]
filtered_penguins = filtered_penguins[['culmen_length_mm', 'culmen_depth_mm']].dropna()
print(filtered_penguins.head())",9,"The generated code correctly performs all the required steps: loading the CSV, filtering by species and island, selecting specific columns, dropping NaNs in those columns, and displaying the first 5 rows. The code is concise and efficient.

Areas for minor improvement:
1. The `dropna()` call without a `subset` argument technically drops rows where _any_ column has a NaN. While in this specific case it works because only the two selected columns remain, specifying `subset=['culmen_length_mm', 'culmen_depth_mm']` would make the intent clearer and be more robust if other columns were accidentally retained earlier.
2. The print statement `print(filtered_penguins.head())` implicitly defaults to 5 rows, which matches the requirement. However, explicitly calling `head(5)` is good practice for clarity.
3. The ground truth includes error handling for `FileNotFoundError`, which is good practice for production-ready code, though not explicitly requested in the prompt's core task requirements. For a direct functional comparison, the generated code is excellent.",gpt-3.5-turbo,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"import pandas as pd
import matplotlib.pyplot as plt

# Load the penguins dataset
df = pd.read_csv('penguins.csv')

# Drop rows with missing values in the specified columns
df = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])

# Create a scatter plot with color based on species
plt.scatter(df['culmen_length_mm'], df['culmen_depth_mm'], c=pd.factorize(df['species'])[0])

# Add title and labels
plt.title('Culmen Length vs Culmen Depth by Species')
plt.xlabel('Culmen Length (mm)...",7,"The generated code correctly loads the data, filters missing values, and creates a scatter plot with a title and axis labels. However, it uses `matplotlib.pyplot.scatter` directly with `c=pd.factorize(df['species'])[0]` for coloring. While this technically colors by species, it assigns an integer color to each species, losing the automatic legend and distinct color mapping that `seaborn.scatterplot` (used in the ground truth) provides. This makes it harder to interpret which color corresponds to which species without additional code for a custom legend. The prompt specifically mentioned using Matplotlib *or* Seaborn and the ground truth used Seaborn for its convenience in handling categorical colors and legends. Using `matplotlib.pyplot.scatter` for categorical coloring and providing a legend requires significant additional effort compared to `seaborn.scatterplot`. The code also lacks proper error handling for `FileNotFoundError`.",gpt-3.5-turbo,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Read the dataset
df = pd.read_csv(""penguins.csv"")

# Fill NA values with the mean for culmen_length_mm and culmen_depth_mm columns
df['culmen_length_mm'].fillna(df['culmen_length_mm'].mean(), inplace=True)
df['culmen_depth_mm'].fillna(df['culmen_depth_mm'].mean(), inplace=True)

# Calculate and print the mean, median, and standard deviation for culmen_length_mm and culmen_depth_mm columns
mean_length = df['culmen_length_mm'].mean()
median_length = df['culmen_leng...",8,"The generated code correctly addresses all the requirements of the task. It loads the dataset, fills NA values in the specified columns with their respective means, calculates and prints the mean, median, and standard deviation for these columns, and finally prints the total number of rows and columns. The use of `inplace=True` for `fillna` is efficient.

Areas for minor improvement (to achieve a perfect score like the ground truth):
1.  **Error Handling:** The generated code lacks error handling for `FileNotFoundError` when reading the CSV. If `penguins.csv` is not present, it will raise an unhandled exception, which is not ideal for robust code. The ground truth includes a `try-except` block for this.
2.  **Readability/Generalization of Statistics Printing:** While functional, the printing of statistics is hardcoded for each column. The ground truth uses a loop, which is more scalable and cleaner if more columns were to be added or if the task implied processing a dynamic list of columns. Using f-strings and formatting (`.2f`) would also improve the output's readability as seen in the ground truth.
3.  **Variable Names:** While `mean_length`, `median_length`, etc., are clear, defining the numerical columns as a list at the top (like `NUMERICAL_COLUMNS` in the ground truth) promotes better maintainability and makes the code slightly more robust to changes in column names, as one would only need to update the list, not every instance of the column name.",gpt-3.5-turbo,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows where flipper_length_mm is NA
df = df.dropna(subset=['flipper_length_mm'])

# Create histogram of flipper_length_mm
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='flipper_length_mm', kde=True)
plt.title('Distribution of Flipper Length (mm)')
plt.xlabel('Flipper Length (mm)')
plt.show()

# Create bar chart showing counts of penguins by island
...",9,"The generated code correctly addresses all parts of the task: loading the data, dropping NA values in `flipper_length_mm`, creating a histogram for `flipper_length_mm` with the specified title and x-axis label, and creating a bar chart for `island` counts with the specified title and axis labels. The use of `seaborn` for plotting is appropriate and efficient.

Strong points:
- Correctly loads the CSV.
- Correctly handles NA values in `flipper_length_mm` by dropping rows.
- Generates the required histogram with correct title and x-axis label.
- Generates the required bar chart with correct title and axis labels.
- Uses `matplotlib.pyplot` and `seaborn` effectively.

Areas for minor improvement (not affecting the core requirements significantly but enhancing robustness/completeness):
- The histogram's y-axis label defaults to 'Count' with `sns.histplot`, but explicitly setting it with `plt.ylabel('Count')` would make it more robust against potential changes in `seaborn` defaults or if `plt.hist` were used.
- The bar chart for 'island' explicitly labels the X and Y axes, which is great.
- No error handling for `FileNotFoundError` when loading the CSV.
- No explicit `plt.ylabel()` call for the histogram, though `seaborn` automatically provides it.
- While the task didn't explicitly ask for `sns.set_theme` or error handling, these are good practices exemplified in the ground truth.

Overall, it's a very good solution that fully meets the explicit requirements of the task.",gpt-3.5-turbo,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"import pandas as pd

# Load the dataset
penguins_df = pd.read_csv('penguins.csv')

# Filter the data to include only penguins from the 'Torgersen' island
torgersen_penguins = penguins_df[penguins_df['island'] == 'Torgersen']

# Count and print the number of penguins belonging to each species
species_count = torgersen_penguins['species'].value_counts()
print(species_count)",9,"The generated code correctly implements all the core requirements of the task: loading the data, filtering by island, and counting species. The code is concise and directly addresses the problem.

The main strong points are:
- **Correctness:** It produces the expected output for the given task.
- **Conciseness:** The code is very direct and avoids unnecessary verbosity.
- **Proper Pandas Usage:** `pd.read_csv`, boolean indexing, and `value_counts()` are used correctly and efficiently.

Areas for minor improvement (which prevent a perfect 10, but recognize that this is a benchmark and not a production-ready error-proof solution):
- **Error Handling:** The generated code lacks explicit error handling for `FileNotFoundError` when loading the CSV or `KeyError` if columns like 'island' or 'species' are missing. While the ground truth includes this for robustness, for a benchmark focused on the core logic, omitting it might be acceptable depending on the specific evaluation criteria. However, in a real-world scenario, this would be crucial.
- **Informative Output:** The output is just the `species_count` Series. Adding a descriptive print statement (e.g., ""Species count for penguins on Torgersen island:"") would make the output more user-friendly, as shown in the ground truth.
- **Edge Case Handling:** There's no specific check for `torgersen_penguins` being empty, which the ground truth includes. If there were no Torgersen penguins, `value_counts()` would still work, but an empty Series would be printed, which might not be as informative as a specific message indicating no penguins were found.

Overall, it's a very good solution that solves the core problem effectively.",gpt-3.5-turbo,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

print(""Shape of DataFrame:"", df.shape)
print(""Data types of columns...",10,"The generated code is excellent. It correctly defines the DataFrame as specified, precisely prints its shape, and accurately lists the data types of each column. All requirements of the task are met, and the code is clean and well-structured, comparable to the ground truth code in its essential operational aspects. The minor differences in print statements (e.g., lack of separation lines) do not detract from its correctness or completeness for the given task.",gpt-3.5-turbo,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('titanic.csv')

# Bar chart showing count of survivors vs non-survivors
survivors_count = df['Survived'].value_counts()
plt.bar(['Non-Survivors', 'Survivors'], survivors_count)
plt.title('Count of Survivors vs Non-Survivors')
plt.xlabel('Survival Status')
plt.ylabel('Count')
plt.show()

# Bar chart showing distribution of passenger classes
pclass_count = df['Pclass'].value_counts()
plt.bar(pclass_count.index...",7,"The generated code successfully creates four distinct plots with proper titles and labels, covering survival, class, age, and gender distributions, which aligns with the core requirements.

However, there are several areas for improvement:
- **Missing Age Value Handling:** The code attempts to plot the age histogram directly using `df['Age'].hist()`. While `DataFrame.hist()` handles NaNs by default by ignoring them, the task explicitly asked to ""handle missing values appropriately."" The ground truth demonstrates filling NaNs with the median, which is a more robust approach for displaying the age distribution accurately, as simply ignoring them can distort the visual representation if many values are missing. The generated code implicitly ignores NaNs, which is not an active strategy for handling them.
- **Plot Customization and Readability:** The generated plots lack some of the polish and readability features present in the ground truth. For example, `plt.show()` is called immediately after each plot, which clears the figure without providing options for `plt.tight_layout()` or adding grids, which improve the visual presentation. The X-axis labels for ""survivors vs non-survivors"" in the generated code are not explicitly mapped to descriptive terms (`0` and `1` are plotted directly as tick labels unless `value_counts()` happens to sort them, which can be less intuitive than 'Non-Survivors' and 'Survivors').
- **Robustness:** The ground truth includes error handling for `FileNotFoundError` when loading the CSV, which is a good practice for real-world scenarios. The generated code assumes the file will always be present.
- **Consistency in Plotting Method:** The first bar chart uses `plt.bar(['Non-Survivors', 'Survivors'], survivors_count)` where `survivors_count` is a Pandas Series, directly plotting the values, but the x-tick labels are hardcoded assuming a specific order. The other bar charts correctly use `.index` and `.values` from the Series/DataFrame for plotting, which is more robust.
- **Imports:** While basic imports are there, `numpy` is not imported, which is useful for more advanced data manipulations, though not strictly required for this specific task.

Overall, it's a solid attempt that gets the job done for the most part, but lacks some of the robustness, best practices, and visualization refinements found in the ground truth.",gpt-3.5-turbo,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'], 
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'], 
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89], 
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93], 
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]}

df = pd.DataFrame(data)

print(""Average Midterm Score:"", df['Midterm_Score'].mean())...",9,"The generated code correctly creates the DataFrame, calculates the average midterm and final scores, and identifies the subject with the highest average final score. The code is clean, concise, and follows standard pandas practices.

The score is 9 instead of 10 because while functionally correct, the output formatting for the average scores could be improved to match the ground truth (e.g., using f-strings for more precise output and formatting to two decimal places). Additionally, the ground truth code also prints the actual highest average score value for the subject, which the generated code omits. This is a minor detail but contributes to the overall completeness and readability of the output. However, for the core requirements, it hits all points.",gpt-3.5-turbo,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
        'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
        'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
        'Volume': [1000000, 120...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It successfully creates the DataFrame, calculates the 'Daily_Range', finds the date with the highest 'Close' price, and prints the updated DataFrame as requested. The use of `idxmax()` is efficient for finding the date.

One minor improvement is that the output format for the highest close date could be more consistent with typical date formats (e.g., YYYY-MM-DD or similar, which is what the ground truth does by using `strftime`). While `max_close_date` (which is a pandas Timestamp object due to `idxmax()` on a datetime index) will print a readable date by default, explicitly formatting it can sometimes prevent unexpected outputs if the internal representation changes or if the user's environment has different default string conversions for Timestamp objects. However, for this specific problem, it performs correctly as requested.

The code quality is good, with proper imports and a logical flow.",gpt-3.5-turbo,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST001']
}

df = pd...",9,"The generated code correctly performs all the required tasks. It creates the DataFrame, calculates the 'Total_Revenue' column, identifies the best-selling product by quantity, and prints the head of the DataFrame. The code is concise and efficient.

However, there is a minor point for improvement:
- The `print(df.head())` implicitly prints the first 5 rows, which matches the requirement. However, for explicit clarity and to closely match the ground truth's verbosity, `print(df.head(5))` would be slightly better practice as it explicitly states the number of rows. This is a very minor point and doesn't detract from the correctness or overall quality significantly.",gpt-3.5-turbo,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}

df = pd.DataFrame(data)

# Calculate and print the average ""SessionDurationMinutes""
average_session_duration = df[""SessionDurationMinutes""].mean()
print(""Average...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the average ""SessionDurationMinutes"" using `mean()`, and correctly counts users per ""DeviceType"" using `value_counts()`. All requirements of the task are met. The code is clean, concise, and directly addresses the problem statement. The output format for printing is slightly different from the ground truth (e.g., no f-string formatting for the average, no separators), but this difference does not impact correctness or completeness of the core tasks.",gpt-3.5-turbo,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

df = pd.DataFrame(data)

max_temp = df[""TemperatureCelsius""].max()
min_humidity = df[""HumidityPercent""].min()

pr...",10,"The generated code is entirely correct and complete. It accurately defines the DataFrame as specified, and correctly finds and prints both the maximum ""TemperatureCelsius"" and the minimum ""HumidityPercent"". The imports are correct, and the code structure is clean and readable. The variable names are appropriate. All requirements of the task are met.",gpt-3.5-turbo,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}

df = pd.DataFrame(data)

# Print the total number of ""Likes"" across all posts
total_likes = df[""Likes""].sum()
print(""Total number of Likes across all posts:"", tota...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the total number of ""Likes"", and precisely identifies the ""PostType"" with the highest average number of ""Shares"". The code is well-structured, uses appropriate pandas functions, and directly addresses all requirements of the task. The output format is also clear and matches the expected behavior.",gpt-3.5-turbo,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"import pandas as pd
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""Male"", ""Mal...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the average age, and counts patients by gender, fulfilling all requirements of the task. The code is clean, well-structured, and directly addresses the prompt without any unnecessary additions. It directly implements the specified DataFrame creation method and then proceeds to perform the requested calculations using appropriate pandas methods (`.mean()` and `.value_counts()`). The output format is also clear and informative.",gpt-3.5-turbo,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"import pandas as pd
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}
df = pd.DataFrame(data)

# 1) Print the ""Title"" of the movie with the highest ""Rating""
highest_rating_movie = df[df[""...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately identifies and prints the title of the movie with the highest rating, and correctly calculates and prints the average year of all movies. The code is concise, efficient, and directly addresses all parts of the task. The use of `.values[0]` to extract the title from the filtered DataFrame is an effective and common Pandas idiom. This solution is production-ready for the given task.",gpt-3.5-turbo,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

df = pd.DataFrame(data)

print(""Total number of cars:"", len(df))
print(""Average PriceUSD of all cars:"",...",10,"The generated code is excellent. It correctly defines the DataFrame as specified, precisely calculates and prints the total number of cars, and accurately calculates and prints the average ""PriceUSD"". The use of `len(df)` for total cars and `df[""PriceUSD""].mean()` for average price is perfectly idiomatic pandas and Python. All requirements of the task are met clearly and concisely. The code is clean, efficient, and directly addresses the prompt.",gpt-3.5-turbo,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}

df = pd.DataFrame(data)

pokemon_highest_hp = df[df[""HP""] == df[""HP""].max()][""Name""].value...",10,"The generated code is excellent. It correctly defines the DataFrame and accurately identifies and prints the names of the Pokemon with the highest HP and lowest Defense. The approach used by the generated code (filtering the DataFrame directly using `df[""HP""] == df[""HP""].max()`) is a valid and often concise alternative to `idxmax()/idxmin()`, especially when only one such value is expected. The code is complete, correct, and well-structured, meeting all requirements.",gpt-3.5-turbo,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('titanic.csv')

# Identify columns with missing values
missing_cols = df.columns[df.isnull().any()].tolist()

# Fill missing Age values with the median age
df['Age'] = df['Age'].fillna(df['Age'].median())

# Fill missing Embarked values with the most frequent port
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])

# Drop the Cabin column due to excessive missing values
df = df.drop(columns=['Cabin'])

# Create a new column '...",8,"The generated code correctly performs all the required data cleaning steps: loading the dataset, identifying missing values (though this was not explicitly printed as in the ground truth, it was identified internally), filling missing 'Age' and 'Embarked' values, dropping the 'Cabin' column, and extracting the 'Title' from the 'Name' column. It also correctly displays the shape and head of the cleaned dataset. The code is concise and efficient.

Areas for minor improvement (to achieve a higher score):
1.  **Error Handling for File Not Found**: The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.
2.  **Displaying Missing Columns**: While the code internally identifies `missing_cols`, it doesn't print this information as suggested by the task's first point (""identify columns with missing values""). The ground truth explicitly shows this.
3.  **Title Normalization**: The ground truth code includes a step to standardize less common titles (e.g., 'Mlle' to 'Miss', 'Capt' to 'Rare'). While the prompt didn't explicitly ask for this, it's a common and beneficial step in data cleaning for the 'Title' feature and makes the generated feature more robust. The generated code simply extracts the title without further normalization.
4.  **Clarity/Comments**: Adding comments, especially for each step as outlined in the task, would improve readability and make the code's purpose clearer, similar to the ground truth.",gpt-3.5-turbo,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}

df = pd.DataFrame(data)

# Print the ""Title"" of the...",10,"The generated code is excellent. It correctly defines the DataFrame, identifies the most expensive book by title, and accurately calculates the total stock. The code is concise, uses appropriate pandas functions (`idxmax`, `loc`, `sum`), and directly addresses all parts of the task. The variable names are clear, and the print statements are informative. It fully meets all requirements.",gpt-3.5-turbo,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"import pandas as pd

# Load the dataset and display the first 10 rows
df = pd.read_csv(""water_potability.csv"")
print(df.head(10))

# Show the shape of the dataset
print(""Shape of the dataset:"", df.shape)

# List all column names and their data types
print(""Column names and data types:"")
print(df.dtypes)

# Calculate the percentage of potable vs non-potable water samples
print(""Percentage of potable vs non-potable water samples:"")
print(df['Potability'].value_counts(normalize=True) * 100)

# Disp...",7,"The generated code correctly performs all the requested tasks: loading the dataset, displaying the head, showing the shape, listing dtypes, calculating potability percentages, showing missing values, and displaying descriptive statistics.

However, it lacks robust error handling, especially for file loading (`FileNotFoundError`), which is present in the ground truth. It also doesn't include informative print statements or separators to clearly delineate each section of the analysis, making the output less readable compared to the ground truth. The ground truth also explicitly checks for the existence of the 'Potability' column before attempting to use it, which is a good practice for robustness. While the core functionality is there, these aspects of resilience and user-friendliness could be improved to achieve a higher score.",gpt-3.5-turbo,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Identify columns with missing values
missing_values = df.isnull().sum()

# Fill missing values using mean for numerical columns
df.fillna(df.mean(), inplace=True)

# Check for and remove duplicate rows
df.drop_duplicates(inplace=True)

# Verify values are within reasonable ranges
# Assuming the range of water quality parameters

# Create a summary
summary = {
    'Before': {
        'Missing Values Count': missin...",4,"The generated code attempts to address the requirements but falls short in several critical areas, leading to incorrect or incomplete execution of the task.

Primary reasons for failure:
1.  **Missing Value Handling:** The `df.fillna(df.mean(), inplace=True)` line will only fill missing values for numerical columns. While this is the correct strategy for numerical columns, the code does not explicitly identify which columns are numerical or verify that this operation successfully filled *only* numerical columns. More importantly, it calculates `df.mean()` *after* `missing_values = df.isnull().sum()` but *before* `df.fillna()`. The `summary['Before']` then uses the `missing_values` calculated *before* filling, but `summary['After']` uses `df.isnull().sum()` *after* filling. This makes the ""Before"" summary inaccurate if `df.mean()` calculation itself is affected by missing values in a way that the user might not expect (though for filling numerical NaNs with mean of numeric cols, this is fine).
2.  **Range Validation (Critical Failure):** The prompt explicitly asks to ""Verify that all values are within reasonable ranges for water quality parameters"". The generated code includes only a comment `# Assuming the range of water quality parameters` and does *not* implement any range validation logic. This is a significant omission and makes the cleaning process incomplete and potentially misleading. The ground truth correctly defines and applies ranges based on common sense for water quality parameters.
3.  **Before/After Comparison (Incomplete and Misleading):** The requested summary should show the *impact* of cleaning. The generated code calculates `missing_values` *before* any cleaning and then stores it in `summary['Before']`. However, it calculates `df.shape` *after* duplicate removal and potentially range checks (if they were implemented) *before* assigning it to `summary['Before']['Dataset Shape']`. This means the ""Before"" shape already reflects changes from duplicate removal, which is incorrect for a ""before cleaning"" state. The ground truth correctly captures the state before *any* cleaning steps begin for the ""Before"" summary. The generated code also calculates `df.shape` for ""Before"" and ""After"" at the same point, making the ""After"" shape effectively the same as ""Before"" in the summary if no further operations were done.
4.  **No Before/After Printout:** The prompt asks to ""Create a summary showing before and after statistics"" and ""Display the first 5 rows"". The generated code creates a `summary` dictionary but *never prints it*. This means the user is not shown the results of the cleaning process, which is a core requirement. The ground truth code correctly prints out all the before and after statistics.
5.  **Error Handling:** The code does not include any error handling for `FileNotFoundError` when loading the CSV, which is good practice for robustness.
6.  **Readability/Structure:** The code is a single block without comments explaining individual steps, unlike the ground truth which explicitly breaks down each requirement with comments.",gpt-3.5-turbo,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
data = pd.read_csv(""water_potability.csv"")

# Bar chart for distribution of potable vs non-potable water samples
plt.figure(figsize=(6, 4))
sns.countplot(x='Potability', data=data)
plt.title('Distribution of Potable vs Non-Potable Water Samples')
plt.xlabel('Potability')
plt.ylabel('Count')
plt.show()

# Histograms for pH, Hardness, and Chloramines
fig, axs = plt.subplots(1, 3, figsize=(15, 5))
d...",7,"The generated code correctly addresses most of the tasks, producing the four required visualizations. It uses proper imports (pandas, matplotlib, seaborn).

However, there are several areas for improvement as per the prompt's explicit requirements and good visualization practices:

1.  **Readability and Specificity in Bar Chart:** The labels for the 'Potability' bar chart are just ""0"" and ""1"", which is less informative than ""Not Potable"" and ""Potable"" as specified in the ground truth. The prompt explicitly asked for ""proper titles, axis labels, and legends where appropriate.""
2.  **Histogram Quality (Pandas vs. Seaborn):** The prompt requested histograms for pH, Hardness, and Chloramines, showing their distributions. While the generated code uses `data['column'].plot(kind='hist', ax=axs[i])`, which is functional, `seaborn.histplot` (as used in the ground truth) is generally preferred for its enhanced aesthetic and statistical capabilities (e.g., `kde=True` for density estimation, more customizable bins, better handling of missing values). The ground truth specifically mentioned ""showing their distributions,"" and `histplot` with KDE often provides a clearer picture of the distribution shape.
3.  **Missing Value Handling:** The prompt's evaluation criteria included ""handle missing values in plots."" While Seaborn functions often handle NaNs reasonably well by default (ignoring them), explicitly calling `.dropna()` on the series before plotting (as in the ground truth for histograms) is good practice for clarity and control when dealing with individual series, ensuring the plot doesn't implicitly hide missing data effects through statistical aggregations. For `hist()`, `data['ph'].plot(kind='hist')` simply ignores NaNs, which is acceptable but not explicitly robust.
4.  **Plot Formatting and Detail:**
    *   The generated code lacks `plt.tight_layout()` after each `plt.show()` block, which is essential to prevent labels/titles from overlapping.
    *   No `sns.set_style(""whitegrid"")` or similar style setting was applied, which helps with consistency and readability.
    *   The `plt.show()` calls are interleaved, which means each plot appears and then closes before the next one is generated. The ground truth groups plots with `plt.show()` after each figure setup, which is generally better for a script generating multiple independent figures, or for interactive viewing.
    *   The titles, while present, are slightly less descriptive (e.g., ""Comparison of pH levels between Potable and Non-Potable Water"" vs. ""pH Levels by Water Potability""). The ground truth adds more specific labels and font sizes, aligning with ""proper titles, axis labels"" and the ""display distribution and relationship patterns clearly"" criteria.
    *   The `figsize` for the correlation heatmap is smaller (8,6) compared to the ground truth's (10,8), which makes the annotations harder to read. Also, the ground truth rotated x-axis labels for better readability on long titles.

The code is functional and produces the requested plots, but it misses several details concerning robust plotting practices, explicit handling of missing values (even if handled implicitly by seaborn), and adherence to the ""proper formatting"" aspect of the prompt.",gpt-3.5-turbo,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"import pandas as pd
import numpy as np
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Feature engineering
data['total_acidity'] = data['fixed acidity'] + data['volatile acidity']
data['alcohol_acidity_ratio'] = data['alcohol'] / data['total_acidity']
data['sugar_alcohol_interaction'] = data['residual sugar'] * data['alcohol']

# Feature selection
X = data.drop...",6,"The generated code correctly attempts to solve the problem by implementing feature engineering and two feature selection methods. However, it falls short in several aspects that prevent it from being a complete and high-quality solution according to the prompt's requirements.

**Strengths:**
*   Correctly loads data.
*   Includes three new engineered features, demonstrating understanding of feature engineering.
*   Applies RFE with RandomForestRegressor and SelectKBest with `f_regression` as requested.
*   Separates features (X) and target (y).
*   Correctly identifies selected features for both methods.

**Areas for Improvement / Missing Aspects:**
1.  **Meaningful Engineered Features:** While three features are created, their 'meaningfulness' is debatable without further context. The ground truth provides more thoughtful features like `acidity_ratio`, `alcohol_sulfates_interaction`, and `free_to_total_sulfur_dioxide_ratio`, explaining their potential significance. The `alcohol_acidity_ratio` is okay, but `total_acidity` is a simple sum and `sugar_alcohol_interaction` might be meaningful but lacks explanation.
2.  **Number of Selected Features:** The prompt asked for 5-7 features, but the generated code fixed it to 5. This is a minor point but demonstrates a lack of flexibility.
3.  **Comparison and Contrast:** This is a major missing component. The prompt explicitly asked to ""Compare and contrast the feature sets selected by each method"" and the generated code only prints the selected features, offering no comparison, commonalities, or differences. This is a crucial step in the task.
4.  **Display Importance Scores/Rankings:** The prompt asked to ""Display the final selected features with their importance scores or rankings."" The RFE output does not provide importance scores for the selected features directly (it just tells you which ones are selected). The `SelectKBest` scores are available (`skb.scores_`) but are not displayed. The ground truth correctly extracts and displays these.
5.  **Handling Division by Zero/NaN/Inf:** The engineered features `alcohol_acidity_ratio` could potentially lead to division by zero if `total_acidity` is zero. The ground truth code handles this robustly with `EPSILON` and subsequent NaN/inf handling. This is a significant quality and correctness issue in real-world data science.
6.  **Scaling:** While not explicitly required for these specific methods (especially Random Forest), it's good practice and often necessary for other machine learning algorithms. The ground truth includes `StandardScaler`.
7.  **Readability/Comments:** The generated code is minimal and lacks comments explaining the purpose of each section, which the prompt implies by asking for ""proper imports, structure, etc.""

The primary shortcomings are the lack of comparison/contrast and the failure to display importance scores/rankings, which are explicit requirements of the prompt. Additionally, the lack of robust error handling for engineered features is a concern.",gpt-3.5-turbo,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.im...",6,"The generated code attempts to address most of the requirements but falls short in several areas related to correctness, completeness, and best practices for data science workflows.

**Strengths:**
*   It loads the data and defines X and y.
*   It implements multiple ensemble methods (RF, GB, XGB, AdaBoost).
*   It includes a voting ensemble.
*   It performs hyperparameter tuning (though very limited in scope).
*   It implements a stacking ensemble.
*   It calculates several evaluation metrics (MAE, MSE, RMSE, R).
*   It generates a residual plot.

**Weaknesses and Areas for Improvement:**

1.  **Preprocessing Workflow (Correctness & Quality):**
    *   **Data Leakage:** The `train_test_split` occurs *before* preprocessing (imputation and one-hot encoding). This leads to data leakage because the `SimpleImputer` and `OneHotEncoder` are `fit_transform`ed on `X_train` and then `transform`ed on `X_test`. While `transform`ing `X_test` is correct, fitting the imputer on `X_train` after `train_test_split` means the imputer's mean/median is computed using data from the training set, which is then applied to the test set, preventing leakage. However, the ground truth uses a `Pipeline` with `ColumnTransformer` *before* cross-validation or `train_test_split`, which is the more robust and common approach in production-ready ML pipelines to avoid fitting on subsets and then applying. The generated code attempts to use `ColumnTransformer` but applies it directly to `X_train` and `X_test` *after* splitting, which is technically okay for splitting but doesn't integrate into a full pipeline for cross-validation later.
    *   **Numerical Imputation Strategy:** Uses `SimpleImputer(strategy=""mean"")` for numerical features. The ground truth uses `median`, which is generally more robust to outliers than the mean for imputation.
    *   **Categorical Encoding:** The generated code uses `LabelEncoder` initially, then `OneHotEncoder` within `ColumnTransformer`. `LabelEncoder` is not appropriate for nominal categorical features in tree-based models as it implies an ordinal relationship. `OneHotEncoder` is the correct approach, but the `LabelEncoder` step is unnecessary and potentially problematic if not fully undone before `OneHotEncoder`. The ground truth uses `OneHotEncoder` directly.
    *   **Missing Values in Target:** Drops rows with `Global_Sales` NaN values, which is good, but doesn't explicitly handle other missing values (e.g., `User_Score` or other columns) before splitting beyond the `SimpleImputer` in `ColumnTransformer`. The ground truth directly converts `'User_Score'` to numeric and then handles NaNs for it.

2.  **Cross-Validation Implementation (Correctness & Completeness):**
    *   The `cross_val_score` is called directly on the models *before* they are integrated into a `Pipeline` with the preprocessor. This means the cross-validation is performed on the already preprocessed `X_train` and `X_test` *outside* of a robust pipeline. The problem asks for cross-validation to evaluate each model on the *full* preprocessing chain. The ground truth uses `Pipeline` for each model during cross-validation, ensuring the preprocessing steps are applied correctly within each fold.
    *   Only MAE is evaluated during cross-validation. The requirement is to evaluate with MAE, MSE, RMSE, and R. The individual model evaluation metrics are only calculated *after* a single `train_test_split` fit, not through cross-validation.

3.  **Hyperparameter Tuning (Completeness & Scope):**
    *   Only `RandomForestRegressor` is tuned, and with a very limited `n_estimators` grid. The task asks for tuning the *best individual model* which requires first identifying it from initial CV results (as done in ground truth). The ground truth intelligently selects the best model and provides specific, relevant parameter grids for each type of regressor.

4.  **Feature Importance Plotting (Correctness & Completeness):**
    *   Uses `yellowbrick.model_selection.FeatureImportances`, but its `fit` method is called on `X_train` which has been preprocessed by `ColumnTransformer` (likely converting column names into generic array indices), making the feature names uninterpretable in the plot without manual mapping back to original features + one-hot encoded names. The ground truth explicitly reconstructs feature names after `OneHotEncoder` to provide meaningful feature importance plots.
    *   It only plots for the tuned Random Forest, not generally for ensemble models or the best ensemble as requested.

5.  **Ensemble Models (Completeness & Quality):**
    *   **VotingRegressor:** `VotingRegressor` `estimators` should be `(name, estimator)` tuples, and their `fit` method requires them to be untransformed estimators. The generated code passes `models` directly, which implicitly uses the tuples `('Random Forest', rf)` etc., but the `cross_val_score` for voting is called on `X_train` directly, again bypassing a proper pipeline.
    *   **StackingRegressor:** Similar to voting, the stacking model is trained directly on `X_train`/`X_test` instead of within a pipeline that includes preprocessing.

6.  **Overall Pipeline and Reproducibility (Quality):**
    *   The code lacks a clear, robust pipeline structure for preprocessing + modeling which is crucial for consistency and avoiding data leakage in real-world ML tasks. The ground truth extensively uses `Pipeline` and `ColumnTransformer`.
    *   No `np.random.seed` or `random_state` set for all models and splits, impacting reproducibility. The ground truth sets `random_state` for `train_test_split`, models, and `KFold`.

7.  **Error Analysis and Residual Plots (Completeness & Quality):**
    *   A residual plot is shown, but it's generated only for the stacking model (on the test set). The ground truth shows more comprehensive residual analysis (distribution and scatter plot) and ties it back to the potentially best performing model for analysis.

In summary, while the generated code touches upon many requested elements, its implementation often lacks the robustness and best practices of a production-ready data science workflow, particularly concerning data preprocessing, cross-validation integration, and full pipeline usage.",gpt-3.5-turbo,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import STL
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error
import pymc3 as pm
from pymc3.gp import Marginal
from pymc3.gp.cov import ExpQuad
from changefinder import ChangeFinder
from lifelines import KaplanMeierFitter

# Load...",2,"The generated code attempts to address the requirements but fails significantly in correctness, completeness, and code quality.

**Primary Reasons for Failure:**

*   **Data Loading and Preprocessing:** The most critical failure is the initial data loading and aggregation. The `df['Year'] = pd.to_datetime(df['Year'], format='%Y')` line will fail because the column in the dataset is `Year_of_Release`, not `Year`. Even if corrected, `df.groupby('Year').sum()` would sum *all* numeric columns, which is not what's intended for `Global_Sales` aggregation. Furthermore, attempting to use `Genre`, `Platform`, and `Publisher` directly in a Pandas DataFrame that has been `groupby().sum()`'d will lead to `KeyError` as these categorical columns would no longer exist after the `sum()` aggregation. This fundamental error cascades throughout the entire code, making almost all subsequent steps invalid.
*   **ARIMA Modeling:** The `statsmodels.tsa.arima.model.ARIMA` model expects a series, but the initial data processing is flawed. It uses a fixed `order=(5,1,0)` without any automated selection (`auto_arima`), which is less robust. There's no visualization or interpretation of the forecast, and crucially, no confidence/prediction intervals are extracted or plotted as required.
*   **Seasonal Decomposition:** The `STL` model is called with `seasonal=13`. For yearly data, `seasonal` should typically be 1 or not defined, as yearly data doesn't have a 13-period seasonality within a year. Also, the output `seasonal, trend, resid = stl.fit()` is not used or visualized.
*   **Multivariate Regression:** The `PolynomialFeatures(interaction_only=True)` is incorrectly applied to `Year`, `Genre`, `Platform`, `Publisher` *after* the `groupby().sum()` operation, meaning these columns do not exist. Even if they did, `Genre`, `Platform`, and `Publisher` are categorical and need proper encoding (e.g., one-hot encoding) before being fed into `LinearRegression` or `PolynomialFeatures`. The model fitting line `reg = LinearRegression().fit(X_poly, df['Global_Sales'])` will fail due to these data preparation issues. No confidence or prediction intervals are extracted for this model.
*   **Polynomial Regression with Regularization:** Similar issues as multivariate regression regarding data preparation and lack of proper interval extraction. `PolynomialFeatures` usually refers to powers of continuous features, not creating interactions among ALL given features (which would be huge and mostly nonsensical for categorical features combined this way).
*   **Bayesian Regression:** The PyMC3 implementation is highly abstract (`Marginal`, `ExpQuad`) and appears to be setting up a Gaussian Process regression, not a general Bayesian linear regression as implied by the task and seen in the ground truth. It uses `df[['Year', 'Genre', 'Platform', 'Publisher']]` as inputs `X`, again facing the same data availability and encoding issues. There's no sampling step (`pm.sample()`) or extraction of posterior predictive samples for intervals.
*   **Change Point Detection:** Uses `changefinder` library, which is different from `ruptures` (often preferred for its clear interface). It just calculates a score `cf.update(x)` without providing meaningful change points or visualizing them on the time series.
*   **Survival Analysis:** The approach `kmf.fit(df['Years'], event_observed=df['Global_Sales'] > 0)` suggests that 'Years' is the duration, which is incorrect after the `groupby().sum()` transformation. 'Years' should represent the game's longevity, which needs to be calculated per game from its release year and last observed sales year. `df['Global_Sales'] > 0` as the event indicator is also questionable in this context. The code only implements Kaplan-Meier and misses the Cox Proportional Hazards model which is a key part of the requirements.
*   **Confidence/Prediction Intervals & Validation:** The prompt explicitly asks for confidence and prediction intervals for *all* models. The generated code mostly fits models but completely omits the extraction and display of these intervals. The time series cross-validation loop is present, but due to the preceding data and model errors, it won't execute correctly or meaningfully. It also lacks evaluation metrics (like RMSE) for the validation folds.

**Completeness:** Many requirements are missed due to the fundamental errors:
*   Proper aggregation of sales data by year (failed).
*   Meaningful ARIMA forecast with intervals (failed to get intervals).
*   Correct seasonal decomposition (failed to apply to yearly data correctly).
*   Multivariate regression with interaction terms (failed data prep and intervals).
*   Polynomial regression with regularization (failed data prep and intervals).
*   Bayesian regression *to model uncertainty* (failed to implement correctly, and no uncertainty/PI extraction).
*   Change point detection (incomplete implementation, no output).
*   Survival analysis (only partial Kaplan-Meier, no Cox, incorrect duration calculation).
*   Confidence/Prediction intervals for *all* models (largely missing).
*   Model validation using time series cross-validation (loop present but execution flawed due to earlier errors).

**Code Quality:**
*   Lack of imports beyond the very basic, e.g., `numpy`, `matplotlib`, `seaborn` are missing, which would be essential for visualization or numerical operations.
*   No error handling (e.g., `try-except` for file loading or model fitting).
*   No comments or explanations for the less obvious steps.
*   The code directly uses columns after `groupby().sum()` that no longer exist (e.g., 'Genre', 'Platform', 'Publisher').
*   There's no `import changefinder` despite its usage.

In summary, the code demonstrates an awareness of the modules and general steps but critically fails in the execution of almost every requirement due to fundamental misunderstandings of data handling, statistical modeling proper practices, and specific task details.",gpt-3.5-turbo,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE
import umap
from sklearn.metrics import silhouette_score
from sklearn.metrics import calinski_harabasz_score
from sklearn.metrics import davies_bouldin_score
from sklearn.neighbors import LocalOutlierFactor
from mlxtend.frequent_patterns import apriori, as...",3,"The generated code fails to meet most of the task requirements, particularly regarding the depth and complexity expected for ""advanced clustering and market segmentation analysis.""

Here's a breakdown of the issues:

**Missing/Incorrect Implementations:**

1.  **Preprocessing and Feature Engineering:**
    *   **Regional_Sales_Distribution:** The generated code calculates `x / x.sum()` for `Regional_Sales_Distribution` which will result in `NaN` if `x.sum()` is 0 (i.e., no sales), and doesn't explicitly tie this to individual sales (e.g., `NA_Sales_Prop`). The ground truth handles the division by zero and creates individual proportion features.
    *   **Publisher_Portfolio_Size:** The generated code uses `data['Publisher'].map(data['Publisher'].value_counts())`. This calculates the total number of games *published by each publisher* in the dataset, not the number of *unique games* a publisher has in their portfolio, which is what ""Publisher_Portfolio_Size"" typically implies and what the ground truth implements (`nunique()`). More critically, it applies this to the raw `Publisher` column, potentially losing information if the original dataset has duplicates or if the feature is meant to be derived from unique game entries.
    *   **Feature Selection:** The generated code only selects the 3 newly created features for clustering (`Regional_Sales_Distribution`, `Market_Position`, `Publisher_Portfolio_Size`). It completely ignores other crucial numerical features like `Global_Sales`, `Year_of_Release`, and all categorical features (`Genre`, `Platform`, `Publisher`) which are essential for comprehensive market segmentation and profiling. The ground truth correctly uses all relevant numerical and one-hot encoded categorical features.
    *   **Handling Missing Values:** No explicit handling of missing values for crucial columns like `Name`, `Genre`, `Platform`, `Publisher`, `Year_of_Release`, or sales figures. The ground truth explicitly addresses this.

2.  **Clustering Algorithms:**
    *   All clustering algorithms (K-Means, DBSCAN, Hierarchical, GMM) are set to a fixed `n_clusters=2` or default parameters. This is a severe simplification and fails to include any ""advanced"" aspects. The ground truth systematically determines optimal clusters.
    *   DBSCAN parameters (`eps=1.0`, `min_samples=10`) are arbitrary; the ground truth attempts a heuristic for `eps` based on nearest neighbors and visual inspection.

3.  **Dimensionality Reduction:**
    *   `umap`: The import `import umap` is correct, but it should be `import umap.umap_ as umap` for the UMAP object directly when using it in that manner (though `UMAP` directly might work if `umap_` is imported as `umap`, but the provided code just imports the top-level `umap` package and then correctly calls `umap.UMAP`). Still, the ground truth is more precise in its import.
    *   No visualization of the reduced dimensions with cluster labels. This is a key part of the requirement.

4.  **Optimal Cluster Determination:**
    *   The generated code *calculates* silhouette scores but does *not* use any methods (elbow, silhouette analysis plots, gap statistic) to *determine* the optimal number of clusters. It fixes `k=2` arbitrarily. This is a critical failure point.

5.  **Consensus Clustering:**
    *   `consensus_clusters = np.vstack(...)` just stacks the labels. It does not implement any consensus clustering *algorithm* (e.g., co-occurrence matrix, ensemble clustering methods) to derive a single, stable set of clusters from the multiple algorithms. This is a major omission.

6.  **Cluster Profiling:**
    *   Completely missing. The generated code calculates cluster labels but does not perform any profiling (mean/mode analysis of features per cluster) or statistical significance testing (`f_oneway`, `chi2_contingency`).

7.  **Anomaly Detection:**
    *   Uses `LocalOutlierFactor` which is a valid method, but it *only* calculates `outlier_scores`. There is no actual identification of anomalies (e.g., `fit_predict`, setting a threshold) or displaying of anomalous games as requested. The ground truth uses `IsolationForest` and clearly identifies and displays anomalies.

8.  **Association Rule Mining:**
    *   The `frequent_itemsets = apriori(data[['Genre', 'Platform', 'Publisher']], ...)` part correctly uses `apriori`. However, for `apriori`, the input DataFrame must be transactional (binary, one-hot encoded). `data[['Genre', 'Platform', 'Publisher']]` is not in this format. This line will likely throw an error or produce incorrect results. The ground truth correctly uses `OneHotEncoder` beforehand.

9.  **Recommendation System:**
    *   The generated code only assigns cluster labels to the DataFrame. It does not implement any functions or logic for *making recommendations* based on these clusters. This requirement is unmet.

10. **Validation Metrics:**
    *   While individual silhouette, Calinski-Harabasz, and Davies-Bouldin scores are calculated for each algorithm, there's no cohesive presentation or interpretation. The ground truth provides detailed validation for each algorithm and presents them clearly.

**Overall Code Quality:**

*   **Imports:** Mostly correct, but `import umap` instead of `import umap.umap_ as umap` might lead to issues depending on usage (though `umap.UMAP` often works).
*   **Structure:** Very linear, lacking functions or logical separations for different steps, making it harder to read and maintain. No print statements or comments to indicate progress or output, which is generally good practice for complex tasks.
*   **Error Handling:** No error handling (e.g., for file not found).
*   **Verbosity:** Lacking meaningful print statements for progress or results, making it difficult to understand what the code is doing or if it's succeeding.

In summary, the generated code provides a skeletal framework, importing some correct libraries and performing some basic operations, but it lacks the depth, logic, and comprehensive implementation required by almost every advanced requirement in the prompt, especially concerning optimal cluster determination, consensus clustering, detailed profiling, and a functional recommendation system.",gpt-3.5-turbo,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.calibration impo...",4,"The generated code attempts to address several requirements but falls short in correctness, completeness, and overall quality compared to the ground truth.

**Major deficiencies:**

1.  **Feature Engineering:** The generated code completely skips comprehensive feature engineering (PolynomialFeatures, interaction terms, custom features) and missing value imputation (0s as NaN then median imputation) as specified in the ground truth and task. It only performs basic scaling.
2.  **Ensemble Methods:**
    *   **Voting Classifier:** Only soft voting is implemented, not hard voting.
    *   **Bagging:** Only one BaggingClassifier instance is created, without exploring different base estimators as explicitly requested (""Bagging with different base estimators"").
    *   **Boosting:** While AdaBoost, Gradient Boosting, XGBoost, and LightGBM are instantiated, they are not integrated into a proper evaluation framework (like nested CV) beyond simple `fit()`. CatBoost is entirely missing.
3.  **Bayesian Optimization:**
    *   It uses Optuna, which is good.
    *   However, it only optimizes `RandomForestClassifier` parameters (`n_estimators`, `max_depth`). The task required optimizing more complex models like XGBoost, which the ground truth demonstrated.
    *   It uses `cross_val_score` for the objective, which is fine, but the overall approach is much simpler than what's expected for ""hyperparameter tuning using libraries like Optuna or Hyperopt"" in an advanced context.
4.  **Nested Cross-Validation:**
    *   The implementation of nested CV is incorrect. It uses `GridSearchCV` for the inner loop, which performs its own CV, effectively making the ""nested"" part apply *another* layer of CV to the `GridSearchCV`.
    *   The outer loop manually splits data and then calls `grid_search.fit` on the `X_train`, `y_train` from the *outer* loop, which is how it should be done. However, it still then performs `best_clf.fit(X_train, y_train)` *again* after the `GridSearchCV`, which is redundant.
    *   Crucially, the nested CV is only applied to a `RandomForestClassifier` and its hyperparameter search, not to evaluate the performance of *all* the ensemble methods implemented, which is the primary purpose of nested CV in this context. The ground truth correctly applies nested CV to evaluate all models and ensembles.
5.  **Custom Ensemble Methods with Dynamic Weight Assignment:** This crucial requirement is completely missing. The generated code does not attempt any custom weighting or dynamic combination.
6.  **CatBoost:** The implementation of CatBoost is entirely absent.
7.  **Model Interpretation (SHAP):** The code attempts `plot_partial_dependence`, which is a different interpretation technique, not SHAP values as specifically requested. SHAP requires a separate library and specific functions (e.g., `shap.TreeExplainer`, `shap.summary_plot`, `shap.force_plot`).
8.  **Model Calibration:** Basic `CalibratedClassifierCV` is shown, but the accompanying reliability analysis (e.g., plotting calibration curves, Brier score comparison for different calibration methods) is missing.
9.  **Ensemble Diversity Analysis and Model Combination Strategies:** This section is largely missing.
    *   It imports `StackingClassifier` again at the end, which is redundant.
    *   It only shows another `StackingClassifier` example, not ""diversity analysis"" (e.g., correlation matrices of predictions) or advanced ""model combination strategies"" (e.g., meta-learner on out-of-fold predictions) as demonstrated in the ground truth.
10. **Code Quality and Structure:**
    *   Lack of clear separation between different steps.
    *   Redundant imports (`StackingClassifier`, `RandomForestClassifier`).
    *   Missing imports for `numpy`, `matplotlib.pyplot`, `seaborn` (for plotting and analysis).
    *   Lack of comments and print statements to explain steps, making it harder to follow.
    *   Absence of a configuration section for random seeds, splits, etc.

In summary, the generated code provides a very basic outline for some ensemble methods and a simplified nested CV/HPO implementation, but it falls far short of the ""advanced"" and ""comprehensive"" requirements of the task, particularly regarding detailed feature engineering, specific advanced ensemble types (CatBoost, custom ensembles), robust evaluation (nested CV across *all* models), and specific interpretation/analysis techniques (SHAP, diversity analysis, detailed calibration).",gpt-3.5-turbo,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
from pymc3.gp.util import plot_gp_dist
import theano.tensor as tt
import theano

data = pd.read_csv(""diabetes.csv"")

# Handle missing values
data.fillna(data.mean(), inplace=True)

with pm.Model() as hierarchical_model:
     = pm.Normal('', mu=0, sigma=10)
     = pm.HalfCauchy('', beta=5)
     = pm.Normal('', mu=, sigma=, shape=data.shape[1])
     = pm.Normal('', mu=, sigma=, shape=data.shape[1])
    
    p = pm.math...",2,"The generated code fails to address most of the requirements and exhibits significant deviations from the ground truth code and best practices for the given task.

**Correctness:**
*   **Missing Value Handling:** `data.fillna(data.mean(), inplace=True)` is overly simplistic and incorrect for the `diabetes.csv` dataset, where 0 values often signify missing data in specific columns (e.g., BloodPressure, Glucose). The ground truth correctly identifies and handles these.
*   **Dataset Column Names:** The generated code uses hardcoded column names like 'age', 'bmi', 'diabetes', 'time_to_event' which do not match the `diabetes.csv` dataset's column names ('Age', 'BMI', 'Outcome'). This would lead to `KeyError` or incorrect data mapping.
*   **Hierarchical Model:** The `hierarchical_model` implementation is fundamentally flawed. It tries to create group-level effects based on `data.shape[1]` (number of columns) and applies `pm.math.dot(data[['age', 'bmi']], )` which doesn't correctly implement a hierarchical structure or mixed-effects model. The `shape` argument for `` and `` is incorrect for typical hierarchical models. The model doesn't define a grouping variable or a correct structure for varying intercepts/slopes.
*   **Mixed-Effects Model:** The `mixed_effects_model` attempts to define `_group` and `_individual` but doesn't correctly integrate them as random effects within a proper mixed-effects context relative to a grouping variable. `_individual` with `shape=data.shape[0]` (number of rows) for each individual is more like a random intercept for each data point rather than a group-level effect.
*   **Bayesian Logistic Regression:** While it tries to perform logistic regression, the lack of proper feature handling, scaling, and informative priors (as requested) makes it simplistic and less robust.
*   **Survival Analysis:** The `survival_analysis_model` defines `T = pm.Exponential('T', , observed=data['time_to_event'])`. The `diabetes.csv` dataset does not have a 'time_to_event' column, which means this model would immediately fail. It also does not implement a Cox Proportional Hazards model or any other common survival model, only a simple exponential distribution for time.
*   **Gaussian Processes:** The GP model is attempting to build a GP over `data[['age', 'bmi']]` to predict a binary 'diabetes' outcome via `pm.Bernoulli('y', p=pm.math.invlogit(f), observed=data['diabetes'])`. While technically possible, GP classification is complex. The ground truth used GP for non-parametric regression of a *continuous* variable (e.g., Glucose vs. Age) which is a more standard and illustrative use case for this dataset. The `pm.gp.Latent` usage with `X=data[['age', 'bmi']]` is an unusual way to map the features to the latent function `f` for classification without a specific GP classifier formulation.
*   **Causal Inference:** The section for causal inference is explicitly commented as `# Add causal inference model here` and then proceeds to `trace = pm.sample(...)` without any model definition. This means it completely fails to implement this crucial requirement.
*   **Priors:** Priors used (`mu=0, sigma=10` or `beta=5`) are often too broad and not 'informative' as requested by the prompt, especially compared to the ground truth's carefully selected priors.
*   **Model Comparison & Checks:** While `pm.waic` and `pm.loo` are called, they are called *after* a single `trace` variable is overwritten by multiple models, meaning only the last `trace` (from the empty causal inference model) would be used, rendering the calls ineffective for comparing the previously defined models. Posterior predictive checks and credible intervals are entirely missing.
*   **MCMC Sampling Parameters:** Using `1000` tune and `1000` draw steps is minimal for complex models, especially without `target_accept` which is critical for PyMC3.

**Code Quality:**
*   **Imports:** Missing `arviz` which is essential for `az.summary`, `az.traceplot`, `az.waic`, `az.loo`, and `az.compare`. The `plot_gp_dist` import is from `pymc3.gp.util` but the standard way to plot GPs is via `arviz`. `theano` imports are generally not needed explicitly by users anymore due to `pymc3`'s abstractions.
*   **Structure:** All models are defined and sampled sequentially, overwriting the `trace` variable. This makes it impossible to compare them or perform subsequent analyses on individual model traces. Each model should have its own trace variable (e.g., `trace_hierarchical`, `trace_mixed`).
*   **Readability:** Variable names like ``, `` are used, which can be less readable than `mu`, `sigma` for some. Column names like `age`, `bmi`, `diabetes` are assumed, but the CSV file uses `Age`, `BMI`, `Outcome`. Case sensitivity is important.

**Completeness:**
*   **Model Comparison:** Fails to implement proper model comparison as `trace` is overwritten and `arviz.compare` is not used.
*   **Credible Intervals & PPC:** Not implemented at all.
*   **Survival Analysis:** Fails to implement correctly as the dataset doesn't have the required column and the model is trivial.
*   **Causal Inference:** Completely missing implementation.
*   **Feature Scaling/Preprocessing:** The code only does `fillna` and doesn't perform standardization, which is crucial for MCMC convergence and GP performance.
*   **Group Variable Creation:** The hierarchical model relies on a group variable, which `diabetes.csv` does not inherently have. The ground truth correctly creates one based on `Age_Group`. The generated code assumes one exists or is implicitly handled, which it isn't.

In summary, the generated code demonstrates a very limited understanding of the requested advanced statistical modeling and Bayesian analysis techniques, fundamentally misinterpreting or entirely missing numerous key requirements.",gpt-3.5-turbo,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np

from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering, OPTICS
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE
from umap import UMAP
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.metrics import adjusted_rand_score
from sklearn.metrics import normalized_mutual_info_score
from sklearn.metr...",2,"The generated code is severely incomplete and largely fails to address the vast majority of the task requirements.

1.  **Correctness**: The code is syntactically mostly correct, but functionally incorrect because it doesn't actually perform the required steps. For example, it initializes clustering models and `fit` them to the raw `data` without any preprocessing or feature engineering, which is fundamentally incorrect for clustering. The dimensionality reduction techniques are also applied directly to `data` without scaling.
2.  **Code quality**:
    *   **Imports**: Imports are present, but many necessary ones for the advanced steps (e.g., `matplotlib`, `seaborn`, `SimpleImputer`, `mlxtend`, `tensorflow`, `scipy.stats`, `scipy.cluster.hierarchy`) are missing from the generated code, whereas they are critical in the ground truth.
    *   **Structure**: The structure follows the task's requested sections but is extremely bare-bones. Most sections are marked with `... (add code for X)` indicating a complete lack of implementation.
    *   **Comments**: Comments are helpful as placeholders but highlight the lack of implementation.
3.  **Completeness**: This is the weakest point.
    *   **Advanced Feature Engineering**: Only a comment `... (add code for feature engineering)` is present. The ground truth includes detailed metabolic syndrome indicators.
    *   **Preprocessing**: Only a comment `... (add code for data preprocessing)` is present. Missing crucial steps like handling 0s, imputation, and scaling.
    *   **Multiple Clustering Algorithms**: While the algorithms are imported and initialized, they are fitted to raw `data` (not scaled or preprocessed) and lack any parameter tuning or evaluation. DBSCAN and OPTICS parameters are hardcoded and not determined programmatically.
    *   **Advanced Dimensionality Reduction**: TSNE, UMAP, PCA are initialized and applied, but Autoencoder-based embeddings are entirely missing. The plotting/visualization aspect is also completely absent.
    *   **Optimal Clustering Parameters**: Only a comment `... (add code for parameter optimization)` is present. The ground truth includes detailed methods like Elbow, Silhouette, BIC/AIC, k-distance graphs.
    *   **Consensus Clustering**: Only a comment `... (add code for consensus clustering)` is present. This is a complex step entirely missed.
    *   **Patient Phenotype Profiles**: Only a comment `... (add code for patient profiling)` is present. Ground truth includes descriptive statistics, statistical significance testing (Kruskal-Wallis, Chi-squared), and visualizations.
    *   **Anomaly Detection**: Only Isolation Forest is minimally implemented (fitted to raw `data`, no output or analysis). OneClassSVM and LOF are missing, as are visualizations of anomalies.
    *   **Time-Series Clustering**: Only a comment `... (add code for time-series clustering)` is present. The ground truth correctly identifies that the dataset is not time-series but provides a conceptual outline, which is an acceptable way to address this part.
    *   **Association Rule Mining**: Only a comment `... (add code for association rule mining)` is present. The ground truth involves data discretization, Apriori, and generating association rules with examples.
    *   **Patient Risk Stratification System**: Only a comment `... (add code for risk stratification)` is present. The ground truth demonstrates linking clusters to risk levels, summarizing risk indicators, and providing clinical interpretation.

In essence, the generated code provides a skeletal outline of imports and algorithm initialization without any substantive implementation of the complex logic and analysis required by the prompt, particularly in the feature engineering, preprocessing, parameter optimization, consensus clustering, phenotyping, ARM, and risk stratification sections.",gpt-3.5-turbo,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import VotingRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_se...",3,"The generated code attempts to address several aspects of the prompt but falls significantly short in correctness, completeness, and overall quality.

**Primary Reasons for Failure/Missing:**

*   **Data Preprocessing and Feature Engineering:** The code completely misses the point of ""comprehensively preprocess the dataset with advanced feature engineering."" It simply loads the CSV and assumes a 'happiness\_score' column, which does not exist in `world-happiness-report-2021.csv`. The ground truth includes robust preprocessing (imputation, scaling, one-hot encoding for categorical features) using `ColumnTransformer` and `Pipeline`, which is entirely absent here. This fundamental flaw makes the subsequent model training completely incorrect.
*   **Missing Features/Columns:** The prompt specifies `world-happiness-report-2021.csv`, which has 'Ladder score' as the target, not 'happiness\_score'. It also has columns like 'Country name' and 'Regional indicator' that require proper handling (dropping/encoding). The generated code hardcodes `happiness_score` and `X.drop(columns=['happiness_score'])` which will lead to a `KeyError`.
*   **Ensemble Method Implementation:**
    *   **Voting Regressor:** The `CatBoostRegressor()` constructor is called without `verbose=0`, leading to excessive console output.
    *   **Bagging:** Uses `base_estimator=LinearRegression()`, which might not be diverse enough, and doesn't showcase bagging with *different* base estimators as implied by the prompt.
    *   **Boosting:** Only shows AdaBoost. XGBoost is then used for hyperparameter tuning. LightGBM and CatBoost are mentioned as imports but not explicitly used as standalone boosting models outside of a fleeting mention in `VotingRegressor`. GradientBoosting is also used in `VotingRegressor` but not explored independently.
*   **Bayesian Optimization:** While Optuna is used, the objective function's `X_train`, `y_train`, `X_test`, `y_test` are fixed from the initial `train_test_split`. In true Bayesian Optimization for cross-validation, the CV should be run *inside* the objective function, not just fitting on `X_train` and predicting on `X_test` from a single split. The prompt specifically asked for ""robust model evaluation"" with nested CV, which means tuning should happen within CV folds.
*   **Nested Cross-Validation:** The implementation of nested CV is a skeletal loop that prints ""Train base models"" and ""Train meta-learner"" comments but doesn't actually implement any of the sophisticated model training or evaluation within the loops. It completely skips the critical step of performing hyperparameter tuning within the inner loop for each fold.
*   **Completeness (Major Sections Missing):**
    *   **Custom Ensemble with Dynamic Weight Assignment:** Empty.
    *   **Blending:** Empty.
    *   **Multi-level Stacking:** Empty.
    *   **Model Interpretation (SHAP and LIME):** SHAP and LIME imports exist, but no actual implementation for generating and displaying interpretations.
    *   **Model Calibration and Uncertainty Quantification:** Empty.
    *   **Ensemble Diversity Analysis:** Empty.
    *   **Model Combination Optimization:** Empty.
*   **Code Quality:**
    *   Lack of comments for complex sections.
    *   Importing `GridSearchCV` but not using it.
    *   `itertools` is imported but not used.
    *   The code directly uses `pandas` DataFrames without proper separation of concerns (e.g., using `Pipeline` from `sklearn` more extensively for preprocessing as in the ground truth).
    *   Repetitive print statements without proper formatting of results.
    *   No handling of `UserWarning` from `use_label_encoder` in `XGBRegressor`.
    *   `base_estimator` in `AdaBoostRegressor` passing `LinearRegression` is deprecated.

The code provides a very basic framework for some requirements but fails to implement the ""advanced"" aspects or even the fundamental data preparation correctly.",gpt-3.5-turbo,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import statsmodels.api as sm
from causalgraphicalmodels import CausalGraphicalModel
from pymc3.distributions.timeseries import GaussianRandomWalk
from pymc3 import Model
from pymc3.distributions import HalfCauchy, Normal, Poisson, Uniform
from pymc3.variational.callbacks import CheckParametersConvergence
from pymc3.distributions import Beta, Laplace
from pymc3.model import Deterministic
from sklearn import preprocessing
from skl...",2,"The generated code attempts to address the task requirements but largely fails in implementation and correctness.

**Critical Failures and Missing/Incorrect Implementations:**

1.  **Preprocessing (Item 1):** The preprocessing is extremely basic (`data.fillna(data.mean(), inplace=True); data.dropna(inplace=True)`). This is not ""comprehensive missing value handling"" as requested. It does not handle categorical data imputations, standardization, or feature engineering as an expert system would. Many subsequent parts of the code rely on correctly preprocessed and standardized data, which is not provided.
2.  **Hierarchical Bayesian Models (Item 2):**
    *   It uses `data['region']` and `data['region_codes']` which do not exist in the loaded `world-happiness-report-2021.csv` dataset. The ground truth correctly identifies `Regional_indicator`.
    *   It uses `data['GDP']` which also does not exist; the correct column is `Logged_GDP_per_capita`.
    *   The model formulation `happiness_est = alpha[data['region_codes'].values] + beta * data['GDP'] + pm.Normal('eps', mu=0, sigma=sigma, shape=data.shape[0])` is mathematically incomplete for a full linear model (missing an intercept, only one `beta` for `GDP`).
    *   There is no sampling associated with this model, rendering it non-functional.
3.  **Structural Equation Models (SEM) (Item 3):**
    *   It uses `CausalGraphicalModel` for visualizing a DAG, which is not SEM. SEM involves statistical modeling of latent variables and their observed indicators, or a system of regression equations. The task asked for *building* SEMs, not just drawing a graph.
    *   The `nodes` and `edges` (`economic_factor`, `social_factor`, `psychological_factor`) are not derived from the dataset itself. The ground truth correctly approximates SEM using a series of specific Bayesian regressions from the available data.
4.  **Causal Inference (IV and PSM) (Item 4):**
    *   **Instrumental Variables (IV):** The `CausalModel` from `causalinference` library is for general causal inference, not specifically instrumental variables. It defines common causes, which is typically for PSM or regression adjustment, not IV. There's no implementation of a 2SLS approach or even an acknowledgment of the difficulty of finding valid instruments in this dataset, which is crucial for a real-world scenario.
    *   **Propensity Score Matching (PSM):** There's no PSM implementation. The `CausalModel` setup attempts to define treatment and outcome but doesn't follow through with generating propensity scores or performing matching.
5.  **Bayesian Regression with Informative Priors (Item 5):**
    *   The model involves `Bernoulli('switch', p=prob) `, `HalfCauchy('tau', 0, 5)`, and `Deterministic('sd', np.sqrt(1/tau))`, which appears to be an attempt at a different type of Bayesian model (e.g., spike-and-slab or a different prior structure for `sigma`), not a straightforward Bayesian regression with informative priors as requested. It uses a `Beta` prior for `prob` which is usually for Bernoulli proportions or binary outcomes. The task requested a simple regression where priors are *informative* on the typical regression coefficients (e.g., effect of GDP on happiness).
    *   It specifies `data.happiness` and `data.GDP`, which are incorrect column names again.
6.  **Gaussian Processes (Item 6):**
    *   It uses `data.GDP.values[:, None]`, which is incorrect column name.
    *   The model formulation is incomplete; it defines a GP but doesn't show how it's used to model the relationship or how predictions would be made. It's missing `plot_gp_dist` or similar visualization/evaluation.
7.  **Mediation Analysis (Item 7):**
    *   It correctly identifies `statsmodels.stats.mediation.Mediation`, but the `data` and factor names (`social_factor`, `economic_factor`, `psychological_factor`) are generic and not mapped to the actual columns in the dataset. This likely won't run because those columns don't exist.
    *   It doesn't calculate or interpret direct, indirect, or total effects, which is the core of mediation analysis.
8.  **Difference-in-Differences (DiD) Analysis (Item 8):**
    *   This is the least implemented. It assumes columns like `policy`, `treatment_measure`, and `control_measure` exist, which they don't. The task specifically stated to *simulate* a time component. There's no panel data creation or proper DiD regression setup.
9.  **Bayesian Model Comparison (Item 9):**
    *   It uses `pm.waic(trace)` and `pm.loo(trace)` but `trace` is only sampled for the `Bayesian regression with informative priors` section; there's no trace for the hierarchical model to compare against.
    *   `pm.BF` is not a valid PyMC3 function for Bayes Factors. PyMC3/ArviZ don't directly compute Bayes Factors as straightforwardly as WAIC/LOO.
10. **Credible Intervals, PPC, Sensitivity Analysis (Item 10):**
    *   Credible intervals/PPC are not explicitly shown or generated for any of the models.
    *   Sensitivity analysis is completely absent.

**Code Quality and Structure:**

*   **Imports:** Excessive and irrelevant imports (e.g., `RandomForestRegressor`, `pearsonr` twice, `CausalGraphicalModel` without proper SEM use, `pymc4` packages even though `pymc3` was specified). Many imported libraries are not used.
*   **Variable Names:** Inconsistent and incorrect variable names (e.g., `data['region']`, `data['GDP']`, `data.happiness` instead of actual column names like `Regional_indicator`, `Logged_GDP_per_capita`, `Ladder_score`).
*   **Assumptions:** Makes incorrect assumptions about the dataset's column names, leading to non-functional code.
*   **Reproducibility:** No random seed set for PyMC3 sampling, affecting reproducibility.
*   **Explanations/Comments:** No comments or print statements to explain steps, making it hard to follow.

Overall, the code demonstrates a superficial understanding of the requirements and libraries. Many sections are placeholders or incorrect implementations rather than genuine solutions to the complex statistical modeling tasks requested.",gpt-3.5-turbo,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from hdbscan import HDBSCAN
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from umap import UMAP
from sklearn.manifold import Isomap
from sklearn.cluster import SpectralEmbedding
from sklearn...",0,"The generated code fails to implement virtually any of the complex requirements outlined in the task. It only includes a basic import section and then an empty sequence of comments for each task. There is no actual implementation of data loading, preprocessing, advanced feature engineering, multiple clustering algorithms, dimensionality reduction, optimal parameter determination, consensus clustering, detailed country profiling, anomaly detection, network analysis, association rule mining, or the recommendation system. The provided code for evaluation is essentially a skeleton of comments with some imports, making it entirely incomplete and non-functional for the given task.",gpt-3.5-turbo,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX
from pmdarima import auto_arima
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Load data
data = pd.read_csv('synthetic_covid19_data.csv')
data['date'] = pd.to_datetime(data['date'])
data.set_index('date', inplace=True)

# Filter for Germany
data_germany = data[data['location'] == 'Germany']

# Handle missing values
data_germany['new_cases'] = data_germany['new_cases'].interpola...",6,"The generated code addresses most of the core requirements but has several shortcomings in terms of robustness, best practices, and accuracy compared to the ground truth.

**Strengths:**
*   Correctly loads data, filters for Germany, and sets the date as the index.
*   Identifies the target and exogenous variables.
*   Splits data into train and test sets (last 30 days).
*   Uses `SARIMAX` from `statsmodels`.
*   Includes exogenous variables in the model training and forecasting.
*   Calculates RMSE.
*   Plots actual vs. forecasted values.

**Areas for Improvement / Missing Aspects:**

1.  **Missing `auto_arima` for order determination:** The prompt specifically required determining appropriate (p,d,q)(P,D,Q,s) orders using `auto_arima` or manual inspection. The generated code hardcodes `order=(1, 1, 1)` and `seasonal_order=(1, 1, 1, 7)`, which is a significant deviation from the task's instruction for order selection. This is a critical component for a robust time series forecast.
2.  **Incomplete Missing Value Handling:**
    *   It uses `interpolate()` for `new_cases`, which is a reasonable approach for time series.
    *   It uses `ffill()` for exogenous variables.
    *   However, it doesn't account for potential `NaN`s at the beginning of the series after `ffill` (which would require a `bfill` or `fillna(0)` depending on context).
    *   More importantly, it directly modifies `data_germany` after slicing (`data_germany['new_cases'] = data_germany['new_cases'].interpolate()`), which can lead to `SettingWithCopyWarning` and silent failures if not handled carefully (e.g., by using `.copy()` after filtering).
    *   The ground truth provides a more robust multi-step missing value handling process (`ffill`, `bfill`, `interpolate`, final `ffill`/`bfill`), which is good practice.
3.  **No Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice for robust scripts dealing with external files.
4.  **No `warnings` suppression:** While minor, the ground truth explicitly suppresses warnings, which can be useful in production scripts to avoid verbose outputs from libraries like `statsmodels`.
5.  **Lack of `n_jobs=-1` for `auto_arima`:** If `auto_arima` were implemented, leveraging `n_jobs=-1` would be beneficial for performance.
6.  **Minor Plotting Improvements:** The plot is functional, but lacks a title with RMSE, proper axis labels, and a grid, which enhance readability as seen in the ground truth.
7.  **No print statements for progress/confirmation:** The ground truth includes helpful print statements to indicate progress (e.g., ""Searching for optimal SARIMA orders..."", ""Training complete""), which improves user experience for longer-running scripts.
8.  **No handling for negative forecasts:** New cases cannot be negative. The ground truth correctly adds a line to convert any negative forecasts to zero.",gpt-3.5-turbo,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
from scipy.stats import f_oneway, kruskal, ttest_ind
from statsmodels.stats.multicomp import pairwise_tukeyhsd, pairwise_tukeyhsd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Define quality tiers
low_quality = data[data['quality'] <= 4]
medium_quality = data[(data['quality'] >= 5) & (data['quality'] <= 6)]
high_quality = data[data['quality'] >= 7]

# Perform statistical tests
for column in dat...",3,"The generated code fails to meet most of the requirements of the task.

1.  **Quality Grouping:** The code attempts to define quality groups (`low_quality`, `medium_quality`, `high_quality`) but then uses `data['quality']` directly for `pairwise_tukeyhsd` and `sns.boxplot`, which uses the raw 0-10 quality scores instead of the defined tiers. This is a fundamental misinterpretation.
2.  **Statistical Tests:**
    *   It *only* performs ANOVA (`f_oneway`). It completely misses the conditional logic for normality and homogeneity of variance to decide between ANOVA and Kruskal-Wallis. This is a critical statistical flaw.
    *   It performs `pairwise_tukeyhsd` unconditionally based on the `p_val` from ANOVA, even if Kruskal-Wallis should have been used. It also incorrectly uses the original `data['quality']` (0-10 scale) as groups for Tukey's, which is not what was requested (comparison across the three defined tiers).
    *   It lacks any non-parametric post-hoc test (e.g., Dunn's equivalent like pairwise Mann-Whitney with correction).
3.  **Post-hoc Analysis:** As mentioned, it incorrectly applies Tukey's HSD and does not handle the case where a non-parametric test would be needed. It also misuses the `quality` column directly instead of the categorized tiers.
4.  **Visualizations:** The box plots are generated using the original `quality` scale (0-10) instead of the three defined quality tiers ('low', 'medium', 'high'), which deviates from the prompt's request.
5.  **Completeness/Structure:**
    *   It iterates over `data.columns[:-1]`, which includes 'quality' itself as a ""chemical property"", leading to nonsensical tests. It should explicitly exclude 'quality'.
    *   It does not store or summarize the results of the statistical tests or post-hoc analyses in a structured way as implied by ""Summarize which features significantly differ between groups.""
    *   It lacks proper error handling for file loading.
    *   Several imports are present but not used (`ttest_ind`, `pairwise_tukeyhsd` imported twice).
    *   It misses the `statsmodels.stats.multitest` import for Bonferroni correction.

In summary, the code provides a very basic and largely incorrect implementation of the requested analysis, demonstrating a severe lack of understanding of the statistical methodologies required by the prompt.",gpt-3.5-turbo,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, f1_score
import shap

# Load data
data = pd.read_csv('synthetic_covid19_data.csv')
data['da...",6,"The generated code addresses most of the requirements but has critical issues in correctness and completeness that prevent a higher score.

**Positive Aspects:**
*   **Data Loading & Parsing:** Correctly loads data and parses the 'date' column.
*   **Feature Engineering:** Implements most requested features (rolling averages/stds, lags, days since first case).
*   **Model Training & Tuning:** Uses XGBoost, GridSearchCV for hyperparameter tuning, and a Pipeline for preprocessing.
*   **Evaluation:** Calculates ROC-AUC and F1-score.
*   **SHAP:** Attempts to use SHAP for feature importance.

**Areas for Improvement / Critical Issues:**

1.  **Incorrect Target Variable Definition:**
    *   **The biggest flaw.** The prompt specifically requires the target variable to be `(df['new_deaths'] > df['location_90th_percentile_deaths']) & (df['new_deaths'] > 0)`.
    *   The generated code's `create_target_variable` function calculates the 90th percentile from `new_deaths > 0` correctly, but then it defines `high_daily_death_increase` as `(group['new_deaths'] > group['90th_percentile']).astype(int)`. This misses the crucial `(new_deaths > 0)` condition for the *current day's* `new_deaths` to be considered a ""high increase"". If `new_deaths` on a particular day is 0, it should *not* be considered a high increase, even if '> 90th percentile' is technically true for 0 vs a negative percentile (which wouldn't happen here, but conceptually if 0 is treated as greater than some threshold). The problem asks for an ""increase"", which implies a positive value.
    *   Additionally, the `fillna(0)` is applied to `group['90th_percentile']` which means locations with no `new_deaths > 0` events will have a 90th percentile of 0, making comparisons potentially misleading. This should be handled more robustly by setting the target to 0 for these cases. The ground truth handles this by implicitly making the comparison `False` when the percentile is NaN (and eventually dropping the column, or initializing to 0 when the data is not available earlier in the time series).

2.  **Incorrect Missing Value Handling for Feature Engineering:**
    *   The prompt specifies handling missing values from feature engineering ""e.g., fill with 0 *or backfill*"".
    *   The generated code uses a blanket `data.fillna(0, inplace=True)`. While this covers many cases, it doesn't consider the temporal nature of the data. For rolling/lag features, `ffill` (forward fill) within each group, followed by `fillna(0)` (for initial NaNs) is generally more appropriate to avoid using future information (`backfill` itself should often be avoided if strict causality is required for prediction). A simple `fillna(0)` across the entire dataset might fill lagged values with 0 even if a prior value was available, or fill the initial `NaN`s of rolling features with 0 indiscriminately, which might not be ideal. The ground truth explicitly handles this per-group with `ffill().fillna(0)`.

3.  **Incorrect Handling of `people_fully_vaccinated`:**
    *   The prompt asks to select `'people_fully_vaccinated' / 'population'`. The generated code simply selects `'people_fully_vaccinated'` and `'population'` as separate features, not their ratio. This misses a key requirement.

4.  **Incorrect Time-Based Split:**
    *   The prompt explicitly states ""time-based split (e.g., last 20% of data for testing)"". The generated code uses `train_test_split` with `shuffle=False`, which maintains order, but it performs the split *before* the StandardScaler and ColumnTransformer.
    *   Crucially, the `ColumnTransformer` is applied *within the pipeline*. The `X_train` and `X_test` passed to `grid_search.fit` are not pre-transformed. This is correct for the pipeline approach.
    *   The problem is that `train_test_split` without explicit sorting could potentially lead to problems if the *original DataFrame* is not sorted by date. While `df.sort_values` is done after loading in the ground truth, it's not explicitly stated or shown for the generated code, which implies the initial order matters for `shuffle=False`. Assuming the loaded data is already sorted by date, `shuffle=False` will preserve the chronological order. However, the ground truth explicitly sorts *before* the train-test split, reinforcing this best practice.

5.  **Incorrect Feature List for Scaling and Encoding:**
    *   The code creates `X` by selecting features, then `pd.get_dummies` for 'continent'. This is inefficient and problematic. `X` now contains one-hot encoded continent columns and also the original numerical features that are *not* scaled yet.
    *   Then, `numerical_features = X.select_dtypes(include=['float64', 'int64']).columns` is applied to this *already dummified* `X`. This means the `get_dummies` operation was done *before* the `ColumnTransformer`. This is a redundancy and also means that `numerical_features` will include the one-hot encoded columns, which should not be scaled.
    *   The correct approach, as shown in the ground truth, is to let `ColumnTransformer` handle *both* one-hot encoding and scaling within the `Pipeline` so that the `X` passed to the pipeline (`X_train`, `X_test`) contains the raw features (including 'continent'). The generated code's `pd.get_dummies(X, columns=['continent'])` line makes the subsequent `ColumnTransformer` less effective and potentially incorrect for the way it's set up.

6.  **Missing `eval_metric` for XGBoost & `use_label_encoder=False` Warning:**
    *   The `XGBClassifier` should ideally have `eval_metric='logloss'` (or similar) specified to suppress a common warning, and `use_label_encoder=False` for newer versions. While not a functional error, it's good practice.

7.  **SHAP Minor Issues:**
    *   `explainer = shap.Explainer(grid_search.best_estimator_.named_steps['classifier'])` is correct for the model.
    *   `shap_values = explainer(X_train)`: This computes SHAP values on the *raw* `X_train` data, not the preprocessed data that the model actually sees *after* the `preprocessor` step in the pipeline. This means the SHAP values are calculated against features that haven't been scaled or had continent one-hot encoded by the pipeline, leading to incorrect explanations. The ground truth correctly uses `best_model.named_steps['preprocessor'].transform(X_train_df)` to get the preprocessed data.
    *   The `shap.summary_plot` also uses `X_train` directly for feature names, which again will be incorrect if the one-hot encoding and scaling was supposed to happen within the pipeline.

**Summary of Critical Failures:**
The definition of the target variable is flawed, missing a crucial condition. The feature preprocessing for SHAP is incorrect, leading to potentially misleading explanations. The handling of `people_fully_vaccinated` is not as per the requirement. The missing value imputation for time-series features is simplistic compared to expert recommendations. The feature selection and scaling/encoding setup is inefficient and incorrect for the intended pipeline usage.",gpt-3.5-turbo,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf

# Load data
data = pd.read_csv('synthetic_covid19_data.csv')

# Parse 'date'
data['date'] = pd.to_datetime(data['date'])

# Calculate 'vaccination_percentage'
data['vaccination_percentage'] = data['people_fully_vaccinated'] / data['population']

# Handle missing values
data['reproduction_rate'] = data.groupby('location')['reproduction_rate'].transform(lambda x: x.fillna(x.median()...",6,"The generated code attempts to address the task but falls short in several critical aspects, particularly concerning missing value handling, correct mixed-effects model specification for nesting, and comprehensive reporting/interpretation.

**Correctness (3/5):**
*   **Missing Value Handling:** The approach to handling missing values is problematic. It uses `fillna(x.median())` after grouping by `location`. This is problematic for time-series data like `reproduction_rate` and `stringency_index` where interpolation is usually preferred. More importantly, `vaccination_percentage` is calculated **before** `people_fully_vaccinated` and `population` missing values are handled, leading to NaNs in the calculated `vaccination_percentage` if either numerator or denominator was missing. Even if *only* `vaccination_percentage` itself was missing for some reason, grouping by `location` and filling with `median()` is not always the best strategy, especially if a location only has a few data points or if the median is not representative over time. The ground truth uses a more robust ffill/bfill for population metrics and interpolation for time-series variables, followed by a global median fallback.
*   **Vaccination Percentage Calculation:** `data['vaccination_percentage'] = data['people_fully_vaccinated'] / data['population']` without multiplying by 100 will yield a proportion, not a percentage. While a proportion is mathematically valid, the task explicitly asked for ""vaccination_percentage' = 'people_fully_vaccinated' / 'population' (as a percentage of population)"".
*   **Nesting in Mixed-Effects Model:** The formula `(1 | continent/location)` is a common way to denote nesting in R/lme4, but `statsmodels.mixedlm` does not parse this syntax from the formula string. Instead, nesting `location` within `continent` (i.e., random intercepts for each `location` *within* each `continent`) is typically handled by creating a unique ID for each `(continent, location)` pair and passing that as the `groups` argument. The generated code passes `data['continent'] + ""/"" + data['location']` as the `groups` argument, which effectively creates unique IDs for each `(continent, location)` combination. This correctly implements random intercepts for each *unique* `(continent, location)` rather than allowing different slopes/intercepts for `location` within each `continent` *and* separately for `continent` (which `(1 | continent/location)` in other languages usually implies). However, the formula syntax is misleading and relies on `groups` argument for the correct interpretation. The `re_formula` argument should be explicitly `re_formula=""1""` to indicate random *intercepts* within those groups, as the formula syntax `(1 | group)` is not native to `mixedlm` (it's parsed by the formula parser, but `mixedlm` doesn't use `re_formula` from the string). The provided formula `smf.mixedlm(formula, data=data, groups=data['continent'] + ""/"" + data['location'])` implicitly takes `re_formula=""1""` if not specified, which is correct for random intercepts.

**Code Quality (3/5):**
*   **Robustness:** Lacks error handling for file not found.
*   **Comments/Readability:** Minimal comments.
*   **Structure:** No clear separation of steps (loading, preprocessing, modeling, reporting).
*   **Warnings:** No handling of potential `statsmodels` warnings which are common with mixed models (e.g., convergence issues).

**Completeness (2/5):**
*   **Detailed Missing Value Strategy:** The task specified ""interpolation or median imputation grouped by location"". The generated code applies median imputation for all, which is suboptimal for time-series. It also misses the crucial step of filling NaNs in `population` and `people_fully_vaccinated` *before* calculating `vaccination_percentage`.
*   **Full Reporting and Interpretation:** The code only `print(result.summary())` and `print(result.random_effects)`. The task explicitly requested:
    *   ""Report the fixed effects coefficients, their standard errors, and p-values."" (The `summary()` does this, but direct extraction and printing per variable, as in GT, is better.)
    *   ""Interpret the significance and direction of these effects."" (Completely missing.)
    *   ""Report the variance components for the random effects."" (`result.random_effects` gives the BLUPs for each group, not the overall variance component of the random effects. The actual variance component is in `result.cov_re`.)
    *   ""Discuss model assumptions... briefly describe how they could be checked"". The generated code provides a very brief one-liner without any helpful details. The ground truth provides detailed explanations for each assumption and how to check it, aligning much better with ""briefly describe how they could be checked"".
*   **Calculation of `vaccination_percentage`**: Fails to multiply by 100 for percentage.
*   **Data Sorting**: Crucial for time-series imputation methods (like the linear interpolation used in the ground truth) but not performed in the generated code. While median imputation doesn't strictly *require* sorting, it's good practice for time-series data.
*   **Clipping `vaccination_percentage`**: The calculated percentage can exceed 100% due to data anomalies; the ground truth handles this by clipping.

**Conclusion:** The code provides a basic attempt at building a mixed-effects model. However, its weak missing value handling, lack of direct interpretation, and misreporting of random effect variance (reporting the random effect predictions per group instead of the overall variance component) significantly limit its utility and correctness score for an ""advanced statistical modeling"" task. The missing value handling and interpretation are the most critical shortcomings.",gpt-3.5-turbo,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# Select features for clustering
features = data[['elixirCost', 'Win Rate', 'Usage']]

# Standardize the features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Determine appropriate number of clusters (K)
wcss = []
for...",7,"The generated code correctly performs the core tasks: loading data, selecting and standardizing features, determining K using the elbow method (though the choice is hardcoded afterward), applying K-Means, performing PCA, and visualizing the results.

However, there are several areas for improvement to reach a higher score:

1.  **Hardcoded K:** While it correctly plots the WCSS for K selection, the choice of K=3 is hardcoded without explicitly stating it's based on the elbow method or allowing for user justification. The ground truth explicitly states the choice of K and gives a rationale.
2.  **Robustness (File Handling and Feature Check):** The generated code lacks error handling for `FileNotFoundError` or checking if the required columns exist in the DataFrame. The ground truth includes these checks, making the code more robust.
3.  **Clarity and Detail in Plotting:** The plots are basic. The elbow method plot lacks proper x-axis ticks for K values and a descriptive title like the ground truth. The PCA plot also lacks the explained variance in the axis labels and a color bar, which significantly enhances interpretability.
4.  **`n_init` Parameter:** The `n_init` parameter in `KMeans` is not explicitly set in the generated code, which can lead to deprecation warnings in newer scikit-learn versions and might not provide the most stable results compared to multiple initializations. The ground truth sets `n_init=10`.
5.  **Output and Intermediate Steps:** The generated code doesn't print intermediate information (like head of dataframes, shape, etc.) or confirmations about the steps being performed, which is good practice for understanding the data flow and debugging, as seen in the ground truth.",gpt-3.5-turbo,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# Select features and target
X = data[['elixirCost', 'Usage', 'rarity']]
y = data['Win Rate']

# Preprocess the 'rarity' column
encoder = OneHotEncoder()
X_encoded = encoder.fit_transform(X[['rar...",5,"The generated code has a critical error in its data preprocessing that makes the model unable to perform correctly for the given features.

**Primary Issues:**

1.  **Incorrect One-Hot Encoding Application:** The `OneHotEncoder` is only applied to the `rarity` column (`X[['rarity']]`), and the resulting `X_encoded` (which is a sparse matrix if `OneHotEncoder` default is used, or a numpy array if `sparse_output=False` or `sparse=False` in older versions) is then directly used for `train_test_split`. This means that the `elixirCost` and `Usage` features, which are numerical and should be included in the model training, are completely omitted from the `X` dataframe before splitting and training.

    Instead of:
    ```python
    encoder = OneHotEncoder()
    X_encoded = encoder.fit_transform(X[['rarity']])
    # X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, ...)
    ```
    It should concatenate the one-hot encoded `rarity` with the original `elixirCost` and `Usage` features. A common way to do this is using `pd.get_dummies` as in the ground truth, or creating a `ColumnTransformer` with `OneHotEncoder` for 'rarity' and 'passthrough' for 'elixirCost' and 'Usage'.

2.  **Missing `random_state` for `RandomForestRegressor`:** The task specifies `random_state=42` for the model. While `RandomForestRegressor` still works without it, adding `random_state` ensures reproducibility, which is important for benchmarking and consistent evaluation.

**Minor Issue:**

*   The print statements for the evaluation scores are missing the descriptive strings and formatting (`:.4f`) present in the ground truth, making the output slightly less readable for a direct benchmark comparison, though the values themselves are printed.

Due to the fundamental error in feature selection and preprocessing (omitting `elixirCost` and `Usage` from `X` entirely when using `X_encoded` for training), the model is not trained on the intended features, leading to an incorrect solution to the task.",gpt-3.5-turbo,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy.stats import f_oneway

data = pd.read_csv('clash_royale_cards.csv')

rarity_categories = data['rarity'].unique()

rarity_groups = [data[data['rarity'] == category]['Win Rate'] for category in rarity_categories]

f_statistic, p_value = f_oneway(*rarity_groups)

print(""F-statistic:"", f_statistic)
print(""p-value:"", p_value)

alpha = 0.05
if p_value < alpha:
    print(""Reject the null hypothesis"")
else:
    print(""Fail to reject the null hypothesis"")
```",6,"The generated code correctly performs the ANOVA test and partially interprets the result. It successfully loads the data, identifies unique rarity categories, groups 'Win Rate' by rarity, and uses `f_oneway` to calculate the F-statistic and p-value. The interpretation based on `alpha` is also correct.

However, it misses several critical aspects required by the prompt:
1.  **Hypotheses Statement:** The prompt explicitly asks to ""State the null and alternative hypotheses for the test."" The generated code does not include this.
2.  **Robustness/Error Handling:** It lacks error handling for `FileNotFoundError` when loading the CSV, or checks for missing/non-numeric `Win Rate` values, or insufficient data for ANOVA. The ground truth code includes checks for `FileNotFoundError`, `pd.to_numeric` with `errors='coerce'`, `dropna`, and checks for `df.empty` or `len(rarity_categories) < 2`.
3.  **Clarity of Output:** While it prints the F-statistic and p-value, it doesn't clearly state the conclusion as requested (""state whether to reject the null hypothesis""). The ground truth provides a more verbose and clear interpretation, including the conclusion.
4.  **Data Preprocessing:** It assumes 'Win Rate' is always numeric. The prompt implies, or at least it's good practice, to ensure the column is numeric, as done in the ground truth using `pd.to_numeric` and `dropna`.

To improve, the code needs to:
*   Explicitly state the hypotheses.
*   Add more robust data loading and preprocessing steps (e.g., handling non-numeric 'Win Rate', checking for sufficient data).
*   Provide a more detailed interpretation of the p-value result.",gpt-3.5-turbo,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
data = pd.read_csv('iris.csv')

# Define features and target variable
X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'Pet...",7,"The generated code correctly addresses most of the requirements. It loads the data, defines features and target, encodes the target, splits the data, trains the requested models, and calculates the specified metrics.

However, there are a few areas for improvement:
1.  **Output Structure and Clarity**: The output for printing the performance metrics is quite compact, making it harder to read than the ground truth. It also doesn't explicitly state ""Model Performance Comparison"" or ""Best Performing Model"" as requested, just prints the model details.
2.  **Best Model Determination**: The logic to find the best model is crammed into a single, somewhat unreadable line using a `max` function with repeated `f1_score` calculations. This is inefficient as it re-predicts for each model to calculate F1-score again after they've already been computed. A better approach, as shown in the ground truth, would be to store the F1-scores and then find the maximum from the stored values.
3.  **Logistic Regression `max_iter`**: While not explicitly stated in the prompt, Logistic Regression often fails to converge with default settings on some datasets without increasing `max_iter`. The ground truth includes `max_iter=200` as a good practice for convergence. The generated code uses default, which might lead to a `ConvergenceWarning` (though for this specific dataset, it often converges).
4.  **Error Handling for File Missing**: The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is robust practice. The generated code lacks this.
5.  **Splitting `stratify`**: The ground truth uses `stratify=y_encoded` in `train_test_split`, which is important for maintaining the class distribution in both training and testing sets, especially with potentially imbalanced datasets. The prompt didn't explicitly ask for `stratify`, but it's a best practice in classification tasks.

Overall, the code is functional and produces correct results for the given task, but could be improved in terms of output formatting, efficiency, and robustness.",gpt-3.5-turbo,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('iris.csv')

# Select features
X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_...",7,"The generated code correctly implements the core logic for K-Means clustering and PCA. It loads the dataset, selects features, standardizes them, applies K-Means with the correct parameters (K=3, random_state=42, n_init='auto'), and performs PCA to 2 components. It also generates both required scatter plots.

However, there are several areas for improvement to achieve a higher score and match the output quality of the ground truth:

1.  **Plotting Quality:**
    *   **Titles:** While titles are present, they are less descriptive than the ground truth's ""PCA of Iris Dataset: Points Colored by K-Means Cluster Labels"" and ""PCA of Iris Dataset: Points Colored by True Species Labels"".
    *   **Labels:** The plots lack x and y axis labels (e.g., 'Principal Component 1', 'Principal Component 2'), which are crucial for clarity.
    *   **Colorbar:** The first plot (K-Means clusters) lacks a colorbar, making it harder to interpret which color corresponds to which cluster.
    *   **Species Encoding & Colorbar for True Labels:** The ground truth used `LabelEncoder` and a `colorbar` that displays the actual species names, which is a much more informative and user-friendly approach than just `astype('category').cat.codes` and no colorbar on the second plot. The current approach assigns arbitrary numerical values to species and doesn't communicate what those numbers signify on the plot.
    *   **Figure Size and Alpha:** The ground truth uses `plt.figure(figsize=(10, 7))` and `alpha=0.8` for better visual presentation, which is missing in the generated code.

2.  **Robustness:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.

3.  **Imports:** While all necessary core libraries are imported, `LabelEncoder` (used in the ground truth for better species encoding visualization) is not imported, nor is `numpy` (which is useful for `np.unique` for colorbar ticks).

Overall, the code fulfills the functional requirements, but the presentation and user experience of the plots are significantly inferior to the ground truth, and it lacks some minor robustness features.",gpt-3.5-turbo,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('iris.csv')

# Create new features
df['PetalArea'] = df['PetalLengthCm'] * df['PetalWidthCm']
df['SepalArea'] = df['SepalLengthCm'] * df['SepalWidthCm']

# Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'PetalArea', 'SepalAr...",10,"The generated code flawlessly addresses all requirements of the task. It correctly loads the dataset, creates the specified new features (`PetalArea` and `SepalArea`), defines `X` to include both original and new features, and correctly encodes the `Species` target for `y`. The Gradient Boosting Classifier is initialized with the specified parameters, and the 5-fold cross-validation is performed correctly. Finally, the mean cross-validated accuracy is reported as requested. The imports are proper, and the code structure is clear and concise. No issues were found.",gpt-3.5-turbo,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load the dataset
data = pd.read_csv('heart.csv')

# Define features and target
X = data[['Age', 'Sex', 'ChestPainTy...",9,"The generated code is very good and correctly addresses almost all requirements of the task.

Strong points:
- **Correct Data Loading and Feature/Target Selection:** Accurately loads the `heart.csv` dataset and correctly defines `X` with the specified features and `y` as `HeartDisease`.
- **Correct Preprocessing:** Successfully identifies and applies `StandardScaler` to numerical features and `OneHotEncoder` to categorical features using `ColumnTransformer` within a `Pipeline`. The numerical and categorical feature lists are correctly identified.
- **Correct Data Splitting:** Performs the train-test split (80/20) with `random_state=42` as specified.
- **Correct Model Training:** Instantiates and trains `RandomForestClassifier` with `n_estimators=100` and `random_state=42` as required.
- **Correct Evaluation Metrics:** Calculates and prints accuracy, macro-averaged precision, macro-averaged recall, macro-averaged F1-score, and ROC-AUC score correctly.
- **Good Code Structure:** Uses a `Pipeline` which is best practice for combining preprocessing and modeling steps, preventing data leakage.

Areas for minor improvement (leading to a score of 9 instead of 10):
- **Missing `handle_unknown='ignore'` for OneHotEncoder:** The ground truth includes `handle_unknown='ignore'` for `OneHotEncoder`. While not strictly necessary for this specific dataset if all categories are present in the training set, it's a good practice for robustness in case new, unseen categories appear in the test set. This detail was implicitly part of ""comprehensive preprocessing"".
- **Lack of File Not Found Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. While the prompt assumes the CSV is in the current directory, robust code often includes such handling.

Overall, the generated code is highly functional and accurate according to the problem description, demonstrating a strong understanding of the required data science pipeline. The missing `handle_unknown='ignore'` is a minor omission in robustness, and the file not found handling was not explicitly asked for but is good practice.",gpt-3.5-turbo,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('heart.csv')

# Select numerical features for clustering
selected_features = df[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']]

# Standardize selected features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(selected_features)

# Determine K using elbo...",7,"The generated code correctly implements most of the requested steps: data loading, feature selection, standardization, K-Means clustering, PCA, and plotting.
However, it falls short in a few key areas that prevent a higher score:
1.  **Silhouette Score Import:** The generated code uses `silhouette_score` but does not import it, which would lead to a `NameError`.
2.  **K-Value Justification/Selection:** While it plots both WCSS and Silhouette scores, it merely assumes `K=3` without explicitly stating the choice or demonstrating how that choice is made from the plots (e.g., printing the scores or highlighting the chosen K). The ground truth provides a print statement for `chosen_k` and a comment justifying it.
3.  **Plotting:**
    *   The generated code creates two separate plots for WCSS and Silhouette scores, whereas the ground truth combines them into a single figure with subplots for better comparison, as implied by ""justify your choice of K"" which often means side-by-side comparison.
    *   The final PCA scatter plot in the generated code lacks a legend for the cluster labels, making it harder to distinguish between clusters visually compared to the ground truth which uses distinct colors and a legend.
    *   The overall plot aesthetics (grid, markers, title clarity, labels) are slightly less polished than the ground truth.
4.  **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is a good practice for robust code. The generated code lacks this.

Overall, it's a solid attempt that gets the core logic right but needs refinement in terms of completeness, robustness, and presentation.",gpt-3.5-turbo,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.preprocessing import StandardScaler

data = pd.read_csv('winequality-red.csv')
data['quality_bin'] = data['quality'].apply(lambda x: 'good' if x >= 7 else 'bad')

X = data.drop(['quality', 'quality_bin'], axis=1)
y = data['quality_bin']

scaler = StandardScaler()
X_scaled = scaler.fit_...",8,"The generated code correctly addresses most of the task requirements.

**Strong Points:**
- **Correct Binary Target Creation:** Successfully creates a binary target variable `quality_bin` based on the quality score.
- **Data Preprocessing:** Correctly uses `StandardScaler` for feature scaling.
- **Train/Test Split:** Implements an 80/20 train/test split.
- **Model Training:** Appropriately trains a `LogisticRegression` model.
- **Evaluation:** Calculates accuracy, F1-score, and generates a classification report.
- **Feature Importance:** Extracts and reports feature importance based on model coefficients.
- **Proper Imports:** All necessary libraries are imported.

**Areas for Improvement (Why it's not a 10):**
- **Missing Value Handling:** The code assumes no missing values. While `winequality-red.csv` typically doesn't have them, the prompt explicitly asked to ""handle missing values if any."" The ground truth includes a robust check and imputation strategy.
- **Target Variable Type:** The target variable `quality_bin` is created as string labels ('good', 'bad'). While `LogisticRegression` can handle this for binary classification, converting it to integers (0 or 1) as in the ground truth is often a cleaner and more explicit approach, especially for `f1_score` where `pos_label` needs to be specified. The ground truth uses `astype(int)` which is more standard.
- **Stratified Split:** The `train_test_split` does not use `stratify=y`. For imbalanced datasets (which this wine quality dataset tends to be for the 'good' vs 'bad' binary split), stratified sampling is crucial to ensure both classes are proportionally represented in train and test sets, leading to more reliable model evaluation.
- **Detailed Feature Importance Output:** While it correctly identifies top features, the output is less comprehensive than the ground truth. The ground truth prints the actual coefficients and their absolute values, which is more informative for understanding the direction and strength of importance.
- **No File Not Found Handling:** The `pd.read_csv` call lacks a `try-except` block to handle `FileNotFoundError`, which was explicitly included in the ground truth for robustness.
- **No Printed Shapes/Information:** The ground truth includes print statements for shapes of splits, value counts, and confirmation of steps, which helps in debugging and understanding the data flow for the user.

Overall, it's a very good attempt that fulfills the core requirements. The improvements would make it more robust, provide better insights, and align more closely with best practices for data science tasks.",gpt-3.5-turbo,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

# Load the dataset
data = pd.read_csv('heart.csv')

# Create box plot
sns.boxplot(x='HeartDisease', y='Oldpeak', data=data)
plt.show()

# Logistic regression
X = data[['Oldpeak']]
y = data['HeartDisease']

model = LogisticRegression()
model.fit(X, y)

print('Coefficient for Oldpeak:', model.coef_[0][0])

# Discretize Oldpeak into three bins
d...",6,"The generated code addresses most parts of the task but has significant shortcomings in completeness and interpretation, preventing a higher score.

**Strong Points:**
*   Correctly loads the dataset.
*   Correctly generates the box plot.
*   Performs logistic regression with the specified variables.
*   Correctly discretizes the `Oldpeak` column using `pd.qcut` into three bins.

**Areas for Improvement (Specifics):**

1.  **Box Plot Presentation:** The generated code shows the box plot but lacks a title, axis labels, and custom x-tick labels which are crucial for interpretability as specified by good plotting practices in the ground truth.
2.  **Logistic Regression Interpretation:** The task explicitly asks to ""report the coefficient for Oldpeak and interpret its meaning in the context of odds ratios."" The generated code only prints the coefficient. It completely misses the calculation and interpretation of the odds ratio, which is a major omission for a data science task.
3.  **Binning Interpretation:** The generated `pd.qcut` is correct for creating 3 equally sized bins. However, it uses default labels, which are intervals (e.g., `(0.0, 0.2]`), while the task explicitly asks for ""low, medium, high"" labels. This is a minor but specified detail.
4.  **Proportion Calculation:** While the calculation `data[data['HeartDisease'] == 1].groupby('Oldpeak_bins').size() / data.groupby('Oldpeak_bins').size()` is technically correct for proportions, a more idiomatic and robust way in pandas for binary columns is `data.groupby('Oldpeak_bins')['HeartDisease'].mean()`, as the mean of a 0/1 variable directly gives the proportion of 1s. This also handles cases where a bin might have no `HeartDisease=1` entries more gracefully.
5.  **Output Clarity:** The ground truth includes `print` statements before and after plot displays to indicate the step completion, which is a good practice for complex scripts interacting with plots. The generated code lacks these.
6.  **Error Handling/Robustness:** The ground truth includes `try-except` for file loading and handles potential `qcut` errors (e.g., `duplicates='drop'`, or falling back to `pd.cut`), which makes it more robust. The generated code assumes the file exists and that `qcut` will always work without issues.
7.  **Variable Naming (Minor):** Using `data` instead of `df` for DataFrame is a minor stylistic choice, but `df` is more conventional.",gpt-3.5-turbo,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

# Load data
data = pd.read_csv('spotify_2023.csv')

# Convert streams to numeric
data['streams'] = pd.to_numeric(data['streams'], errors='coerce')

# Drop rows where streams is NaN
data = data.dropna(subset=['streams'])

# Crea...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Correct Data Loading and Preprocessing:** Successfully loads data, converts `streams` to numeric with error coercing, and drops NaN values.
- **Correct Target Variable Creation:** `is_popular` is accurately created based on the 75th percentile of `streams`.
- **Feature Selection and One-Hot Encoding:** Identifies the correct numerical and categorical features and applies `pd.get_dummies` for one-hot encoding as required.
- **Correct Data Splitting:** Uses `train_test_split` with the specified `test_size` and `random_state`.
- **Correct Model Training:** Trains a Logistic Regression model with `max_iter=1000`.
- **Correct Evaluation Metrics:** Calculates and prints F1-score (macro), ROC AUC score, and the confusion matrix.
- **Proper Imports:** All necessary libraries are imported.

**Areas for Minor Improvement (for a perfect 10):**
- **Slight Issue with One-Hot Encoding Application:** The line `data_enc = pd.get_dummies(data[features])` creates new columns that implicitly combine the numerical and one-hot encoded categorical features. While this works in this specific case, it's slightly less explicit than separating numerical features and then concatenating with one-hot encoded categorical features (e.g., `pd.get_dummies(data[categorical_features])` and then `pd.concat`). The ground truth's approach of encoding only the categorical columns within X after `X = df[all_features].copy()` is marginally cleaner and more robust for complex feature sets. However, for this task, the generated code's approach is functionally correct and yields the same result.
- **Lack of `stratify` in `train_test_split`:** The ground truth used `stratify=y` which is good practice for imbalanced datasets (which `is_popular` likely is, given it's based on the top 25% vs bottom 75%). While not explicitly requested, it's a best practice for classification tasks aiming for robust evaluation. This is a minor omission but not a critical error given the prompt didn't specify it.

Overall, the code is highly effective and meets the core requirements of the task. The minor points are more about best practices and explicitness rather than functional errors.",gpt-3.5-turbo,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load data
data = pd.read_csv('spotify_2023.csv')

# Select audio features
selected_features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%', 'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm']
data = data[selected_features]
data.dropna(inplace=True)

# Standardize features
scaler = Standar...",7,"The generated code correctly addresses most of the requirements but has a significant issue that affects the ""calculate and display mean values of original features for each cluster"" step.

**Strengths:**
- **Correct Imports:** All necessary libraries are imported.
- **Data Loading:** Correctly loads the CSV.
- **Feature Selection & NaN Handling:** Correctly selects the specified features and drops NaNs.
- **Standardization:** Correctly uses `StandardScaler`.
- **K-Means Clustering:** Correctly applies K-Means with the specified parameters (K=4, `random_state=42`, `n_init='auto'`).
- **PCA:** Correctly applies PCA with 2 components and `random_state=42`.
- **Visualization:** Creates a scatter plot of the principal components colored by cluster, with a title and labels.

**Areas for Improvement / Missing Features:**

1.  **Mean Calculation of Original Features:** The most critical flaw is in how the mean values are calculated.
    *   `data = data[selected_features]` followed by `data.dropna(inplace=True)` modifies the *original* `data` DataFrame in place.
    *   Then, `data_scaled = scaler.fit_transform(data)` uses this modified `data` to scale.
    *   However, when `data['cluster'] = clusters` is executed, the `data` DataFrame already contains only the *selected features* and has *NaN rows dropped*. This means the `data` DataFrame used for calculating means *is* the one that was used for scaling, not the *true* original (unstandardized) data but rather the *unstandardized but filtered* data.
    *   The prompt specifically asks for ""mean values of the original (unstandardized) selected audio features"". While the features themselves are unstandardized in the `data` DataFrame, the best practice is to always store the original (unstandardized) data *before* any in-place modifications for subsequent analysis, especially when scaling is involved. The ground truth code handles this by creating `df_original_for_analysis = df_features.copy()` *before* scaling. While the current code does eventually use the unstandardized filtered data, it's not explicitly saving the state *before* the scaling process which can lead to confusion or subtle bugs in more complex pipelines.

2.  **Robustness (File Not Found):** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for production code. The generated code lacks this.

3.  **Plot Details:**
    *   The `plt.colorbar()` call does not specify ticks or a label, making it less informative than the ground truth's `cbar.set_label('Cluster Label')`.
    *   No `plt.show()` call is explicitly present in the generated code, though it usually isn't strictly necessary in interactive environments (like notebooks) due to implicit calls. For standalone scripts, it's good practice.
    *   The plot title is a bit generic (""K-Means Clustering of Spotify Audio Features"") compared to the ground truth's (""K-Means Clusters (K=4) on PCA-Reduced Spotify Audio Features"").
    *   No `plt.grid(True)` is present.

The `data` DataFrame undergoes transformations (`data = data[selected_features]`, `data.dropna(inplace=True)`) before `data_scaled` is created. This `data` DataFrame then gets the `cluster` column added and is used for mean calculations. This is technically correct for ""unstandardized but filtered"" data. However, for a user who might expect the *absolutely original* features (even if those had NaNs), it could be confusing. The ground truth keeps a separate copy of the relevant original data *before* scaling for this specific purpose, which is generally a better design choice for clarity and avoiding unexpected side effects on the `data` variable.

The core logic is sound, but the handling of the original data for mean calculations is slightly less robust and clear than ideal, and some plotting details are missing.",gpt-3.5-turbo,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load data
data = pd.read_csv('spotify_2023.csv')

# Convert streams to numeric
data['streams'] = pd.to_numeric(data['streams'], errors='coerce')

# Filter data
filtered_data = data[(data['streams'] > 10000000) & (data['released_year'].between(2010, 2023))].dropna(subset=['streams'])

# Group data by released_year
grouped_data = filtered_data.groupby('released_year')

# Calculate average audio features
avg_danceability = grouped_dat...",9,"The generated code correctly performs all the required steps: loading data, converting streams to numeric, filtering data based on streams and released_year, grouping by year, calculating average audio features, and plotting them. The code is well-structured and uses appropriate pandas and matplotlib functions.

Strong points:
- Correctly handles `streams` conversion with `errors='coerce'`.
- Correctly implements the filtering criteria for `streams` and `released_year`.
- Correctly calculates the yearly averages for the specified audio features.
- Generates three separate line plots as requested, with appropriate titles and labels.
- Uses `plt.subplot` for good visualization layout.
- `tight_layout()` is used for better plot presentation.

Areas for minor improvement (to reach a perfect 10):
- The `dropna(subset=['streams'])` is applied *after* the initial filtering of `streams > 10M` in the generated code. While this technically works because `NaN > 10M` evaluates to `False`, it's generally clearer and safer to `dropna` immediately after `to_numeric` to ensure only valid numeric streams are considered for comparison, as done in the ground truth. The ground truth dropped NaNs *before* the stream comparison, which conceptually makes more sense to ensure the `$10M` comparison only happens on valid numbers. In this specific case, it doesn't break correctness but is a slight stylistic difference.
- Error handling for `FileNotFoundError` is missing for the CSV loading, though not explicitly requested as a core task requirement, it's good practice.
- The plots could benefit from rotating x-axis labels if there are many years, as done in the ground truth, for better readability, and potentially adding markers to the lines for clarity. However, for a small range of years (2010-2023), it's often fine without.
- The `grouped_data` is used for individual series extractions (e.g., `grouped_data['danceability_%'].mean()`). While functional, it's often more concise to do `filtered_data.groupby('released_year')[['danceability_%', 'energy_%', 'valence_%']].mean()` to get a DataFrame directly, which then makes plotting slightly cleaner (e.g., `plt.plot(yearly_avg_features.index, yearly_avg_features['danceability_%'])`). However, the current approach is also valid.

Overall, excellent solution that fully meets the core requirements.",gpt-3.5-turbo,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
data = pd.read_csv('youtube_trendings.csv')

# Data cleaning
data['Likes'] = data['Likes'].str.replace(',', '').astype(float).fillna(0)
data['Dislikes'] = data['Dislikes'].str.replace(',', '').astype(float).fillna(0)
data['Video views'] = da...",7,"The generated code addresses most of the requirements and successfully builds and evaluates a regression model.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly removes commas and converts 'Likes', 'Dislikes', 'Video views' to numeric, and fills NaNs with 0.
*   Correctly fills missing 'Category' values with 'Unknown'.
*   Correctly applies one-hot encoding for 'Category'.
*   Correctly defines X and y.
*   Correctly splits data with the specified `test_size` and `random_state`.
*   Correctly trains a Linear Regression model.
*   Correctly evaluates and reports R-squared and MAE.

**Areas for improvement:**
*   **Handling of `published` column:** The prompt specifically stated to treat `published` column (year) as a numeric feature. The generated code `data['published'] = data['published'].str.extract(r'(\d{4})').astype(float)` attempts to extract the year but renames the column back to 'published'. It would have been better to extract the year and rename it to something like 'published_year' explicitly, as intended by the phrase ""the `published` year"". Also, `astype(float)` without `fillna(0)` after extraction might leave NaNs if extraction fails, though the prompt implies filling *resulting* NaNs for `Likes`, `Dislikes`, `Video views` but not explicitly for `published` year. The ground truth extracts the year and fills NaNs with 0, which is a safer approach for a numeric feature.
*   **Hardcoded `X` features**: The one-hot encoded `Category` columns are hardcoded in the `X` definition (`Category_Autos & Vehicles`, etc.). This makes the code less robust to changes in the unique categories in the dataset. A better approach, as demonstrated in the ground truth using `pd.get_dummies` and concatenation, would automatically include all generated OHE columns. While it works for the given dataset, it's not ideal for generalizability.
*   **Minor formatting/output**: The R2 and MAE print statements don't include the same precision as the ground truth (e.g., `:.4f` for R2, `:.2f` for MAE). This is a minor point but affects output consistency.",gpt-3.5-turbo,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '').astype(float).fillna(0)

# Convert 'published' column to numeric
df['published'] = pd.to_numeric(df['published'], errors='coerce')

# Group data by 'published' year
grouped = df.groupby('published')

# Calculate metrics for each year
avg_views = grouped['Video views'].mean()
unique_vide...",7,"The generated code addresses most of the requirements. It correctly loads the data, cleans the `Video views` column, performs grouping and calculation of metrics, and generates the required subplots.

**Strengths:**
*   Correctly loads the dataset.
*   Successfully cleans the `Video views` column by removing commas, converting to float, and filling NaNs with 0.
*   Correctly groups the data and calculates `mean` for video views and `nunique` for video titles.
*   Generates two subplots and plots the correct data on each.
*   Includes appropriate titles and labels for the plots.

**Areas for Improvement (leading to a score of 7 instead of higher):**
*   **""published"" column handling:** The task specifically states: ""Ensure the `published` column (year) is treated as a numeric or integer type."" The ground truth code correctly infers that `published` actually means `publishedAt` (a common column name for full timestamps) and extracts the year from it. The generated code simply tries `pd.to_numeric(df['published'], errors='coerce')`. If the `published` column in `youtube_trendings.csv` is not simply a year but a full timestamp string (which is highly probable for a `published` column in such a dataset), this conversion will likely result in a column full of NaNs or incorrect values, thus breaking the yearly trend analysis. This is a significant functional flaw based on typical dataset structures.
*   **Robustness:** The code assumes the file exists. Adding a `try-except` block for `FileNotFoundError` as in the ground truth would improve robustness.
*   **Best Practices:** While the request was only for code, wrapping the analysis logic in a function would be better practice for reusability, especially if the file path might change.
*   **Plotting details:**
    *   The `plt.tight_layout()` is good, but the ground truth uses `rect=[0, 0.03, 1, 0.96]` which can prevent the suptitle from overlapping axis labels if there is one.
    *   Adding `marker='o'` and `linestyle='-'` and `grid(True)` as in the ground truth helps readability for line plots.
    *   The ground truth also sets specific x-axis ticks to be unique integer years, which is often better for year-based plots than allowing `matplotlib` to choose arbitrary float ticks.
    *   The ground truth also includes `ticklabel_format(style='plain', axis='y')` on the view plot to prevent scientific notation, which can be an issue with large view numbers.

The most critical issue is the `published` column handling, as it might lead to incorrect results if the column is a full date string. This prevents a higher score.",gpt-3.5-turbo,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
from nltk.corpus import stopwords

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Create title_word_count feature
df['title_word_count'] = df['Video'].apply(lambda x: len(re.findall(r'\w+', x)))

# Clean Video views column
df['Video views'] = df['Video views'].str.replace(',', '').astype(float).fillna(0)

# Calculate Pearson correlation coefficient
corr = df['title_word_count'].corr(df['Video views'])
print(...",7,"The generated code correctly addresses most of the requirements. It loads the dataset, creates the `title_word_count`, cleans the `Video views` column, calculates the correlation, filters by year, processes titles, and finds the most common words.

However, there are a few areas for improvement:
1.  **NLTK Download:** The ground truth includes `try-except` block to ensure NLTK stopwords are downloaded, which is a good practice for robustness. The generated code assumes they are already downloaded.
2.  **Date Filtering Mismatch:** The generated code uses `df['published'].str.contains('2023|2024')` for filtering. While this might work for some date formats, it's not robust. The ground truth correctly converts the 'published' column to datetime objects and then filters by the year attribute, which is much more reliable and handles various date formats correctly. This is a significant point of difference for robustness.
3.  **Word Processing (Punctuation Removal & Flattening):** The `words_list = filtered_df['Video'].str.lower().str.replace('[^\w\s]', '').str.split()` line is efficient but the subsequent flattening `[word for sublist in words_list for word in sublist if word not in stop_words]` needs to handle potential `NaN` values if any titles were non-string or generated `NaN` after `str.split()`. The `re.sub(r'[^\w\s]', '', title)` method in the ground truth is often more robust as it operates on each string before splitting. Also, the ground truth uses `.isalpha()` to ensure only alphabetic words are counted, which is a good refinement missed by the generated code.
4.  **Output Clarity:** The ground truth provides more descriptive print statements for the correlation and the most common words, making the output easier to understand. For instance, clearly stating the `Pearson correlation coefficient` and how many occurrences each common word has.
5.  **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is a good practice for real-world scenarios. The generated code lacks this.",gpt-3.5-turbo,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score
from sklearn.preprocessing import OneHotEncoder
import re

# Load data
data = pd.read_csv('mobiles_2025.csv')

# Clean Launched Price (USA)
data['Launched Price (USA)'] = data['Launched Price (USA)'].replace({'USD ': '', ',': ''}, regex=True).astype(float, errors='coerce')
data['is_expensive_usa'] = (data['Launc...",7,"The generated code addresses most of the requirements but has several issues that affect its correctness and robustness.

**Positives:**
*   Loads data correctly.
*   Correctly cleans `Launched Price (USA)` and creates `is_expensive_usa`.
*   Correctly cleans `Mobile Weight`, `RAM`, `Battery Capacity`, and `Screen Size`.
*   Correctly uses `RandomForestClassifier` and evaluates with F1 and ROC AUC.
*   Uses `random_state` for reproducibility in `train_test_split` and `RandomForestClassifier`.

**Areas for Improvement/Corrections:**
1.  **Target `is_expensive_usa` Creation Precision:** The ground truth meticulously defines `is_expensive_usa` as `1 if x > 1000 else (0 if pd.notna(x) else np.nan)`. The generated code uses `(data['Launched Price (USA)'] > 1000).astype(int)`. This converts `NaN` values in `Launched Price (USA)` (from `errors='coerce'`) into `0` (or `False` then `0`) in `is_expensive_usa` instead of `NaN`. The task explicitly states: ""Drop rows where `is_expensive_usa` is NaN."" This implies `NaN` should be allowed to propagate to the target where the price was unparseable. The ground truth correctly handles this by allowing `np.nan` for unparseable prices and then dropping rows where `is_expensive_usa` explicitly became `NaN`. The generated code's initial `data = data.dropna(subset=['is_expensive_usa'])` call removes rows where the *original* `Launched Price (USA)` was `NaN`, but not those where `pd.to_numeric` resulted in `NaN` *and* this `NaN` was then converted to `0` by `.astype(int)`. This is a subtle but important difference in NaN handling that could affect the dataset used for training.
2.  **Camera Feature Cleaning:** The task states: ""For `Front Camera` and `Back Camera`, extract the first numerical MP value (default 0 if none)."" The generated code uses `.str.extract('(\d+)')` and converts to `float`, which extracts only integers. This might miss decimal MP values (e.g., ""12.5 MP""). The ground truth uses `re.search(r'(\d+(\.\d+)?)\s*MP', ...)` which correctly captures decimal values.
3.  **One-Hot Encoding Index Alignment:** The one-hot encoding for `company_name_encoded` and `processor_encoded` creates new DataFrames without preserving the original index. When these are `pd.concat`enated with `data[...]`, they are concatenated purely by positional index, which can lead to misalignment if rows were dropped from `data` previously. While in this specific case, `dropna(subset=['is_expensive_usa'])` doesn't reorder the index, it's generally a bad practice. `OneHotEncoder` should be applied directly to the DataFrame or the encoded results should explicitly carry the original DataFrame's index. The ground truth approaches this better by performing encoding *before* the final `X` and `y` definition and then `dropna`ing `X` and `y` together, which ensures index alignment. A more robust way for the generated code would be to use `pd.get_dummies` which automatically handles index alignment, or set `index=data.index` when creating `company_name_encoded` and `processor_encoded`.
4.  **Final NaN Handling during X/y Definition:** The task states ""Drop rows with any NaNs in features or target."" The generated code does `X = X.dropna()` and then `y = y[X.index]`. This process *could* lead to a mismatch if `X.dropna()` actually changed the `X` DataFrame (which it will). The ground truth's approach using `pd.concat([X, y], axis=1).dropna()` is much more robust as it ensures that `X` and `y` always have the same set of rows after any NaN removal.
5.  **`OneHotEncoder` `sparse=False`:** While technically required because we are concatenating to a dense DataFrame, not explicitly mentioned in the original task was `sparse=False`. Good to see it included.
6.  **Readability/Structure:** The ground truth code includes comments, print statements, and logical sectioning, making it much easier to follow the execution flow and understand each step, aligning with principles of good code quality for a general user/maintainer. The generated code is more compact but less descriptive. For an ""expert code evaluator,"" this isn't a critical failure, but it's a difference in quality.

The primary reason for the score of 7 is the slight deviation in the target variable creation's NaN handling, the less precise camera cleaning, and the less robust index alignment during one-hot encoding and final NaN removal, which could lead to subtle data integrity issues or incorrect results if the input data were slightly different.",gpt-3.5-turbo,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Define a function to clean price columns
def clean_price(price):
    try:
        numeric_price = pd.to_numeric(price.str.replace('[^\d.]', ''), errors='coerce')
        return numeric_price
    except:
        return np.nan

df['Launched Price (Pakistan)'] = clean_price(df['Launched Price (Pakistan)'])
df['Launched Price (India)'] = clean_price(df['Launched Price (India)'])
df['Launched ...",6,"The generated code addresses most parts of the task but has significant issues in the price cleaning function and its application.

**Issues:**

1.  **Incorrect `clean_price` function logic:**
    *   The `clean_price` function takes a pandas Series (e.g., `df['Launched Price (Pakistan)']`) as input, not a single price string as intended for row-wise application or a common standalone cleaning utility.
    *   `price.str.replace('[^\d.]', '')` regex is insufficient. The task explicitly requires removing currency symbols (`PKR`, `INR`, `CNY`, `USD`, `AED`) and commas. The regex `[^\d.]` would only keep digits and dots, which *might* remove symbols if they are not digits or dots, but it's not robust for explicitly listed symbols and commas. For example, `USD 1,234.56` would become `1234.56` but `PKR 1,234.56` might become `1234.56`, `INR 1,234.56` would become `1234.56`, etc. This looks like it works for the `USD` example given, but it doesn't meet the explicit requirement of removing ""currency symbols (PKR, INR, CNY, USD, AED) and commas"". The ground truth code specifically addresses `replace` for each symbol and then for commas, which is more robust and directly addresses the prompt.
    *   The `pd.to_numeric` with `errors='coerce'` *partially* handles conversion errors, but the overall `clean_price` function structure is flawed for the intended application (it operates on a Series, not scalar inputs).
2.  **Missing Error Handling for `pd.read_csv`:** The code does not include `try-except FileNotFoundError` for loading the CSV, which is good practice.
3.  **No `skipna=True` for `mean()`:** The prompt explicitly states: ""This average should only consider countries for which price data is available (not NaN) for that specific model."" While `mean()` *may* default to `skipna=True`, it's better practice and more explicit to include it, especially given the task's requirement.
4.  **Display Format:** The last line `top_5_models[['Model Name', 'Average Global Price']]` will return a DataFrame, but it won't *print* it to the console (unless it's in an interactive environment like a Jupyter notebook, which isn't guaranteed). For a script, `print()` is necessary.

**Strengths:**

*   Correct use of pandas for loading, sorting, and selecting data.
*   Correct identification of the columns for price calculation.
*   Correctly sorts and selects the top 5 models.",gpt-3.5-turbo,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean RAM, Battery Capacity, and Launched Price (USA)
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''), errors='coerce')

# Drop rows...",4,"The generated code addresses some aspects of the task but critically fails on a major requirement: generating *separate* box plots for each `Launched Year`. Instead, it attempts to plot all years on a single set of three axes, overwriting the previous year's plot with the current one, leading to only the last year's data being displayed. This fundamentally breaks the comparative analysis per year.

Specific issues:
- **Incorrect Plotting Logic**: The `for year, data in grouped_data:` loop should create *new* figures or subplots for each iteration (i.e., for each year) to show individual year's distributions. Trying to plot all years onto `ax[0]`, `ax[1]`, `ax[2]` within the loop means only the very last year processed will be visible.
- **Missing String Conversion before cleaning**: The cleaning steps `df['RAM'].str.replace('GB', '')` etc. assume the columns are already strings. While this might work if `df['RAM']` is initially an object type containing strings, it's safer and more robust to explicitly convert to string (e.g., `astype(str)`) before applying `.str` methods, especially if there might be mixed types or NaN values.
- **Minor Cleaning Omission**: It missed cleaning `RAM`, `Battery Capacity`, and `Price` values that might have leading/trailing whitespace after removing 'GB', 'mAh', etc. The Ground Truth uses `.str.strip()` which is a good practice.
- **Handling `Launched Year` NaNs**: The task description implies that cleaning should handle errors and drop rows where *any* of the three specified columns are NaN. The Ground Truth also explicitly cleans the `Launched Year` column to ensure it's numeric and handles potential NaNs there, which the generated code does not.
- **Limited Imports**: It lacks `seaborn` which is used in the ground truth for styling, though not explicitly required by the prompt, it usually leads to better aesthetics.
- **Error Handling**: No `try-except` block for file loading, which is good practice.

Overall, the core plotting requirement for *comparative* analysis *per year* is not met.",gpt-3.5-turbo,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import PolynomialFeatures, FunctionTransformer
from sklearn.feature_extraction.text import Co...",5,"The generated code attempts to address many requirements but falls short significantly in several critical areas, making it fundamentally incorrect or incomplete for an ""advanced"" solution as requested by the task.

**Critical Issues (Primary reasons for low score):**

1.  **Data Preprocessing and Pipelines:**
    *   **Direct Modification of `df`:** The generated code directly modifies the `df` DataFrame (`df.drop(['Name'], axis=1, inplace=True); df['Title'] = ...`). This is problematic in a robust MLOps context. Data preprocessing should be encapsulated within a `Pipeline` or `ColumnTransformer` to ensure consistent application to training and test sets and ease of model deployment. The ground truth uses a custom `FeatureEngineerTransformer` within a `Pipeline` for this reason, which is the correct and advanced approach.
    *   **Feature Engineering Incompleteness:** The task specifically asked for ""comprehensive data preprocessing including advanced feature engineering (polynomial features, interaction terms, text mining from names)"". The generated code only extracts 'Title' from 'Name' and performs some basic mapping. It completely misses:
        *   **Polynomial Features/Interaction Terms:** Despite importing `PolynomialFeatures`, it's not used in the `preprocessor` or any pipeline.
        *   **Text Mining beyond Title:** While `CountVectorizer` is mentioned for 'Title', the `Title` column is then mapped to numerical values (`df['Title'].map(...)`) making `CountVectorizer` on `text_features = ['Title']` nonsensical and likely creating errors or unexpected behavior. The `Title` column should be treated as categorical and one-hot encoded after extraction, or if genuinely text-mined (e.g., sentiment from names, which is less applicable here), it needs a different approach. The current `make_pipeline(SimpleImputer(strategy='constant', fill_value=''), CountVectorizer())` for `text_features = ['Title']` after `df['Title'].map(...)` is a major conceptual flaw.
        *   **Cabin/Deck Feature Engineering:** Completely missed.
        *   **FamilySize/IsAlone/Fare_Per_Person:** Completely missed.
    *   **Imputation Strategy:** While `IterativeImputer` is imported and used, it's applied on `numerical_features` that don't include `Age` and `Fare` after the initial FE. The `preprocessor` setup is also confusing: `('num', IterativeImputer(max_iter=10), numerical_features)` but `numerical_features` in the generated code itself only `['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']`, `Age` and `Fare` are correctly in it. However, the `fit_transform` then happens on the *entire* `df.drop('Survived', axis=1)` (including categorical columns which are also then passed to the numerical imputer pipeline, this would throw an error or lead to incorrect behavior).
    *   **Incorrect `preprocessor` Application:** In the hyperparameter tuning loop, `grid_search.fit(preprocessor.fit_transform(df.drop('Survived', axis=1)), df['Survived'])` is incorrect. `preprocessor.fit_transform` should be applied *once* on `X_train` and `X_test` *separately* (`X_train_processed`, `X_test_processed`), or the `preprocessor` itself should be part of each model's `Pipeline` to prevent data leakage and ensure consistent preprocessing. The current approach applies `fit_transform` repeatedly within the loop for *each* grid search, which is inefficient and methodologically unsound for grid search.
    *   **No Train-Test Split in Base Model Tuning:** The base models are tuned on the *entire dataset* (`df.drop('Survived', axis=1)`), not on a dedicated training set. This is a severe form of data leakage and compromises the validity of the hyperparameter tuning and model evaluation. The task explicitly asks for ""extensive model evaluation with stratified k-fold cross-validation"" *after* training, implying a proper train-test split for initial model development (which the ground truth does).
    *   **Incomplete ColumnTransformer:** The `text` transformer in `ColumnTransformer` is redundant and incorrectly used given the `Title` column is already mapped to numbers *before* `preprocessor` is applied.

2.  **Model Building and Tuning:**
    *   **`GridSearchCV` vs. `RandomizedSearchCV`:** The task asked for `GridSearchCV` *or* `RandomizedSearchCV`. While `GridSearchCV` is used, `RandomizedSearchCV` is generally preferred for its efficiency in high-dimensional hyperparameter spaces, particularly for complex tasks. This is a minor point, but worth noting for ""advanced"" solutions.
    *   **SVC `probability=True`:** The `SVC` model is missing `probability=True`, which is necessary for `VotingClassifier(voting='soft')`. This would likely cause an error in the ensemble.
    *   **Ensemble Model Setup:** The stacking classifier `make_pipeline(SelectFromModel(best_models['rf']), VotingClassifier(estimators=estimators))` is an arbitrary combining of feature selection and voting. A proper stacking classifier involves training a meta-learner on the predictions of the base models, which is what `sklearn.ensemble.StackingClassifier` is for. The generated code's stacking implementation is incorrect.

3.  **Feature Selection:**
    *   **`SelectFromModel` and `RFECV`:** While these are implemented, they are not integrated into a proper `Pipeline` or `ColumnTransformer` for consistent application. Their impact is not really ""compared"" as requested, only applied. The `RFECV` is applied to `best_models['rf']` but this model itself would need to operate on processed data.

4.  **Model Evaluation:**
    *   **No Validation Curves:** `validation_curve()` function is called but its output is neither saved nor plotted, making it non-functional.
    *   **Limited Learning Curves:** Similar to validation curve, the `learning_curve()` function is called but its output is not stored or plotted.
    *   **Missing Performance Metrics:** The final model comparison only prints mean accuracy from `cross_val_score`. The task explicitly asked for ""comprehensive performance metrics"" (e.g., precision, recall, F1, ROC AUC, confusion matrix) and ""statistical significance tests."" ROC AUC is only used in SHAP as target. The `classification_report`, `confusion_matrix`, `roc_auc_score` are imported but not used for comprehensive evaluation.
    *   **No Statistical Significance Tests:** The task explicitly asked for ""statistical significance tests,"" which are completely missing.

5.  **Model Interpretability:**
    *   **SHAP Application:** While SHAP is used, the `shap_values = explainer.shap_values(preprocessor.fit_transform(df.drop('Survived', axis=1)))` line re-processes the entire dataset again. This is redundant and could lead to issues.
    *   **LIME:** Completely missing. The task explicitly asked for ""SHAP or LIME"".

**Positive Points (but not enough to salvage the score):**
*   Proper import of most necessary libraries.
*   Attempts to include various components: FE, imputation, tuning, ensembles, interpretability.
*   Basic structure of using `GridSearchCV` for hyperparameter tuning.

In summary, the generated code demonstrates a superficial understanding of the requirements without implementing them robustly or correctly according to best practices for advanced ML development (e.g., systematic pipelines, proper train-test split, comprehensive evaluation metrics, and correct ensemble/feature selection methodologies). The preprocessing steps are particularly flawed.",gpt-3.5-turbo,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
data = pd.read_csv('imdb_top_250.csv')

# Define features and target
X = data[['Year', 'Runtime']]
y = data['Rating']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Linear Regression mod...",8,"The generated code correctly implements all the core requirements of the task: loading data, defining features and target, splitting data, training a Linear Regression model, and evaluating it with R-squared and MAE. The imports are correct, and the code structure is logical and easy to follow.

However, the ground truth code includes crucial data preprocessing steps (handling ' min' in 'Runtime', coercing errors for 'Year' and 'Rating' to numeric, and dropping NaNs) and robust error handling for file loading and missing columns, which the generated code lacks. While these were not explicitly requested in the prompt's main points, they are often necessary for real-world data science tasks and are good practices. Without them, the generated code might fail or produce incorrect results if the CSV is not perfectly clean (e.g., if 'Runtime' contains ' min' or if there are non-numeric values or NaNs in 'Year' or 'Rating').

Despite these omissions, the generated code directly addresses all outlined requirements in the prompt. Its simplicity could be seen as an advantage if strict adherence to the prompt was the sole criterion and data quality was guaranteed.",gpt-3.5-turbo,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('imdb_top_250.csv')

# Select features for clustering
X = data[['Rating', 'Runtime']]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
data['Cluster'] = kmeans.fit_predict(X_scaled)

# ...",7,"The generated code correctly implements most of the requirements. It loads the data, selects features, standardizes them, applies K-Means with the specified parameters, and generates a scatter plot. It also calculates and prints the mean `Rating` and `Runtime` for each cluster.

However, there are two main areas for improvement:
1. **Robustness for 'Runtime' column:** The ground truth code includes crucial data cleaning for the 'Runtime' column to handle string formats (e.g., '142 min') and convert them to numeric. The generated code does not include this, which could lead to errors if the 'Runtime' column is not already purely numeric or if there are missing values (e.g., `ValueError: could not convert string to float`).
2. **Missing values handling:** The generated code implicitly assumes no missing values or that `KMeans` can handle them gracefully. The ground truth explicitly drops rows with `NaN` in the selected features after conversion, which is a more robust approach for real-world datasets.
3. **Plot Aesthetics:** While the scatter plot is created, it lacks some aesthetic details present in the ground truth, such as `figsize`, `cmap`, `s`, `alpha`, `colorbar`, and `grid`, which make the plot more informative and visually appealing.
4. **Error Handling for File Not Found:** The generated code does not include error handling for `FileNotFoundError` when loading the CSV, which the ground truth does.

Addressing the data cleaning and robustness for the 'Runtime' column is critical for a production-ready solution and is the primary reason it's not scored higher.",gpt-3.5-turbo,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"import pandas as pd
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
from scipy.stats import pearsonr

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create new feature title_length
df['title_length'] = df['Title'].apply(lambda x: len(x))

# Calculate Pearson correlation coefficient
correlation_coefficient, _ = pearsonr(df['title_length'], df['Rating'])
print(f""Pearson correlation coefficient between title_length ...",7,"The generated code correctly addresses most of the tasks.

**Strengths:**
*   All required imports are present (though `pearsonr` is used from `scipy` instead of `df.corr()`, which is acceptable).
*   Correctly loads the dataset.
*   Correctly calculates `title_length`.
*   Successfully calculates the Pearson correlation coefficient.
*   Implements a clean `preprocess_title` function that handles lowercasing, punctuation removal (though less robustly than regex), tokenization, and stop word removal.
*   Correctly finds and prints the 10 most common words and their frequencies.

**Areas for Improvement:**

1.  **NLTK Downloads:** The code assumes NLTK resources (`stopwords`, `punkt`) are already downloaded. The ground truth includes `try-except LookupError` blocks to handle this, which is a robustness improvement. While the prompt implied *ensuring* NLTK resources were available, the ground truth explicitly handled the download, which is better practice. The generated code needs to handle this.
2.  **Punctuation Removal Robustness:** The punctuation removal `title = ''.join([char for char in title if char.isalnum() or char.isspace()])` is *less robust* than the regex approach (`re.sub(r'[^a-z0-9\s]', '', text)`) used in the ground truth. It will remove any characters that are not alphanumeric or whitespace, including certain symbols that might be present in titles (e.g., copyright symbols, non-English characters with diacritics if not handled by `isalnum()` for specific locales, though the prompt implies standard English titles). The `string` module import is present but not used for its common purpose in punctuation removal, reinforcing the less robust method.
3.  **Correlation Calculation:** While `scipy.stats.pearsonr` is valid, it calculates between two series and doesn't inherently handle potential `NaN` values as gracefully as `df.corr()` after `dropna`. The prompt didn't strictly require `dropna` for correlation, but it's good practice for numerical analysis and the ground truth demonstrated it. The generated code implicitly relies on `pearsonr` to handle or error on non-numeric types, but it doesn't explicitly convert `Rating` to numeric or handle potential errors, which was a point in the ground truth.
4.  **Clarity of Output:** The Pearson correlation output for the generated code is just a raw number, whereas the ground truth includes a descriptive string and formatting (`f""Pearson correlation between title_length and Rating: {correlation:.4f}""`). This improves readability.

Overall, it's a good attempt, but a few refinements in robustness and output formatting would make it excellent.",gpt-3.5-turbo,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

data = pd.read_csv('penguins.csv')

X = data[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']]
y = data['species']

num_pipeline = P...",6,"The generated code attempts a robust solution using `Pipeline` and `ColumnTransformer`, which is generally good practice. However, it fails on key requirements, particularly regarding the handling of missing categorical data and the target variable.

**Strong Points:**
*   Uses `Pipeline` and `ColumnTransformer` for streamlined preprocessing, which is excellent for reproducibility and maintaining a clean workflow.
*   Correctly identifies numerical and categorical features for separate processing.
*   Correctly uses `SimpleImputer` with 'median' for numerical features and `StandardScaler` for scaling.
*   Correctly uses `SimpleImputer` with 'most_frequent' for categorical features and `OneHotEncoder` for encoding.
*   Trains `RandomForestClassifier` with specified parameters.
*   Performs 5-fold cross-validation and prints the mean accuracy.

**Areas for Improvement/Missing Requirements:**

1.  **Missing Target Variable Handling (Critical):** The ground truth explicitly handles potential `NA` values in the `species` target. The generated code does not check for or handle these, which could lead to errors or incorrect results if `species` has missing values.
2.  **Missing Label Encoding for Target (Critical):** `RandomForestClassifier` (and many other scikit-learn classifiers) expects numerical labels for the target variable `y`. The `species` column is categorical (`Adelie`, `Gentoo`, `Chinstrap`). The generated code passes `y` directly to `cross_val_score` without `LabelEncoder`, which would cause an error because `RandomForestClassifier` cannot handle string targets. The ground truth correctly uses `LabelEncoder`.
3.  **Specific `island` NA Handling (Minor but Specified):** The task stated: ""if mode imputation results in NA for island, drop those rows"". While `SimpleImputer(strategy='most_frequent')` will generally impute the mode, the exact edge case of ""if all were NA"" for `island` and then dropping those rows was a specific requirement that the pipeline approach might not perfectly cover without customization (e.g., a custom transformer in the pipeline or an additional cleaning step). The ground truth correctly addresses this. In practice, `SimpleImputer` will not produce `NaN` if the mode is taken from a non-empty set of values; the ground truth's handling assumes a very specific edge case where the whole column might be `NaN`.
4.  **Implicit Data Dropping:** The pipeline implicitly handles NaNs by imputing them. The task mentioned that *if* `island` still has NA after mode imputation (e.g., if the mode itself was NA, which is highly unlikely for strings, or if the whole column was NA resulting in no mode), then those rows should be dropped. The pipeline approach does not explicitly check for or drop rows where `island` might still be NA after imputation, though for typical string data, `most_frequent` imputation does not result in `NaN` values. The ground truth includes a safeguard for this.

Overall, the use of `Pipeline` and `ColumnTransformer` is a more professional and modular approach, but the code misses fundamental requirements regarding target variable preparation, which is a critical flaw for classification tasks.",gpt-3.5-turbo,LLM_judge_config
H44,hard,Clustering,penguins.csv,"import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv(""penguins.csv"")

# Select the features
selected_features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']

# Drop rows with missing values in selected features
data = data.dropna(subset=selected_features)

# Standardize the selected features
scaler = Standar...",7,"The generated code correctly addresses most of the task requirements, successfully loading data, selecting features, handling NAs (implicitly by dropping from the main DataFrame), standardizing, applying K-Means and PCA, and generating two scatter plots.

However, there are a few areas for improvement:
1.  **NA Handling and Data Alignment:** The generated code applies `dropna` *before* separating the `species` column from the features used for standardization and clustering. While this *works* for the specified task, it's generally better practice to explicitly handle the `species` column alignment, as shown in the ground truth, especially if additional columns were present and not intended for feature selection and NA checks. The ground truth explicitly selects `species` along with the numerical features for `dropna` to ensure proper alignment. The generated code drops NAs from the *entire* `data` DataFrame, which could be problematic if NAs exist in other columns *not* relevant to this task but were implicitly removed. For this specific task, it works because `species` is the only other relevant column and is implicitly aligned.
2.  **PCA Output Assignment:** The line `data['pc1'], data['pc2'] = zip(*pca.fit_transform(data[selected_features]))` is a less conventional way to assign PCA components to new columns. While it achieves the desired result, creating a separate DataFrame for PCA results and then merging/assigning, as in the ground truth using `pd.DataFrame`, is often cleaner and more readable, especially if you need to inspect the PCA DataFrame directly.
3.  **Plotting:**
    *   The second plot (species labels) uses `data['species'].astype('category').cat.codes` to map species to numerical values for `cmap='viridis'`. While this works, using a `hue` argument with `seaborn.scatterplot` (as in the ground truth) is more direct and automatically handles categorical coloring with a legend, making it less prone to misinterpretations with arbitrary `cmap` choices like `viridis`. The current approach makes species look like a continuous variable.
    *   The default `matplotlib.pyplot.scatter` doesn't automatically generate distinct legends for categories as easily as `seaborn`. The color bar for `c=data['cluster']` is also less intuitive for discrete cluster labels compared to a categorical legend.
    *   Titles are present, but axes labels are missing, which reduces clarity.
4.  **Code Structure and Readability:** The ground truth separates concerns more clearly (e.g., creating a `pca_df` for plotting, explicitly defining `X` and `original_species_labels`), leading to slightly more modular and readable code. The code modifies the original `data` DataFrame extensively, which might not always be desired in larger pipelines.

Overall, the code performs the core operations correctly but lacks some of the robustness, clarity, and best practices seen in the ground truth, particularly in data manipulation and plotting for categorical variables.",gpt-3.5-turbo,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Impute missing values
df['culmen_length_mm'].fillna(df['culmen_length_mm'].mean(), inplace=True)
df['culmen_depth_mm'].fillna(df['culmen_depth_mm'].mean(), inplace=True)
df['flipper_length_mm'].fillna(df['flipper_length_mm'].mean(), inplace=True)
df['body_m...",7,"The generated code correctly addresses most of the requirements.

**Strong Points:**
*   Loads the dataset.
*   Correctly imputes numerical columns with their mean and 'sex' with its mode.
*   Correctly drops rows where 'island' is NA.
*   One-hot encodes 'island' and 'sex' and combines them with numerical features.
*   Label encodes the 'species' target.
*   Trains a `GradientBoostingClassifier` with the specified parameters.
*   Extracts and prints feature importances, associating them with feature names.

**Areas for Improvement (leading to a score of 7 rather than higher):**

1.  **Plotting:** The plotting section is close but not quite right according to the prompt.
    *   **Prompt:** ""For each `species`, create box plots to visualize the distribution of `flipper_length_mm` (using imputed data) **grouped by the imputed `sex` column**.""
    *   **Generated Code:** Creates *separate* box plots for each species, which is okay, but each plot only shows `flipper_length_mm` vs `sex`. It *should* show `flipper_length_mm` grouped by `sex` *within* each species, which is typically done by using `hue='sex'` or by plotting `x='species'` and `hue='sex'` on a single large plot, as the ground truth demonstrates for better comparison. The generated code's approach generates three separate plots, each showing the distribution for a single species across sexes, which is a valid interpretation but less efficient and comparative than a single faceted plot or a single plot with `hue`. The ground truth gives a more concise and comparative visualization by having `species` on the x-axis and `sex` as a `hue`.
    *   **Missing Detail:** The prompt asks for titles on the plots, which the generated code provides, but the overall visualization approach is a bit less effective for direct comparison between species for the `sex` grouping.

2.  **Code Structure/Robustness:**
    *   The missing value imputation is done without explicitly checking `if df[col].isnull().any()`, which means `fillna` might be called unnecessarily if there are no NaNs in a column, though this doesn't cause errors.
    *   The `X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']]` line creates a view/copy issue when `pd.get_dummies` is called subsequently. It's generally safer to ensure operations are performed on explicit copies or by concatenating processed parts as the ground truth does. While it works here, it can lead to `SettingWithCopyWarning` in more complex scenarios.

3.  **No `sns.set_theme`**: While not explicitly asked for, setting a consistent theme (as the ground truth does) is good practice for visualization.

Overall, it's a very good attempt that almost perfectly fulfills core requirements. The plotting section needs a slight refinement to perfectly match the requested grouping visualization style, and some very minor structural improvements could enhance robustness.",gpt-3.5-turbo,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"import pandas as pd
from sklearn.impute import KNNImputer

# Load the dataset
df = pd.read_csv('penguins.csv')

# Impute NA values in numerical columns using K-Nearest Neighbors
imputer = KNNImputer(n_neighbors=3)
num_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df[num_cols] = imputer.fit_transform(df[num_cols])

# Impute NA values in the 'sex' column with the mode
df['sex'].fillna(df['sex'].mode()[0], inplace=True)

# Print the number of remaining NA values...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, performs KNN imputation for the specified numerical columns using `n_neighbors=3`, and imputes the 'sex' column with its mode. Finally, it prints the sum of NA values to verify the imputation.

Strong points:
- Correctly uses `KNNImputer` with the specified `n_neighbors`.
- Correctly identifies and uses the mode for categorical imputation.
- Provides the requested verification output.

Minor improvements could include:
- Adding a `try-except` block for `FileNotFoundError` when loading the CSV, similar to the ground truth, for robustness.
- Adding comments to explain the different steps, though the code is fairly self-explanatory for this problem.
- Initial and final print statements to delineate the process, but this is a stylistic choice.",gpt-3.5-turbo,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the dataset to include only 'Adelie' and 'Gentoo' species
df_filtered = df[df['species'].isin(['Adelie', 'Gentoo'])]

# Handle NA values in the culmen_length_mm column
df_filtered = df_filtered.dropna(subset=['culmen_length_mm'])

# Perform an independent two-sample t-test
adelie_lengths = df_filtered[df_filtered['species'] == 'Adelie']['culmen_length_mm']
gentoo_lengths = df_filt...",8,"The generated code correctly performs all the requested tasks: loading data, filtering species, handling NAs, performing the t-test, and printing the results with interpretation. It's concise and achieves the objective.

However, there are a few areas for improvement:
1.  **Error Handling (File Not Found):** The generated code doesn't include any `try-except` block for `pd.read_csv`, which means it would crash if `penguins.csv` isn't found. The ground truth includes this, making it more robust.
2.  **Modularity/Reusability:** The ground truth encapsulates the logic in a function (`perform_penguin_culmen_ttest`), which makes the code more organized, reusable, and testable. The generated code is a script, which is fine for a simple task, but less ideal for larger projects.
3.  **Readability/Clarity:** While functional, adding some print statements during the process (like loading confirmation, filtering success, NA handling count) as done in the ground truth aids in debugging and understanding the flow for the user.
4.  **Implicit Assumptions (equal_var):** `scipy.stats.ttest_ind` by default assumes `equal_var=True`. While this is usually acceptable as a default, explicitly stating it or acknowledging it (as the ground truth does in its comments) can improve clarity and intentionality.
5.  **Output Formatting:** The ground truth has more structured and descriptive print statements for the results and conclusion, making the output more user-friendly and professional.

Overall, it's a very good attempt that meets the core requirements, but lacks some of the robustness, modularity, and user-friendliness of the ground truth.",gpt-3.5-turbo,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer

# Load the dataset
data = pd.read_csv('penguins.csv')

# Define features and target
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
target ...",7,"The generated code correctly addresses most of the requirements but has some areas for improvement regarding robustness and best practices.

**Strengths:**
*   **Correct Data Loading and Feature Definition:** Loads the CSV and correctly identifies features and target.
*   **Correct Imputation Strategies:** Uses median for numerical features and mode for 'sex'.
*   **Correct One-Hot Encoding:** Applies `OneHotEncoder` to 'island' and 'sex' and integrates it back into the DataFrame.
*   **Correct Label Encoding for Target:** `LabelEncoder` is used for the `species` target.
*   **Correct Train-Test Split:** Uses `train_test_split` with the correct `test_size`, `random_state`, and `stratify`.
*   **Correct Model Training and Evaluation:** `RandomForestClassifier` is trained and evaluated using `accuracy_score` and `classification_report`.

**Areas for Improvement (leading to a score of 7):**

1.  **Chained Assignments/In-place Modification:** The numerical imputation: `data[['culmen_length_mm', ...]] = num_imputer.fit_transform(...)` is prone to `SettingWithCopyWarning` if `data` was a slice. While `data = pd.read_csv(...)` creates a fresh DataFrame, using `pd.DataFrame(num_imputer.fit_transform(...), columns=...)` and then `pd.concat` or a `ColumnTransformer`/`Pipeline` approach is generally safer and more explicit for transforming subsets of the DataFrame.
2.  **One-Hot Encoding Structure:** The `OneHotEncoder` with `drop='first'` is a reasonable choice, but `ohe.get_feature_names(['island', 'sex'])` is deprecated. The recommended way is `ohe.get_feature_names_out()`. Additionally, directly manipulating DataFrames by dropping and concatenating can become complex and error-prone with many features.
3.  **Missing Features from `X` after Preprocessing:** The code `X = data.drop('species', axis=1)` after OHE will drop the original `island` and `sex` columns, but the one-hot encoded columns are now part of `data`. This is technically correct for the final `X`, but the initial `features` list was not used for `X` in a robust way after all the in-place modifications. A `ColumnTransformer` approach handles this more cleanly by keeping track of which features are being processed and outputted.
4.  **No Pipeline for Preprocessing and Model:** The ground truth code uses `ColumnTransformer` and `Pipeline` which are best practices for streamlining preprocessing steps with the model training. This makes the code more robust, prevents data leakage (fitting imputers on training data only), and is easier to maintain and deploy. The generated code performs all preprocessing *before* the train-test split on the entire dataset, which is a form of data leakage for the `sex` Imputer (fitting its mode on the entire dataset, including test data) and potentially for the numerical Imputer.
    *   **Data Leakage for Imputation:** The `fit_transform` for `num_imputer` and determining `sex_mode` happens on the *entire* `data` DataFrame, meaning information from the test set is used to impute missing values in the training set. This can artificially inflate performance metrics. The correct approach (as in the ground truth) is to fit imputers *only* on the training data.
5.  **Robustness for `species` Encoding:** The `if data['species'].dtype != 'int64':` check is okay, but `LabelEncoder` expects categorical data, not necessarily integer data. Applying it directly is generally safe and more explicit.
6.  **Readability/Clarity:** While functional, the manual chaining of transformations for `island` and `sex` then dropping and concatenating is less readable and scalable than a `ColumnTransformer` within a `Pipeline`.

To get a higher score, the code should ideally adopt a `ColumnTransformer` and `Pipeline` based approach to ensure no data leakage during preprocessing and better overall code structure, as demonstrated in the ground truth.",gpt-3.5-turbo,LLM_judge_config
H49,hard,Statistical Analysis,,"import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

correlation_years_exp = df[""PerformanceScore""].corr(df[""YearsExper...",9,"The generated code correctly defines the DataFrame, calculates the Pearson correlation coefficients, and prints them. It also correctly identifies the feature with the stronger absolute correlation using `abs()`.

**Strong Points:**
- Correct DataFrame creation.
- Accurate calculation of correlation coefficients using `df.corr()`.
- Correct use of `abs()` to compare the strength of correlations.
- Prints all required outputs.

**Area for minor improvement:**
- The `else` clause for comparing the stronger correlation could be slightly more robust to handle the edge case where the absolute correlations are exactly equal. The current `else` implies `TrainingHours` is stronger when they could be equal. While statistically unlikely with floating-point numbers, it's good practice to account for it, as the ground truth does. However, for this specific problem, it's a minor point.
- The default method for `corr()` is Pearson, so `method='pearson'` isn't strictly necessary but is good for explicitness if not mentioned in the problem statement. Given the problem explicitly asks for Pearson, adding it would make the code more robust against future `pandas` changes that might alter the default or if the reader is unfamiliar with the default. This is a very minor detail.
- Formatting the printed correlation coefficients to a specific number of decimal places (e.g., `{:.4f}`) would make the output cleaner and more professional, similar to the ground truth. This is a stylistic point, but enhances readability.",gpt-3.5-turbo,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
from lifelines import KaplanMeierFitter, CoxPHFitter
from statsmodels.stats.multitest import multipletests
from statsmodels.formula.api import ols
import pymc3 as pm
import numpy as np
import matplotlib.pyplot as plt

# Load titanic dataset
data = pd.read_csv('titanic.csv')

# Kaplan-Meier survival analysis
kmf = KaplanMeierFitter()
kmf.fit(durations=data['Voyage'], event_observed=data['Survived'])
kmf.plot_survival_function()

# Cox proportional hazards regression
...",1,"The generated code is severely incomplete and incorrect.

Here's a detailed breakdown:

**Correctness:**
*   **Kaplan-Meier:** It tries to use `data['Voyage']` which does not exist in `titanic.csv`. The prompt specified treating ""voyage as time-to-event data (with survival as event indicator)"". The ground truth explicitly interprets this by using a fixed time `T=1` and `E=1-Survived` (death as event). The generated code assumes a nonexistent 'Voyage' column.
*   **Cox Proportional Hazards:** Similar to KM, it tries to use a non-existent 'Voyage' column as `duration_col`. It directly passes the entire `data` DataFrame, which is problematic as `lifelines` expects numerical features for the covariates, and the dataset has categorical columns like 'Sex', 'Embarked', and strings like 'Name', 'Ticket', 'Cabin'. It also doesn't handle missing values. The default `Survived` for `event_col` is correct, but applying `CoxPHFitter` directly to raw, un-preprocessed data is wrong.
*   **Multivariate Analysis:** It uses `statsmodels.formula.api.ols` for `Survived ~ Age + Fare + PassengerClass`. `Survived` is a binary outcome, making OLS (ordinary least squares, typically for linear regression) inappropriate. Logistic regression or other binary classification models would be more suitable. `PassengerClass` also does not exist; it should be `Pclass`. It also doesn't perform ""advanced statistical tests"" like interaction effects or model diagnostics beyond what's built into `lifelines`.
*   **Propensity Score Matching, Bootstrap Resampling, Bayesian Analysis, Sensitivity Analysis:** All these sections are marked ""TO DO"" or entirely missing their implementation. This is a critical failure given the prompt's requirements.
*   **Model Diagnostics:** `cph.check_assumptions(data)` will fail for the same reasons as the `cph.fit` call: un-preprocessed data and incorrect duration column.

**Code Quality:**
*   **Imports:** Some necessary imports like `sklearn.impute`, `sklearn.preprocessing`, `scipy.stats`, `arviz` are missing. `statsmodels.stats.multitest` and `ols` are imported but used incorrectly or unnecessarily.
*   **Structure:** While it attempts to structure according to the task requirements, the actual implementation within each section is absent or flawed.
*   **Preprocessing:** Completely ignores data preprocessing steps such as handling missing values (Age, Embarked), encoding categorical variables (Sex, Embarked, Pclass), and dropping irrelevant columns. This is fundamental for any statistical modeling.

**Completeness:**
*   **Kaplan-Meier:** Only the base KM plot is attempted; no grouping by categories (Sex, Pclass) is done as asked for by the spirit of ""advanced statistical modeling"".
*   **Cox PH:** No interaction effects, no proper model diagnostics beyond a failed assumption check.
*   **Advanced Statistical Tests:** The `ols` is inadequate. No multivariate analysis in the sense of testing multiple factors simultaneously in the same model (beyond the basic linear model), no interaction effects tested as part of the Cox PH, no proper model diagnostics were performed.
*   **Propensity Score Matching:** ""TO DO"".
*   **Bootstrap Resampling:** ""TO DO"".
*   **Bayesian Analysis:** ""TO DO"".
*   **Model Validation/Sensitivity Analysis:** ""TO DO"".

In summary, the generated code demonstrates a very superficial understanding of the requirements. It fails to implement almost all key components of the prompt, particularly the advanced statistical methods, Bayesian modeling, and comprehensive validation. The basic data interpretation and preprocessing are also incorrect or missing, leading to the code being largely non-functional.",gpt-3.5-turbo,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'], 
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'], 
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77], 
        'Final_Score': [90, 88, 82, 92, 80, 91,...",9,"The generated code is highly correct and addresses almost all requirements of the task.

**Strong Points:**
- **Correct DataFrame Creation:** The DataFrame is created accurately as specified.
- **Correct Correlation Calculation:** The Pearson correlation coefficient is calculated and printed correctly.
- **Accurate Scatter Plot:** The scatter plot correctly uses 'Attendance_Rate' on the x-axis and 'Final_Score' on the y-axis, colors points by 'Subject', includes a legend, a title, and clear axis labels. Using `seaborn.scatterplot` with `hue='Subject'` is an excellent way to achieve the coloring and automatic legend.
- **Correct Grade Assignment:** The `Overall_Grade` column is correctly added based on the specified conditions using an `apply` function, which is a perfectly valid and readable approach.
- **Correct Grade Distribution:** The count of students in each grade category is correctly calculated and printed using `value_counts()`.

**Areas for Minor Improvement:**
- **Clutter/Output:** The ground truth code includes additional `print` statements (e.g., ""--- Creating DataFrame ---"") and `df.head()` calls which were not explicitly requested but serve to make the output more verbose and easier to follow, especially in a benchmark setting. The generated code is more concise in its output.
- **`np.select` vs. `apply` for Grading:** While using `.apply()` with a function for grading is perfectly correct, `np.select` (as used in the ground truth) can sometimes be more performant for large DataFrames with many conditions. However, for this dataset size, the difference is negligible, and `apply` is very readable. This is a stylistic choice and not a correctness issue.
- **Plot Enhancements (Minor):** The ground truth included minor plot enhancements like `s=100`, `alpha=0.8`, `edgecolor='w'` for the scatter plot, and a `grid` and `tight_layout()`. These are aesthetic improvements not explicitly requested but often good practice. The generated code's plot is perfectly functional without these.
- **Import Redundancy:** The ground truth imported `numpy` which was used for `np.select`. The generated code does not import `numpy` as it uses `apply` for grading. This is not an error but a consequence of the chosen implementation.

Overall, the generated code successfully completes the task as described. The minor differences are stylistic or minor aesthetic additions that do not detract from the core functionality or correctness.",gpt-3.5-turbo,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']), 
        'Ticker': ['TECHCO']*10, 
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0], 
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0], 
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5,...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong Points:**
- **Correct DataFrame Creation and Indexing:** The DataFrame is created correctly, and 'Date' is set as the index as requested.
- **Accurate Daily Return Calculation:** The `Daily_Return` is calculated using the specified formula `(Close_t / Close_t-1 - 1) * 100`.
- **Correct SMA Calculation:** Both the 3-day and 5-day Simple Moving Averages are calculated correctly using `rolling().mean()`.
- **Proper Plot Generation:** The plot correctly displays the 'Close' price, 'SMA_3Day', and 'SMA_5Day' on the same chart.
- **Appropriate Plot Labels and Title:** The plot has the correct title, x-axis label, y-axis label, and a legend.
- **Correct Bullish Crossover Logic:** The identification of bullish crossovers (`(df['SMA_3Day'] > df['SMA_5Day']) & (df['SMA_3Day'].shift(1) < df['SMA_5Day'].shift(1))`) is correct. This successfully covers the condition where a crossover from below to above occurs. It correctly identifies the date '2024-01-08' as a bullish signal based on the provided data.

**Areas for Minor Improvement (for a perfect 10):**
- **Plot Readability/Enhancements:** While the plot fulfills the requirements, the ground truth code includes `plt.figure(figsize=(12, 7))`, `linewidth` for lines, `linestyle` for SMAs, `plt.grid(True)`, and `plt.tight_layout()`. These are good practices for professional-looking plots and improve readability. The generated code lacks these small enhancements.
- **Bullish Signal Output Formatting:** The bullish signal output `print(bullish_signals.index)` is functional but less user-friendly than iterating and printing each date in a formatted string, as done in the ground truth (`date.strftime('%Y-%m-%d')`). Also, the ground truth includes a message for when no crossovers are found.

Overall, an excellent solution that provides all the core functionalities accurately. The minor points are aesthetic and best practice suggestions rather than critical errors.",gpt-3.5-turbo,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

data = {'OrderID': [101, 102, 103, 104, 105, 106, 107], 
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 
        'Quantity': [1, 2, 1, 1, 3, 1, 1], 
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150], 
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'C...",9,"The generated code is highly correct and completes almost all aspects of the task effectively.

**Strong Points:**
- **Correctness:** All calculations (total sales revenue per product, average order value, identifying repeat customers) are correct.
- **DataFrame Creation:** The DataFrame is created correctly.
- **Visualization:** A horizontal bar chart is correctly generated and displayed, with appropriate labels and title.
- **Completeness:** All the core requirements for calculations and visualization are met.
- **Conciseness:** The code is quite concise while remaining readable.

**Areas for Minor Improvement (Why not a 10):**
- **Plotting Enhancements:** The `figsize` and `grid` additions in the ground truth make the plot more readable and polished. Also, `plt.tight_layout()` is good practice to prevent labels from overlapping. While the generated code's plot is functional, these minor additions improve user experience.
- **Output Formatting:** The ground truth provides more descriptive print statements for the average order value and repeat customers, making the output clearer and more user-friendly. For example, `f""Average Order Value: ${average_order_value:,.2f}""` is more professional.
- **Handling Empty Repeat Customers:** The ground truth explicitly checks if `repeat_customers` is empty and prints a message, which is good practice for robustness. The generated code would just print an empty Series if no customers made multiple purchases, which is less informative.
- **Import Statements:** While `import pandas as pd` and `import matplotlib.pyplot as plt` are present, they are not explicitly mentioned as ""necessary imports"" in the prompt, but it's understood. The code correctly includes them.",gpt-3.5-turbo,LLM_judge_config
H53,hard,Feature Engineering,,"import pandas as pd

# Define the initial DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 1) Create a new numerical feature ""DeviceType_Numeric""
df['DeviceType_Numeric'] = df['DeviceType'].map({...",10,"The generated code successfully completes all aspects of the task.

1.  **DataFrame Definition:** The initial DataFrame is correctly defined.
2.  **`DeviceType_Numeric`:** The feature is created accurately using the specified mapping.
3.  **`PagesPerMinute`:** This feature is calculated correctly. Crucially, the code handles division by zero by replacing `inf` and `-inf` with `NaN` and then filling all `NaN` values with `0`, which is the correct and concise way to implement this requirement. It correctly uses `pd.np.inf` and `pd.np.nan`.
4.  **Feature Selection:** The correct columns are selected for the final feature DataFrame.
5.  **Output:** The head of the resulting feature DataFrame is printed as requested.

The code is clean, efficient, and directly addresses all requirements without unnecessary complexity.",gpt-3.5-turbo,LLM_judge_config
H54,hard,Anomaly Detection,,"import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26, 45, 28]
}
df = pd.DataFrame(data)

mean_temp...",9,"The generated code correctly defines the DataFrame, calculates the mean and standard deviation, identifies outliers based on the 1.5 standard deviation threshold, and prints the required information for each outlier. It directly addresses all parts of the task.

Strengths:
- Correct DataFrame creation.
- Accurate calculation of mean and standard deviation.
- Correct application of the 1.5 standard deviation rule for outlier detection.
- Correctly iterates through outliers and prints the specified details.

Areas for minor improvement (not critical for this task, hence a high score):
- The `Timestamp` column is not converted to datetime objects. While not strictly necessary for this specific outlier detection logic, it's generally good practice when dealing with time-series data in pandas and helps in more robust handling (e.g., specific time formatting for output).
- The output format for printing outliers is functional but could be slightly more readable (e.g., using f-strings for better formatting, or adding introductory/concluding print statements as in the ground truth for better user experience).
- No message is printed if no outliers are found (though in this specific dataset, outliers exist).",gpt-3.5-turbo,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"import pandas as pd
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [10, 5, 25, 15, 8, 30, 9, 20, 6]
}
d...",6,"The generated code correctly defines the DataFrame and calculates the `EngagementScore`. This fulfills the fundamental requirements.

However, there's a significant logical error in determining the `PostType` with the highest average `EngagementScore` per platform.

The line `platform_posttype_highest_avg = df.groupby(['Platform', 'PostType'])['EngagementScore'].mean().idxmax(level=0)` does not yield the intended result.
`idxmax(level=0)` on a grouped Series with a MultiIndex (`Platform`, `PostType`) returns the *index* (Platform, PostType tuple) where the *maximum value across all groups corresponding to each top-level index* (Platform) is found. This means it will find the PostType that gave the highest average for each platform, but the subsequent printing logic attempts to recalculate the mean, which is inefficient and in some cases, incorrect as it doesn't correctly retrieve the *calculated* maximum average from `idxmax`.

Let's break down where it goes wrong in the printing loop:
`platform, posttype = platform_posttype_highest_avg.items()` will unpack `platform` as the platform name and `posttype` as the (Platform, PostType) tuple result from `idxmax`.
So, `posttype[1]` correctly gets the `PostType`.
However, `df.loc[(df['Platform'] == platform) & (df['PostType'] == posttype[1]), 'EngagementScore'].mean()` then recalculates the mean engagement score for that specific platform and post type from the original `df`. While this might give the correct *value* in this specific example (because the data used for `idxmax` is the same as the data used for recalculation), it's redundant and inefficient. The value of `idxmax` only tells you *where* the maximum is, not the maximum value itself.

To properly retrieve the highest average engagement score and the associated PostType for each platform, the proper approach involves `groupby`, `mean`, and then either `sort_values` followed by `drop_duplicates` (as in the ground truth) or using `idxmax` on the grouped result and then *looking up* the value from the results of the `mean()` operation.

The code's structure and variable naming are reasonable for the parts that work. The main issue is the incorrect statistical aggregation logic.

To improve, the logic for finding the max average engagement score per platform should be revisited to correctly store not just the best `PostType` but also its `EngagementScore` from the aggregation step, rather than recalculating. The ground truth demonstrates a robust way to achieve this.",gpt-3.5-turbo,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu"", ""Injury"", ""Flu""],
  ...",9,"The generated code correctly performs all the required tasks. It accurately defines the DataFrame, calculates ""LengthOfStay"", computes and prints the average length of stay and patient count per diagnosis, and correctly identifies the diagnosis with the longest average stay.

The code quality is good, using appropriate pandas functions for grouping and aggregation. The variable naming is clear.

One minor point for improvement, which keeps it from a perfect 10, is in the final print statement for the longest average `LengthOfStay`. While functionally correct, `diagnosis_info[diagnosis_info[""Average LengthOfStay""] == diagnosis_info[""Average LengthOfStay""].max()].index[0]` is a bit less readable and explicit than using `idxmax()` directly on the `Average LengthOfStay` series (after renaming the column in `diagnosis_info`) or using `.loc[diagnosis_info['Average LengthOfStay'].idxmax()]` as in the ground truth. Also, the ground truth code formats the output more nicely for the final answer, showing the diagnosis and its average length of stay, whereas the generated code only prints the diagnosis name. However, given the prompt asked to ""Identify and print the 'Diagnosis' that has the longest average 'LengthOfStay'"", simply printing the name is acceptable. The lack of a function wrapper is also a slight difference but not a critical flaw for this specific task.",gpt-3.5-turbo,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
# Calculate WeightedRating for each movie
total_votes = df[""Votes""].sum()
df[""WeightedRating""] = (df[""Rating""] * df[""Votes""]) / total_votes

# Find the movie with the highest WeightedRating
max_weighted_rating = df[""WeightedRating""].max()
best_movie = df[df[""WeightedRating""] == max_weighted_rating][[""Title"", ""WeightedRating""]]

# Print the Title and WeightedRating of the movie with the highest WeightedRating
print(best_movie)
```",7,"The generated code correctly performs the calculation of `WeightedRating` and adds it as a new column. It also correctly identifies the movie with the highest `WeightedRating`.

However, the generated code is incomplete because it omits the crucial first step of defining the DataFrame, which was explicitly part of the problem description and provided in the prompt's initial setup. The prompt clearly stated: ""Define the following pandas DataFrame representing movie ratings...After creating the DataFrame, the code should..."". The generated code starts directly with calculations, assuming the `df` DataFrame is already defined, hence making it non-executable as a standalone solution for the entire task.

Additionally, while `print(best_movie)` does output the requested information, the Ground Truth's formatted print statement `print(f""Movie with the highest WeightedRating:"") print(f""Title: {title_highest}"") print(f""WeightedRating: {weighted_rating_highest:.6f}"")` provides a more user-friendly and explicit output, which is generally preferred for clarity in such tasks. The generated code also does not handle the edge case of `total_votes` being zero, though this particular dataset would not trigger that error.",gpt-3.5-turbo,LLM_judge_config
H58,hard,Data Aggregation,,"import pandas as pd
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 10000, 8000]
}
df = pd.DataFrame(...",9,"The generated code correctly defines the DataFrame, calculates the average price and mileage per brand, and prints the resulting grouped DataFrame, addressing all requirements of the task. The imports are correct, and the structure is clean and efficient.

One minor point for improvement, compared to the ground truth, is the naming of the aggregated columns. The ground truth uses named aggregations (`Average_PriceUSD=(""PriceUSD"", ""mean"")`, `Average_Mileage=(""Mileage"", ""mean"")`) which results in more descriptive column names in the output DataFrame (`Average_PriceUSD`, `Average_Mileage`). The generated code outputs columns named simply `PriceUSD` and `Mileage`, which, while technically correct for the mean, might be less explicit in a larger context where original column names could be confused with aggregated values. This is a very minor stylistic preference and does not affect the correctness of the output.",gpt-3.5-turbo,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"import pandas as pd
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63, 83],
    ""Defense"": [49...",10,"The generated code is excellent. It correctly defines the DataFrame, adds the 'TotalStats' column using a concise and idiomatic pandas approach (`.sum(axis=1)`), correctly identifies and prints the Pokemon with the highest 'TotalStats', and accurately calculates and prints the average 'Speed' for each 'Type1' using `groupby()`. The code is clean, readable, and directly addresses all requirements of the task.",gpt-3.5-turbo,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from umap import UMAP
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load data...",0,"The generated code is almost entirely empty. It provides only imports and a skeleton of comments, without any executable logic for feature engineering, preprocessing, clustering, dimensionality reduction, or analysis. It fails to address any of the 8 core requirements of the task. Therefore, it cannot be evaluated for correctness, completeness, or even basic code quality beyond the presence of correct import statements. This is essentially a blank slate.",gpt-3.5-turbo,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dystopian""],
    ""Stock"": [12, 8, 15,...",9,"The generated code correctly performs all the requested tasks: creating the DataFrame, calculating book counts and average prices per author, printing this information, and identifying the author with the most books in stock.

**Strong Points:**
- Correctly initializes the DataFrame.
- Uses appropriate pandas functions (`value_counts`, `groupby`, `mean`, `sum`, `idxmax`) to achieve the required aggregations efficiently.
- The logic for calculating per-author statistics and identifying the author with the most stock is sound and directly answers the prompt.
- The output format for author statistics is clear, although the ground truth provides slightly more robust formatting (e.g., controlling decimal places, alignment).

**Areas for Minor Improvement:**
- **Combining Aggregations:** For the first part (count and average price), the generated code uses `value_counts()` for author counts and `groupby().mean()` for average price, then iterates to print. The ground truth demonstrates a more idiomatic pandas approach using a single `groupby().agg()` call. This is generally more efficient and concise when performing multiple aggregations on the same group.
- **Output Formatting:** While functional, the printing of author statistics could benefit from more refined formatting (e.g., using `:.2f` for price and controlling field width as in the ground truth) for better readability.
- **Task Numbering:** The comments `# 1)` and `# 2)` in the generated code are slightly off when compared to the problem description. The problem stated ""1) For each 'Author', count... and calculate... 2) Print this information... 3) Identify..."". The generated code combined 1 and 2, then did 3, but reversed the comment numbering. This is a minor stylistic point.

Overall, it's a very good solution that achieves all objectives. The suggested improvements are mainly about stylistic choices and leveraging pandas' `agg()` for conciseness, rather than correctness issues.",gpt-3.5-turbo,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer
f...",4,"The generated code attempts to address several aspects of the prompt but falls significantly short in implementation details, correctness, and completeness.

**Primary Reasons for Failure/Critical Missing or Incorrect Aspects:**

1.  **Data Preprocessing:**
    *   `data.dropna(inplace=True)` at the beginning of preprocessing is a critical flaw. The prompt explicitly requires ""multiple imputation strategies,"" and `dropna` completely negates this. It also significantly reduces the dataset size, leading to biased results.
    *   `SimpleImputer` is used, but the prompt asked for ""multiple imputation strategies,"" implying more advanced methods like MICE (IterativeImputer).
    *   Outlier detection/treatment and advanced feature engineering (polynomial and interaction features) are poorly handled. `PolynomialFeatures(interaction_only=True)` is used, which is only a subset of what was asked (`PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)`). The ground truth implemented a custom `OutlierCapper`.
    *   Missing `train_test_split`. The entire optimization and evaluation are done on the *full* dataset (`X_preprocessed`, `y`), which is a fundamental error leading to data leakage and overly optimistic performance estimates.

2.  **Model Optimization:**
    *   `GridSearchCV` is used, but the models are fit on `X_preprocessed` and `y` directly without a train-test split, which is incorrect.
    *   SVM's `probability=True` is missing, which is essential for `predict_proba` used in ROC AUC, Brier Score, and calibration curves.
    *   Neural Network (`MLPClassifier`) should have `max_iter` increased to ensure convergence, which is often an issue.

3.  **Ensemble Models:**
    *   Blending is completely missing.
    *   Voting classifier is initialized with `voting='soft'` but `SVC` by default doesn't support `predict_proba` unless `probability=True` is set during initialization, leading to potential crashes or incorrect behavior.

4.  **Advanced Evaluation:**
    *   **Stratified K-Fold Cross-Validation:** Only used within `GridSearchCV` parameter. The subsequent ""Evaluate models"" section misleadingly uses `skf.split` on `X_preprocessed` and `y` but then directly fits models and calculates `accuracy_score` and `roc_auc_score` *without* proper evaluation against `y_test`. The ""learning curves"" and ""validation curves"" in the generated code are just lists of aggregate scores per fold, not actual learning/validation curves as defined by `sklearn.model_selection.learning_curve` and `validation_curve`. This is a critical misunderstanding of these concepts.
    *   **Learning Curves and Validation Curves:** Not implemented using sklearn's `learning_curve` and `validation_curve` functions, which are crucial for diagnostics. The plotting for these is also missing.
    *   **Bootstrap Sampling:** Completely missing. Confident intervals on metrics are not calculated.
    *   **Evaluation Metrics:** Only accuracy and ROC AUC are explicitly calculated for the final dashboard. Metrics like Precision, Recall, F1-Score, Brier Score are missing.

5.  **Model Interpretability (SHAP/LIME):**
    *   Only SHAP is attempted, without conditional check for LIME.
    *   `shap_values = explainer.shap_values(X_preprocessed)` is potentially problematic as SHAP values should ideally be calculated on the test set or a representative sample, not the entire preprocessed data used for training.
    *   No SHAP dependence plot is generated.

6.  **Statistical Significance Testing:**
    *   The `ttest_rel` is performed on `learning_curves['Random Forest']` and `learning_curves['Gradient Boosting']`, which are just lists of evaluation scores from the manual cross-validation loops, not properly collected CV scores (e.g., from `GridSearchCV.cv_results_`). The prompt asked for testing *between models*, implying a shared set of folds for comparison, which isn't robustly handled here.

7.  **Model Comparison Dashboard:**
    *   Very rudimentary. Doesn't include feature importance, confidence intervals, or performance metrics in a structured dataframe for dashboarding. Plots beyond a basic calibration plot and shap summary are missing (e.g., bar plots of metrics, ROC curves for all models, confusion matrices).

8.  **Model Calibration:**
    *   The calibration plot is attempted, but it uses the full `X_preprocessed` and `y` for calibration curve calculation (due to missing train/test split and fitting models on entire data), which is incorrect.
    *   `CalibratedClassifierCV` is not used.
    *   Brier Score calculation for calibrated vs. uncalibrated models is missing.

9.  **Code Quality/Structure:**
    *   Many imports are missing (e.g., `train_test_split`, `ColumnTransformer`, `CalibratedClassifierCV`, `brier_score_loss`, `calibration_curve`, `resample`).
    *   Lack of proper comments, print statements, and structuring into functions makes the code less readable and maintainable compared to the prompt's implied standard.
    *   Warnings are not suppressed.
    *   Redundant model fitting: models are fitted for GridSearchCV, then fitted again within the manual stratified K-Fold loop.",gpt-3.5-turbo,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.impute import IterativeImputer, KNNImputer
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.linear_model import LogisticRegression
from statsmodels.genmod.generalized_linear_model import GLM
from statsmodels.genmod.families import Binomial
import pymc3 as pm
from scipy.stats import multivariate_normal, f_oneway
from sklearn.utils import resam...",4,"The generated code attempts to address the task but falls significantly short in terms of correctness, completeness, and adherence to advanced statistical practices.

**Critical Issues:**

1.  **Missing Data Imputation Comparison:** While three techniques are mentioned in the prompt, only `IterativeImputer` and `KNNImputer` are used. Comparison of their **effectiveness** is explicitly required but completely absent. The code just creates two imputed dataframes and then silently proceeds with only one (`imputed_data_mice`) for subsequent steps, making the `KNNImputer` usage pointless.
2.  **Multivariate Models - Logistic Regression:** The `LogisticRegression` is fit using `imputed_data_mice` (which is the full DataFrame, including 'Potability' as a feature, which is incorrect for predicting it). The target variable `data[""Potability""]` is used directly from the original `data` which likely still has missing values, even if the imputed dataframes were created. This is a fundamental flaw in model training. No regularization is applied as requested.
3.  **Multivariate Models - GAM:** The prompt asks for GAM (Generalized Additive Models) using `pygam`. The generated code uses `statsmodels.genmod.generalized_linear_model.GLM` with a `Binomial` family, which is a Generalized Linear Model, *not* a Generalized Additive Model. This is a complete misunderstanding of the request. Furthermore, `imputed_data_mice` again includes the target variable.
4.  **Multivariate Models - Bayesian Logistic Regression:** The PyMC3 model has several fundamental errors.
    *   `imputed_data_mice` is passed directly as the `X` matrix, again including the target variable.
    *   The `shape` argument for `beta` in `pm.Normal` should be `imputed_data_mice.shape[1]` (total number of columns), but `imputed_data_mice` here represents the entire dataset. This is incorrect. It should be `X_train.shape[1]`.
    *   No `pm.Data` is used, making it impossible to perform predictions on a separate test set.
    *   The code directly uses `pm.sample()` without specifying chains, cores, or draws, and it does not show any posterior predictive checks or evaluation metrics (like ROC AUC which was requested). It also doesn't perform inference on the posterior.
5.  **Hypothesis Testing - Multivariate ANOVA:** The prompt specifically asks for `Multivariate ANOVA (MANOVA)`. The code provides `f_oneway`, which is a univariate ANOVA. This is a critical failure.
6.  **Hypothesis Testing - Permutation Tests:** The permutation test logic is completely flawed. `f_oneway(*[resample(data[col]...` makes no sense for a permutation test of mean differences between two groups. It appears to be trying to permute *columns* and then run ANOVA on them, which is not what a permutation test for comparing two group means looks like.
7.  **Hypothesis Testing - Bootstrap Confidence Intervals:** This is completely missing.
8.  **Robust Statistical Methods:** The request was to ""Apply robust statistical methods to handle outliers and non-normal distributions"" and specifically the ground truth demonstrated `RLM`. The generated code only has `qqplot(data[""Potability""], line='s')`, which is a diagnostic plot, not a robust statistical method.
9.  **Model Diagnostics:** The prompt explicitly asks for ""residual analysis, influence measures, and goodness-of-fit tests"". The generated code only has a `qqplot` (which is a diagnostic plot, not a GOF test for logistic regression), and a comment `# Residual analysis, influence measures, goodness-of-fit tests` indicating that these were not implemented.
10. **Visualizations:** The requested Q-Q plots, leverage plots, and diagnostic dashboards are missing. The `sns.lmplot` and `sns.jointplot` are basic EDA plots, not advanced statistical visualizations for model diagnostics as requested.
11. **Imports:** Many necessary imports (e.g., `statsmodels.api`, `statsmodels.graphics.regressionplots`, `statsmodels.multivariate.manova`, `pygam`, `sklearn.model_selection`, `sklearn.metrics`) are missing or incorrectly referenced.
12. **Data Splitting:** No train-test split is performed for model evaluation, which is standard practice in machine learning and statistical modeling. Models are trained on the entire (imputed) dataset and not validated against unseen data.
13. **Assumptions:** The code implicitly assumes the data structure matches the column names, but directly uses column indices (`imputed_data_mice`) which is fragile and error-prone. The `Potability` column is also implicitly handled by position within the imputed dataframes.

In summary, the code provides basic placeholders for some techniques but fails to correctly implement the advanced statistical modeling, comprehensive diagnostics, and specific hypothesis testing methods requested. The lack of proper data separation, incorrect model application (especially GAM and PyMC3), and fundamental errors in hypothesis test implementations prevent this code from being a functional solution to the detailed task description.",gpt-3.5-turbo,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.manifold import TSNE
from umap import UMAP
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn_extra.cluster import KMedoids
from sklearn.metrics.pairwise import pairwis...",2,"The generated code is severely lacking in correctness, completeness, and adherence to the task requirements.

**Primary Reasons for Failure:**

1.  **Missing Feature Engineering:** The task explicitly asks for ""advanced features including water quality indices, chemical balance ratios, contamination risk scores, and temporal patterns."" The generated code skips this entirely, only performing `StandardScaler` on the raw data. This is a critical omission, as feature engineering is central to the water quality analysis.

2.  **Incomplete Preprocessing:** The code loads the data and then immediately scales it. It completely ignores missing value imputation, which `water_potability.csv` *does* have. This will lead to errors or incorrect results when models are trained on NaN values.

3.  **No Optimal Cluster Determination:** The task requires determining the ""optimal number of clusters using multiple validation techniques (silhouette analysis, gap statistic, elbow method, Davies-Bouldin index, Calinski-Harabasz index)."" The generated code hardcodes `n_clusters=2` for all algorithms and then *calculates* scores for *these fixed 2 clusters*, rather than iterating to find an optimal K. Only Silhouette, Calinski-Harabasz, and Davies-Bouldin are used, and incorrectly so (not for finding optimal K).

4.  **Incorrect Clustering Validation:** The `silhouette_score`, `calinski_harabasz_score`, and `davies_bouldin_score` are calculated by trying to access `model.labels_` for `GaussianMixture` and `SpectralClustering`. `GaussianMixture` does not have a `labels_` attribute directly; it has `predict` or `predict_proba`. `SpectralClustering` also predicts labels differently or requires `fit_predict`. This will likely cause errors. `DBSCAN`'s `labels_` also includes -1 for noise, which needs special handling for these metrics.

5.  **No Cluster Stability Analysis:** The task specifies ""cluster stability analysis using bootstrap resampling and consensus clustering."" This is entirely absent.

6.  **No Water Quality Profile Analysis:** The requirement to ""Analyze water quality profiles for each cluster including chemical composition patterns, contamination risks, and potability rates"" is not met. The code only performs clustering and visualization; it doesn't extract insights per cluster.

7.  **Limited Visualization Dashboard:** Only one 2D UMAP plot is generated. The task asks for a ""comprehensive cluster visualization dashboard with 2D/3D plots, radar charts for cluster profiles, and interactive visualizations."" Radar charts and 3D plots are missing.

8.  **No External Validation/Domain Knowledge:** The task requires ""Validat[ing] clustering results using external criteria and domain knowledge about water quality standards."" This is not addressed.

9.  **No Anomaly Detection:** The task asks for ""cluster-based anomaly detection to identify unusual water samples."" This is entirely absent.

10. **Minor Issues:**
    *   `KMedoids` is imported but never used.
    *   `plotly.graph_objects` is not imported, but `plotly.express` is.
    *   The imports list is very basic and misses many essential libraries that would be needed for a complete solution (e.g., `scipy.spatial.distance`).
    *   The code quality is poor due to the critical omissions; it's a barebones attempt at some sub-parts of the task without integrating them meaningfully.
    *   The `sklearn_extra.cluster` import suggests a non-standard library, which should ideally be avoided unless specifically required or justified, or provide installation instructions.

Overall, the generated code performs basic clustering and dimensionality reduction on raw data, but utterly fails to meet almost all advanced requirements of the prompt, particularly around feature engineering, validation, stability, profiling, and anomaly detection.",gpt-3.5-turbo,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
from sklearn.linear_model import LinearRegression

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Calculate correlations with 'quality'
correlations = data.corr()['quality'].drop('quality').sort_values()

# Top 3 positively correlated features
top_pos_corr = correlations.tail(3)

# Top 3 negatively correlated features
top_neg_corr = correlations.head(3)

# Correlation matri...",7,"The generated code correctly addresses most of the task requirements, including loading data, calculating correlations, identifying top positive and negative correlations, creating a heatmap, and generating scatter plots with regression lines.

**Strengths:**
*   Uses appropriate libraries (pandas, matplotlib, seaborn, sklearn).
*   Correctly calculates correlations with 'quality' and drops 'quality' itself from the series.
*   Correctly identifies the top 3 positive and negative correlations (though `tail(3)` for positive and `head(3)` for negative works because `sort_values()` by default sorts ascendingly).
*   The heatmap generation is correct.
*   The scatter plot generation with `LinearRegression` from `sklearn` is a valid approach.

**Areas for Improvement:**
*   **Missing Output for Top Correlated Features:** The task explicitly asks to ""List the top 3 positively correlated and top 3 negatively correlated features with their correlation coefficients."" The generated code calculates these (`top_pos_corr`, `top_neg_corr`) but does not print them to the console, which is a key part of the deliverable.
*   **Scatter Plot Regression Line Details:** While the regression line is plotted, it lacks common additions like R-squared value in the legend or title, which was present in the ground truth and is generally good practice for communicating the fit. It also uses `pearsonr` in imports but `LinearRegression` for plotting, which isn't an issue but `scipy.stats.linregress` simplifies getting all relevant stats (slope, intercept, r-value, p-value) in one go for regression analysis. However, `sklearn.linear_model.LinearRegression` is perfectly fine here.
*   **Clarity in Scatter Plot Selection:** The `top_pos_corr.index[-1]` and `top_neg_corr.index[0]` correctly pick the single most positive and most negative feature, but adding comments or more descriptive variable names could enhance readability.
*   **Error Handling:** The ground truth includes robust error handling for file not found, which is good practice for production-ready scripts. The generated code lacks this. While not explicitly asked for, it contributes to overall code quality.
*   **Plot Customization:** The ground truth included more detailed plot customizations (e.g., specific figsize, DPI, grid, legend, title font size, x-axis label rotation for heatmap) which makes the plots more presentable and readable. The generated code's plots are fine but could be enhanced.",gpt-3.5-turbo,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"import pandas as pd
from scipy.stats import ttest_ind, mannwhitneyu, chi2_contingency

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Clean missing values
df.dropna(inplace=True)

# Test for Nintendo games vs all other publishers
nintendo_sales = df[df['Publisher'] == 'Nintendo']['Global_Sales']
other_sales = df[df['Publisher'] != 'Nintendo']['Global_Sales']
t_stat, p_value = ttest_ind(nintendo_sales, other_sales)

print('Nintendo vs Other Publishers:')
print('T-statistic:', t_s...",3,"The generated code fails to meet several critical requirements of the task.

**Correctness:**
- **Data Cleaning:** The initial `df.dropna(inplace=True)` is too aggressive. It removes rows where `User_Score` is NaN, which is not required for the tests and significantly reduces the dataset, potentially impacting the statistical power and validity of the tests. The ground truth carefully drops NaNs only for columns essential to the specific tests.
- **Interpretation:** The generated code only prints p-values and statistics, but **lacks any interpretation** of the results based on the significance level (=0.05). This was a explicit requirement: ""Display p-values and interpret results with significance level =0.05."" This is a major failure.
- **Specific Test Parameters:**
    - For the t-test (Nintendo vs. Others), the task asks to test if Nintendo has **significantly higher** sales, implying a one-sided test. The generated code uses the default two-sided `ttest_ind`.
    - For `Year_of_Release`, the generated code assumes a column named 'Year' which doesn't exist in the dataset (it's 'Year_of_Release'). This will cause a `KeyError`.
    - The `mannwhitneyu` function for ""significantly different"" should ideally be `alternative='two-sided'`, which is the default but good to be explicit.
    - The chi-square `contingency_table` generation is flawed: `pd.crosstab(df['Genre'], df['Platform'])[top_platforms]` without filtering `df` first by top genres means the contingency table will still contain all genres, potentially leading to errors or less focused analysis if `top_platforms` doesn't fully align with the top 5 platforms across all genres.

**Code Quality:**
- **Readability/Structure:** Lacks clear separation of concerns (e.g., dedicated cleaning section, separate print statements for each test with descriptive headers).
- **Error Handling:** No `try-except` for file loading.
- **Constants:** `ALPHA` is not defined as a constant, making the interpretation step (if it were present) less maintainable.

**Completeness:**
- **Missing Interpretations:** As mentioned, this is a critical missing component.
- **Incomplete Cleaning:** The data cleaning is insufficient and overly aggressive.
- **Missing `equal_var=False` for t-tests:** While not explicitly stated in the prompt, Welch's t-test (`equal_var=False`) is generally preferred when variances are unknown or unequal, which is often the case in real-world data and good practice. The ground truth correctly uses it.
- **No Null/Alternative Hypotheses printed:** While not explicitly asked to print these, the ground truth does, which is excellent for clarity and understanding the test's purpose. The generated code just throws out numbers without context.

This code produces output, but it's largely incorrect or incomplete regarding the problem statement and typical data science practices for hypothesis testing.",gpt-3.5-turbo,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values
df.fillna(0, inplace=True)

# Create binary target variable
df['High_Sales'] = df['Global_Sales'].apply(lambda x: 1 if x > 1.0 el...",6,"The generated code addresses most of the requirements but has significant issues in handling missing values and feature selection, which would lead to incorrect model training and evaluation.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly creates the 'High_Sales' target variable.
*   Correctly uses `LabelEncoder` for Platform, Genre, and Publisher.
*   Correctly creates 'Total_Regional_Sales' and 'Years_Since_Release'.
*   Correctly splits the data for training and testing.
*   Correctly trains a `RandomForestClassifier` with 100 estimators.
*   Correctly evaluates the model using accuracy, precision, recall, F1-score, and confusion matrix.
*   Correctly displays feature importance.
*   Proper imports are included.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling (Critical Issue):** The generated code uses `df.fillna(0, inplace=True)`. This is a problematic approach, especially for `Year`, `Publisher`, `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`, and `Global_Sales`.
    *   Setting `Global_Sales` NaN to 0 would incorrectly classify games as ""Low Sales"" even if they might have high sales but missing data. The ground truth correctly drops rows where 'Global_Sales' is NaN.
    *   Setting 'Year' NaN to 0 would create a 'Years_Since_Release' of 2023, which is highly inaccurate for any game. The ground truth correctly drops these rows and converts 'Year' to `int`.
    *   Setting 'Publisher' NaN to 0 would try to label encode '0' as a publisher, which is not a valid approach for categorical data.
    *   For sales figures, 0 might be a plausible imputation if the game truly made no sales, but dropping rows with missing sales data is generally safer if the amount of missing data is small, as it preserves the integrity of the data. The ground truth's approach of dropping NaNs in essential columns is more robust for this task.

2.  **Feature Selection (Major Issue):** The generated code selects `X = df[['Platform', 'Genre', 'Publisher', 'Total_Regional_Sales', 'Years_Since_Release']]`. This is missing the individual regional sales columns (`NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`) which are critical indicators of sales performance and were explicitly mentioned in the problem description (by creating `Total_Regional_Sales` as an *additional* feature, implying the original ones should also be used). The ground truth code correctly includes these original sales columns.

3.  **No Stratification in `train_test_split`:** The `stratify=y` parameter is missing in `train_test_split`. This is important, especially for binary classification tasks, to ensure that the proportions of target classes are preserved in both training and testing sets, preventing potential imbalances.

4.  **No Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.",gpt-3.5-turbo,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
# Import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures

# Load dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values
df.fillna(0, inplace=True)

# Create engineered features
df['Sales_Ratio_NA_EU'] = df['NA_Sales'] / df['EU_Sales']
df['Publisher_Avg_Sales'] = df.groupby('Publisher')['Global_Sales'].transform('...",4,"The generated code attempts to address the requirements but has significant flaws in correctness, completeness, and robustness, leading to potential errors and incorrect results.

**Correctness Issues:**
*   **Missing Value Handling:** `df.fillna(0, inplace=True)` is a very naive and often incorrect way to handle missing data. For 'Year', 0 is a meaningless value. For categorical columns like 'Publisher' or 'Genre', 0 doesn't make sense. The ground truth uses `SimpleImputer` with appropriate strategies (median for numerical, mode for categorical).
*   **Sales Ratio Division by Zero:** `df['Sales_Ratio_NA_EU'] = df['NA_Sales'] / df['EU_Sales']` will raise a `ZeroDivisionError` or result in `inf` if `EU_Sales` contains zeros. The ground truth adds a small epsilon to prevent this.
*   **Genre Market Share:** `df.groupby('Genre')['Global_Sales'].transform(lambda x: x / x.sum() * 100)` is incorrect. `x` within `transform` is a Series *for each group*, so `x.sum()` is the sum *for that specific genre*, not the total global sales. This calculates the percentage of a game's sales within its genre, not the genre's market share of total global sales. The ground truth correctly divides by `total_global_sales`.
*   **Platform Popularity:** `df['Platform_Popularity'] = df['Platform'].map(df['Platform'].value_counts())` is a correct way to get the count of games per platform, but `groupby().transform('count')` is generally more idiomatic in pandas for this kind of operation when calculating a value for each row based on group properties.
*   **PCA Input:** `pca.fit(df.select_dtypes(include=['float64', 'int64']))` is problematic. PCA requires scaled data, which is completely missing. Also, passing all numerical columns (including potentially game 'Rank' or other non-features, and also the *newly engineered features*) without proper selection or scaling will yield poor results. The ground truth scales data using `StandardScaler` and carefully selects features.
*   **Polynomial Features:** `poly.get_feature_names(['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales'])` is deprecated. It should be `poly.get_feature_names_out(input_features)`. Also, the polynomial features dataframe is created but *never added back to the original `df`*, making this step incomplete.
*   **Interaction Features:** `df['Year_Genre'] = df['Year'].astype(str) + '_' + df['Genre']` just creates a string concatenation, not a numerical interaction feature as typically understood (e.g., `Year * one_hot_encoded_Genre`). The ground truth uses one-hot encoding for genre and multiplies it by year. This is a severe misunderstanding of ""interaction features"" in a mathematical context (e.g., for regression).
*   **Outlier Handling:** `df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR)).any(axis=1)]` is syntactically incorrect (missing parenthesis and an incorrect `any(axis=1)` application for filtering across all columns) and functionally flawed. Filtering by row for outliers across *all* numerical columns simultaneously will likely remove too much data. The task typically implies handling outliers column-wise (e.g., capping), not dropping rows where *any* column has an outlier. The ground truth demonstrates column-wise capping.
*   **Missing Imports:** `StandardScaler` and `SimpleImputer` are used in the ground truth but not imported by the generated code, indicating missing necessary components for a robust solution based on the ground truth's best practices.

**Completeness Issues:**
*   **Polynomial Features not integrated:** As mentioned, `poly_df` is created but never merged back into `df`.
*   **Interaction Features are not numerical:** The generated interaction feature is a string, not a numerical one useful for analysis or modeling.
*   **Lack of Visualization for PCA and Polynomial/Interaction Feature Impact:** While a heatmap for correlation is present, there is no visualization or analysis of PCA components or the general impact of polynomial/interaction features (though the latter were poorly implemented).
*   **No explanation of variance after PCA:** Only `pca.explained_variance_ratio_` is printed, without any visual like the cumulative explained variance plot provided in the ground truth.

**Code Quality Issues:**
*   **Lack of Robustness:** Hardcoding `fillna(0)` and not handling `ZeroDivisionError` demonstrates a lack of robustness.
*   **No warning suppression:** While not a critical flaw, the ground truth's inclusion of warning suppression is a good practice.
*   **No print statements for progress/verification:** The ground truth includes ample `print` statements to show the state of the DataFrame at various stages, which is helpful for debugging and understanding.",gpt-3.5-turbo,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, mannwhitneyu, chi2_contingency, pearsonr, f_oneway

# Load dataset
df = pd.read_csv('diabetes.csv')

# Clean zero values
df['Glucose'] = df['Glucose'].replace(0, np.nan)
df['Glucose'].fillna(df['Glucose'].median(), inplace=True)

# Hypothesis testing
# T-test for glucose levels between diabetic and non-diabetic patients
diabetic_glucose = df[df['Outcome'] == 1]['Glucose']
non_diabetic_glucose = df[df['Outcome'] == 0]['Gluc...",6,"The generated code addresses most of the statistical tests required by the task, performing t-test, Mann-Whitney U test, Chi-square test, Pearson correlation, and ANOVA. This covers significant portions of the problem statement.

However, there are several critical omissions and inaccuracies that prevent a higher score:
1.  **Incomplete Data Cleaning:** The task explicitly asks to clean zero values in `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, and `BMI` by replacing them with NaN and then performing median imputation. The generated code *only* performs this for 'Glucose' and ignores the other specified columns. This is a significant data preparation failure.
2.  **No Interpretation of Results:** The task clearly states, ""Display p-values and interpret results with significance level =0.05."" The generated code only prints the p-values without any interpretation (e.g., comparing to =0.05, stating rejection/failure to reject null hypothesis, and concluding the finding based on the p-value). This is a major omission from the core requirement of statistical analysis.
3.  **Missing Error Handling:** The ground truth includes `try-except` for file loading, which is good practice. The generated code lacks this.
4.  **No Comprehensive Imputation Check:** The ground truth thoughtfully checks `if df[col].isnull().any():` before attempting imputation, ensuring it only runs if there are NaNs. While not strictly a correctness issue here since we know there will be zeros to replace, it's a better practice.
5.  **Age Group Binning:** The problem specified `young: <30, middle: 30-50, older: >50`. The code uses `bins=[0, 30, 50, 200]`. This translates to `(0, 30]`, `(30, 50]`, `(50, 200]`. The ground truth uses `bins=[0, 29, 50, np.inf]` with `right=True`, which correctly captures the `<30` (i.e., upto 29), `30-50`, and `>50` (or `>50` for the last bin) logic better by making `30` fall into the 'middle' category. While the difference might be minor for some datasets, it's a subtle inaccuracy.
6.  **BMI Category Binning:** Similarly, the BMI categories are `Underweight: <18.5, Normal: 18.5-24.9, Overweight: 25-29.9, Obese: >=30`. The generated code uses `bins=[0, 18.5, 25, 30, 200]`. Without `right=False` (as in the ground truth), default `right=True` means `(0, 18.5]`, `(18.5, 25]`, `(25, 30]`, `(30, 200]`. This doesn't perfectly align with `<18.5`, `18.5-24.9`. For example, `18.5` would fall into the 'normal' bin, not 'underweight' as per `<18.5`. The `right=False` in ground truth handles this correctly.
7.  **ANOVA Robustness:** The ANOVA call in the generated code directly passes the four groups without checking if any of them are empty. While unlikely with this dataset, a robust solution would check for empty groups and filter them out as the ground truth does.

To improve, the generated code needs to prioritize fulfilling all requirements, especially the data cleaning and interpretation steps, which are fundamental to the task.",gpt-3.5-turbo,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metri...",6,"The generated code addresses most of the tasks, but several critical aspects are either incomplete, incorrect, or sub-optimally implemented compared to the ground truth.

**Correctness:**
*   **Missing Value Handling:** The code only handles NaNs for 'Glucose' and 'BMI'. The task explicitly mentions all 0s for specified columns (Glucose, BloodPressure, SkinThickness, Insulin, BMI) are missing values and should be replaced with the median. This is incorrect.
*   **Feature Engineering:** The `Age_group` bins are different from the ground truth and possibly too coarse ('0-30', '30-50', '50-100'). The `Insulin_level` tertiles are correctly implemented using `qcut`.
*   **Pipeline Usage:** The code uses `Pipeline` for cross-validation and `GridSearchCV`, which is good. However, when evaluating the models *after* cross-validation/grid search, it refits models *outside* a pipeline (`model.fit(X_train, y_train)`). This is problematic because `X_train` is *not* preprocessed in these lines, leading to potential errors or incorrect results when the models expect scaled numerical features and one-hot encoded categorical features. The ground truth correctly preprocesses `X_train` and `X_test` separately and then passes the *transformed* data to the models.
*   **SVC probability:** SVC needs `probability=True` for `predict_proba` to work. The generated code does not set this, leading to `predict_proba` failing for SVC.
*   **Feature Importance Display:** The `plot_confusion_matrix` and `plot_roc_curve` functions are deprecated in newer Scikit-learn versions (replaced by `ConfusionMatrixDisplay.from_estimator` and `RocCurveDisplay.from_estimator`). While the code might run on older versions, it's not best practice. The feature importance calculation is only done for the best model (Random Forest), and it incorrectly combines numerical features with `get_feature_names_out()` which is only for one-hot encoded features. The numerical features are not explicitly named in the `ColumnTransformer`'s output, leading to a mismatch. The feature importance is also not properly visualized as a bar plot, just printed.

**Code Quality:**
*   **Imports:** Generally good, though `plot_confusion_matrix` and `plot_roc_curve` are deprecated.
*   **Structure:** The flow is somewhat logical, but the non-pipeline model fitting and evaluation part needs refinement.
*   **Readability:** Decent, but could benefit from more comments and descriptive print statements like the ground truth.

**Completeness:**
*   **Missing Value Handling:** Incomplete as mentioned.
*   **Feature Engineering:** Provides `Glucose_BMI_ratio` and `Age_group` (with different bins) and `Insulin_level`.
*   **Data Split and Encoding:** Correctly splits data and defines `ColumnTransformer` for encoding/scaling.
*   **Multiple Models & Cross-Validation:** Correctly trains and cross-validates multiple models, but the evaluation structure post-CV is flawed.
*   **Hyperparameter Tuning:** Correctly performs `GridSearchCV` for Random Forest.
*   **Comprehensive Evaluation:** Calculates required metrics but the `predict_proba` for SVC issue remains. Visualizations are attempted but use deprecated functions.
*   **Confusion Matrices and ROC Curves:** Attempts these, but uses deprecated plotting functions. Also, it plots a confusion matrix and ROC curve for *each* model *individually* inside the loop, which will result in many separate plots, making comparison difficult. The ground truth plots them together.
*   **Feature Importance:** Only provides feature importance for the tuned Random Forest and just prints it. It doesn't visualize it, and the feature name mapping is incorrect for all features. It also doesn't provide importance for Gradient Boosting.

**Key areas for improvement:**
1.  **Comprehensive Missing Value Handling:** Ensure all specified columns (`Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`) have their 0s replaced with NaNs and then imputed (e.g., with median).
2.  **Consistent Preprocessing:** All model training and evaluation should happen *within* the `Pipeline` or on explicitly preprocessed data. The current evaluation loop does not use the preprocessed data correctly for direct model fitting.
3.  **Modern Visualization:** Use `ConfusionMatrixDisplay.from_estimator` and `RocCurveDisplay.from_estimator` or manually extract FPR/TPR and plot using `matplotlib`/`seaborn` for better, non-deprecated visualization and combined plots.
4.  **SVC `probability=True`:** Add `probability=True` to the `SVC` initialization if ROC AUC is to be calculated.
5.  **Feature Importance:** Correctly extract feature names for the `ColumnTransformer`'s output for all features (numerical and one-hot encoded) and visualize the feature importance (e.g., using a bar plot) for *both* tree-based models (Random Forest and Gradient Boosting).",gpt-3.5-turbo,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
import seaborn as sns
import matplotlib.pyplot as plt

# Load and preprocess dataset
df = pd.read...",5,"The generated code attempts to address most of the requirements but falls short in several key areas, leading to incorrect or incomplete implementations for a ""data science benchmark"" quality.

Here's a breakdown:

**Correctness:**
*   **Missing Value Handling:** The ground truth correctly identifies that 0s in certain columns ('Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI') are actual missing values, which should be imputed. The generated code uses `SimpleImputer(strategy='mean')` on the entire dataframe directly after loading, without specifying which columns to impute, and assumes 0s are not present, which is incorrect for this dataset.
*   **Engineered Features:**
    *   `Metabolic_Score`, `Glucose_per_Age`, `BMI_BloodPressure_interaction`, `Pedigree_Age_product` are correctly implemented.
    *   `Risk_Factor_Count`: The generated code's implementation `(x > 140).sum()` for `Risk_Factor_Count` is not aligned with the ground truth's more nuanced definition which includes different thresholds for different conditions and counts individual conditions as factors. The generated code applies the same threshold (140) to all specified columns for an unclear reason and is likely incorrect for a ""high-risk factor count"".
*   **Polynomial Features:** The generated code applies `PolynomialFeatures(degree=2, interaction_only=True)` to only three features (`Glucose`, `BMI`, `Insulin`). The task explicitly asks for ""polynomial features (degree 2) to selected numerical columns"", and the ground truth applies it to a much broader set of core numerical features, which is more comprehensive. Also, it adds the polynomial features as a new dataframe `df_poly` but never integrates it back into the main `df_imputed` dataframe for subsequent steps, making this step functionally useless for the rest of the pipeline.
*   **Feature Scaling:** While both `StandardScaler` and `MinMaxScaler` are used, there's no clear continuity or choice on which one to use for subsequent steps. The ground truth explicitly states it will use `StandardScaler` output (`X_processed`).
*   **PCA:**
    *   The generated code uses `pca = PCA()` without specifying `n_components`. This will keep all components, effectively just rotating the data without dimensionality reduction. The ground truth uses `n_components=0.95` to retain components explaining 95% of variance, which is a common and appropriate approach for dimensionality reduction.
    *   The `explained_variance_ratio` is calculated but not printed or explained as required by the task.
*   **Feature Selection (SelectKBest & RFE):** Both are implemented, but the `n_features_to_select` for RFE is hardcoded (5) and quite small without justification. The ground truth uses 10 for RFE and 15 for SelectKBest, indicating a more thorough exploration. Also, it uses `RandomForestClassifier()` directly in RFE without specifying a `random_state`, which can lead to non-reproducible results. The ground truth uses `LogisticRegression` which is a more standard estimator for RFE due to its simpler characteristics.
*   **Correlation-based Feature Filtering:** The implementation of the correlation matrix and dropping highly correlated features is functionally similar to the ground truth, but it's applied on `df_scaled1` which includes polynomial features not properly integrated and potentially issues with the initial imputation.
*   **SMOTE:** SMOTE is applied, but the code does not perform a train-test split *before* SMOTE, which is a critical mistake in practice. SMOTE should only be applied to the training data to prevent data leakage. The task asks to apply SMOTE for handling class imbalance, and the ground truth correctly performs a train-test split first.
*   **Feature Importance:** The feature importance is calculated and visualized, but again, it's done on `df_filtered` (which has issues from previous steps) and the full dataset, not just the training set as best practice dictates.

**Code Quality:**
*   **Imports:** Mostly correct, but `imblearn.over_sampling.SMOTE` and `sklearn.ensemble.RandomForestClassifier` are imported twice.
*   **Structure:** The flow is generally sequential, but steps are not clearly compartmentalized or explicitly identified with print statements like the ground truth (e.g., ""--- 1. Loading and Preprocessing Data ---""), which improves readability and aligns with benchmark style.
*   **Variable Naming:** `df_imputed`, `df_poly`, `df_scaled1`, `df_scaled2` are decent, but lack consistency in which processed dataframe is carried forward.
*   **Comments/Prints:** Lacks sufficient descriptive print statements to show progress or outputs (e.g., explained variance ratio for PCA, number of features after filtering).

**Completeness:**
*   **Explicit Explanations:** The task asks to ""explain variance ratios"" for PCA, which the generated code fails to do.
*   **Continuous Dataflow:** The data transformation steps are not always well-connected. For example, `df_poly` is created but never merged back. `df_scaled1` and `df_scaled2` are created, but only `df_scaled1` is used, and there's no clear choice or justification. The PCA and feature selection steps operate on `df_scaled1`, which isn't the final processed dataset after polynomial features or the best choice without proper integration.
*   **Train-Test Split:** Crucially missing a train-test split before applying SMOTE and for feature importance evaluation.

Overall, the generated code contains fundamental errors in imputation, feature engineering integration, PCA application, and class imbalance handling that would render the results untrustworthy in a real-world scenario. While it attempts to cover most points, the quality of implementation is low.",gpt-3.5-turbo,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, mannwhitneyu, f_oneway, pearsonr, chi2_contingency

df = pd.read_csv('world-happiness-report-2021.csv')

df.dropna(inplace=True)

western_eu = df[df['Regional indicator'] == 'Western Europe']['Ladder score']
other_regions = df[df['Regional indicator'] != 'Western Europe']['Ladder score']

t_stat, p_val = ttest_ind(western_eu, other_regions)

top_25_happy = df.nlargest(int(0.25*len(df), 'Ladder score'))['Logged GD...",4,"The generated code attempts to perform the requested statistical tests but fails significantly in several key areas, leading to incorrect results and incomplete fulfillment of the task requirements.

**Primary Reasons for Failure:**

1.  **Missing Interpretation and Significance Level:** The task explicitly asked to ""Display p-values and interpret results with significance level =0.05"". The generated code only prints p-values without any interpretation against the alpha level, which is a core requirement of hypothesis testing. It also doesn't define or use `alpha = 0.05`.
2.  **Incomplete Data Handling (Missing Values):** While `df.dropna(inplace=True)` is present, the ground truth code specifically highlights `critical_columns` and explains *why* those columns are necessary for analysis, then provides informative print statements about the number of rows dropped. The generated code's `dropna` is too generic and lacks transparency.
3.  **Critical Errors in Test Implementations:**
    *   **Mann-Whitney U Test (Crucial Logic Error):** The `nlargest` and `nsmallest` calls are fundamentally incorrect. `int(0.25*len(df))` will give the number of rows, but the second argument should be the column name for `nlargest` and `nsmallest`. The provided code uses `int(0.25*len(df), 'Ladder score')` which is a `TypeError` because `int` takes a base argument which 'Ladder score' is not. Even if corrected, it wouldn't select the GDP for the top/bottom 25% *happiest* countries in the way the ground truth does by first sorting the entire dataframe by 'Ladder score'. It just pulls a random 25% of the GDP values.
    *   **Chi-Square Test (Missing Categorization):** The task requires ""happiness categories (High/Medium/Low) are associated with regional indicators"". The generated code uses `df['Happiness Categories']` but *never defines* this column or how the categories (`High/Medium/Low`) are created. This will lead to a `KeyError`. The ground truth explicitly creates these categories based on quantiles.
    *   **T-test for Freedom/Generosity:** Similar to the chi-square, the median calculation for `Freedom to make life choices` is correct, but the interpretation part (comparing means and stating which group is higher/lower) is missing.
4.  **Lack of Robustness:**
    *   **File Handling:** No `try-except` block for `FileNotFoundError`.
    *   **`ttest_ind` parameters:** The ground truth uses `equal_var=False` (Welch's t-test), which is often more appropriate for real-world data and was a good choice for robustness. The generated code defaults to `equal_var=True`.
5.  **Lack of Output Clarity:** The output is just p-values. It lacks informative statements about what each test is comparing (e.g., ""Mean Happiness (Western Europe): X.X"", ""T-statistic: Y.Y"") which the ground truth provides, making the results much clearer.
6.  **Readability/Structure:** The code is very condensed, with all calculations first and then all prints. While not inherently wrong, spreading out the print statements and interpretations alongside their respective calculations (as in the ground truth) improves readability and understanding of each individual test result block.

In summary, the generated code only partially implements the statistical tests but fundamentally fails on data preparation specific to certain tests (like happiness categorization for chi-square and correct selection for Mann-Whitney U), robustness, and the crucial interpretation/reporting aspect of the task.",gpt-3.5-turbo,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as ...",7,"The generated code addresses most of the requirements but has several areas for improvement, particularly concerning data preprocessing, hyperparameter tuning choices, and the implementation of cross-validation and new country predictions.

**Strong Points:**
*   Correctly loads the dataset.
*   Handles missing values (though implicitly by dropping).
*   Correctly selects target and feature columns.
*   Splits data into training and testing sets.
*   Trains and evaluates multiple regression models.
*   Performs hyperparameter tuning using GridSearchCV.
*   Evaluates models with MAE, MSE, RMSE, and R score.
*   Generates residual plots and feature importance visualizations.
*   Implements cross-validation.
*   Makes predictions for a new hypothetical country.
*   Includes necessary imports.

**Areas for Improvement:**

1.  **Data Preprocessing and Categorical Variables:**
    *   The prompt explicitly states ""encoding categorical variables"". The generated code subsets the DataFrame to *only* the specified numerical columns and then calls `pd.get_dummies(df)`. Since the selected columns are all numerical, `pd.get_dummies` will have no effect. This means no categorical encoding is actually performed if there were any, which would be a misinterpretation of the instruction if `Country name` or `Regional indicator` were intended to be features. The ground truth correctly identifies that no categorical features were requested *as input features* and thus no encoding was needed for `X`, but it discusses the overall dataset.
    *   The feature `Freedom` and `GDP per capita` in the prompt refer to `Freedom to make life choices` and `Logged GDP per capita` in the actual dataset. While the generated code uses the correct dataset names, it's good practice to be explicit about this mapping or note it. The ground truth handles this more robustly by defining `FEATURE_COLUMNS` with the correct names.

2.  **Hyperparameter Tuning:**
    *   The prompt asks to perform hyperparameter tuning for ""the best model"". The generated code arbitrarily picks `RandomForestRegressor` for tuning, even if another model (like Gradient Boosting) performed better initially. The ground truth correctly identifies the best initial model and then chooses Gradient Boosting (a common good choice) for tuning.
    *   The `param_grid` for `RandomForestRegressor` is quite limited (only `n_estimators` and `max_depth`). A more comprehensive grid search would explore `min_samples_split`, `min_samples_leaf`, etc., as seen in the ground truth.

3.  **Cross-Validation:**
    *   The `cross_val_score` is applied only to `X_train`, `y_train`. Cross-validation is typically performed on the *entire* dataset `X, y` (or the training set if embedded within GridSearchCV) to assess overall model stability, not just the training subset for a final CV. The ground truth applies it to `X, y`.
    *   Only one metric (default R2) is used for cross-validation. The ground truth shows how to do it for multiple metrics (R2 and RMSE).

4.  **Prediction for New Hypothetical Countries:**
    *   The `new_country` DataFrame has column names exactly matching the original features. However, the `StandardScaler` was fit on `X_train` which included the original column names. When `scaler.transform(new_country)` is called, it *expects* the same column order and feature names; passing a DataFrame with named columns works, but it's crucial if the order could be different.
    *   More importantly, `new_country` values are just arbitrary numbers (e.g., `Logged GDP per capita` as `1.2`). These values should ideally be *realistic* or scaled to align with the training data's distributions. For example, `Logged GDP per capita` typically ranges from ~6 to ~11 in the dataset. `1.2` is an extremely low, unrealistic value. This impacts the meaningfulness of the prediction. The ground truth uses more realistic values.
    *   The analysis of predicted results is very minimal, just printing the score. The ground truth code provides a more descriptive analysis.

5.  **Code Quality/Structure:**
    *   Missing `np.random.seed(42)` which is good for reproducibility, particularly important for models like Random Forest and Gradient Boosting, and for `train_test_split`.
    *   The `pd.get_dummies(df)` line for `df = df[['Ladder score', ...]]` does nothing productive as all these columns are numerical. It shows a potential misunderstanding of `get_dummies` purpose here.
    *   No handling for `FileNotFoundError` for the CSV. The ground truth includes a robust try-except block.
    *   The prompt asks for 'perceptions of corruption' while the code uses 'Perceptions of corruption' which is the correct column name. Consistency is good but the prompt's wording slightly differs. The ground truth explicitly states these column names.
    *   The output formatting for results is slightly less clear than the ground truth (e.g., ground truth prints a summary DataFrame).
    *   Missing analysis of the hypothetical country predictions.

Overall, the code follows the general flow but lacks some fine-tuning in data handling logic and robustness, and doesn't fully optimize the hyperparameter tuning and cross-validation steps as thoroughly as the ground truth.",gpt-3.5-turbo,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import make_pipeline
from sklearn.featur...",3,"The generated code attempts to address many parts of the prompt but falls significantly short in correctness, completeness, and structure.

**Correctness & Completeness Issues:**

1.  **Preprocessing:** The generated code completely misses handling missing values (`fillna`, `dropna`) which is a crucial first step in any data science pipeline, especially for calculations like mean and division. The ground truth explicitly handles this.
2.  **Engineered Features:**
    *   `Happiness_Efficiency`: Correctly implemented.
    *   `Social_Wellness_Index`: The ground truth uses `+` (combination/sum) while the generated code uses `/ 2`. The prompt asked for a ""combination,"" which usually implies a sum or a more complex function, not an average of just two terms that makes the scale different.
    *   `Governance_Score`: Similar to `Social_Wellness_Index`, it uses `/ 2`. Also, `(1 - df['Perceptions of corruption'])` is correctly used, but the prompt just asked for ""combination"", the division is an arbitrary choice.
    *   `Regional_Happiness_Rank`: Correctly implemented.
    *   `GDP_vs_Regional_Average`: Correctly implemented.
3.  **Polynomial Features:**
    *   The prompt asked for `degree 2` to ""key happiness factors"". The generated code applies it to a selected list `key_features`, which is reasonable.
    *   It correctly uses `poly.get_feature_names(key_features)`. However, `poly.get_feature_names(input_features)` is deprecated and `get_feature_names_out(input_features)` should be used, which the ground truth does.
    *   A significant issue is `df_poly = pd.DataFrame(poly_features, columns=poly_columns)`. This line creates a new DataFrame `df_poly` without aligning its index with the original `df`. When `df = pd.concat([df, df_poly], axis=1)`, this will lead to misaligned rows if the original `df` had any rows dropped for NaNs (which it should have, but the generated code omitted this step). The ground truth ensures index alignment by passing `index=df_processed.index`.
4.  **Interaction Features (Missing):** The prompt explicitly asked to ""Create interaction features between economic and social factors"". This step is completely missing in the generated code.
5.  **Feature Scaling Loop:** The generated code attempts to loop through different scalers and then performs PCA, t-SNE, `SelectKBest`, and `RFE` *inside* the loop. This means these steps are performed 3 times, once for each scaler. The prompt implies applying *multiple scaling methods*, not necessarily applying *all* subsequent pipelines to *all* scaled versions. The ground truth applies all three scalers, but then picks one (StandardScaler) for the subsequent steps, which is more typical and efficient for demonstration.
6.  **Dimensionality Reduction (PCA & t-SNE):**
    *   Code for PCA and t-SNE is present.
    *   **Visualization:** The prompt explicitly asks for visualization of PCA and t-SNE results. The generated code performs the reduction but *does not visualize* the results using `matplotlib.pyplot` or `seaborn` as requested and as done in the ground truth. This is a critical omission.
    *   No print statements for explained variance or t-SNE details.
7.  **Feature Selection:**
    *   `SelectKBest`: Correctly implemented using `f_regression`. However, the prompt also asked for `mutual_info_regression`, which is missing.
    *   `RFE`: Uses `RandomForestRegressor()` which is different from the `LinearRegression()` used in the ground truth. While not strictly ""wrong"", the prompt didn't specify the estimator. The crucial omission here is that `rfe.estimator_.feature_importances_` will only work if the base estimator (RandomForestRegressor here) has `feature_importances_` attribute. The `feature_importances_` obtained *after* RFE is based on the final fitted estimator on the *reduced* feature set, not necessarily the original feature set or a comprehensive list of all importances from the RFE process itself. The prompt implied obtaining feature importances from a separate Random Forest model for validation.
8.  **Correlation-based Feature Filtering (Missing):** This entire section is completely missing.
9.  **Mutual Information Scoring (Missing):** While `SelectKBest` was used in the ground truth, the prompt also asked for ""mutual information scoring"" specifically, implying displaying the scores, which is not done.
10. **Validate Feature Importance with Random Forest and Visualize:**
    *   The `rfe.estimator_.feature_importances_` is used for visualization, but this is only for the `n_features_to_select` features that RFE chose, not for all features from a separate Random Forest model trained on the full feature set as requested.
    *   The `plt.barh` call is problematic as `df.columns.drop(['Country name', 'Regional indicator', 'Ladder score'])` will usually return *all* original numerical features (plus engineered ones), not just the ones selected by RFE, leading to a mismatch in length with `feature_importances`. This graph would likely fail or be misleading. The plotting logic for `feature_importances` is incorrect.
    *   No print statements for top features by importance.
11. **Error Handling/Robustness:** The ground truth includes `try-except` for file loading and handles `inf/-inf` values after feature engineering. The generated code lacks these critical robustness measures.
12. **Comments and Explanations:** While the prompt asked for only Python code, the ground truth provided comments that clarify intent, which is a good practice. The generated code is bare and hard to follow in places.

**Code Quality/Structure Issues:**

1.  **Imports:** Imports are generally correct but lack `seaborn` for plotting.
2.  **Variable Naming:** `df_scaled` is overwritten in each loop iteration, making it unusable outside the loop for specific scaler effects.
3.  **Redundant Calculations:** Performing PCA, t-SNE, SelectKBest, and RFE three times (once for each scaler) is inefficient and unnecessary for the task's intent, which was to *demonstrate* different scaling methods, not run the entire pipeline on each one.
4.  **`df_scaled` dimensions:** `df.drop(['Country name', 'Regional indicator', 'Ladder score'], axis=1)` is performed, but then `df_scaled` is directly used with `df['Ladder score']` for target, and the columns for plotting are taken from the original `df`. This mixing can lead to silent errors or misinterpretations, especially since `df_scaled` has no column names after `fit_transform`. Proper handling of column names for `df_scaled` is crucial for debugging and plotting with feature names. The ground truth meticulously preserves column names by putting scaled data back into a DataFrame.

Overall, the generated code misses several key requirements, implements others incorrectly or inefficiently, and lacks the robust error handling and visualization details present in the ground truth.",gpt-3.5-turbo,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"import pandas as pd
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# 1) Load the dataset, parse the 'date' column, and filter for 'location' == 'USA'
data = pd.read_csv('synthetic_covid19_data.csv')
data['date'] = pd.to_datetime(data['date'])
usa_data = data[data['location'] == 'USA']

# 2) Set the 'date' column as the index for the filtered data
...",7,"The generated code correctly implements most of the requested functionalities: data loading and filtering, setting index, resampling, time series decomposition and plotting, ADF test, differencing, and plotting ACF/PACF. However, there are several areas for improvement in terms of robustness, completeness, and adherence to specific instructions.

Missing/Improvements:
- **Missing `period` parameter in `seasonal_decompose`**: The ground truth uses `period=52` for weekly data, which is crucial for correct seasonality. The generated code defaults this, which might lead to incorrect decomposition results.
- **Incomplete ADF test logic**: The task explicitly asks to ""If non-stationary, apply first-order differencing and re-test for stationarity."" The generated code *always* applies differencing and re-tests, regardless of whether the original series was found to be non-stationary (i.e., it doesn't check the p-value of the first ADF test).
- **ACF/PACF plot on *potentially differenced* stationary series**: The generated code *always* plots ACF/PACF on the differenced series (`diffed_weekly_new_cases`), even if the original `weekly_new_cases` was already stationary. It should choose the appropriate series based on the stationarity test results.
- **Handling of edge cases/robustness**:
    - No error handling for file not found or empty data after filtering.
    - No check for sufficient data points before performing decomposition or ADF test (e.g., if `weekly_new_cases` is too short or empty).
    - `seasonal_decompose` can fail or produce meaningless results with too few data points relative to the `period`.
- **Clarity of output**: While it prints ADF results, it doesn't clearly state the conclusion of stationarity for either the original or differenced series.
- **Plotting display**: `plt.show()` is called twice after the ACF/PACF plots, which might lead to separate windows for no reason if plots are combined. The ACF and PACF plots should ideally be combined into a single figure for better presentation (as shown in the ground truth).
- **Implicit assumptions**: `decomposition.plot()` is called directly without assigning `fig` which limits further customization of the plot.",gpt-3.5-turbo,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import f_oneway
import numpy as np

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Categorize wines based on pH values
data['pH_group'] = pd.cut(data['pH'], bins=[0, 3.2, 3.4, 5], labels=['Low pH', 'Medium pH', 'High pH'])

# Calculate average quality for each pH group
mean_quality = data.groupby('pH_group')['quality'].mean()

# ANOVA test
low_pH = data[data['pH_group'] == 'Low pH']['quality']
medium_pH = data[...",5,"The generated code attempts to address all parts of the task but falls short on several critical points, particularly concerning the accuracy of the categorization, the confidence interval calculation, and the ANOVA conclusion.

1.  **Correctness**:
    *   **pH Categorization**: The `bins` for `pd.cut` are `[0, 3.2, 3.4, 5]`. The upper bound for 'High pH' is `5`, which is arbitrary and not `inf` as stated in the problem (`> 3.4`). This limits the 'High pH' group to wines with pH less than or equal to 5, which might exclude valid data points if they exist.
    *   **Confidence Intervals**: The calculation of error bars is incorrect. It uses `1.96 * np.std(...) / np.sqrt(...)`, which corresponds to Z-score based standard error for a population standard deviation (or large sample approximation), not a t-distribution-based confidence interval for a sample mean, which is what `scipy.stats.t.interval` does and is generally more appropriate for sample means. Furthermore, `np.std` calculates sample standard deviation, but the multiplication factor of 1.96 typically applies to the standard error of the mean for large samples (using Z-distribution). For small samples and typical confidence intervals, the t-distribution should be used, requiring `sem` and degrees of freedom. The calculation used `np.std(low_pH)` which gives the standard deviation of the quality *values themselves* within that group, not the standard error of the *mean quality* for that group. The error bar should represent the CI of the *mean*.
    *   **ANOVA Conclusion**: The code calculates the F-statistic and p-value but does not print them or provide any interpretation or conclusion about statistical significance, which was explicitly requested (""determine if there is a statistically significant difference"").

2.  **Code Quality**:
    *   **Readability/Structure**: The code is functional but basic. It lacks comments, clear constant definitions, and error handling for file not found (though this was not strictly required by the prompt, it's good practice).
    *   **Imports**: Imports are correct, but `scipy.stats.f_oneway` is specifically imported rather than the broader `scipy.stats`. This is a minor point but shows a slightly less robust import strategy than the ground truth.

3.  **Completeness**:
    *   **pH Group Labels**: While the bins are somewhat off, the labels (""Low pH"", ""Medium pH"", ""High pH"") are functional, but the ground truth's detailed labels `Low pH (< 3.2)`, `Medium pH (3.2 to 3.4)`, `High pH (> 3.4)` are more informative and closer to the problem description.
    *   **Reporting Average Quality**: The code calculates `mean_quality` but does not explicitly *report* it by printing it, which was requested.
    *   **ANOVA Output**: As mentioned, the ANOVA results are calculated but not reported or interpreted.
    *   **Visualization**: The visualization is created but lacks axis labels, a proper title including the CI information, and specific quality range (`plt.ylim`). The ground truth also adds quality values on top of the bars, enhancing readability. The error bars are present but incorrectly calculated.",gpt-3.5-turbo,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load the dataset
data = pd.read_csv('synthetic...",6,"The generated code addresses most of the requirements but has critical flaws in the preprocessing steps that affect correctness and completeness.

**Areas of concern:**

1.  **Feature Selection and Imputation Mismatched:**
    *   The `features` list initially does *not* include `'continent'`.
    *   Missing value imputation with `SimpleImputer` is applied to `data[features + [target]]`. This means `'continent'` is excluded from this imputation, which is correct as it's categorical.
    *   However, after `data = pd.get_dummies(data, columns=['continent'])`, when `data[features]` is used for scaling, it still refers to the *original* numerical features *before* the one-hot encoded 'continent' columns are added to the feature set. This means the one-hot encoded continent columns are *not* scaled and are *not* included in `X_train` and `X_test` when `X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.2, random_state=42)` is called. This is a critical error as the models will be trained without the one-hot encoded 'continent' features.

2.  **Order of Operations for Scaling and Splitting:**
    *   The code scales *all* numerical features (`data[features]`) *before* splitting the data into training and testing sets. This leads to **data leakage**, as the `StandardScaler` learns the mean and standard deviation from the entire dataset, including the test set, which should ideally remain unseen during training. Scaling should happen *after* the train-test split, fitting only on the training data, then transforming both training and test data.

3.  **Missing Value Imputation for Target Variable:**
    *   The task specifies to impute missing values in *selected features and target*. The ground truth correctly imputes `target` (`new_deaths`) if it has missing values. The generated code uses `imputer.fit_transform(data[features + [target]])`, which includes the target, but `new_deaths` should ideally be transformed separately or at least not imputed *after* it's already defined as the target. More robustly, the target variable missing values should be handled before separating X and Y, or by dropping rows. Imputing the target might be debatable depending on the context, but the prompt asks for it.

4.  **Implicit Feature Definition:**
    *   After one-hot encoding `continent`, the new dataframe `data` contains columns like `continent_Europe`, `continent_Asia`, etc. However, these new columns are never explicitly added to a `features` list or considered in the `X` (feature matrix) used for training. The `X` is formed by `data[features]`, which only contains the `features` defined *before* one-hot encoding. This means the one-hot encoded 'continent' information is completely ignored by the models. This is a major functional error.

**To improve the score:**

*   **Correct Feature Set for X:** After one-hot encoding, ensure that `X` includes all the original numerical features *and* the newly created one-hot encoded columns. The easiest way is to define `X` and `y` *after* all preprocessing steps (imputation and one-hot encoding) and then split `X` and `y`.
*   **Prevent Data Leakage:** Ensure scaling (and potentially more complex imputation strategies if not done globally) is applied *after* the train-test split, with the scaler fitted only on the training data. The `ColumnTransformer` and `Pipeline` approach (imported but unused in the generated code) would be an excellent way to handle this correctly and cleanly.
*   **Clearer Data Flow:** Structure the code so that each preprocessing step (imputation, encoding, scaling) logically builds upon the previous one, leading to the final `X` and `y` for model training.",gpt-3.5-turbo,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"import pandas as pd
from scipy.stats import f_oneway, pearsonr, ttest_ind
from statsmodels.stats.multicomp import pairwise_tukeyhsd

# Load the dataset and handle missing values
df = pd.read_csv('synthetic_covid19_data.csv')
df['reproduction_rate'].fillna(df['reproduction_rate'].median(), inplace=True)
df['stringency_index'].fillna(df['stringency_index'].median(), inplace=True)
df['median_age'].fillna(df['median_age'].median(), inplace=True)
df['total_deaths'].fillna(df['total_deaths'].median(),...",6,"The generated code addresses most of the requirements but has several issues in correctness and completeness.

**Correctness Issues:**

1.  **Missing Value Handling:** The imputation is performed directly on the `df` without checking for the presence of the columns, which isn't robust if a column is missing. Also, it implements `fillna` for each column individually, which is less efficient than iterating through a list of columns as in the ground truth.
2.  **ANOVA Context:** The ANOVA is performed on the entire dataset for each continent, which includes multiple entries per country and date. The task implies comparing `reproduction_rate` *across* continents, implying a statistical comparison of the populations rather than individual time series entries. While `f_oneway` can handle this, it's generally cleaner and more robust to pass pre-filtered, clean lists of data. More critically, the problem asks for the latest entry per country for the death rate calculation, but the ANOVA performs on all data points. While not explicitly stated for ANOVA, consistency tends towards aggregated data.
3.  **Pearson Correlation:** The `pearsonr` function implicitly handles NaNs by removing them if present in a pair-wise manner. However, the imputation phase *should* ensure no NaNs in these columns. A more robust approach, as sometimes necessary, is to explicitly drop NaNs from the relevant subset before correlation.
4.  **T-test Data Preparation:**
    *   The `death_rate` calculation `df['total_deaths'] / df['population']` is done on the entire `df`, not on the *latest entry per country* as explicitly stated in the task. This is a critical deviation from the requirement.
    *   The `high_death_rate_country` binary classification is also applied to every row of the `df`, not based on the country's overall 75th percentile of the death rate.
    *   The t-test is performed on `high_death_age` and `low_death_age`, which are derived from all entries of the `df`, not aggregated country-level data. This means a single country with varying `death_rate` entries would contribute to both groups if its daily death rate fluctuates around the threshold, which is incorrect for a country-level classification.

**Completeness Issues:**

1.  **Null Hypothesis Statement:** While present, the null hypothesis statements could be slightly more formal or detailed for each test.
2.  **Output Clarity:** The output does not clearly state the conclusion (reject/fail to reject) in the format requested (""conclude whether to reject the null hypothesis at =0.05""). The ground truth provides more verbose and clear conclusions.
3.  **Missing `alpha` value:** The generated code hardcodes `0.05` instead of defining a configurable `ALPHA` variable, which is good practice.
4.  **Handling Edge Cases/Empty Data:** The generated code does not include robust checks for empty dataframes or groups before performing statistical tests (e.g., if a continent has no data for `reproduction_rate`, `f_oneway` might raise an error without `dropna()` or `if not ... empty` checks). The ground truth handles this much better.

**Code Quality:**

1.  **Imports:** Correct and minimal imports.
2.  **Structure:** Generally logical flow.
3.  **Comments:** Lacks detailed comments for clarity, especially for the complex logic around the t-test.
4.  **Redundancy:** The `fillna` calls could be combined into a loop for better maintainability as in the ground truth.

To improve, the generated code needs significant correction in the t-test data preparation phase to strictly adhere to the ""latest entry per country"" requirement. Additionally, adding more robust checks for data presence and clear, explicit output messages would enhance its completeness and user-friendliness.",gpt-3.5-turbo,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select numerical columns
numerical_cols = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
numerical_df = df[numerical_cols]

# Calculate pairwise correlation matrix
corr_matrix = numerical_df.corr()

# Visualize correlation matrix
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
```",8,"The generated code correctly performs all the requested tasks: loading the CSV, selecting the specified numerical columns, calculating the correlation matrix, and visualizing it as a heatmap with annotations and a suitable colormap. The imports are correct.

Areas for improvement to reach a higher score:
1.  **Missing `matplotlib.pyplot` import and `plt.show()`**: The heatmap is generated, but `plt.show()` is implicitly handled in some environments (like Jupyter notebooks). For standalone script execution, `import matplotlib.pyplot as plt` and `plt.show()` are necessary to display the plot.
2.  **No `plt.title()`**: Adding a title to the heatmap would improve readability and context.
3.  **No descriptive comments or function encapsulation**: While the code is short, encapsulating the logic in a function and adding comments would improve maintainability and reusability, similar to the ground truth.
4.  **No error handling**: The ground truth includes `try-except` blocks for `FileNotFoundError` and missing columns, which is crucial for robust code.",gpt-3.5-turbo,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"import pandas as pd

# Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# Group the data by rarity
grouped_data = data.groupby('rarity')

# Calculate average values for each rarity
average_values = grouped_data.agg({'elixirCost': 'mean', 'Win Rate': 'mean', 'Usage': 'mean'})
print(average_values)",10,"The generated code correctly loads the dataset, groups it by the `rarity` column, and calculates the average `elixirCost`, `Win Rate`, and `Usage` for each rarity. It then displays these values as required. The use of `.agg()` is an equally valid and clear approach to achieve the desired result as direct column selection after `groupby()`. All requirements are met, and the code quality is good with proper imports and clear variable names.",gpt-3.5-turbo,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Create the binary target column
df['is_high_win_rate'] = df['Win Rate'].apply(lambda x: 1 if x > 55 else 0)

# Select features
features = df[['elixirCost', 'Usage', 'rarity']]

# Apply one-hot encoding to the categorical 'rarity' column
features = pd.get_dummies(features, columns=['rarity'])

# Split the data in...",9,"The generated code correctly addresses all requirements of the task. It successfully loads the data, creates the `is_high_win_rate` target column, selects the specified features, applies one-hot encoding to `rarity`, and splits the data into training and testing sets with the correct parameters (80/20 split, `random_state=42`). The imports are also appropriate.

A minor stylistic point that could be considered for improvement, though not critical for correctness, is the line `features = df[['elixirCost', 'Usage', 'rarity']]`. While it works, creating a new `features` DataFrame and then applying `pd.get_dummies` to it can sometimes lead to confusion or silent bugs in more complex pipelines if `df` itself is modified later. The ground truth's approach of separating features and target (`X = df[features].copy()` and `y = df[target]`) is often preferred for clarity and robustness, especially when `X` is subsequently altered by transformations like one-hot encoding. However, for this specific task, the generated code's approach yields the correct result.

The use of `df['Win Rate'].apply(lambda x: 1 if x > 55 else 0)` for creating the target column is perfectly valid and achieves the same result as `(df['Win Rate'] > 55).astype(int)`. Both are good ways to perform boolean to integer conversion.",gpt-3.5-turbo,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('iris.csv')

# Define features X
X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Define target y
y = data['Species']

# Encode the categorical target variable
le = LabelEncoder()
y = le.fit_transform(y)

#...",9,"The generated code is largely correct and addresses most of the requirements.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly identifies and separates features (`X`) and target (`y`).
*   Successfully encodes the categorical target variable using `LabelEncoder`.
*   Correctly splits the data into training and testing sets with the specified `test_size` and `random_state`.
*   Properly trains a `KNeighborsClassifier` with `n_neighbors=3`.
*   Makes predictions and calculates accuracy using `accuracy_score`.
*   Includes all necessary imports.

**Area for improvement:**
*   The ground truth code includes `stratify=y_encoded` in the `train_test_split` function. This is good practice for classification tasks, especially with imbalanced datasets, as it ensures that the proportion of target classes is roughly the same in both the training and testing sets. While not explicitly mentioned as a required parameter in the prompt, it's a common and beneficial addition for robust model evaluation, and its absence is a minor oversight that could lead to less representative splits for imbalanced datasets. For this specific task, it doesn't cause incorrectness but is a good practice point.
*   The error handling for `FileNotFoundError` is missing, which the ground truth included, but the prompt didn't explicitly ask for.",gpt-3.5-turbo,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
data = pd.read_csv('iris.csv')

# Filter the data
versicolor_data = data[data['Species'] == 'Iris-versicolor']['PetalLengthCm']
virginica_data = data[data['Species'] == 'Iris-virginica']['PetalLengthCm']

# Perform independent two-sample t-test
t_statistic, p_value = ttest_ind(versicolor_data, virginica_data)

# Null hypothesis: There is no difference in the mean PetalLengthCm between Iris-versicolor and Iris-vir...",8,"The generated code correctly performs the requested operations: loading data, filtering, performing a t-test, displaying the t-statistic and p-value, and interpreting the result against an alpha of 0.05. The code structure is clean and uses appropriate libraries.

However, it loses points because it does not explicitly print the null and alternative hypotheses as requested in task requirement #4. While they are commented out, the task explicitly asks to ""Clearly state the null and alternative hypotheses for this test"" which usually implies printing them as part of the output, similar to the ground truth. Also, it lacks the explicit `try-except` block for `FileNotFoundError` for loading the CSV, which is a good practice for robustness in code. The final interpretation of rejecting/failing to reject the null hypothesis is concise but could be a bit more verbose, stating the implication for the actual question (e.g., ""There is a statistically significant difference..."").",gpt-3.5-turbo,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
iris_data = pd.read_csv('iris.csv')

# Create box plots
numerical_features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
for feature in numerical_features:
    sns.boxplot(x='Species', y=feature, data=iris_data)
    plt.title(f'Box Plot of {feature} by Species')
    plt.show()
```",8,"The generated code correctly addresses the core requirements of loading the dataset, generating box plots for each numerical feature grouped by species, and adding titles to the plots. Proper imports are included.

Areas for improvement (to achieve a higher score):
1.  **Error Handling:** The generated code lacks error handling for `FileNotFoundError` when reading the CSV, which is present in the ground truth. This makes the code less robust.
2.  **Plot Aesthetics:** While the basic plots are generated, the ground truth includes additional aesthetic improvements like `plt.figure(figsize=...)`, `plt.xlabel()`, `plt.ylabel()`, `plt.grid()`, and `plt.tight_layout()`. These make the plots more readable and professional. The problem description mentions ""appropriate titles"" but also implies general good plotting practices.
3.  **Clarity/Readability:** The ground truth code uses constants (`DATASET_PATH`, `NUMERICAL_FEATURES`, `GROUPING_COLUMN`) which can improve readability and maintainability for more complex scripts, although for this small example, it's less critical.

Overall, it's a good solution that delivers on the main task, but could be made more robust and aesthetically pleasing with a few additions.",gpt-3.5-turbo,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
data = pd.read_csv('heart.csv')

# Select features and target variable
X = data[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']]
y = data['HeartDisease']

# Preprocess categorical features
X = pd.get_dummies(X, columns=['Sex', 'ChestPainType'])

# Split data into training and...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, selects the specified features and target, correctly performs one-hot encoding using `pd.get_dummies`, splits the data with the correct test size and random state, trains a Logistic Regression model, and reports the accuracy. The imports are proper and the code structure is clean and readable.

The use of `pd.get_dummies` for one-hot encoding, while a common and effective approach, implicitly handles the `handle_unknown` parameter differently than `sklearn.preprocessing.OneHotEncoder` used directly or within a `ColumnTransformer`. `pd.get_dummies` will not create columns for categories not present in the training set during transformation of the test set, which can lead to issues if categories are missing in the test set. However, for a simple benchmark, this is often acceptable and leads to correct results if the test set contains all categories seen in training. The `score` method of the model directly calculates accuracy, which is convenient.

The only minor improvement could be to explicitly separate the preprocessing step for the test set or to use a `ColumnTransformer` and `Pipeline` for more robust and maintainable preprocessing, especially if different preprocessing steps were needed for numerical vs. categorical features, or if the `handle_unknown` behavior of `OneHotEncoder` was strictly preferred. However, for this specific task, `pd.get_dummies` on the full `X` before splitting works correctly and simplifies the code.",gpt-3.5-turbo,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
data = pd.read_csv('heart.csv')

# Create two groups based on the `Sex` column
male_group = data[data['Sex'] == 'M']['Cholesterol']
female_group = data[data['Sex'] == 'F']['Cholesterol']

# Perform an independent two-sample t-test
t_statistic, p_value = ttest_ind(male_group, female_group)

# Null hypothesis: There is no difference in the mean `Cholesterol` levels between male and female patients
# Alternative hyp...",7,"The generated code successfully performs the core task of loading the data, creating groups, and performing the t-test. It correctly states the hypotheses (implicitly through comments) and interprets the result based on the alpha level.

However, there are several areas for improvement that prevent a higher score:
1.  **Missing Robustness/Error Handling**: The generated code assumes `heart.csv` always exists and contains the `Sex` and `Cholesterol` columns. The ground truth includes `try-except` blocks for `FileNotFoundError` and checks for column existence, which is crucial for real-world applications.
2.  **Lack of Function Encapsulation**: The ground truth encapsulates the logic within a function (`perform_cholesterol_t_test`), which is a good practice for reusability and organization. The generated code is a script.
3.  **No explicit dropping of NaNs**: The ground truth explicitly uses `.dropna()` on the cholesterol columns to ensure the t-test isn't affected by missing values. While `ttest_ind` might handle NaNs in certain ways depending on the version, explicitly dropping them is safer and clearer.
4.  **Hypotheses Statement**: The hypotheses are stated as comments, which is acceptable but less formal than printing them out as the ground truth does. The prompt specifically asked to ""Clearly state the null and alternative hypotheses"".
5.  **Output Clarity**: While it prints the t-statistic and p-value, the interpretation could be more verbose and clearly state the conclusion in terms of the initial question (e.g., ""There is a statistically significant difference..."" or ""There is no statistically significant difference..."").
6.  **`equal_var` parameter**: The `ttest_ind` function has an `equal_var` parameter (defaults to `True`). The ground truth specifically chooses `equal_var=False` (Welch's t-test) which is generally more robust as it doesn't assume equal variances. The generated code uses the default, which might not always be the most appropriate choice without further checks.",gpt-3.5-turbo,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Calculate IQR for 'volatile acidity'
Q1 = df['volatile acidity'].quantile(0.25)
Q3 = df['volatile acidity'].quantile(0.75)
IQR = Q3 - Q1

# Find potential outliers for 'volatile acidity'
outliers_volatile_acidity = df[(df['volatile acidity'] < (Q1 - 1.5 * IQR)) | (df['volatile acidity'] > (Q3 + 1.5 * IQR)]

# Calculate IQR for 'residual sugar'
Q1 = df['residual sugar'].quantile(0.25)
Q3 = df['re...",4,"The generated code has significant issues that prevent it from correctly fulfilling the task requirements.

**Critical Errors/Missing Functionality:**

1.  **Incorrect Winsorization Logic (Major Flaw):** The most critical error is in the winsorization section.
    *   It tries to use `Q1` and `Q3` variables *after* they have been redefined for 'residual sugar' in the 'volatile acidity' winsorization step. This means `Q1` and `Q3` used for 'volatile acidity' winsorization are actually the ones calculated for 'residual sugar'. This will lead to incorrect capping.
    *   Similarly, `Q1` and `Q3` for 'residual sugar' winsorization are correctly calculated, but the previous issue undermines the approach.
    *   It attempts to modify the `df` DataFrame in place **before** calculating the 'before' statistics. This is a fundamental logical error. The ""before"" statistics *must* be calculated on the original, untransformed data. The generated code calculates the 'before' statistics (`before_mean_volatile_acidity`, etc.) *after* the `df` has already been modified (winsorized). This means `before_mean_volatile_acidity` and `after_mean_volatile_acidity` will be the same, which is incorrect.
    *   It fails to create a *new* dataset version. It modifies the original `df` directly, which makes it impossible to genuinely compare ""before"" and ""after"" if the 'before' statistics are not captured first. The task explicitly asks for ""a new dataset version where these identified outliers are capped"".

2.  **Redundant and Incorrect Comparison:** Because of the in-place modification before capturing 'before' statistics, the ""before"" and ""after"" means/stds for each column will always be identical in the output, which is meaningless.

3.  **Lack of Structure/Modularity:** The code copies and pastes the IQR calculation and outlier identification logic for each column. While functional for two columns, this is not robust or scalable for more columns. The ground truth uses a loop, which is much better practice.

**Minor Issues/Improvements:**

*   **Readability:** The ground truth code includes helpful print statements for clarity and to show the steps being performed. The generated code is very minimal in its output, making it harder to follow the process.
*   **Variable Naming:** Reusing `Q1`, `Q3`, `IQR` for different columns without proper scoping or renaming (e.g., `Q1_sugar`, `Q3_sugar`) makes the code prone to logical errors, as seen in the winsorization bug.
*   **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError`, which is good practice. The generated code lacks this.
*   **Completeness:** While it attempts to address all parts of the task, the critical flaw in the processing of ""before"" vs. ""after"" makes it incomplete in its functionality.

In summary, the core logic for comparing before and after treatment is fundamentally flawed due to in-place modification of the DataFrame before capturing the 'before' state, and the winsorization itself uses incorrect bounds for 'volatile acidity'.",gpt-3.5-turbo,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

heart_data = pd.read_csv('heart.csv')

sns.countplot(data=heart_data, x='ChestPainType', hue='HeartDisease')
plt.title('Count of Heart Disease for Each Chest Pain Type')
plt.xlabel('Chest Pain Type')
plt.ylabel('Count')
plt.legend(title='Heart Disease', labels=['No', 'Yes'])
plt.show()
```",6,"The generated code correctly addresses the core task of creating a grouped bar chart for `HeartDisease` by `ChestPainType`. It loads the data, uses `seaborn.countplot` effectively, and includes a title, labels, and a legend.

However, there are several areas for improvement:
1.  **Robustness/Error Handling**: It lacks any error handling for `FileNotFoundError` or other potential issues when loading the CSV, which is present in the ground truth.
2.  **Legend Clarity**: The `labels=['No', 'Yes']` in `plt.legend` is manually applied. While it works for this specific binary case, it's not robust. The ground truth explicitly maps 0 and 1 to more descriptive labels ('No Heart Disease', 'Heart Disease') before plotting, which makes the legend more informative and dynamic.
3.  **Encapsulation**: The code is not encapsulated within a function, making it less reusable compared to the ground truth which uses a structured function with parameters.
4.  **Plot Customization**: It doesn't incorporate `figsize` or `palette` for better visualization or `tight_layout` for proper spacing, which are good practices.
5.  **`ChestPainType` Order**: The ground truth explicitly sets the categorical order for `ChestPainType`, ensuring consistent visualization. The generated code relies on the default order, which might not always be desirable.

Overall, it's functional but could be significantly improved in terms of robustness, readability, and best practices for data visualization.",gpt-3.5-turbo,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"import pandas as pd

# Load the dataset
data = pd.read_csv('spotify_2023.csv')

# Clean the 'in_shazam_charts' column
data['in_shazam_charts'] = pd.to_numeric(data['in_shazam_charts'].str.replace(',', ''), errors='coerce')
print('Number of NaNs in in_shazam_charts:', data['in_shazam_charts'].isnull().sum())

# Group the data by 'key' and 'mode' and calculate averages
grouped_data = data.groupby(['key', 'mode'])[['danceability_%', 'valence_%']].mean()
print(grouped_data)",9,"The generated code correctly addresses all requirements of the task. It loads the CSV, cleans the `in_shazam_charts` column by removing commas and converting to numeric, reports the count of NaNs, and then groups the data by `key` and `mode` to calculate the average `danceability_%` and `valence_%`. The code is concise and efficient.

One minor point that could be slightly improved for robustness (as seen in the ground truth) is explicitly converting `in_shazam_charts` to string *before* calling `.str.replace(',', '')`. While `pd.to_numeric` with `errors='coerce'` often handles mixed types, ensuring the column is string via `astype(str)` before string operations is a slightly more robust practice. However, in this specific case, the generated code's approach is likely to work for typical CSV data and is very clean. The output format for the NaN count is also slightly less descriptive than the ground truth but still perfectly acceptable.",gpt-3.5-turbo,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Load the dataset
data = pd.read_csv('spotify_2023.csv')

# Ensure streams column is numeric
data['streams'] = pd.to_numeric(data['streams'], errors='coerce')
data = data.dropna(subset=['streams', 'in_spotify_playlists', 'energy_%'])

# Select features and target
X = data[['in_spotify_playlists', 'energy_%']]
y = dat...",9,"The generated code is highly correct and complete, addressing almost all requirements. It successfully loads the dataset, handles the `streams` column coercing non-numeric values to NaN and then drops rows with NaNs in relevant columns, selects the correct features and target, splits the data, trains a Linear Regression model, and evaluates it reporting R2 and MSE.

The only minor point of improvement compared to the ground truth is the lack of a `try-except` block for `pd.read_csv`, which would make the code more robust to `FileNotFoundError`. The ground truth also includes print statements to show progress and details about data shapes and dropped rows, which are good practices for readability and debugging but not strictly required by the prompt for functionality. However, for a data science benchmark, the core analytical steps are performed perfectly.",gpt-3.5-turbo,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Combine released_year, released_month, and released_day into a single datetime column
df['release_date'] = pd.to_datetime(df[['released_year', 'released_month', 'released_day']], errors='coerce')

# Create a new feature days_since_release
df['days_since_release'] = (pd.Timestamp('2023-12-31') - df['release_date']).dt.days

# Display track_name, release_date, and days_since_release for the first 5 tracks
pri...",8,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, successfully combines the date components into a `release_date` column with error handling using `errors='coerce'`, and calculates `days_since_release` relative to '2023-12-31'. It also correctly displays the required columns for the first 5 tracks.

The strong points are its conciseness and effective use of pandas functionalities, particularly `pd.to_datetime` with a dictionary-like input for date components and `errors='coerce'`.

The primary area for minor improvement relates to the data type of `days_since_release`. Although the `dt.days` attribute usually returns integers, if `release_date` contains `NaT` values (which can happen due to `errors='coerce'`), the `days_since_release` column will contain `NaN` values. Pandas then typically casts the column to a float type to accommodate these `NaN`s, even though the requirement specified an *integer*. To strictly adhere to the ""integer"" type while allowing for missing values, using `astype('Int64')` (pandas nullable integer type) would be more robust, as demonstrated in the ground truth. However, for practical purposes, the current implementation is often acceptable and still fulfills the spirit of the requirement.

Additionally, the ground truth includes more robust error handling for file loading and provides comments, which improves readability and maintainability, though these were not explicitly requested as part of the core functionality.",gpt-3.5-turbo,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"import pandas as pd

data = pd.read_csv('youtube_trendings.csv')

data['Video views'] = pd.to_numeric(data['Video views'].str.replace(',', ''), errors='coerce').fillna(0)
data['Likes'] = pd.to_numeric(data['Likes'].str.replace(',', ''), errors='coerce').fillna(0)

data['Category'] = data['Category'].fillna('Unknown')

grouped_data = data.groupby('Category').agg({'Video views': 'sum', 'Likes': 'sum'})
print(grouped_data)",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans ""Video views"" and ""Likes"" columns by removing commas, converting to numeric, and filling NaNs with 0. It also correctly fills missing ""Category"" values with ""Unknown"" and then groups the data to calculate the sum of views and likes per category. The code is concise and efficient.

Strong points:
- Correctly uses `pd.to_numeric` with `errors='coerce'` and `fillna(0)` for robust numerical conversion and NaN handling.
- Efficiently handles the string replacement and type conversion in a single line.
- Correctly fills missing 'Category' values.
- Correctly groups by 'Category' and sums the required columns.
- The output format is as expected (`print(grouped_data)`).

Minor areas for improvement (not critical for this task but good practice):
- The ground truth includes robust error handling for `FileNotFoundError` and checks for column existence. While not explicitly asked for in the prompt's requirements for *this specific task*, in a real-world scenario, checking if columns like 'Video views', 'Likes', and 'Category' actually exist before attempting operations on them would make the code more robust against malformed or unexpected datasets. The prompt's focus was on the data transformation logic given the columns *would* exist, so the generated code is perfectly acceptable under that interpretation.",gpt-3.5-turbo,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the Likes column
df['Likes'] = df['Likes'].str.replace(',', '')
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

# Fill missing values in the Category column
df['Category'] = df['Category'].fillna('Unknown')

# Create two groups of cleaned Likes values
music_likes = df[df['Category'] == 'Music']['Likes']
sports_likes = df[df['C...",7,"The generated code correctly addresses most of the task requirements but has a few omissions and minor issues that prevent it from scoring higher.

**Strong Points:**
*   Correctly loads the dataset.
*   Correctly cleans `Likes` column by removing commas, converting to numeric, and filling NaNs with 0.
*   Correctly fills missing `Category` values with 'Unknown'.
*   Correctly creates the `music_likes` and `sports_likes` groups.
*   Performs the independent two-sample t-test using `scipy.stats.ttest_ind`.
*   Correctly prints the t-statistic and p-value.
*   Correctly interprets the p-value against the alpha=0.05 and states whether to reject the null hypothesis.
*   States the null and alternative hypotheses, although not explicitly printed as requested in the task.

**Areas for Improvement / Missing Aspects:**
*   **Missing `equal_var=False` in t-test:** The ground truth uses `equal_var=False` for Welch's t-test, which is generally recommended when equal variances cannot be assumed. The generated code defaults to `equal_var=True`, which assumes equal population variances. This is a subtle but important statistical consideration for robust analysis.
*   **Hypotheses statement:** While the hypotheses are correctly stated in comments, the task specifically asked to ""Clearly state the null and alternative hypotheses for this test."" This would imply printing them out, similar to the ground truth.
*   **Robustness of `Likes` cleaning:** `df['Likes'] = df['Likes'].str.replace(',', '')` will fail if the 'Likes' column contains non-string types initially, or NaNs, causing an AttributeError. The ground truth first converts to string (`.astype(str)`) to handle this more robustly, ensuring `.str` accessor always works.
*   **No handling for `FileNotFoundError`:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust. The generated code does not.
*   **Clarity in output:** The output could be more clearly formatted with descriptive print statements, similar to the ground truth, to indicate each step's completion and the final interpretation more explicitly. The t-statistic and p-value are printed without formatting to a specific decimal place.",gpt-3.5-turbo,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"import pandas as pd

df = pd.read_csv('youtube_trendings.csv')

df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)
df['Dislikes'] = pd.to_numeric(df['Dislikes'].str.replace(',', ''), errors='coerce').fillna(0)

df['TotalEngagement'] = df['Likes'] + df['Dislikes']
df['LikeDislikeRatio'] = df['Likes'] / (df['Dislikes'] + 1)

print(df[['Video', 'TotalEngagement', 'LikeDislikeRatio']])",9,"The generated code correctly implements all specified requirements: loading the dataset, cleaning 'Likes' and 'Dislikes' columns (removing commas, converting to numeric, handling NaNs by filling with 0), creating `TotalEngagement`, creating `LikeDislikeRatio` with the +1 in the denominator, and displaying the required columns. The implementation is concise and efficient.

One minor area for improvement, though not strictly required by the prompt's output format, would be to include error handling for the `pd.read_csv` call, similar to the ground truth. This makes the code more robust if the dataset file is not found. However, for a direct solution to the described task, the code is very good.",gpt-3.5-turbo,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"import pandas as pd

df = pd.read_csv('mobiles_2025.csv')

df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''), errors='coerce')
df['Screen Size'] = pd.to_numeric(df['Screen Size'].str.replace(' inches', ''), errors='coerce')

grouped_data = df.groupby(['Company Name', 'Launched Year']).agg({'Launched Price (USA)': 'mean', 'Screen Size': 'mean'})

print(grouped_data)",9,"The generated code correctly performs all the required steps: loading the CSV, cleaning both price and screen size columns by removing specific strings and converting to numeric (handling errors), grouping by `Company Name` and `Launched Year`, and calculating the mean for the cleaned columns. The output format is also correct.

The only minor area for improvement is that the cleaning of the `Launched Price (USA)` column could be slightly more robust by ensuring the column is treated as a string before applying `.str` methods, especially if there's a chance of mixed types, similar to the explicit `astype(str)` in the ground truth. While `pd.to_numeric` typically handles this gracefully with `errors='coerce'`, explicitly ensuring string type for `.str.replace` can prevent potential warnings or unexpected behavior in edge cases. However, for typical CSV data where these columns are read as objects (strings), the current implementation generally works correctly. It successfully reuses the original column names for the cleaned data, which is acceptable for this task, though creating new columns explicitly named for cleaned data can sometimes improve readability and prevent overwriting original data if it were needed for other purposes.",gpt-3.5-turbo,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"import pandas as pd
import re

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Define function to clean camera columns
def clean_camera_spec(camera_spec):
    mp = re.search(r'\d+', camera_spec)
    if mp:
        return int(mp.group())
    else:
        return 0

# Apply function to create cleaned columns
df['Cleaned Front Camera'] = df['Front Camera'].apply(clean_camera_spec)
df['Cleaned Back Camera'] = df['Back Camera'].apply(clean_camera_spec)

# Create Total Camera MP column
df['...",7,"The generated code correctly performs all the required steps: loading the dataset, defining and applying the cleaning function, creating the `Total Camera MP` feature, and displaying the specified columns for the first 10 rows.

However, there is a minor omission in the `clean_camera_spec` function's handling of non-string inputs (like `NaN` values which can come from the CSV). While `re.search` might gracefully handle some non-string types by raising a TypeError, it's best practice to explicitly check for and convert inputs to strings or handle `NaN`s, as demonstrated in the ground truth. The ground truth also includes error handling for `FileNotFoundError` and verbose print statements which were part of the ground truth but not explicitly asked for in the prompt's requirements.

The code is otherwise concise and correct for the core logic.

Areas for improvement:
- The `clean_camera_spec` function would be more robust if it explicitly handled `NaN` or non-string inputs, similar to the ground truth.
- Adding error handling for `FileNotFoundError` when loading the CSV would make the code more robust against missing files.",gpt-3.5-turbo,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the columns
df['RAM'] = df['RAM'].str.replace(' GB', '').apply(pd.to_numeric, errors='coerce')
df['Battery Capacity'] = df['Battery Capacity'].str.replace(' mAh', '').apply(pd.to_numeric, errors='coerce')
df['Screen Size'] = df['Screen Size']....",9,"The generated code correctly performs all the steps required by the task: loading the dataset, cleaning the specified columns, handling NaNs, splitting the data, training a Linear Regression model, and evaluating it with R-squared and MAE. The approach to cleaning the columns is concise and effective using `str.replace` and `pd.to_numeric` with `errors='coerce'`. The specific columns are dropped after cleaning.

The minor point for improvement, preventing a perfect 10, is that the `str.replace` patterns for `RAM`, `Battery Capacity`, and `Screen Size` include a leading space (`' GB'`, `' mAh'`, `' inches'`). While this works for the current dataset, it's slightly less robust than explicitly stripping whitespace after replacing the unit (e.g., `df['RAM'].str.replace('GB', '').str.strip().apply(...)`). However, for the given problem description and likely dataset, this is a very minor detail and doesn't affect correctness. Additionally, the print statements for the evaluation metrics could benefit from f-string formatting to control decimal places, similar to the ground truth, for better readability and consistency in reporting. These are very minor points.",gpt-3.5-turbo,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
from scipy.stats import chi2_contingency, ttest_ind, f_oneway
import numpy as np

# Load the Titanic dataset
df = pd.read_csv('titanic.csv')

# 1) Chi-square test for association between Pclass and survival
ct = pd.crosstab(df['Pclass'], df['Survived'])
chi2, p1, dof, exp = chi2_contingency(ct)
effect_size1 = np.sqrt(chi2 / len(df))

# 2) T-test to compare ages of survivors vs non-survivors
survived_age = df[df['Survived'] == 1]['Age'].dropna()
not_survived_age = df...",4,"The generated code attempts to address all parts of the prompt but falls short in critical areas, leading to incorrect or incomplete statistical analysis.

**Major Issues:**

1.  **Incorrect Effect Size Calculations**:
    *   **Chi-square**: `effect_size1 = np.sqrt(chi2 / len(df))` is not Cramer's V. Cramer's V requires `min(rows-1, cols-1)` in the denominator, and the formula used is too simplistic and often incorrect for a proper interpretation of association strength. `len(df)` is the total number of observations, which is part of the denominator but `min_dim` is missing.
    *   **T-test**: `effect_size2` uses a formula for Cohen's d that implicitly assumes equal variances and uses a simplified pooled standard deviation. While closer, it's not the robust Cohen's d commonly calculated which often involves the pooled standard deviation based on sample sizes, or a formula that doesn't assume equal variances if `equal_var=False` is used in the t-test. The ground truth correctly handles the pooled standard deviation calculation.
    *   **ANOVA**: `effect_size3` uses `np.sqrt(f / (f + (len(df) - len(fare_data))))` which is a very simplified and incorrect approximation of Eta-squared. Eta-squared requires calculation of Sums of Squares (SSB and SST), which is a core part of ANOVA effect size. The generated code did not perform these calculations.

2.  **Missing Interpretation Logic**: The code declares conclusions *before* performing the p-value comparison (`p1 < alpha`, etc.). It hardcodes ""Reject null hypothesis"" and ""significant difference"" for all tests, which is fundamentally incorrect. Statistical conclusions *must* be conditional on the p-value result compared to alpha. This is a critical statistical methodology error.

3.  **Missing Alpha Level Definition and Use**: While `alpha = 0.05` is defined, it is only used *after* the results are printed and not used to make conditional conclusions.

4.  **No `dropna()` for `Fare` in ANOVA**: While Titanic's Fare column is usually complete, the ground truth correctly included `dropna()` for robustness, which is good practice. The generated code relies on the assumption it's complete.

5.  **No Handling of Missing Values for Age Robustly (Minor)**: While `dropna()` is used for age, the ground truth explicitly creates `df_age_cleaned` to ensure further operations on age use a consistent, cleaned subset, which is better practice.

6.  **Lack of Robustness Checks**:
    *   No handling for `titanic.csv` not found.
    *   No empty group checks for t-test or ANOVA.
    *   No Levene's test for variance equality assumption for t-test or ANOVA, which influences the interpretation and even the choice of t-test (e.g., Welch's t-test). The ground truth correctly implements this and adjusts `equal_var` for t-test.

7.  **Output Clarity**: The ground truth provides more detailed and structured output, including hypotheses and clearer explanations of results based on p-values.

In summary, while the code attempts the required tests, the incorrect effect size calculations and the hardcoded interpretations (before checking p-values) make it statistically unsound and prevent it from fulfilling the prompt's requirements regarding accurate interpretation and effect size calculation.",gpt-3.5-turbo,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Group data by Year and calculate average Rating and Runtime
grouped_data = df.groupby('Year').agg({'Rating': 'mean', 'Runtime': 'mean'}).reset_index()

# Create a line plot showing the trend of average Rating over the Year
plt.figure(figsize=(10, 6))
plt.plot(grouped_data['Year'], grouped_data['Rating'], marker='o', color='b')
plt.title('Average Rating Trend over the Years')
...",7,"The generated code correctly loads the data, groups it, calculates the required averages, and generates a line plot with a title and labels. This addresses the main requirements of the task. However, it lacks robustness that the ground truth provides. Specifically, it does not include any error handling (e.g., `FileNotFoundError`, `KeyError`) or data cleaning for potential issues in the `Year` or `Runtime` columns. The ground truth code correctly anticipates common data quality issues in real-world datasets (like `Year` being non-numeric or `Runtime` needing string extraction) and includes a more comprehensive function structure. The generated code is good and functional for a clean dataset but could be improved by adding more robust error handling and data cleaning steps.",gpt-3.5-turbo,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create two groups of movies based on the `Year` column
group1 = df[df['Year'] < 1990]['Rating']
group2 = df[df['Year'] >= 1990]['Rating']

# Perform independent two-sample t-test
t_stat, p_val = ttest_ind(group1, group2)

# Null hypothesis: There is no difference in the mean `Rating` between movies released before 1990 and movies released in or after 1990.
# Alternative hyp...",8,"The generated code correctly performs the core task of loading the data, splitting it into two groups, and conducting an independent two-sample t-test. It correctly prints the t-statistic and p-value and interprets the result based on the given significance level.

Strengths:
- Correctly uses pandas to load data and filter groups.
- Correctly uses `scipy.stats.ttest_ind` for the t-test.
- Correctly states the hypotheses (though not explicitly printed as requested, they are present as comments).
- Correctly interprets the p-value against alpha.

Areas for Improvement (why not a perfect 10):
- **Hypotheses Statement:** The task explicitly asks to ""Clearly state the null and alternative hypotheses for this test."" The generated code includes them as inline comments, which is not as explicit as printing them to the console as done in the ground truth.
- **Robustness/Error Handling:** The ground truth includes `try-except` for `FileNotFoundError` and explicit `pd.to_numeric` calls with `errors='coerce'` and `dropna` for better handling of potential data type issues or missing files. The generated code assumes clean data and file presence.
- **Output Clarity:** While it prints the t-statistic and p-value, adding descriptive labels like in the ground truth (e.g., ""T-statistic: X, P-value: Y"") improves readability. The interpretation could also be slightly more elaborate, explaining why the null is rejected/failed to be rejected, as in the ground truth.
- **`equal_var` parameter:** The `ttest_ind` function defaults to `equal_var=True`. While often acceptable for introductory examples, explicitly setting this parameter (`equal_var=True` or `False` based on assumptions/tests) makes the code more robust and transparent about the underlying statistical assumptions. The ground truth explicitly states this.",gpt-3.5-turbo,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create a new column 'Decade'
df['Decade'] = pd.cut(df['Year'], bins=[1920, 1929, 1939, 1949, 1959, 1969, 1979, 1989, 1999, 2009, 2019, 2021],
                      labels=['1920s', '1930s', '1940s', '1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s'])

# Display title, year, and decade for first 10 movies
print(df[['Title', 'Year', 'Decade']].head(10))",8,"The generated code correctly performs all the requested tasks: loading the CSV, creating the `Decade` column, and displaying the required columns for the first 10 movies. The use of `pd.cut` is a valid and generally good approach for binning.

However, there are a few areas for improvement:
1.  **Hardcoded Bins:** The `bins` and `labels` for `pd.cut` are hardcoded. While it works for this specific dataset and its known year range, it's not robust. If the dataset contained movies from the 1900s or 2020s (beyond 2019), the code would fail to categorize them or produce NaNs. A more robust solution dynamically calculates decades. The ground truth's `((df['Year'] // 10) * 10).astype(str) + 's'` approach is more flexible as it works for any year range.
2.  **Edge Case for `pd.cut`:** The upper bound for the last bin (2021) covers up to 2020. However, if there was a movie from 2020, it would fall into the '2010s' bucket based on the hardcoded bin definition. The prompt implied `1990-1999` for '1990s', but `pd.cut`'s default behavior is `(left, right]`. To perfectly align with the `1990-1999` definition, the bins should be `[1990, 2000)` type or more carefully managed. For `pd.cut`, `right=False` would be needed if the definition was inclusive on the left and exclusive on the right, but the chosen bins and labels imply exclusive on the left and inclusive on the right (e.g., (1989, 1999]), which means 1990-1999 would correctly be 1990s. The issue is more the manual definition of bins that might not cover all possible year ranges or be easily adaptable.
3.  **No error handling:** The code assumes the file exists and has the correct columns. The ground truth adds robust error handling for `FileNotFoundError` and `KeyError`, which is good practice for production-ready code.
4.  **No explicit functions:** While not strictly required by the prompt, encapsulating logic in functions (as in the ground truth) improves modularity, readability, and testability.

Overall, it's a good solution that directly addresses the prompt's requirements effectively, but could be more robust and structured.",gpt-3.5-turbo,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Impute missing values for numerical columns
df['culmen_length_mm'].fillna(df['culmen_length_mm'].mean(), inplace=True)
df['culmen_depth_mm'].fillna(df['culmen_depth_mm'].mean(), inplace=True)
df['flipper_length_mm'].fillna(df['flipper_length_mm'].mean(), inplace=True)
df['body_mass_g'].fillna(df['body_mass_g'].mean(), inplace=True)

# Impute missing values for categorical column
df['sex'].fillna(df['sex'].mode()[0], inpl...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, imputes missing numerical values with their respective column means, imputes missing categorical 'sex' values with the mode, and then correctly groups the data and calculates the average `body_mass_g`. The code is concise and directly implements the requested operations.

One minor point for improvement, compared to the ground truth, is the explicit check for `isnull().any()` before attempting imputation. While `fillna` with `mean()` or `mode()` doesn't throw an error if no NaNs are present, adding an if-condition can sometimes make the code marginally more robust or explicit about its intent (though the performance impact is negligible for this simple case). Also, the ground truth code includes error handling for `FileNotFoundError` and `KeyError`, which is good practice for production-ready code but not strictly required by the prompt's core task (data manipulation). Given the prompt's focus on the data manipulation logic, the generated code's directness is acceptable.",gpt-3.5-turbo,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the data
filtered_df = df[(df['species'] == 'Adelie') | (df['species'] == 'Gentoo')]

# Remove rows with missing values
filtered_df = filtered_df.dropna(subset=['flipper_length_mm', 'species'])

# Independent two-sample t-test
adelie_lengths = filtered_df[filtered_df['species'] == 'Adelie']['flipper_length_mm']
gentoo_lengths = filtered_df[filtered_df['species'] == 'Gent...",8,"The generated code correctly performs the requested operations: loading, filtering, NA removal, and performing a t-test. It correctly states the hypotheses (as comments) and interprets the results based on the provided alpha.

Strengths:
- Correctly loads and filters the data.
- Correctly removes NA values in the specified columns for the filtered subset.
- Correctly performs an independent two-sample t-test using `scipy.stats.ttest_ind`.
- Correctly states the null and alternative hypotheses.
- Correctly prints the t-statistic and p-value.
- Correctly interprets the p-value against `alpha = 0.05`.

Areas for improvement:
- **Hypotheses Statement:** While the hypotheses are stated, they are only in comments. The ground truth prints them explicitly, which is better for fulfilling the requirement ""Clearly state the null and alternative hypotheses for this test.""
- **Robustness/Error Handling:** The ground truth includes `try-except` blocks for file loading and checks for empty groups after filtering/NA removal, which makes the code more robust. The generated code lacks these checks.
- **`ttest_ind` `equal_var` parameter:** The generated code uses the default `equal_var=True` for `ttest_ind`. The ground truth explicitly uses `equal_var=False` (Welch's t-test), which is generally more robust when population variances are unknown or unequal. While the problem didn't specify, it's a good practice to consider. However, for a basic task, the default might be acceptable.
- **Clarity of output:** The ground truth provides more descriptive print statements at each step, showing dataset shapes and NA removal counts, which enhances clarity and debugging. The generated code only prints the final results.",gpt-3.5-turbo,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression

# Load the dataset
df = pd.read_csv('penguins.csv')

# Select features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]
y = df['species']

# Drop rows with missing values
df.dropna(subset=['species', 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_...",6,"The generated code addresses most of the task requirements but has a critical flaw in how it handles missing values, leading to incorrect data separation.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly selects the specified features `X` and target `y` initially.
*   Correctly encodes the categorical `species` target using `LabelEncoder`.
*   Correctly splits the data into training and testing sets (80/20, `random_state=42`).
*   Correctly trains a Logistic Regression model with `max_iter=1000`.
*   Correctly calculates and prints the accuracy score.
*   Includes all necessary imports.

**Areas for Improvement / Critical Flaw:**
*   **Incorrect Handling of Missing Values:** The most significant issue is the order of operations for dropping NA values.
    *   The features `X` and target `y` are extracted *before* the `dropna` operation is applied to the DataFrame `df`.
    *   This means `X` and `y` still contain rows with missing values that should have been removed.
    *   When `df.dropna(...)` is called, it modifies `df` in place, but `X` and `y` (which were copies of columns from the original `df`) are not updated.
    *   Consequently, when `train_test_split` is called, it operates on `X` and `y` that may still contain `NaN`, which will likely cause issues with the model training, or at best, train on unclean data for `X`. `LabelEncoder` can handle NaNs by default so `y` might not immediately throw error but the data will still be skewed.
    *   The correct approach (as in the ground truth) is to first clean the *relevant subset* of the DataFrame (`df_selected` in ground truth) by dropping NAs and *then* extract `X` and `y` from this cleaned DataFrame.
*   **No Error Handling for File Not Found:** The code assumes `penguins.csv` always exists, lacking the `try-except` block present in the ground truth for robust file loading.
*   **Minor Missing `accuracy_score` import/usage:** While `model.score()` is acceptable for accuracy, the ground truth explicitly uses `sklearn.metrics.accuracy_score`, which is often preferred for clarity (though `model.score()` for classifiers typically defaults to accuracy). This is a minor point but makes it deviate slightly from a common practice in benchmarks.

Due to the critical flaw in data preparation (specifically around the `dropna` step and its impact on `X` and `y`), the model is not trained on the truly cleaned data as intended, leading to a correctness issue. This significantly impacts the reliability of the trained model and its reported accuracy.",gpt-3.5-turbo,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows with NA values in body_mass_g or species
df = df.dropna(subset=['body_mass_g', 'species'])

# Calculate average body_mass_g for each species
avg_body_mass = df.groupby('species')['body_mass_g'].mean().reset_index()

# Create a bar plot
plt.figure(figsize=(10, 6))
sns.barplot(x='species', y='body_mass_g', data=avg_body_mass)
plt.title('Average Body ...",9,"The generated code correctly addresses all requirements of the task. It successfully loads the dataset, handles NA values in the specified columns, calculates the average body mass per species, and generates a well-labeled bar plot. The use of pandas for data manipulation and seaborn/matplotlib for plotting is appropriate.

**Strong points:**
*   Correctly loads the CSV.
*   Correctly uses `dropna(subset=[...])` for targeted NA removal.
*   Correctly uses `groupby().mean().reset_index()` to calculate and format the averages.
*   Correctly uses `seaborn.barplot` for visualization.
*   Includes a title and labeled axes as requested.

**Areas for minor improvement (to reach a perfect 10):**
*   **Error Handling (File Not Found):** The generated code lacks error handling for `FileNotFoundError` when reading the CSV. The ground truth includes a `try-except` block which is good practice for robust code.
*   **Clarity of Labels:** While the labels are correct, the ground truth's approach of using `TARGET_COLUMN.replace(""_"", "" "").title()` makes the labels dynamically generated from the column name, which can be more robust if column names change slightly or are more complex. However, for this specific task, hardcoding 'Average Body Mass (g)' is perfectly understandable and clear.
*   **Aesthetics (Optional):** The ground truth adds `palette='viridis'` and `plt.xticks(rotation=45, ha='right')` and `plt.grid(axis='y', linestyle='--', alpha=0.7)` and `plt.tight_layout()`. These are minor aesthetic enhancements that improve readability but are not strictly required by the prompt's core functional requirements.

Overall, the generated code is highly effective and directly solves the problem.",gpt-3.5-turbo,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows with NA values in specific columns
df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'], inplace=True)

# Create new feature culmen_area
df['culmen_area'] = df['culmen_length_mm'] * df['culmen_depth_mm']

# Calculate Pearson correlation matrix
corr_matrix = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_...",9,"The generated code successfully completes almost all aspects of the task.

**Strong Points:**
*   **Correctness:** It correctly loads the dataset, drops NA values from the specified columns, creates the `culmen_area` feature, calculates the Pearson correlation matrix, and visualizes it as an annotated heatmap with the correct title.
*   **Conciseness:** The code is quite concise and directly addresses the requirements.
*   **Imports:** Necessary libraries are correctly imported.

**Areas for Minor Improvement (for a perfect score):**
*   **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. While not explicitly asked for, robust code often includes this, especially in a production-like environment. The generated code assumes the file will always be present.
*   **Clarity/Readability of Dropping NAs:** The `inplace=True` argument used for `dropna` is functional but can sometimes lead to unexpected behavior if not handled carefully in larger codebases. While acceptable here, creating a new DataFrame (`df_cleaned = df.dropna(...)`) is often preferred for clarity and to avoid modifying the original DataFrame directly, as shown in the ground truth.
*   **Figure Size:** The `figsize=(10, 8)` is a good choice for the heatmap, slightly larger than the ground truth's `(8, 6)`, which can improve readability.
*   **Layout Adjustments:** The ground truth includes `plt.xticks(rotation=45, ha='right')`, `plt.yticks(rotation=0)`, and `plt.tight_layout()`. While the generated code's plot is likely readable for this small matrix, these adjustments can be beneficial for larger correlation matrices to prevent label overlapping and improve aesthetics.

Overall, the generated code is very good and directly addresses the prompt effectively. The minor points are about robustness and best practices rather than incorrectness for this specific task.",gpt-3.5-turbo,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Selecting features and target variable
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
target = 'species'

# Impute NA values for numerical features
numerical_features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df[numerical_features] = df[numerical_features].fillna(df[numerical_features].mean())

# Impute NA values for the sex colu...",7,"The generated code addresses most of the requirements but has a few omissions and potential issues that prevent it from being a perfect solution.

**Strong Points:**
*   Correctly loads the data.
*   Correctly identifies and imputes numerical NA values with the mean.
*   Correctly fills NA values in the 'sex' column with 'Unknown'.
*   Correctly maps 'MALE', 'FEMALE', and 'Unknown' to 0, 1, and -1 respectively.
*   Correctly applies one-hot encoding to the 'island' column.

**Areas for Improvement / Issues:**
1.  **Feature Selection Mismatch:** The code selects `features` at the beginning, but after one-hot encoding `island`, the original `island` column is replaced by `island_Adelie`, `island_Biscoe`, etc. When `print(df[features].shape)` and `print(df[features].head())` are called, it will raise a `KeyError` because 'island' is no longer a column in `df`. The printed output should reflect the final processed feature set, including the new one-hot encoded columns.
2.  **Missing Target Variable:** The task explicitly states the goal is to predict `species`, and the ground truth separates features (X) from the target (y). The generated code doesn't explicitly separate or mention the target variable in the final output, meaning `df[features]` only prints the *selected* features, not necessarily all *processed* features, and completely excludes the target.
3.  **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError`, which is good practice. The generated code lacks this.
4.  **Implicit DataFrame Modification:** While not strictly incorrect, the generated code modifies the `df` in place for most operations. For clarity and to avoid `SettingWithCopyWarning` in more complex scenarios, creating copies (`.copy()`) or using chained assignments can sometimes be safer, though in this linear flow, it's less of an issue.
5.  **Output Clarity:** The final print statements `'print(df[features].shape)'` and `'print(df[features].head())'` are problematic as noted in point 1. The output should clearly be the shape and head of the *processed feature matrix* (X), which would include the new one-hot encoded 'island' columns and exclude 'species'. The ground truth correctly defines `X` as `df_processed.drop(columns=[target_variable])` and prints `X.shape` and `X.head()`.",gpt-3.5-turbo,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

data = {'OrderID': [101, 102, 103, 104, 105, 106, 107], 
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 
        'Quantity': [1, 2, 1, 1, 3, 1, 1], 
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150], 
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', '...",9,"The generated code correctly performs all the required tasks: it creates the DataFrame, calculates total spending per customer, identifies and prints the top 3 customers by spending, and calculates the total quantity sold per category. The code quality is good, with proper imports and a clear, concise structure.

One minor point for improvement, which is not a correctness issue but a presentation one, is that the output for the top 3 customers is a pandas Series, whereas the ground truth formats it as a more readable string for each customer. However, the task specifically asked to ""identify and print the 'CustomerID' and their total spending"", and printing the Series achieves this. The ground truth also explicitly formats the spending with two decimal places and commas, which the generated code doesn't do, but again, this is a minor formatting choice. For a more direct comparison to the ground truth's specific print format, an iteration over the series would be needed, but the current output fulfills the requirement.",gpt-3.5-turbo,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
impor...",7,"The generated code addresses most of the requirements but has some notable areas for improvement and a few correctness issues compared to the ground truth.

**Strong Points:**
*   Correctly loads data and performs basic preprocessing (handling missing values, dropping irrelevant columns).
*   Uses `ColumnTransformer` and `Pipeline` for efficient preprocessing and model training.
*   Splits data into training and testing sets.
*   Trains and evaluates a logistic regression model with multiple metrics (accuracy, precision, recall, F1, ROC-AUC).
*   Attempts to display feature coefficients.
*   Attempts to visualize the confusion matrix and ROC curve.
*   Performs cross-validation.

**Areas for Improvement / Correctness Issues:**

1.  **Data Preprocessing - Feature Engineering:** The generated code misses the ""FamilySize"" feature engineering step (`df['FamilySize'] = df['SibSp'] + df['Parch'] + 1`) and dropping of original `SibSp` and `Parch` columns, which was present in the ground truth. Instead, it defines `SibSp` and `Parch` as numeric features, which is not ideal, as they should be combined into a single feature for better interpretability and model performance.
2.  **Feature Coefficient Interpretation:**
    *   The `feature_names` extraction logic for coefficients is flawed. `model.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names(categorical_features)` only gets names for the categorical features, not the numerical ones. It also doesn't handle the potential `_` prefix that `get_feature_names_out()` provides when used with `ColumnTransformer`. The ground truth correctly uses `get_feature_names_out()` directly on the *fitted* preprocessor, which provides names for all transformed features (e.g., `num__Age`, `cat__Sex_male`).
    *   The interpretation of coefficients is missing. The ground truth provides detailed explanation of positive/negative coefficients and magnitude.
3.  **Visualization:**
    *   **Confusion Matrix:** The generated code manually plots the confusion matrix using `plt.imshow`. While functional, it's less robust and user-friendly than `sklearn.metrics.ConfusionMatrixDisplay.from_estimator` used in the ground truth, which handles labels, titles, and clearer plotting directly. The text overlay is not formatted well (e.g., color `red` on a blue background is not ideal).
    *   **ROC Curve:** Similarly, it manually plots the ROC curve. The ground truth uses `sklearn.metrics.RocCurveDisplay.from_estimator`, which is more concise and convenient, and also includes the random classifier line for better context.
4.  **Cross-Validation:** The generated code uses `cross_val_score` but only displays the accuracy scores by default. The ground truth uses `cross_validate` with a `scoring` dictionary to get and display mean and standard deviation for *all* relevant metrics (precision, recall, f1, roc-auc) across folds, providing a much more comprehensive assessment of model stability.
5.  **Reproducibility:** Missing `np.random.seed(42)` at the beginning, which affects `train_test_split` and potentially `LogisticRegression` internal randomness, making results less reproducible compared to the ground truth.
6.  **Missing `ConfusionMatrixDisplay` and `RocCurveDisplay` imports:** The generated code manually plots these, so these specific imports are not strictly needed, but their absence indicates a less modern and efficient approach to visualization in sklearn.
7.  **Minor Code Style/Readability:** The code uses `print` statements without descriptive headers, making the output less organized than the ground truth. The error handling for `titanic.csv` not found is also missing.",gpt-3.5-turbo,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']), 'Ticker': ['TECHCO']*8, 'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0], 'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5], 'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0], 'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0, 157.0, 154.0], 'Volume': [1000000, 1200000, 110000...",10,"The generated code is entirely correct and complete. It successfully creates the DataFrame, sets 'Date' as the index, calculates the 3-day SMA for 'Close' (handling NaNs for the initial days), calculates the percentage change for 'Volume' (handling NaN for the first day), and prints the resulting DataFrame, all as required by the task. The imports are minimal and appropriate. The code is concise and follows best practices for pandas operations.",gpt-3.5-turbo,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"import pandas as pd

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]}

df = pd.DataFrame(data)

df['Score_...",9,"The generated code correctly performs all the required tasks: it creates the DataFrame, calculates the 'Score_Improvement' column, computes the average score improvement per subject, and identifies students meeting the specified criteria. The code is clean and uses appropriate pandas functionalities.

The only minor difference, which doesn't affect correctness but slightly a cosmetic output, is that the ground truth code explicitly iterates and prints each student ID for the final condition, whereas the generated code prints the pandas Series directly. Both are correct interpretations of ""printing the StudentID"". The ground truth also included additional print statements for intermediate steps, which were not strictly required by the prompt but are good for debugging/validation. The prompt specifically asked for 'StudentID' of the student(s) to be identified and printed, and printing the Series with the 'StudentID' column header is acceptable.",gpt-3.5-turbo,LLM_judge_config
M52,medium,Data Manipulation,,"import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

filtered_df = df[(df['Department'] == 'Tech') & (df['PerformanceSco...",9,"The generated code correctly defines the DataFrame, filters it according to the specified conditions, and accurately calculates the average number of ""ProjectsCompleted"" for the filtered subset. All requirements of the task are met.

**Strong points:**
*   Correct DataFrame creation.
*   Accurate and concise filtering using boolean indexing.
*   Correct calculation of the mean for the 'ProjectsCompleted' column.
*   Provides only the necessary Python code as requested.

**Areas for minor improvement (for a perfect score):**
*   The output format of the `print` statement is very basic (`print(average_projects_completed)`). While it prints the correct value, the ground truth provides a more user-friendly and descriptive output string using an f-string, which is generally preferred in professional settings for clarity.
*   The code doesn't include an `if not filtered_df.empty:` check like the ground truth. While for this specific dataset, the filter will always return results, in a more general scenario where the filtered DataFrame could potentially be empty, calculating `mean()` on an empty Series would result in `NaN` and might not be the desired behavior. Handling this edge case explicitly could make the code more robust. However, for a simple problem as stated, it's not a critical omission.",gpt-3.5-turbo,LLM_judge_config
M53,medium,Data Manipulation,,"import pandas as pd

# Define the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate conversion rate for users on ""Desktop""
desktop_conversion_rate = df[df[""DeviceType""] == ""Desktop""][""Co...",9,"The generated code correctly defines the DataFrame, calculates the conversion rates for Desktop and Mobile users, and prints them clearly labeled. The calculations are accurate, leveraging pandas' `mean()` method on boolean series effectively. The code is concise and easy to understand.

A minor improvement could be to handle edge cases where a `DeviceType` might not exist in the DataFrame (though with the given data, this is not an issue). If, for example, there were no 'Desktop' users, `df[df[""DeviceType""] == ""Desktop""][""Converted""].mean()` would return `NaN`, which might not be the desired output (a 0% rate would be more informative). The ground truth code handles this by checking for empty DataFrames. However, for the given problem context and data, the generated code produces the correct output, which is why it receives a high score.",gpt-3.5-turbo,LLM_judge_config
M54,medium,Data Aggregation,,"sensor_averages = df.groupby('SensorID')['AirQualityIndex'].mean()
for sensor_id, avg in sensor_averages.items():
    print(f""Average AirQualityIndex for Sensor {sensor_id}: {avg}"")",7,"The generated code correctly calculates and prints the average AirQualityIndex for each sensor. The logic for grouping by `SensorID` and calculating the mean is sound. However, the generated code snippet is incomplete as it misses the initial DataFrame creation, which is a significant part of the task prompt. It only provides the second part of the task. While the provided snippet for the calculation is correct, the overall solution is incomplete without the DataFrame definition. To achieve a higher score, the full code including the DataFrame creation should have been provided. The output format is also slightly different from the ground truth, but still clear.",gpt-3.5-turbo,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"import pandas as pd
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

df[""EngagementScore""] = df[""Likes""] + (df[""Shares""] * 2) + (df[""Comments""] * 3)

filtered_df = df...",9,"The generated code correctly performs all the required tasks:
1. Defines the DataFrame as specified.
2. Calculates the ""EngagementScore"" accurately using the given formula.
3. Filters the DataFrame to include only ""Instagram"" posts.
4. Calculates the average ""EngagementScore"" for these Instagram posts.
5. Prints the final result.

The code is concise, readable, and directly addresses the prompt's requirements.

The only minor area for improvement is the printing format. The ground truth code formats the output to two decimal places and provides a more descriptive print statement, which is good practice for user-facing output in data science tasks. The generated code simply prints the raw float value, which is acceptable but less polished. However, this does not affect the correctness of the computation. Another minor point is the ground truth's inclusion of a check for an empty filtered DataFrame, which is a robust programming practice, though not strictly necessary for this specific dataset given its structure.",gpt-3.5-turbo,LLM_judge_config
M56,medium,Data Manipulation,,"import pandas as pd

# Creating the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Femal...",10,"The generated code correctly addresses all aspects of the task:
1. It accurately defines the pandas DataFrame with the specified data and correctly converts date columns to datetime objects.
2. It correctly calculates the ""LengthOfStay"" column by subtracting AdmissionDate from DischargeDate and extracting the days.
3. It accurately filters the DataFrame to include only patients with a ""Diagnosis"" of ""Flu"".
4. It correctly calculates and prints the average ""LengthOfStay"" for the filtered Flu patients.

The code is clean, well-structured, and directly implements the requirements. It follows standard pandas practices. The omission of print statements for intermediate DataFrames (present in the ground truth) is not an error but a choice, as the task only explicitly asked to print the final average. The `copy()` method used in the ground truth when filtering (`flu_patients_df = df[df[""Diagnosis""] == ""Flu""].copy()`) is a good practice to avoid `SettingWithCopyWarning`, but its absence does not make the generated code incorrect for this specific task as no further chained assignments are made to `flu_patients`.",gpt-3.5-turbo,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}

df = pd.DataFrame(data)

sci_fi_movies = df[df[""Genre""] == ""Sc...",9,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates their average rating. It adheres to all specified task requirements. The code is clean, concise, and functions as expected.

One minor area for improvement, present in the ground truth but not strictly required by the prompt, would be to consider an edge case where no Sci-Fi movies might exist in the dataset. While it doesn't affect this specific task instance, robust code often includes such checks. Additionally, formatting the output with two decimal places for the average rating (as in the ground truth) would be a nice touch for readability, but the current output is still correct.",gpt-3.5-turbo,LLM_judge_config
M58,medium,Data Manipulation,,"import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

df = pd.DataFrame(data)

toyota_cars = df[df[""Brand""] == ""Toyota""]
average_mileage = toyota_cars[""Milea...",10,"The generated code is excellent. It correctly defines the DataFrame, filters for Toyota cars, and accurately calculates their average mileage, fulfilling all requirements of the task. The code is concise, clean, and directly addresses the prompt without unnecessary complexity. It adheres to all evaluation criteria.",gpt-3.5-turbo,LLM_judge_config
M59,medium,Data Filtering,,"import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 80]
}

df = ...",9,"The generated code correctly defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat. All functional requirements of the task are met. The code quality is good, with correct imports and a clear, concise structure that directly addresses the problem.

The only minor area for improvement, preventing a perfect score, is the output formatting. The ground truth code includes a more descriptive print statement (e.g., ""The average Attack stat for Fire-type Pokemon is: 66.67"") and handles the edge case where no Fire-type Pokemon might be found (although not strictly necessary for this specific dataset). The generated code simply prints the numerical value. While functionally correct, a more user-friendly output and robust handling for edge cases often contribute to higher code quality in a production setting.",gpt-3.5-turbo,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Load the dataset
df = pd.read_csv('titanic.csv')

# 1) Multi-panel figure
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.barplot(x='Pclass', y='Survived', data=df, ci=None)

plt.subplot(1, 3, 2)
sns.barplot(x='Sex', y='Survived', data=df, ci=None)

plt.subplot(1, 3, 3)
df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 18, 50, 100], labels=['Child', 'Adult', 'Elderly'])
sns.barplot(...",6,"The generated code addresses most of the tasks but lacks the sophistication and storytelling elements requested.

**Strengths:**
*   It correctly loads the data and imports necessary libraries.
*   It attempts to create all specified visualization types (multi-panel, heatmap, violin, interactive scatter, dashboard).
*   The basic functionality of each plot type is present.

**Areas for Improvement / Missing Elements (compared to ground truth and task requirements):**

1.  **Data Preprocessing and Handling Missing Values:** The generated code does not handle missing `Age` values. The ground truth fills `Age` with the median, which is crucial for accurate age-based visualizations. It also lacks the `Fare` imputation and conversion of `Pclass` to categorical for better labeling.
2.  **Sophistication in Visualizations:**
    *   **Multi-panel figure:** The titles and labels are very minimal. It misses the overall title and clear consistent labeling on the y-axis (sharing).
    *   **Heatmap:** Titles and labels are basic.
    *   **Violin Plots:** The `split=True` is used, which is good, but the `hue` is `Survived` directly without mapping to descriptive names ('Yes'/'No'), which could improve clarity.
    *   **Interactive Scatter Plot:** While `px.scatter` is used, the `size='Survived'` is not ideal for representing survival (a binary outcome) as size. Using `color` for `Survived` and `symbol` or `size` for `Pclass` or vice versa would be more intuitive. The ground truth uses `color` for `Pclass_Cat` and `symbol` for `Survived_Str`, which is more effective. It also lacks additional hover data and customized colors.
    *   **Dashboard-Style Figure:** This is the weakest point. The task explicitly asks for a ""comprehensive dashboard-style figure combining multiple visualizations with proper annotations and insights."" The generated code merely creates a 2x2 grid of four basic plots, which is far from a ""dashboard-style figure with insights."" It lacks textual insights, varied plot types (e.g., density plots, histograms as seen in GT), and a more sophisticated layout. The ground truth uses `gridspec` and dedicates a section for textual insights, which are crucial for ""storytelling.""

3.  **Aesthetics and Storytelling:**
    *   Lack of descriptive titles, axis labels, and legends in many plots.
    *   No color palettes are specified for Seaborn plots, which can improve readability.
    *   No annotations or insights are explicitly mentioned or included in the ""dashboard.""

4.  **Redundancy:** The age grouping and some bar charts are repeated in the general multi-panel section and again in the ""dashboard"" section, which is inefficient. The ground truth calculates these once and reuses the results.

To achieve a higher score, the generated code needs:
*   More robust data preprocessing (handling NaNs, converting data types for plotting).
*   More descriptive titles, labels, and legends for all plots.
*   Improved visual encoding for interactive plots.
*   A much more elaborate ""dashboard"" that truly combines diverse visualizations and, importantly, includes textual insights/interpretations to fulfill the ""storytelling"" aspect of the task.",gpt-3.5-turbo,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.2...",9,"The generated code correctly defines the DataFrame, filters for the Dystopian genre, and accurately calculates the average stock for that genre. It imports pandas correctly and the structure is clean and concise for the given task.

The only minor area for improvement is in the `print` statement for the average stock. While `print(""Average stock for Dystopian books:"", average_stock_dystopian)` is functionally correct, formatting the output to a specific number of decimal places (e.g., using an f-string like `f""Average stock for Dystopian books: {average_stock_dystopian:.2f}""`) would enhance readability and consistency, especially for numerical results like averages. This is a minor aesthetic point and doesn't affect the correctness of the calculation. Additionally, the ground truth includes optional print statements for debugging, which are helpful for development but not strictly required by the prompt's output specification. The generated code directly addresses the core requirements without unnecessary verbosity. The ground truth also includes an `if not dystopian_books_df.empty:` check, which is a good practice for robustness in a more general-purpose application, but for this specific dataset and task description (where we know Dystopian books exist), it's not strictly necessary for correctness.",gpt-3.5-turbo,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency, shapiro
import numpy as np

# Load dataset
data = pd.read_csv('water_potability.csv')

# Perform t-test for each water quality parameter
parameters = data.columns[1:-1]
for param in parameters:
    t_stat, p_val = ttest_ind(data[data['Potability'] == 0][param], data[data['Potability'] == 1][param])
    print(f'{param}: t-statistic = {t_stat}, p-value = {p_val}')

# Perform chi-square goodness-of-fit test
observed_...",4,"The generated code attempts to perform the requested statistical tests but fails significantly in handling missing values, calculating Cohen's d correctly, and performing the Chi-square goodness-of-fit test using the appropriate function and interpretation. The structure and interpretation of results are also very basic compared to the requirements.

Key Issues:
1.  **Missing Value Handling:** The generated code completely ignores missing values. Statistical tests like `ttest_ind` and `shapiro` can implicitly drop NaNs, but this is not robust or explicit. The ground truth explicitly addresses and imputes missing values, which is crucial for real-world data analysis tasks.
2.  **Chi-square Test:** The generated code uses `chi2_contingency` for a goodness-of-fit test. `chi2_contingency` is for testing independence between two categorical variables (contingency tables), not for goodness-of-fit (comparing observed counts to expected counts for a single categorical variable). The correct function for goodness-of-fit is `scipy.stats.chisquare`.
3.  **Cohen's d Calculation:**
    *   The `pooled_std` calculation is incorrect. It uses `len(data)` in the denominator where it should be `n1 + n2 - 2`.
    *   It calculates variance (`.var()`) but does not take the square root of the pooled variance before dividing. Cohen's d uses pooled standard deviation.
    *   It does not handle cases where standard deviation might be zero (e.g., if one group has constant values), which could lead to division by zero errors.
    *   It does not provide interpretation for Cohen's d, which was requested.
4.  **Normality Test (Shapiro-Wilk):** While it calls `shapiro`, it doesn't handle the known limitation of Shapiro-Wilk for larger sample sizes (typically > 5000), which the ground truth gracefully addresses.
5.  **T-test Details:** It uses `ttest_ind` without specifying `equal_var=False` (Welch's t-test), which is often a safer default as it doesn't assume equal variances, a common violated assumption. It also doesn't display the mean values for comparison, which aids interpretation.
6.  **Interpretation:** The output is very minimal. It only prints p-values and statistics but lacks the detailed interpretation of results (reject/fail to reject null, meaning of the result), which was explicitly requested.
7.  **Robustness:** No `try-except` block for file loading, making it less robust. No `ALPHA` constant is defined or used consistently for significance.
8.  **Feature Selection:** The `parameters = data.columns[1:-1]` line assumes 'Potability' is always the last column and the first is an index, which is not robust. The ground truth correctly identifies numerical features by dropping 'Potability'.",gpt-3.5-turbo,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
from sklearn.model_selection import KFold
from sklearn.metrics...",6,"The generated code addresses most of the requirements but has several critical issues and omissions that prevent it from being a good solution.

**Correctness Issues:**
1.  **Missing Value Handling:** The generated code uses `data.dropna(inplace=True)` which removes rows with *any* missing values. The ground truth, and typically a better approach for this dataset, uses `SimpleImputer(strategy='median')` which imputes missing values. Dropping rows can lead to significant data loss, especially if missing values are scattered across different columns. This is a critical difference in preprocessing.
2.  **Train-Test Split and Scaling Order:** The generated code scales the entire dataset `X` *before* the train-test split (`X_scaled = scaler.fit_transform(X)`). This is a common data leakage error. The `fit_transform` operation on the entire dataset means the scaler learns from the test data, which then biases the training process. The correct approach (as in the ground truth) is to fit the scaler *only* on the training data (`scaler.fit_transform(X_train_imputed)`) and then transform both train and test sets (`scaler.transform(X_test_imputed)`).
3.  **SVC `probability=True`:** The `SVC` model needs `probability=True` set during initialization to allow `predict_proba` (and thus `roc_auc_score` with probabilities and `plot_roc_curve`) to work correctly when `plot_roc_curve` or `roc_auc_score` are used with `y_proba`. The generated code calculates `roc_auc` using `y_pred` (class labels) instead of `y_proba` (probabilities), which is incorrect for `roc_auc_score`.
4.  **`plot_confusion_matrix` and `plot_roc_curve` are deprecated:** The `sklearn.metrics.plot_confusion_matrix` and `sklearn.metrics.plot_roc_curve` functions used in the generated code are deprecated since scikit-learn version 1.0. They should be replaced with `ConfusionMatrixDisplay.from_estimator` and `RocCurveDisplay.from_estimator` respectively.
5.  **K-Fold Cross-Validation:** The K-Fold cross-validation is performed on `X_scaled, y` (the full dataset after *incorrect* scaling) but doesn't handle the imputation/scaling steps correctly within each fold. A proper CV setup for preprocessing involves a `Pipeline` (as demonstrated in the ground truth). The current implementation re-fits models on differently scaled data in CV than what was used for the initial training. It also only calculates accuracy for CV, not the full suite of metrics requested.
6.  **Feature Importance for Best Model:** The code assumes Random Forest is the best model and only plots its feature importance. The task explicitly asks to *select the best performing model* and then display feature importance for *that* best model. If Logistic Regression or SVM were better, the code would fail to show their importance (and it correctly notes that SVM doesn't have direct importance, though it should still *try* to handle other models if they win).

**Completeness Issues:**
1.  **Imports:** While most necessary imports are there, `matplotlib.pyplot` is imported but not given an `as plt` alias, and `seaborn` is missing entirely (which is useful for better visualizations, especially confusion matrices).
2.  **Visualization Customization:** The confusion matrix and ROC curve plots are basic. The ground truth provides more legible confusion matrices (with labels, `fmt='d'`, `cmap='Blues'`) and a combined ROC plot for easier comparison, which aligns better with ""visualize results"".
3.  **Comparison and Selection:** The generated code prints metrics but doesn't explicitly compare them or select a ""best"" model programmatically based on a defined metric (e.g., F1-score or ROC-AUC) as requested. It just loops through models and then assumes Random Forest later for feature importance.
4.  **No `SimpleImputer` import:** Missing `from sklearn.impute import SimpleImputer`.
5.  **No `StratifiedKFold` import:** The task asks for appropriate splits, `StratifiedKFold` is generally better for classification tasks to maintain class balance across folds, which the `KFold` used doesn't guarantee.

**Code Quality/Structure:**
1.  **Redundant best model training:** The `best_model = RandomForestClassifier()` line followed by `best_model.fit(X_train, y_train)` means the Random Forest model is trained *again* after it's already been trained in the loop. While minor, it's inefficient.
2.  **No clear structure for each step:** The code flows directly without clear separation or print statements indicating what step is being performed. The ground truth uses headers like ""--- 1. Load Data ---"", which improves readability.
3.  **Error handling:** No `try-except` for file loading which can lead to script failure.
4.  **`random_state` for models:** Models like Logistic Regression, Random Forest, and SVC should have `random_state` set for reproducibility.

The solution has the basic structure but significant architectural and methodological flaws in data preprocessing and model evaluation that make it less reliable and less robust than expected for an expert code evaluator.",gpt-3.5-turbo,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('water_potability.csv')

# Feature engineering
data['pH_Chloramines_ratio'] = data['ph'] / data['Chloramines']
data['Hardness_Solids_ratio'] = d...",6,"The generated code addresses most of the task requirements but falls short in several areas, particularly regarding robustness, comprehensiveness of feature engineering, and the detail of comparisons. While it provides a basic structure, it lacks the depth and best practices seen in the ground truth.

**Strengths:**
- **Identifies key libraries:** Correctly imports necessary libraries.
- **Attempts feature engineering:** Includes ratio features and polynomial features.
- **Applies feature selection:** Uses SelectKBest and RFE as requested.
- **Includes visualizations:** Generates a heatmap, feature importance plot, and pair plot.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling:** The code completely misses handling missing values (`data = pd.read_csv('water_potability.csv')`). The `water_potability.csv` dataset is known to have missing values in 'ph', 'Sulfate', and 'Trihalomethanes'. This omission would lead to errors or incorrect results when calculations involve these columns (e.g., correlations, scaling, model training). The ground truth correctly addresses this by imputing with the mean.
2.  **Breadth of Feature Engineering:**
    *   **Ratios:** Only two ratio features are created (`pH_Chloramines_ratio`, `Hardness_Solids_ratio`). The prompt asked for ""new meaningful features such as ratios between chemical parameters (e.g., pH/Chloramines, Hardness/Solids)"". The ground truth provides more examples, demonstrating a better understanding of ""meaningful features.""
    *   **Interaction Terms:** No explicit interaction terms (e.g., `ph * Chloramines`) are created. The polynomial features might implicitly create some, but dedicated interaction terms offer more control and interpretability for specific relationships.
    *   **Polynomial Features Application:** When creating polynomial features, `data_poly` is created on `data.drop(['Potability'], axis=1)`. However, `data_poly` is then concatenated with the *original* `data` frame, which itself has already been altered by adding the two ratio features. This concatenation strategy (`pd.concat([data, data_poly], axis=1)`) can lead to duplicated columns (if `get_feature_names` includes original columns and they're already present in `data`) or overly complex/large dataframes, especially since `PolynomialFeatures` by default includes original features (`x_i`) as well. It's often better to apply polynomial features to a specific subset of *original* features, and then merge the *new* polynomial terms (like `x_i^2`, `x_i x_j`) back to the main DataFrame, as done in the ground truth. The ground truth also explicitly excludes bias and carefully handles column names.
3.  **Correctness of `PolynomialFeatures` Column Names/Concatenation**: `poly.get_feature_names(data.columns)` is likely incorrect. `poly.get_feature_names(input_features=X.columns)` should be used after fitting. The ground truth uses `get_feature_names_out` which is the modern method and handles multiple cases more robustly. The `data.columns` likely includes 'Potability', leading to issues if `Potability` is processed by `poly.fit_transform`.
4.  **Feature Selection Input (`X`):** The generated code uses `X = data_combined.drop(['Potability'], axis=1)` for feature selection. `data_combined` itself includes redundant original columns, and the `PolynomialFeatures` likely added original features again when using `poly.get_feature_names(data.columns)`. This can lead to issues with duplicated column names or unexpected behavior during selection if not handled carefully. The ground truth creates `X_fe` with engineered features and then scales it (`X_scaled_df`) before selection, which is a cleaner approach.
5.  **Standardization:** The features are not standardized before applying SelectKBest and RFE (`skb = SelectKBest(...)`, `rfe = RFE(...)`, `rf = RandomForestClassifier()`). While not strictly necessary for `f_classif` or tree-based models, it is generally good practice to standardize features before model training or certain feature selection techniques, especially when dealing with potentially widely varying scales, as indicated in the ground truth.
6.  **Top Correlated Features Identification:** `top_corr_features = corr_matrix['Potability'].abs().sort_values(ascending=False).head(6)` includes 'Potability' itself as the top correlated feature. The prompt asks for ""top 5 features most correlated with water potability,"" which means excluding 'Potability' itself. The ground truth correctly uses `[1:6]` to get the top 5 *other* features.
7.  **Comparison of Feature Sets:** The comparison is very basic, just printing the lists. It misses the specific analysis requested, such as overlap, unique features, and the number of overlapping or unique features, which the ground truth meticulously provides.
8.  **Plotting Issues:**
    *   The `sns.heatmap` uses `annot=True` on a potentially very large correlation matrix (due to polynomial features). This makes the heatmap unreadable, as the annotations will overlap excessively. The ground truth correctly uses `annot=False` for the full matrix.
    *   The `pairplot` uses `data_combined[top_features]`. `top_features` will include 'Potability' itself, which is needed for the hue, but the ground truth handles this more explicitly, ensuring `valid_features_for_pairplot` contains only the selected features + the target.
    *   Figure sizing in visualizations is inconsistent or missing in some cases, leading to potentially unreadable plots.
9.  **RandomForestClassifier Instantiation**: The `rf = RandomForestClassifier()` for feature importance is instantiated without a `random_state`, which makes the results non-reproducible.
10. **Hardcoded `k` and `n_features_to_select`**: The code hardcodes `k=5` and `n_features_to_select=5`. While this satisfies the prompt's request for *applying* the techniques, the prompt asks for ""optimal features"" and a more comprehensive selection often involves exploring different `k` values, or setting `k` to a more reasonable number given the large potential feature set (e.g., 10-20 features as in the ground truth, rather than just 5 from a potentially hundreds of features).",gpt-3.5-turbo,LLM_judge_config
