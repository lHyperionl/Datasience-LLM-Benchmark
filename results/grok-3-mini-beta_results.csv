task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1. List all column names and identify the data type of each column
print(""Column names and their data types:"")
for col in df.columns:
    dtype = df[col].dtype
    if dtype == 'object':
        col_type = 'categorical'
    elif dtype in ['int64', 'float64']:
        col_type = 'numerical'
    else:
        col_type = 'other'
    print(f""{col}: {dtype} ({col_type})"")

# 2. For all numerical columns, calculate a...",7,"The generated code correctly loads the dataset, identifies data types, calculates descriptive statistics for numerical columns, and reports dataset dimensions. It handles most of the requirements well.

However, there are areas for improvement:
1.  **Robustness in file loading**: The generated code assumes the `winequality-red.csv` file always exists. The ground truth includes `try-except` block for `FileNotFoundError` and provides user-friendly messages for debugging. This makes the code more robust in real-world scenarios.
2.  **Categorical/Ordinal Identification**: The identification of categorical/ordinal columns is a bit simplistic.
    *   It correctly identifies `object` types as categorical. However, the `winequality-red.csv` dataset contains only numerical columns already. The ground truth correctly points this out and adds a note about 'quality' being often treated as ordinal, which is a good contextual detail for this specific dataset.
    *   The heuristic `df[col].nunique() < 10` for identifying ordinal columns is a reasonable heuristic but can be misleading without further context. For `winequality-red.csv`, 'quality' has 6 unique values (3-8), making it less than 10, so it would be flagged as ordinal, which aligns with common treatment. However, other numerical columns might also have few unique values. The ground truth's approach of identifying strictly non-numerical types first and then adding context about the 'quality' column is more precise for this dataset.
3.  **Clarity of Output**: The ground truth provides more descriptive print statements and clear headings for each section, making the output easier to read and understand. For example, ""--- Column Names and Identified Data Types ---"" vs. ""Column names and their data types:"".
4.  **Pandas Dtype Usage**: The ground truth leverages `pd.api.types.is_numeric_dtype` and similar functions which are more robust for type checking than explicit string comparisons (`dtype == 'object'`, `dtype in ['int64', 'float64']`). While the explicit string comparisons work for common cases, the `pd.api.types` functions are generally preferred for their broader applicability to different numerical dtypes (e.g., `uint8`, `float32`).",grok-3-mini-beta,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 1. Display the first 10 rows
print(df.head(10))

# 2. Show the shape of the dataset
print(df.shape)

# 3. List all column names and their data types
print(""Column names:"", df.columns)
print(""Data types:\n"", df.dtypes)

# 4. Display the top 10 games by Global_Sales
top_10_games = df.sort_values(by='Global_Sales', ascending=False).head(10)
print(top_10_games)

# 5. Show the unique values in the Platform ...",7,"The generated code correctly performs most of the requested tasks. It loads the data, displays the head, shape, unique values, summary statistics, and missing values. However, it lacks robust error handling for file not found or missing columns, which is present in the ground truth. The output formatting is also very minimal compared to the ground truth, making it harder to read and distinguish between sections. Specifically:

Minor issues:
- **Output Clarity:** The ground truth includes `print` statements with section headers (`--- 1. Loading Dataset ---`, etc.) and separators (`=`), which significantly improves readability for an EDA task. The generated code just prints the results directly.
- **Column Names and Data Types:** While it prints `df.columns` and `df.dtypes` separately, `df.info()` as used in the ground truth combines column names, non-null counts, and data types in a more concise and informative way, which is standard practice for EDA.

Significant issues / Areas for Improvement:
- **Error Handling:** The most significant omission is the lack of error handling.
    - It does not handle `FileNotFoundError` for the CSV, which could crash the script.
    - It does not check if `Global_Sales` or other required columns exist before attempting operations, which could lead to `KeyError`. The ground truth gracefully handles this by checking `if 'Column_Name' in df.columns:`.
- **Top 10 Games Display:** The generated code prints all columns of the `top_10_games` DataFrame. The ground truth specifically selects `[['Name', 'Platform', 'Genre', 'Publisher', 'Global_Sales']]`, which is a more sensible and focused display for ""top games.""
- **Missing Values Output:** The ground truth filters `missing_values[missing_values > 0]` to show only columns with actual missing values and adds a clarifying message. The generated code simply prints all columns, which can be noisy if only a few columns have NAs.

The code is functional for the happy path but could be much more robust and user-friendly with the additions mentioned.",grok-3-mini-beta,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values in the Year column
df['Year'].fillna(df['Year'].median(), inplace=True)

# Remove games with missing Publisher information
df.dropna(subset=['Publisher'], inplace=True)

# Create a new column 'Years_Since_Release'
df['Years_Since_Release'] = 2023 - df['Year']

# Remove duplicate games based on Name and Platform
df.drop_duplicates(subset=['Name', 'Platform'], inplace=True)

# Filter out ...",8,"The generated code correctly addresses most of the requirements. It loads the dataset, handles missing 'Year' values using the median, removes entries with missing 'Publisher', creates the 'Years_Since_Release' column, removes duplicates, and filters out low-selling games. It also displays the shape and head of the cleaned dataframe.

Areas for improvement to achieve a higher score:
1.  **Robust Type Conversion for 'Year'**: The ground truth explicitly converts 'Year' to numeric with `errors='coerce'` before calculating the median and then to `int` after filling NaNs. The generated code assumes 'Year' is already numeric or can be implicitly handled, which might lead to errors if the column contains non-numeric strings.
2.  **Robust Type Conversion for 'Global_Sales'**: Similar to 'Year', `Global_Sales` might contain non-numeric data. The ground truth explicitly converts it to numeric and handles potential NaNs from that conversion. The generated code directly uses it in a comparison, which could fail if it's not a numeric type.
3.  **Handling Negative 'Years_Since_Release'**: The ground truth includes a step to ensure 'Years_Since_Release' is non-negative (`max(0, x)`), which is a good practice for this type of calculation. The generated code doesn't include this, which might result in negative values if any game's release year is after the `CURRENT_YEAR` (2023).
4.  **Clarity on File Path and Current Year**: While not explicitly asked for in the prompt, using constants for `DATASET_PATH` and `CURRENT_YEAR` as in the ground truth improves code readability and maintainability.
5.  **Error Handling**: The ground truth includes a `try-except` block for `FileNotFoundError` and general exceptions, which makes the code more robust. This was not a specific requirement of the prompt but is good practice.

Overall, the generated code provides a functional solution to the task. The missing explicit type conversions and handling of potential negative 'Years_Since_Release' are minor issues but prevent it from being a perfect solution.",grok-3-mini-beta,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 1. Bar chart for top 10 gaming platforms by total global sales
plt.figure(figsize=(10,6))
platforms = df.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False).head(10)
plt.bar(platforms.index, platforms.values)
plt.title('Top 10 Gaming Platforms by Total Global Sales')
plt.xlabel('Platform')
plt.ylabel('Total Global Sales')
plt.xticks(rotation=45)
plt.show()

...",6,"The generated code addresses most of the visualization requirements, including bar, pie, histogram, scatter, and line plots. However, it has several issues that prevent it from being production-ready and fully accurate.

**Key Issues and Areas for Improvement:**

1.  **Robustness in Data Loading:** The code directly calls `pd.read_csv` without any error handling for `FileNotFoundError`. The ground truth includes a `try-except` block which is crucial for real-world applications.
2.  **Lack of Data Cleaning/Preprocessing:**
    *   `Year` column: The generated code directly uses `df['Year'].value_counts()`. The `video_games_sales.csv` dataset typically has a `Year_of_Release` column, and it's often of mixed type or float. The ground truth explicitly converts it to numeric and handles NaNs. The generated code implicitly assumes `Year` is clean and directly present, which is a common source of errors in this dataset. This causes a `KeyError: 'Year'` if the column doesn't exist, or incorrect plotting if it's not numeric.
    *   Sales columns (`NA_Sales`, `EU_Sales`, `Global_Sales`): These are also often numeric, but can contain non-numeric values or NaNs in real datasets. The ground truth includes explicit `pd.to_numeric` calls and NaN handling. The generated code relies on `dropna()` for the histogram but doesn't explicitly ensure that all sales columns are numeric before plotting, which could lead to errors or incorrect plots.
3.  **Visualization Specifics:**
    *   **Titles and Labels:** While titles and basic labels are present, they lack the specific descriptive quality seen in the ground truth (e.g., ""Total Global Sales (Millions)"" clarifies units).
    *   **Pie Chart:** The ground truth includes a robust way to handle smaller genres by grouping them into ""Other,"" which significantly improves readability for datasets with many categories. The generated code plots all genres, which can result in a cluttered and unreadable pie chart if there are too many small genres.
    *   **Histogram:** The ground truth includes a `range` parameter for the histogram, which is useful for focusing on the bulk of the data and avoiding long tails from extreme outliers, leading to a more informative plot.
    *   **Line Plot x-axis:** The `Year` plot relies on `sort_index()`, but doesn't handle potential non-numeric year values or filter out unrealistic year values (e.g., years significantly outside a plausible range), which the ground truth addresses.
    *   **Readability/Aesthetics:** The ground truth code has more thoughtful use of `figsize`, `xticks(rotation=45)`, `tight_layout()`, `grid`, and color/alpha adjustments to make the plots more visually appealing and informative.

For a data science benchmark, robustness, comprehensive data handling, and optimized visualization techniques are crucial. The generated code produces basic plots but lacks the necessary robustness and refinement for a complete solution.",grok-3-mini-beta,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```
import pandas as pd

# Load the dataset and display the first 10 rows
df = pd.read_csv('diabetes.csv')
print(df.head(10))

# Show the shape of the dataset
print(df.shape)

# List all column names and their data types
print(""Column names:"", df.columns.tolist())
print(""Data types:\n"", df.dtypes)

# Calculate the percentage of diabetic vs non-diabetic patients
total = len(df)
diabetic = (df['Outcome'] == 1).sum()
non_diabetic = (df['Outcome'] == 0).sum()
diabetic_pct = (diabetic / total) * 100
...",7,"The generated code correctly performs most of the requested EDA tasks.

**Strong Points:**
*   Correctly loads the CSV and displays the first 10 rows.
*   Accurately shows the dataset shape.
*   Calculates and prints the percentage of diabetic/non-diabetic patients correctly.
*   Displays missing value counts.
*   Shows summary statistics.

**Areas for Improvement/Missing Parts:**
*   **Item 3 (Column names and data types):** While it prints `df.columns.tolist()` and `df.dtypes`, it doesn't give the comprehensive output of `df.info()` which shows non-null counts alongside data types, which is generally preferred for EDA.
*   **Robustness:** Lacks error handling for `FileNotFoundError` when loading the CSV, which is crucial for robust code.
*   **Clarity/Formatting:** The output isn't as clearly delineated or labeled as the ground truth. This isn't a functional error but impacts readability for large outputs.
*   **Item 7 (Zero values):** The ground truth specifically lists `COLUMNS_TO_CHECK_FOR_ZERO_AS_MISSING` (Glucose, BloodPressure, etc.) because `Pregnancies` and `Outcome` can legitimately be zero. The generated code checks *all* numerical columns for zeros, which can be misleading as 0 for 'Pregnancies' or 'Outcome' is a valid value, not missing data. The task specifically asked to ""Identify any columns with zero values *that might represent missing data*,"" implying a selective check based on domain knowledge.",grok-3-mini-beta,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Replace zero values with NaN in specified columns
columns_to_check = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
for col in columns_to_check:
    df[col] = df[col].replace(0, np.nan)

# Fill missing values with the median of each column
for col in columns_to_check:
    median_value = df[col].median()
    df[col] = df[col].fillna(m...",7,"The generated code addresses most of the requirements but has some areas for improvement to be complete and robust.

**Strong Points:**
- Correctly identifies and replaces zero values with NaN in the specified columns.
- Fills missing NaN values with the median of their respective columns.
- Successfully removes duplicate rows.
- Creates the `BMI_Category` column using a clear function.
- Scales numerical features using `StandardScaler`.
- Displays the final shape and head of the DataFrame.

**Areas for Improvement / Missing Aspects:**
1.  **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.
2.  **Column Existence Checks:** The generated code assumes all `columns_to_check` and `numerical_cols` exist. The ground truth includes checks like `if col in df.columns:` before attempting operations, which prevents errors if a column name is misspelled or absent.
3.  **Clarity in Numerical Column Selection for Scaling:** The generated code hardcodes all non-target numerical columns for scaling. While this works for this specific dataset, a more dynamic approach (like `df.select_dtypes(include=np.number).columns.tolist()`) followed by excluding the target variable (`Outcome`) is more robust and less prone to errors if the dataset structure changes slightly.
4.  **Excluding Target Variable from Scaling:** The task implies that numerical features should be scaled. `Outcome` is typically the target variable and should usually not be scaled along with features. The generated code scales all numerical columns without explicitly excluding `Outcome`.
5.  **Output Clarity:** The ground truth provides more descriptive print statements regarding the steps performed (e.g., ""Dataset loaded successfully."", ""Removed X duplicate rows."", ""Scaled numerical features: [...]""), which improves readability and understanding of the code's execution.
6.  **Minor 'Normal' Category Range:** The `categorize_bmi` function uses `bmi < 25` for 'Normal', which correctly includes 24.9. The ground truth uses `pandas.cut` with an explicit upper bound of 24.9, which is slightly more precise for clear binning (e.g., 24.9 falling into Normal vs some interpretation of <25). Both effectively achieve the goal for now but `pd.cut` is generally preferred for binning.",grok-3-mini-beta,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Correlation Heatmap
plt.figure(figsize=(10,8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Diabetes Dataset')
plt.show()

# Histograms for all numerical features
numerical_features = df.select_dtypes(include=['float64', 'int64']).columns
num_plots = len(numerical_features)
n_cols = 3
n_rows = (num_plots // n_cols) + (num_p...",7,"The generated code successfully creates most of the requested visualizations and demonstrates a good understanding of matplotlib and seaborn.

**Strengths:**
*   **Correctness and Completeness:**
    *   Loads the dataset correctly.
    *   Generates a correlation heatmap.
    *   Generates histograms for all numerical features.
    *   Generates box plots comparing features by diabetes outcome.
    *   Creates a pie chart for outcome distribution.
    *   Creates scatter plots (though with swapped axes and one missing).
    *   Generates a pair plot.
*   **Code Quality:**
    *   Uses appropriate libraries (pandas, seaborn, matplotlib).
    *   Subplotting logic for histograms and box plots is generally correct.
    *   Manages subplots well by deleting unused axes.

**Areas for Improvement:**
*   **Scatter Plots (Completeness & Correctness):**
    *   The task explicitly requested ""BMI vs Glucose"" and ""Age vs Pregnancies"". The generated code provides ""Glucose vs BMI"" (swapped axes) and ""Pregnancies vs Age"" (swapped axes and the task asked for 'Age vs Pregnancies'). More critically, one of the two requested scatter plots (Age vs Pregnancies) was implemented as ""Pregnancies vs Age"", and the other ""BMI vs Glucose"" was implemented as ""Glucose vs BMI"". The task asked for two *specific* pairs.
    *   The scatter plots are in separate figures, whereas the ground truth correctly puts them in a single figure with subplots, which is more efficient for comparing related plots.
*   **Labels and Titles (Completeness & Clarity):**
    *   Many titles and labels are generic (e.g., ""Correlation Heatmap of Diabetes Dataset"" instead of ""Correlation Heatmap of Numerical Features"").
    *   The task asked for specific labels for the pie chart (""Non-Diabetic (0)"", ""Diabetic (1)""). The generated code provides ""Non-diabetic"" and ""Diabetic"" which is acceptable but the ground truth is more precise on the '0' and '1' context.
    *   The legend labels for scatter plots could be explicitly set for clarity (e.g., Non-Diabetic, Diabetic) instead of just 0 and 1.
*   **Redundancy/Efficiency:**
    *   The `df.select_dtypes` for `numerical_features` includes 'Outcome' if it's int64, which is then handled by the box plot loop `col != 'Outcome'`. While it works, explicitly defining numerical columns excluding 'Outcome' earlier, as in the ground truth, would be cleaner and less prone to errors if 'Outcome' were a float.
*   **Axis Limits/Aesthetics (Minor):**
    *   The ground truth `plt.tight_layout()` and specific `figsize` choices, along with `sns.set_style`, lead to slightly more polished and readable outputs. The generated code uses `plt.tight_layout()` but could refine figure sizes for better scaling.
*   **Error Handling (Minor):**
    *   The ground truth includes `try-except FileNotFoundError`, which is good practice for robust code, though not explicitly required by the prompt.

The code is functional and largely correct, but lacks some of the fine-tuning and precise adherence to all labeling and pairing requests, particularly for the scatter plots, that would elevate it to a higher score.",grok-3-mini-beta,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```
import pandas as pd

# Load the dataset and display the first 10 rows
df = pd.read_csv('world-happiness-report-2021.csv')
print(df.head(10))

# Show the shape of the dataset
print(df.shape)

# List all column names and their data types
print(df.columns)
print(df.dtypes)

# Display the unique regional indicators and count of countries per region
unique_regions = df['Regional indicator'].unique()
print(unique_regions)
region_counts = df['Regional indicator'].value_counts()
print(region_counts)...",6,"The generated code correctly performs most of the requested tasks but lacks some robustness and completeness compared to the ground truth.

**Strengths:**
*   Correctly loads the dataset.
*   Successfully displays the first 10 rows.
*   Correctly shows the shape of the dataset.
*   Identifies unique regional indicators and counts countries per region correctly.
*   Correctly identifies the top 10 happiest countries.
*   Calculates summary statistics for the specified factors (assuming column names match).
*   Identifies missing values correctly.

**Areas for Improvement (leading to a lower score):**
*   **Lack of Error Handling:** The generated code does not include any `try-except` block for `FileNotFoundError` when attempting to load the CSV, which is crucial for robust code. If the file is not present, the script would simply crash.
*   **Incomplete Column/Dtype Display:** For task 3 (""List all column names and their data types""), `df.columns` only lists the names, and `df.dtypes` only lists the types. `df.info()` (used in ground truth) provides a more comprehensive and standard way to display this information, including non-null counts, which is very useful for EDA.
*   **Hardcoded Column Names in Summary Stats:** The ground truth code intelligently filters for `existing_factors` before calling `describe()`. The generated code directly tries to select `key_factors`, which might fail if one of the column names differs (e.g., ""GDP per capita"" vs. ""Logged GDP per capita"" in the actual dataset's column names, which is a common discrepancy in datasets). This makes the code less robust.
*   **Missing Output Labels/Separators:** The generated code lacks clear print statements indicating which output corresponds to which task, making the output less readable and structured compared to the ground truth. The ground truth adds textual headers and separators, which are good practice for clarity.
*   **""GDP per capita"" vs ""Logged GDP per capita"":** The ground truth correctly identifies the column name as 'Logged GDP per capita' (which is the actual name in the dataset), while the generated code uses 'GDP per capita'. This will lead to a `KeyError` or an incomplete summary statistics table if run against the actual dataset.",grok-3-mini-beta,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Handle missing values
for column in df.columns:
    if df[column].dtype == 'object':
        df[column].fillna(df[column].mode()[0], inplace=True)
    elif df[column].dtype in ['float64', 'int64']:
        df[column].fillna(df[column].mean(), inplace=True)

# Create new derived f...",9,"The generated code is very good and correctly addresses most of the task requirements.

**Strong Points:**
*   **Correctness:** All specified data processing steps (loading, missing value handling, feature creation, outlier removal, scaling, encoding) are implemented.
*   **Missing Value Handling:** The approach correctly distinguishes between numerical and categorical columns for filling missing values with mean and mode, respectively.
*   **Feature Engineering:**
    *   `Happiness_Category` is correctly created using `pd.cut`, which is a good alternative to `np.select`.
    *   `GDP_Category` uses `pd.qcut` as requested.
    *   `Well_being_Score` is correctly calculated as the average.
*   **Outlier Removal:** The IQR method for 'Ladder score' is correctly applied.
*   **Standardization:** `StandardScaler` is used effectively on numerical features.
*   **Encoding:** `LabelEncoder` is correctly applied to 'Regional indicator'.
*   **Output:** The code prints the final shape and head.

**Areas for Minor Improvement (to reach a perfect 10):**
*   **Robustness (File Loading):** The ground truth includes a `try-except` block for file loading, which makes the code more robust against `FileNotFoundError`. The generated code assumes the file exists.
*   **Clarity on Numerical Columns for Scaling:** While the current `numerical_cols` selection works, explicitly defining which columns go into `scaler.fit_transform` (e.g., excluding identifier columns if they existed, or ensuring newly created categorical columns aren't included) can sometimes make the code clearer, though in this specific case, `select_dtypes` handles it well.
*   **Print statements:** While the prompt requested ""only the Python code without explanations"" and the generated code followed that, the ground truth's use of intermediate print statements (commented out in the final provided version) or an end-of-task summary print helps a user understand execution flow and successful completion of steps. The generated code only has the final `print(df.shape)` and `print(df.head())`. Adding a descriptive print before these, like ""Shape of the cleaned dataset:"" as in the ground truth, would be clearer.

Overall, an excellent solution that fulfills the prompt's requirements effectively and efficiently.",grok-3-mini-beta,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import numpy as np
from math import pi

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# 1. World map visualization
fig = px.choropleth(df, locations='Country name', locationmode='country names', 
                    color='Ladder score', hover_name='Country name', 
                    projection='natural earth', title='Happiness Scores by Country')
fig.show()

# 2. ...",7,"The generated code addresses most of the requirements and produces the requested visualizations. However, there are several areas for improvement to enhance correctness, completeness, and code quality, particularly in handling the `world-happiness-report-2021.csv` dataset's specific column names and ensuring proper scaling for the radar chart.

Here's a breakdown:

**Strengths:**
*   **Most Visualizations Present:** The code generates a world map, bar chart, heatmap, scatter plots, box plots, and a radar chart as requested.
*   **Correct Libraries Used:** `pandas`, `matplotlib`, `seaborn`, and `plotly.express` are all imported and used appropriately for their respective visualization types.
*   **Basic Labels/Titles:** Most plots have titles and axis labels.

**Areas for Improvement:**

1.  **Column Name Consistency/Renaming:** The ground truth code renames columns (`Ladder score` to `Happiness Score`, `Logged GDP per capita` to `GDP per Capita`, etc.) for consistency and ease of use. The generated code uses the original column names, which are less readable and can lead to longer, less clean code (e.g., `Logged GDP per capita` vs `GDP per Capita`). While functional, it's not ideal for maintainability or readability. This also impacts the radar chart's feature names directly from the dataset.

2.  **Robustness in Data Loading:** The ground truth includes a `try-except` block for `FileNotFoundError`. The generated code assumes the file exists, which is less robust.

3.  **Correlation Heatmap - Column Selection:** The generated code calculates `df.corr()` on the entire DataFrame. While this might work for this specific dataset, best practice is to explicitly select the numerical columns relevant to the ""happiness factors"" as done in the ground truth. This prevents unrelated categorical columns from being included in the correlation matrix, which can lead to errors or nonsensical correlations.

4.  **Radar Chart - Data Scaling (Critical):** This is the most significant issue. The radar chart aims to show the ""profile"" by comparing different features. **Crucially, the raw values for features like 'Logged GDP per capita' (which is already logged) and 'Generosity' are on vastly different scales.** Plotting them directly on a radar chart without normalization (0-1 scaling) makes the chart misleading and difficult to interpret. For example, a ""Logged GDP per capita"" value of ~10 completely dwarfs a ""Generosity"" value of ~0.2, making the generosity axis look almost non-existent. The ground truth correctly uses `MinMaxScaler` to normalize these features, which is essential for a meaningful radar chart comparison. This alone significantly impacts the correctness of the radar chart's representation.

5.  **Radar Chart - `yticklabels`:** The ground truth sets `yticklabels` on the radar chart to provide a sense of scale for the normalized values. The generated code has `ax.set_yticklabels([])`, which removes any radial labels, making it harder to interpret the magnitude of the features even if they were scaled.

6.  **`tight_layout()`:** The ground truth consistently uses `plt.tight_layout()` to prevent labels or titles from overlapping, especially when rotations are applied. The generated code uses it for some plots but not all, leading to potential minor formatting issues.

7.  **Plotly Map `locationmode`:** The ground truth explicitly sets `locationmode=""country names""` and includes `update_layout` for better map control. While `plotly.express` often infers this, being explicit is good practice.

Overall, the generated code fulfills the basic requirements but lacks the polish, robustness, and analytical correctness (especially with the radar chart scaling) of the ground truth code.",grok-3-mini-beta,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd

# Load the dataset and parse the 'date' column
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Display the first 10 rows
print(df.head(10))

# Show the shape of the dataset
print(df.shape)

# List all column names and their data types
print(df.columns)
print(df.dtypes)

# Calculate and display the total sum of 'total_cases' and 'total_deaths' grouped by 'continent'
grouped_sum = df.groupby('continent')[['total_cases', 'total_deaths']].sum()
...",7,"The generated code correctly addresses most of the requirements.

**Strong points:**
*   Correctly loads data and parses dates.
*   Displays first 10 rows and shape.
*   Calculates continental sums.
*   Calculates summary statistics for `new_cases` and `new_deaths`.
*   Counts missing values.

**Areas for improvement:**
1.  **Readability/Clarity of Output:** The ground truth includes print statements that clearly label each section of the output, making the analysis much easier to follow. The generated code prints results without context.
2.  **Robustness (File Handling):** The ground truth includes error handling for `FileNotFoundError`, which is good practice. The generated code assumes the file will always be found.
3.  **Column Names and Data Types:** While `print(df.columns)` and `print(df.dtypes)` show the requested information, `df.info()` as used in the ground truth is a more comprehensive and standard way to display this, as it also includes non-null counts and memory usage.
4.  **Top 5 Locations (Logic Issue):**
    *   The `latest_df = df.sort_values('date', ascending=False).drop_duplicates('location', keep='first')` logic for finding the latest date is correct in terms of getting *a* latest date, but `keep='first'` combined with `ascending=False` means it keeps the *first* occurrence of each location *after* sorting the dates in descending order, thus effectively getting the latest date. This is an improvement over the ground truth's `keep='last'` which relies on sorting by location primarily.
    *   **However,** the crucial part the generated code misses is handling potential non-numeric `'total_cases'` values. `df.sort_values('total_cases', ascending=False)` will fail if `total_cases` is not purely numeric (e.g., if it imported as object type due to some non-numeric entries). The ground truth correctly `pd.to_numeric` with `errors='coerce'` to handle this robustness.
    *   The ground truth also explicitly checks for column existence (`if all(col in df.columns for col in required_cols_for_top_locations):`) which adds robustness.
5.  **Summary Statistics (Completeness):** The task specifically asked for ""mean, median, min, max, std"". `describe()` provides these along with count, 25%, 50%, and 75% quartiles. While `describe()` includes most of what's asked, explicitly using `.agg(['mean', 'median', 'min', 'max', 'std'])` as in the ground truth is more precise to the prompt's request and allows better error handling/numeric coercion before calculation. The ground truth also correctly applies `pd.to_numeric` and `dropna()` before calculating stats to prevent errors from non-numeric data.

Addressing these points would make the code more robust, user-friendly, and complete.",grok-3-mini-beta,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Task 1: Calculate and report the average 'quality'
mean_quality = df['quality'].mean()
print(f""The average quality is: {mean_quality}"")

# Task 2: Group by 'quality' and calculate average 'alcohol'
avg_alcohol_by_quality = df.groupby('quality')['alcohol'].mean()
print(avg_alcohol_by_quality)

# Task 3: Create a bar chart
sns.barplot(x='quality', y='alcohol'...",7,"The generated code correctly performs the main tasks: calculating the mean quality, grouping by quality to find average alcohol, and creating a bar chart. Imports are correct and the structure is clean.

However, there are a few areas for improvement:
1.  **Readability/User Experience**: The print statements could be more descriptive, similar to the ground truth, to clearly label what each reported number/table represents.
2.  **Visualization Correctness**: The bar chart in `sns.barplot(x='quality', y='alcohol', data=df)` is not correctly visualizing the *average* alcohol content by quality. It's attempting to plot individual `alcohol` values grouped by `quality` from the original `df`, which Seaborn will compute the mean for, but it's more explicit and robust to use the `avg_alcohol_by_quality` series/DataFrame generated in Task 2. The `data` argument should ideally refer to a DataFrame where each row is a quality level with its corresponding *average* alcohol content. While Seaborn's barplot can compute the mean on the fly, explicitly passing the pre-aggregated data is clearer and often preferred, especially if you modify the aggregation later. The Ground Truth uses `data=average_alcohol_by_quality` which is the result of the `groupby().mean().reset_index()`.
3.  **Robustness**: The generated code lacks error handling for `FileNotFoundError` for the CSV, which the ground truth includes, making it more robust.
4.  **Aesthetics/Clarity of Visualization**: While the basic chart is present, it lacks the enhanced aesthetics, title, and axis labels the ground truth provides, which significantly improve the clarity and presentation of the visualization. For example, adding units to the y-axis label.
5.  **Completeness of plot details**: The ground truth adds data labels and uses `plt.tight_layout()`, which are good practices for professional plots.",grok-3-mini-beta,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Handle missing values
df['hosp_patients'] = df['hosp_patients'].fillna(0)
df['icu_patients'] = df['icu_patients'].fillna(0)
df['positive_rate'] = df['positive_rate'].fillna(df['positive_rate'].mean())
df['reproduction_rate'] = df['reproduction_rate'].fillna(df['reproduction_rate'].mean())

# Create new columns
df['mortality_rate'] = (df['total_deaths'] / df['tota...",8,"The generated code correctly solves the task. It loads the data, parses the date column, handles missing values as specified, creates the `mortality_rate` and `vaccination_rate_per_capita` columns with correct division-by-zero handling, filters the data for 2021, and displays the required output.

The strong points include:
- Efficient date parsing directly in `read_csv`.
- Concise handling of missing values using chained `fillna` and `replace` for the new columns.
- Correctly filters the data for 2021.

Areas for minor improvement to achieve a higher score:
- The missing value handling for `positive_rate` and `reproduction_rate` assumes the columns are already numeric without explicit conversion. While `pd.read_csv` often infers numeric types, adding `pd.to_numeric(errors='coerce')` before calculating the mean would make it more robust, similar to the ground truth.
- The `mortality_rate` and `vaccination_rate_per_capita` calculations could explicitly convert `total_deaths`, `total_cases`, `people_fully_vaccinated`, and `population` to numeric types before division to prevent potential `TypeError` if these columns are originally loaded as objects (e.g., due to mixed types or non-numeric strings).
- The print statements in the display section are slightly less descriptive than the ground truth. Adding informative strings like ""Shape of the cleaned dataset"" and ""First 5 rows"" would improve clarity.",grok-3-mini-beta,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 1. Time series line plot of 'new_cases' for India
plt.figure(figsize=(10, 5))
india_df = df[df['location'] == 'India'].sort_values('date')
plt.plot(india_df['date'], india_df['new_cases'])
plt.title('New Cases in India Over Time')
plt.xlabel('Date')
plt.ylabel('New Cases')
plt.show()

# 2. Bar chart of sum of 'total_deaths...",6,"The generated code addresses most of the plotting requirements, correctly implementing time series, bar, scatter, histogram, and box plots. Essential imports are present, and titles/labels are included.

However, there are several significant areas for improvement:

1.  **Bar Chart for Total Deaths:** The prompt specifically asked for the ""sum of 'total_deaths' for each 'continent'"". The ground truth correctly interprets this as needing the *latest* `total_deaths` value for each *country* first, then summing those up by continent. The generated code `df.groupby('continent')['total_deaths'].sum()` incorrectly sums up *all* `total_deaths` entries for each continent, which will lead to a very large and misleading number due to daily cumulative counts. This is a critical error in data aggregation for this specific plot.

2.  **Scatter Plot Data Aggregation:** While the generated code attempts to get the latest entry (`latest_df = df.sort_values(by='date').groupby('location').tail(1)`), it does not handle missing `gdp_per_capita` or `total_vaccinations` values, which might cause the plot to be empty or incomplete if those columns have NaNs in the latest entries. The ground truth explicitly uses `dropna` for the scatter plot.

3.  **Plotting Style and Readability:** The generated code uses basic `matplotlib.pyplot` for the first plot, while `seaborn.lineplot` is often preferred for consistency and better aesthetics when Seaborn is already imported. It also lacks advanced formatting like `plt.grid(True)`, `plt.xticks(rotation=...)`, `plt.tight_layout()`, suggested log scales for scatter plot axes, and better color palettes (e.g., `palette='viridis'`). These details improve the professionalism and readability of the plots.

4.  **Error Handling:** The ground truth includes robust error handling for file not found and general loading issues, which is crucial for production-ready code. The generated code lacks this.

5.  **Scatter Plot Hue/Size:** The ground truth enhances the scatter plot by adding `hue='continent'` and `size='population'`, which provides more insightful visual information. The generated code's scatter plot is more basic.

To achieve a higher score, the generated code needs to correct the data aggregation logic for the bar chart and scatter plot, improve plot aesthetics and readability using more comprehensive Seaborn/Matplotlib features, and consider adding basic error handling.",grok-3-mini-beta,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# List all column names and their data types
print(""Column names and their data types:"")
for column, dtype in df.dtypes.items():
    print(f""{column}: {dtype}"")

# Calculate and report descriptive statistics for numerical columns
numerical_columns = ['elixirCost', 'Win Rate', 'Usage']
stats = df[numerical_columns].describe()

print(""\nDescriptive statistics:"")
for column in numerical_columns:
    print(f""\nFo...",8,"The generated code correctly addresses all requirements of the task, including loading the data, listing column names and types, calculating descriptive statistics for the specified columns, and reporting the total number of rows. It uses `pandas` effectively for these operations.

**Strong Points:**
- **Correctness:** All calculations and reports are accurate.
- **Completeness:** Fully covers all parts of the prompt.
- **Readability:** The code is well-structured and easy to understand.
- **Efficient Stats Calculation:** Uses `df[numerical_columns].describe()` which is an efficient way to get multiple statistics.

**Areas for Improvement (minor, for a perfect score):**
- **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions during file loading, which is a robust practice. The generated code assumes the file will always be present, which could lead to a `FileNotFoundError` if the CSV isn't in the expected location.
- **Output Formatting:** While functional, the output formatting for the statistics is a bit verbose compared to the ground truth which uses `print(descriptive_stats)` to output the aggregated statistics in a table format, which is typically preferred for data presentation. The ground truth also provides clearer headings for each section of the output.
- **Redundancy in Stats Printing:** The generated code iterates through `numerical_columns` and then manually prints each statistic, whereas `describe()` already provides this in a DataFrame view. While not incorrect, printing the `stats` DataFrame directly (as `print(stats)`) would be more concise and standard for pandas output. The ground truth's use of `agg` and printing the resulting DataFrame is also more concise.",grok-3-mini-beta,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

df = pd.read_csv('clash_royale_cards.csv')

filtered_df = df[df['elixirCost'] <= 3]

selected_df = filtered_df[['Card', 'Win Rate']]

sorted_df = selected_df.sort_values(by='Win Rate', ascending=False)

print(sorted_df)
```",10,"The generated code perfectly solves the task. It correctly loads the CSV, filters the data by elixir cost, selects the specified columns, and sorts the result by win rate in descending order. The code is concise, readable, and directly addresses all requirements of the prompt. While the ground truth includes more robust error handling and configuration, the prompt specifically asked for ""only the Python code without explanations"" for the core task, which the generated code provides admirably. For a benchmark evaluating functionality, this submission is excellent.",grok-3-mini-beta,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Generate histogram for elixirCost
df['elixirCost'].hist()

# Add title and labels
plt.title('Distribution of Elixir Cost')
plt.xlabel('Elixir Cost')
plt.ylabel('Frequency')

# Show the plot
plt.show()
```",7,"The generated code correctly loads the CSV, creates a histogram of the `elixirCost` column, and adds a title and axis labels. These are the core requirements of the task. However, there are several areas for improvement to enhance its robustness and visual quality, which prevent it from scoring higher.

**Areas for improvement:**
1.  **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and other exceptions during CSV loading, and also checks for the existence of the `elixirCost` column. The generated code lacks all this. In a real-world scenario, this makes the code less robust to missing files or malformed datasets.
2.  **Plotting Library Usage:** While `df['elixirCost'].hist()` directly uses pandas' built-in plotting (which relies on `matplotlib`), it offers less control and visual appeal compared to using `matplotlib.pyplot` or especially `seaborn` directly. The ground truth uses `seaborn.histplot` which is more aesthetically pleasing and often preferred in data visualization for its enhanced features and default styles.
3.  **Plot Customization & Readability:**
    *   The `bins` in the generated histogram are automatically determined, which might not be optimal for discrete data like `elixirCost`. The ground truth explicitly sets `bins` to cover common integer elixir costs, making the distribution clearer.
    *   The generated code doesn't set figure size, add a grid, or customize font sizes for better readability, which are good practices for publishable plots.
    *   It also doesn't explicitly set x-axis ticks to integers, which is important for `elixirCost` as it's a discrete variable.
4.  **Clarity and Best Practices:** The ground truth code separates loading and plotting into distinct sections with comments, making the code more organized and easier to understand. It also includes `import seaborn as sns` as a common practice when using seaborn for plotting, even if not explicitly used in the simple `hist()` call.

Overall, the generated code meets the basic functional requirements but lacks the robustness, visual polish, and adherence to common best practices seen in the ground truth.",grok-3-mini-beta,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('iris.csv')

# 1. List all column names and their data types
print(""Column names and data types:"")
for column in df.columns:
    print(f""{column}: {df[column].dtype}"")

# 2. Calculate descriptive statistics for numerical columns
numerical_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

for column in numerical_columns:
    print(f""\nDescriptive statistics for {column}:"")
    print(f""Mean: {df[column]....",7,"The generated code correctly performs the requested operations: loading the CSV, identifying column data types, calculating descriptive statistics for the specified numerical columns, and reporting unique species counts. However, it lacks robustness compared to the ground truth.

**Areas for Improvement:**
*   **Error Handling (File Not Found):** The generated code doesn't include any `try-except` block for `FileNotFoundError` when loading `iris.csv`. If the file is missing, the script will crash, which is not ideal for robust applications. The ground truth gracefully handles this by printing an error and exiting.
*   **Column Existence Check:** While the code assumes `numerical_columns` and `Species` column exist, the ground truth explicitly checks for their existence before proceeding. This makes the code more robust against variations in the dataset schema (e.g., misspelled column names, missing columns). If a numerical column is missing, the generated code will raise a `KeyError`.
*   **Output Formatting for Descriptive Statistics:** The generated code prints each statistic individually for each numerical column. While correct, it's less concise and harder to read than `dataframe.describe()` or `dataframe.agg()` which provide a tabular summary of statistics, as demonstrated in the ground truth. The ground truth's use of `.agg()` or `describe()` is more idiomatic pandas for this task.
*   **Clarity and Structure:** The ground truth uses clear print statements and separators (`---` lines) to delineate different sections of the output, making it much easier to read and understand the different results. The generated code's output is a bit more jumbled without these separators.

**Strong Points:**
*   Correctly uses pandas functions for all tasks.
*   Achieves the required output for all three parts of the task if the file and columns are present.
*   Proper imports are included.",grok-3-mini-beta,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('iris.csv')

# Filter the dataset
filtered_df = df[df['Species'] == 'Iris-setosa']

# Select and display the required columns
selected_df = filtered_df[['SepalLengthCm', 'SepalWidthCm']]
print(selected_df)
```",8,"The generated code correctly addresses all the core requirements of the task: it loads the `iris.csv` dataset, filters it to include only 'Iris-setosa' species, and then selects and displays the `SepalLengthCm` and `SepalWidthCm` columns. The code is concise and directly achieves the goal.

Areas for improvement:
- **Error Handling**: The generated code lacks explicit error handling (e.g., `FileNotFoundError` for the CSV, `KeyError` if columns are missing). While not explicitly requested in the prompt's requirements list, robust code for data science tasks often includes such handling to prevent crashes and provide informative messages to the user. The ground truth demonstrates good practice in this regard.
- **Comments/Readability**: While the task's brevity makes detailed comments less crucial, adding a few high-level comments to explain each step (loading, filtering, selecting) similar to the ground truth would enhance readability for larger or more complex scripts.",grok-3-mini-beta,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('iris.csv')

# Create the scatter plot
ax = df.plot.scatter(x='SepalLengthCm', y='SepalWidthCm', c='Species')

# Add title and labels
ax.set_title('Sepal Length vs Sepal Width')
ax.set_xlabel('Sepal Length (cm)')
ax.set_ylabel('Sepal Width (cm)')

# Display the plot
plt.show()
```",6,"The generated code loads the data and creates a scatter plot with a title and axis labels, addressing most requirements. However, it fails to color the points according to the `Species` column as requested by the prompt. The `c='Species'` argument in `df.plot.scatter` in pandas does not directly map categorical values to distinct colors for scatter plots in the same way `hue` would in seaborn. Instead, it might convert the categorical column to numerical representation or cause an error depending on the pandas version and underlying backend. This is a critical functional omission for the prompt's requirements.

To improve, the code should explicitly handle coloring by species. A common approach is to iterate through unique species and plot each subsets with a different color, similar to the ground truth, or use `seaborn.scatterplot` which has a direct `hue` argument for this purpose. The axis labels are also slightly different from the expected `SepalLengthCm` and `SepalWidthCm` but are semantically correct for the plot. Removing the `ax =` assignment and directly calling `plt.scatter` with iteration or using seaborn would be better for coloring.",grok-3-mini-beta,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# 1. List all column names and identify the data type of each column
print(""Column names and data types:"")
for column, dtype in df.dtypes.items():
    print(f""{column}: {dtype}"")

# 2. Calculate and report basic descriptive statistics for specified numerical columns
numerical_columns = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']
for column in numerical_columns:
    print(f""\nStatistics for {column}:"")
    print(f""Mean: {...",7,"The generated code correctly performs all the required operations: loading the CSV, listing column names and data types, calculating descriptive statistics for the specified numerical columns, and reporting value counts for 'HeartDisease'.

Areas for improvement:
1.  **Error Handling**: The ground truth code includes robust `try-except` blocks for `FileNotFoundError` and general exceptions during file loading. The generated code lacks this, which is crucial for real-world applications where the file might not always be present.
2.  **Output Clarity/Formatting**: While functional, the output format for descriptive statistics could be more concise and easier to read, similar to how `df[column].describe()` presents it. The generated code prints each statistic on a separate line for each column, which can be verbose. The ground truth uses `describe()` and then selects specific rows, which is more idiomatic pandas for this type of aggregation.
3.  **Efficiency for Statistics**: The ground truth calculates statistics for multiple columns in one go using `df[existing_numerical_cols].describe()`, which is more efficient than iterating through each column and calling `mean()`, `median()`, etc., individually as the generated code does.
4.  **Descriptive Print Statements**: The ground truth code includes more descriptive print statements that clearly delineate each section (e.g., ""--- Column Names and Data Types ---,"" ""--- Descriptive Statistics for Numerical Columns ---""), improving readability.
5. **Column Existence Check**: While the problem implies the columns exist, robust code would check if `Age`, `RestingBP`, `Cholesterol`, `MaxHR`, and `HeartDisease` actually exist in the DataFrame before attempting operations on them, as done in the ground truth for selected numerical columns and `HeartDisease`. This prevents `KeyError` if the dataset structure changes.",grok-3-mini-beta,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# Filter the dataset
filtered_df = df[(df['Age'] > 50) & (df['HeartDisease'] == 1)]

# Select and display the specified columns
selected_df = filtered_df[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]
print(selected_df)
```",9,"The generated code correctly loads the dataset, filters it based on the specified criteria (`Age` > 50 AND `HeartDisease` == 1), and then correctly selects and displays the required columns (`Age`, `Sex`, `ChestPainType`, and `Cholesterol`). It properly uses pandas for all operations and includes the necessary import. The code is concise and directly addresses all parts of the task.

The only minor area for improvement that prevents a perfect 10 is the lack of error handling (e.g., for `FileNotFoundError` if 'heart.csv' doesn't exist, or `KeyError` if a column is missing). While the prompt asked for ""only the Python code without explanations"" and didn't explicitly request robust error handling, best practices in production-ready code would include it. However, for a direct solution to the data manipulation task itself, it's very good. The ground truth provides a more robust, function-based structure with error handling, which is generally better practice for reusable code.",grok-3-mini-beta,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Filter the dataframe
filtered_df = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]

# Identify the wine with the highest quality score
max_quality_index = filtered_df['quality'].idxmax()
best_wine = filtered_df.loc[max_quality_index]

# Display all features of the specific wine
print(best_wine)
```",9,"The generated code correctly implements all the core requirements of the task:
1. It correctly applies the compound filtering condition for 'alcohol' and 'pH'.
2. It correctly identifies the wine with the highest quality score within the filtered subset using `idxmax()` and `loc`.
3. It correctly displays all features of the identified wine.
4. It includes the necessary `pandas` import.

Strengths:
- Concise and to the point.
- Effectively uses pandas capabilities (`idxmax`, boolean indexing `&`, `.loc`).

Areas for minor improvement (why it's not a 10 and how the ground truth is better):
- **Error Handling:** The generated code lacks robust error handling (e.g., `FileNotFoundError` if `winequality-red.csv` is missing, or `KeyError` if columns are misnamed). The ground truth includes comprehensive `try-except` blocks.
- **Edge Case Handling:** If `filtered_df` ends up being empty (i.e., no wines match the criteria), `idxmax()` would raise an error. The ground truth explicitly checks for an empty filtered DataFrame and prints a message.
- **Readability/Output:** While `print(best_wine)` works, `print(best_wine.to_string())` as used in the ground truth can be slightly better for ensuring all features are displayed without potential truncation by default pandas `print` behavior for Series, especially in environments without wide display settings.
- **Encapsulation/Modularity:** The ground truth encapsulates the logic within a function (`analyze_wine_quality`) and uses a `if __name__ == ""__main__""` block, which promotes better code organization, reusability, and testability, especially for larger scripts. The generated code is more of a script.

Despite these minor areas for improvement, the core logic for solving the problem is perfectly implemented.",grok-3-mini-beta,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('heart.csv')

# Generate histogram for Age
plt.figure(figsize=(8, 6))
df['Age'].hist()
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Generate bar chart for Sex
sex_counts = df['Sex'].value_counts()
plt.figure(figsize=(8, 6))
sex_counts.plot(kind='bar')
plt.title('Distribution of Sex')
plt.xlabel('Sex')
plt.ylabel('Count')
plt.show()
```",7,"The generated code correctly loads the dataset and generates the requested plots with titles and labeled axes. This fulfills the core requirements of the task.

However, there are several areas for improvement:
- **Plotting Library:** While `matplotlib.pyplot` is used, the prompt explicitly mentioned ""Use matplotlib or seaborn for plotting"", and the `Ground Truth Code` utilized Seaborn, which often produces more aesthetically pleasing and informative plots (e.g., `sns.histplot` with KDE, `sns.countplot` for bar charts of categories). The generated code uses basic pandas `.hist()` and `.plot(kind='bar')` which is functional but less sophisticated.
- **Sex Column Interpretation:** The `heart.csv` dataset typically encodes 'Sex' as 0 for Female and 1 for Male. The generated code directly plots these numerical values on the x-axis (`0` and `1`), which is less interpretable than converting them to 'Male' and 'Female' labels as done in the ground truth.
- **Error Handling:** The ground truth code includes robust `try-except` blocks for `FileNotFoundError`, `KeyError`, and general exceptions, which is crucial for production-ready code. The generated code lacks any error handling.
- **Customization/Readability:** The ground truth code includes more detailed customization for plots (e.g., `color`, `kde`, `bins` for histogram; specific `palette`, `xticks` rotation for bar chart; `grid` and `tight_layout`), and provides more descriptive labels (e.g., ""Age (Years)"", ""Number of Individuals""). While not strictly required by the prompt, these improve the quality and interpretability of the visualizations.
- **Import:** `seaborn` was not imported or used.

Overall, it's a correct but basic implementation. Adopting Seaborn and improving plot aesthetics/readability, along with adding error handling, would significantly improve the solution.",grok-3-mini-beta,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# List all column names and their data types
print(""Column names:"", df.columns.tolist())
print(""Data types:\n"", df.dtypes)

# Convert 'streams' column to numeric, coercing errors to NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
print(""Number of NaNs in streams:"", df['streams'].isna().sum())

# Calculate descriptive statistics for specified numerical columns
numerical_cols = ['streams', 'bpm', 'd...",7,"The generated code correctly addresses most of the tasks, but lacks some robustness and completeness compared to the ground truth.

**Strengths:**
- Correctly loads the CSV.
- Correctly lists column names and data types (though `df.dtypes` is less informative than `df.info()`).
- Successfully converts 'streams' to numeric with `errors='coerce'` and reports NaNs.
- Correctly calculates descriptive statistics for the specified columns.
- Accurately counts unique artists.

**Areas for Improvement:**
- **Error Handling for CSV Loading:** The generated code lacks a `try-except` block for `FileNotFoundError` when loading the CSV, which is crucial for robust scripts.
- **Reporting Column Info:** While `df.columns.tolist()` and `df.dtypes` fulfill the requirement, `df.info()` as used in the ground truth provides a more comprehensive and standard way to display column names, non-null counts, and data types, which is generally preferred in data analysis.
- **Specific Statistics:** The task asked for ""mean, median, std, min, max"". While `describe()` outputs all of these, it also outputs 25th and 75th percentiles. The ground truth explicitly filters for the requested statistics, making the output more precise to the prompt's request.
- **Clarity of Output:** The ground truth includes more descriptive print statements for each step, making the output easier to understand and follow, which is important for a benchmark evaluation. For example, explicitly stating ""Column Names and Data Types"" or ""Converting 'streams' to Numeric and Reporting NaNs"" enhances readability.
- **Robustness for Missing Columns:** The ground truth includes checks like `if col in df.columns:` when selecting columns for statistics, and `if 'artist(s)_name' in df.columns:` which prevents potential `KeyError` if the dataset structure deviates slightly. This is good practice.",grok-3-mini-beta,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Ensure the streams column is numeric, coercing errors to NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# Filter for released_year == 2023
filtered_df = df[df['released_year'] == 2023]

# Select the required columns
selected_df = filtered_df[['track_name', 'artist(s)_name', 'streams']]

# Sort by streams in descending order
sorted_df = selected_df.sort_values(by='streams', ascending=Fals...",9,"The generated code correctly performs all requested operations: loading the CSV, ensuring 'streams' is numeric, filtering by year, selecting columns, and sorting. It uses appropriate `pandas` functions and follows a logical flow.

**Strengths:**
*   Correctly imports `pandas`.
*   Loads the CSV.
*   Correctly uses `pd.to_numeric` with `errors='coerce'` for the 'streams' column.
*   Filters the DataFrame by `released_year == 2023`.
*   Selects the specified columns (`track_name`, `artist(s)_name`, `streams`).
*   Sorts the data by `streams` in descending order.
*   Displays the final DataFrame.

**Minor Area for Improvement (Not critical for this task):**
*   The ground truth included error handling for `FileNotFoundError`. While not explicitly requested in the prompt's requirements for the *generated* code, it's good practice for robust applications. For a benchmark task focused on data manipulation steps, its absence is not a major flaw.
*   The ground truth explicitly defined `DATASET_PATH` and handled the `dropna` before sorting. While `dropna` is technically not needed for the core requirement (sorting will still put NaNs at the end), it can sometimes lead to cleaner output if NaNs are not desired in the sorted list. However, the prompt didn't explicitly forbid NaNs in the final display.

Overall, it's a very solid and correct solution for the given requirements.",grok-3-mini-beta,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Generate histogram for the 'bpm' column
plt.figure(figsize=(10, 5))
plt.hist(df['bpm'], bins=30)
plt.title('Distribution of BPM')
plt.xlabel('BPM')
plt.ylabel('Frequency')
plt.show()

# Create scatter plot for 'danceability_%' vs 'energy_%'
plt.figure(figsize=(10, 5))
plt.scatter(df['danceability_%'], df['energy_%'])
plt.title('Relationship between Danceability and Energy')
p...",8,"The generated code correctly addresses all requirements: loading the CSV, generating a histogram for `bpm`, and a scatter plot for `danceability_%` vs `energy_%`. Both plots have titles and labeled axes, and proper imports are included.

Areas for improvement to achieve a higher score and match the robustness of the ground truth:
1.  **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and `KeyError` (if columns are missing), which is crucial for real-world applications. The generated code lacks this.
2.  **Clarity/Readability:** While not strictly necessary for this specific problem, defining column names as constants (like in the ground truth) can improve readability and maintainability for larger projects.
3.  **Plot Customization (Minor):** The ground truth adds minor visual enhancements like `edgecolor='black'` and `alpha` for better histogram appearance, and `s=50` for scatter plot marker size, along with gridlines. These are good practices for professional-looking plots, though not explicitly required.
4.  **`plt.show()` Placement:** The current `plt.show()` after each plot will display them one by one. Often, it's preferred to show all plots at once at the very end of the script using `plt.tight_layout()` and then a single `plt.show()` to manage multiple figures displayed simultaneously.",grok-3-mini-beta,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# List all column names and their inferred data types
print(""Column names and their inferred data types:"")
print(df.dtypes)

# Clean the specified columns
columns_to_clean = ['Video views', 'Likes', 'Dislikes']
for col in columns_to_clean:
    df[col] = pd.to_numeric(df[col].str.replace(',', ''), errors='coerce')
    df[col] = df[col].fillna(0)

# Calculate and report descriptive statistics
stats = df[columns_...",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans the specified numeric columns by removing commas, converting to numeric types, and filling NaNs with 0. It calculates and reports descriptive statistics for these columns (though it uses `describe()` which includes median as '50%', which is acceptable for 'median'). It correctly fills missing 'Category' values with 'Unknown' and reports the count of unique categories. The code is concise and efficient.

One minor point for improvement, not impacting correctness:
- The ground truth includes more robust error handling for `FileNotFoundError` when loading the CSV, which is good practice for real-world scenarios. The generated code assumes the file will always be present, which is typical for benchmark tasks but less robust.
- The reporting of descriptive statistics directly from `describe().loc[['mean', '50%', 'std', 'min', 'max']]` is very efficient, but the prompt asked for `mean, median, std, min, max` explicitly, and `50%` is technically a quantile, though it serves as the median. This is a very minor stylistic difference.

Overall, it's a strong solution that successfully completes the task.",grok-3-mini-beta,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Process the 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '')
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce')
df['Video views'] = df['Video views'].fillna(0)

# Filter the dataset
filtered_df = df[df['Video views'] > 50000000]

# Select columns and sort
result_df = filtered_df[['Video', 'Video views']].sort_values(by='Video views', ascending=False)

# Displ...",9,"The generated code correctly performs all the required steps: loading the CSV, cleaning the 'Video views' column (removing commas, converting to numeric, filling NaNs with 0), filtering videos with views greater than 50,000,000, selecting the 'Video' and 'Video views' columns, and sorting the result in descending order by views. It uses proper pandas operations.

The only minor difference from the ground truth is the lack of a `try-except` block for `FileNotFoundError` when loading the CSV. While the prompt stated to assume the CSV is in the current directory, robust code often includes this check. However, for a benchmark focusing on the core data manipulation logic, its absence is a minor point. The `regex=False` in `str.replace` is also a minor optimization/best practice in the ground truth that is missing in the generated code, but doesn't affect correctness for this specific task.",grok-3-mini-beta,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the 'Likes' column
df['Likes'] = df['Likes'].astype(str).str.replace(',', '')
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

# Clean the 'Video views' column
df['Video views'] = df['Video views'].astype(str).str.replace(',', '')
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce')
df['Video views'] = df['Vi...",9,"The generated code correctly loads the dataset, cleans the 'Likes' and 'Video views' columns by removing commas, converting to numeric, and filling NaNs with 0, and then generates the specified scatter plot with the correct title and axis labels.
The code is concise and directly addresses all requirements.
The only minor point, which doesn't affect correctness but could improve robustness in a real-world scenario, is the lack of error handling for `pd.read_csv` (e.g., `FileNotFoundError`). However, given the prompt's simplicity and focus on the core task, this is not a major deduction. The prompt also stated ""Provide only the Python code without explanations"", which the generated code adheres to.

Overall, it's a very good solution that perfectly accomplishes the task.",grok-3-mini-beta,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the columns
df['Mobile Weight'] = df['Mobile Weight'].str.replace('g', '').astype(float, errors='coerce')
df['RAM'] = df['RAM'].str.replace('GB', '').astype(float, errors='coerce')
df['Battery Capacity'] = df['Battery Capacity'].str.replace('mAh', '').astype(float, errors='coerce')
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', '').astype(float, errors='coer...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Correct Cleaning:** Successfully cleans `Mobile Weight`, `RAM`, `Battery Capacity`, and `Launched Price (USA)` columns, removing specified strings and converting them to numeric, handling errors with `coerce`.
- **Data Type Listing:** Correctly prints the data types after cleaning.
- **Descriptive Statistics:** Calculates and prints the descriptive statistics for the specified numeric columns.
- **Unique Company Count:** Correctly reports the count of unique values in the `Company Name` column.
- **Conciseness:** The code is quite concise and directly implements the requirements.

**Areas for Minor Improvement:**
- **Error Handling for File Loading:** The generated code lacks explicit error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. While the prompt implies the file will be present, robust code usually includes this.
- **Output Clarity:** The ground truth separates the output sections with descriptive print statements (e.g., ""--- Cleaning Data ---"", ""--- Column Names and Inferred Data Types After Cleaning ---""), which improves readability and understanding of the output. The generated code directly prints the results without much context.
- **Import Statements:** While `import pandas as pd` is sufficient, `import numpy as np` from the ground truth is often included as a standard practice when working with pandas, although it's not strictly used for this specific task.
- **Stripping Whitespace:** For `Mobile Weight`, `RAM`, and `Battery Capacity` columns, after replacing 'g', 'GB', 'mAh' respectively, it's good practice to also `strip()` any leading/trailing whitespace before converting to numeric, just in case the original data had spaces (e.g., ""150 g ""). The ground truth includes `str.strip()` which is slightly more robust. This didn't seem to cause an issue for this dataset, but it's a good practice.
- **Type Coercion for String Operations**: Using `df['Column'].astype(str)` before string operations (like `replace`, `strip`) is a good defensive programming practice, as the type might not always be string initially (e.g., if there are NaNs or numeric values that need to be treated as strings for character removal). The ground truth includes this.

Overall, it's a very good solution that successfully achieves the core requirements. The minor points are mostly about robustness, output formatting, and defensive programming practices.",grok-3-mini-beta,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean Launched Price (India) column
df['Launched Price (India)'] = df['Launched Price (India)'].astype(str).str.replace('INR ', '').str.replace(',', '')
df['Launched Price (India)'] = pd.to_numeric(df['Launched Price (India)'], errors='coerce')

# Clean RAM column
df['RAM'] = df['RAM'].astype(str).str.replace('GB', '')
df['RAM'] = pd.to_numeric(df['RAM'], errors='coerce')

# Filter the dataset
filtered_df =...",10,"The generated code correctly addresses all requirements in the task. It loads the CSV, cleans both `Launched Price (India)` and `RAM` columns as specified (including handling conversion errors to NaN), filters the data for 'Apple' and price less than 100,000, selects the required columns (`Model Name`, `RAM`, `Launched Price (India)`), and sorts the result by `Launched Price (India)` in ascending order. The code is concise and efficient, demonstrating a good understanding of pandas operations. The `astype(str)` before string operations is a good practice to handle potential non-string types.",grok-3-mini-beta,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the RAM column
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')

# Clean the Battery Capacity column
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')

# Create the scatter plot
plt.scatter(x=df['RAM'], y=df['Battery Capacity'], c=df['Company Name'])
plt.title('Relationship between RAM and Battery...",6,"The generated code attempts to address all requirements but falls short in a few critical areas.

**Strengths:**
- Correctly loads the CSV.
- Successfully attempts to clean `RAM` and `Battery Capacity` columns by removing units and converting to numeric with error handling.
- Creates a scatter plot with appropriate labels and title.

**Areas for Improvement:**
- **Coloring by `Company Name`:** The most significant issue is `c=df['Company Name']` in `plt.scatter()`. Matplotlib's `scatter` function expects `c` to be a sequence of floats or a single color string when passed as a keyword argument. It does not automatically map categorical strings to distinct colors like Seaborn's `hue` parameter. This line will likely raise an error (e.g., `ValueError: 'c' argument has one entry per-point, but at least one entry per-point is not a numeric value`) or produce an incorrect plot (if the categorical data is coerced in an unexpected way by a specific matplotlib version). This fails requirement 4.
- **Handling of `astype(str)`:** While `str.replace` works on string columns, it's safer to include `.astype(str)` before `.str.replace()` if there's any chance the original column might contain non-string types (e.g., numbers, NaN) that could cause `.str` accessor failures. The ground truth correctly includes this.
- **Robustness for file loading:** The ground truth includes `try-except FileNotFoundError`, which is good practice for scripts dealing with external files. The generated code does not.
- **Code Clarity/Best Practices:** While not strictly incorrect, using `sns.scatterplot` as in the ground truth often provides better aesthetics and simpler handling of categorical coloring (`hue`).

To achieve a higher score, the primary fix would be to correctly implement the coloring by `Company Name`, either by manually mapping colors to companies or by using a library like Seaborn that handles this automatically with `hue`.",grok-3-mini-beta,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('titanic.csv')

# Display the first 5 rows
print(""First 5 rows:"")
print(df.head(5))

# Show the shape of the dataset
print(""Shape of the dataset:"")
print(df.shape)

# List all column names and their data types
print(""Column names and data types:"")
print(df.dtypes)

# Calculate the survival rate
survival_rate = (df['Survived'].mean() * 100)
print(""Survival rate:"")
print(f""{survival_rate}%"")

# Show the count of missing values for each c...",9,"The generated code is very good and largely fulfills all requirements of the task.

**Strong Points:**
- **Correctness:** All calculations and data displays are correct.
- **Completeness:** It addresses all five requirements outlined in the prompt (load data, shape, dtypes, survival rate, missing values).
- **Conciseness:** The survival rate calculation `df['Survived'].mean() * 100` is a very concise and effective way to calculate the survival rate, as `mean()` directly gives the proportion of 1s in a binary column.
- **Proper Imports:** `pandas` is correctly imported.

**Areas for Minor Improvement (Why not a 10):**
- **Error Handling:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is robust practice. While not explicitly asked for, it's good defensive programming, especially in data analysis scripts. The generated code lacks this.
- **Clarity of Output:** The ground truth code includes specific labels like ""1. First 5 rows of the dataset:"", ""2. Shape of the dataset..."", etc., which make the output much clearer and easier to follow, especially when dealing with multiple output blocks. The generated code's labels are a bit more generic.
- **`df.dtypes` vs. `df.info()`:** While `df.dtypes` is correct for listing data types, `df.info()` (used in ground truth) provides more comprehensive information (non-null counts, memory usage) which is generally more useful in initial EDA without cluttering the output excessively. For just column names and types, `df.dtypes` is fine, but `df.info()` is often preferred in a full EDA context.

Overall, it's an excellent attempt and achieves the core task requirements very well. The minor improvements are mostly about robust error handling and presenting the output in a more user-friendly way.",grok-3-mini-beta,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 1. List all column names and their data types
print(""Column names and their data types:"")
for column, dtype in df.dtypes.items():
    print(f""{column}: {dtype}"")

# 2. Calculate and report descriptive statistics for Year, Rating, and Runtime
print(""\nDescriptive statistics:"")
for col in ['Year', 'Rating', 'Runtime']:
    print(f""\nFor column {col}:"")
    print(f""Mean: {df[col].mean()}"")
    print(f""Median: {df[co...",7,"The generated code correctly loads the dataset, lists column names and data types, and reports the total number of movies. It also calculates the basic descriptive statistics for the specified columns.

However, there are a few areas for improvement, primarily regarding robustness and handling potential data issues:

1.  **Robustness for 'Runtime' column**: The ground truth code includes crucial preprocessing for the 'Runtime' column, converting it from a string format (e.g., '142 min') to a numeric type. This is a very common scenario for such datasets. The generated code assumes 'Runtime' is already numeric, which would cause an error if it's encountered as a string.
2.  **Error Handling**: The generated code lacks error handling for `FileNotFoundError`. If the specified CSV file is not present, the script would simply crash. The ground truth code includes a `try-except` block for this.
3.  **Output Formatting for Statistics**: While the statistics are correct, the ground truth uses `df.describe()` and then selects specific rows, which provides a more compact and often preferred tabular output for multiple statistics, rather than printing each statistic individually for each column.
4.  **Clarity of Imports**: While `pandas` is imported, `sys` is required for robust error handling (as shown in the ground truth for `sys.exit()`).

Overall, it's a good start, but misses important considerations for real-world data handling and robustness.",grok-3-mini-beta,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Filter the dataset
filtered_df = df[df['Rating'] >= 9.0]

# Select and sort the columns
selected_df = filtered_df[['Title', 'Rating']]
sorted_df = selected_df.sort_values(by='Rating', ascending=False)

# Display the result
print(sorted_df)
```",9,"The generated code correctly implements all the requirements of the task: it loads the CSV, filters movies with a rating greater than or equal to 9.0, selects the 'Title' and 'Rating' columns, and sorts the result by 'Rating' in descending order, finally printing the output. The code is concise and clear.

The only minor area where it falls short compared to the ground truth is the lack of error handling (e.g., `FileNotFoundError`, `KeyError`), which is crucial for robust real-world applications. However, the prompt specifically asked for ""only the Python code without explanations"" and focused on the core data manipulation steps, without explicitly demanding error handling. Given the specific constraints for this task, the generated code performs excellently.",grok-3-mini-beta,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create histogram for the Rating column
plt.figure(figsize=(8, 6))
plt.hist(df['Rating'], bins=20, edgecolor='black')
plt.title('Distribution of Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# Create scatter plot for Rating vs. Runtime
plt.figure(figsize=(8, 6))
plt.scatter(df['Runtime'], df['Rating'], alpha=0.5)
plt.title('Rating vs. Runtime')
plt.xlabel('Runti...",6,"The generated code correctly loads the CSV, creates a histogram for 'Rating', and a scatter plot for 'Rating' vs. 'Runtime'. It also includes titles and axis labels as requested.

However, there are significant areas for improvement, particularly regarding data handling and robustness:

1.  **Runtime Column Preprocessing:** The most critical missing piece is the preprocessing of the 'Runtime' column. In the `imdb_top_250.csv` dataset, 'Runtime' is typically a string (e.g., ""120 min""). The generated code attempts to directly plot `df['Runtime']` which will likely cause an error or produce an empty scatter plot if the column is not numeric. The ground truth code correctly handles this by extracting numerical values and converting them.
2.  **Error Handling (File Not Found):** The generated code does not include any error handling for `FileNotFoundError` if `imdb_top_250.csv` is not present, which the ground truth code does.
3.  **Missing `seaborn` import/usage:** While `matplotlib` is sufficient, the task description specifically mentioned ""Use matplotlib or seaborn for plotting."" The generated code only uses `matplotlib`. While not a critical flaw, it's a minor missed opportunity if `seaborn` was intended.
4.  **Plot Aesthetics:** While the basic plots are there, the ground truth code uses `seaborn` for potentially more aesthetically pleasing plots (e.g., `sns.histplot` with `kde`) and includes `grid` and `tight_layout` for better presentation. This is less about correctness and more about quality and detail.

Overall, it addresses the core plotting requirements but lacks the robustness and practical data preprocessing needed for real-world data, leading to potential failures with the given dataset.",grok-3-mini-beta,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Report the number of missing values for each column
print(df.isnull().sum())

# Define the numerical columns
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']

# Drop rows where any of these four columns have missing values
df_clean = df.dropna(subset=numerical_cols)

# Calculate and report descriptive statistics for the four columns
for col in numerical_cols:
    print(f""St...",7,"The generated code correctly addresses most of the task requirements.

**Strengths:**
*   **Correct Loading:** Loads the `penguins.csv` dataset correctly.
*   **Correct NA Handling:** Correctly drops rows based on NAs in the specified subset of numerical columns.
*   **Correct Statistics Calculation:** Calculates and reports mean, median, standard deviation, min, and max for the numerical columns on the filtered data.
*   **Correct Unique Counts:** Accurately counts and reports unique values for 'species' and 'island'.

**Areas for Improvement:**
*   **Missing Value Reporting Clarity:** While `print(df.isnull().sum())` provides the raw counts, it doesn't filter out columns with no missing values, nor does it explicitly state ""Missing Values Report"" as requested or explain what the numbers mean, making the output less user-friendly than the ground truth.
*   **Descriptive Statistics Format:** The descriptive statistics are printed column by column with repeated labels, which is less concise and harder to read at a glance compared to the aggregate table format provided by `agg()` in the ground truth. The prompt asked to ""report basic descriptive statistics (mean, median, standard deviation, min, max)"" which is better represented in a tabular form.
*   **Robustness/Error Handling:** The code lacks error handling for `FileNotFoundError` when loading the CSV, which is a good practice for real-world scenarios.
*   **Output Readability/Formatting:** The ground truth includes more descriptive print statements (e.g., ""--- Missing Values Report ---"", ""--- Descriptive Statistics for Filtered Numerical Columns ---"") and print statements about row count changes, making the output much clearer and more structured. The generated code just prints the raw data without much context.
*   **Completeness:** The prompt asked to ""Report the number of missing values (NA) for each column"" and implicitly that it should be clear. The ground truth took care of ""reporting"" better by adding context and filtering out zeros. Also, the prompt asked to ""report basic descriptive statistics"" which the generated code does but in a less ideal format. The ground truth also adds context about the number of rows before and after filtering, which is a good practice but not explicitly asked for.",grok-3-mini-beta,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the dataset
filtered_df = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')]

# Select the columns
selected_df = filtered_df[['culmen_length_mm', 'culmen_depth_mm']]

# Drop rows with missing values
cleaned_df = selected_df.dropna()

# Display the first 5 rows
print(cleaned_df.head())
```",9,"The generated code correctly performs all the required steps: loading the CSV, filtering by species and island, selecting specific columns, dropping NA values, and displaying the first 5 rows. The code is concise and well-structured, following common pandas practices.

The only minor area for improvement is in the `dropna()` call. While `selected_df.dropna()` will correctly drop rows with `NaN`s in `culmen_length_mm` or `culmen_depth_mm` because these are the only columns in `selected_df`, explicitly providing the `subset` argument (e.g., `selected_df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm'])`) as in the ground truth is often considered better practice for readability and robustness, especially if `selected_df` were to contain other columns in a more complex scenario. However, for this specific task, `dropna()` without `subset` has the same effect and is therefore still considered correct. The lack of error handling for `FileNotFoundError` is also a minor omission but not a task requirement for the core data manipulation.",grok-3-mini-beta,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the dataframe
filtered_df = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])

# Create the scatter plot
sns.scatterplot(data=filtered_df, x='culmen_length_mm', y='culmen_depth_mm', hue='species')

# Add title and labels
plt.title('Culmen Length vs Culmen Depth')
plt.xlabel('Culmen Length (mm)')
plt.ylabel('Culmen Depth (mm)')

# Show the...",9,"The generated code correctly loads the data, filters it as required, and creates the specified scatter plot with appropriate labels and title. It uses the correct libraries (pandas, matplotlib, seaborn).

**Strengths:**
*   Correctly loads `penguins.csv` using `pd.read_csv`.
*   Correctly filters out rows where `culmen_length_mm`, `culmen_depth_mm`, or `species` are missing using `dropna(subset=...)`.
*   Successfully generates a scatter plot with `culmen_length_mm` on the x-axis and `culmen_depth_mm` on the y-axis.
*   Correctly colors the points by `species` using the `hue` argument in `sns.scatterplot`.
*   Includes a title (`plt.title`) and axis labels (`plt.xlabel`, `plt.ylabel`).
*   Properly imports all necessary libraries.

**Areas for minor improvement (not critical enough to reduce score significantly):**
*   The ground truth code includes error handling for `FileNotFoundError`, which is good practice for robust code but not explicitly requested in the prompt.
*   The ground truth also wraps the logic in a function and uses an `if __name__ == '__main__':` block, which is better for reusability and scripting, but again, not explicitly requested.
*   The ground truth adds `s=70` and `alpha=0.8` to `scatterplot` and `bbox_to_anchor` for the legend, which are minor aesthetic enhancements. The prompt only asked for a plot, title, labels, and coloring by species, which the generated code fulfills.",grok-3-mini-beta,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Step 1: Read the dataset
df = pd.read_csv('penguins.csv')

# Step 2: Fill NA values with the mean for the specified columns
if 'culmen_length_mm' in df.columns:
    mean_length = df['culmen_length_mm'].mean()
    df['culmen_length_mm'].fillna(mean_length, inplace=True)

if 'culmen_depth_mm' in df.columns:
    mean_depth = df['culmen_depth_mm'].mean()
    df['culmen_depth_mm'].fillna(mean_depth, inplace=True)

# Step 3: Calculate and print mean, median, and standa...",8,"The generated code correctly addresses all parts of the task: reading the dataset, filling NA values in the specified columns with their respective means, calculating and printing the mean, median, and standard deviation for the cleaned columns, and printing the total number of rows and columns.

**Strong Points:**
- Correctly uses pandas for data manipulation.
- Accurately fills NA values with the mean for each specified column.
- Calculates and prints the requested statistics (mean, median, standard deviation) for both `culmen_length_mm` and `culmen_depth_mm`.
- Correctly reports the total number of rows and columns using `df.shape`.
- Includes checks (`if 'column_name' in df.columns:`) before attempting operations on columns, which is good practice for robustness.

**Areas for Improvement (to achieve a higher score):**
- **Error Handling for File Not Found:** The code lacks error handling for `FileNotFoundError` when attempting to read the CSV, which the ground truth includes. This makes the code less robust in real-world scenarios.
- **Code Duplication/Readability:** There's some repetition in the code, particularly in Step 2 and Step 3, where similar operations are performed for `culmen_length_mm` and `culmen_depth_mm` using separate `if` blocks. The ground truth elegantly handles this with a loop over a list of numerical columns, which improves maintainability and reduces redundancy.
- **Clarity of Output Formatting:** While the output is correct, the ground truth's output formatting with headers and more detailed explanations (e.g., NA imputation summary, initial NA count, mean used for imputation) provides better diagnostic information and a more professional presentation. For instance, the ground truth explicitly states the mean used for imputation and the NA count before/after, which is helpful for verification.
- There's no explicit `inplace=True` for the `df.fillna()` calls in the ground truth, as `df = df.fillna()` reassigns the Series directly, which is also a valid approach. However, the generated code uses `inplace=True` which is fine.",grok-3-mini-beta,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows where flipper_length_mm is NA
df = df.dropna(subset=['flipper_length_mm'])

# Create a histogram of the flipper_length_mm column
plt.figure(figsize=(10, 6))
plt.hist(df['flipper_length_mm'], bins=20, edgecolor='black')
plt.title('Distribution of Flipper Length (mm)')
plt.xlabel('Flipper Length (mm)')
plt.ylabel('Frequency')
plt.show()

# Create a bar chart showing the c...",8,"The generated code correctly addresses all the requirements of the task. It successfully loads the dataset, handles NA values, creates a labeled histogram for flipper length, and generates a labeled bar chart for island counts. The use of `matplotlib.pyplot` is appropriate.

**Strong Points:**
*   **Correctness:** All tasks are completed as requested.
*   **Clarity:** The code is straightforward and easy to understand.
*   **Functional:** It produces the required plots.

**Areas for Improvement (to achieve a higher score):**
*   **Robustness:** The ground truth includes error handling for `FileNotFoundError` when loading the CSV, which is a good practice for real-world scenarios. The generated code assumes the file exists.
*   **Visualization Enhancement:** While `matplotlib.pyplot` is used correctly, incorporating `seaborn` (as in the ground truth) often leads to more aesthetically pleasing and informative plots with less code (e.g., `sns.histplot` and `sns.countplot`), and allows for easier integration of themes for professional-looking plots.
*   **Informative Output:** Adding print statements for debugging or informing the user about data loading status or NA handling (like the ground truth does) can improve the user experience and code's observability.
*   **Axis Labeling Consistency:** For the histogram, 'Frequency' is used on the y-axis, which is fine, but 'Count' (as in the bar chart and often used with histograms of counts) might be slightly more consistent or common for simple count-based histograms.
*   **Code Structure:** While pandas and matplotlib are imported, the ground truth also imports seaborn and sets a style for consistency, which is a nice touch for a data science benchmark.",grok-3-mini-beta,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the data for penguins from 'Torgersen' island
filtered_df = df[df['island'] == 'Torgersen']

# Count and print the number of penguins for each species in the filtered subset
species_counts = filtered_df['species'].value_counts()
print(species_counts)
```",9,"The generated code correctly performs all the required steps: loading the dataset, filtering by 'Torgersen' island, and counting species. It directly addresses the prompt's requirements.

**Strong points:**
*   **Correctness:** The logic for loading, filtering, and counting is sound and directly produces the expected output.
*   **Conciseness:** It achieves the task with minimal and readable code.
*   **Proper library usage:** Uses pandas effectively for data manipulation.

**Areas for improvement (minor):**
*   **Error Handling:** The ground truth includes robust error handling (e.g., `FileNotFoundError` for the CSV, checking for column existence before access). The generated code assumes the file exists and that the 'island' and 'species' columns are present, which could lead to runtime errors in less ideal scenarios.
*   **Clarity of Output:** While `print(species_counts)` works, the ground truth adds descriptive headers and footers to the output, making it more user-friendly.

Overall, the generated code is highly functional and correct for the given task under ideal conditions. The lack of robust error handling is the primary reason it's not a perfect 10, but it successfully completes the core task.",grok-3-mini-beta,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Print the shape of the DataFrame
print(df.shape)

# Lis...",9,"The generated code correctly defines the DataFrame, prints its shape, and lists the data types of all columns, fulfilling all task requirements. The code is concise and efficient.

One minor point for improvement would be to add descriptive print statements before `df.shape` and `df.dtypes` outputs, similar to the ground truth code, to enhance clarity for the user. For instance, `print(""Shape of the DataFrame:"")` before `print(df.shape)`. This is a matter of presentation rather than correctness or functionality, hence the high score.",grok-3-mini-beta,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('titanic.csv')

# 1. Bar chart for survivors vs non-survivors
survived_counts = df['Survived'].value_counts()
plt.figure()
survived_counts.plot(kind='bar')
plt.title('Count of Survivors vs Non-Survivors')
plt.xlabel('Survived')
plt.ylabel('Count')
plt.xticks([0, 1], ['Non-Survivor', 'Survivor'])
plt.show()

# 2. Bar chart for distribution of passenger classes
pclass_counts = df['Pclass'].value_coun...",7,"The generated code creates all four required plots with appropriate titles and labels. It correctly uses pandas for data loading and matplotlib for plotting.

However, there are a few areas for improvement where it deviates from the ground truth or could be more robust/user-friendly:

1.  **Handling Missing Age Values:** The ground truth explicitly handles missing 'Age' values by filling them with the median. The generated code uses `df['Age'].hist()`, which by default will ignore NaN values, but it doesn't explicitly `fillna` them as requested in the prompt (""handle missing values appropriately""). While `hist` might handle it, the prompt implies a specific imputation step.
2.  **Readability/Clarity for Survival Plot:** While the generated code correctly plots survivors vs. non-survivors, the `plt.xticks([0, 1], ['Non-Survivor', 'Survivor'])` is a good effort, but the ground truth's approach of mapping the index `survivor_counts.index = survivor_counts.index.map(survivor_labels)` is generally more robust and cleaner for categorical plots where the index itself represents the category.
3.  **Visualization Aesthetics/Refinements:** The ground truth includes more aesthetic improvements like `figsize`, `color`, `xticks(rotation=0)`, `grid`, and `tight_layout`. While not strictly required by ""basic visualizations,"" these make the plots more professional and readable, which is a common expectation in data science visualization.
4.  **Error Handling:** The ground truth includes `try-except FileNotFoundError` for loading the CSV, which is good practice for robust code. The generated code lacks this.
5.  **Axis Labels for Count Plots:** For count plots (survivors, Pclass, gender), the ground truth uses ""Number of Passengers"" as the Y-axis label, which is more descriptive than ""Count"".

Overall, it's a solid attempt that fulfills the core requirements but lacks some of the robustness, clarity, and best practices present in the ground truth.",grok-3-mini-beta,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```
import pandas as pd

# Create the DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]
}
df = pd.DataFrame(data)

# Calculate and print the average Midterm_Score
average_m...",9,"The generated code correctly performs all the required tasks: it creates the DataFrame, calculates and prints the average midterm and final scores, and identifies the subject with the highest average final score. The code is well-structured and uses appropriate pandas functions.

The only minor improvements would be:
1. The ground truth code formats the output of the average scores to two decimal places (`.2f`), which is good practice for numerical results. The generated code does not.
2. The ground truth also explicitly prints the value of the highest average score for the identified subject, which adds more context to the output. While not explicitly asked for, it enhances readability.

These are minor stylistic and output formatting differences that do not affect the core correctness of the solution.",grok-3-mini-beta,LLM_judge_config
E51,easy,Time Series Analysis,,"```
import pandas as pd

data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
    'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
    'Volume': [1000000, 1200000, 1100000, 1300000, 9...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It successfully creates the DataFrame, sets 'Date' as the index, calculates the 'Daily_Range' column, finds the date with the highest 'Close' price, and prints the updated DataFrame as requested. The imports are correct, and the code structure is logical.

The only minor difference from the ground truth is in the printing of the date with the highest close price. The ground truth formats it for readability (`strftime('%Y-%m-%d')`) and adds a descriptive sentence, whereas the generated code simply prints the `Timestamp` object directly. While functionally correct, the ground truth's output is slightly more user-friendly. This is a very minor point and doesn't detract from the core functionality or correctness of the solution.",grok-3-mini-beta,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```
import pandas as pd

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUS...",10,"The generated code is correct, complete, and follows good code quality practices. It accurately creates the DataFrame, calculates the 'Total_Revenue' column, correctly identifies the best-selling product by quantity, and prints the first 5 rows of the DataFrame as requested. The imports are present, and the code structure is clear and concise. The output format also matches the requirements of the task.",grok-3-mini-beta,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate and print the average SessionDurationMinutes
average_duration = df['SessionDurationMinutes'].mean()
print(""Average ...",9,"The generated code is highly correct and complete, successfully addressing all requirements of the task. It correctly defines the DataFrame, calculates the average session duration, and counts users by device type using appropriate pandas methods (`.mean()` and `.value_counts()`). The code is clean, well-structured, and directly implements the specified tasks.

Areas for minor improvement (to achieve a perfect score of 10):
1.  **Output Formatting:** The ground truth code includes f-strings for more precise output formatting (e.g., `:.2f` for decimal places) and uses separators (`print(""-"" * 50)`) for better readability in the console, especially when multiple outputs are present. While the generated code's print statements are correct, they are less polished.
2.  **Comments:** The ground truth code provides extensive comments explaining each step's purpose and the rationale behind using specific pandas methods. While the generated code is simple enough that extensive comments might not be strictly necessary for understanding, adding brief comments for clarity, especially for those learning or reviewing the code, can be beneficial.

These points are minor stylistic and best-practice considerations rather than functional errors. The code performs exactly as requested.",grok-3-mini-beta,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

# Find and print the maximum TemperatureCelsius
max_temperature = df['TemperatureCels...",10,"The generated code is excellent. It correctly defines the DataFrame exactly as specified, and then accurately finds and prints both the maximum ""TemperatureCelsius"" and the minimum ""HumidityPercent"". The variable names are clear, and the use of pandas' built-in `.max()` and `.min()` methods is efficient and correct. All task requirements are fully met.",grok-3-mini-beta,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# Print the total number of ""Likes"" across all posts
total_likes = df['Likes'].sum()
print(total_likes)

# Find and print the ""PostType...",9,"The generated code correctly defines the DataFrame, calculates the total number of likes, and identifies the post type with the highest average number of shares. The logic implemented is accurate and follows the requirements.

However, the output format for the `print` statements is minimalistic, just printing the values without descriptive text. The ground truth code uses f-strings to provide more human-readable output, which enhances clarity for the user running the code. While the generated code produces the correct numerical and categorical answers, improving the print statements would make it more user-friendly and align better with typical data science output presentation.

Specifically:
- For total likes, it prints `520` instead of `Total number of Likes across all posts: 520`.
- For the PostType, it prints `Video` instead of `PostType with the highest average number of Shares: Video`.

This is a minor point related to presentation rather than correctness or functionality, hence a high score.",grok-3-mini-beta,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",9,"The generated code is highly correct and complete, successfully defining the DataFrame, calculating the average age, and counting patients by gender as requested. The code quality is good, with proper imports and a clear structure.

The only minor area for improvement, which doesn't detract significantly from the correctness or completeness, is the formatting of the `print` statements. The ground truth code uses f-strings for more precise output formatting (e.g., `:.2f` for rounding the average age) and a slightly more descriptive initial print for gender counts. However, the generated code's output is still perfectly understandable and correct.",grok-3-mini-beta,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}

df = pd.DataFrame(data)

# Print the Title of the movie with the highest Rating
max_rating_index = df['Rating']...",9,"The generated code correctly performs all the specified tasks. It accurately defines the DataFrame, correctly identifies the movie with the highest rating using `idxmax()`, and calculates the average year. The code is concise and efficient. The main area for minor improvement, preventing a perfect score, is the lack of descriptive print statements for the outputs. The ground truth code includes more user-friendly output messages, which is generally good practice for reproducibility and understanding.",grok-3-mini-beta,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

df = pd.DataFrame(data)

# Print the total number of cars
print(""Total number of cars:"", len(...",10,"The generated code is excellent. It correctly defines the DataFrame as specified, precisely calculates and prints the total number of cars using `len(df)` (which is a perfectly valid and common way to get the number of rows), and accurately calculates and prints the average ""PriceUSD"" using `df['PriceUSD'].mean()`. All requirements of the task are met, and the code is clean and efficient. The use of `len(df)` instead of `df.shape[0]` for the number of cars is a minor stylistic difference, both are correct. The formatting of the output string for the average price isn't as polished as the ground truth's f-string with formatting, but the task only asked to ""Print the average 'PriceUSD'"", which it does correctly.",grok-3-mini-beta,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}

df = pd.DataFrame(data)

# Pokemon with the highest HP
highest_hp_pokemon = df.l...",9,"The generated code correctly defines the DataFrame and accurately identifies and prints the names of the Pokemon with the highest HP and lowest Defense. The code is concise and uses appropriate pandas methods (`idxmax()`, `idxmin()`).

The only minor area for improvement, preventing a perfect score, is the print statements. The ground truth provides more descriptive output, e.g., ""The Pokemon with the highest HP is: Venusaur"", which is generally better for user understanding. The generated code simply prints the name, which might be sufficient depending on the context, but less explicit. However, this is a minor stylistic point and does not affect the correctness of the solution.",grok-3-mini-beta,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('titanic.csv')

# Identify columns with missing values
missing_values = df.isnull().sum()

# Fill missing Age values with the median age
median_age = df['Age'].median()
df['Age'].fillna(median_age, inplace=True)

# Fill missing Embarked values with the most frequent port
most_frequent_embarked = df['Embarked'].mode()[0]
df['Embarked'].fillna(most_frequent_embarked, inplace=True)

# Drop the Cabin column
df.drop('Cabin', axis=1, i...",8,"The generated code successfully performs all the required data cleaning steps. It correctly identifies and handles missing values for 'Age' and 'Embarked', drops the 'Cabin' column, and extracts titles from the 'Name' column. The use of `df.isnull().sum()` for identifying missing values is good, and `df.drop()` is correctly used. The `str.extract()` method for extracting titles is a concise and effective way to achieve this, which is a strong point.

Areas for improvement:
1.  **Robust Title Extraction**: While `str.extract(' ([A-Za-z]+)\.', expand=False)` works for most cases, it doesn't handle all possible title variations as robustly as a regex function combined with `replace()` for standardizing less common titles (`Mlle`, `Ms`, `Mme`, `Rare`, etc.). The ground truth's approach using a function and then standardizing is more comprehensive for real-world scenarios, even if not strictly required by the prompt's wording (""Mr., Mrs., Miss., Master., etc."").
2.  **Explicit Imports**: Although `re` is implicitly used by `str.extract()`, it's generally good practice to explicitly import modules whose functionalities are being leveraged, even if Pandas provides an abstraction. (This is a minor point here as pandas handles it).
3.  **Error Handling**: The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust. This is a good practice for data loading.
4.  **Clarity/Comments**: Adding comments, especially for the step where missing values are identified (`missing_values = df.isnull().sum()`), would enhance readability. While the prompt implied showing *after* cleaning, it's good to identify *before* as well. The code executes it, just doesn't print it.

Overall, it's a very good solution that directly addresses the prompt's requirements effectively and concisely. The `str.extract` method is a particularly elegant solution for the title extraction.",grok-3-mini-beta,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```
import pandas as pd

# Create the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}
df = pd.DataFrame(data)

...",10,"The generated code is correct, complete, and of good quality. It precisely follows the instructions to define the DataFrame, identify the title of the book with the highest price, and calculate the total stock. The variable names are clear, and the pandas methods used (`idxmax()`, `.loc[]`, `.sum()`) are appropriate and efficient for the given tasks. The output format is also as expected, printing just the required values.",grok-3-mini-beta,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('water_potability.csv')

# 1. Display the first 10 rows
print(df.head(10))

# 2. Show the shape of the dataset
print(df.shape)

# 3. List all column names and their data types
print(""Column names:"", df.columns.tolist())
print(""Data types:"", df.dtypes)

# 4. Calculate the percentage of potable vs non-potable water samples
if 'Potability' in df.columns:
    percentages = df['Potability'].value_counts(normalize=True) * 100
    print...",7,"The generated code correctly addresses most of the tasks.

**Strong Points:**
- Correctly loads the dataset.
- Accurately displays the first 10 rows, shape, missing values, and summary statistics.
- The calculation for potability percentages is correct and concise using `normalize=True`.

**Areas for Improvement:**
- **Clarity and Readability:** The ground truth code significantly improves readability by adding descriptive print statements (e.g., ""--- 1. First 10 rows of the dataset ---"") and separators. The generated code simply prints the output without much context for each section.
- **Robustness:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is crucial for real-world scenarios.
- **Column Names and Data Types Display:** While `df.columns.tolist()` and `df.dtypes` are correct, `df.info()` as used in the ground truth is a more comprehensive and standard way to display column names, their data types, and non-null counts all at once, which is often preferred in EDA.
- **Missing Values Display:** The generated code correctly shows all missing values. However, displaying only columns with missing values (`missing_values[missing_values > 0]`) as done in the ground truth, along with a message if no missing values are found, makes the output cleaner and more informative when there are many columns with no NaNs.
- **Potability Percentage Output:** While the calculation is correct, the output for potability is a bit sparse (`print(""Percentage of non-potable (0):"", percentages.get(0, 0))`). The ground truth's `potability_percentage.round(2)` and additional interpretation make it more user-friendly.",grok-3-mini-beta,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Before statistics
before_missing = df.isnull().sum()
before_shape = df.shape
print(""Before cleaning:"")
print(""Missing values:\n"", before_missing)
print(""Shape:"", before_shape)

# Fill missing values for numerical columns
numerical_cols = df.select_dtypes(include=['number']).columns
for col in numerical_cols:
    if df[col].isnull().any():
        df[col].fillna(df[col].mean(), inplace=True)

# Remove du...",6,"The generated code correctly addresses several requirements but falls short on a few key aspects and lacks the robustness of the ground truth.

**Strengths:**
*   Correctly loads the dataset.
*   Identifies and fills missing values using the mean for numerical columns.
*   Correctly removes duplicate rows.
*   Displays before and after statistics for missing values and shape.
*   Displays the first 5 rows of the cleaned dataset.

**Weaknesses and Areas for Improvement:**

1.  **Missing File Handling:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth gracefully handles.
2.  **Incomplete Range Verification (Crucial):** This is the biggest gap. The task specifically asks to ""Verify that all values are within reasonable ranges for water quality parameters."" The generated code only uses `df.describe()` for ""verification,"" which *shows* statistics but does *not* actively *clean* or *remove* out-of-range values as explicitly required by the task and performed by the ground truth. This is a critical omitted step for ""data cleaning.""
3.  **Less Informative Output:** The ""Before cleaning"" and ""After cleaning"" printouts are less detailed than the ground truth. For example, the ground truth specifically shows *only* columns with missing values before cleaning. The ""After cleaning"" missing values check in the ground truth explicitly confirms ""No missing values remaining.""
4.  **No Before/After Missing Values Per Column:** While it prints `before_missing` and `after_missing`, it doesn't clearly state *which* columns had missing values and that they were filled, as the ground truth does (e.g., ""Filled missing values in 'ph' with its mean."").
5.  **Lack of `numpy` import:** While not strictly needed for this specific implementation, `np.number` is often used for selecting numerical columns, and the ground truth uses it. It's a common and good practice in data science code.
6.  **No detailed summary of actions:** The ground truth provides more verbose output, indicating *what* was done (e.g., ""Removed X duplicate rows"", ""Removed X rows due to out-of-reasonable-range values""). The generated code is silent on these actions.

To improve, the code needs to implement active range validation and removal, similar to the ground truth's `ph` and non-negative checks, and enhance the output to be more informative about the cleaning steps performed.",grok-3-mini-beta,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('water_potability.csv')

# 1. Bar chart for potable vs non-potable distribution
potable_counts = df['Potability'].value_counts()
plt.figure(figsize=(8, 6))
potable_counts.plot(kind='bar')
plt.title('Distribution of Potable vs Non-Potable Water Samples')
plt.xlabel('Potability')
plt.ylabel('Number of Samples')
plt.xticks([0, 1], ['Non-Potable', 'Potable'], rotation=0)
plt.legend(['Co...",7,"The generated code correctly addresses most of the requirements and produces the four requested visualizations.

**Strong points:**
*   Correctly loads the dataset.
*   Successfully generates a bar chart, histograms, a boxplot, and a heatmap.
*   Handles missing values using `.dropna()` for the histograms, which is appropriate.
*   Includes proper titles and axis labels for all plots.
*   Uses `seaborn` and `matplotlib` correctly for most plot types.

**Areas for improvement to achieve a higher score:**

1.  **Histograms as a single figure:** The request specified ""histograms for pH, Hardness, and Chloramines showing their distributions"" (plural), implying they should preferably be grouped together for better comparison and conciseness (e.g., using `plt.subplots`), rather than three separate figures. The ground truth demonstrates this effectively.
2.  **`Potability` bar chart details:**
    *   Using `potable_counts.plot(kind='bar')` is functional but `sns.countplot` (as used in the ground truth) is generally preferred for categorical counts as it often leads to more aesthetically pleasing and informative plots with less manual setup.
    *   The `plt.legend(['Counts'])` is unnecessary and redundant for a simple bar chart of counts.
    *   The `figsize` is set, but `plt.tight_layout()` is missing for this (and other) plots, which can sometimes lead to overlapping labels/titles.
3.  **Consistency in `kde` for histograms:** The generated code uses `kde=False` for all histograms. While not explicitly stated to have KDE, including it often provides a smoother representation of the distribution, which is a common practice for showing distributions.
4.  **Minor plot enhancements:**
    *   Adding `palette` to `countplot` or `boxplot` can enhance visual appeal.
    *   Setting `sns.set_style(""whitegrid"")` at the beginning (as in the ground truth) provides a consistent and clean aesthetic across all plots.
    *   Adding grid lines (`plt.grid(axis='y')`) can improve readability for some plots.
5.  **Error Handling:** The ground truth includes `try-except FileNotFoundError` for robust file loading. While not strictly required by the prompt, it's good practice for real-world scenarios.

Overall, the code is functional and largely correct, but it misses some opportunities for better visualization practices, conciseness, and aesthetic refinements that would make it more robust and professional.",grok-3-mini-beta,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Create new features
df['total_acidity'] = df['fixed acidity'] + df['volatile acidity'] + df['citric acid']
df['alcohol_density_ratio'] = df['alcohol'] / df['density']
df['ph_sulphates_interaction'] = df['pH'] * df['sulphates']

# Prepare features and target
X = df.drop('qua...",7,"The generated code addresses most of the requirements but has some notable shortcomings compared to the ground truth.

**Strong Points:**
- **Feature Engineering:** It successfully creates three new features, demonstrating an understanding of this task requirement.
- **Multiple Feature Selection:** It correctly applies both `SelectKBest` with `f_regression` and `RFE` with `RandomForestRegressor`, fulfilling a core task requirement.
- **Comparison:** It performs a basic comparison of the selected feature sets, identifying common and unique features.
- **Imports:** Necessary imports are present.

**Areas for Improvement (leading to an 8+ score):**

1.  **Meaningful Engineered Features:** While three features are created, their 'meaningfulness' can be debated. `total_acidity` is a simple sum of existing acidity measures, which might not add significant new information or capture a complex relationship. `alcohol_density_ratio` and `ph_sulphates_interaction` are better. The ground truth provides more conceptually sound engineered features like `free_to_total_sulfur_dioxide_ratio` or `alcohol_sulfates_interaction` with clear explanations of their potential meaning in the context of wine.

2.  **Handling Division by Zero/NaN/Inf:** The `alcohol_density_ratio` could potentially lead to division by zero or very large numbers if `density` is close to zero, or `NaN`/`inf` if `density` is exactly zero. The ground truth explicitly handles this by adding `EPSILON` to denominators and replacing `inf` values with `NaN` before filling with the median. This is a crucial robustness check in real-world data science.

3.  **Scaling:** The ground truth includes `StandardScaler`. While not strictly mandatory for `SelectKBest` with `f_regression` or `RFE` with tree-based models, scaling is a critical pre-processing step when working with many other machine learning algorithms and is generally considered good practice to prevent features with larger magnitudes from dominating the process.

4.  **Displaying Importance Scores for RFE:** The generated code attempts to display RFE importance scores but the approach `importances_rfe = rfe.estimator_.feature_importances_` followed by indexing based on the original `X.columns` after RFE has *already filtered* the features will likely lead to incorrect results or unexpected behavior. `rfe.estimator_.feature_importances_` will contain importances for the *final subset of features that RFE kept*, not for the original full set of features. The ground truth correctly retrains the `RandomForestRegressor` on *only the selected features* after RFE has identified them, and then extracts `feature_importances_` from *that* refitted estimator to get accurate importances for the selected features. This is a subtle but important detail for displaying accurate RFE-based importances.

5.  **Output Readability/Clarity:** While the comparison is present, the ground truth provides a more extensive and explanatory comparison section, discussing the differences between wrapper (RFE) and filter (SelectKBest) methods and why their selected features might diverge. This adds significant educational value and fulfills the ""compare and contrast"" requirement more deeply.

6.  **Explicit Number of Features to Select:** The ground truth clearly defines `N_FEATURES_TO_SELECT` as a constant, making the code more configurable and readable.

Overall, the code performs the requested operations, but lacks some of the robustness, best practices (scaling, NaN handling), and detailed output/analysis that an ""expert"" solution would provide.",grok-3-mini-beta,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor, StackingRegressor
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot ...",5,"The generated code attempts to address most requirements but falls short in crucial areas, particularly data preprocessing and the implementation of cross-validation and evaluation.

**Major deficiencies:**

1.  **Data Preprocessing (Critical Flaw):**
    *   **Missing Values:** Uses `df.dropna()` which is a very aggressive and usually detrimental approach for real-world datasets. The ground truth uses `SimpleImputer` more appropriately.
    *   **Categorical Encoding:** Uses `LabelEncoder` for features like 'Platform', 'Genre', 'Publisher', and even 'Name' (which should ideally be dropped). `LabelEncoder` is inappropriate for nominal categorical variables in most regression models as it assigns arbitrary ordinal relationships. `OneHotEncoder` (as used in the ground truth) is the correct approach.
    *   **Irrelevant Columns:** Does not drop 'Name' or 'Rank' which are clearly irrelevant as per the ground truth and good practice for this dataset.
    *   **User_Score:** Does not handle 'User_Score' which might contain non-numeric values (like 'tbd' mentioned in the ground truth context).
    *   **No Preprocessing Pipeline:** The generated code does not use `ColumnTransformer` or `Pipeline` for preprocessing. This means preprocessing is applied *before* the train-test split and cross-validation folds, leading to data leakage. The ground truth correctly integrates preprocessing into a `Pipeline` for each model.

2.  **Cross-Validation and Evaluation (Severe Issue):**
    *   **Incorrect `cross_val_score` Usage:** `cross_val_score` is used on `X_train`, `y_train` only. It should be used on the *entire* dataset (`X`, `y`) when implementing the task's requirement of evaluating models with 5-fold cross-validation, and the `kf` object should be passed directly to `cross_val_score` (though the ground truth shows a manual KFold loop which is also valid and more detailed for capturing fold-wise metrics). More critically, the problem statement implies cross-validation for evaluation, not just to pick models for GridSearchCV.
    *   **Double `split`:** The `X_train, X_test` split and then `cross_val_score` on `X_train` is inefficient and doesn't fully utilize the data for evaluation as intended by comprehensive cross-validation. The ground truth performs the entire cross-validation on the full X and y for robust evaluation.
    *   **Stacking Evaluation:** The stacking model is only evaluated on a single `X_test`, `y_test` split, not with cross-validation like the other models, making its comparison inconsistent. The ground truth performs cross-validation for stacking too.

3.  **Hyperparameter Tuning:**
    *   **Hardcoded Best Model:** The code assumes XGBoost is the best model (`best_model = xgb`) without dynamically determining it from CV results, which contradicts the prompt's implied dynamic selection.
    *   **No Tuning for Stacking/Voting:** The prompt asked for tuning the *best individual model*, which it attempts, but it doesn't consider tuning the ensembles, which is often a goal in advanced ML.

4.  **Feature Importance:**
    *   The feature importance plot for `best_model_tuned` is generated, but `best_model_tuned` is not a pipeline, meaning it expects preprocessed data. If `X` contains categorical features encoded with LabelEncoder, the feature importance will be misleading as it doesn't account for one-hot encoding.
    *   The ground truth shows how to correctly extract feature names after one-hot encoding for proper plotting.

5.  **Residual Plots:**
    *   The residual plot is generated for the stacking model, which is good. However, the ground truth includes a histogram of residuals for a more complete analysis of error distribution.

6.  **Code Quality/Completeness:**
    *   Imports are mostly there, but some crucial ones like `SimpleImputer` or `ColumnTransformer` are missing due to the flawed preprocessing approach.
    *   Lack of clear print statements for progress and results, compared to the detailed output in the ground truth.
    *   The problem specifically asked for an `XGBRegressor` from the `xgboost` library; the generated code does this but doesn't handle `n_jobs` for parallel processing.",grok-3-mini-beta,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import pymc3 as pm
import ruptures as rpt
import lifelines
from statsmodels.regre...",4,"The generated code attempts to address many aspects of the prompt but falls short significantly in correctness, completeness, and robustness for a ""data science benchmark"" expert evaluation.

**Major Issues in Correctness and Completeness:**

1.  **Dataset Loading and Aggregation:**
    *   `annual_sales = df.groupby('Year')['Global_Sales'].sum().reset_index()`: The original dataset does not have a 'Year' column, it's 'Year_of_Release'. This immediately causes a `KeyError` and stops execution.
    *   No handling for missing values or non-numeric entries in 'Year_of_Release' as in the ground truth.
    *   The index for time series (`ts_data`) is not converted to a datetime index, which is crucial for `seasonal_decompose` and `ARIMA` to fully leverage time series functionality (though it might work for basic cases, it's not robust).

2.  **ARIMA Modeling:**
    *   `ts_data = annual_sales.set_index('Year')['Global_Sales']`: Due to the 'Year' KeyError, this fails.
    *   `ARIMA(ts_data, order=(1, 1, 1))`: This uses a fixed order. The prompt implies finding the best model by omitting an explicit order (as `pmdarima` does). `auto_arima` or manual grid search is generally preferred.
    *   No confidence/prediction intervals are *extracted or printed* for ARIMA, only the forecast. The prompt explicitly asks for them.

3.  **Seasonal Decomposition:**
    *   `decomposition = seasonal_decompose(ts_data, model='additive', period=5)`: The `period` argument '5' assumes a 5-year seasonality, which is not necessarily true for yearly data (yearly data *normally* has no seasonality for `m=1/period=1`). While `seasonal_decompose` *can* run with `period > 1`, it's often more about cycles than ""seasonality"" in the traditional sense for yearly data. The ground truth correctly identifies this nuance. This is not a complete failure, but less robust.

4.  **Multivariate Regression with Interaction Terms:**
    *   Again, relies on `df['Year']` which should be `df['Year_of_Release']`.
    *   `PolynomialFeatures(interaction_only=True, include_bias=False)` applied to `X_multivar` which contains `Year`, `Genre`, `Platform`, `Publisher` *after* one-hot encoding. This structure is very problematic: `PolynomialFeatures` is designed for numerical interaction terms (e.g., `x1*x2`). Applying it broadly on one-hot encoded categorical features for `interaction_only=True` will create *a massive number of interaction terms* (e.g., `Genre_Action * Platform_PS2_dummy`) which is rarely intended or manageable. The ground truth carefully selects *specific meaningful* interaction terms (Year with top categories).
    *   No handling for too many categories (like in the ground truth) which will lead to feature explosion and likely memory/computation errors, or perfect multicollinearity.
    *   Does not *print* any results or summary of the multivariate model.
    *   Confidence/prediction intervals for this model are requested, but only `sm.OLS` is used which is separate from the `Pipeline` setup. The `Pipeline` is fitted, but its results are not used for CI/PI directly.

5.  **Polynomial Regression with Regularization:**
    *   `df[['Year']]` error again.
    *   `X_poly = poly_features.fit_transform(df[['Year']])`: This uses *all* data for fitting the polynomial features. In `TimeSeriesSplit`, this should be done within the loop to avoid data leakage.
    *   No confidence/prediction intervals are *extracted or printed* specifically for Ridge/Lasso models, only mentioned to be derived from `OLS` models which is an approximation and not directly from the regularized models themselves in a proper statistical package sense.

6.  **Bayesian Regression with PyMC3:**
    *   Uses `X_poly` as input, which is `Year` alone, missing the other variables (`Genre`, `Platform`, `Publisher`) that the prompt specifies for multivariate modeling. This is a severe simplification and fails to meet the task's requirement of modeling uncertainty in predictions using other variables.
    *   Categorical variables are not handled correctly for PyMC3. The ground truth uses numerical coding for categories and models their effects `beta_genre[X_bayesian[:, 1].astype(int)]`, while the generated code simply uses `beta` with `shape=X_poly.shape[1]`, which is wrong for handling multiple categorical features as separate effects.
    *   No printing of Bayesian summary or intervals (`pm.summary(trace)` is called but not printed). No example prediction intervals are shown.

7.  **Change Point Detection:**
    *   `pen=10`: This is an arbitrary penalty. The ground truth provides a more principled way to select penalty (`2 * log(n)` for BIC-like behavior) for general use.
    *   No plotting or clear output of detected change points (years).

8.  **Survival Analysis:**
    *   This section is largely commented out or uses placeholder logic (`if 'Release_Year' in df.columns...`). The CSV provided in the prompt description (which is the input `video_games_sales.csv`) does not have `Release_Year` or `Last_Profitable_Year`. The ground truth correctly infers how to create a `Longevity_Years` column from the available `Year_of_Release` data.
    *   The `kmf.fit` line will fail because `df['time']` and `df['event']` will not exist if the `if` condition is false (which it is for the given dataset).
    *   No Cox Proportional Hazards Model implementation, which is explicitly requested. This is a critical omission for survival analysis beyond basic Kaplan-Meier.

9.  **Confidence and Prediction Intervals:**
    *   The approach for non-ARIMA models (OLS) to derive CIs/PIs is correct using `statsmodels` for those specific models. However, it applies `OLS` models *after* Ridge/Lasso, potentially misrepresenting the actual regularized models.
    *   Missing PI for ARIMA (only CI shown).
    *   Bayesian intervals are not printed.

10. **Time Series Cross-Validation:**
    *   It correctly sets up `TimeSeriesSplit`.
    *   The implementation inside the loop is incomplete and only provides a placeholder for `ARIMA` but no actual evaluation (e.g., `mean_squared_error`) or reporting of results for all models. It only says ""Example: Fit ARIMA..."". For other models, it merely comments ""Similarly for other models..."". This is insufficient.
    *   It doesn't consider data leakage in cross-validation for `PolynomialFeatures` or `StandardScaler` for regression models (which the ground truth does).

**Code Quality and Structure:**
*   Lack of comments for different sections.
*   No plotting as required for ARIMA forecast, seasonal decomposition, Kaplan-Meier, or change points. Plots are crucial for time series and visualization.
*   No error handling for file loading or model fitting failures.
*   Imports are generally okay, but `statsmodels.tsa.arima.model.ARIMA` is used instead of `pmdarima` for auto-ARIMA, which is a key part of modern ARIMA analysis for non-experts.
*   Lack of variable definition before use (e.g., `X_poly` is used in Bayesian regression as if it contains all polynomial features + interactions, but it only contains `Year`).

Overall, the code provides a high-level outline of required steps but fails to implement them correctly or completely. The reliance on `df['Year']` instead of `df['Year_of_Release']` is a fundamental error that prevents the code from running on the specified dataset. Many sections are either incomplete, conceptually incorrect for the data, or omit crucial details requested in the prompt (e.g., proper interval extraction, full model summaries, detailed cross-validation results).",grok-3-mini-beta,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE
import umap.umap_ as umap
from sklearn.metrics import silhouette_score, silhouette_samples
from yellowbrick.cluster import KElbowVisualizer
from scipy.stats import ttest_ind
from sklearn.ensemble import...",6,"The generated code attempts to address most of the requirements but falls short in several critical areas, impacting its correctness, quality, and completeness.

**Correctness:**
*   **Preprocessing:** The original code performs `df.dropna()` early, which is problematic. The ground truth carefully handles missing values for specific columns and imputes/fills sensible defaults (e.g., 0 for sales, mode for year). Dropping all NaNs without consideration can drastically reduce the dataset and remove valuable information.
*   **Feature Engineering:** The generated `NA_Prop`, `EU_Prop`, etc., are calculated `df['NA_Sales'] / df['Global_Sales']` directly, without handling `Global_Sales` being zero, which leads to `inf` or `NaN` values. The ground truth uses `apply` with a check for `Global_Sales > 0`.
*   **Optimal Cluster Determination:** The optimal `k` is determined only for K-Means using silhouette. The Elbow Method is implemented but not explicitly used to determine `optimal_k`. The ground truth uses both KElbowVisualizer and Silhouette scores for K-Means, and silhouette for GMM and Hierarchical, providing a more robust approach.
*   **Consensus Clustering:** The assigned consensus clustering implementation is a very simplistic majority vote based on numeric labels, which is not a standard or robust method for consensus clustering. It should typically involve building a co-occurrence matrix and then clustering that matrix, as demonstrated in the ground truth.
*   **DBSCAN:** The `eps` and `min_samples` parameters are hardcoded without any attempt at estimation or visualization (`k-distance graph`). This makes DBSCAN unreliable and likely to produce poor results or classify most points as noise.
*   **Cluster Profiling and Statistical Significance:** The `cluster_profile` function only takes a single `feature` at a time and performs a t-test against *all other clusters combined*. This is not a proper statistical test for distinguishing between multiple clusters. The ground truth uses ANOVA for numerical features and Chi-squared for categorical features across *all* clusters, which is appropriate for multi-group comparisons. It also only profiles `Global_Sales` and `Year`, ignoring other features.
*   **Recommendation System:** The recommendation system is very basic, returning only `n` games from the same cluster based on `df.loc[game_index, 'Consensus_Cluster']`. This `game_index` is an arbitrary row index, not a game name or ID, making the function unusable as intended. The ground truth takes a `game_name` and provides actual game details.
*   **Validation:** Only internal silhouette score is calculated for consensus. External validation is not present (""if ground truth existed""). The ground truth provides specific internal validation metrics (Silhouette, Davies-Bouldin, Calinski-Harabasz) and correctly handles DBSCAN noise points for these calculations.

**Code Quality:**
*   **Modularity/Structure:** Key steps are not clearly separated by comments or functions, making the code harder to read and debug. The ground truth has clear sections for each task.
*   **Imports:** `sklearn.compose.ColumnTransformer`, `sklearn.pipeline.Pipeline`, `mlxtend.frequent_patterns.association_rules`, `sklearn.neighbors.NearestNeighbors`, `kneed.KneeLocator` are missing, which are crucial for proper preprocessing, optimal cluster determination, and association rule mining.
*   **Warnings:** No warning suppression.
*   **Robustness:** Lacks error handling (e.g., `FileNotFoundError` for CSV, division by zero in feature engineering).

**Completeness:**
*   **Feature Engineering:** Missing 'Year_of_Release' from `features` list used in `X_scaled`, and also the one-hot encoding for categorical features specified in the task for clustering. The ground truth performs robust feature engineering using `ColumnTransformer` for both numerical scaling and one-hot encoding categorical features.
*   **Optimal Cluster Determination:** Only K-Means is fully implemented for optimal `k` determination. Missing Gap Statistic.
*   **Consensus Clustering:** The implementation is fundamentally incorrect as discussed above.
*   **Cluster Profiling:** Lacks comprehensive profiling for all numerical and categorical features.
*   **Association Rule Mining:** `basket` creation (`pd.get_dummies(df[['Genre', 'Platform']])`) is too simple and doesn't capture the full scope of game attributes available (e.g., Publishers, Year, Sales, Cluster). The ground truth processes categorical labels more effectively.
*   **External Validation:** External validation is completely absent.

In summary, while the generated code touches upon many aspects of the prompt, the implementations are often superficial, incorrect, or incomplete, particularly in crucial areas like preprocessing, advanced clustering techniques, statistical testing, and the recommendation system.",grok-3-mini-beta,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier, RandomForestClassifier
from sklearn.linea...",6,"The generated code attempts to address most requirements but falls short in key areas, particularly regarding the rigor of the implementation for some advanced techniques.

**Strengths:**
*   **Comprehensive Imports:** Includes most necessary libraries.
*   **Basic Structure:** Attempts to separate sections for different tasks.
*   **Feature Engineering:** Includes a couple of new features.
*   **Ensemble Methods:** Implements several required ensemble types (Voting, Bagging, Boosting, Stacking).
*   **Bayesian Optimization:** Uses Optuna for hyperparameter tuning.
*   **SHAP Interpretation:** Includes `shap_interpret` function.
*   **Calibration:** Implements `CalibratedClassifierCV`.

**Weaknesses and Areas for Improvement:**

1.  **Preprocessing Rigor and Feature Engineering:**
    *   **Missing '0's as NaN Handling:** The most critical preprocessing step from the Ground Truth (replacing '0's with NaN in specific columns and then imputing) is completely missed. This is crucial for the diabetes dataset.
    *   **Polynomial Features:** Not implemented, which was a core part of the ""comprehensive feature engineering"" in the ground truth.
    *   **Preprocessing Pipeline for all Models:** The `ColumnTransformer` is used, but it's only correctly integrated into the `objective` function for Optuna and later used in `shap_interpret` and `calibrate_model` by explicitly creating a pipeline. It's not uniformly applied to *all* models during training and evaluation, meaning many models will be trained on unscaled and unimputed data (e.g., in `nested_cv`, `diversity_analysis`, `combine_models`). This is a major correctness issue.

2.  **Nested Cross-Validation Implementation:**
    *   **Incorrect Application:** The `nested_cv` function as implemented `scores = cross_val_score(model, X, y, cv=outer_cv, scoring='accuracy')` is *not* nested cross-validation. It's just standard K-fold cross-validation. True nested CV requires an inner loop (for hyperparameter tuning or model selection) and an outer loop (for unbiased performance evaluation). The ground truth correctly implements an explicit outer loop and trains/evaluates all models within each fold.
    *   **Missing Model Training within CV:** The generated code defines many models but doesn't explicitly train and evaluate them all within the `nested_cv` framework. It only shows `nested_cv(voting_soft)` as an example. The ground truth trains *all* models/ensembles within *each* outer fold and collects results.

3.  **Custom Ensemble with Dynamic Weight Assignment:**
    *   **Incorrect Implementation:** The `custom_ensemble` function attempts dynamic weighting, but it fits a `LogisticRegression` *directly on predictions* (`predictions`, `y_val`). This is a meta-learner applied to validation set predictions, but the ""dynamic weight assignment"" specified usually implies dynamically calculating weights based on performance and applying them either directly or via another model during prediction. The provided `weights = LogisticRegression().fit(predictions, y_val).coef_[0]` is an unconventional way to get weights and assumes linear separability of predictions. It also isn't directly used to score the performance of this ensemble properly.
    *   **Missing OOF Predictions:** For rigorous custom ensembles and diversity analysis, Out-Of-Fold (OOF) predictions are crucial. The generated code does not explicitly collect OOF predictions as the ground truth does.

4.  **Model Calibration and Reliability Analysis:**
    *   **Limited Scope:** `calibrate_model` correctly uses `CalibratedClassifierCV` but then calculates `calibration_curve` on the *entire dataset* (`y`, `full_pipe.predict_proba(X)[:, 1]`). This is data leakage and leads to an optimistically biased calibration curve. Calibration should ideally be performed on a held-out set or using cross-validation within the overall nested CV structure, akin to how `CalibratedClassifierCV` uses `cv`. The ground truth used the test set of the last fold.
    *   **Missing Brier Score:** The output for Brier Score is not present like in the ground truth.

5.  **Ensemble Diversity Analysis:**
    *   **Incorrect Diversity Metric:** The `diversity_analysis` calculates `pearsonr` correlation of *errors*. While error correlation is a valid diversity metric, it's more common and informative to look at the correlation of the *prediction probabilities* themselves, as implemented by the ground truth. The current implementation is also less clear about how it defines diversity (e.g., average pairwise correlation).
    *   **Data Leakage:** Similar to calibration, the diversity analysis calculates predictions `fit(X_processed, y).predict(X_processed)` on the full dataset, which again is data leakage. Diversity should be analyzed on OOF predictions generated within the nested CV.

6.  **SHAP Interpretation:**
    *   **`X.columns` for `summary_plot`:** `explainer.shap_values` is passed the transformed data `pipe.named_steps['preprocessor'].transform(X)` but `shap.summary_plot` is given original `X.columns`, which might lead to a mismatch if preprocessing adds/removes columns or reorders them. The ground truth consistently uses the columns of the preprocessed data.
    *   **Missing Visualizations:** Only `summary_plot` for bar is shown for `shap`, no beeswarm, dependence plots, or force plots.

7.  **Model Combination Strategies (beyond simple voting/stacking):**
    *   **Simplicity:** The `combine_models` function offers ""average"" and ""weighted"" (with hardcoded weights). While these are strategies, the ground truth's meta-learner on *OOF predictions* is a more advanced and robust approach than simply averaging all predictions on the full training data, which again has data leakage.

8.  **Overall Rigor and Output:**
    *   **Lack of Performance Metrics Tracking:** The generated code defines models but lacks a centralized mechanism to store and report the performance metrics (AUC, Accuracy, etc.) across folds for various models, unlike the `results` dictionary in the ground truth. This makes it impossible to compare model performance.
    *   **Missing Configuration/Constants:** No `RANDOM_SEED`, `N_SPLITS_OUTER`, etc., making the code less reproducible.
    *   **Console Output:** No print statements to show progress or final results, which makes it hard to understand what the code is doing or evaluating.

The code provides a template for many tasks but lacks the detailed, rigorous implementation and proper handling of data splits (especially OOF predictions and nested CV) that are crucial for a robust data science solution and for meeting the ""advanced"" aspects of the prompt.",grok-3-mini-beta,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```
import pandas as pd
import numpy as np
import pymc3 as pm
import theano.tensor as tt
import lifelines as ll
from scipy import stats
from sklearn.model_selection import train_test_split
from causalinference import CausalModel  # For simple causal inference

# Load and preprocess the dataset
df = pd.read_csv('diabetes.csv')

# Handle missing values: Impute numerical columns with mean, drop rows with missing categorical if any
numerical_cols = df.select_dtypes(include=['float64', 'int64']).colu...",3,"The generated code attempts to address all requirements but largely fails on correctness, completeness, and code quality for a demanding task like advanced statistical modeling and Bayesian analysis.

**Correctness:**
*   **Missing Value Handling:** The ground truth pre-processes missing values in specific columns where 0 logically implies missing data (e.g., BloodPressure = 0). The generated code uses `fillna(df[numerical_cols].mean())` without this domain-specific consideration, leading to incorrect imputation.
*   **Target Transformation:** The task requires modeling diabetes risk, which is binary (Outcome). The generated code creates a `diabetes` column based on a `target` column (which doesn't exist in `diabetes.csv`) with an arbitrary threshold of 150, incorrectly assuming the target is continuous and needs binarization. The actual target `Outcome` is already binary.
*   **Hierarchical Model:** The generated code arbitrarily assumes grouping by `sex` which is not present in the `diabetes.csv` dataset and requires imputation or simulation as done in the ground truth for `Age_Group`. The `mu` formulation `beta + pm.Normal('beta_features', mu=0, sd=1) * df['bmi'].values` is incorrect for a standard hierarchical model structure. The likelihood is also `pm.Normal` (`observed=df['target']`), which is wrong for a binary outcome.
*   **Mixed-Effects Model:** Similar issues with `sex` assumed as a categorical variable without proper handling or existence in the dataset. The `mu_mixed` calculation also seems structurally flawed for a typical mixed-effects setup for binary outcome modeling.
*   **Bayesian Logistic Regression:** Uses `df['age']` and `df['sex']` directly without standardization or ensuring `sex` exists. Again, `df['sex']` is not in the dataset. The priors are not particularly informative as requested beyond a 0 mean and specified standard deviation.
*   **Survival Analysis:** The `Weibull` likelihood in PyMC3 requires specific format for observed data (`value` and `event` or `lower`, `upper`, `event`), and `beta` parameter should be `exp(log_mu)` to ensure positivity. The `observed={'value': df['time_to_event'], 'event': df['event']}` correctly specifies the data structure but the `log_mu` depends on `lambda_` which isn't typically how AFT models are parameterized. It also uses `df['target']` for simulating time, which doesn't exist.
*   **Gaussian Processes:** Assumes `df['target']` for `y`, which is not available. The `X` selection also seems ad-hoc.
*   **Causal Inference:** The CausalModel from `causalinference` library is imported, but its usage is extremely simplified and likely incorrect for the specific problem. It initializes `cm` without an instrument or specifying treatment/outcome/confounders on existing dataframe columns (`treatment='bmi'`, `outcome='diabetes'`, `confounders=['age']`, `instrument='age'`). The `est_iv()` call without a properly constructed `CausalModel` is very likely to fail or produce meaningless results. The PSM is merely described, not robustly implemented.
*   **Model Comparison:** The `pm.compare` function expects PyMC3 traces, which is correct, but since many models above are fundamentally flawed or based on non-existent columns, the comparison itself would be invalid.

**Code Quality:**
*   **Imports:** Includes `theano.tensor` which isn't explicitly necessary when using PyMC3's high-level API. Imports `causalinference` but then the usage is highly simplified.
*   **Structure:** The code is structured section-wise as requested but lacks the robustness and error handling (e.g., for missing files, non-existent columns) present in the ground truth.
*   **Readability/Comments:** Lacks detailed comments explaining design choices or assumptions, especially crucial given the complexities of 10 distinct modeling tasks.
*   **Non-existent Columns:** The repeated reliance on `df['target']` and `df['sex']` and `df['target']` for simulating `time_to_event` and `event` for survival analysis, when these columns do not exist in the `diabetes.csv` dataset, makes the code fundamentally unrunnable or yields `KeyError`.

**Completeness:**
*   **Missing Value Handling:** Incomplete as it ignores the domain-specific 0-to-NaN conversion.
*   **Comprehensive Preprocessing:** Lacks feature scaling which is crucial for MCMC convergence and GP performance. Lacks creation of a `group` variable (e.g., `Age_Group`) which is necessary for a *hierarchical* model beyond a simplistic `sex` assumption.
*   **Mixed-Effects:** Not fully implemented as a proper mixed-effects model for a binary outcome using PyMC3 for classification.
*   **Survival Analysis:** While a PyMC3 Weibull model is attempted, no standard survival analysis (like CoxPH) is performed using libraries such as `lifelines`. The task asked for ""survival analysis techniques"", implying both frequentist and Bayesian methods. Ground truth provides robust `lifelines` solution with simulated data.
*   **Causal Inference:** The implementation is minimal and likely incorrect/non-functional. It doesn't perform a proper propensity score matching or a robust Instrumental Variables approach. The ground truth provides detailed, multi-step implementations.
*   **Model Validation:** While `pm.plot_posterior` and `pm.sample_posterior_predictive` are called, the plotting and interpretation of PPCs are minimal, unlike the ground truth which provides detailed plotting and explanation. Credible intervals are only plotted for one variable.

Overall, the generated code demonstrates a superficial understanding of the task requirements and the complexities involved in implementing advanced statistical and Bayesian models. Many critical components are either missing, incorrect, or based on non-existent data columns, making the code largely non-functional or producing invalid results if run with the specified dataset.",grok-3-mini-beta,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering, OPTICS
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE
import umap.umap_ as umap
from ten...",6,"The generated code attempts to address all requirements of the task, which is commendable given its complexity. However, it falls short in several areas regarding correctness, completeness, and adherence to specific best practices.

**Strengths:**
- **Attempts to cover all requirements:** The code structure follows the task breakdown, including sections for all requested functionalities (preprocessing, multiple clustering, dim reduction, consensus, phenotyping, anomaly detection, time-series placeholder, ARM, risk stratification).
- **Includes necessary libraries:** Most essential libraries are imported.
- **Dimensionality Reduction:** Correctly implements t-SNE, UMAP, and a basic Autoencoder.
- **Clustering Algorithms:** Includes K-Means, DBSCAN, Hierarchical, GMM, Spectral, and OPTICS.
- **Anomaly Detection:** Implements Isolation Forest.
- **Association Rule Mining:** Uses `mlxtend` for ARM.

**Weaknesses and Areas for Improvement:**

1.  **Correctness of Feature Engineering (Metabolic Syndrome):**
    *   The `engineer_features` function uses `df.fillna(df.median(), inplace=True)`. This is a general fill, but the original dataset documentation specifies that `0` in certain columns (Glucose, BP, SkinThickness, Insulin, BMI) are likely missing values, not true zeros. The ground truth correctly handles this by replacing 0s with NaN first, then imputing.
    *   The thresholds for `HighBP` (130) and `HighTriglycerides`/`LowHDL` (proxies using BMI) are simplified and deviate significantly from standard clinical definitions (which typically involve specific triglyceride and HDL cholesterol levels, which are not present in this dataset). The ground truth makes reasonable approximations for the available data.
    *   `MetabolicSyndrome` is defined as `sum >= 3`. Clinically, it's usually 3 out of 5 specified criteria. The generated code uses 4 (Glucose, BP, BMI as triglycerides, BMI as HDL), so `sum >= 3` is somewhat reasonable within its own simplified definition, but the proxies are weak.

2.  **Optimal Parameter Determination:**
    *   **Elbow and Silhouette:** The plotting for these methods is missing, which was explicitly requested in the task as ""statistical validation."" The generated code only calculates the optimal K without visualizing the criteria.
    *   **Gap Statistic:** The implementation of the gap statistic is a placeholder (`np.mean(np.log(pdist(X).sum() / k))`) and is not a correct calculation of the Gap Statistic. It also uses KMedoids for this, which is fine, but the formula is off. The ground truth uses standard optimal parameter methods (silhouette, elbow, BIC/AIC).
    *   **DBSCAN/OPTICS Parameter tuning:** No attempt is made to determine optimal `eps` and `min_samples` for DBSCAN/OPTICS, which are critical for these algorithms. The ground truth provides a visualization method (k-distance graph).

3.  **Consensus Clustering:**
    *   The consensus clustering implementation is flawed. It uses `GaussianRandomProjection` and then `KMeans` on the *projected data* for *each* clustering method, then attempts a ""simple majority vote"" (which is `np.array(consensus_labels).mode(axis=0)`). This is not how standard consensus clustering methods work. Consensus clustering typically builds a co-occurrence matrix (how many times observations cluster together across different algorithms) and then clusters *that matrix*. The ground truth correctly implements a co-occurrence matrix approach.

4.  **Patient Phenotype Profiles:**
    *   The `phenotype_profiles` function describes but doesn't *print* the descriptive statistics for each cluster, making it hard to interpret.
    *   It only performs `ttest_ind` for *two example* features (`Glucose`, `BMI`) against the *overall mean*, not a comparison between *all clusters* for statistical significance difference, which is usually the goal of multi-cluster phenotyping. The ground truth uses Kruskal-Wallis/Chi-squared across all clusters for all relevant features.
    *   Visualization of cluster profiles (e.g., boxplots per feature per cluster) is missing.

5.  **Anomaly Detection:**
    *   Only Isolation Forest is used, neglecting One-Class SVM and LOF mentioned in the requirements.
    *   Visualization of anomalies on the dimensionality-reduced plots is missing.

6.  **Time-Series Clustering:**
    *   While the data is cross-sectional, the task explicitly asked to ""Apply time-series clustering if temporal patterns exist in the data."" The generated code includes `TimeSeriesKMeans` but then `if 'Time' in original_df.columns:` which will always be false. The ground truth provides a conceptual outline for handling its absence, which is better than a non-functional conditional check.

7.  **Association Rule Mining:**
    *   The feature discretization is completely missing. `apriori` requires binary (or one-hot encoded) categorical data. The generated code directly passes `df[['HighGlucose', 'HighBP', 'HighTriglycerides', 'Outcome']]` which are binaries, but it implies it should be applied to *all* features, which need discretization (e.g., age, glucose, BMI). The ground truth correctly discretizes continuous features into bins.

8.  **Patient Risk Stratification System:**
    *   The `risk_stratification` logic (`risk_score = cluster_df['Glucose'].mean() + cluster_df['BMI'].mean()`) is overly simplistic and lacks clinical interpretation. It labels clusters as 'High Risk' or 'Low Risk' based on a simple comparison to the overall mean, not based on the rich phenotypic profiles derived. The ground truth provides a more nuanced approach using the cluster characteristics and offers an example of clinical interpretation.

9.  **Visualizations:** Many crucial visualizations are missing (optimal K plots, DBSCAN k-distance, cluster profiles, anomaly visualization).

10. **Code Quality:**
    *   Several imports are present but not used (`PCA`, `OneHotEncoder`, `ColumnTransformer`, `Pipeline`, `KMedoids`, `sch`, `pdist`, `GaussianRandomProjection`).
    *   `tensorflow.keras` is used without `import tensorflow as tf` which is bad practice.
    *   Hardcoded `eps` and `min_samples` for DBSCAN/OPTICS.
    *   `autoencoder.fit(X, X, ...)` attempts to reconstruct the input `X`, which is reasonable, but the `encoder` model to actually retrieve the lower-dimensional embedding is not explicitly extracted and used for `X_autoencoder`. It just uses `autoencoder.predict(X)`, which returns the *reconstructed* output, not the latent embedding. The ground truth correctly defines and uses an `encoder_model`.

In summary, while the code attempts broadly to address each requirement, the implementation details often miss the mark regarding standard practices, statistical rigor, and completeness, leading to a much less robust and insightful analysis compared to the ground truth.",grok-3-mini-beta,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, NestedCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import VotingRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.tree import DecisionTreeRegressor
from sk...",6,"The generated code attempts to address most of the requirements but falls short in several areas, particularly regarding the implementation of advanced techniques and the overall robustness.

**Strengths:**
*   **Data Preprocessing and Feature Engineering:** Includes basic preprocessing steps (imputation, scaling, one-hot encoding) and some feature engineering.
*   **Multiple Ensemble Methods:** Implements VotingRegressor, BaggingRegressor, and lists boosting algorithms.
*   **Bayesian Optimization:** Uses Optuna for hyperparameter tuning, which is a good choice.
*   **SHAP and LIME attempted:** Includes basic SHAP and LIME code snippets.
*   **Diversity Analysis:** Attempts to calculate correlation for diversity.

**Weaknesses and Areas for Improvement:**

1.  **Correctness & Completeness of Ensemble Methods:**
    *   **Voting Regressor (Hard/Soft):** The comment wrongly states ""VotingRegressor is for averaging, which is soft"". `VotingRegressor` performs weighted averaging by default (soft voting) and simple averaging if weights are `None` (akin to hard voting for regression). The implementation doesn't explicitly show both with meaningful distinction.
    *   **Bagging:** While `BaggingRegressor` is used, it's shown with `DecisionTreeRegressor` and `AdaBoostRegressor` as `base_estimator`. Bagging `AdaBoostRegressor` as a base estimator is unusual and generally not recommended as AdaBoost is already an ensemble method.
    *   **Boosting:** Models are initialized but not extensively used or integrated into a coherent evaluation framework beyond individual tuning.
    *   **Custom Ensemble:** The `custom_ensemble` function doesn't actually **train** and predict with an ensemble; it merely calculates weights based on cross-validation scores and then defines `weighted_predict` which tries to multiply model predictions with weights *if models were trained on X*. There's no clear mechanism for how this ensemble would be trained and used for final predictions. The dynamic weight assignment is very simplistic (based on CV scores) and not properly implemented to combine predictions from *trained* models. The `optimized_ensemble` is a placeholder.

2.  **Nested Cross-Validation:**
    *   The generated code defines `NestedCV` but this class is not part of `sklearn.model_selection`. It's a conceptual term or would require a custom implementation or a third-party library like `imblearn`. This module import error alone makes the nested CV section non-functional.
    *   For a true nested CV, hyperparameter tuning (Optuna) should happen *within* each outer fold, not globally upfront. The current approach tunes models on the entire `X_train` and then evaluates these already-tuned models in a ""nested"" way, which is not the correct robust evaluation of a *tuning process*.

3.  **Model Calibration and Uncertainty Quantification:**
    *   `CalibratedClassifierCV` is explicitly for classification, as indicated in its name and purpose. Using it for regression is incorrect and will likely raise errors or produce nonsensical results. The ground truth correctly notes this and suggests adaptation, but the generated code misses this nuance entirely.
    *   Uncertainty quantification via bootstrapping is mentioned but not fully integrated into a model pipeline or evaluation, and there's no visualization or clear metric for it.

4.  **Advanced Techniques (Blending & Multi-level Stacking):**
    *   **Blending:** Implementation is basic. It trains base models on `X_train_split` and predict on `X_val`, then trains a meta-learner. This is a correct *concept*, but it doesn't clearly integrate how this blended model would then make predictions on unseen `X_test` data (it would require retraining base models on full `X_train` and then getting meta-predictions).
    *   **Multi-level Stacking:** The `multi_stacking` model is defined as `StackingRegressor` whose *only* base estimator is `stacking1` (which is another `StackingRegressor`). This isn't how multi-level stacking typically works (where level 1 models feed predictions to level 2). It's more of a nested `StackingRegressor` rather than distinct levels feeding each other. Needs clearer implementation of OOF predictions passing between levels.

5.  **Code Quality:**
    *   `df.fillna(df.median())` is applied directly to the DataFrame `df` before numerical/categorical features are identified. This imputes categorical columns with ""median"" which is illogical and will likely error or cause issues. The preprocessing pipeline later has `SimpleImputer`, making this initial `fillna` potentially redundant or problematic.
    *   The `preprocess_data` function returns `X, y, preprocessor`, which implies the preprocessor isn't part of a full pipeline. This means every time a model is used (`fit(preprocessor.fit_transform(X_train))`), the preprocessor is fitted again, which isn't ideal in a proper pipeline setup. The ground truth uses a `Pipeline` for the entire process, which is more robust.
    *   `preprocessor.get_feature_names_out()` is called for LIME, but `preprocessor` might not have been fitted on `X` or `X_train` in a way that allows `get_feature_names_out()` to work correctly for all transformers (especially with `OneHotEncoder` and `StandardScaler` combined).
    *   Some parts feel like placeholders (e.g., `optimized_weights`).

6.  **Missing Visualizations/Metrics:** While `shap` and `lime` are included, the code doesn't include the necessary `matplotlib.pyplot` imports or commands to actually *display* the plots (e.g., `plt.show()`). The ground truth includes these.

7.  **Dependence on `NestedCV` Class:** The non-existent `NestedCV` class is a critical problem for the execution of the nested cross-validation section.",grok-3-mini-beta,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```
import pandas as pd
import numpy as np
import pymc as pm
import arviz as az
from sklearn.impute import KNNImputer
import semopy
from linearmodels.iv import IV2SLS
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors
import statsmodels.api as sm
from mediation import Mediation  # Assuming mediation package

# Load and preprocess dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Handle missing values
numeric_cols = df.select_dtypes(inc...",4,"The generated code attempts to address all requirements but falls significantly short in correctness, completeness, and robustness for a data science benchmark expert.

**Major Issues (leading to low score):**

1.  **PyMC3 vs. PyMC4:** The prompt explicitly asks for PyMC3 or PyMC4. The generated code uses `pymc` which typically resolves to PyMC4/v5. The ground truth uses PyMC3 for consistency and common library stability. While using `pymc` is not inherently wrong if it's the newer version, the generated code's PyMC usage is often incorrect for the specific operations intended (e.g., `pm.Normal('mu', 0, 1)` uses `sigma` not `sd` for standard deviation, `az.compare` syntax for sensitivity analysis is incorrect). This indicates a lack of familiarity or errors in implementing probabilistic models in the chosen library.
2.  **Missing Value Handling:** The generated code uses `KNNImputer` but lacks comprehensive preprocessing (like scaling of numerical features, proper one-hot encoding of categorical features with `ColumnTransformer`). The ground truth implements a robust pipeline for this. The generated code reassigns `numeric_cols = df.select_dtypes(include=[np.number]).columns` *after* imputer, but `imputer.fit_transform` returns a numpy array, losing column names and types. This will break subsequent operations expecting dataframe columns.
3.  **Hierarchical Model Implementation:**
    *   `regions = df['Regional indicator'].astype('category').cat.codes`: This line generates integer codes, but these codes are not robustly handled for non-existent categories in training vs. test or correctly mapped back if needed. The ground truth uses `unique()` and a map for explicit ID handling.
    *   The `mu` in `pm.Normal('mu', 0, 1)` uses `sigma` as the standard deviation parameter, which is incorrect syntax for PyMC3/PyMC4 where it should be `sd`. This is a critical error in defining priors.
    *   The `region_effect` is defined as `pm.Normal('region_effect', mu=0, sigma=sigma_region, shape=len(df['Regional indicator'].unique()))`, but `mu` is a global mean. The regional effects should ideally be added to the *intercept* or a varying intercept. The ground truth uses `a = pm.Normal('a', mu=mu_a, sd=sigma_a, shape=n_regions)` where `mu_a` is the mean of the random effects.
    *   `theta = pm.Deterministic('theta', mu + region_effect[regions])` implies `mu` is the overall mean and `region_effect` are deviations, which is a plausible setup, but the `sigma` parameter issue remains.
4.  **Structural Equation Models (SEM):** The generated code uses `semopy`, which is a valid library, but the prompt asks to build SEM *using PyMC3/PyMC4* explicitly, implying a Bayesian SEM approach. The ground truth simulates this requirement by writing multiple Bayesian regressions that represent the causal pathways, which aligns with the spirit of the task if a direct Bayesian SEM library wasn't available or easy to integrate.
5.  **Causal Inference (IV and PSM):**
    *   **Instrumental Variables:** The prompt requires a *simulation* of a time component or plausible instrument due to the cross-sectional data. The generated code simply asserts reliance on `'some_instrument'` without any simulation or justification. This makes the IV section non-functional for a real dataset without this column. The ground truth properly simulates an instrument.
    *   **PSM:** `df['treatment'] = (df['Logged GDP per capita'] > df['Logged GDP per capita'].median).astype(int)` will fail because `median` is a method, not an attribute. It should be `df['Logged GDP per capita'].median()`.
    *   `matched_df = df.iloc[indices.flatten()]`: This is an oversimplified and incorrect matching approach. It selects only the matched *control* group's indices, but it should be combining treated units with their matched controls. It does not select the treated units themselves, and the matching distance is not used for quality control. It also does not handle cases where a treated unit has no good match. The ground truth correctly identifies treated and control groups and then matches.
6.  **Mediation Analysis:** The generated code imports `mediation.py` but then uses `med.estimate_Sobel()`. There is no standard `mediation` package in Python with an `estimate_Sobel` method for general use that matches the context of Bayesian or more complex mediation. This looks like a placeholder or an assumption about a specific, potentially custom, library. The ground truth performs the mediation analysis by combining posteriors from fitted Bayesian models, directly aligning with the Bayesian modeling theme.
7.  **Difference-in-Differences (DiD):**
    *   The simulation of the time component is very basic (`np.random.choice([0, 1], size=len(df))`) and does not realistically create pre/post periods or define treated/control groups for policy intervention. The ground truth duplicates data to create realistic pre/post states and defines a treated group based on a sensible criterion (e.g., GDP threshold).
    *   `did_model = sm.OLS(df['Ladder score'], sm.add_constant(df[['treated', 'time', 'treated*time']])).fit()`: This formula is incorrect. Interaction terms in `statsmodels` OLS are done dynamically with `*` in the formula string `f""{outcome} ~ treated * time""`. It expects `treated*time` to be a pre-calculated column, or for the formula to be passed as a string.
8.  **Bayesian Model Comparison:** `az.bf(trace_hier, trace_reg)` is an attempt, but Bayes factors are computationally intensive and not directly calculated with `az.bf` in general cases; it's typically for specific, nested models or estimated via bridging sampling. WAIC and LOO-CV are correctly called, but the comparison itself is limited.
9.  **Completeness & Robustness:** Many plots, credible interval summaries, and especially the detailed sensitivity analysis (varying priors and comparing posteriors) outlined in the requirements are missing. The generated code mostly just calls the sampling or fitting function without showing proper analysis of the results (e.g., summary tables, plots, interpretations). The ground truth provides examples for these.

In summary, while the generated code imports relevant libraries and touches upon each required section, the implementations are frequently incorrect, incomplete, or rely on assumptions that contradict the problem statement (e.g., using `semopy` instead of Bayesian SEM, non-simulated IV). The PyMC implementation specifically shows errors in parameter naming (`sigma` vs `sd`) and setup of random effects.",grok-3-mini-beta,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE, Isomap
import umap.umap_ as umap
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from hdbscan import...",4,"The generated code attempts to address many aspects of the prompt but falls short in several critical areas, making it functionally incomplete and conceptually flawed for an ""advanced"" analysis.

**Primary Reasons for Failure & Critical Missing/Incorrect Aspects:**

1.  **Incomplete/Incorrect Feature Engineering:**
    *   The ""Happiness_Profile"" feature is `df['Ladder score'] + df['Logged GDP per capita'] + df['Social support']`. This is a simplistic sum without any clear justification or advanced derivation. The prompt explicitly requested ""advanced feature engineering including happiness profiles and regional characteristics."" The ground truth shows more thoughtful interaction terms and ratios.
    *   The `features` selection is `[col for col in df.columns if col not in ['Country name']]`. This is problematic because it includes the happiness score itself, which is often the target variable, and doesn't handle the regional indicators properly after one-hot encoding if `Regional indicator` is removed before `get_dummies`. The ground truth carefully selects numerical features and then scales them.
    *   Missing handling of `Regional indicator` as a categorical feature in the `features` list. Yes, it's one-hot encoded, but the subsequent `X = df[features].dropna()` loses context of which columns are numerical vs. categorical.

2.  **Suboptimal Dimensionality Reduction (Autoencoder):**
    *   The autoencoder is trained to reconstruct the input (`autoencoder.fit(X_scaled, X_scaled)`), but the `encoded` layer used for `X_autoencoder` has an `relu` activation (`encoded = Dense(2, activation='relu')(encoded)`). For output layers or bottleneck layers that need to represent data in a range, a linear activation (or no activation) is usually more appropriate for reconstruction, especially if the data is scaled to arbitrary ranges. Sigmoid is often used if scaling is strictly 0-1. The ground truth scales to 0-1 explicitly before feeding to an autoencoder with sigmoid output.
    *   The autoencoder training is `verbose=0`, which is fine, but there's no early stopping or validation split, which makes training less robust to overfitting.

3.  **Flawed Optimal Parameter Determination:**
    *   The `elbow_method` and `silhouette_analysis` functions are provided but are *not called* or used to determine `optimal_k`. Instead, `optimal_k` is hardcoded to `4` (a placeholder). The prompt explicitly asked to *determine* optimal parameters. The plotting functions are there, but the determination logic is missing.
    *   It only considers K-Means for optimal K, ignoring other algorithms like GMM, Agglomerative, and Spectral, which also require `n_components` or `n_clusters`.

4.  **Clustering Algorithm Issues:**
    *   All clustering algorithms (except HDBSCAN) are applied on `X_reduced` (`X_umap` in this case), without explicit consideration of whether `optimal_k` (which was likely derived from K-Means on *original* `X_scaled` based on the optimal K functions) is appropriate for all these algorithms or for the reduced space.
    *   DBSCAN parameters (`eps=0.5, min_samples=5`) are hardcoded without any attempt at optimization, which is crucial for DBSCAN's performance. The ground truth shows an attempt to iterate and find suitable parameters.
    *   HDBSCAN is imported, but the call `hdbscan_model.fit_predict(X_reduced)` is problematic as HDBSCAN's `fit_predict` is designed to be used in conjunction with `hdbscan.HDBSCAN(min_cluster_size=5, prediction_data=True)`. The ground truth uses `prediction_data=True` and shows parameter iteration.

5.  **Critically Flawed Consensus Clustering:**
    *   The generated consensus clustering `final_labels = KMeans(n_clusters=optimal_k).fit_predict(consensus_matrix)` is incorrect. A co-association matrix shows *similarity* (or co-occurrence counts), not features on which to run K-Means directly. K-Means expects feature vectors, not a similarity matrix. The concept is usually to transform the co-association matrix into a distance matrix and then apply a clustering algorithm like Hierarchical Clustering to *that distance matrix*, as done in the ground truth.

6.  **Incomplete Country Profiles:**
    *   The `create_country_profiles` function performs t-tests against the *entire dataset's mean and std*, not against other clusters. True statistical significance testing should be comparing clusters *against each other* (e.g., ANOVA across all clusters for a feature, or pairwise t-tests). The ground truth correctly uses ANOVA.
    *   Effect size calculation is simplistic (difference of mean / overall std), whereas a proper eta-squared or Cohen's d would be more appropriate for cluster comparisons.

7.  **Network Analysis:**
    *   `sim_matrix = np.corrcoef(X_scaled)` will yield NaNs if any row has zero variance, which could happen after scaling especially if some features are constant. The ground truth uses cosine similarity, which is more robust for feature vector comparison.
    *   No thresholding or sophisticated edge creation, leading to a fully connected graph if all similarities are calculated. A fully connected graph is not very informative. The ground truth correctly thresholds edges.

8.  **Association Rule Mining:**
    *   `discretized_df = pd.DataFrame(X_scaled).apply(lambda x: pd.qcut(x, q=2, labels=['low', 'high'], duplicates='drop'))`. This discretizes the *scaled* data. For interpretability, it's generally better to discretize the *original*, unscaled data, as the ranges of 'low' and 'high' would then correspond to meaningful real-world values. This makes the output less insightful for policy.

9.  **Policy Insights:**
    *   The `policy_insights` function is extremely rudimentary, with a simple hardcoded `if` condition using `Logged GDP per capita` and `Social support` (even though the scaled dataset was used). It doesn't truly synthesize insights from the cluster profiles or statistical analysis, as requested. The ""geopolitical validation"" is merely mentioned as a comment, not implemented.

**Overall:** The generated code touches on various aspects but often with superficial or incorrect implementations. The lack of robust parameter tuning, the erroneous consensus clustering approach, and the simplistic statistical analysis significantly reduce its value for ""advanced"" analysis and policy insights. It feels like a collection of attempted implementations rather than a coherent, robust solution.",grok-3-mini-beta,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from statsmodels.tsa.statespace.sarimax import SARIMAX
from pmdarima import auto_arima
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Load data
data = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Filter for Germany and set date as index
df_germany = data[data['location'] == 'Germany'].copy()
df_germany.set_index('date', inplace=True)
df_germany.sort_index(inplace=True)

# Handle missing valu...",7,"The generated code correctly implements the core requirements of SARIMA forecasting with exogenous variables. It loads data, filters, handles missing values (though somewhat simplistically), splits data, uses `auto_arima` for order selection, trains and forecasts, and calculates RMSE. It also includes a plot.

However, there are several areas for improvement to meet the full quality and robustness of the ground truth:

**Areas for Improvement:**
1.  **Missing Value Handling:** The generated code uses `ffill()` for each column individually. While effective for simple cases, the ground truth's approach of `ffill()` then `bfill()` (and then `interpolate` as a fallback) is more robust, especially for leading `NaN`s in a time series. The ground truth also includes checks for remaining `NaN`s after imputation, which is good practice.
2.  **Error Handling:** The generated code lacks robust error handling, such as checking for `FileNotFoundError` when loading the CSV or ensuring there's enough data for the test set. The ground truth demonstrates better defensive programming.
3.  **Clarity and Robustness of Data Splitting:** The ground truth explicitly separates `y_train`, `y_test`, `X_train`, `X_test`, which is clearer and less error-prone when passing data to `SARIMAX`. The generated code relies on accessing columns from `train` and `test` dataframes directly, which is okay but slightly less explicit when defining `y` and `X`.
4.  **Forecast Indexing:** The ground truth includes a specific step to ensure `forecast_values.index = y_test.index` to align indices for plotting and comparison, which is crucial. While the generated code's `predict` call might implicitly handle this correctly given `start` and `end` are from `test.index`, making it explicit is safer.
5.  **Handling Negative Forecasts:** The ground truth includes a practical step to set negative forecasts to zero, which is common in 'new cases' scenarios as negative cases are illogical. This is a good domain-specific refinement.
6.  **Plotting:** The ground truth's plot is more comprehensive, including the training data in the plot and a more descriptive title, RMSE in the title, and better labeling/grid, making it more informative.
7.  **Warnings:** The ground truth includes `warnings.filterwarnings(""ignore"")` and `suppress_warnings=True` in `auto_arima` and `disp=False` in `model.fit()`. While not strictly required for correctness, it contributes to cleaner output for the user.

Overall, the generated code is functional and produces a SARIMA forecast. Its main weaknesses lie in robustness, comprehensive error/edge case handling, and presentation, which are strong points of the ground truth.",grok-3-mini-beta,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import scikit_posthocs as skph  # For Dunn's test

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Define quality groups
df['quality_group'] = pd.cut(df['quality'], bins=[0, 4, 6, 10], labels=['low', 'medium', 'high'])

# Get list of features
features = [col for col in df.columns if col not in ['quality', 'quality_...",6,"The generated code correctly addresses several aspects of the task, but has significant limitations and errors, preventing it from achieving a higher score.

**Strong points:**
*   Correctly loads data and defines quality groups.
*   Correctly identifies chemical properties for analysis.
*   Attempts to use Shapiro-Wilk for normality checks and applies ANOVA/Kruskal-Wallis accordingly.
*   Performs post-hoc tests (Tukey's HSD and Dunn's test using `scikit_posthocs`).
*   Generates box plots for significant features.
*   Summarizes significant features.

**Areas for Improvement and Errors:**

1.  **Incomplete Normality Check:** The normality check `if all(normality)` is problematic. It only checks if *all* groups are normal. If even one group fails Shapiro-Wilk, it immediately defaults to Kruskal-Wallis. A more robust approach, as demonstrated in the ground truth, would also incorporate Levene's test for homogeneity of variance when all groups are normal. Without checking variance equality for ANOVA, its assumptions are not fully met.

2.  **Handling Empty Groups in Normality Check:** The line `if len(group) > 0: ... else: normality.append(False)` is a minimal attempt to handle empty groups. However, `stats.shapiro` requires at least 3 data points. For groups with 1 or 2 data points, `shapiro` will raise an error. The code simply appends `False` (non-normal), but this doesn't fully protect against `shapiro` errors.

3.  **Missing Error Handling for File Not Found:** No `try-except` block for `pd.read_csv`, which is crucial for robust code.

4.  **No Configuration for Alpha:** The significance level (`ALPHA`) is hardcoded as `0.05` multiple times rather than being defined once as a constant.

5.  **Lack of Verbose Output/Debugging Information:** The ground truth provides much more detailed print statements about which test is being run, p-values, and the rationale, which is very helpful for understanding the analysis flow. The generated code's print statements are minimal.

6.  **Post-hoc for Kruskal-Wallis (Dunn's test):** While `scikit_posthocs` is a valid library for Dunn's test, it introduces an external dependency not explicitly mentioned in the prompt (though implied by ""Dunn's test""). The ground truth demonstrates how to achieve a similar result using `scipy.stats.mannwhitneyu` and `statsmodels.stats.multitest.multipletests` for Bonferroni correction, which is a good alternative particularly if `scikit_posthocs` isn't preferred or available. The output format of `scikit_posthocs.posthoc_dunn` is a matrix, which isn't as immediately readable as the `pairwise_tukeyhsd` output or custom printed statements for summary.

7.  **No Summary of Post-hoc Results:** The summary section only lists the significant features. It doesn't print the post-hoc results again, which is very useful for summarizing the exact comparisons that were significant. The ground truth explicitly states which comparisons were significant after post-hoc analysis.

8.  **Minor Code Style/Readability:**
    *   The `features` list generation is slightly less robust than the ground truth's `chemical_properties` that explicitly excludes `quality` and the newly created `quality_tier`.
    *   `df.cut` for `quality_group` is acceptable, but `apply` with a function (like in the ground truth) is perhaps more explicit and flexible for complex categorization.
    *   Box plot titles could be more descriptive.

To improve, the code should:
*   Implement a robust normality and variance homogeneity check (Shapiro-Wilk + Levene's).
*   Add comprehensive error handling for file reading and statistical tests (e.g., minimum group size for shapiro).
*   Provide more descriptive print statements for the analysis process.
*   Ensure the summary section includes details from the post-hoc tests.
*   Consider defining constants for alpha values and file paths.",grok-3-mini-beta,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, f1_score
import xgboost as xgb
import shap

# Load data
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Create target variable
def create_target(df):
    percentiles = df.groupby('locatio...",7,"The generated code addresses most of the requirements and successfully implements a classification pipeline including data loading, feature engineering, modeling with hyperparameter tuning, evaluation, and SHAP explanations.

**Strengths:**
*   **Correct Pipelining and Preprocessing**: Uses `ColumnTransformer` and `Pipeline` effectively for preprocessing and model training.
*   **Hyperparameter Tuning**: Correctly uses `GridSearchCV` for hyperparameter optimization of the XGBoost model.
*   **Time-based Split**: Implements a correct time-based split for training and testing.
*   **Evaluation Metrics**: Computes ROC-AUC and F1-score as requested.
*   **SHAP Explanation**: Attempts to use SHAP for feature importance and correctly identifies the need for transformed feature names.

**Areas for Improvement (Why it lost points):**

1.  **Target Variable Definition (Major Issue):**
    *   The prompt states: ""A 'high_daily_death_increase' is defined as a day where 'new_deaths' for a 'location' is in the top 10% of its own historical 'new_deaths' (excluding days with zero new deaths).""
    *   The generated code's definition: `((df['new_deaths'] > 0) & (df['new_deaths'] >= df['percentile_90']))`. This is a crucial misinterpretation. The `>=` operator in `new_deaths >= percentile_90` is problematic. If the 90th percentile is `X`, then any value *equal* to `X` should *not* be considered in the ""top 10%"". It should be strictly greater (`>`).
    *   More importantly, the calculation of the percentile itself should *only* consider `new_deaths > 0`. The generated code does: `df.groupby('location')['new_deaths'].apply(lambda x: x[x > 0].quantile(0.9) if len(x[x > 0]) > 0 else 0)`. The `if len(x[x > 0]) > 0 else 0` will assign a 0 as the 90th percentile if a location only has zero deaths, leading to incorrect target creation for those locations. The ground truth correctly handles `NaN` for such cases and then filters, or maps NaN to 0 for locations without any non-zero deaths.

2.  **Missing Value Handling for Engineered Features:**
    *   The prompt specified: ""Handle missing values from feature engineering (e.g., fill with 0 or backfill).""
    *   The generated code simply fills all engineered NaNs with 0. While ""fill with 0"" was given as an example, for time-series features like rolling averages and lags, `ffill` (forward-fill) followed by `fillna(0)` for remaining NaNs (e.g., at the very start of a series) is a much more appropriate and robust approach to avoid data leakage and maintain temporal consistency. The simple `fillna(0)` might not be ideal for these types of features, especially if a value exists but was preceded by NaNs. The `days_since_first_case` calculation also has a potential issue: `df['first_case_date'] = df['location'].map(first_case_dates)`. If a location never had `new_cases > 0`, `first_case_dates` might be empty for it, causing `first_case_date` to be `NaT` and `days_since_first_case` to be `NaN`. The ground truth explicitly calculates `x - x.min()` on the `date` column within each group, which is a more robust way to get days since the *first record* in that location, whether or not it `had new_cases > 0`. The prompt implies ""days since first case"" for a location, which could be interpreted as ""days since the first non-zero case"". The generated code attempts to do that, but might miss cases based on the definition of `first_case_dates`.
    *   `vaccination_rate` calculation `df['people_fully_vaccinated'] / df['population']` lacks robustness for `population` being 0 or NaN, which can lead to `inf` values. The ground truth correctly handles this with `fillna(0)` and `replace([np.inf, -np.inf], 0)`.

3.  **SHAP Plotting and Interpretation:**
    *   The SHAP explanation part requires some refinement. `shap.Explainer(best_model.named_steps['classifier'], X_test_transformed)` is okay for a simple explanation, but `TreeExplainer` is generally preferred for tree-based models like XGBoost.
    *   The generated code tries to print top 10 features by importance manually, but the `shap.summary_plot` automatically handles this if `max_display` is provided. The method of getting `importances` via `np.abs(shap_values.values).mean(axis=0)` is a common way to derive mean absolute SHAP values, but `shap.summary_plot(..., plot_type=""bar"")` does this automatically and plots it. The generated code also re-uses `X_test_transformed` for SHAP value calculation for global importance, which implies evaluating feature importance based on the test set. While not strictly wrong, it's more common to calculate global feature importance using a sample of the training data or the entire training data (as done in the ground truth) to reflect what the model learned during training, and then use summary plots.
    *   The code omits `matplotlib.pyplot` imports and `plt.show()` which are necessary for the plots to actually display/save.

4.  **Minor Code Quality/Robustness:**
    *   No warnings suppression.
    *   The `select_features` part doesn't check if all specified features actually exist in the DataFrame, which could lead to `KeyError` if typos or missing columns occur. The ground truth includes a check.
    *   Missing `classification_report` for more detailed evaluation.

The primary reason for a 7 instead of higher is the fundamental misinterpretation of the target variable definition and the less robust handling of missing values for crucial engineered features.",grok-3-mini-beta,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.formula.api as smf

# Load data and parse 'date'
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Calculate vaccination_percentage
df['vaccination_percentage'] = (df['people_fully_vaccinated'] / df['population']) * 100

# Handle missing values using median imputation grouped by location
columns_to_impute = ['reproduction_rate', 'stringency_index', 'vaccination_percentage']
for col in columns_to_impute:
   ...",5,"The generated code attempts to address the task but falls short on several critical requirements, leading to a much lower score.

**Major Issues:**

1.  **Incorrect Handling of Nested Random Effects:** The prompt explicitly asks for ""Random effects: random intercepts for 'location' nested within 'continent'."" The generated code uses `groups=df[""location""]`, which only accounts for 'location' as a random effect, completely ignoring the 'continent' nesting. This is a fundamental misunderstanding of the specified mixed-effects model structure. The ground truth correctly creates a `continent_location_id` for this purpose.

2.  **Inadequate Missing Value Handling:**
    *   The prompt specifies ""Handle missing values in 'reproduction_rate', 'stringency_index', and 'vaccination_percentage' (e.g., interpolation or median imputation grouped by location).""
    *   The generated code uses `fillna(x.median())` which is a median imputation. While this is *a* method, it's very simplistic. Critically, it does not handle missing `population` or `people_fully_vaccinated` before calculating `vaccination_percentage`, which could lead to NaNs in `vaccination_percentage` that median imputation might struggle with if an entire group is missing.
    *   The ground truth demonstrates a more robust approach: forward-fill/backward-fill for population/vaccination counts (as they are cumulative), then interpolation, and finally a global median fallback, addressing potential issues more comprehensively. The generated code also implies that `vaccination_percentage` can be calculated *before* handling NaNs in `population` and `people_fully_vaccinated`, which will produce many downstream NaNs if these base columns are missing initially.

3.  **Lack of Robustness:**
    *   No error handling for file loading.
    *   No warnings suppression (which can be important for `statsmodels` mixed models).
    *   No sorting by date and location, which is crucial if interpolation were to be used for time-series data.

4.  **Incomplete Interpretation/Reporting:**
    *   The prompt asks to ""Report the fixed effects coefficients, their standard errors, and p-values. Interpret the significance and direction of these effects."" While the code prints the full `mdf.summary()`, its custom interpretation loop is repetitive and doesn't explicitly print the standard errors alongside the coefficients and p-values for each term as requested.
    *   For random effects, it just prints `mdf.cov_re`, which is correct for a random intercept model, but it doesn't provide any interpretation or explanation of what that value represents, unlike the ground truth.

**Minor Issues / Areas for Improvement:**

*   **Fixed Effects Reporting Clarity:** The prompt asks to ""Report the fixed effects coefficients, their standard errors, and p-values."" While `mdf.summary()` provides this, the subsequent custom loop for interpretation could be more concise and clearly present all three pieces of information together for each term, rather than individual print statements.
*   **Model Assumptions Discussion:** The discussion is very brief and provides only basic information. While it covers the points, the ground truth offers a more thorough explanation without implementing the checks, which is helpful.

Overall, the failure to correctly implement the nested random effects is a critical error, as it changes the entire statistical model from what was requested. The simplistic NaN handling and lack of robustness further detract from the solution's quality.",grok-3-mini-beta,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select features
X = df[['elixirCost', 'Win Rate', 'Usage']]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Determine K using elbow method
wcss = []
for i in range(2, 8):  # K from 2 to 7
    kmeans = KMeans(n_...",8,"The generated code correctly addresses most of the requirements of the task. It loads the dataset, selects the specified features, standardizes them, determines K using the elbow method (and plots it), applies K-Means, performs PCA, and visualizes the clusters.

**Strong points:**
- All necessary libraries are imported.
- Data loading and feature selection are correct.
- Standardization is correctly applied.
- The elbow method for determining K is implemented, including the plot.
- K-Means is applied with the determined K.
- PCA is correctly used to reduce dimensionality.
- The final scatter plot visualizes the clusters on PCA components with appropriate coloring and a color bar.

**Areas for improvement to achieve a higher score:**
- The `KMeans` initialization in the elbow method loop and final application should include `n_init=10` to avoid future warnings from scikit-learn (or explicitly set `n_init='auto'` which is the default in newer versions but `10` is a good practice for reproducibility and stability). The ground truth specifically included `n_init=10`.
- Error handling for file not found or missing columns is absent. While the prompt said ""assume the CSV file is in the current directory,"" robust code should include error handling.
- The `chosen_K` is set as a comment, and the plot is shown before the actual clustering. A more integrated approach would be to dynamically pick a K or at least prompt the user or state an assumed K clearly after the plot is shown (as done in the ground truth). Although, for a fixed benchmark, setting it to 3 is acceptable as a demonstration.
- Axis labels in the PCA plot could be more descriptive by including the explained variance ratio of each principal component, which adds more analytical value.
- The generated code doesn't explicitly print intermediate steps or data shapes as the ground truth does, which can be helpful for debugging or understanding the output, although not strictly required by the prompt.

Overall, it's a very good attempt that fulfills the core requirements well.",grok-3-mini-beta,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select features and target
X = df[['elixirCost', 'Usage', 'rarity']]
y = df['Win Rate']

# Split the data
X_train, X_test, y_train, y_test = train_test_spli...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, correctly selects features and the target, preprocesses the `rarity` column using one-hot encoding (specifically `OneHotEncoder` and `ColumnTransformer` which is a robust approach), splits the data, trains a `LinearRegression` model, and evaluates it using R2 and MSE.

Strong points:
- Correct use of `ColumnTransformer` and `OneHotEncoder` for preprocessing, which is generally preferred for clarity and pipeline integration over `pd.get_dummies` when dealing with specific columns and different transformations.
- Proper handling of training and testing data processing (fitting `ColumnTransformer` only on train and transforming both).
- All required libraries are imported.
- The evaluation metrics are correctly calculated.

Areas for minor improvement (not critical for score as per prompt):
- Lack of print statements for the evaluation results, which makes it harder to immediately see the output. The prompt asked for ""Provide only the Python code without explanations,"" and implies the code should be runnable and produce the evaluation results, which printing would facilitate.
- The generated code does not include any error handling for `FileNotFoundError` or missing columns, which the ground truth code includes. While not explicitly asked for in the prompt's requirements, it's good practice for robust code.
- While `LinearRegression` is a valid choice from the prompt, `RandomForestRegressor` was specifically given as an example in the ground truth code. The choice is fine, but it leads to different results.",grok-3-mini-beta,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```
import pandas as pd
from scipy.stats import f_oneway

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Identify the unique categories in the rarity column
unique_rarities = df['rarity'].unique()

# State the null and alternative hypotheses
print(""Null Hypothesis: There is no significant difference in mean Win Rate across different rarity categories."")
print(""Alternative Hypothesis: There is a significant difference in mean Win Rate across at least one rarity category."")

# Pe...",6,"The generated code correctly performs the ANOVA test and interprets the results. It includes the necessary imports and generally follows the required steps.

However, it lacks critical error handling and robustness, which is important for production-ready or even reliable data analysis code. Specifically:
- **No Handling for Missing 'Win Rate' Data:** The code doesn't explicitly convert 'Win Rate' to numeric or handle potential `NaN` values, which can lead to errors if the column is not perfectly clean or if `0` win rates are interpreted incorrectly. The `groupby` will likely fail or produce incorrect results if non-numeric `Win Rate` values are present.
- **No File Not Found Error Handling:** The code assumes 'clash_royale_cards.csv' will always be present, without a `try-except` block for `FileNotFoundError`.
- **No Handling for Insufficient Data:** If a rarity category has only one or zero data points, `f_oneway` might raise an error or produce meaningless results. The ground truth explicitly preprocesses data to ensure each group has at least two observations.
- **Output Formatting:** While functional, the output isn't as clear or as well-formatted as the ground truth (e.g., specific print statements for hypotheses, clear separation of sections).
- **Hypothesis Wording:** The alternative hypothesis in the generated code is slightly less precise ('at least one rarity category has a significantly different mean Win Rate' is better than 'a significant difference in mean Win Rate across at least one rarity category').

These issues make the code less robust and potentially prone to failure in real-world scenarios with less-than-perfect data. Addressing these would significantly improve its reliability and score.",grok-3-mini-beta,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
data = pd.read_csv('iris.csv')

# Define features and target
X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm...",9,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, defines features and target, encodes the target, splits data, trains all three specified models with the correct parameters, and calculates and prints all required metrics (accuracy, precision, recall, F1-score with macro average). It also correctly identifies the best model based on F1-score.

**Strong points:**
- All required imports are present.
- Data loading and feature/target selection are correct.
- Target encoding is correctly implemented.
- Data splitting uses the correct `test_size` and `random_state`.
- All three models are instantiated with the specified parameters.
- Evaluation metrics are correctly calculated using `average='macro'`.
- The best model selection logic is sound.

**Area for minor improvement:**
- The ground truth code included `stratify=y_encoded` in `train_test_split`, which is good practice for classification tasks to ensure the test set has a representative distribution of classes, especially with smaller or imbalanced datasets. Although not explicitly asked for in the prompt, it aligns with robust machine learning practices. The generated code omitted this, which is a minor point but worth noting.
- The Logistic Regression model in the ground truth includes `max_iter=200`, which is often needed for convergence in some environments. While the prompt did not explicitly ask for this, using default `max_iter` might sometimes lead to convergence warnings. For this specific dataset, it usually converges fine with default, but it's a detail worth considering for robustness.",grok-3-mini-beta,LLM_judge_config
H26,hard,Clustering,iris.csv,"```
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Load the dataset
df = pd.read_csv('iris.csv')

# Select the features
features = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Standardize the features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Apply K-Means clustering
kmeans = KMeans(n_clust...",8,"The generated code correctly performs all the main tasks: loading data, selecting and standardizing features, applying K-Means and PCA, and generating two scatter plots with the specified coloring.

Strong points:
- Correctly identifies and uses the required libraries.
- Implements K-Means with the specified parameters (`n_clusters=3`, `n_init='auto'`, `random_state=42`).
- Correctly applies PCA to 2 components.
- Generates both required scatter plots, coloring one by K-Means labels and the other by true species labels.
- Adds appropriate titles and axis labels to both plots.

Areas for improvement to achieve a higher score (9 or 10):
- **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.
- **Colorbar:** The ground truth code includes colorbars for better interpretability of the clusters and species. For the species plot, it correctly maps the encoded numerical labels back to the original species names in the colorbar, which is very helpful. The generated code omits colorbars.
- **Figure Sizing:** The ground truth uses `plt.figure(figsize=(10, 7))` for more readable plots. The generated code uses default figure sizes, which might be less optimal for presentation.
- **Variable Naming:** While `features` is a valid name, `X` (as used in the ground truth) is a more common convention for the feature matrix in machine learning contexts. Similarly, `principal_components` could be `X_pca`. This is a minor point, but good practice.
- **Comments/Readability:** While not explicitly requested to add comments, the ground truth uses comments to delineate each step of the task, which improves readability. The generated code is reasonably clear without them, but it's a difference.

Overall, it's a very good attempt that fulfills the core requirements. The missing colorbars and error handling are the main reasons it's not a perfect score.",grok-3-mini-beta,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# Load the dataset
df = pd.read_csv('iris.csv')

# Create new features
df['PetalArea'] = df['PetalLengthCm'] * df['PetalWidthCm']
df['SepalArea'] = df['SepalLengthCm'] * df['SepalWidthCm']

# Define features X
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'PetalArea', 'SepalArea']]

# Define...",9,"The generated code correctly performs all steps outlined in the task: loading the dataset, creating the required feature engineering features (`PetalArea` and `SepalArea`), defining `X` with both original and new features, encoding the `Species` target, training a Gradient Boosting Classifier with specified parameters, performing 5-fold cross-validation, and reporting the mean accuracy.

The code is clean, well-structured, and includes all necessary imports.

The primary area for minor improvement, preventing a perfect score, is the output formatting. The ground truth provides a more informative print statement (`print(f""Mean cross-validated accuracy: {mean_accuracy:.4f}"")`) which is better for a typical data science report than just printing the raw mean score. Additionally, the ground truth includes error handling for `FileNotFoundError`, which is a good practice for robust scripts, though not explicitly required by the prompt's main task.",grok-3-mini-beta,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load the dataset
df = pd.read_csv('heart.csv')

# Define features and target
X = df[['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingB...",9,"The generated code is largely correct and addresses most of the requirements.

**Strong points:**
- Correctly loads the data.
- Correctly defines `X` and `y` based on the specified columns.
- Accurately identifies and separates categorical and numerical features for preprocessing.
- Uses `ColumnTransformer` for intelligent preprocessing of different feature types.
- Correctly applies `StandardScaler` to numerical features and `OneHotEncoder` to categorical features.
- Splits the data into training and testing sets with the correct `test_size` and `random_state`.
- Trains a `RandomForestClassifier` with the specified `n_estimators` and `random_state`.
- Evaluates the model using all requested metrics: accuracy, macro-averaged precision, recall, F1-score, and ROC-AUC.
- The imports are correct and comprehensive.

**Areas for improvement (minor):**
- **Data Leakage Prevention (Pipeline):** The ground truth uses a `Pipeline` to encapsulate the preprocessing steps (`ColumnTransformer`) and the classifier. This is generally considered best practice as it prevents data leakage (especially in more complex scenarios like cross-validation or hyperparameter tuning) by ensuring that the preprocessor is fitted *only* on the training data. The generated code `X_preprocessed = preprocessor.fit_transform(X)` fits the preprocessor on the *entire* dataset `X` before splitting, which technically introduces a subtle form of data leakage if not careful. While for simple `StandardScaler` and `OneHotEncoder` on fixed datasets this might not cause a catastrophic issue, it's not the ideal pipeline approach. The ground truth correctly fits the `ColumnTransformer` *inside* the `Pipeline` during `model_pipeline.fit(X_train, y_train)`.
- **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is a good practice for robust code. The generated code lacks this.
- **`handle_unknown='ignore'` for `OneHotEncoder`:** The ground truth includes `handle_unknown='ignore'` in `OneHotEncoder`. While not strictly necessary for this specific dataset if all categories are present in both train and test, it's a good practice for robustness in real-world scenarios where unseen categories might appear in the test set.

Overall, the generated code performs the task very well and achieves the desired results. The primary difference is the more robust and best-practice approach of the Ground Truth using a `Pipeline` to prevent potential data leakage and streamline the workflow.",grok-3-mini-beta,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('heart.csv')

# Select the numerical features
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']
X = df[features]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform...",8,"The generated code correctly performs K-Means clustering on the specified features, standardizes them, and visualizes the results with PCA. It includes all required libraries, uses `n_init='auto'` and `random_state=42`, and attempts to determine K.

Specific points:
*   **Correctness**: The core logic for clustering, standardization, PCA, and visualization is correct. The elbow method plot is generated.
*   **Completeness**:
    *   It correctly loads the dataset, selects features, and standardizes them.
    *   It determines K by generating WCSS and silhouette scores and plotting the elbow curve.
    *   However, it **only plots the WCSS (Elbow Method)**, it does not plot the silhouette scores, which was explicitly requested and is crucial for ""justifying your choice of K"" using both methods.
    *   It then chooses K based *solely* on the highest silhouette score programmatically, which is a valid approach but bypasses the visual inspection of both plots (especially the silhouette plot, which wasn't generated). The prompt asked to ""Use the elbow method (plotting Within-Cluster Sum of Squares - WCSS) and silhouette scores to justify your choice of K"", implying visual justification and perhaps a slightly more nuanced approach than just taking the max silhouette.
    *   The final scatter plot correctly shows the clusters in 2D PCA space with a title.
*   **Code Quality**:
    *   The code is generally clean and well-structured, with appropriate variable names.
    *   Imports are correct.
    *   There's no error handling for `FileNotFoundError` as in the ground truth, but that wasn't a strict requirement in the prompt's main bullet points, though it is good practice.

Areas for improvement to get a higher score:
1.  **Plotting Silhouette Scores**: The prompt specifically asked for plotting both WCSS and silhouette scores to justify the choice of K. The generated code calculates silhouette scores but does not plot them. This makes the justification of K incomplete as per the instructions.
2.  **Explicit K Justification**: While choosing the maximum silhouette score is a valid programmatic choice, the prompt implied a visual justification (""Use the elbow method ... and silhouette scores to justify your choice of K""). Without the silhouette plot, visual justification based on both becomes challenging.
3.  **Title and Labels for Plots**: While the PCA plot has a title, the WCSS plot has a title but lacks x/y axis labels, which is a minor detail but contributes to clarity. (Upon review, the WCSS plot does have X/Y labels, which is good).

Overall, the code is very good and functional, but missing the silhouette score plot is a noticeable omission given the explicit prompt.",grok-3-mini-beta,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Create binary target variable
df['target'] = [1 if q >= 7 else 0 for q in df['quality']]

# Handle missing values
df.dropna(inplace=True)

# Separate features and t...",8,"The generated code correctly addresses most of the requirements and successfully builds and evaluates a logistic regression model.

**Strong Points:**
- Correctly loads the dataset.
- Correctly creates the binary target variable using a list comprehension (alternative to `.astype(int)` but achieves the same result).
- Handles missing values using `dropna()` (though for this dataset, it's typically not needed, it's a valid approach).
- Correctly separates features and target.
- Correctly scales features using `StandardScaler`.
- Correctly splits data into train/test sets.
- Correctly trains a `LogisticRegression` model.
- Correctly evaluates performance using accuracy, F1-score, and classification report.
- Correctly identifies and reports the most important features based on model coefficients, including absolute coefficients for ranking.

**Areas for Improvement to achieve a higher score:**
- **Missing Value Handling Detail:** While `dropna()` is used, the prompt asked to ""handle missing values if any"" implying a check before dropping or a more robust imputation strategy might be considered, especially if `dropna` could lead to significant data loss in other scenarios. The ground truth includes a check and a more general imputation strategy (median).
- **Stratified Split:** The `train_test_split` does not use `stratify=y`. For imbalanced datasets (which this wine quality dataset can be for the binary classification task), stratification is crucial to ensure the train and test sets have representative proportions of the target classes. This is a best practice often overlooked but important for robust model evaluation.
- **Scaling Order:** The scaling is done on `X` before splitting (`X_scaled = scaler.fit_transform(X)`). It is best practice to fit the scaler *only* on the training data and then transform both training and test data separately to prevent data leakage from the test set into the scaling process. The ground truth correctly performs `scaler.fit_transform(X_train)` and `scaler.transform(X_test)`.
- **Informative Prints:** The ground truth includes more descriptive print statements at various stages (e.g., successful load, value counts, data split shapes, scaling confirmation, model training confirmation). While not explicitly asked for, these are good for readability and debugging in a real-world scenario.
- **`max_iter` for Logistic Regression:** The default `max_iter` (100) for LogisticRegression might sometimes lead to convergence warnings, especially on scaled data. Increasing it to a higher value (like 1000) as in the ground truth can ensure convergence without warnings.
- **Reporting Most Important Features:** The printed output `print(sorted_features[['Feature', 'Abs_Coefficient']])` is fine, but the ground truth also includes the actual `Coefficient` value, which gives more context (positive vs. negative impact).",grok-3-mini-beta,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

# Load the dataset
df = pd.read_csv('heart.csv')

# Create a box plot
sns.boxplot(x='HeartDisease', y='Oldpeak', data=df)
plt.title('Distribution of Oldpeak by HeartDisease')
plt.show()

# Perform logistic regression
X = df[['Oldpeak']]  # Independent variable
y = df['HeartDisease']  # Dependent variable

model = LogisticRegression()
model.fit(X, y...",8,"The generated code correctly addresses most of the requirements. It successfully loads the data, creates the specified box plot, performs logistic regression, reports the coefficient and odds ratio, discretizes `Oldpeak` using `pd.qcut`, and calculates the proportion of `HeartDisease=1` within each bin.

Here are the strong points:
- All required libraries are imported.
- The box plot is correctly generated with `HeartDisease` on the x-axis and `Oldpeak` on the y-axis.
- Logistic regression is correctly implemented, and `X` is properly reshaped.
- The coefficient and odds ratio are correctly calculated and printed.
- `pd.qcut` is used as requested for discretization.
- The proportion calculation using `groupby().mean()` is correct and concise.

Areas for improvement to reach a higher score:
- **Interpretation of Odds Ratio:** While the generated code states what the odds ratio means (multiplication factor), the ground truth provides a more robust and comprehensive interpretation, including how to translate the odds ratio into a percentage increase/decrease in odds and explicitly stating ""holding other factors constant."" This level of detail is important for a complete interpretation.
- **Robustness/Error Handling:** The ground truth includes `try-except` blocks for file loading and a `duplicates='drop'` argument for `pd.qcut`, making the code more robust to common issues. It also includes checks for missing values, which is good practice in data analysis.
- **Output Clarity and Detail:** The ground truth adds print statements to clearly indicate which step of the analysis is being executed and provides more descriptive output for each section.
- **Plot Enhancements:** While the box plot is correct, the ground truth adds more informative labels, a grid, and `xticks` for better readability. It also includes an optional visualization for the proportions of Heart Disease per bin, which was not explicitly requested but adds value to the analysis.
- **Assumptions/Best Practices:** The ground truth includes comments about `X` needing to be 2D for scikit-learn and explicitly checks for missing values in relevant columns, which are good practices.

Overall, the generated code performs the core tasks effectively. The main improvements would be in the depth of interpretation, error handling, and presentation of the results.",grok-3-mini-beta,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

# Load data
df = pd.read_csv('spotify_2023.csv')

# Convert 'streams' to numeric and drop rows with NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams'])

# Create binary target va...",7,"The generated code successfully completes most of the requirements. It loads data, converts `streams` to numeric and handles NaNs, creates the `is_popular` target, selects specified features, one-hot encodes categorical features, splits data, trains a Logistic Regression model, and evaluates it using F1-score (macro), ROC AUC, and confusion matrix.

However, there are a few areas for improvement:
1.  **Missing `random_state` in Logistic Regression:** The task specified `LogisticRegression(default parameters, max_iter=1000)`. While `max_iter=1000` is included, a `random_state` for the model is generally good practice for reproducibility, especially with solvers that might have a stochastic component (though typically less critical for `lbfgs` which is the default for `max_iter=1000`). The ground truth includes `random_state=42` for the model.
2.  **Handling of `pd.concat` and `OneHotEncoder` `drop='first'`:** The `OneHotEncoder(drop='first', sparse=False)` is used. While `drop='first'` is a valid approach to avoid multicollinearity, the task itself didn't explicitly ask for it, and the ground truth uses `pd.get_dummies` without `drop_first=True`. Sticking to `drop_first=False` might be closer to the implicit ""default"" behavior regarding one-hot encoding if not specified.
3.  **Potential for `SettingWithCopyWarning` with `numerical_features` and `categorical_features`:**
    ```python
    numerical_features = df[['danceability_%', 'energy_%', 'valence_%',
                             'acousticness_%', 'bpm', 'artist_count']]
    categorical_features = df[['key', 'mode']]
    ```
    These lines create views of the DataFrame. If `df` were to be modified later (which it isn't in this specific code in a way that would trigger a warning here), it could lead to `SettingWithCopyWarning`. Using `.copy()` (`df[['col1', 'col2']].copy()`) is safer practice, though not strictly an error given the current code flow. The ground truth extracts X as `df[all_features].copy()`, which is a better overall approach for the feature matrix.
4.  **Implicit index alignment in `pd.concat`:** When `numerical_features` and `categorical_features` are created by slicing `df`, their indices align correctly for `pd.concat`. This works in this specific case, but it's important to be aware that if the original `df` had been reindexed or rows dropped non-contiguously *after* creating these feature subsets, `pd.concat` might behave unexpectedly due to index misalignment. The ground truth's approach of creating `X = df[all_features].copy()` and then dummifying `X` directly avoids this potential issue by keeping all features in one `X` DataFrame before encoding.

Overall, the code is functional and largely correct, but these minor points prevent a higher score.",grok-3-mini-beta,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('spotify_2023.csv')

# Select audio features and drop rows with NaNs
features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%', 
            'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm']
df_clean = df[features].dropna()

# Standardize features
scaler = StandardSca...",9,"The generated code correctly addresses all requirements of the task.

**Strengths:**
- **Correctness:** All steps are performed accurately: data loading, feature selection, NaN handling, standardization, K-Means clustering with specified parameters, PCA with specified parameters, visualization, and calculation of cluster means from original (unstandardized) features.
- **Completeness:** All seven main points of the task are covered.
- **Clarity and Conciseness:** The code is quite readable and to the point.
- **Proper Imports:** All necessary libraries are imported.
- **Parameter Adherence:** `K=4`, `random_state=42`, and `n_init='auto'` for KMeans, and `random_state=42` for PCA are correctly used.

**Minor Areas for Improvement (preventing a 10):**
1.  **Error Handling for File Not Found:** The generated code does not include any `try-except` block for `FileNotFoundError` when reading the CSV, which is a good practice for robust code. The ground truth includes this.
2.  **Explicit Copying:** While `df[features].dropna()` creates a *new* DataFrame, explicitly using `.copy()` after feature selection (`df[features].copy()`) makes the intent clearer and prevents potential `SettingWithCopyWarning` if subsequent operations were to modify the slice directly before `.dropna()`. In this specific case, `dropna()` returns a new DataFrame, so it's less critical, but good to be aware of.
3.  **Use of `df_clean` for Mean Calculation:** The code correctly attaches the cluster labels to `df_clean`, which holds the *original, unstandardized* features that passed the NaN check. This is crucial for calculating the means of the original features per cluster, and it's done correctly.
4.  **Plot Labels/Title:** While main title/labels are there, the ground truth adds more details like colorbar label which slightly enhances the plot readability.

Overall, it's a very strong submission that fulfills the core prompt instructions effectively. The minor points are mostly about robustness and slight polish rather than functional correctness.",grok-3-mini-beta,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```
import pandas as pd
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('spotify_2023.csv')

# Convert 'streams' to numeric, coercing errors to NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# Filter for songs with streams > 10,000,000 and released_year between 2010 and 2023
filtered_df = df[(df['streams'] > 10000000) & (df['released_year'] >= 2010) & (df['released_year'] <= 2023)]

# Drop rows with NaN streams
filtered_df = filtered_df.dropna(subset=['streams']...",8,"The generated code correctly addresses most of the requirements. It loads the data, converts 'streams' to numeric, filters the data based on `streams` and `released_year` criteria, groups by `released_year`, and calculates the average `danceability_%`, `energy_%`, and `valence_%`. The plots are generated with appropriate titles and labels.

**Strengths:**
*   Correctly handles `pd.to_numeric` with `errors='coerce'`.
*   Applies all specified filtering conditions.
*   Correctly groups and calculates averages.
*   Generates individual plots with titles and labels.

**Areas for Improvement:**
*   **Order of Operations for Filtering:** The generated code filters for `streams` > 10M *before* dropping NaNs from streams. While in this specific case, it might not produce a different result if the NaNs are not greater than 10M, it's generally safer and more robust to drop NaNs in the column used for numerical comparison *before* applying the numerical comparison. The ground truth correctly drops NaNs in `streams` *first*, then applies the `streams > 10M` filter.
*   **Plotting Efficiency/Consolidation:** The prompt asks to ""Display all three plots"". The generated code creates three separate `plt.figure()` calls and calls `plt.show()` three times, resulting in three separate plot windows appearing sequentially. A better approach, as demonstrated in the ground truth, is to use `plt.subplot` or create a single figure with multiple axes to display all plots in one go, which is generally more user-friendly and coherent for presenting related trends.
*   **Error Handling:** The code lacks basic error handling for file not found, which was included in the ground truth. While not explicitly part of the core ""analysis logic"" criteria, it's good practice for robust code.
*   **Clarity of Filters:** The prompt explicitly says: ""Drop rows with NaN `streams` after conversion."" The generated code performs this *after* the initial filtering by `streams` value and `released_year`. While it eventually drops them, doing it before the numerical comparison on `streams` (as in the ground truth) is more logical and robust.",grok-3-mini-beta,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean Likes, Dislikes, and Video views
columns_to_clean = ['Likes', 'Dislikes', 'Video views']
for col in columns_to_clean:
    df[col] = df[col].astype(str).str.replace(',', '').astype(float).fillna(0)...",9,"The generated code is largely correct and addresses almost all requirements of the task.

**Strong Points:**
- **Correct Data Loading:** Successfully loads the `youtube_trendings.csv` dataset.
- **Correct Cleaning of Numeric Columns:** Accurately removes commas, converts to numeric, and fills NaNs with 0 for 'Likes', 'Dislikes', and 'Video views'.
- **Correct `published` Year Extraction:** Extracts the year from `published` column correctly.
- **Correct `Category` Handling:** Fills missing values with 'Unknown' and applies one-hot encoding.
- **Correct Feature and Target Definition:** `X` and `y` are correctly defined as specified.
- **Correct Data Splitting:** Uses the correct test size (20%) and `random_state=42`.
- **Correct Model Training:** Trains a `LinearRegression` model.
- **Correct Evaluation Metrics:** Calculates and reports R2 and MAE.
- **Proper Imports:** Includes all necessary libraries.

**Areas for Minor Improvement/Missing Aspects (leading to a score of 9 instead of 10):**
1.  **Handling `pd.to_datetime` errors:** The generated code does not explicitly handle potential errors in `pd.to_datetime(df['published'])`. If the `published` column contains unparseable dates, this line would raise an error or produce `NaT` values. The ground truth uses `errors='coerce'` which is a more robust approach. While the prompt implied a clean dataset for this specific column, in a real-world scenario, this is crucial.
2.  **`fillna(0)` for `published_year`:** Even if `pd.to_datetime` handles errors, `dt.year` can still produce `NaN` if the datetime conversion results in `NaT`. The ground truth explicitly handles this `NaN` by filling it with 0, which is good practice. The generated code implicitly relies on `X` being numeric, but it's better to explicitly ensure no NaNs are passed to the model. In practice, `OneHotEncoder` `handle_unknown='ignore'` is good, but `NaN` in numerical features will break `LinearRegression` if not handled. If `pd.to_datetime` produced `NaT` and `dt.year` produced `NaN`, these `NaN`s would be passed to the model and cause an error.
3.  **Type conversion to `int` for cleaned columns:** The ground truth converts `Likes`, `Dislikes`, and `Video views` to `astype(int)` after cleaning. The generated code converts them to `float`. While `float` works for regression, `int` is more natural for these count-based metrics if they are whole numbers, as they are. The prompt didn't explicitly ask for `int`, but it's a detail worth noting for data consistency.

Overall, the code is very good and effectively solves the problem. The minor points are mostly about robustness and slight type precision, which are important in practice but don't fundamentally break the solution for the given task context.",grok-3-mini-beta,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '')
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce')
df['Video views'] = df['Video views'].fillna(0)

# Extract year from 'published' and ensure it's numeric
df['published_year'] = pd.to_datetime(df['published']).dt.year

# Group by published year and calculate metrics
groupe...",8,"The generated code successfully addresses most of the requirements.

**Strengths:**
*   **Data Loading:** Correctly loads the `youtube_trendings.csv` dataset.
*   **Video Views Cleaning:** Accurately removes commas, converts to numeric, and fills NaNs with 0, which is crucial for numerical analysis.
*   **Published Year Handling:** Correctly extracts the year from the 'published' column and treats it as numeric.
*   **Grouping and Metric Calculation:** Correctly groups by `published_year` and calculates both the average video views and the count of unique videos per year.
*   **Plotting:** Creates two subplots with appropriate titles and labels for each axis. The plots are clear and represent the requested trends.
*   **Imports:** Includes the necessary `pandas` and `matplotlib.pyplot` imports.

**Areas for Improvement (to reach a higher score):**
*   **Robustness of 'published' column:** The prompt specified `published` column but the input data schema typically has `publishedAt`. The generated code assumes 'published' directly contains datetime strings. While this works if the column is indeed `published`, a more robust solution would account for the `publishedAt` column (as in the ground truth) if `published` refers to something else or is not present. *Correction*: Rereading the prompt, it only mentions `published` column (year), so the interpretation of `df['published_year'] = pd.to_datetime(df['published']).dt.year` is valid given the prompt text directly. My internal thought process assumed an `publishedAt` based on common YouTube datasets.
*   **Error Handling:** The code does not include any error handling for file not found or issues during data processing, which would make it more robust for real-world scenarios.
*   **Axis Label Readability (Minor):** For the x-axis (Year), setting `xticks` to unique years derived from the data would make the x-axis consistent with the data points, especially if there are missing years or if the range is large. This is a minor plotting refinement.
*   **Color consistency (Minor):** While not a strict requirement, using consistent colors or a more diverse palette for the line plots could enhance visual appeal, but the chosen colors are fine.
*   **Docstrings/Comments:** While the prompt asked for only code, in a real-world scenario, adding comments or a docstring explaining the code's purpose would improve maintainability. This is not penalized as the prompt asked for ""only the Python code without explanations"".",grok-3-mini-beta,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```
import pandas as pd
import re
from collections import Counter
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Create title_word_count feature
df['title_word_count'] = df['Video'].apply(lambda x: len(x.split()))

# Clean Video views column
df['Video views'] = df['Video views'].str.replace(',', '')
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce')
df['Video views'] = df['Video views']...",8,"The generated code correctly performs all the specified steps for text feature analysis.

**Strong points:**
- Successfully loads the dataset.
- Correctly computes `title_word_count`.
- Accurately cleans the `Video views` column (removes commas, converts to numeric, fills NaNs).
- Calculates and prints the Pearson correlation coefficient correctly.
- Filters the dataset for the specified years (2023/2024).
- Processes video titles (lowercase, removes punctuation, removes stop words) for the filtered dataset.
- Correctly identifies and prints the 5 most common words.
- Uses appropriate libraries and imports.

**Areas for improvement:**
- **Robustness in `nltk.download`:** The ground truth includes a `try-except` block for `nltk.data.find('corpora/stopwords')` to check if stopwords are already downloaded, preventing unnecessary re-downloads and handling potential errors more gracefully. The generated code simply calls `nltk.download('stopwords')` which is less robust.
- **Error Handling for `FileNotFoundError`:** The ground truth includes a `try-except` block for `pd.read_csv` to handle cases where the CSV file is not found, providing a user-friendly error message and exiting, which is good practice for production-ready code. The generated code lacks this.
- **Output clarity:** The correlation print statement in the generated code just prints the number. The ground truth provides more context, e.g., `print(f""Pearson correlation between 'title_word_count' and 'Video views': {correlation:.4f}"")` and also adds a separator `print(""-"" * 50)`. Similarly, for the most common words, the ground truth provides a header and a more detailed output format (e.g., `- 'word': count occurrences`), making the output much clearer.
- **Slight over-processing in title cleaning:** In the word extraction for filtered videos, The ground truth uses `word.isalpha()` in addition to checking against stop words. This ensures that only true alphabetic words are counted, excluding numbers or empty strings that might result from regex and split operations. While the generated code's approach is often sufficient, `isalpha()` adds a layer of robustness.
- **Type casting for `Video` column:** The ground truth explicitly casts `df['Video']` to `str` before applying `split()`. While pandas often handles this implicitly, explicit casting is safer to prevent errors if non-string types somehow appear in the column, especially when processing titles. The generated code assumes all `Video` values are strings.",grok-3-mini-beta,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score

# Load data
df = pd.read_csv('mobiles_2025.csv')

# Clean Launched Price (USA)
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '')
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace(',', '')
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (...",8,"The generated code correctly addresses most of the requirements and demonstrates a good understanding of the task.

**Strong points:**
- Correctly loads the data.
- Handles cleaning of `Launched Price (USA)` including `USD` and commas and converting errors to NaN, then creating the `is_expensive_usa` target, and dropping NaNs.
- Correctly cleans all specified numerical features (`Mobile Weight`, `RAM`, `Battery Capacity`, `Screen Size`) by removing units and converting to numeric.
- Correctly cleans `Front Camera` and `Back Camera` by extracting the first numerical MP value and defaulting to 0 if none, using a more concise regex extraction.
- Correctly performs one-hot encoding for `Company Name` and `Processor`.
- Defines `X` and `y` using the specified features and target.
- Handles dropping rows with NaNs correctly before splitting.
- Splits the data with the correct parameters (`train_test_split`, `test_size=0.2`, `random_state=42`).
- Trains a `RandomForestClassifier` with the specified `n_estimators` and `random_state`.
- Evaluates the model using `f1_score` (macro average) and `roc_auc_score`.
- Proper imports are included.

**Areas for improvement (minor issues):**
- **Clarity and Robustness for `is_expensive_usa`:**
    - The ground truth code explicitly converts `is_expensive_usa` to `int` after creation and NaN dropping. While `astype(float)` then dropping NaNs and then `model.fit` usually handles it, explicitly casting to `int` for a binary target is good practice and matches the task's implicit expectation of a binary integer target. The generated code's `(df['Launched Price (USA)'] > 1000).astype(float)` directly creates 0.0 or 1.0 or NaN, which is fine, but `astype(int)` would be more precise for a binary target.
    - The ground truth includes `if pd.notna(x) else np.nan)` in the `is_expensive_usa` creation lambda, which is redundant if `pd.to_numeric` already coerces to NaN. However, `(df['Launched Price (USA)'] > 1000).astype(float)` correctly propagates NaNs, so it's a non-issue here.
- **`Launched Year` as a feature:** The ground truth explicitly lists 'Launched Year' as a numerical feature. The generated code implicitly includes it because it's in the original `df` and isn't acted upon by any cleaning, and the definition of `numerical_features` covers it, so this point is minor.

Overall, the generated code is highly functional and correctly implements the core logic required by the task. The minor improvements focus on robustness and explicit type handling.",grok-3-mini-beta,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Define the function to clean price columns
def clean_price(value):
    if pd.isna(value):
        return np.nan
    try:
        value = str(value).replace('PKR', '').replace('INR', '').replace('CNY', '').replace('USD', '').replace('AED', '')
        value = value.replace(',', '')
        return float(value)
    except:
        return np.nan

# Apply the cleaning function to the specified colum...",9,"The generated code is highly correct and effectively solves the given task.

**Strong Points:**
1.  **Correct Loading:** Correctly loads the `mobiles_2025.csv` dataset.
2.  **Effective Price Cleaning:** The `clean_price` function correctly removes currency symbols and commas, and converts values to `float`, handling `NaN` and other errors gracefully. It correctly applies this to all five specified price columns.
3.  **Accurate Average Calculation:** Correctly calculates `Average Global Price` using `mean(axis=1, skipna=True)`, which is crucial for handling missing price data correctly.
4.  **Correct Top 5 Identification:** Identifies the top 5 models based on `Average Global Price` and sorts them correctly.
5.  **Clean Output:** Displays the `Model Name` and `Average Global Price` clearly for the top 5 models.
6.  **Proper Imports:** Includes necessary pandas and numpy imports.

**Minor Areas for Improvement (why not a 10):**
1.  **Error Handling for File Not Found:** The generated code lacks explicit `try-except` block for `pd.read_csv`, which means it would crash if `mobiles_2025.csv` is not found, unlike the ground truth which gracefully handles this.
2.  **Robustness in `clean_price` (Subtle):** While the `clean_price` function works, the ground truth's `for symbol in currency_symbols:` loop and `str.strip()` provide slightly more robustness. The current implementation assumes symbols are always at the beginning/end or not followed by other non-numeric characters. However, for the given data, it functions correctly.
3.  **Clarity in `clean_price` error handling:** The `except:` block is a bare except, which catches all exceptions. It's generally better practice to catch specific exceptions (e.g., `ValueError`) to avoid hiding unexpected issues.

Overall, it's a very solid solution that meets almost all requirements. The missing file error handling is the most significant omission for a production-ready script.",grok-3-mini-beta,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the columns
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''), errors='coerce')

# Drop rows where any of the three cleaned colu...",8,"The generated code correctly addresses most of the requirements.

**Strong points:**
*   **Correct Cleaning:** The code successfully cleans the specified columns (`RAM`, `Battery Capacity`, `Launched Price (USA)`) by removing units/characters and converting them to numeric types, handling errors by coercing to `NaN`.
*   **Correct NaN Handling:** It correctly drops rows where any of the three cleaned columns are `NaN`.
*   **Correct Filtering:** It properly filters the data to include only 'Apple' products.
*   **Correct Grouping:** It correctly groups the filtered Apple data by `Launched Year`.
*   **Correct Plotting:** It generates box plots for each year for `RAM`, `Battery Capacity`, and `Launched Price (USA)` using subplots.
*   **Appropriate Titles and Labels:** Plots have titles indicating the year and metric, and y-axis labels are appropriate.

**Areas for Improvement (Why not a 10):**
*   **Robustness in Cleaning `Launched Price (USA)`:** The generated code directly applies `.str.replace('USD ', '')` without first ensuring the column is of string type. While `pd.to_numeric` can sometimes handle non-string types, it's safer and more explicit to convert to `str` first, especially for methods like `.str.replace()`. The ground truth uses `astype(str)`.
*   **Handling `Launched Year` Conversion:** The task implies that `Launched Year` should implicitly be clean or made numeric for grouping. The generated code relies on `groupby` to handle potential non-numeric years, which might work if the year is already numeric, but it doesn't explicitly convert or handle errors for the `Launched Year` column itself prior to grouping, unlike the ground truth which explicitly ensures it's numeric and handles NaNs. This could lead to issues if `Launched Year` contains non-numeric values.
*   **Import Statements:** While `matplotlib.pyplot` is imported, `seaborn` is not, even if it's not strictly required by the prompt's examples, it's a good practice for data science tasks involving advanced plotting and can improve aesthetics. The ground truth includes it and sets a style for plots.
*   **No General Error Handling for CSV Load:** The code doesn't include a `try-except` block for loading the CSV file, which means it would crash if the file is not found, unlike the ground truth.
*   **No Explicit Styling:** The plots produced are functional but do not have any specific styling applied (e.g., `plt.style.use()`), which was mentioned in the ground truth for ""professional style"". While not a strict requirement, it contributes to overall code quality in a data science context.
*   **Missing `plt.tight_layout()`:** For subplots, `plt.tight_layout()` is often necessary to prevent titles or labels from overlapping, which was included in the ground truth.",grok-3-mini-beta,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, learning_curve, validation_curve
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.preprocessing import PolynomialFeatures, StandardSca...",6,"The generated code attempts to address all aspects of the prompt but falls short in several areas regarding advanced implementation, thoroughness, and proper handling of pipelines for preprocessing and model fitting.

**Strengths:**
*   Includes a good range of standard libraries for data science.
*   Attempts text mining (`extract_title`).
*   Implements multiple imputation strategies (SimpleImputer, KNNImputer, though not IterativeImputer as requested).
*   Applies polynomial features and interaction terms.
*   Includes multiple base models (RF, GB, SVM, NN, XGBoost) and ensemble methods (Voting, Stacking).
*   Utilizes GridSearchCV and RandomizedSearchCV for tuning.
*   Performs feature selection using SelectKBest and RFE.
*   Includes learning curves and validation curves.
*   Attempts SHAP for model interpretability.
*   Compares models using performance metrics.

**Areas for Improvement / Missing or Incorrect Implementations:**

1.  **Preprocessing Pipeline (Critical):**
    *   **Data Leakage in Preprocessing:** The generated code applies feature engineering, imputation, and scaling *before* splitting the data into training and test sets. This leads to data leakage, where information from the test set influences preprocessing steps, resulting in an overly optimistic evaluation. The correct approach is to split first, then apply preprocessing steps *within* a pipeline or separately to train/test sets to prevent leakage.
    *   **Lack of `ColumnTransformer` and comprehensive `Pipeline`:** The prompt specifically asks for a `ColumnTransformer` to handle different types of features and a robust `Pipeline` structure. The generated code performs preprocessing steps sequentially on the entire `df` or on `X` after feature selection, which is inefficient and prone to errors. For example, `PolynomialFeatures` is applied to `df` *before* train-test split and even before feature scaling, which is unusual.
    *   **Multiple Imputation:** While `KNNImputer` is used, the prompt explicitly asked for ""Multiple Imputation by Chained Equations"" (referring to `IterativeImputer`), which is more advanced than just KNN.

2.  **Advanced Feature Engineering:**
    *   **Text Mining:** The `extract_title` is basic. It doesn't perform grouping of rare titles or other sophisticated name-based features like the ground truth does.
    *   **Interaction Terms / Polynomial Features:** These are applied globally before splitting, which is a data leakage issue.

3.  **Hyperparameter Tuning (Scope):**
    *   `RandomizedSearchCV` for SVM only includes 10 iterations which is very limited for comprehensive search. The prompt mentioned ""comprehensive data preprocessing"" and ""tuning multiple base models using GridSearchCV or RandomizedSearchCV"", implying a more rigorous tuning.

4.  **Ensemble Models:**
    *   **Hard vs. Soft Voting:** The `VotingClassifier` uses `hard` voting and `SVC` needs `probability=True` for `soft` voting to be applicable. The prompt implied combining ""best models"", and soft voting is generally superior.
    *   **StackingClassifier `final_estimator`:** The `final_estimator` is `XGBClassifier()` without any tuning. It should also be a pipeline step and potentially tuned.

5.  **Feature Selection:**
    *   **Comparison of Impact:** The comparison is only rudimentary (accuracy of a simple RF model post-selection). The ground truth shows how to integrate feature selection *pipelines* or apply it systematically before evaluation, comparing results across all models.
    *   **Integration with Pipeline:** Feature selection is done upfront and then `X_selected` is used, preventing proper integration into a robust `Pipeline` structure that would apply selection (if desired) during cross-validation consistently.

6.  **Extensive Model Evaluation:**
    *   **`cross_val_score` omission:** While `evaluate_model` function simulates cross-validation, it manually loops through splits. `cross_val_score` is a cleaner, more standard way to do this.
    *   **Statistical Significance Tests:** The implementation of the t-test on `cross_val_predict` output is fundamentally incorrect. `cross_val_predict` returns predictions for *each fold* in the training set, not independent scores across folds, and thus should not be used for feature selection comparisons. A paired t-test requires comparing the *distribution of scores* (e.g., ROC AUC for each fold from `cross_val_score`) for different models, not just their aggregated predictions. The ground truth correctly uses `cv_results` (scores per fold) for this.

7.  **Model Interpretability:**
    *   **LIME is missing:** The prompt specifically asked for SHAP *or* LIME. Only SHAP is implemented.
    *   **SHAP Application:** The SHAP features are `X.columns[:X_selected.shape[1]]`, which might not correctly map if columns were renamed or reordered by `get_dummies` and `PolynomialFeatures`. A clear `feature_names` array from the preprocessed data is needed. The SHAP plot won't be as meaningful without correct feature names.

8.  **Code Quality / Structure:**
    *   No proper use of `Pipeline` for preprocessing.
    *   Direct modification of `df` in global scope, which can lead to issues.
    *   Repeated code for `GridSearchCV` fitting.
    *   Lack of warnings suppression (minor).
    *   Missing `enable_iterative_imputer` import.
    *   No `lime` import.

Overall, the generated code addresses most keywords from the prompt but lacks the advanced detail, proper structural implementation (especially pipelines), and correctness in several key areas (like statistical testing and preprocessing preventing data leakage) that the prompt requested.",grok-3-mini-beta,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Define features and target
X = df[['Year', 'Runtime']]
y = df['Rating']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = LinearRegression()
model.fit(X_train, y_train)

...",7,"The generated code successfully completes the core requirements of loading data, defining features and target, splitting data, training a Linear Regression model, and evaluating it with R2 and MAE. It includes all necessary imports and follows a logical flow.

However, it lacks robustness and completeness in a few key areas that are present in the ground truth:
1.  **Error Handling for File Not Found:** The generated code does not handle the `FileNotFoundError` if `imdb_top_250.csv` is not present, which would cause the script to crash. The ground truth includes a `try-except` block for this.
2.  **Data Preprocessing/Cleaning:** The `Runtime` column often contains ' min' strings which need to be removed and then converted to numeric. Similarly, `Year` and `Rating` might contain non-numeric values. The ground truth correctly handles these conversions and explicitly drops rows with NaNs resulting from these conversions, ensuring the model receives clean numerical data. The generated code assumes the columns are perfectly clean and numeric, which is often not the case with real-world datasets. This could lead to errors or incorrect model training if the data isn't pre-processed.
3.  **Output/Printing:** The generated code performs all calculations but does not print the results (R2 score and MAE). The task explicitly states to ""evaluate the model using R-squared (R2) score and Mean Absolute Error (MAE)"" which implies displaying these values. The ground truth prints these evaluations.

These missing pieces make the code less robust and user-friendly compared to the ground truth. Addressing points 1 and 2 would significantly improve its reliability and general applicability, while point 3 is about fulfilling the implicit output requirement of ""evaluate the model"".",grok-3-mini-beta,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Select features
X = df[['Rating', 'Runtime']]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
kmeans.fit(X_scaled)
labels = kmeans.labels_

# Create scatter plot
plt.sca...",6,"The generated code correctly implements the core K-Means clustering steps: loading data, selecting features, standardizing, and applying K-Means. It also attempts to create a scatter plot and calculate cluster means, which are good.

However, there are significant issues that prevent it from being a higher score:
1.  **Missing Data Cleaning/Preprocessing for 'Runtime' column**: The `imdb_top_250.csv` dataset typically has 'Runtime' in a string format like '142 min'. The generated code does not handle this conversion to numeric, which will cause `pd.to_numeric` (implicitly called by `StandardScaler` if not pre-converted) or the `df[['Rating', 'Runtime']]` selection to fail or produce NaNs, leading to errors or inaccurate clustering. The ground truth explicitly addresses this.
2.  **Lack of Error Handling for File Not Found**: The ground truth includes a `try-except` block for `FileNotFoundError`, which is a good practice. The generated code assumes the file is always present.
3.  **No `dropna` after feature selection**: If 'Rating' or 'Runtime' had missing values, the current code would fail during standardization or clustering. The ground truth handles this for robustness.
4.  **Minor Plotting Deficiency**: The scatter plot does not include a colormap (`cmap`), making the colors less distinct, and lacks `colorbar`, which is helpful for interpreting the cluster labels.
5.  **Implicit Grouping**: `df.groupby(labels)` implicitly associates the labels with the dataframe. While it might work if `labels` has the same index/order, explicitly adding labels back to the DataFrame (`df['cluster_label'] = labels`) before grouping is more robust and clearer, as done in the ground truth.",grok-3-mini-beta,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```
import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter

# Download necessary NLTK resources
nltk.download('stopwords')
nltk.download('punkt')

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create a new feature 'title_length'
df['title_length'] = df['Title'].apply(len)

# Calculate and print the Pearson correlation coefficient
correlation = df['title_length'].corr(df['Rating'])
print(...",7,"The generated code largely addresses the requirements but has several areas for improvement to reach the quality of the ground truth.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly calculates `title_length`.
*   Attempts to calculate and print the correlation coefficient.
*   Correctly downloads NLTK resources.
*   Implements a `preprocess_title` function that converts to lowercase, removes punctuation (though with a specific method), tokenizes, and removes stopwords.
*   Correctly finds and prints the 10 most common words and their frequencies using `collections.Counter`.

**Areas for Improvement:**
*   **Correlation Calculation Accuracy & Robustness:** The generated code calculates `df['title_length'].corr(df['Rating'])` directly. This assumes both columns are numeric and clean. The ground truth explicitly uses `pd.to_numeric(df['Rating'], errors='coerce')` and `dropna(subset=['Rating', 'title_length'])` to handle potential non-numeric values in 'Rating' (which is common in real-world data) and missing values, making the correlation calculation more robust and accurate. The prompt specifies Pearson correlation, which the `.corr()` method does by default, but the robustness of the input data is critical. Also, the output format of the correlation could match the ground truth for consistency (e.g., `:.4f`).
*   **Punctuation Removal:** The generated code's punctuation removal (`''.join([char for char in title_lower if char.isalnum() or char.isspace()])`) is functionally correct but less robust than using regex (`re.sub(r'[^a-z0-9\s]', '', text)`) for general character filtering, particularly if titles contain non-ASCII alphanumeric characters or other complex patterns. The prompt also *required* `string` module, but the generated code doesn't use it for punctuation removal (while the ground truth implies it could be used for general string constants or methods, but uses `re` for the actual removal logic). This isn't a critical flaw but worth noting.
*   **Preprocessing Flow:** The separate iterations for preprocessing and tokenization in the ground truth (`df['processed_title']`, `df['tokenized_title']`, `df['filtered_title']`) offer better debugging and inspection capabilities, as intermediate steps are stored in the DataFrame. The generated code uses a single function and loop to process all titles, which is more concise but less transparent for debugging.
*   **Prompt Adherence - `string` module:** The prompt explicitly mentioned `string` module as a required import. While the generated code imports it, it doesn't utilize it. The ground truth imports `string` but actually uses `re` for a more robust solution, which is acceptable because it correctly solves the problem even if it's not the exact method implied by the `string` module. The generated code could have directly removed punctuation using `str.translate` with `string.punctuation` if it aimed to follow the module hint.
*   **Output Formatting:** The ground truth includes an f-string for the correlation printout that labels the values explicitly and formats them to 4 decimal places. The most common words output also has a slightly different format. These are minor but contribute to clarity.
*   **Word Filtering (`word.isalpha()`):** The generated code adds `and word.isalpha()` during token filtering. While this helps clean up non-alphabetic tokens (e.g., numbers that might pass the initial `isalnum()` check if not properly filtered), the ground truth's regex approach (`[^a-z0-9\s]`) more explicitly targets what to keep, letting the `word_tokenize` handle actual words better. The `isalnum()` approach followed by `isalpha()` might filter out words with numbers in them if that's not desired (e.g., ""star_wars_episode_IV"" would lose ""IV"" if `isdigit()` wasn't also checked). The ground truth's `[^a-z0-9\s]` regex explicitly allows numbers in the text before tokenization.",grok-3-mini-beta,LLM_judge_config
H43,hard,Classification,penguins.csv,"```
import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer

# Load the dataset
df = pd.read_csv('penguins.csv')

# Define features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']]
y = df['species']

# Label encode the target
le = ...",6,"The generated code addresses most of the requirements but has a critical error and some minor issues.

**Correctness (3/4):**
- **Critical Error:** The generated code attempts to use `ColumnTransformer` (which requires `from sklearn.compose import ColumnTransformer`) without importing it. This will cause a `NameError` and the code will not run. This is a fundamental flaw for a solution that aims to be comprehensive in preprocessing.
- **DataFrame/Series handling after `SimpleImputer` and `ColumnTransformer`:** `SimpleImputer.fit_transform` returns a NumPy array, not a DataFrame. Assigning it directly back to `X[num_features]` and `X[cat_features]` can lead to column name/type issues or unexpected behavior if not handled carefully (though it might work for a direct assignment of values to existing columns if `X` is already a DataFrame and types align). More importantly, `ColumnTransformer.fit_transform` also returns a NumPy array. Subsequent operations (like `X['island'].isna()` after the categorical imputation and `cross_val_score`) will fail or behave unexpectedly because `X` will no longer be a pandas DataFrame with named columns. This means the imputation for `island` (dropping NAs) and the `cross_val_score` will not work as intended once `X` is an array. The ground truth correctly maintains `X` as a DataFrame through most preprocessing steps, allowing for column-wise operations.
- **NA in `island` after imputation:** The task specifies: ""(Note: if a row still has NA in `island` after mode imputation, e.g. if all were NA, it can be dropped, or a placeholder like 'Unknown' can be used if mode is NA itself. For this task, if mode imputation results in NA for island, drop those rows)"". The generated code attempts `mask = X['island'].isna()`, which is a good step towards handling this, but as stated above, `X` might not be a DataFrame by then. If `SimpleImputer(strategy='most_frequent')` is used, it will almost certainly fill any missing values in `island` with the mode, so `X['island'].isna()` might always return `False`. The specific edge case where *all* 'island' values are NA, leading to `mode()` being empty or `SimpleImputer` potentially not working as expected (though `SimpleImputer` should handle it by setting the fill value to `np.nan` if the strategy can't find a valid mode, which would then be caught by `isna()`), is not explicitly handled as robustly as in the ground truth.

**Code Quality (1/3):**
- **Missing `ColumnTransformer` import:** As noted, this is a critical missing import.
- **Mixing pandas and numpy array operations:** The code shifts `X` from being a DataFrame to a NumPy array by direct assignment from `SimpleImputer.fit_transform`. This makes debugging and subsequent pandas-specific operations (like dropping rows based on a column condition, or then passing the preprocessed array to `cross_val_score`) less robust and more error-prone without careful management of column names and types.
- **Variable shadowing:** `y` is first assigned the series `df['species']` and then overwritten with the label-encoded numpy array `le.fit_transform(y)`. While this is common, it can sometimes make debugging slightly harder if the original `y` (Series) was needed later.

**Completeness (2/3):**
- All steps are *attempted* to be covered (loading, feature/target definition, numerical median imputation, categorical mode imputation, one-hot encoding, scaling, model training, cross-validation).
- The specific detail about dropping rows if `island` remains NA *after* mode imputation (especially the edge case of all NAs) is attempted, but the execution might be flawed due to the DataFrame/array conversion issues.
- The task asked for specific n_estimators and random_state, which are correctly applied.
- The mean cross-validated accuracy is printed.

Overall, the code structure shows an understanding of the required steps, but the implementation details, particularly around managing data types (DataFrame vs. NumPy array) during preprocessing and the critical missing import, prevent it from being a fully functional and robust solution.",grok-3-mini-beta,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Load the dataset
df = pd.read_csv('penguins.csv')

# Select features and species
df_subset = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'species']]

# Drop rows with missing values in the selected features
df_clean = df_subset.dropna(subset=['culmen_length_mm', 'culmen_depth_m...",7,"The generated code correctly performs most of the required steps: loading data, selecting features, dropping NAs, standardizing, applying K-Means (with correct parameters), and applying PCA. It also generates two scatter plots.

However, there are a few areas for improvement, primarily regarding the second plot and general plotting practices:

1.  **Plotting Original Species:** The original `species` column (`df_clean['species']`) is a categorical string column. When `plt.scatter` is given a string array for `c`, it will attempt to assign a unique color to each unique string. While this *might* work in some matplotlib versions, it's not the robust or intended way to color points by discrete categories. Instead, it's better to map categories to numerical values (e.g., using `factorize` or `LabelEncoder`) or, as in the ground truth, rely on libraries like `seaborn` that handle categorical `hue` mapping gracefully. The current implementation for the second plot will likely produce an error or unexpected color mapping if there are many unique string values.

2.  **`n_init` parameter for K-Means:** The generated code uses `kmeans.fit(X_scaled)` and then retrieves `kmeans.labels_`. This is correct. The prompt specified `n_init='auto'` which was included and used. No issues here.

3.  **Visualization Quality:** While `matplotlib` is used, the plots lack legends, which are crucial for interpreting what the colors represent, especially for the K-Means clusters. Also, `seaborn` provides more aesthetically pleasing and often more informative plots with less boilerplate, especially for categorical data. Including `sns.scatterplot` as in the ground truth simplifies adding legends and handling categorical `hue` variables.

4.  **Error Handling (Minor):** The ground truth includes basic error handling for `FileNotFoundError`, which is good practice for production code, though not explicitly required by the prompt's core task.

To achieve a higher score, the primary focus should be on correctly and robustly coloring the second plot by the original `species` labels and improving the overall plot quality (e.g., adding legends).",grok-3-mini-beta,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows where 'island' is NA
df = df.dropna(subset=['island'])

# Impute missing values
num_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in num_cols:
    df[col] = df[col].fillna(df[col].mean())

...",8,"The generated code correctly addresses most of the requirements.

**Strong Points:**
- **Correct Data Loading and Imputation:** Successfully loads the dataset and correctly handles missing values as specified (numerical columns with mean, 'sex' with mode, 'island' NA rows dropped).
- **Correct Feature and Target Definition:** Properly defines `X` by combining numerical features with one-hot encoded 'island' and 'sex' columns, and correctly label encodes 'species' for `y`.
- **Correct Model Training:** Trains a `GradientBoostingClassifier` with the specified parameters.
- **Correct Feature Importance Extraction and Printing:** Extracts feature importances and associates them with the correct feature names from `X.columns`.
- **Correct Box Plot Generation:** Generates box plots for `flipper_length_mm` for each species, grouped by 'sex', with appropriate titles.

**Areas for Improvement (Why not a 10):**
- **Feature Importance Output Formatting:** While it prints the feature importances, the ground truth code formats this into a sorted DataFrame, which is more readable and aligns better with typical data science analysis outputs. The current output is just a raw list of features and importances.
- **Consolidated Plotting:** The ground truth code generates a single, comprehensive box plot using `hue` for 'sex', which is often more efficient and provides a better comparative view than separate plots for each species. While the generated code's approach is not incorrect, the single plot approach is generally preferred for this type of visualization.
- **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is a good practice. The generated code lacks this.
- **Code Style/Readability:** While generally good, slight improvements could be made. For example, using `plt.figure(figsize=...)` for consistency and better plot sizing. The ground truth also explicitly sets `sns.set_theme(style=""whitegrid"")`, which provides a nicer default aesthetic.
- **`pd.get_dummies` `dtype` argument:** The ground truth uses `dtype=int` for `pd.get_dummies` to ensure the dummy variables are integers, which is a minor but good practice.

Overall, it's a very good attempt that fulfills the core requirements effectively. The areas for improvement are mostly around presentation, robustness, and slight refinements in visualization and output formatting.",grok-3-mini-beta,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer

# Load the dataset
df = pd.read_csv('penguins.csv')

# Impute numerical columns using KNNImputer
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
imputer = KNNImputer(n_neighbors=3)
df[numerical_cols] = imputer.fit_transform(df[numerical_cols])

# Impute categorical column 'sex' with the mode
mode_value = df['sex'].mode()[0]
df['sex'].fillna(mode_value, inplace=True)

# Print the numb...",8,"The generated code correctly performs all the core requirements of the task. It successfully loads the data, applies KNN imputation to the specified numerical columns with `n_neighbors=3`, and imputes the 'sex' column with its mode. Finally, it prints the sum of NA values, verifying the imputation.

**Strong Points:**
*   Correctly uses `KNNImputer` with the specified `n_neighbors`.
*   Correctly applies mode imputation to the 'sex' column using `fillna` and `mode()[0]`.
*   Successfully verifies the remaining NA values.
*   Concise and readable code.

**Areas for Improvement (to reach a perfect 10):**
*   **Error Handling (File Not Found):** The ground truth code includes a `try-except` block for `FileNotFoundError` when loading the CSV. This is a robust practice for real-world scenarios. The generated code assumes the file will always be present, which might lead to a `FileNotFoundError` if the file is missing or misspelled.
*   **Print Statements and Verification Clarity:** While the generated code prints `df.isnull().sum()`, the ground truth provides more verbose output, indicating the initial NA count, confirmation of each imputation step, and a clear final verification. This improves user understanding and debugging.
*   **Comments:** The ground truth code includes comments explaining each step, which enhances readability and maintainability, especially for complex data cleaning tasks. The generated code is self-explanatory for this problem, but comments are generally good practice.

Overall, the generated code is highly functional and solves the problem effectively, but it could be more robust and provide better user feedback.",grok-3-mini-beta,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```
import pandas as pd
import scipy.stats as stats

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the dataset to include only 'Adelie' and 'Gentoo' species
filtered_df = df[df['species'].isin(['Adelie', 'Gentoo'])]

# Handle NA values in the culmen_length_mm column by dropping rows with NAs
cleaned_df = filtered_df.dropna(subset=['culmen_length_mm'])

# Separate the data for the two species
adelie_lengths = cleaned_df[cleaned_df['species'] == 'Adelie']['culmen_length_mm']
gentoo...",9,"The generated code correctly performs all the required steps: loading the data, filtering for the specified species, handling NA values, performing the t-test, and interpreting the results based on the p-value and significance level. The code is concise and directly addresses the prompt.

**Strong points:**
*   Correctly loads the CSV and filters the data.
*   Handles NA values appropriately in `culmen_length_mm` before the test.
*   Correctly uses `scipy.stats.ttest_ind` for the independent two-sample t-test.
*   Correctly interprets the p-value against the 0.05 significance level.
*   Clear and functional.

**Areas for minor improvement:**
*   **Error Handling:** It lacks robust error handling for `FileNotFoundError` or other potential issues when loading the CSV, which the ground truth includes.
*   **Modularity/Function:** While not explicitly requested, encapsulating the logic within a function (as in the ground truth) would improve reusability and testability for larger projects.
*   **Comments/Docstrings:** The ground truth includes more comprehensive comments and a docstring, which improves code readability and maintainability, especially for complex tasks.
*   **Clarity of Interpretation:** The interpretation message is slightly less detailed than the ground truth. The ground truth explicitly states ""statistically significant evidence to suggest a difference..."" which is more precise.",grok-3-mini-beta,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('penguins.csv')

# Define features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']]
y = df['species']

# Preprocessing: Impute numerical features with m...",8,"The generated code correctly implements all the specified requirements for building and evaluating a Random Forest classification model.

**Strong Points:**
*   **Correct Logic:** All steps, from data loading to evaluation, are implemented logically and accurately.
*   **Data Loading:** Correctly loads the `penguins.csv` dataset.
*   **Feature/Target Definition:** Correctly identifies and separates features and target.
*   **Preprocessing:**
    *   Numerical features are correctly imputed with the median.
    *   `sex` is correctly imputed with the mode.
    *   `island` and `sex` are correctly one-hot encoded using `pd.get_dummies`.
    *   `species` (target) is correctly label encoded.
*   **Data Splitting:** The data is split into training and testing sets with the correct `test_size`, `random_state`, and includes `stratify=y` for proper stratification.
*   **Model Training:** A `RandomForestClassifier` is initialized and trained with the specified `n_estimators` and `random_state`.
*   **Prediction and Evaluation:** Predictions are made, and both accuracy score and classification report are printed as required.

**Areas for Improvement (to reach a perfect 10):**
*   **Pandas SettingWithCopyWarning:** The in-place `fillna` operations (`X[col].fillna(X[col].median(), inplace=True)`) might trigger a `SettingWithCopyWarning` in pandas. While it often works, it's generally safer and more explicit to use `.loc` for assignment after selecting columns or explicitly create a copy if modification is intended, or use `ColumnTransformer` and `Pipeline` as shown in the ground truth for robust preprocessing.
*   **Robustness of Imputation/Encoding:** For real-world scenarios, using `sklearn.impute.SimpleImputer` and `sklearn.preprocessing.OneHotEncoder` within a `ColumnTransformer` and `Pipeline` (as demonstrated in the ground truth) is more robust and standard practice. It ensures that the *same* imputation strategy (median, mode) and *same* one-hot encoding categories learned from the training data are applied consistently to the test data and new unseen data without data leakage. The current approach with `pd.fillna` and `pd.get_dummies` is less streamlined for applying transformations to new data or within a machine learning pipeline. The ground truth's use of `ColumnTransformer` and `Pipeline` is generally preferred for building reproducible and robust ML workflows.
*   **Classification Report `target_names`:** While the classification report works, providing `target_names` (from the `LabelEncoder`'s `classes_` attribute) to `classification_report` would make the output more readable by showing actual species names rather than just `0`, `1`, `2`.

Overall, the code is very good and directly addresses the prompt's requirements. The suggested improvements are primarily about adopting more robust and conventional `sklearn` pipeline practices for production-ready code, which wasn't strictly enforced by the prompt but is the standard for good ML engineering.",grok-3-mini-beta,LLM_judge_config
H49,hard,Statistical Analysis,,"```
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Calculate Pearson correlation coefficients
corr_years = df['P...",9,"The generated code correctly defines the DataFrame, calculates the Pearson correlation coefficients, prints them, and correctly determines which feature has a stronger absolute correlation. The code is clean and directly addresses all requirements.

One minor improvement, though not strictly incorrect without it, would be to explicitly specify `method='pearson'` in the `.corr()` calls for clarity, even though 'pearson' is the default. Also, the print statements for the correlation coefficients could benefit from formatting to a specific number of decimal places for better readability, similar to the ground truth. These are minor stylistic choices rather than functional issues.

Strong points:
- Correct DataFrame creation.
- Accurate correlation calculation.
- Correctly identifies the stronger absolute correlation using `abs()`.
- Clear and concise code.",grok-3-mini-beta,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import lifelines as ll
import scipy as sp
import statsmodels.api as sm
import pymc3 as pm
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors

# Load dataset
df = pd.read_csv('titanic.csv')

# Preprocess data
df = df.dropna(subset=['Survived', 'Pclass', 'Sex', 'Age'])  # Handle missing values
df = pd.get_dummies(df, columns=['Sex', 'Pclass'], drop_first=True)
df['event'...",4,"The generated code attempts to address many aspects of the prompt but fails significantly in correctness, completeness, and statistical rigor.

**Major Issues:**

1.  **Incorrect Kaplan-Meier and CoxPH Setup:** The prompt explicitly states to treat the voyage as ""time-to-event data (with survival as event indicator)"". The ground truth code interprets this as a fixed time `T=1` for all, and the event is `death (Survived=0)`. The generated code sets `duration=1` and `event=1-df['Survived']`, which is a correct interpretation of the event, but the `duration=1` makes the Kaplan-Meier and Cox models effectively equivalent to logistic regression, which seems to be the intended workaround. However, the KM plot `kmf.plot()` will not show meaningful Kaplan-Meier curves for a fixed duration of 1, as there's only one point. The generated code also tries to iterate through `df['Pclass_2'].unique()` for KM plots, which is incorrect; it should directly use categories like Pclass (original, not dummified) or Sex.

2.  **Severe Data Preprocessing Errors:**
    *   `df.dropna(subset=['Survived', 'Pclass', 'Sex', 'Age'])`: This drops a significant portion of the dataset (e.g., 177 rows for Age). Median/mode imputation is generally preferred for this kind of data to retain more observations.
    *   One-hot encoding `Pclass` and then trying to use `Pclass_2` in the KM loop and `original_Pclass` (which doesn't exist) in PSM. This indicates a lack of consistency and incorrect handling of categorical variables.
    *   `df['original_Pclass']` is used in PSM without being defined or ensured to exist, leading to a `KeyError`.

3.  **Flawed Propensity Score Matching Implementation:**
    *   The matching logic `matched_indices = indices[df['treatment'] == 1]` and `matched_df = df.iloc[matched_indices.flatten()]` is severely oversimplified and incorrect. It matches treated individuals to arbitrary controls, not controls that have been matched to treated based on propensity scores. It doesn't perform 1-to-1 matching or ensure balanced groups.
    *   It lacks common PSM steps like checking covariate balance before and after matching (e.g., using SMD).
    *   No analysis of the treatment effect on the outcome (survival) after PSM is performed.

4.  **Incomplete and Incorrect Bootstrap Resampling:**
    *   The `bootstrap_cox_coefficient` function only extracts the first coefficient (`.values[0]`) which is insufficient for general coefficient bootstrapping.
    *   It doesn't provide confidence intervals for all coefficients or other critical metrics like the C-index.

5.  **Partial and Unrobust Bayesian Analysis:**
    *   The Bayesian model uses `df['Sex_male']` and `df['Age']` directly without standardization. While not strictly ""wrong"" for PyMC3, it's generally good practice and often improves sampling efficiency.
    *   The model structure for logistic regression (predicting `Survived` directly) is acceptable, but the prompt requested ""probabilistic modeling of survival factors"" using Bayesian methods typically implies Bayesian Cox Proportional Hazards or a similar survival model, which is much more complex and usually not done with `pymc3` directly for continuous time as `lifelines` handles it. Given the `duration=1` setup, this effectively becomes a Bayesian logistic regression, which is okay as an interpretation if the `duration=1` setup was intentional for all survival analyses.
    *   `pm.traceplot` requires `arviz` which is not imported; `return_inferencedata=False` is deprecated. Best practice is to use `arviz.plot_trace`.

6.  **Missing/Vague Model Validation and Sensitivity Analysis:**
    *   ""Assumptions: Already checked via cph.check_assumptions()"": This is true but brief. The ground truth provides more context, especially for the `duration=1` case.
    *   ""Sensitivity: Refit Cox model with subsets (e.g., exclude Age > 50)"": This is a very rudimentary form of sensitivity analysis. More comprehensive sensitivity analysis (e.g., imputation strategy, prior sensitivity for Bayesian model) is missing.

7.  **Unstructured Output:** The generated code lacks print statements or structured output to explain the different sections, making it hard to follow the analysis flow.

Overall, the generated code demonstrates some familiarity with the libraries but lacks the depth, correctness, and statistical rigor required for ""advanced statistical modeling and survival analysis."" Many components are either incorrectly implemented, missing crucial parts, or rely on undefined variables.",grok-3-mini-beta,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Create the DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89...",8,"The generated code correctly performs all the required tasks: it creates the DataFrame, calculates and prints the Pearson correlation coefficient, generates a scatter plot colored by subject with appropriate labels and legend, adds the 'Overall_Grade' column, and prints the count of students in each grade category.

**Strong points:**
- Correctly implemented DataFrame creation.
- Accurately calculates and prints the Pearson correlation coefficient.
- The scatter plot is correctly generated, with points colored by subject and a legend, title, and axis labels.
- The grading logic for 'Overall_Grade' is correct.
- The grade distribution is correctly calculated and printed.

**Areas for improvement:**
- The scatter plot uses a manual loop to plot each subject, which is less efficient and flexible than using `seaborn.scatterplot` with a `hue` parameter, as shown in the ground truth. While functional, it's not the most idiomatic or aesthetically pleasing approach for this specific problem in Python's data visualization ecosystem.
- The output messages are minimal compared to the ground truth, which includes descriptive print statements for each step, improving readability and understanding of the code's execution flow.
- The import for `seaborn` was suggested in the prompt for potentially better aesthetics but was not included in the generated code, leading to the manual loop for coloring. Although not strictly required, it's a best practice for such plots.
- There's no specific import for `numpy` in the generated code, which is usually part of standard data science setups, though it wasn't explicitly used for the grading logic in the generated code (unlike the `np.select` used in the ground truth). In this case, it was not strictly necessary for the generated code's implementation.",grok-3-mini-beta,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame and set 'Date' as index
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']),
    'Ticker': ['TECHCO']*10,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0],
    'Low': [14...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It successfully creates the DataFrame, calculates daily returns and both SMAs, generates a well-labeled plot, and correctly identifies the bullish crossover dates.

Strong points:
- All calculations (Daily Return, SMA_3Day, SMA_5Day) are correct.
- The plot includes all required lines ('Close', 'SMA_3Day', 'SMA_5Day'), has the correct title, legend, and axis labels.
- The identification of bullish crossovers (`df['SMA_3Day_above_SMA_5Day'] = df['SMA_3Day'] > df['SMA_5Day']` and `crossover_dates = df[(df['SMA_3Day_above_SMA_5Day']) & (~df['SMA_3Day_above_SMA_5Day'].shift(1))].index`) is a standard and effective way to detect such events.

Minor areas for improvement (to achieve a perfect 10):
- While the crossover logic is sound, the output format for the crossover dates (`print(""Dates where SMA_3Day crossed above SMA_5Day:"", crossover_dates)`) prints the dates as a pandas DatetimeIndex. Formatting it to a more human-readable string (e.g., `date.strftime('%Y-%m-%d')` as in the ground truth) would be a small enhancement for user readability.
- The plot could benefit from setting the `figsize` explicitly for better visual presentation, though it's not a critical error.
- Adding `plt.grid(True)` for better readability of the plot, as included in the ground truth, would be a good practice.",grok-3-mini-beta,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001',...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It successfully creates the DataFrame, calculates total sales revenue per product, generates and displays the horizontal bar chart, calculates the average order value, and identifies repeat customers. The imports are correct, and the code structure is logical.

One minor area for improvement is in the plotting. While `plt.barh` works, using the pandas `.plot(kind='barh')` method on the grouped series (as in the ground truth) often integrates better with pandas data structures and can sometimes simplify plotting operations for aggregated data. Additionally, the ground truth includes some formatting improvements for the plot (like `figsize`, `grid`, `tight_layout`), which makes the visualization more professional and readable, but these were not explicitly requested.

Another minor point for improvement is how the average order value and repeat customers are printed. The ground truth provides more descriptive print statements that enhance readability and clarity for the user, especially when presenting the results of an analysis. For instance, clearly labeling ""Average Order Value:"" and then showing the formatted number, and for repeat customers, providing a header for the output table.

Overall, the code is very good and fully functional for the given task. The suggested improvements are mainly around presentation and minor best practices rather than correctness or functionality.",grok-3-mini-beta,LLM_judge_config
H53,hard,Feature Engineering,,"```
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Create DeviceType_Numeric
df['DeviceType_Numeric'] = df['DeviceType'].map({""Desktop"": 0, ""Mobile"": 1})

# Create PagesPerMinute and...",9,"The generated code correctly defines the initial DataFrame and performs all the required data preparation steps. It successfully creates the `DeviceType_Numeric` feature using the specified mapping and calculates `PagesPerMinute`. The division-by-zero handling for `PagesPerMinute` is mostly correct, although its order of operations (`fillna(0).replace([float('inf'), float('-inf')], 0)`) is less conventional than the ground truth's approach of replacing infinities first, then filling NaNs. If an intermediate division resulted in `NaN` (e.g., 0/0), `fillna(0)` would handle it. However, if it resulted in `inf` or `-inf`, those would still be present *before* the `.replace()` call. This could lead to a `NaN` from say, 0/0, being converted to 0, which is correct, but then an `inf` from 1/0, also being converted to 0, which is also correct. While the order is slightly different, the end result for `PagesPerMinute` is correct based on the task's requirements. The feature selection and printing of the head are also correctly implemented. The only minor point is the slightly less robust (though still functional in this case) handling of NaNs/Infs compared to explicitly handling NaNs and Infs separately or using `np.inf` requires importing `numpy`. Despite this minor point, the code is very good.",grok-3-mini-beta,LLM_judge_config
H54,hard,Anomaly Detection,,"```
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26, 45, 28]
}
df = pd.DataFrame(data)

mean_...",9,"The generated code correctly defines the DataFrame, calculates the mean and standard deviation, identifies outliers based on the 1.5 standard deviation threshold, and prints the required information for each outlier. This addresses all core requirements of the task.

**Strong points:**
*   Correct DataFrame creation.
*   Accurate calculation of mean and standard deviation.
*   Correct application of the 1.5 STD threshold for outlier detection, considering both upper and lower bounds.
*   Correctly iterates through identified outliers and prints the specified details (SensorID, Timestamp, TemperatureCelsius).

**Areas for minor improvement (to achieve a perfect 10):**
*   The `Timestamp` column is not converted to datetime objects. While not strictly necessary for the exact outlier detection logic in this specific problem, it's good practice for time-series data and was included in the ground truth. This can affect how the `Timestamp` is displayed if the task asked for specific formatting (e.g., `strftime`).
*   The output format for the outliers could be slightly more descriptive, similar to the ground truth which adds context like ""(Outlier)"" and formats the temperature to one decimal place.
*   It does not include a message for when no outliers are found, unlike the ground truth which provides user feedback in that scenario. This is a minor omission but enhances user experience.

Overall, it's a very good solution that fulfills the core requirements.",grok-3-mini-beta,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [10, 5, 25, 15, 8, 30, 9,...",9,"The generated code correctly defines the DataFrame, calculates the EngagementScore, and accurately determines and prints the PostType with the highest average EngagementScore for each platform. The code is well-structured and uses appropriate pandas operations. It successfully addresses all requirements of the task.

One minor area for improvement, though not critical, is the final iteration logic. The ground truth leverages pandas' vectorized operations and sorting to achieve the final result more concisely before iterating for printing. While the generated code's loop works perfectly and clearly, for very large datasets, the ground truth's approach using `sort_values` and `drop_duplicates` might offer a slight performance edge over iterating through unique platforms and then filtering within the loop, although for this dataset size, the difference is negligible. The use of `.loc[platform_data['EngagementScore'].idxmax()]` is a perfectly valid and clear way to find the maximum row within a subgroup.

Overall, it's a very good solution.",grok-3-mini-beta,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""C...",10,"The generated code is correct, complete, and of good quality. It precisely addresses all parts of the task: defining the DataFrame, calculating 'LengthOfStay', computing average stay and patient count per diagnosis, printing the summary information, and identifying the diagnosis with the longest average stay. The use of `groupby().agg()` is efficient and standard for this type of operation. The printing format is clear, although slightly less formatted than the ground truth in the table, it still conveys the information effectively. The `idxmax()` and `max()` methods are correctly used to find the desired diagnosis.",grok-3-mini-beta,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",10,"The generated code is highly correct and complete, addressing all requirements of the task.

**Strong points:**
*   **Correct DataFrame Definition:** The DataFrame is correctly defined and populated as specified.
*   **Accurate WeightedRating Calculation:** The `WeightedRating` is calculated precisely using the provided formula and added as a new column. The use of vectorized operations (`df['Rating'] * df['Votes']`) is efficient and idiomatic pandas.
*   **Correct Identification of Max WeightedRating:** The code correctly identifies the movie with the highest `WeightedRating` using `idxmax()`.
*   **Clear Output:** The final print statement provides the `Title` and `WeightedRating` of the identified movie as requested.
*   **Conciseness and Readability:** The code is clean, concise, and easy to understand.

**Minor observation (not a defect):**
The ground truth code includes error handling for `total_votes == 0`, which is good practice. However, given the specific dataset provided in the task description (where `total_votes` will never be zero), the generated code's omission of this check does not constitute a failure for *this specific task*. For a more robust general-purpose solution, handling potential division by zero would be an enhancement, but it's not a requirement for the given prompt.",grok-3-mini-beta,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 10000, 8000]
}
df = pd...",9,"The generated code correctly defines the DataFrame and calculates the average `PriceUSD` and `Mileage` per `Brand`. It also correctly creates a new DataFrame with these averages and prints it.

The solution is concise and uses a common pandas chaining method (`groupby().mean().reset_index()`) which is idiomatic for this type of aggregation.

The only minor area for improvement, preventing a perfect 10, is that the column names in the grouped DataFrame are `PriceUSD` and `Mileage`, whereas the ground truth used descriptive names like `Average_PriceUSD` and `Average_Mileage`. While not explicitly requested to rename, using more descriptive names for aggregated columns is often good practice for clarity. However, the request was to calculate averages, which it does, and to show *these averages* in a new DataFrame, which it also does accurately.

Overall, it's a highly correct and efficient solution.",grok-3-mini-beta,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63, 83],
    ""Defense""...",10,"The generated code correctly defines the DataFrame, accurately calculates the ""TotalStats"" column, identifies the Pokemon with the highest ""TotalStats"" by name, and correctly calculates the average speed for each ""Type1"". All requirements of the task are met with correct and efficient pandas operations. The code is concise and clear. There are no unnecessary imports or complex structures. The solution directly addresses all parts of the prompt.",grok-3-mini-beta,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap.umap_ as umap
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, GaussianMixture
from sklearn.metrics impo...",6,"The generated code provides a reasonable attempt at addressing the prompt's requirements, but it falls short in several key areas, particularly in advanced feature engineering, comprehensive cluster validation, and detailed cluster analysis.

Here's a breakdown:

**Strengths:**

*   **Proper Imports and Structure:** The imports are mostly correct, and the code follows a logical flow from data loading to analysis.
*   **Multiple Clustering Algorithms:** K-Means, Hierarchical, DBSCAN, and GMM are all used as requested.
*   **Dimensionality Reduction:** PCA, t-SNE, and UMAP are correctly implemented for visualization purposes.
*   **Basic Cluster Validation:** It includes elbow method, silhouette score, and Calinski-Harabasz score, which are good starting points.
*   **Cluster Stability Analysis:** A basic `stability_analysis` function using ARI is present.
*   **Some Visualizations:** It attempts to visualize cluster distributions on reduced dimensions and survival rates/fare distributions.

**Weaknesses and Areas for Improvement:**

1.  **Limited Advanced Feature Engineering (Critical):**
    *   **Social Network Analysis:** The prompt asked for ""social network analysis (family groups, ticket sharing)"". While `FamilySize` and `TicketShare` (which is `TicketGroupSize` in the ground truth) are present, it misses `FarePerPerson`. More importantly, it doesn't try to link passengers by actual family (e.g., using `Parch` + `SibSp` in combination with names for more sophisticated family grouping, or even shared cabins).
    *   **Socioeconomic Indicators:** Extracts `Title`, `FamilySize`, and `Deck` but the ground truth goes further with `IsAlone`, handling rare titles more robustly, and `TicketPrefix`.
    *   **Text-based features:** Only `Title` is extracted, but the ground truth includes `NameLength` and `TicketPrefix` which are more sophisticated text features.
    *   **Interaction Features:** The ground truth created `Age_Class`, `Fare_Class`, `FarePerPerson_Class` which are completely missing here.
    *   **Cabin Deck & HasCabin:** The generated code only uses `Cabin`.str[0], which extracts the first letter, but doesn't explicitly create a `HasCabin` indicator, nor does it handle the 'U' for unknown which the ground truth does for consistent categorical encoding.

2.  **Incomplete Preprocessing:**
    *   **`handle_unknown='ignore'` for OneHotEncoder:** This is good, but the input data for `OneHotEncoder` needs proper handling of `NaN`s, especially for Embarked (which is in `categorical_features`). The `SimpleImputer` for categorical features correctly addresses this for `Embarked` and `Deck`.
    *   **Hardcoding `features`:** The `features` list is manually curated and doesn't include `SibSp` and `Parch` themselves, even though `FamilySize` is derived from them. The ground truth keeps them as separate numerical features, which provides more data points for clustering.
    *   **Missing `IsAlone` and `HasCabin` during preprocessing:** These important engineered features are not included in the `features` list for processing.

3.  **Suboptimal Cluster Validation & Selection:**
    *   **Missing Metrics:** The ground truth includes `Davies-Bouldin Index` and `Adjusted Rand Score` for validation, and critically, `Gap Statistic` was requested but not implemented by either code. The generated code only uses `silhouette_score` and `calinski_harabasz_score` (and inertia), which is less comprehensive than requested.
    *   **Lack of GMM/Hierarchical/DBSCAN specific validation loops:** The `evaluate_clusters` function only considers K-Means for optimal `k`, ignoring other algorithms, making the `optimal_k` selection potentially biased. The ground truth evaluates all three (K-means, Hierarchical, GMM) across multiple metrics.
    *   **Simplified `optimal_k` selection:** `max(cluster_scores, key=lambda x: cluster_scores[x]['Silhouette'])` is a very rudimentary way to select optimal k, which often isn't the best practice. A more robust approach would involve visual inspection of multiple metrics or using a more sophisticated aggregation if a single `k` is needed across algorithms.

4.  **Inadequate Cluster Stability & Consensus Clustering:**
    *   **Limited Stability Analysis:** The generated code only performs stability for K-Means and simply prints the mean ARI. The ground truth provides more detailed ARI scores, mean, and std, and interprets the result.
    *   **Consensus Clustering (Conceptual vs. Implementation):** The consensus matrix calculation is technically correct but the output (a large matrix) is not directly used or interpreted for cluster assignment in the typical sense of consensus clustering (which usually leads to a single, more robust partitioning). The ground truth simplifies this to illustrate a stability metric (ARI) rather than implementing a full consensus clustering algorithm for final assignments, which was likely more realistic for the scope of the prompt.

5.  **Lack of Comprehensive Cluster Analysis & Visualizations:**
    *   **DBSCAN Handling:** DBSCAN generates a `-1` label for noise, which needs careful handling in analysis and visualization (e.g., excluding noise points, or treating them as a separate category). The generated code performs `fit(X_pca)` then `df[...] = dbscan.labels_` directly, which won't account for `-1` when analyzing. The ground truth explicitly filters these.
    *   **Feature Importance:** The PCA components as feature importance is a good start, but it's not directly for *each segment* as requested. The ground truth uses individual feature distributions (violin plots) and a heatmap of scaled feature means for *each* cluster, which is more directly relevant to feature importance per segment.
    *   **Missing Dendrogram:** Hierarchical clustering was performed, but no dendrogram was plotted, which is standard for visualizing its structure.
    *   **`plot_cluster_distributions` Limitation:** This function is only called for K-Means, not for other algorithms.
    *   **Survival Rate Visualisation:** The `plot_cluster_distributions` combines scatter plots with survival rates, which is less ideal than separate, dedicated plots for clarity, as done in the ground truth.

6.  **Minor Issues:**
    *   Using `GaussianMixture` as a clustering class directly in `sklearn.cluster` is a bit of a misnomer; it's `sklearn.mixture.GaussianMixture`. The generated code gets it right in the import, but then lists it under `sklearn.cluster` in the prompt's implied structure.
    *   The `umap` import `umap.umap_ as umap` is incorrect if the library name is `umap-learn`. The correct import should be just `import umap`. (The ground truth uses outdated syntax for `umap-learn` as well, but reflects common past usage; `import umap.UMAP` for the class is the modern way.)

Overall, the generated code provides a foundational structure but misses many of the ""advanced"" and ""comprehensive"" details requested in the prompt, especially in feature engineering, detailed validation, and actionable cluster analysis.",grok-3-mini-beta,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Define the data
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dystopian...",10,"The generated code is correct, complete, and of good quality.

**Strong points:**
*   **Correctness:** It accurately defines the DataFrame as specified, correctly calculates the count of books and average price per author, and correctly identifies the author with the most books in stock.
*   **Completeness:** All three requirements of the prompt are met:
    1.  Count of books and average price per author.
    2.  Printing this information.
    3.  Identifying and printing the author with the most total stock.
*   **Efficiency:** Using `groupby().agg()` to perform multiple aggregations at once (`count`, `mean`, `sum`) is efficient and idiomatic pandas.
*   **Clarity:** The variable names are descriptive (`aggregated_df`, `author_with_most_stock`).
*   **Readability:** The f-strings used for printing make the output clear and easy to understand.
*   **Formatting:** The average price is formatted to two decimal places, which is appropriate for currency.

**Minor observations (not deductions):**
*   The ground truth provides a bit more verbosity in comments and print statements (e.g., separators), but the generated code is perfectly functional and clear without it.
*   The ground truth separated the final printing of statistics into two steps (first the per-author stats, then the max stock author), while the generated code performed all aggregations in one go and then printed. Both approaches are valid. The generated code's aggregation strategy is arguably more concise.",grok-3-mini-beta,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, learning_curve, validation_curve
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neural_network imp...",4,"The generated code attempts to address many requirements but falls short in implementation correctness, completeness, and adherence to advanced methodologies specified in the prompt.

**Key areas of failure/weakness:**

1.  **Data Preprocessing:**
    *   **Multiple Imputation:** The prompt asked for ""multiple imputation strategies."" The generated code creates two imputed dataframes (`df_imputed_mean`, `df_imputed_knn`) but then *only* uses `df_imputed_knn` for subsequent steps. This defeats the purpose of trying multiple imputation strategies if only one is ultimately used. The ground truth, in contrast, uses `IterativeImputer` (MICE), which is a more advanced single multiple imputation technique.
    *   **Outlier Treatment:** The generated code uses naive IQR-based capping which replaces outliers with the median. This is a very basic approach. The ground truth uses Winsorization (clipping to quantiles), which is generally more robust, and applies it *after* imputation as part of a pipeline.
    *   **Feature Engineering:** The generated code only uses `PolynomialFeatures(interaction_only=True)`. The prompt specifically asked for ""polynomial and interaction features."" The ground truth uses `interaction_only=False` which generates both. Also, the feature engineering is done *before* scaling and then duplicated (scaling and poly features are re-applied to the *entire* `df_processed` which includes the target before splitting, which is data leakage for feature engineering). The ground truth correctly applies this within a `Pipeline` after imputation and standard scaling, using `ColumnTransformer` to ensure it's applied correctly before training.
    *   **Preprocessing Pipeline:** The generated code lacks a comprehensive preprocessing pipeline (e.g., using `sklearn.pipeline.Pipeline` with `ColumnTransformer`) that correctly handles the order of operations (imputation -> outlier -> scaling -> feature engineering) and applies transformation consistently to both training and test data without leakage. The generated code performs these steps outside a structured pipeline, leading to potential issues and less robust code.

2.  **Model Optimization:**
    *   The generated code uses `scoring='accuracy'` for `GridSearchCV`. The ground truth uses `'roc_auc'`, which is generally more appropriate for imbalanced classification problems like potability prediction and was explicitly requested by the prompt (""ROC AUC as primary scoring metric"").
    *   The meta-learner for stacking is `LogisticRegression` without specific optimization.

3.  **Ensemble Methods:**
    *   **Blending:** The blending implementation is done manually but isn't as robust as it could be. It also doesn't provide a callable 'blending' model that can be easily evaluated or used in the same way as other models. The ground truth provides a more structured approach to how blending is performed and its output.

4.  **Advanced Evaluation:**
    *   **Evaluation Metrics:** The `evaluate_model` function in the generated code only returns mean and std of accuracy. The prompt explicitly requested ""comprehensive evaluation metrics including ROC AUC, Brier Score"". The generated code later prints `classification_report` but doesn't centrally capture all required metrics for all models in a structured way for comparison.
    *   **Learning/Validation Curves:** While implemented, the `plot_learning_curve` and `plot_validation_curve` functions are basic and don't include standard deviations or fill regions for better visualization.
    *   **Bootstrap Sampling:** The `bootstrap_ci` function calculates CI for accuracy, but the prompt implicitly requested CIs for performance metrics (e.g., ROC AUC, F1-Score). The ground truth correctly calculates CIs for ROC AUC and F1-Score. Also, `X_train` and `y_train` are used for bootstrap CI for base models, when it should typically be done on the test set for generalization.
    *   **Stratified K-Fold:** While `StratifiedKFold` is used in GridSearchCV, the `evaluate_model` function uses it directly on the full `X`, `y` without a clear train/test split context for the final evaluation, which makes the interpretation unclear.

5.  **Model Interpretability (SHAP):**
    *   The generated code initializes `shap.TreeExplainer` with `best_rf` but then uses `shap_values = explainer.shap_values(X_train)` and displays `shap.summary_plot(shap_values, X_train)`. SHAP values are typically computed on the test set to understand interpretations on unseen data, and the ground truth correctly uses `X_test_processed_df`.
    *   There's no mention of LIME, which was also requested. The ground truth focused on SHAP but adequately covered it.

6.  **Statistical Significance Testing:**
    *   The generated code attempts a paired t-test but computes it on `accuracy_score(..., normalize=False)`. This is incorrect. Paired t-tests for model comparisons typically refer to the scores obtained across *different folds* of cross-validation for the models. The generated code has `ttest_rel(accuracy_score(y_test, y_pred_rf, normalize=False), accuracy_score(y_test, y_pred_gb, normalize=False))`, which is a single pair of accuracy scores on the *test set* and won't yield meaningful statistical significance between models. The ground truth correctly stores CV scores for each fold and performs paired t-tests on these scores.

7.  **Model Comparison Dashboard:**
    *   The `model_dashboard` function prints `classification_report` and shows confusion matrices, but it doesn't create a ""comprehensive model comparison dashboard with performance metrics, feature importance, and prediction confidence intervals"" as required. It lacks a consolidated table or graphical comparison for all models and all metrics including CIs. The feature importance shown is only SHAP for one model, not a general comparison.

8.  **Model Calibration:**
    *   Implemented correctly for one model, but the comprehensive plots and Brier scores expected are not as detailed or comparative as in the ground truth.

**Overall:** The code demonstrates an understanding of the components requested, but the implementation often misses the ""advanced"" aspect, suffers from correctness issues (e.g., statistical testing, data leakage in preprocessing steps), and lacks the robustness and comprehensive output specified in the prompt. Many sections are rudimentary or implicitly incorrect, leading to a low score.",grok-3-mini-beta,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import KNNImputer, IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler
fro...",6,"The generated code attempts to address all requirements of the task but falls short on several key aspects including correctness, completeness, and adherence to advanced statistical practices.

**Strengths:**
*   Includes most required imports.
*   Attempts all specified sections: imputation, modeling, hypothesis testing, anomaly detection, dimensionality reduction, robust methods, diagnostics, and visualizations.
*   Correctly uses `KNNImputer`, `IterativeImputer`, `LogisticRegression`, `PCA`, `FactorAnalysis`, `IsolationForest`, `LocalOutlierFactor`, `OneClassSVM`, `StandardScaler`.
*   Includes `pymc3` for Bayesian modeling, though the implementation has issues.
*   Attempts `MANOVA`, permutation tests, and bootstrap CIs.

**Weaknesses and Areas for Improvement:**

1.  **Missing Data Imputation Comparison (Completeness & Correctness):**
    *   The prompt specifically asked to compare the *effectiveness* of imputation techniques. The generated code compares them based on a `LogisticRegression` *accuracy* score on a test set, which is a reasonable approach for practical comparison. However, the ground truth offers a more direct comparison of *descriptive statistics* after imputation, which is useful for understanding how each method alters the data distribution.
    *   The ""MICE-like using IterativeImputer with BayesianRidge (approximation)"" comment is misleading. While `IterativeImputer` with its default `BayesianRidgeRegressor` *is* a MICE-like approach, the code then changes the estimator to `sm.OLS()`. `sm.OLS()` is not available directly within `IterativeImputer`'s `estimator` parameter, leading to an error (`TypeError: OLS() missing 1 required positional argument: 'data'`). This makes the MICE-like part of the imputation incorrect and non-functional as written.
    *   The `impute_and_compare` function splits `X` and `y` *before* imputation, which is a common practice for modeling, but then performs imputation *only* on `X_train` and `X_test` for `X` columns, leaving `Potability` untouched. The `df` itself is not fully imputed across *all* columns in a unified way for subsequent steps, which makes the downstream reliance on `df` problematic if `Potability` or other `y` related columns had missing values. The ground truth imputes the *entire dataset* and then splits, or at least handles the target variable explicitly. The task implies a full imputation of the *dataset* first.

2.  **Multivariate Statistical Models (Completeness & Correctness):**
    *   **Logistic Regression with Regularization:** Only shows a `sm.Logit` example with an `alpha` for regularization, but doesn't clearly state or demonstrate L1/L2 or compare them as explicitly as requested. `fit_regularized` is correct, but the prompt asks for comparison of L1/L2.
    *   **GAM:** The ground truth uses `pygam` which is the standard dedicated library for GAMs. The generated code attempts to approximate GAM using `sm.GLM` and polynomial features (`ph_sq`), which is not a true GAM and is a weak approximation. It misses the core purpose of GAMs to model non-linear relationships using smooth functions without pre-specifying polynomial degrees.
    *   **Bayesian Logistic Regression (PyMC3):**
        *   The `pm.math.dot(X_train_imp, beta)` is problematic if `X_train_imp` is still a numpy array from `impute_and_compare` that does not account for the intercept, or if the `y_train` was not correctly aligned. The intercept `alpha` is missing from the linear model definition.
        *   The `trace` is printed without being stored correctly (`pymc3`'s `summary` function often takes `trace` directly).
        *   No sampling from the posterior predictive is performed, nor is there any evaluation on the test set, making the Bayesian model incomplete in terms of demonstrating its use.

3.  **Hypothesis Testing (Completeness & Correctness):**
    *   **MANOVA:** The formula `ph + Hardness + Solids ~ Potability` is used. While technically correct for MANOVA, it only uses 3 features, not all features that could be dependent variables as implied by ""multiple dependent variables simultaneously"" in the prompt. The ground truth used all features.
    *   **Permutation Test:** The implementation is acceptable for a single variable.
    *   **Bootstrap CI:** Correctly implemented.

4.  **Robust Statistical Methods (Completeness):**
    *   Uses `RobustScaler` for scaling, but then incorrectly uses `sm.RLM(y, sm.add_constant(X_robust))`. `sm.RLM` expects `exog` (X) to be a DataFrame (or similar) but `X_robust` is a numpy array. It also applies robust scaling to the *entire* dataset for `y` (Potability which is binary), and then uses it in for `RLM` which is generally for continuous outcomes. The prompt asks to handle outliers and non-normal distributions, and `RLM` is suitable for regression when dealing with outliers, but using `y` (Potability) as the dependent variable for `RLM` (a linear model) is not appropriate for binary classification. The ground truth correctly demonstrates `RLM` on a continuous variable.

5.  **Model Diagnostics (Completeness & Correctness):**
    *   **Residual Analysis:** A Q-Q plot of *residuals* is created for `model_log` (Logistic Regression), which is generally not appropriate as residuals from a binary logistic regression are not expected to be normally distributed. This is a common misunderstanding. Deviance residuals are a better choice for GLMs, which the ground truth correctly uses.
    *   **Influence Measures:** `plot_leverage_resid2` is called without linking it to the model and residuals, causing an error if not handled carefully (`TypeError: plot_leverage_resid2() missing 1 required positional argument: 'result_obj'`). The ground truth correctly passes the `glm_results` object.
    *   **Goodness-of-Fit:** Only `prsquared` is mentioned, which is a pseudo R-squared. Hosmer-Lemeshow or Deviance/AIC/BIC (which are printed in summary, but not explicitly stated as GOF tests) are more standard.

6.  **Advanced Statistical Visualizations (Completeness & Quality):**
    *   The ""dashboard"" is very rudimentary. The plots for Q-Q and Leverage are called separately before the dashboard, which is redundant.
    *   The `leverage_plot = plot_leverage_resid2(model_log)` assignment is incorrect, as it doesn't return the plot directly. It returns the matplotlib axes object.
    *   The `Anomalies Dashboard` trying to boxplot the entire `df` on a single axis without specifying the orientation or which variables to plot makes it a generic boxplot, not specific to anomalies in a dashboard context.

**Code Quality/Structure:**
*   Lack of comments and print statements makes the code harder to follow and understand, especially given the complexity of the requirements.
*   Many sections are implemented without checking for `fit` errors or ensuring data types are correct for the statistical methods (e.g., `X_robust` in `RLM`).
*   The `impute_and_compare` function returns `X_train_mice` and `X_test_mice` as numpy arrays, which then need to be converted back to DataFrames for `sm.add_constant` or feature naming in some downstream models, adding extra friction.

In summary, the generated code demonstrates a general understanding of the tools but struggles with the nuanced application and integration required for an ""expert"" level solution, particularly in statistical correctness and comprehensive implementation of the diagnostics and advanced modeling techniques.",grok-3-mini-beta,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import umap
from scipy.cluster.hierarchy import linkage, dendrogram
import matplotlib.pyplot as pl...",6,"The generated code makes a good attempt at addressing most of the requirements but has significant shortcomings in implementing the ""advanced"" aspects and completeness as requested by the prompt.

**Correctness (6/10):**
*   **Missing Value Handling:** `df.dropna()` is a very simplistic way to handle missing values and often detrimental for datasets with sporadic missingness. The ground truth used `SimpleImputer` which is more robust.
*   **Feature Engineering:** The engineered features are basic and do not fully align with ""advanced"" water quality indices or contamination risk scores. The WQI is a simple sum, not a true index, and risk scores are just binary flags. The ground truth provides more nuanced calculations.
*   **Optimal K Determination:** The functions for calculating `gap_statistic`, `db_index`, and `ch_index` are correctly defined, but their calls (`optimal_k = np.argmax(sil_scores) + 2`) for setting `optimal_k` are only based on silhouette and don't integrate the results of all metrics or provide plots for visual inspection to determine optimal K, which was a specific requirement.
*   **DBSCAN Parameters:** `DBSCAN(eps=0.5, min_samples=5)` uses fixed parameters, which is usually not optimal. The ground truth includes a k-distance graph for `eps` estimation.
*   **Consensus Clustering:** The `consensus_clustering` function is a placeholder (`return consensus_matrix # Placeholder`) and does not perform real consensus clustering or stability analysis using techniques like bootstrap resampling as requested. This is a major omission of a key advanced requirement.
*   **Factor Analysis:** While included, it's not actually used for downstream plotting or analysis in the example (e.g., `X_fa` is generated but not plotted in `create_dashboard`).
*   **Anomaly Detection:** Only one method (Isolation Forest) is implemented, and it's applied globally rather than 'cluster-based' as requested. The ground truth shows two methods, one of which is distance from centroid *per cluster*.
*   **Plotting:** The 3D plot in `create_dashboard` uses original features (`ph`, `turbidity`, `hardness`) instead of the dimensionality reduction results (e.g., PCA 3D components) as typically done for clustering visualization.

**Code Quality (7/10):**
*   **Imports:** Most necessary imports are present.
*   **Structure:** The code is structured into logical functions, which is good.
*   **Efficiency:** `KMeans` should ideally use `n_init=10` or `auto` explicitly to ensure robust initializations. The generated code defaults to 10 for KMeans after scikit-learn 1.4, but prior versions default to 1.
*   **Readability:** Variable names are generally clear.

**Completeness (5/10):**
*   **Advanced Feature Engineering:** Lacks complexity. WQI definition is very simple; Contamination risk scores are just binary flags and not comprehensive. Temporal patterns are mentioned as ""if applicable"" but not addressed (though this dataset is static).
*   **Multiple Clustering Algorithms:** All algorithms are implemented.
*   **Optimal Cluster Determination:** While functions are there, the actual process of using *multiple* validation techniques to decide on the *optimal* K (e.g., visual inspection of plots for elbow/silhouette scores) is missing from the active code flow. Plots for these metrics are not generated.
*   **Dimensionality Reduction:** All specified techniques are implemented.
*   **Cluster Stability Analysis:** This is largely missing. `resample` is imported but only used as a placeholder in `consensus_clustering`, which itself is not truly implemented. No bootstrap resampling for stability analysis with ARI scores is performed.
*   **Water Quality Profiles:** Basic profiles are generated but lack deeper analysis points or presentation.
*   **Comprehensive Visualization Dashboard:** Lacks the radar charts being produced. The 3D plot is not leveraging the DR results. Interactive elements required (hover data etc. in Plotly) are sometimes missing or basic.
*   **External Validation:** Attempts `adjusted_rand_score` if `Potability` exists, which is good, but the overall ""domain knowledge"" part is just a print statement, not an actual validation of clusters against standards within the code.
*   **Cluster-based Anomaly Detection:** Only one method, `IsolationForest`, applied globally, not per cluster.

Overall, the code demonstrates a good understanding of the individual components but struggles to integrate them into a truly ""advanced"" and ""comprehensive"" analysis as specified by the prompt. Many sections are started but not fully fleshed out or implemented as requested (e.g., consensus clustering, full optimal K determination, detailed cluster analysis).",grok-3-mini-beta,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression  # Imported as per instructions, though seaborn's regplot is used

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Compute correlation matrix
corr_matrix = df.corr()

# Correlations with 'quality'
corr_with_quality = corr_matrix['quality'].sort_values(ascending=False)

# Top 3 positively correlated features (excludes 'quality' itself)
top_positive = ...",9,"The generated code is largely correct and addresses most of the requirements. It successfully loads the data, calculates correlations, identifies the top positive and negative features, generates a heatmap, and creates scatter plots with regression lines.

**Strong Points:**
*   Correctly uses `df.corr()` to get the correlation matrix.
*   Correctly sorts correlations with 'quality' and extracts top/bottom features.
*   The use of `sns.heatmap` for the correlation matrix is correct.
*   The use of `sns.regplot` for scatter plots with regression lines is efficient and accurate for the task.
*   Handles the exclusion of 'quality' itself when determining the top positive feature correctly by using `iloc[1:4]`.

**Areas for Improvement (Why not a 10):**
*   **Error Handling:** The ground truth includes robust error handling for file not found and other exceptions when loading the dataset, which is a good practice for deployed scripts. The generated code lacks this.
*   **Clarity on `iloc[-3:]`:** While `iloc[-3:]` correctly gets the last 3 (most negative) features, the sorting `ascending=False` means 'negative' values are at the bottom of the list. Explicitly sorting ascending for negative values might be clearer or commenting on why `iloc[-3:]` works with `ascending=False`.
*   **`sklearn.linear_model` import:** The instructions mention importing `sklearn` or `scipy` for regression. The generated code imports `LinearRegression` from `sklearn.linear_model` but then uses `seaborn.regplot` which internally handles the regression, making the explicit `sklearn` import redundant for this particular implementation. While not strictly wrong, it adds an unused import. The ground truth used `scipy.stats.linregress` for explicit calculation, which is also a valid approach.
*   **Plot Customization/Details:** The ground truth includes more plot customizations (figure sizes, titles, labels, DPI, grid, legend for R value, etc.) which make the plots more presentable and informative for a data analysis task. The generated code's plots are functional but less polished. Specifically, showing the R-squared value on the regression line legend, as done in the ground truth, adds significant value for evaluating the linear fit.
*   **No explicit output for most positive/negative correlation values:** While `top_positive` and `top_negative` are printed, the actual correlation coefficient for the single most positive/negative feature when introducing the scatter plots is not explicitly printed as it is in the ground truth. This is a minor point but adds to the completeness of the printed analysis.",grok-3-mini-beta,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load the dataset and clean missing values
df = pd.read_csv('video_games_sales.csv')
df = df.dropna(subset=['Global_Sales', 'Publisher', 'Genre', 'Platform', 'Year'])

# Task 2: T-test for Nintendo vs others
nintendo_sales = df[df['Publisher'] == 'Nintendo']['Global_Sales'].dropna()
others_sales = df[df['Publisher'] != 'Nintendo']['Global_Sales'].dropna()
t_stat, p_value_two_tailed = stats.ttest_ind(nintendo_sales, others...",6,"The generated code addresses most of the requirements but has several issues related to correctness, completeness, and robustness.

**Strengths:**
*   Uses correct libraries (`pandas`, `scipy.stats`).
*   Attempts to perform all requested statistical tests (t-test, chi-square, Mann-Whitney U).
*   Correctly identifies and attempts to filter for top 5 genres/platforms for the chi-square test.
*   Includes basic interpretation of p-values.

**Areas for Improvement/Corrections:**

1.  **Data Cleaning:**
    *   The `Year` column missing from the `dropna` subset. The ground truth uses `Year_of_Release`. If the column name is truly `'Year'`, it should be handled: `df.dropna(subset=['Global_Sales', 'Publisher', 'Genre', 'Platform', 'Year_of_Release'])`.
    *   It doesn't convert `Year_of_Release` to integer type, which may cause issues if used for numerical comparisons downstream and `dropna` for 'Year' might not be sufficient if there are non-numeric entries.
    *   It does not handle `FileNotFoundError` for the CSV, making the code less robust.
    *   It doesn't print any cleaning summaries, making it harder to verify the cleaning process.

2.  **Hypothesis Test 1 (Nintendo vs. Others):**
    *   The requirement states: ""Test whether Nintendo games have significantly *higher* global sales"". This implies a one-tailed (greater) t-test. The generated code performs a two-tailed test and then manually halves the p-value. While this can work for one-tailed tests, it's not the most robust or explicit way. `scipy.stats.ttest_ind` has an `alternative` parameter (`'greater'`, `'less'`, `'two-sided'`) which should be used directly for clarity and correctness in all cases.
    *   The interpretation of the one-tailed test is correct only if the t-statistic is positive. The generated code handles this, but using the `alternative='greater'` parameter directly is cleaner.
    *   It does not explicitly state the null and alternative hypotheses, which is good practice for clarity.

3.  **Hypothesis Test 2 (Action vs. Sports):**
    *   The requirement is ""significant *difference*"", which correctly implies a two-sided test. The code does this.
    *   Again, no explicit null/alternative hypotheses.

4.  **Hypothesis Test 3 (Genre vs. Platform Chi-square):**
    *   No handling for potential errors if `filtered_df` is empty or if the `contingency_table` has zero rows/columns or entirely zero values, which would prevent the chi-square test from running or produce errors. The ground truth handles this effectively.
    *   No explicit null/alternative hypotheses.

5.  **Hypothesis Test 4 (Sales After 2000 vs Before 2000):**
    *   The column name `Year` is used, whereas the dataset generally uses `Year_of_Release`. This is a potential column name mismatch that could cause the code to fail.
    *   No explicit null/alternative hypotheses.

6.  **Overall:**
    *   The output lacks the systematic presentation of results (e.g., specific p-value values, null/alternative hypotheses) that the ground truth provides. This makes it harder to interpret the output directly without knowing the exact critical value.
    *   Lack of robustness checks (e.g., checking `len` of groups before t-test or Mann-Whitney U test, checking `contingency_table` validity). This could lead to runtime errors if the data is sparse or unexpected.
    *   No `ALPHA` constant defined for significance level, it's hardcoded as `0.05` in multiple places.

The generated code is functional for a typical case but lacks the thoroughness, robustness, and clarity expected from ""expert"" level code, especially in a data science context where assumptions and edge cases matter.",grok-3-mini-beta,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Load the dataset and handle missing values
df = pd.read_csv('video_games_sales.csv')
df.dropna(inplace=True)

# Create binary target variable
df['High_Sales'] = (df['Global_Sales'] > 1.0).astype...",6,"The generated code addresses most of the requirements but has several critical issues that make it less robust and potentially incorrect for the given task.

**Strengths:**
*   Correctly loads the dataset and initializes necessary libraries.
*   Successfully creates the 'High_Sales' target variable.
*   Performs Label Encoding on the specified categorical features.
*   Creates 'Total_Regional_Sales' and 'Years_Since_Release' features.
*   Splits the data into training and testing sets.
*   Trains a Random Forest Classifier with 100 estimators.
*   Calculates and prints accuracy, precision, recall, F1-score, and confusion matrix.
*   Displays feature importance.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling (Critical Flaw):** The most significant issue is `df.dropna(inplace=True)`. This line drops *any* row containing *any* NaN. The problem statement explicitly asks to handle missing values, and the ground truth code demonstrates a more nuanced approach. The `Year`, `Publisher`, and sales columns are critical for feature engineering, and dropping them entirely if a single value is missing from *any* column (e.g., `Name`, `Developer` etc.) is too aggressive and will remove a lot of data unnecessarily. Specifically, the ground truth drops NaNs on `Global_Sales` first, then `Year`, `Publisher`, and regional sales, which is more appropriate as these are the columns used for analysis. The generated code also doesn't convert `Year` to integer *after* dropping NaNs, which might cause issues if NaNs were present.

2.  **Incomplete Feature Selection for Training:** The feature set `X = df[['Platform', 'Genre', 'Publisher', 'Total_Regional_Sales', 'Years_Since_Release']]` overlooks the individual regional sales columns (`NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`) that were present in the original dataset and are highly predictive. The problem explicitly implies their inclusion by mentioning them in the features section of the ground truth and generally in the context of building a robust model for video game sales. `Total_Regional_Sales` is a good aggregated feature, but the individual components might also carry important information.

3.  **Missing `random_state` in Random Forest Classifier:** The `RandomForestClassifier` is initialized as `rf = RandomForestClassifier(n_estimators=100)`. It lacks `random_state=42`, which makes the model training non-reproducible.

4.  **Missing Stratification in `train_test_split`:** The `train_test_split` call `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)` does not include `stratify=y`. For classification tasks, especially with imbalanced classes (which 'High_Sales' vs 'Low_Sales' is likely to be), stratification ensures that the proportion of target classes is maintained in both training and test sets, leading to more reliable evaluation.

5.  **Lack of `n_jobs=-1` for Random Forest:** While not a correctness issue, `n_jobs=-1` (as used in the ground truth) allows the Random Forest to utilize all available CPU cores, significantly speeding up training.

6.  **Readability/Presentation of Feature Importance:** The output for feature importance is a simple loop, which is functional but less organized than using a Pandas Series and sorting it, as demonstrated in the ground truth, for clearer visualization.

7.  **Error Handling (File Not Found):** The ground truth includes basic error handling for `FileNotFoundError`, which is good practice for robustness. The generated code assumes the file will always be present.

8.  **Minor code quality:** The prints for evaluation metrics are less formatted than the ground truth's, lacking clear labels and consistent formatting.",grok-3-mini-beta,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, PolynomialFeatures

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values
numerical_cols = df.select_dtypes(include=[np.number]).columns
df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())

categorical_cols = df.select_dtypes(include=['object']).columns
for col...",7,"The generated code addresses most of the requirements but has several areas for improvement in terms of correctness, robustness, and best practices.

**Strengths:**
*   Successfully imports necessary libraries.
*   Attempts to handle missing values for both numerical and categorical columns.
*   Correctly calculates and creates all five requested engineered features.
*   Performs correlation analysis and visualizes it with a heatmap.
*   Applies PCA and correctly scales data before PCA.
*   Applies polynomial features to the sales columns as requested.
*   Creates interaction features between Year and Genre using one-hot encoding.
*   Attempts outlier handling using the IQR method.

**Areas for Improvement/Weaknesses:**

1.  **Missing Value Handling:**
    *   The `fillna(df[numerical_cols].mean())` for numerical columns is acceptable but `SimpleImputer` is generally preferred for robustness and explicit strategy.
    *   For categorical columns, `df[col].fillna(df[col].mode()[0], inplace=True)` is okay, but `SimpleImputer(strategy='most_frequent')` is a more robust and consistent approach, especially if you have `NaN`s in `Publisher` or `Genre` which are later used in `groupby` operations. The `ground truth` uses `SimpleImputer` which is better.
    *   It doesn't handle `Year` and `Publisher`/`Genre` specifically before feature engineering, which can lead to issues if `mean()` or `mode()` are not appropriate for these types. The ground truth specifically handles `Year` as `int` and `Publisher`/`Genre` as categorical.

2.  **Engineered Features - Robustness:**
    *   `Sales_Ratio_NA_EU`: The `apply` method with a lambda function is significantly slower and less efficient than vectorized operations (`df['NA_Sales'] / df['EU_Sales']`, with a small epsilon in the denominator to avoid division by zero). The ground truth's `df['NA_Sales'] / (df['EU_Sales'] + 1e-6)` is more robust and efficient.
    *   `Platform_Popularity`: `value_counts().to_dict()` then `map()` is correct but `df.groupby('Platform')['Name'].transform('count')` (as in ground truth) is more direct and efficient for broadcasting to the original DataFrame's size.
    *   The `median_sales_per_year` is mapped, which works but might lead to NaNs if a year in `df['Year']` was not present in the `median_sales_per_year` dictionary keys. `transform('median')` is safer.

3.  **PCA:**
    *   It includes ALL numerical columns for PCA `numerical_df = df.select_dtypes(include=[np.number])`, which might include 'Rank' or other non-features. The ground truth explicitly removes 'Rank' for PCA which is usually a better practice as 'Rank' is often just an index.
    *   The output for explanatory variance is just a print of the array, while the ground truth also visualizes cumulative explained variance which is very helpful for PCA analysis.

4.  **Polynomial Features:**
    *   The second `sns.heatmap` for ""Correlation with original sales to analyze impact"" is a good thought, but the task asked to *analyze their impact*, not necessarily visualize a new correlation matrix. The prompt was a bit vague on ""analyze,"" but the ground truth didn't include this second heatmap. It's not incorrect, just extra.

5.  **Interaction Features (`Year` and `Genre`):**
    *   `df = pd.get_dummies(df, columns=['Genre'], drop_first=True)`: While this creates dummies, it overwrites the original `df` and removes the original 'Genre' column from the DataFrame for subsequent operations if not careful. The ground truth creates dummies and then concatenates them, preserving the original 'Genre' column which might be useful for other analyses. Also, the `drop_first=True` argument can be a good idea to prevent multicollinearity if going into a linear model, but it's not strictly necessary for just creating an interaction feature.

6.  **Outlier Handling (Critical Issue):**
    *   The biggest flaw is in the outlier handling: `df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]`. This line **removes rows with outliers**. The task specifically asked to ""Identify and **handle** outliers using IQR method."" Handling typically implies capping (as done in the ground truth) or transforming, not outright deletion, especially for multiple columns in a loop. Deleting rows repeatedly in a loop based on different columns will lead to significant data loss and an inconsistent DataFrame. The ground truth correctly caps the outliers, preserving data.

7.  **Code Structure and Readability:**
    *   The feedback and print statements from the ground truth are very helpful for debugging and understanding the flow of operations. The generated code lacks these.
    *   The `try-except` block for file loading in the ground truth is a good practice.

Overall, the code provides a functional approach but suffers from issues related to robustness, efficiency, and a critical flaw in outlier handling that changes the intended ""handling"" to ""deletion,"" which is often undesirable for data analysis.",grok-3-mini-beta,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```
import pandas as pd
import numpy as np
from scipy import stats

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Clean zero values by replacing with NaN and then median imputation
columns_to_clean = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
for column in columns_to_clean:
    df[column] = df[column].replace(0, np.nan)
    median_value = df[column].median()
    df[column] = df[column].fillna(median_value)

# Test 2: T-test for Glucose levels between diabetic and no...",8,"The generated code correctly performs all the requested statistical tests.

**Strong Points:**
*   **Correct Data Imputation:** The code correctly identifies the columns to be cleaned and performs median imputation for zero values.
*   **All Tests Implemented:** T-test, Mann-Whitney U test, Chi-square test, Pearson correlation, and ANOVA are all implemented as required.
*   **Correct Test Parameters:** `equal_var=False` for the t-test and `alternative='two-sided'` for Mann-Whitney U test are appropriate choices.
*   **Correct Grouping:** Age and BMI categorization are done correctly for chi-square and ANOVA, respectively.
*   **P-value Display and Interpretation:** P-values are printed, and a basic interpretation of significance is provided for each test.

**Areas for Improvement to reach 9-10:**
*   **Missing Output Clarity/Detail:** The Ground Truth provides much more detailed output for each test, including calculated statistics (T-stat, U-stat, Chi2, F-stat, Correlation Coefficient), means/medians of groups, and more verbose interpretations explaining *why* a conclusion is drawn (e.g., ""diabetic patients have significantly higher glucose levels"" instead of just ""The result is significant""). This is crucial for a data science benchmark.
*   **One-sided T-test Justification/Implementation:** While the Ground Truth *implies* a two-sided test by not adjusting the p-value and then interpreting based on which mean is higher, the generated code explicitly calculates a one-sided p-value for the t-test for ""higher glucose levels."" This is a valid interpretation of ""significantly higher,"" but it would be good to state that assumption (e.g., in a comment or print statement) or to clarify the interpretation logic. If the null hypothesis states ""not higher,"" then a one-sided test is appropriate. The ground truth used a two-sided test and then interpreted the direction. The generated code's specific one-sided implementation (calculating `t_stat > 0`) is also slightly incomplete; if `t_stat < 0`, the one-sided p-value for ""higher"" should be 1. It also calculates `1 - (p_value_ttest / 2)` which for a negative t-stat correctly yields a p-value > 0.5 (meaning no evidence for ""higher""). This isn't necessarily incorrect, but the ground truth's simpler two-sided approach with directional interpretation is often preferred unless a directional hypothesis is explicitly stated and justified.
*   **Repetitive Significance Level (alpha):** The `0.05` alpha level is hardcoded multiple times. Defining it as a constant like `ALPHA = 0.05` (as in the Ground Truth) improves readability and maintainability.
*   **Code Structure and Readability:** The Ground Truth uses section headers and more descriptive print statements, which enhance code readability and user understanding of the analysis flow. The generated code performs the tasks but lacks this presentation.
*   **Robustness in ANOVA:** The Ground Truth includes a check for empty groups before performing ANOVA, which makes the code more robust against scenarios where some BMI categories might be empty. The generated code `df['BMICategory'].unique()` might produce categories that have no data points in them, leading to errors if `dropna()` still results in an empty series. Although `dropna()` addresses `NaN`s, it doesn't guarantee data in every category.
*   **Error Handling for File Loading:** The Ground Truth includes a `try-except` block for file loading, which is good practice.",grok-3-mini-beta,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as pl...",7,"The generated code addresses most of the requirements but has several areas for improvement in terms of correctness, completeness, and code quality.

**Strengths:**
*   Successfully loads the dataset.
*   Handles missing values (zeros replaced with median).
*   Creates the three specified additional features ('Glucose_BMI_ratio', 'Age_group', 'Insulin_level').
*   Encodes categorical variables using `pd.get_dummies`.
*   Splits data into training and testing sets.
*   Applies `StandardScaler`.
*   Trains and compares multiple classification models (Logistic Regression, Random Forest, SVM, Gradient Boosting).
*   Uses `cross_val_score` for initial evaluation.
*   Performs hyperparameter tuning for *one* of the models (the ""best"" one chosen by CV accuracy).
*   Evaluates models using accuracy, precision, recall, F1-score, and ROC-AUC.
*   Generates confusion matrices and ROC curves.
*   Displays feature importance for tree-based models.

**Areas for Improvement:**

1.  **Missing Value Handling:** The ground truth correctly identifies `0`s in `Insulin` as missing and replaces them with median. The generated code also replaces `0`s in `Insulin` with median, however, the `pd.cut` for `Insulin_level` creates `NaN`s internally because `pd.qcut` (used in ground truth) is more robust for creating quantiles especially when there are many identical values or zeros. The generated code's `tertiles` calculation and subsequent `pd.cut` might lead to issues with `Insulin_level` categories if `df['Insulin'].median()` (which could be the 0s now) is still 0, resulting in `NaN`s for the `low` category. The ground truth correctly handles `NaN`s after imputing, ensuring `pd.qcut` works robustly.

2.  **Preprocessing Pipeline:** The ground truth uses `ColumnTransformer` and `Pipeline` for more robust and maintainable preprocessing, handling numerical and categorical features separately and integrating scaling and encoding. The generated code processes categorical variables via `pd.get_dummies` *before* the train-test split, which is a data leakage issue for new categorical levels in test data (though less likely with this dataset's structure). It then scales numerical features *after* the train-test split but doesn't integrate the full preprocessing into a pipeline for model training. This breaks the best practice of a `ColumnTransformer` and `Pipeline`.

3.  **Hyperparameter Tuning:**
    *   The prompt specifies ""Perform hyperparameter tuning for the best model using GridSearchCV"". The generated code selects *a single* ""best model"" based on CV accuracy from the initial run and *only tunes that one*. The ground truth approach of evaluating all models initially, then *choosing one* (e.g., Random Forest as a generally good candidate) for tuning aligns better with typical workflows, potentially providing more robust insights than just tuning the one that happened to perform best in the initial, untuned CV run.
    *   The hyperparameter grids are very basic (e.g., `C: [0.1, 1, 10]` for LR). While it *does* perform tuning, the scope is limited. The ground truth provides a more comprehensive grid for Random Forest.

4.  **Overall Evaluation Flow:**
    *   The generated code *trains* all models *again* after the cross-validation for the final evaluation, and then within the evaluation loop, it performs visualization. This leads to duplicate model training and printing of plots intertwined with the evaluation metrics, making the output less clean.
    *   The ground truth performs cross-validation, then *fits* the models for final evaluation on test set, then *separately* performs hyperparameter tuning for one model, and *then* gathers all evaluation results and finally visualizes. This separation of concerns improves code readability and execution flow.

5.  **Imports and Warnings:** The ground truth includes more comprehensive imports (`ColumnTransformer`, `Pipeline`, `StratifiedKFold`, `ConfustionMatrixDisplay`, `RocCurveDisplay`) and warning suppression, which are good practices.

6.  **Feature Importance (`pd.get_dummies` impact):** When `pd.get_dummies` is used on the entire `df` before `train_test_split`, `X_train` and `X_test` will correctly have the dummy variables. However, `X.columns` used for feature importance plotting will correctly reflect these `_Age_group_*` and `_Insulin_level_*` columns. The ground truth, by using `ColumnTransformer`, has to explicitly get the new feature names (e.g., `get_feature_names_out`), which is a more robust way when a `Pipeline` is used. The generated code's approach for feature importance column names works here but isn't as robust as having a list of processed feature names if a more complex preprocessor was used.

7.  **Plotting:** The ROC curves are plotted one by one within the loop for each model, which is less effective for comparison than plotting all on a single graph, as done in the ground truth. The confusion matrices also popup one by one instead of a single figure with subplots.",grok-3-mini-beta,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, RFE, f_classif
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt

# Load and preprocess the dataset
df = pd.read_csv('diabetes.csv')

# Handle missing values by replacing zeros with NaN and imputing with mean
c...",7,"The generated code correctly addresses most of the requirements but has several areas for improvement in terms of robustness, completeness, and clarity.

**Strengths:**
*   Correctly loads the dataset.
*   Handles missing values by replacing zeros and then imputing (though strategy could be improved).
*   Creates all requested engineered features.
*   Applies polynomial features.
*   Performs both StandardScaler and MinMaxScaler.
*   Applies PCA and correctly prints explained variance ratio.
*   Uses SelectKBest and RFE for feature selection.
*   Applies SMOTE for class imbalance.
*   Evaluates feature importance using Random Forest and visualizes them.
*   Includes correlation-based feature filtering.

**Areas for Improvement:**

1.  **Missing Value Imputation Strategy:** The ground truth code uses `SimpleImputer(strategy='median')` which is generally more robust to outliers than `df.fillna(df.mean())`. While `df.mean()` works, it's less standard for this particular dataset where 0s are place-holders for missing values and could skew the mean significantly if some 'missing' values were non-zero. The generated code also uses `df.fillna(df.mean())` after replacing 0s, which is fine, but the median is often preferred for non-normal distributions or when outliers are present.
2.  **Order of Operations (Correlation Filtering vs. Scaling):** The generated code performs correlation filtering *before* scaling. Correlation filtering is typically done on the pre-processed (scaled) data, especially if you are using distance-based metrics or PCA afterwards. The ground truth does this correctly by applying it to `X_processed` (which is `X_scaled_standard_df`).
3.  **Feature Selection Integration:** The generated code applies KBest and RFE, but then proceeds with SMOTE on the `X_kbest` output, effectively discarding the `X_rfe` result. The problem statement asked to ""Use SelectKBest and Recursive Feature Elimination for feature selection,"" which implies both should be used, but doesn't explicitly state how their results should be integrated or which one to proceed with. The ground truth cleanly performs both but doesn't explicitly state which feature set is then used for the final analysis (SMOTE, RF importance). The generated code's choice to use `X_kbest` for SMOTE is a reasonable interpretation, but it could be clearer.
4.  **PCA `n_components`:** The ground truth sets `n_components=0.95` for PCA, which is a good practice to retain components explaining a certain percentage of variance. The generated code initializes `pca = PCA()` without specifying `n_components`, which means it will return all components, potentially leading to high dimensionality if not followed by further dimensionality reduction or selection.
5.  **Polynomial Feature Redundancy:** The generated code drops original columns `numerical_cols` from the `poly_df` and then concatenates `df` with `poly_df[new_poly_cols]`. This is correct. The ground truth also removes these columns before concatenating to avoid duplication, achieving the same result. The ground truth's `poly.get_feature_names_out` directly provides the names for `df_poly`, and then `df.drop` and `pd.concat` is a clean way to manage it.
6.  **Output and Explanations:** The ground truth includes `print` statements at each major step, which greatly enhances readability and understanding of the code's flow and status. This was requested in the prompt (""Include proper imports and assume the CSV file is in the current directory. Provide only the Python code without explanations."") ""without explanations"" refers to prose, not print statements within the code. The generated code lacks these informative print statements.
7.  **Smote and Evaluation Data Split:** The ground truth correctly splits data into train/test *before* applying SMOTE to the training set. The generated code applies SMOTE to the entire `X_kbest` and `y` dataset, which is incorrect as it introduces data leakage from the test set into the SMOTE process.
8.  **Feature Importance Plotting:** The generated code simply plots `top_features` from `X_kbest` (which are abstract feature indices after `SelectKBest`). The ground truth maps the importance back to the original feature names, providing a much more interpretable visualization. This is a critical detail for evaluating feature importances.
9.  **RandomForestClassifier in RFE:** The generated code uses `RandomForestClassifier` as the estimator for RFE. While possible, `RFE` is generally more efficient with linear models or those with explicit `coef_` or `feature_importances_` attributes for ranking. Logistic Regression (as in the ground truth) is a more common and often more stable choice for RFE's underlying estimator.
10. **Suppressed Warnings:** The ground truth includes `warnings.filterwarnings('ignore')` which is a minor but good practice for cleaner output in a script like this.

Overall, the generated code captures the essence of most tasks but misses crucial professional practices and robustness considerations, particularly regarding data leakage (SMOTE before train/test split) and informative visualization.",grok-3-mini-beta,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load dataset and handle missing values
df = pd.read_csv('world-happiness-report-2021.csv')
df = df.dropna()

# 1. T-test: Western European countries vs others
western_europe_scores = df[df['Regional indicator'] == 'Western Europe']['Ladder score']
others_scores = df[df['Regional indicator'] != 'Western Europe']['Ladder score']
t_stat, p_value_1 = stats.ttest_ind(western_europe_scores, others_scores)
print(f""T-test p-valu...",6,"The generated code attempts to address all parts of the task but has several significant deficiencies.

**Correctness Issues:**
*   **Missing Value Handling:** The generated code uses `df.dropna()`. While this handles NaNs, the ground truth explicitly drops NaNs only from ""critical columns"" (`critical_columns`), which is a more nuanced and potentially less data-lossy approach if other columns unrelated to the analysis have missing values. The generated code's approach is acceptable but less robust.
*   **Interpretation of T-test for Western Europe:** The generated code only states ""Western European countries have significantly higher happiness scores."" if p < 0.05. It doesn't check the mean values to confirm if they are indeed *higher* or *lower*. The current interpretation is incomplete.
*   **Mann-Whitney U Test Group Selection:** The generated code selects `top_25_percent = df_sorted.iloc[:n//4]['Logged GDP per capita']` and `bottom_25_percent = df_sorted.iloc[-n//4:]['Logged GDP per capita']`. This is incorrect. The `iloc[:n//4]` selects the *lowest* 25% (i.e., the least happy), and `iloc[-n//4:]` selects the *highest* 25% (i.e., the happiest). The prompt asks for ""top 25% happiest"" and ""bottom 25% happiest"". In the provided code, `top_25_percent` actually maps to the ""bottom 25% happiest"" and `bottom_25_percent` maps to the ""top 25% happiest"". This is a critical error in group definition.
*   **Interpretation of T-test for Generosity:** Similar to the first t-test, the interpretation for generosity (`if p_value_6 < 0.05:`) also just states ""significantly different"", without checking mean values to determine if ""above-median freedom"" leads to ""higher"" or ""lower"" generosity.
*   **Alpha Definition:** The generated code uses hardcoded `0.05` for significance level checks. The request asked to ""Display p-values and interpret results with significance level =0.05"", implying `alpha` should be a defined variable for consistency and ease of modification, as done in the ground truth.
*   **Output Readability:** The generated code simply prints p-values and a single interpretation line. The ground truth provides more detailed print statements, including means/medians, statistics, and clearer result interpretations (`Result: Reject/Fail to reject`, `Interpretation: ...`). This makes the output much more informative and aligns better with ""expert code evaluator"" standards.
*   **Assumptions on T-test:** The generated code uses `ttest_ind` without `equal_var=False`, which defaults to `True`. The ground truth correctly uses `equal_var=False` (Welch's t-test), which is often more appropriate when group variances are unknown or unequal, making the test more robust.

**Completeness:**
*   It performs all requested statistical tests.
*   It loads data and handles missing values (though less optimally).
*   It provides p-values and interprets results (though interpretations are incomplete/incorrect).

**Code Quality:**
*   Imports are correct.
*   Structure is largely functional but lacks the explicit `alpha` variable and the detailed output formatting of the ground truth.
*   No error handling for `FileNotFoundError`.

Overall, while the code attempts to fulfill the requirements, the critical errors in group definition for the Mann-Whitney U test, incomplete interpretations for t-tests, and less robust missing value handling and T-test assumptions significantly reduce its correctness and reliability. The output lacks the clarity expected for a ""statistical analysis"" task.",grok-3-mini-beta,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load dataset and preprocess
df = pd.read_csv('world-happiness-report-2021.csv')
df = df.dropna(subset=['Ladder score',...",7,"The generated code addresses most of the tasks, but several aspects need improvement for higher quality and completeness.

**Strong Points:**
- Correctly loads data and identifies features/target.
- Implements multiple regression models and evaluates them.
- Performs hyperparameter tuning (though with limited parameters).
- Generates residual plots and feature importance.
- Implements cross-validation.
- Makes predictions for new data.

**Areas for Improvement:**

1.  **Missing Value Handling:** The ground truth code specifically drops rows only for the relevant `FEATURE_COLUMNS` and `TARGET_COLUMN`. The generated code uses a hardcoded list for `dropna`. While it might work for this specific dataset, specifying the exact relevant columns from `FEATURE_COLUMNS` and `TARGET_COLUMN` is more robust and less error-prone if the dataset structure changes.
2.  **Categorical Variable Encoding:** The ground truth explicitly states that no categorical variables are used as features and therefore no encoding is performed. The generated code completely omits this point, which is fine since no categorical features are *used*, but a comment or explicit consideration, as in the ground truth, would show a more thorough understanding of the preprocessing step.
3.  **Specific Features:** The generated code uses `'Healthy life expectancy at birth'` which is not a column in the `world-happiness-report-2021.csv` dataset. The correct column name, as per the ground truth and the dataset, is `'Healthy life expectancy'`. This will cause a `KeyError` at runtime.
4.  **Hyperparameter Tuning `best_model_name` Logic:** The logic `min(results, key=lambda x: results[x]['RMSE'])` to determine the ""best model"" for tuning might not always align with what users consider ""best."" R2 score is often preferred for regression model selection, or at least it should be a conscious choice. The ground truth explicitly picks `GradientBoostingRegressor` (a common strong performer) as the candidate for tuning, which is a reasonable approach.
5.  **Hyperparameter Tuning Parameters (Completeness):** The `param_grid` for `GridSearchCV` in the generated code is very minimal (`n_estimators` for both, `max_depth` for RF, `learning_rate` for GB). The ground truth provides a more comprehensive set of hyperparameters, which is crucial for effective tuning.
6.  **Visualization Libraries:** The generated code imports `matplotlib.pyplot` but not `seaborn`, which is used in the ground truth for visually appealing and informative plots (e.g., `sns.scatterplot`, `sns.barplot`). While `matplotlib` can do the job, `seaborn` enhances the quality.
7.  **Plot Labels and Titles:** While residual and feature importance plots are generated, their labels and titles are quite basic. The ground truth provides more descriptive titles and axis labels.
8.  **Output and Analysis:** The ground truth code provides more detailed print statements for intermediate steps (e.g., `df.info()`, shapes, head of X and y), and a comprehensive analysis of the hypothetical country predictions. The generated code's outputs are more concise.
9.  **Random States:** While `RandomForestRegressor` and `GradientBoostingRegressor` have `random_state` set, other models like `Ridge` and `Lasso` do not, which means their default initializations could vary. The ground truth consistently sets `random_state` where applicable for reproducibility.
10. **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.",grok-3-mini-beta,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_regression, RFE, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import seaborn as sns

# Load and preprocess the dataset
df = pd.read_csv('world-happiness-report-2021.csv...",7,"The generated code addresses most of the requirements but has several areas for improvement in terms of robustness, completeness, and adherence to best practices.

**Strengths:**
*   Successfully loads and preprocesses the data.
*   Correctly implements the requested engineered features: 'Happiness_Efficiency', 'Social_Wellness_Index', 'Governance_Score', 'Regional_Happiness_Rank', and 'GDP_vs_Regional_Average'.
*   Applies polynomial features with `PolynomialFeatures`.
*   Performs feature scaling using StandardScaler, MinMaxScaler, and RobustScaler.
*   Applies PCA and t-SNE for dimensionality reduction and includes visualization.
*   Uses `SelectKBest` (with `f_regression`) and `RFE` for feature selection.
*   Performs correlation-based feature filtering.
*   Calculates mutual information scores.
*   Creates the specified interaction features.
*   Validates feature importance using `RandomForestRegressor` and visualizes it.

**Areas for Improvement:**

1.  **Robustness of Preprocessing:**
    *   `df = df.dropna()`: This is too aggressive. The ground truth specifically handles
        `'Perceptions of corruption'` NaNs by filling them with the mean, which is a more sensible approach than dropping all rows, especially given the smaller dataset size. The generated code drops any row with any NaN, which might remove valuable data if other less critical columns have missing values.
    *   The generated code does not explicitly handle `np.inf` or `-np.inf` values that might arise from feature engineering (e.g., division by zero in `Happiness_Efficiency`). The ground truth includes this crucial step (`X.replace([np.inf, -np.inf], np.nan, inplace=True); X.fillna(X.mean(), inplace=True);`). This can lead to errors in subsequent steps (e.g., scaling, PCA, t-SNE, feature selection).

2.  **Consistency in DataFrame Usage for Scaling/Dimensionality Reduction/Feature Selection:**
    *   The generated code applies scaling to `df[numerical_cols]` and creates `df_scaled_std`, `df_scaled_minmax`, etc., but then directly modifies the original `df` by adding 'PCA1', 'PCA2', 'TSNE1', 'TSNE2' columns onto it. This mixes scaled and unscaled features in the same DataFrame (`df`), which is generally not good practice for machine learning pipelines. The ground truth keeps a separate `X_scaled` DataFrame for these operations, which is cleaner.
    *   For `SelectKBest` and `RFE`, `X` is defined as `df_scaled_std.drop('Ladder score', axis=1, errors='ignore')`. This is good, but the subsequent interactions (`df['GDP_Social_Interaction']`) and random forest importance are computed on the *original* `df` (which now has PCA/t-SNE components and potentially unscaled features) rather than consistently using `df_scaled_std`. This can lead to inconsistencies or incorrect feature importance calculations if features are not on the same scale when comparing them.

3.  **Visualization of PCA:**
    *   The task explicitly asks for PCA visualization, but the generated code only includes t-SNE visualization.

4.  **Information Messages/Print Statements:**
    *   The ground truth provides informative print statements at each major step (`--- Feature Scaling ---`, `--- Dimensionality Reduction ---`, etc.) and summary information (e.g., explained variance for PCA, shape after filtering). The generated code is silent except for the plots.

5.  **Perplexity for t-SNE:**
    *   The ground truth carefully sets the `perplexity` value for t-SNE based on `len(X_for_tsne) - 1` to avoid errors if the dataset is small. The generated code uses a fixed `perplexity=30`, which might lead to errors if the dataset size is less than 31.

6.  **RFE Estimator:**
    *   The generated code uses `RandomForestRegressor` as the estimator for RFE. While theoretically valid, `LinearRegression` (as in the ground truth) or `LogisticRegression` are more common and computationally efficient choices for RFE, especially when the goal is purely feature selection. Using a complex model like RF for RFE can be very slow.

7.  **Mutual Information Scoring Display:**
    *   The generated code calculates `mi_scores_series` but doesn't print it to the console, making it difficult to explicitly see the scores as expected by the prompt (""mutual information scoring"" implies showing the scores).

8.  **Redundancy in Feature Selection Output:**
    *   The `select_k_best` and `rfe` results are stored in `pd.Series` or lists (`selected_features_kbest`, `rfe_features`), but they are not printed to the console for the user to see the selected features explicitly. The ground truth prints these lists.

9.  **Clear separation of X and y:** The ground truth clearly separates `X` (features) and `y` (target) right after feature engineering and before scaling, which is a good practice. The generated code mixes this slightly, defining X and y before feature selection.",grok-3-mini-beta,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```
import pandas as pd
import statsmodels.api as sm
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt

# Load dataset and filter for USA
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
usa_df = df[df['location'] == 'USA'].set_index('date')

# Resample to weekly sum
weekly_cases = usa_df['new_cases'].resample('W').sum()

# Perform...",8,"The generated code largely addresses all requirements of the task.

**Strong Points:**
- **Correctness:** The code correctly loads, filters, resamples, decomposes, performs ADF tests (including differencing if needed), and plots ACF/PACF.
- **Completeness:** All the core steps requested in the prompt are implemented.
- **Conciseness:** The code is quite concise and gets straight to the point without excessive boilerplate (though for a real-world application, more robust error handling and function encapsulation would be beneficial).
- **Proper Imports:** All necessary libraries are imported.

**Areas for Improvement (Why not a 9 or 10):**
- **Error Handling/Robustness:** The code lacks error handling for file not found, empty data after filtering, or insufficient data points for certain operations (e.g., if `weekly_cases` is empty after resampling or too short for decomposition/ADF test with `period=52`). The ground truth specifically includes these checks.
- **Missing `period` in `seasonal_decompose`:** The prompt specifically mentions using an additive model and implicitly for weekly data, a period of 52 is standard. The generated code uses `period=52`, which is good. The prompt also implies an additive model, which is used.
- **Plotting Individual Components:** The prompt asks to ""Plot the original time series and its decomposed components."" While `decomposition.plot()` does this, it's not explicitly calling out the original time series plot as a separate step, which might be interpreted differently. The ground truth's `decomposition.plot()` is sufficient, but sometimes direct plotting of `weekly_cases` first is desired.
- **ADF Test Output Detail:** The ADF test output is minimal; it only prints the statistic and p-value. The critical values are important for a complete interpretation, which the ground truth includes.
- **ACF/PACF Lags:** The chosen `lags=40` for ACF/PACF is arbitrary. While it's generally reasonable, the ground truth uses a more dynamic calculation (`max_lags = min(50, len(stationary_series) // 2 - 1)`) which makes it more robust for different dataset sizes.
- **Non-Reproducible Plotting (Minor):** The ground truth includes `plt.tight_layout()` and `fig.suptitle` and adjusts figure size, which makes the plots more readable and presentable. The generated code only uses `plt.show()` after each plot, which can lead to overlapping plots if run in an environment that doesn't clear figures automatically.

Overall, it's an excellent attempt that successfully completes the core technical requirements. The improvements mainly revolve around robust production-level code practices and more detailed output, which are common aspects of expert-level solutions.",grok-3-mini-beta,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import f_oneway
import numpy as np
from scipy.stats import t

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Categorize pH groups
df['pH_group'] = pd.cut(df['pH'], bins=[float('-inf'), 3.2, 3.4, float('inf')], labels=['Low pH', 'Medium pH', 'High pH'])

# Calculate average quality for each pH group
group_stats = df.groupby('pH_group')['quality'].agg(['mean', 'std', 'count'])
print(""Average quality for each pH...",8,"The generated code correctly addresses most of the requirements. It categorizes pH groups, calculates average quality, performs an ANOVA test, and creates a visualization with confidence intervals.

**Strong Points:**
- Correctly uses `pd.cut` for categorization.
- Accurately calculates mean and standard deviation for quality per group.
- Correctly performs ANOVA using `scipy.stats.f_oneway`.
- Generates a bar plot with error bars, demonstrating understanding of confidence intervals for visualization.
- Handles the case where `n <= 1` for CI calculation, preventing errors.

**Areas for Improvement:**
- **Confidence Interval Calculation (Minor):** The confidence interval calculation `t_crit * se` calculates half the width of the CI. The `yerr` in `plt.bar` expects the symmetrical error bar length (the distance from the mean to one end of the CI). While this is technically correct for symmetrical error bars, it's generally clearer to define `ci_lower` and `ci_upper` and then use `[means - ci_lower, ci_upper - means]` for `yerr` if asymmetrical CIs were needed, or just `ci` for symmetrical as done here. The labels should technically be `Low pH (< 3.2)`, etc., matching the ground truth for better clarity and alignment with standard practice.
- **Error Handling:** The code does not include error handling for `FileNotFoundError` if `winequality-red.csv` is not found, which is a good practice for robust code.
- **Clarity of `pH_group` Labels:** The labels for `pd.cut` are `'Low pH'`, `'Medium pH'`, `'High pH'`. While these are functional, the ground truth's `Low pH (< 3.2)`, `Medium pH (3.2 to 3.4)`, `High pH (> 3.4)` are more descriptive and directly reflect the binning logic, improving readability and understanding for others.
- **Categorical Ordering:** The ground truth explicitly sets the order of categorical variables using `pd.Categorical`, which ensures consistent plotting and grouping order, even if some categories are empty. While `pd.cut` often orders them correctly, explicit ordering is more robust.
- **Output Reporting:** The ANOVA output is concise, but the ground truth provides a more detailed conclusion based on the p-value, which is excellent for analysis. The group quality reporting could also be more descriptive, similar to the ground truth's detailed output of means and CI bounds.",grok-3-mini-beta,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Select features and target
selected_features = ['new_cases', 'hosp_patients', 'icu_patients', 'reproduc...",7,"The generated code addresses most of the requirements but has potential issues and areas for improvement.

**Strong Points:**
*   Correctly loads the dataset and parses the 'date' column.
*   Correctly identifies and separates features and target.
*   Performs one-hot encoding for the 'continent' column.
*   Splits the data into training and testing sets.
*   Trains both Linear Regression and Random Forest Regressor models with specified parameters.
*   Evaluates models using MAE, MSE, and R-squared.
*   Prints the evaluation metrics.

**Areas for Improvement/Issues:**

1.  **Missing Value Imputation for Target Variable and Feature Separation Order:** The ground truth code handles missing values in both features and the target *after* selecting them into a `df_selected` DataFrame, ensuring consistency. The generated code separates `X` and `y` first, then imputes `X`'s numerical columns, then `X`'s 'continent', and finally `y`. This is acceptable but the ground truth's approach is slightly cleaner for ensuring all relevant columns are handled together.
2.  **Missing Value Imputation for Features - Pandas `SettingWithCopyWarning`:** The line `X[numerical_cols] = X[numerical_cols].fillna(X[numerical_cols].median())` could potentially lead to a `SettingWithCopyWarning` in pandas, depending on how `X` was created. While it might work in some cases, it's generally safer to use `.loc` for chained assignments or to ensure `X` is a distinct copy. The ground truth explicitly uses `.copy()` when creating `df_selected` and then iterates or uses `.fillna()` which avoids this warning.
3.  **Missing Value Imputation for 'continent' in Generated Code:** The generated code tries to impute 'continent' with its mode: `X['continent'] = X['continent'].fillna(X['continent'].mode()[0])`. While technically possible, the task clearly states: ""Encode the 'continent' column using one-hot encoding and add it to the features."" and ""Handle missing values in selected features and target by imputing with the median."" The median imputation specifically applies to numerical features. Categorical features like 'continent' are typically handled by the one-hot encoder itself (which will create 'NaN' columns for categories if `NaN`s are present and `handle_unknown='ignore'` or similar is not used with `get_dummies`) or by dropping rows/imputing with a placeholder *before* one-hot encoding. If `pd.get_dummies` encounters `NaN`s, it will create dummy variables for them unless they are explicitly dropped or imputed prior. The ground truth correctly omits `continent` from median imputation and lets `get_dummies` handle the absence of a value (which will just mean all dummy columns for that row will be 0 if `NaN` was not encoded as a category itself). The generated code's imputation of 'continent' isn't explicitly requested and adds a small inconsistency with the median imputation requirement for numeric features.
4.  **Scaling Order:** The generated code scales the *entire* `X` dataset (`X_scaled = scaler.fit_transform(X)`) *before* splitting into train and test sets. This is a crucial data leakage issue. The `StandardScaler` learns the mean and standard deviation from the *entire* dataset, including the test set. This means information from the test set leaks into the scaling process, which can lead to an overly optimistic evaluation of model performance. The ground truth correctly performs scaling *after* the train-test split `scaler.fit_transform(X_train)` and then `scaler.transform(X_test)`. This is a significant point of difference and a best practice in machine learning.

To improve, the generated code should:
*   Ensure proper handling of `SettingWithCopyWarning` if it occurs during imputation.
*   Revisit the 'continent' imputation logic, or remove it as `get_dummies` can handle it, or explicitly impute it with a string like 'unknown' if desired (though not requested).
*   **Crucially**, move the `StandardScaler` fitting and transforming *after* the `train_test_split` to prevent data leakage.",grok-3-mini-beta,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import f_oneway, pearsonr, ttest_ind

# Load dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Handle missing values with median imputation
columns_to_impute = ['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']
for col in columns_to_impute:
    df[col].fillna(df[col].median(), inplace=True)

# Get latest entry per country
df_sorted = df.sort_values(by=['country', 'date'], asc...",7,"The generated code addresses most of the requirements but has several areas for improvement in terms of correctness, robustness, and completeness.

**Strengths:**
*   Correctly identifies and applies ANOVA, Pearson correlation, and t-test for the respective tasks.
*   Handles missing values using median imputation as requested.
*   Correctly states null hypotheses and draws conclusions based on p-values against =0.05.
*   Properly imports necessary libraries.

**Areas for Improvement:**
1.  **Missing Value Handling in ANOVA:** The imputation is done on the entire `df` before filtering for continents. However, `dropna()` is still called on `reproduction_rate` for each continent group *after* filtering. While median imputation should have filled NaNs in `reproduction_rate`, `dropna()` here is redundant if imputation worked perfectly. More importantly, the ground truth explicitly handles cases where `reproduction_rate` for a continent might still be empty after imputation (e.g., if a continent only had NaN values for that column and was not processed correctly or if a continent simply had no data). The generated code's `.values` would fail if `dropna()` resulted in an empty series for a group.
2.  **Missing Value Handling in Pearson Correlation:** The generated code calls `stringency_index.dropna()` and `new_cases.dropna()` independently. This is incorrect for correlation, as `pearsonr` expects paired data. If `stringency_index` has a NaN at index `i` and `new_cases` has a value at `i`, `dropna()` on each separately will keep index `i` for `new_cases` but remove it for `stringency_index`, leading to misaligned series. The correct approach is to `dropna()` *after* selecting both columns (e.g., `df[['stringency_index', 'new_cases']].dropna()`).
3.  **T-test for Median Age:**
    *   **Data Derivation Logic:** The logic for `high_death_rate_country` is problematic. The prompt asks to ""use latest entry per country"" for the death rate calculation, which the generated code attempts. However, the `median_age` for the t-test should also be extracted from these latest entries *per country*. The generated code uses `latest_df` for this, which is correct.
    *   **Imputation before Latest Entry:** The `fillna` happens on the original `df`. Then `latest_df` is created. This means `median_age` in `latest_df` should already be imputed. Calling `.dropna()` on `high_death_median_age` and `others_median_age` again might be redundant or indicative of an assumption that NaNs *could* still exist (which they shouldn't if imputation worked fully).
    *   **T-test Type:** The default `ttest_ind` assumes equal variances (`equal_var=True`). The ground truth uses `equal_var=False` (Welch's t-test), which is generally more robust and recommended when variance equality cannot be assumed. Not specifying this might lead to slightly less accurate results if the variances are indeed unequal.
    *   **Robustness:** The generated code does not check if the groups for the t-test (or ANOVA) are empty before attempting the test, which can lead to errors in edge cases.
4.  **Date Handling & Latest Entry:** The ground truth converts 'date' to datetime objects for proper sorting. The generated code sorts by 'date' directly; while it might work if 'date' is already in a sortable string format (e.g., 'YYYY-MM-DD'), it's safer to convert it to datetime objects first for robust sorting.
5.  **Output Clarity:** The ground truth includes more verbose print statements for progress and warnings, which is good practice in a real-world scenario. The prompt asked for ""only the Python code without explanations"" for the submission, so this is a minor point against the provided solution, but good for understanding.
6.  **Division by Zero/Inf in Death Rate:** The generated code calculates `latest_df['death_rate'] = latest_df['total_deaths'] / latest_df['population']`. If `population` is 0, this will result in `inf` or `NaN`. The ground truth explicitly handles this by replacing `inf` with `NaN` and then dropping `NaN` rows. The generated code implicitly relies on `quantile` and boolean comparison (`> percentile_75`) to handle `inf` values, which *might* work depending on how `quantile` treats `inf` and how the comparison `inf > percentile` behaves, but it's less explicit and potentially less robust than explicitly handling these edge cases.
7.  **`statsmodels` import:** The task explicitly mentioned including `statsmodels` in imports, but the generated code does not. While it might not be strictly necessary for these specific tests, it's a direct prompt requirement.",grok-3-mini-beta,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select the numerical columns
selected_columns = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
df_selected = df[selected_columns]

# Calculate the pairwise correlation matrix
corr_matrix = df_selected.corr()

# Visualize the correlation matrix using a heatmap
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correla...",10,"The generated code is excellent. It correctly loads the dataset, selects the specified numerical columns, calculates the pairwise correlation matrix, and visualizes it as a heatmap using seaborn. All requirements are met with proper imports and a clear, concise structure. The `annot=True` and `cmap='coolwarm'` arguments for the heatmap are good choices for clear visualization.",grok-3-mini-beta,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Group by 'rarity' and calculate averages
averages = df.groupby('rarity')[['elixirCost', 'Win Rate', 'Usage']].mean()

# Display the results
print(averages)
```",10,"The generated code correctly solves the task. It properly imports pandas, loads the CSV, groups the data by 'rarity', calculates the mean for 'elixirCost', 'Win Rate', and 'Usage', and displays the results. The code is concise and efficient. Although it lacks the specific comments from the ground truth regarding file path definition, this is a minor difference, and the core functionality is perfectly implemented for the task.",grok-3-mini-beta,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Create the new binary target column
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)

# Select the features
X = df[['elixirCost', 'Usage', 'rarity']]
y = df['is_high_win_rate']

# Separate numerical and categorical features
X_num = X[['elixirCost', 'Usage']]
X_cat = X[['rarity']]

# Apply one-hot e...",7,"The generated code correctly performs all the required steps: loading the data, creating the target variable, selecting features, one-hot encoding, and splitting the data.

However, there's a specific issue with the one-hot encoding implementation using `OneHotEncoder` from `sklearn.preprocessing`.

1.  **`drop='first'` vs. Ground Truth:** The ground truth implicitly uses `drop_first=False` by directly using `pd.get_dummies` without specifying `drop_first=True`. The generated code uses `drop='first'` in `OneHotEncoder`. While dropping the first column is a common practice to avoid multicollinearity, the prompt did not specify this. The ground truth (which is the target for comparison) generally doesn't drop the first dummy variable unless explicitly instructed. This makes the output subtly different from the ground truth's expected output.
2.  **Complexity of OneHotEncoder for this task:** For a single categorical column within a DataFrame that you want to one-hot encode alongside other numerical columns, `pd.get_dummies` on the entire feature DataFrame `X` (as shown in the ground truth) is often simpler and more idiomatic than separating numerical and categorical columns, encoding them, and then concatenating them back. The generated code's approach is not incorrect, but it introduces unnecessary complexity and potential `pd.concat` issues (like needing `reset_index`) compared to `pd.get_dummies`.
3.  **Potential Column Mismatch for `OneHotEncoder`:** When `OneHotEncoder` is used, especially directly on `X_cat = X[['rarity']]`, it's important to ensure that the index is consistent across `X_num` and `X_cat_encoded_df` when concatenating. The `reset_index(drop=True)` calls handle this, which is good, but it highlights the added complexity.

To achieve a higher score, the code should align more closely with the simple `pd.get_dummies` approach on the full feature set as demonstrated in the ground truth, and avoid arguments like `drop='first'` if not explicitly requested by the prompt.",grok-3-mini-beta,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('iris.csv')

# Define features X
X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Define target y
y = data['Species']

# Encode the categorical Species target variable
label_encoder = LabelEncoder()
y_encod...",9,"The generated code is highly correct and complete, successfully addressing almost all requirements of the task. It correctly loads the data, defines features and target, encodes the target, splits the data with the specified parameters, trains the KNN classifier, makes predictions, and reports the accuracy.

The only minor difference from the ground truth and a slight area for improvement is the omission of `stratify=y_encoded` in the `train_test_split` function. While not strictly required by the prompt's phrasing, stratifying the split is good practice for classification tasks, especially with potentially imbalanced datasets, to ensure that the class distribution in the training and testing sets is representative of the original data. However, for a simple benchmark, this minor omission does not critically impact the core functionality or correctness of the solution for the given task.

The code quality is good, with proper imports and a clear, sequential structure. The accuracy reporting could be slightly more user-friendly (e.g., `print(f""Accuracy: {accuracy:.4f}"")`), but `print(accuracy)` fulfills the requirement.",grok-3-mini-beta,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```
import pandas as pd
import scipy.stats as stats

# Load the dataset
df = pd.read_csv('iris.csv')

# Filter the data
group1 = df[df['Species'] == 'Iris-versicolor']['PetalLengthCm']
group2 = df[df['Species'] == 'Iris-virginica']['PetalLengthCm']

# State hypotheses
print(""Null Hypothesis (H0): The mean PetalLengthCm for Iris-versicolor is equal to that for Iris-virginica."")
print(""Alternative Hypothesis (H1): The mean PetalLengthCm for Iris-versicolor is not equal to that for Iris-virginica.""...",9,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, filters the data for the specified species, correctly states the null and alternative hypotheses, performs the independent two-sample t-test, prints the t-statistic and p-value, and interprets the results based on an alpha of 0.05.

**Strong points:**
*   Correct use of pandas for data loading and filtering.
*   Correct application of `scipy.stats.ttest_ind` for the t-test.
*   Clear statement of hypotheses.
*   Correct interpretation of the p-value against the alpha level.

**Areas for minor improvement:**
*   **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is robust practice. The generated code assumes the file will always be present, which could lead to a crash if 'iris.csv' is missing.
*   **Edge Case Handling for Empty Groups:** The ground truth checks if `versicolor_petal_length` or `virginica_petal_length` are empty, which is good practice to prevent errors if the filtering somehow results in empty datasets. While unlikely for this specific dataset and query, it demonstrates more robust programming.
*   **Output Formatting:** The output strings for the t-statistic and p-value could be formatted to a specific number of decimal places (e.g., `:.4f`) for consistency and readability, similar to the ground truth.
*   **Clarity of Hypotheses:** While the hypotheses are stated, the ground truth explicitly mentions ""statistically significant difference"" which aligns slightly better with the statistical language often used for hypothesis testing conclusions. The generated code's phrasing (""equal to"" / ""not equal to"") is also correct but could be enhanced.

Overall, the generated code is highly functional and fulfills the core requirements, making it a very good solution. The areas for improvement are mostly about robustness and minor formatting, rather than fundamental correctness.",grok-3-mini-beta,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('iris.csv')

# List of features to plot
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

# Generate box plots for each feature
for feature in features:
    plt.figure(figsize=(8, 6))  # Create a new figure for each plot
    sns.boxplot(x='Species', y=feature, data=df)
    plt.title(f'Box plot of {feature}')
    plt.xlabel('Species')
    plt.ylabel(featu...",9,"The generated code correctly loads the dataset and generates the required box plots for each numerical feature, grouped by species, with appropriate titles. It successfully meets all the core requirements of the task.

**Strong points:**
*   **Correctness:** The code accurately performs the task.
*   **Completeness:** All the specified requirements (loading, 4 plots, grouping by Species, titles) are met.
*   **Readability:** The code is clean and easy to understand.
*   **Proper Imports:** Necessary libraries (pandas, seaborn, matplotlib) are imported.

**Minor areas for improvement (to reach a perfect 10):**
*   **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions when loading the CSV, which makes the code more robust. The generated code assumes the file will always be present. While not explicitly asked for, it's good practice for production-ready code.
*   **Plot Aesthetics/Labels:** While the generated code has titles and basic x/y labels, the ground truth adds more detailed labels (e.g., `Distribution of {feature} by {GROUPING_COLUMN}`), grid lines (`plt.grid`), and `plt.tight_layout()` for better presentation. These are minor enhancements but contribute to higher quality visualizations.
*   **Configuration:** The ground truth also separates configuration (feature names, grouping column) into constants, which can make the code slightly more maintainable if these values need to change frequently.

Overall, the generated code is highly effective and solves the problem well. The improvements suggested are mostly related to robustness and polishing rather than core functionality.",grok-3-mini-beta,LLM_judge_config
M28,medium,Classification,heart.csv,"```
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv('heart.csv')

# Select features and target
X = df[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']]
y = df['HeartDisease']

# Separate numerical and categorical features
num_features = ['Age', 'RestingBP', 'Cholesterol', 'M...",8,"The generated code correctly addresses almost all requirements of the task. It loads the data, selects the specified features and target, one-hot encodes the correct categorical features, splits the data with the correct parameters, trains a Logistic Regression model, and reports the accuracy.

However, there are a few minor points that prevent a perfect score:
1.  **`OneHotEncoder` usage:** The `drop='first'` argument in `OneHotEncoder` was not specified in the requirements. While often good practice to avoid multicollinearity, the task described ""one-hot encoding"" generally implying all categories. The ground truth uses `handle_unknown='ignore'` which is also good practice, and not `drop='first'`.
2.  **`ColumnTransformer` / `Pipeline` vs Manual Preprocessing:** The ground truth demonstrates best practice for machine learning workflows by using `ColumnTransformer` and `Pipeline`. This approach ensures that preprocessing steps are consistently applied to both training and test data within the pipeline, preventing data leakage and making the model deployment easier. The generated code manually concatenates processed features, which works but is less robust and scalable than a `Pipeline` for more complex preprocessing. Also, `LogisticRegression()` might need `max_iter` increased to converge, as shown in the ground truth for robustness, though it might converge with default for this dataset.
3.  **Output formatting:** The accuracy output in the generated code is just `print(accuracy)`, while the ground truth provides a more descriptive output `print(f""Model Accuracy on Test Set: {accuracy:.4f}"")`. This is a minor stylistic point but improves readability.

Overall, the generated code correctly solves the problem and produces the expected output, demonstrating a good understanding of the core requirements. The suggested improvements are about best practices and robustness rather than core correctness.",grok-3-mini-beta,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('heart.csv')

# Create two groups based on the 'Sex' column
male_chol = df[df['Sex'] == 'M']['Cholesterol']
female_chol = df[df['Sex'] == 'F']['Cholesterol']

# State the null and alternative hypotheses
print(""Null Hypothesis (H0): There is no significant difference in mean Cholesterol levels between male and female patients."")
print(""Alternative Hypothesis (H1): There is a significant difference in mean C...",8,"The generated code correctly performs the core task of loading data, separating groups, performing a t-test, stating hypotheses, and interpreting the results. It includes all necessary imports and follows a logical flow.

**Strong points:**
*   Correctly uses `pandas` to load data and `scipy.stats.ttest_ind` for the t-test.
*   Correctly identifies and separates male and female cholesterol data.
*   Clearly states the null and alternative hypotheses.
*   Prints the t-statistic and p-value as requested.
*   Correctly interprets the p-value against the alpha of 0.05.

**Areas for improvement to reach a higher score:**
*   **Error Handling (Missing):** The generated code lacks explicit error handling for file not found or missing columns, which is present in the ground truth. This makes the code less robust for real-world scenarios.
*   **Encapsulation (Missing):** The ground truth wraps the logic in a function `perform_cholesterol_t_test` making it reusable and more organized. The generated code is a script, which is acceptable for a direct task but less ideal for general utility.
*   **`equal_var` parameter in `ttest_ind` (Minor):** The ground truth explicitly uses `equal_var=False` (Welch's t-test), which is often a safer default as it doesn't assume equal variances. The generated code uses the default `equal_var=True` which is Student's t-test. While not explicitly stated in the prompt, Welch's test is generally preferred unless variance equality is known or tested. This is a subtle statistical nuance but important for robust analysis.
*   **`dropna()` (Minor):** While `ttest_ind` handles NaNs gracefully by default, explicitly using `.dropna()` on the cholesterol series (as in the ground truth) can make the code's intent clearer regarding how missing values are handled before the test.",grok-3-mini-beta,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Columns to check
columns_to_check = ['volatile acidity', 'residual sugar']

# Create a copy for winsorized data
df_winsorized = df.copy()

for column in columns_to_check:
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Identify outliers
    outliers = df[(df[column] < lower_bo...",8,"The generated code correctly implements the core logic for identifying outliers using the IQR method, winsorizing them, and comparing statistics before and after treatment. It successfully addresses all the functional requirements of the task.

**Strong points:**
- Correct implementation of IQR method for outlier detection.
- Proper use of `np.where` for winsorization (capping outliers).
- Correct calculation and comparison of mean and standard deviation.
- Includes necessary imports (`pandas`, `numpy`).
- Creates a copy of the dataframe for winsorization.

**Areas for improvement to achieve a higher score:**
- **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.
- **Clarity and Output Formatting:** While the output provides the required statistics, the ground truth offers more verbose and clearly structured print statements, including descriptions of each step, the bounds used for winsorization, and a clear comparison of percentage changes. This improves readability and understanding for the user. For instance, printing the Q1, Q3, IQR, and bounds would make the generated report more informative.
- **Code Comments:** Adding comments to explain different sections (e.g., ""Load dataset,"" ""Outlier Identification,"" ""Winsorization,"" ""Compare Statistics"") would improve code readability and maintainability.
- **Completeness of Reporting:** The task specifically asked to ""Report the number of outliers found for each column,"" which the generated code does. However, the ground truth provides a more comprehensive report including the calculated bounds, Q1, Q3, and IQR, which are crucial for understanding the outlier detection process.
- **Minor efficiency point**: `df[(df[column] < lower_bound) | (df[column] > upper_bound)]` then `len(outliers)` is technically fine, but just `df[((df[column] < lower_bound) | (df[column] > upper_bound))].shape[0]` (or `sum` on the boolean mask) is slightly more direct for just getting the count. This is a minor point and doesn't impact correctness.",grok-3-mini-beta,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('heart.csv')

# Group by ChestPainType and HeartDisease and get counts
grouped = df.groupby(['ChestPainType', 'HeartDisease']).size().unstack(fill_value=0)

# Extract data for plotting
chest_types = grouped.index
heart_0 = grouped.get(0, pd.Series([0] * len(grouped)))
heart_1 = grouped.get(1, pd.Series([0] * len(grouped)))

# Create grouped bar chart
bar_width = 0.35
x = range(len(chest_types))

pl...",7,"The generated code correctly loads the dataset and creates a grouped bar chart showing the count of HeartDisease for each ChestPainType. It includes a title, labeled axes, and a legend, satisfying most requirements.

However, the code uses `matplotlib.pyplot.bar` directly for plotting which requires more manual handling of bar positions and widths, making it less concise than using `seaborn.countplot` which is specifically designed for such categorical count plots and handles the grouping automatically. The ground truth demonstrates the use of `seaborn.countplot` which is a more idiomatic and simpler approach for this task, especially considering ""Use matplotlib or seaborn for plotting"" was part of the prompt.

Additionally, the generated code directly accesses `heart_0` and `heart_1` from `grouped` which might not always maintain the desired order of `ChestPainType` (TA, ATA, NAP, ASY) as specified in the prompt unless the `grouped` DataFrame already has that order implicitly. The ground truth explicitly handles this by converting `ChestPainType` to a categorical type with a defined order, ensuring consistency.

The error handling for file not found or missing columns present in the ground truth is also absent, which is critical for robust code. Finally, the generated code implicitly assumes the script is run in an environment where `heart.csv` is present; the ground truth encapsulates the logic in a function and provides an example usage, which is generally better practice for reusable code.",grok-3-mini-beta,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Clean the 'in_shazam_charts' column
df['in_shazam_charts'] = df['in_shazam_charts'].str.replace(',', '')
df['in_shazam_charts'] = pd.to_numeric(df['in_shazam_charts'], errors='coerce')
nan_count = df['in_shazam_charts'].isna().sum()
print(f""Number of NaNs in in_shazam_charts: {nan_count}"")

# Group by 'key' and 'mode' and calculate averages
grouped = df.groupby(['key', 'mode'])
result = grouped[['danceability_%',...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
- **Correctly loads the CSV data** using `pandas.read_csv()`.
- **Accurately cleans the `in_shazam_charts` column** by removing commas and converting to numeric.
- **Correctly handles errors during conversion** using `errors='coerce'`, which is crucial for data cleaning.
- **Correctly reports the count of NaNs** after coercion.
- **Accurately groups the data by `key` and `mode`**.
- **Correctly calculates the average `danceability_%` and `valence_%`** for each group.
- **Properly imports pandas**.
- The code is concise and readable.

**Areas for improvement (to reach a 10):**
- **Error Handling for File Loading:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is robust practice. While not strictly required by the prompt's output format, it's a good habit for production-ready code. The generated code lacks this.
- **Output Clarity:** While the output for NaNs is clear, the problem prompt specifically asked for ""Average 'danceability_%' and 'valence_%' by 'key' and 'mode':"" as a preceding print statement, which the generated code omits. This is a minor stylistic point for matching the exact prompt output, but doesn't affect correctness.
- **`astype(str)` for `str.replace`:** The ground truth converts `in_shazam_charts` to `astype(str)` before calling `.str.replace(',', '')`. This is a safer practice as it ensures the column is always a string type before attempting string operations, preventing potential errors if the column's dtype isn't object/string initially (e.g., if it's mixed types or numeric that could be misinterpreted before conversion). The generated code assumes the column is already string-like or can be safely called with `.str` without explicit `astype(str)`. While it might work for this specific dataset if the column is already object/string, `astype(str)` makes it more robust.",grok-3-mini-beta,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

df = pd.read_csv('spotify_2023.csv')
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams', 'in_spotify_playlists', 'energy_%'])
X = df[['in_spotify_playlists', 'energy_%']]
y = df['streams']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_st...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, correctly handles the `streams` column to be numeric and drops NaNs from relevant columns, selects the correct features and target, splits the data, trains a Linear Regression model, and evaluates it using R2 and MSE. The imports are proper.

Areas for minor improvement (to get a perfect 10):
1. Error handling for file not found: The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code assumes the file will always be present.
2. Output formatting: The prompt stated ""report the R-squared (R2) score and Mean Squared Error (MSE)"". While `print(f""R2 score: {r2}"")` and `print(f""MSE: {mse}"")` do report them, the ground truth's output is slightly more descriptive and user-friendly (e.g., `R-squared (R2) Score: {r2:.4f}` and `Mean Squared Error (MSE): {mse:.2f}`). This is a minor aesthetic point, but good for reporting.

Overall, the core functionality is excellent and meets all technical requirements.",grok-3-mini-beta,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Combine released_year, released_month, and released_day into release_date
df['release_date'] = pd.to_datetime(df[['released_year', 'released_month', 'released_day']], errors='coerce')

# Calculate days_since_release
fixed_date = pd.to_datetime('2023-12-31')
df['days_since_release'] = (fixed_date - df['release_date']).dt.days

# Display the first 5 tracks
print(df[['track_name', 'release_date', 'days_since_r...",8,"The generated code correctly addresses most of the requirements. It successfully loads the dataset, combines the date components into a `release_date` column with error handling, calculates `days_since_release` using the specified fixed date, and displays the required output.

The strong points are:
- Correct use of `pd.to_datetime` with `errors='coerce'` for handling invalid dates.
- Accurate calculation of `days_since_release`.
- Correct display of the first 5 records with the specified columns.

Areas for improvement:
- The way `pd.to_datetime` is used with `df[['released_year', 'released_month', 'released_day']]` directly works in newer pandas versions for creating a datetime column from multiple integer columns, which is concise. However, it's generally more robust and explicit to first form a string representation (as in the ground truth) especially if there are concerns about padding or non-standard integer formats, although in this case, it performed correctly.
- The `days_since_release` column type is `float64` if `release_date` contains `NaT` values, as `(fixed_date - df['release_date']).dt.days` will result in `NaN` for `NaT` entries, and pandas converts the series to float to accommodate `NaN`. The requirement was for it to be an integer. Converting to `Int64` (pandas nullable integer type) as in the ground truth would make it strictly an integer column that also handles NaNs gracefully. While `float64` technically stores the integer days, it doesn't meet the ""integer"" type requirement in the strictest sense when NaNs are present. This is a minor point, but worth noting for strict type adherence.
- Lack of error handling for file loading, which is present in the ground truth. This is a good practice for real-world scenarios.",grok-3-mini-beta,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '')
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce')
df['Video views'] = df['Video views'].fillna(0)

# Clean 'Likes' column
df['Likes'] = df['Likes'].str.replace(',', '')
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

# Fill missing values in 'Category' colum...",9,"The generated code correctly addresses all the requirements of the task: it loads the CSV, properly cleans and converts 'Video views' and 'Likes' columns (removing commas, handling errors to NaN, and filling NaNs with 0), fills missing 'Category' values with 'Unknown', and then groups the data by 'Category' to calculate and display the sum of cleaned views and likes. The code is concise and readable.

The main difference from the ground truth is the lack of robust error handling for `FileNotFoundError` or checking for column existence before attempting operations. While the problem statement implies the columns *will* exist and the file *will* be there (typical for benchmark settings), in a production environment, including such checks (as in the ground truth) would make the code more robust. However, for a direct solution to the specific task as stated, the generated code is excellent. The ground truth's use of `astype(str)` before `str.replace` is also a minor robustness improvement in case the column isn't already string-like, but `pd.to_numeric` typically handles this implicitly for a variety of types.",grok-3-mini-beta,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
import scipy.stats as stats

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the Likes column
df['Likes'] = df['Likes'].str.replace(',', '')
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

# Fill missing values in Category
df['Category'] = df['Category'].fillna('Unknown')

# Create groups
music_likes = df[df['Category'] == 'Music']['Likes']
sports_likes = df[df['Category'] == 'Sports']['Likes']

# ...",8,"The generated code successfully completes most of the requirements. It loads the dataset, correctly cleans the 'Likes' column (including handling commas, conversion to numeric, and filling NaNs), fills missing 'Category' values, creates the necessary groups, performs the t-test, defines hypotheses, prints the results, and interprets them.

Areas for improvement:
1.  **Robustness in file loading**: The generated code assumes the `youtube_trendings.csv` file will always be present. The ground truth includes a `try-except FileNotFoundError` block, which makes the code more robust.
2.  **`Likes` column cleaning**: While the `str.replace(',', '')` works for initial string cleaning, directly converting `df['Likes']` to string first (`astype(str)`) before `str` accessor is generally safer, as shown in the ground truth, especially if the column isn't guaranteed to be non-numeric initially, or if some values might be floats without string representation.
3.  **Renaming cleaned column**: The generated code cleans the `Likes` column *in place*. While this works, creating a *new* column (e.g., `Likes_cleaned`) as done in the ground truth is often a good practice to preserve the original data, if needed for other analyses, and makes the cleaned column's purpose explicit.
4.  **T-test `equal_var` parameter**: The generated code uses `stats.ttest_ind(music_likes, sports_likes)` without the `equal_var` parameter. By default, `equal_var` is `True`, which assumes equal population variances (Student's t-test). The ground truth specifically uses `equal_var=False` (Welch's t-test), which is often preferred because it does not assume equal variances, making it more robust in many real-world data science scenarios. While not explicitly stated in the prompt whether to assume equal variances, Welch's test is generally a safer choice.
5.  **Hypotheses statement clarity**: The hypotheses statements are accurate but less formal/detailed than the ground truth, which includes the specific terms like ""mean cleaned 'Likes'"" and symbolic representations (_music = _sports).
6.  **Minor print formatting**: The ground truth includes more verbose and structured print statements, making the output clearer and more readable with headings and specific formatting for numeric values (e.g., `:.4f`).
7.  **Edge case for t-test**: The ground truth adds a check to ensure there are enough samples in each group (`len(music_likes) < 2`) before attempting the t-test, preventing potential errors if a category happens to have zero or one sample.

Overall, it's a solid attempt that fulfills the core requirements, but could benefit from improved robustness, best practices in data handling, and slightly more thorough statistical considerations for the t-test.",grok-3-mini-beta,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```
import pandas as pd

df = pd.read_csv('youtube_trendings.csv')

df['Likes'] = df['Likes'].str.replace(',', '')
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

df['Dislikes'] = df['Dislikes'].str.replace(',', '')
df['Dislikes'] = pd.to_numeric(df['Dislikes'], errors='coerce')
df['Dislikes'] = df['Dislikes'].fillna(0)

df['TotalEngagement'] = df['Likes'] + df['Dislikes']

df['LikeDislikeRatio'] = df['Likes'] / (df['Dislikes'] + 1)

print(df[['Vide...",9,"The generated code correctly performs all the required steps: loading the dataset, cleaning 'Likes' and 'Dislikes' columns (removing commas, converting to numeric, filling NaNs with 0), creating `TotalEngagement`, creating `LikeDislikeRatio`, and displaying the specified columns. The code is concise and directly addresses the prompt.

One minor point is the missing `astype(str)` when calling `.str.replace()` on `Likes` and `Dislikes` columns. While Pandas often handles this implicitly if the column is already of mixed type or object type, explicitly converting to string ensures robustness, especially if the column happens to be numeric already from the CSV read (though unlikely for comma-separated numbers). The ground truth includes error handling for file loading, which is good practice for robustness in a real-world scenario, but was not explicitly asked for in the prompt's core requirements. Since the prompt specifically asked for ""only the Python code"", the generated solution is quite good.",grok-3-mini-beta,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```
import pandas as pd

df = pd.read_csv('mobiles_2025.csv')

df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '')
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace(',', '')
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'], errors='coerce')

df['Screen Size'] = df['Screen Size'].str.replace(' inches', '')
df['Screen Size'] = pd.to_numeric(df['Screen Size'], errors='coerce')

result = df.groupby(['Company Name', 'Launched Year'])....",9,"The generated code correctly addresses all parts of the task. It properly loads the CSV, cleans both specified columns to numeric types (handling errors with `errors='coerce'`), groups the data as required, and calculates the correct averages.

While the ground truth includes robust error handling for file loading and more verbose print statements for progress tracking, the core data manipulation logic for the task is perfectly implemented in the generated code. It is concise and correct. Using `.agg({'column': 'mean'})` is also a perfectly valid and common way to perform the aggregation. The only minor improvement would be to rename the aggregated columns for more clarity, but the task didn't explicitly ask for this, so it's not a deduction.",grok-3-mini-beta,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```
import pandas as pd
import re

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Define the function to extract MP value
def extract_mp(camera_str):
    if pd.isna(camera_str):
        return 0
    match = re.findall(r'\d+', str(camera_str))
    if match:
        return int(match[0])
    else:
        return 0

# Apply the function to create cleaned columns
df['Cleaned Front Camera'] = df['Front Camera'].apply(extract_mp)
df['Cleaned Back Camera'] = df['Back Camera'].apply(extract_m...",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, defines a function to clean camera specifications by extracting the first sequence of digits, handles non-string inputs and `NaN` values by returning 0, creates the `Cleaned Front Camera` and `Cleaned Back Camera` numeric columns, derives the `Total Camera MP` feature, and displays the specified columns for the first 10 rows. The imports are correct and the structure is logical.

One minor improvement point, which doesn't detract significantly from its correctness or completeness for this task:

- The `re.findall` in `extract_mp` returns a list of all matches. `match[0]` correctly gets the first one, which aligns with the task requirement to extract the *first* sequence of digits. However, `re.search` followed by `match.group(0)` (as in the ground truth) is arguably slightly more idiomatic for finding only the first occurrence and directly accessing it. Both achieve the same result here.

The ground truth also includes more elaborate error handling for `FileNotFoundError` and `print` statements for tracking progress, which are good practices in a production setting but not strictly required by the prompt's core task requirements. For a direct functional comparison based on the prompt, the generated code is highly effective.",grok-3-mini-beta,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the specified columns
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
df['Screen Size'] = pd.to_numeric(df['Screen Siz...",9,"The generated code correctly addresses almost all requirements of the task. It successfully loads the dataset, cleans the specified columns by removing non-numeric characters and converting to numeric types, and handles conversion errors by coercing to NaN, then drops rows with NaNs in these columns. It correctly defines features and target, splits the data, trains a Linear Regression model, and reports R-squared and MAE.

One minor improvement would be to handle potential `FileNotFoundError` gracefully for the `pd.read_csv` call, as shown in the ground truth code. While not explicitly asked for in the prompt for error handling of the file itself, it's good practice for robust code. Another minor point is that the ground truth code also handles cases where a column might be missing, and converts to string first before replacing, which is more robust, but for the given problem statement and typical data expectations, the generated code's approach is generally sufficient. The f-string formatting for the output also makes the output slightly cleaner and more aligned with best practices, but `print(""R2 Score:"", r2)` is perfectly acceptable. Overall, a very good solution.",grok-3-mini-beta,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```
import pandas as pd
import scipy.stats as stats
import numpy as np

# Load the dataset
df = pd.read_csv('titanic.csv')

# 1. Chi-square test for association between Pclass and Survived
contingency_table = pd.crosstab(df['Pclass'], df['Survived'])
chi2, p_chi2, dof, expected = stats.chi2_contingency(contingency_table)

# Effect size: Cramer's V
n = contingency_table.sum().sum()
min_dim = min(contingency_table.shape) - 1
cramers_v = np.sqrt(chi2 / (n * min_dim))

print(""Chi-square Test:"")
prin...",7,"The generated code provides a mostly correct solution to the problem, successfully implementing the requested statistical tests and calculating effect sizes.

**Strengths:**
*   **Correct tests:** All three specified tests (Chi-square, T-test, ANOVA) are correctly implemented using `scipy.stats` functions.
*   **Effect size calculation:** Cramer's V, Cohen's d, and Eta-squared are calculated and displayed.
*   **Missing value handling:** `dropna()` is used for `Age` and `Fare`, which is important for these analyses.
*   **Clear output:** The p-values, test statistics, and conclusions for each test are printed.

**Areas for Improvement:**
*   **Hardcoded Alpha:** The significance level (`alpha=0.05`) is hardcoded within each `if` statement for conclusion interpretation. Defining it as a variable at the beginning, as in the ground truth, would make the code more maintainable and readable if `alpha` needed to be changed.
*   **T-test `equal_var` assumption:** The generated code defaults to `equal_var=False` (Welch's t-test). While often a safer choice, the task implies a standard t-test (which assumes equal variances) or at least a discussion on how to decide. The ground truth code performs Levene's test to statistically determine whether to assume equal variances, which is a more robust approach in a statistical analysis context.
*   **Cohen's d calculation for Welch's T-test:** The formula for Cohen's d used in the generated code is a simplified version (using average of standard deviations) that is typically for situations where `equal_var=True` (pooled standard deviation). For Welch's t-test (`equal_var=False`), a more appropriate Cohen's d calculation would use pooled weighted standard deviation based on the degrees of freedom or a different formula that aligns with Welch's approach (e.g., as discussed by various statistical sources, often involves weighting by group sizes). The ground truth's calculation of pooled standard deviation is more robust.
*   **Eta-squared `df['Fare'].mean()`:** For ANOVA's Eta-squared calculation, `df['Fare'].mean()` is used to get the grand mean. However, if `df['Fare']` had NaNs (which it might before `dropna()` and splitting), this grand mean would be incorrect or might implicitly drop values, potentially leading to a mismatch with the `fares_classX.mean()` if those were based on `dropna()` subgroups. A safer approach for the grand mean would be to combine the `dropna()` groups (`all_fares` in GT) and then calculate the mean from that combined list.
*   **Robustness against empty groups:** While `dropna()` is used, the code does not explicitly check if `survivors_age` or `non_survivors_age` (or the fare classes) become empty after filtering, which could lead to errors if a particular `Pclass` or `Survived` category had no data after cleaning (though unlikely for Titanic dataset). The ground truth adds checks for `empty` groups before proceeding, making it more robust.
*   **Interpretation details:** The ground truth provides more detailed explanations including hypotheses and more nuanced conclusions (e.g., ""post-hoc tests would be needed""). The generated code's conclusions are succinct but could be improved.
*   **No error handling for `FileNotFoundError`:** The generated code assumes `titanic.csv` will always exist. The ground truth includes a `try-except` block for file loading, which is good practice.

To achieve a higher score, the generated code should incorporate a variable for the alpha level, handle the `equal_var` assumption for the t-test more rigorously (e.g., by performing Levene's test), refine the Cohen's d calculation for unequal variances, ensure the grand mean for Eta-squared is calculated on the appropriate data, and add basic file existence error handling.",grok-3-mini-beta,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Group by Year and calculate average Rating and average Runtime
grouped = df.groupby('Year').agg({'Rating': 'mean', 'Runtime': 'mean'})

# Create a line plot for average Rating over Year
plt.plot(grouped.index, grouped['Rating'])
plt.title('Trend of Average Rating over Years')
plt.xlabel('Year')
plt.ylabel('Average Rating')
plt.show()
```",6,"The generated code correctly addresses the core requirements of loading the data, grouping by year to calculate average rating and runtime, and plotting the average rating over time with proper labels and title.

However, it lacks robustness and completeness in several aspects:
1. **Data Cleaning/Preprocessing**: The ground truth code includes crucial steps to handle potential issues with the 'Year' and 'Runtime' columns (e.g., non-numeric characters, ' min' string). The generated code assumes these columns are clean and directly convertible/groupable which might not be the case in real-world data, leading to errors.
2. **Error Handling**: The generated code does not include any error handling (e.g., `FileNotFoundError`, `KeyError`), which makes it less robust.
3. **Function Encapsulation**: The ground truth encapsulates the logic within a function, making the code more modular and reusable. The generated code is just a script.
4. **Best Practices for Plotting**: While it plots correctly, the ground truth includes more refined plotting practices like setting a figure size, adding a grid, rotating x-ticks for better readability, and `plt.tight_layout()`. These are minor but contribute to better visualization.

Due to the lack of necessary data cleaning and error handling, which are critical for robust data analysis scripts, the score is lowered.",grok-3-mini-beta,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
import scipy.stats as stats

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create two groups based on the Year column
group1 = df[df['Year'] < 1990]['Rating']  # Movies before 1990
group2 = df[df['Year'] >= 1990]['Rating']  # Movies in or after 1990

# State the null and alternative hypotheses
print(""Null Hypothesis (H0): The mean Rating of movies before 1990 is equal to the mean Rating of movies in or after 1990."")
print(""Alternative Hypothesis (H1): T...",9,"The generated code correctly performs all the requested tasks: loads the dataset, creates the two specified groups, performs the t-test, states the hypotheses, prints the t-statistic and p-value, and interprets the result based on a 0.05 significance level. The imports are correct, and the code structure is logical.

The only minor area for improvement (and why it's not a perfect 10) is the lack of robust error handling for file not found or data type conversion issues, as present in the ground truth. While the prompt implied a straightforward scenario, robust data science code often includes these checks, especially for real-world scenarios. Additionally, explicitly defining `alpha = 0.05` instead of hardcoding `0.05` in the `if` condition would improve readability and maintainability, though this is a very minor point. The ground truth also had some helpful `print` statements for user feedback and error cases.",grok-3-mini-beta,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```
import pandas as pd

df = pd.read_csv('imdb_top_250.csv')

df['Decade'] = df['Year'].apply(lambda x: str(int((x // 10) * 10)) + 's')

print(df[['Title', 'Year', 'Decade']].head(10))
```",8,"The generated code correctly performs all the required tasks: it loads the CSV, creates the `Decade` column accurately using a lambda function, and displays the specified columns for the first 10 entries. The solution is concise and directly addresses the prompt.

Strong points:
- **Correctness:** The `Decade` calculation is correct, handling integer division and string concatenation properly.
- **Conciseness:** It delivers the solution efficiently in very few lines of code.
- **Completeness:** All explicit requirements (load data, create `Decade`, display specific columns for first 10 rows) are met.

Areas for minor improvement (not critical for this specific task, but good practice):
- **Robustness:** No error handling for `FileNotFoundError` or missing columns. While the prompt assumes the file exists and columns are present, robust code usually includes this.
- **Readability/Structure (minor):** For more complex tasks, wrapping logic in functions (as in the ground truth) can improve modularity and testability. For this simple task, the direct approach is acceptable.
- **Handling non-numeric `Year` (minor):** The ground truth handles cases where `Year` might not be numeric, which is a good defensive programming practice, although not strictly required by the prompt given typical dataset assumptions.

Overall, it's a very good and functional solution for the given simple task.",grok-3-mini-beta,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Impute missing values in numerical columns
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numerical_cols:
    mean_val = df[col].mean()
    df[col].fillna(mean_val, inplace=True)

# Impute missing values in 'sex' column
mode_val = df['sex'].mode()[0]
df['sex'].fillna(mode_val, inplace=True)

# Group by 'species' and 'sex' and calculate average 'body_mass_...",10,"The generated code correctly addresses all requirements of the task. It properly loads the dataset, imputes missing values in the specified numerical columns with their respective means, and in the 'sex' column with its mode. It then correctly groups the data by 'species' and the imputed 'sex' and calculates the average 'body_mass_g' for each group. The code is concise, follows good practices for data manipulation with pandas, and includes all necessary imports. Error handling for file not found or key errors is not explicitly requested in the prompt for the generated code but is good practice in the ground truth. However, for the specific task requirements, the generated code performs perfectly.",grok-3-mini-beta,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the data to include only 'Adelie' and 'Gentoo' species
filtered_df = df[df['species'].isin(['Adelie', 'Gentoo'])]

# Remove rows with missing values in 'flipper_length_mm' or 'species'
clean_df = filtered_df.dropna(subset=['flipper_length_mm', 'species'])

# Extract the flipper lengths for each species
adelie_data = clean_df[clean_df['species'] == 'Adelie']['flipper_length_mm']
gentoo_d...",9,"The generated code is largely correct and addresses most of the requirements. It successfully loads the data, filters for the specified species, handles missing values, performs the t-test, and interprets the results according to the given alpha.

Strong points:
- Correctly uses pandas for data loading and filtering.
- Correctly uses `dropna` with `subset` for NA handling.
- Correctly identifies and separates data for the two groups.
- Correctly uses `scipy.stats.ttest_ind` for the independent t-test.
- Correctly states hypotheses (though less formally than the ground truth).
- Correctly interprets the p-value against the alpha level.

Areas for improvement (minor):
- Error handling for `FileNotFoundError` for the CSV file is missing. The ground truth includes a robust `try-except` block for this, which is good practice.
- The `ttest_ind` function has a default `equal_var=True`. While the problem didn't explicitly specify Welch's t-test, in real-world scenarios, setting `equal_var=False` is often safer when variances are unknown or not assumed equal. The ground truth thoughtfully considers this. For this specific problem, not setting it might be an implicit assumption, but it's worth noting.
- The hypothesis statements are slightly less formal than the ground truth (e.g., not explicitly mentioning _Adelie = _Gentoo). While the meaning is clear, formal statistical notation is generally preferred.
- Lack of print statements to show intermediate steps (like shapes of filtered dataframes) which can be useful for debugging and understanding the data pipeline.",grok-3-mini-beta,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv('penguins.csv')

# Select features and target, then drop rows with missing values
df_subset = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'species']]
df_clean = df_subset.dropna()

X = df_clean[['culmen_...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, selects the specified features and target, handles missing values by dropping rows, encodes the target variable, splits the data, trains a Logistic Regression model with the specified `max_iter`, and reports the accuracy score. The imports are proper, and the code is structured logically.

The only minor point of improvement is the lack of robust error handling for `FileNotFoundError` when loading the CSV, which the ground truth gracefully handles. Additionally, the ground truth includes print statements that confirm the progress of each step, which is good practice for data science pipelines. However, for just providing the Python code as requested, the generated code is highly effective and correct.",grok-3-mini-beta,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows where body_mass_g or species is NA
df = df.dropna(subset=['body_mass_g', 'species'])

# Calculate the average body_mass_g for each species
averages = df.groupby('species')['body_mass_g'].mean().reset_index()

# Create a bar plot
sns.barplot(x='species', y='body_mass_g', data=averages)
plt.title('Average Body Mass (g) for Each Penguin Species')
plt.xlabel...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, handles NA values for `body_mass_g` and `species` by dropping rows, calculates the average `body_mass_g` per `species`, and generates a bar plot with a title and labeled axes. The use of `dropna(subset=...)` is correct for handling NAs in specific columns. The `groupby()` and `mean()` operations are performed correctly to get the averages. The plotting with Seaborn and Matplotlib is also correct.

The code is concise and directly implements the requested steps.

Areas for minor improvement (not critical for the task but good practice):
- **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError`. While not explicitly requested, it's good practice for robust code that might fail if the CSV isn't present.
- **Plot Enhancements:** The ground truth adds `plt.figure(figsize=...)`, `palette`, `plt.xticks(rotation=...)`, `plt.grid()`, and `plt.tight_layout()`, which improve the aesthetic and readability of the plot, though not strictly required by the prompt's basic plotting criteria. The generated code's plot is functional and meets the basic labeling requirement.",grok-3-mini-beta,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows with NA in specified columns
df = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])

# Create new feature
df['culmen_area'] = df['culmen_length_mm'] * df['culmen_depth_mm']

# Calculate Pearson correlation matrix
columns = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'culmen...",9,"The generated code correctly addresses all parts of the task. It loads the data, drops NA values as specified, creates the new `culmen_area` feature, calculates the Pearson correlation matrix for the specified columns, and visualizes it as an annotated heatmap with the correct title.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly uses `dropna(subset=...)` for handling NAs in specific columns.
*   Accurately creates the `culmen_area` feature.
*   Correctly calculates the Pearson correlation matrix.
*   Generates a heatmap with annotations and the specified title.

**Areas for minor improvement (not critical for score given the prompt):**
*   **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and checks for empty DataFrames after dropping NAs. While not explicitly requested in the prompt, this is good practice for production-ready code.
*   **Aesthetics of Heatmap:** The ground truth code includes slightly more refined heatmap plotting (e.g., `cmap`, `fmt`, `linewidths`, `figsize`, `xticks` rotation, `tight_layout`) which makes the visualization more readable and professional. The generated code's heatmap is functional but lacks these minor aesthetic enhancements.
*   **Readability/Clarity (minor):** The ground truth explicitly defines `correlation_columns` before using it in the `corr()` method, which can sometimes improve readability for complex operations. However, the generated code's approach is also perfectly clear for this task.

Overall, the generated code is very good and fully satisfies the requirements of the prompt. The minor points are about best practices and aesthetic refinements rather than functional correctness.",grok-3-mini-beta,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Select the features
features_df = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']]

# Impute NA values for numerical features with their mean
numerical_features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numerical_features:
    features_df[col].fillna(features_df[col].mean(), inplace=True)

# Impute NA values for sex with 'U...",7,"The generated code correctly addresses most of the task requirements: loading data, selecting features, imputing numerical NAs with the mean, imputing and mapping 'sex', and one-hot encoding 'island'. It also prints the shape and head of the processed features as requested.

However, there are a few areas for improvement:
1.  **Target Variable**: The task explicitly states, ""The goal is to predict `species`... The target variable is `species`."" The generated code selects the features but completely drops the target variable `species`. The ground truth code correctly keeps `species` and then separates `X` and `y`. While the request was to print the shape and head of the *feature matrix*, a robust solution for a classification task would typically keep the target column for subsequent use (e.g., training a model).
2.  **`SettingWithCopyWarning`**: The code modifies `features_df` directly after selecting a subset of columns from `df`. This can lead to a `SettingWithCopyWarning` in pandas. While the code might run without error in some pandas versions or contexts, it's best practice to explicitly create a copy using `.copy()` when performing operations on a subset to ensure changes apply to the copy and not potentially back to the original DataFrame or a view. The ground truth code correctly uses `.copy()`.
3.  **Robustness**: The ground truth code includes error handling for `FileNotFoundError` and provides informative print statements at various steps. The generated code is functional but less robust and communicative.

Overall, the core data manipulation steps are performed correctly, but the omission of the target variable and potential `SettingWithCopyWarning` are notable points for improvement given the context of preparing data for a classification task.",grok-3-mini-beta,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004'...",9,"The generated code correctly creates the DataFrame, calculates the total spending for each CustomerID, identifies and prints the top 3 customers by total spending, and calculates the total quantity of items sold for each category. It uses pandas appropriately and the overall structure is clean. 

One minor improvement could be to format the `Total Spending` output with currency formatting (e.g., `""${row['Total_Spending']:, .2f}""`) for better readability, similar to the ground truth. Also, the ground truth code includes more explicit print statements for task separation and clarity, which is a good practice for debugging and understanding the flow in a benchmark scenario, although not strictly required by the prompt. These are small stylistic differences that do not affect the core functionality.

The generated code successfully addresses all requirements of the prompt.",grok-3-mini-beta,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipel...",7,"The generated code addresses most of the requirements. It successfully performs data preprocessing (handling missing values, encoding, scaling), splits data, trains a logistic regression model, evaluates it using the specified metrics, visualizes the confusion matrix and ROC curve, and performs cross-validation.

However, there are several areas for improvement to meet the high standards of the ground truth:

1.  **Missing Value Imputation:** While `SimpleImputer` is used, the ground truth uses `median` for Age and `mode` for Embarked directly on the DataFrame before pipeline creation, and also demonstrates feature engineering (`FamilySize`) and careful column dropping. The generated code uses `mean` for numerical features and `most_frequent` for categorical within the pipeline, which is generally acceptable but lacks the nuanced approach of the ground truth (e.g., median for age).
2.  **Feature selection/engineering:** The generated code does not drop irrelevant columns ('PassengerId', 'Name', 'Ticket', 'Cabin') and does not perform feature engineering (e.g., `FamilySize`) as the ground truth does. This impacts the quality of the model and the interpretability of coefficients.
3.  **Coefficient Interpretation:** The `get_feature_names_out()` was called on `onehot` directly, which only includes categorical features. It misses the numerical features in the coefficient display, making the interpretation incomplete. The ground truth correctly retrieves all transformed feature names from the preprocessor pipeline.
4.  **Visualization Quality:** The confusion matrix visualization is done manually with `plt.imshow`, which is less readable and slightly less robust than `ConfusionMatrixDisplay.from_estimator` used in the ground truth. Similarly, for ROC curve, `RocCurveDisplay.from_estimator` is a more streamlined and idiomatic way in scikit-learn.
5.  **Cross-Validation Metrics:** The generated code only performs cross-validation for accuracy. The ground truth retrieves multiple metrics (accuracy, precision, recall, f1, roc_auc) and displays their mean and standard deviation, which provides a more comprehensive assessment of model stability.
6.  **Code Structure and Readability:** The ground truth code uses clear comments, section headers, and better variable naming, which improves readability and maintainability. It also includes explicit calls to `np.random.seed` for full reproducibility.
7.  **Robustness (File Handling):** The ground truth includes a `try-except` block for file loading, making it more robust.

Overall, it's a good attempt that covers the core functionality, but it lacks the polish, robustness, and detailed implementation of best practices seen in the ground truth, particularly in data preprocessing and result presentation.",grok-3-mini-beta,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']),
    'Ticker': ['TECHCO']*8,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0, 15...",10,"The generated code is correct, complete, and follows good coding practices. It successfully creates the DataFrame, sets 'Date' as the index, accurately calculates the 3-day SMA for 'Close' prices (handling NaNs for initial days correctly), and computes the percentage change in 'Volume' (also handling NaNs for the first day). The inclusion of necessary imports and the clear structure contribute to high code quality. The output format is as requested, printing the final DataFrame.",grok-3-mini-beta,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]
}
df = pd.DataF...",10,"The generated code is correct, complete, and of good quality. It precisely follows all instructions:
1. It correctly creates the pandas DataFrame.
2. It correctly calculates and adds the 'Score_Improvement' column.
3. It accurately calculates and prints the average 'Score_Improvement' per subject.
4. It correctly identifies and prints the 'StudentID' of students meeting both the `Final_Score > 90` and `Attendance_Rate >= 0.95` criteria.
The imports are proper, and the code structure is clear and readable. The output format for the final part is slightly different from the ground truth (list vs. individual prints), but both are acceptable ways to present the student IDs. The generated code also includes helpful print statements to clarify the output.",grok-3-mini-beta,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

filtered_df = df[(df['Department'] == 'Tech') & (df['Perf...",9,"The generated code correctly performs all the required tasks:
1. It accurately defines the initial DataFrame using the provided data.
2. It correctly filters the DataFrame to include only ""Tech"" department employees with a ""PerformanceScore"" greater than 3.
3. It accurately calculates the average number of ""ProjectsCompleted"" for the filtered subset.
4. It correctly prints the result.

The code is concise and directly addresses the prompt.

**Areas for minor improvement (to reach a perfect 10):**
*   **Edge Case Handling:** The ground truth includes a check for an empty `filtered_df` before calculating the mean, which is good practice to prevent potential errors if no rows match the criteria. While for this specific dataset and criteria, it won't be empty, in a real-world scenario with dynamic data, this check would enhance robustness.
*   **Output Formatting:** The ground truth formats the output with a descriptive string and uses `:.2f` for two decimal places, which is more user-friendly than just printing the raw float.

These are minor points, and the core functionality and correctness are excellent.",grok-3-mini-beta,LLM_judge_config
M53,medium,Data Manipulation,,"```
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate conversion rate for Desktop
desktop_df = df[df['DeviceType'] == 'Desktop']
conversion_rate_desktop = (desktop_df['Convert...",9,"The generated code correctly defines the DataFrame and accurately calculates and prints both conversion rates as requested. The code is clean, well-structured, and directly addresses all parts of the prompt.

One minor improvement, as seen in the ground truth, would be to add checks for `empty` DataFrames before calculating the mean, though for this specific dataset, it's not strictly necessary as both 'Desktop' and 'Mobile' users are guaranteed to exist. This would make the code more robust to arbitrary input data. However, given the context of the problem (fixed data), it performs perfectly. The requested precision of 2 decimal places is also correctly applied in the print statements.",grok-3-mini-beta,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

# Define the data
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

# Create the DataFrame
df = pd.DataFrame(data)

# Calculate and print the average Air...",10,"The generated code is excellent. It correctly defines the DataFrame as specified, and then accurately calculates and prints the average ""AirQualityIndex"" for each unique ""SensorID"". The use of `groupby()` and `mean()` is idiomatic pandas, and the f-string for printing makes the output clear and readable, fulfilling all requirements of the task. The code is clean, concise, and complete.",grok-3-mini-beta,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

# Create a new feature ""EngagementScore""
df['EngagementScore'] = df['Likes'] + (df['Sh...",9,"The generated code correctly defined the DataFrame, calculated the `EngagementScore`, filtered for Instagram posts, and computed their average `EngagementScore`. All requirements of the task were met accurately. The code is concise and follows good practices for pandas operations.

The only minor point of improvement, which doesn't affect correctness but enhances user experience, is the output format of the average score. The ground truth code used an f-string with formatting (`.2f`) and a more descriptive message, which is beneficial in real-world applications for clarity. However, this is a minor presentation detail and does not detract from the functional correctness of the code.",grok-3-mini-beta,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",10,"The generated code is entirely correct and complete. It successfully defines the DataFrame, calculates the ""LengthOfStay"" column, filters for ""Flu"" patients, and accurately calculates and prints their average ""LengthOfStay"". The code structure is clean and efficient, following standard pandas conventions. All requirements of the task have been met.",grok-3-mini-beta,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}

df = pd.DataFrame(data)

# Filter the DataFrame for Sci-Fi...",9,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates their average rating. The code is concise and directly addresses all requirements of the task.

Strong points:
- **Correctness**: The core logic for defining the DataFrame, filtering, and calculating the mean is perfectly correct.
- **Completeness**: All stated requirements (DataFrame definition, filtering, average calculation, and printing) are met.
- **Readability**: The code is easy to understand and follows standard pandas conventions.

Areas for minor improvement (not critical for this specific task, but good practice):
- **Output formatting**: While `print(average_rating)` is correct, formatting the output with a descriptive message (e.g., `print(f""The average rating for Sci-Fi movies is: {average_rating:.2f}"")`) makes the output more user-friendly, as seen in the ground truth.
- **Edge case handling**: The ground truth includes a check for an empty `sci_fi_movies_df` to handle cases where no Sci-Fi movies might be present. While not strictly necessary for *this specific dataset* as there are Sci-Fi movies, it's good practice for robust code. However, for a simple benchmark, this level of detail is often optional.

Overall, it's a very good solution that directly answers the prompt.",grok-3-mini-beta,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

# Filter the DataFrame for Toyota cars
toyota_df = df[df['Brand'] == ...",9,"The generated code correctly performs all requested operations: it defines the DataFrame, filters it for Toyota cars, and accurately calculates their average mileage. The code is concise and uses appropriate pandas operations. The only minor point for improvement is the print statement for the average mileage. While it prints the correct number, the ground truth's use of an f-string with `.2f` for formatting and more descriptive text is generally preferred for clarity and user-friendliness in real-world applications. However, for a basic task, the current output is functionally correct.",grok-3-mini-beta,LLM_judge_config
M59,medium,Data Filtering,,"```
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 80]
}

d...",9,"The generated code correctly defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat. All requirements of the task are met. The code is concise and efficient. The only minor improvement would be to print the result with a more descriptive message as in the ground truth code, especially if this were part of a larger application where context matters. However, for the given task, simply printing the number is acceptable. The ground truth also includes an `if not fire_pokemon_df.empty:` check, which is good practice for robustness in a general-purpose solution, but for this specific dataset where Fire types are known to exist, it's not strictly necessary for correctness.",grok-3-mini-beta,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Load and preprocess data
df = pd.read_csv('titanic.csv')
df['Age'].fillna(df['Age'].median(), inplace=True)
df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 12, 18, 60, 80], labels=['Child', 'Teen', 'Adult', 'Senior'])

# 1. Multi-panel figure for survival rates
fig1, axes1 = plt.subplots(1, 3, figsize=(15, 5))
sns.barplot(x='Pclass', y='Survived', data=df, ax=axes1[0])
axes1[0].set_titl...",6,"The generated code addresses most of the requirements but falls short in implementation details and sophistication compared to the ground truth.

**Correctness:**
- **Task 1 (Multi-panel):** Partially correct. It creates a multi-panel figure, but instead of calculating survival rates for the groups and then plotting (which is better), it plots directly from the raw data `df` using `sns.barplot`, which defaults to mean. This is acceptable but less explicit. The labels for `AgeGroup` are `[0, 12, 18, 60, 80]` which is also less granular than the ground truth.
- **Task 2 (Heatmap):** Correct and well-implemented.
- **Task 3 (Violin plots):** Partially correct. It creates two separate violin plots for male and female, but the request was ""by gender"" which typically implies `hue='Sex'` on the single plot, showing both distributions side-by-side or split for survivors vs non-survivors within each gender. The current implementation only compares survivors vs non-survivors *within* each gender separately, which is a valid interpretation but not quite as comprehensive as a single plot with `hue`.
- **Task 4 (Interactive scatter plot):** Correct. Uses `plotly.express` effectively. `size='Survived'` is a reasonable way to show survival but not explicitly requested.
- **Task 5 (Dashboard):** This is the major area of concern. Instead of *combining* actual visualizations within `matplotlib` subplots, it *saves* individual plots as images and then *loads* and *displays* these images within the dashboard. This is an extremely unconventional and inefficient approach for a dashboard, losing all vector graphics scalability and interactivity. It also prevents any dynamic adjustments or fine-tuning of the embedded plots within the dashboard environment. The ""insights"" are hardcoded text, which is fine, but the *embedding* of plots via `imshow` is a significant flaw.

**Code Quality:**
- **Imports:** Correct.
- **Structure:** Generally follows the outlined tasks.
- **Preprocessing:** Basic and correct for `Age` and `AgeGroup`, but `AgeGroup` bins are wider and less specific ('Teen' vs 'Teenager', 'Adult' vs 'Young Adult', 'Adult', 'Senior'). `Fare` missing values are not handled, though for titanic.csv it's usually complete.
- **Readability:** Clear for individual plots.
- **`plt.savefig` and `plt.imread`:** The choice to save and then re-embed images for the dashboard is a major quality issue. This makes the dashboard static and clunky.

**Completeness:**
- **Plot Types:** All requested plot types are generated.
- **Dashboard:** The 'dashboard' is essentially a collage of static images rather than a dynamically generated plot, which significantly deviates from the spirit of ""combining multiple visualizations"" in a single matplotlib figure.
- **Annotations/Insights:** The insights in the dashboard are basic and hardcoded. The ground truth demonstrates how to add more comprehensive textual insights programmatically.
- **Demographic Groups:** Covers class, gender, age groups.
- **Assumptions:** Correctly assumes CSV in current directory.

**Specific Areas for Improvement:**
- **Dashboard Implementation:** The dashboard should directly create subplots (`fig.add_subplot` or `axes[row, col]`) and then plot directly onto those subplots instead of saving and loading image files. This is fundamental for a proper `matplotlib` dashboard.
- **Violin Plot for Task 3:** Consider using `sns.violinplot(x='Sex', y='Age', hue='Survived', data=df, split=True)` for a more concise and comparative visualization of age distributions by gender for survivors vs non-survivors.
- **Data Preprocessing Sophistication:** Add more explicit categorical conversions for `Pclass` and `Sex` to ensure proper ordering and labeling in plots, similar to the ground truth. Handle `Fare` NaNs.
- **Context/Annotations:** Add more descriptive titles, labels, legends, and potentially textual insights directly within dashboard subplots, as seen in the ground truth.
- **Plotting Aesthetics:** Use `set_ylim` for consistency in bar plots, apply `plt.suptitle` for overall titles, and make color palettes consistent where desirable. The ground truth makes excellent use of `plt.rcParams` and `sns.set_style` for global plot aesthetics, which is missing here.",grok-3-mini-beta,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, ...",9,"The generated code correctly performs all requested operations: it defines the DataFrame, filters it for ""Dystopian"" genre books, and accurately calculates and prints the average stock for these books. The code is concise and uses appropriate pandas functionalities.

Areas for minor improvement (not critical for correctness, but for robustness/readability):
1.  **Print Formatting:** The output `print(average_stock)` could be improved by using an f-string to provide more context to the user, perhaps `print(f""Average stock for Dystopian books: {average_stock:.2f}"")` as seen in the ground truth. This enhances user understanding of the printed value.
2.  **Edge Case Handling:** The generated code doesn't explicitly handle the edge case where no Dystopian books are found. In such a scenario, `dystopian_books['Stock'].mean()` would return `NaN`. While technically correct for pandas in this specific case, for general-purpose applications, adding a check (`if not dystopian_books.empty:`) like in the ground truth can make the code more robust and provide a more informative message to the user. However, for the given dataset, this isn't an issue.

Overall, the code is highly effective and correct for the given task.",grok-3-mini-beta,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```
import pandas as pd
import numpy as np
from scipy import stats

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Numerical features
numerical_features = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']

# Task 1: T-tests and Cohen's d
for feature in numerical_features:
    group0 = df[df['Potability'] == 0][feature].dropna()
    group1 = df[df['Potability'] == 1][feature].dropna()
    
    if len(group0) > ...",7,"The generated code addresses most of the requirements but has several areas for improvement.

**Strengths:**
*   Correctly performs t-tests, Shapiro-Wilk tests, and chi-square goodness-of-fit.
*   Calculates Cohen's d for t-test significant results.
*   Handles missing values by `dropna()` before statistical tests.
*   Provides interpretations for each test based on the p-value.
*   Uses appropriate libraries (`pandas`, `numpy`, `scipy.stats`).

**Areas for Improvement (leading to a score of 7):**

1.  **Missing Value Handling:**
    *   **Crucial Difference:** The prompt explicitly asks to ""handle missing values"". The ground truth code imputes missing values with the mean, which is a common and often necessary step for statistical analysis where the tests expect complete data or rely on sample size. The generated code uses `.dropna()` on the features (`group0`, `group1`, `data`) *before* performing tests. While this prevents errors, it leads to discarding data, potentially reducing statistical power and introducing bias if missingness isn't completely random. The ground truth's approach of global imputation is generally preferred for this type of task.
    *   No print statement for missing values before/after, which was a nice-to-have in the ground truth for clarity.

2.  **Order of Operations:** The task asks to ""Apply normality tests... on key numerical features"" (point 3), then ""Calculate effect sizes... for significant differences"" (point 4). The generated code performs t-tests first, then chi-square, then Shapiro-Wilk. While it's not a hard dependency, following the specified order from the prompt can be beneficial for clarity or if there were hidden dependencies.

3.  **T-test Type:** The generated code uses `stats.ttest_ind(group0, group1)`, which by default assumes equal variances (`equal_var=True`). The ground truth uses `equal_var=False` (Welch's t-test), which is generally more robust and recommended when variances are not known to be equal (a common scenario in real-world data). While not explicitly stated in the prompt, this detail often distinguishes robust statistical analysis.

4.  **Clarity and Robustness of Cohen's d:**
    *   The Cohen's d calculation handles missing values via `dropna()`, but doesn't explicitly check for sample sizes being too small (e.g., `n0 + n1 - 2 <= 0` for pooled std denominator) which the ground truth does, leading to safer code.
    *   The `ddof=1` for `np.std` is correct for sample standard deviation.

5.  **Output Formatting and Readability:**
    *   The ground truth code has more structured and detailed print statements (e.g., specific significance level mentioned for each test block, clear headers and sub-headers).
    *   The ground truth provides a more nuanced interpretation of Cohen's d (trivial, small, medium, large).

6.  **Significance Level (Alpha):** The generated code hardcodes `0.05` multiple times. A good practice, as followed by the ground truth, is to define `ALPHA = 0.05` as a constant at the top for easy modification and consistency.

7.  **Shapiro-Wilk Test Sample Size:** The ground truth code includes a check for large sample sizes for Shapiro-Wilk (over 5000), noting its limitations. While the dataset size might not hit this limit, it's a mark of more robust and knowledgeable statistical programming.

To improve, the generated code should prioritize mean-based imputation for missing values, use Welch's t-test, define alpha as a constant, and refine output for better readability and detail.",grok-3-mini-beta,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrix...",7,"The generated code addresses most of the requirements but has some significant issues that affect its correctness and completeness, particularly in data preprocessing and cross-validation methodology.

**Strong Points:**
*   Correctly loads data and performs basic separation of features and target.
*   Implements multiple classification algorithms (Logistic Regression, Random Forest, SVM).
*   Calculates and displays a good set of evaluation metrics (accuracy, precision, recall, F1-score, ROC-AUC).
*   Generates confusion matrices and ROC curves.
*   Attempts k-fold cross-validation.
*   Compares models and selects a best model based on F1-score.
*   Attempts to display feature importance.

**Areas for Improvement / Critical Issues:**

1.  **Data Leakage in Preprocessing:**
    *   **Imputation:** The generated code imputes missing values on the *entire dataset* (`df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)`) *before* the train-test split. This leads to data leakage because the imputation learned from the test set is applied to the training set. The ground truth correctly handles this by performing `fit_transform` on `X_train` and `transform` on `X_test` *after* the split.
    *   **Scaling:** Similar to imputation, feature scaling is applied to the entire dataset (`X_scaled = scaler.fit_transform(X)`) *before* the train-test split. The scaler should be fitted only on the training data and then used to transform both training and test data to prevent data leakage. The ground truth correctly performs `fit_transform` on `X_train_imputed` and `transform` on `X_test_imputed` after the split.

2.  **Cross-Validation Methodology:**
    *   The prompt asks for k-fold cross-validation (`k=5`) to assess model stability. The generated code performs `cross_val_score(model, X_scaled, y, cv=5, scoring='f1')`. This is fundamentally incorrect because `X_scaled` is an already scaled version of the *entire* dataset, which means it too has suffered from data leakage from the scaling step.
    *   **Crucially, the preprocessing steps (imputation and scaling) themselves need to be part of the cross-validation folds** to ensure proper assessment of model stability and avoid data leakage in the CV process. The ground truth correctly demonstrates this by using a `sklearn.pipeline.Pipeline`, which encapsulates the imputation, scaling, and classification steps within each CV fold. This is a critical deficiency in the generated code's CV implementation.
    *   The generated code also only calculates F1-score for CV, whereas the prompt asked for k-fold cross-validation to assess stability (implying the need to report stability of various metrics, which the ground truth does by reporting mean and std for multiple metrics).

3.  **Visualization:**
    *   The ROC Curve plotting directly uses `plt.plot` and doesn't leverage `RocCurveDisplay.from_estimator`. While it works, `RocCurveDisplay` is generally preferred for consistency and convenience with other sklearn display tools.
    *   The prompt asked for one set of ROC curves for all models on a single plot, which the ground truth provided. The generated code plots individual ROC curves for each model, which is less effective for direct comparison against each other on a single visualization as requested.

4.  **Feature Importance:**
    *   The feature importance display for Logistic Regression uses `best_model.coef_[0]` but then prints them directly without associating them with feature names, and doesn't sort them or visualize them as the ground truth does. The ground truth also correctly clarifies that SVM does not have direct feature importances.
    *   The feature importance logic for Random Forest also lacks sorting and clear visualization (like a bar plot) as specified in the ground truth.

5.  **Imports and Structure:**
    *   The `train_test_split` in the generated code takes `X_scaled` and `y`, but `X_scaled` is the scaled version of the *entire* `X`, not just the training set's `X`. This reiterates the data leakage problem.
    *   The generated code imports `roc_curve` directly, whereas `RocCurveDisplay` is generally more convenient for plotting.

In summary, while the generated code provides a functional skeleton that meets many requirements, the critical data leakage issues during preprocessing and cross-validation make the results unreliable and the methodology unsound from a machine learning perspective.",grok-3-mini-beta,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import PolynomialFeatures

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Feature engineering
df['ph_chloramines_ratio'] = df['ph'] / df['Chloramines']
df['hardness_solids_ratio'] = df['Hardness'] / df['Solids']
df['ph_turbidity_interac...",6,"The generated code addresses most of the requirements but has several shortcomings that prevent it from being a robust solution.

**Strengths:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Basic Feature Engineering:** It attempts to create ratio, interaction, and polynomial features.
*   **Correlation Matrix & Top Features:** It correctly calculates the correlation matrix and identifies top correlated features.
*   **Feature Selection:** It uses both `SelectKBest` and `RFE` as requested.
*   **Visualizations:** It creates a correlation heatmap, feature importance plot, and attempts pairwise scatter plots.
*   **Comparison of Feature Sets:** It attempts to compare the selected feature sets.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling:** The ground truth explicitly handles missing values by imputing with the mean. The generated code entirely misses this crucial step, which would cause issues with `df.corr()` potentially, and definitely with `SelectKBest` if missing values are present in relevant columns. This is a critical omission for data science tasks.
2.  **Robust Feature Engineering:**
    *   **Division by Zero:** The ratio calculations `df['ph'] / df['Chloramines']` do not handle potential division by zero, which could lead to `inf` or `NaN` values. The ground truth uses `epsilon` to prevent this.
    *   **Limited Engineered Features:** The number of engineered features (3 ratios, 1 interaction, 2 polynomial) is very limited compared to the ground truth (7 ratios, 5 interactions, and a more comprehensive set of polynomial features derived from multiple features). The task specifically asked for ""new meaningful features such as ratios... interaction terms, and polynomial features"" implying a more comprehensive approach.
    *   **Polynomial Features:** The ground truth correctly uses `PolynomialFeatures` from `sklearn.preprocessing` to generate combinations and powers, which is the standard way. The generated code manually creates only `ph_squared` and `hardness_squared`, missing many potential polynomial terms and the utility of `PolynomialFeatures`.
3.  **Visualization - Pairwise Scatter Plots:**
    *   The generated code creates individual scatter plots for each top correlated feature against 'Potability'. However, the request was for ""pairwise scatter plots for top correlated features,"" which typically implies a `sns.pairplot` showing relationships *among* these top features, colored by the target variable. The ground truth correctly implements `sns.pairplot`.
    *   The feature importance plot from RFE's `estimator_` is problematic. RFE works by iteratively removing features, and its internal estimator might not have been trained on all features or represent the full feature importance across the entire dataset as well as a separate RandomForestClassifier trained on the full dataset would. The ground truth trains a separate `RandomForestClassifier` on the full dataset for this purpose.
4.  **Feature Scaling:** The ground truth correctly applies `StandardScaler` before feature selection (`SelectKBest`, `RFE`). While Random Forest is less sensitive to scaling, `SelectKBest` (especially with `f_classif` which assumes normally distributed data) and potentially other models benefit greatly from scaling. This is a best practice often overlooked but important for robust feature selection.
5.  **Comparison Analysis:** The comparison focuses only on the overlap between `SelectKBest` and `RFE`. The ground truth provides a more comprehensive comparison including unique features to each method and overlaps with the ""top 5 correlated features,"" fulfilling the ""analyze their overlap"" requirement more thoroughly.
6.  **Readability/Structure:** While functional, the ground truth's code organizes the sections more clearly with extensive comments, which improves maintainability and understanding, especially for complex tasks like feature engineering.
7.  **Robustness (File Handling):** The ground truth includes `try-except` for file loading, which is good practice. The generated code assumes the file exists.
8.  **Output:** The generated code only prints the selected features and overlap but lacks the informative print statements the ground truth provides (e.g., listing top correlated features, sizes of overlaps).

To achieve a higher score, the generated code needs to handle missing values, implement a more comprehensive and correct approach for feature engineering (especially polynomial features), ensure robust and correct visualization types (especially for pair plots), apply feature scaling, and provide a more detailed comparison of feature sets.",grok-3-mini-beta,LLM_judge_config
