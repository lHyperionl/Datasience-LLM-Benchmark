task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd

# Load the CSV file
df = pd.read_csv('winequality-red.csv')

# 1. List all column names and identify data types
print(""Column Names and Data Types:"")
print(df.dtypes)
print()

# Identify categorical/ordinal columns (non-numeric columns or likely ordinal columns)
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
# Also possible to check for columns like 'quality' as ordinal if present
ordinal_cols = []
if 'quality' in df.columns:
    ordinal_c...",7,"The generated code correctly loads the dataset, identifies column names and their pandas data types, calculates descriptive statistics for numerical columns, and reports the total number of rows and columns. It effectively uses `select_dtypes` for identifying numerical and object columns.

However, there are a few areas for improvement:
1.  **Categorical/Ordinal Identification**: The ground truth goes into more detail about how it identifies data types (numerical vs. categorical/ordinal based on dtypes vs. further discussion for 'quality' column) and provides a more robust way to categorize columns beyond just pandas dtypes for the user's understanding. The generated code's identification of categorical/ordinal columns is less comprehensive. It correctly identifies `object` types, but 'quality' being ordinal is handled by a hardcoded check, which isn't generally robust. The prompt specifically asked to ""Identify any columns that appear to be categorical or ordinal"", and the ground truth's approach using `pd.api.types.is_object_dtype` or `is_categorical_dtype` is more robust for general datasets.
2.  **Descriptive Statistics Presentation**: While the generated code calculates the correct statistics, the ground truth uses `describe()` which inherently includes quartiles (25%, 50%, 75%) and also has `count` and `std` in a standard, concise output format, making it very readable. The generated code modifies the `describe()` output and adds `median` redundantly (as 50% is median), and reorders columns, which is less standard and could be simpler.
3.  **Error Handling**: The generated code lacks error handling for `FileNotFoundError`, which the ground truth includes and is good practice for robust code.
4.  **Clarity of Output**: The output phrasing in the ground truth is more aligned with the specific requirements in the prompt (e.g., ""Column Names and Identified Data Types"" vs. just printing `df.dtypes`).",gpt-4.1,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
df = pd.read_csv('video_games_sales.csv')
print(""First 10 rows:"")
print(df.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(""\nShape of the dataset:"")
print(df.shape)

# 3) List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# 4) Display the top 10 games by Global_Sales
print(""\nTop 10 games by Global_Sales:"")
print(df.sort_values(by='Global_Sa...",8,"The generated code correctly performs all the requested tasks, demonstrating a clear understanding of the requirements. It loads the data, displays the head, shows the shape, lists column types, identifies top global sales, unique platforms/genres, provides summary statistics for sales columns, and checks for missing values.

Areas for improvement to reach a higher score:
1.  **Robustness**: The ground truth includes `try-except` blocks for file loading and checks for column existence before accessing them (e.g., `if 'Global_Sales' in df.columns:`). This makes the code more robust to unexpected data or file issues. The generated code assumes the file and columns always exist, which can lead to runtime errors if they don't.
2.  **Output Clarity**: While the generated code includes print statements, the ground truth uses more verbose and structured print statements (e.g., ""--- 1. Loading Dataset and Displaying First 10 Rows ---"", separators) which enhance readability and user experience in a benchmark task context.
3.  **Display of Top Games**: The generated code prints all columns for the top 10 games, whereas the ground truth explicitly selects more relevant columns (`'Name', 'Platform', 'Genre', 'Publisher', 'Global_Sales'`) for a cleaner display, which is often preferred in EDA.
4.  **`df.info()` vs. `df.dtypes`**: For displaying column names and data types, `df.info()` is generally preferred over `df.dtypes` as it also provides non-null counts and memory usage, offering a more comprehensive overview crucial for EDA. The ground truth correctly uses `df.info()`.",gpt-4.1,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd
import numpy as np

# 1. Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 2. Handle missing values in the Year column by filling them with the median year
if df['Year'].dtype != np.float64 and df['Year'].dtype != np.int64:
    df['Year'] = pd.to_numeric(df['Year'], errors='coerce')
median_year = int(df['Year'].median())
df['Year'] = df['Year'].fillna(median_year).astype(int)

# 3. Remove games with missing Publisher information
df = df[~df['Publisher']....",8,"The generated code is largely correct and addresses most of the requirements. It successfully loads the data, handles missing Year values, removes missing Publisher values, creates the 'Years_Since_Release' column, removes duplicates, and filters out low-selling games.

**Strong points:**
- Correctly uses `pd.to_numeric` with `errors='coerce'` for the 'Year' column, which is good practice.
- Handles median year calculation and filling missing 'Year' values, then converting to int.
- Correctly identifies and drops duplicates based on 'Name' and 'Platform'.
- Correctly filters games based on 'Global_Sales' and handles `pd.to_numeric` for this column as well.
- Displays the shape and head as requested.

**Areas for improvement to reach a higher score:**
- **Publisher handling:** The ground truth's `dropna(subset=['Publisher'])` is more robust and standard than `df[~df['Publisher'].isna() & (df['Publisher'].str.strip() != '')]`. The latter might be slightly over-engineered unless 'Publisher' could contain empty strings that are not `NaN`.
- **Years_Since_Release validation:** The ground truth includes `df['Years_Since_Release'] = df['Years_Since_Release'].apply(lambda x: max(0, x))` to ensure the years since release are non-negative. While unlikely for release years that are already `int`, it's a good defensive programming step if there are future dates or data errors. This was missed in the generated code.
- **Handling of `Global_Sales` conversion:** The generated code directly filters using `pd.to_numeric(df['Global_Sales'], errors='coerce') >= 0.1` without storing the numeric conversion back into the DataFrame or dropping NaN rows from this conversion first. This works but might lead to issues if subsequent operations needed the numeric `Global_Sales` column in the same way. The ground truth explicitly converts it and then drops rows where `Global_Sales` became NaN, which is slightly safer practice before filtering, especially if the filter was on the column itself rather than the temporary `pd.to_numeric` output.
- **Robustness (try-except):** The ground truth includes `try-except` blocks for file not found and general errors, which is a good practice for deployed code. While not explicitly asked for, it contributes to overall code quality and robustness.
- **Readability:** The generation code is missing the print statements at each step of the cleaning process, which were present in the ground truth and helpful for debugging/understanding data flow. While not a functional requirement, it was implied by the ground truth's structure given as a reference.

Overall, the generated code largely fulfills the core requirements, but could benefit from some minor adjustments to improve robustness and adhere closer to best practices demonstrated in the ground truth.",gpt-4.1,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 2. Bar chart: Top 10 platforms by total global sales
platform_sales = df.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False).head(10)
plt.figure(figsize=(10,6))
platform_sales.plot(kind='bar', color='skyblue')
plt.title('Top 10 Gaming Platforms by Total Global Sales')
plt.xlabel('Platform')
plt.ylabel('Total Global Sales (Millions)')
plt.xticks(rota...",6,"The generated code successfully creates all the requested visualizations. However, there are several issues that reduce its score.
1.  **Data Cleaning/Preparation:** The generated code completely misses crucial data cleaning steps, especially for `Year_of_Release` and sales columns. This leads to potential errors or skewed plots if the raw data contains non-numeric values or NaNs in these fields. The ground truth explicitly handles this.
2.  **Missing `numpy` import:** Although `numpy` is not directly used in the plotting logic, it's a common dependency for data manipulation with Pandas, and its absence could be an oversight given the context.
3.  **Pie Chart Readability:** The pie chart directly plots all genres, which if there are many, can lead to an unreadable chart with tiny slices and overlapping labels. The ground truth addresses this by aggregating smaller categories into an ""Other"" slice, improving readability.
4.  **Histogram Range:** The histogram for `Global_Sales` without a specified range or handling of outliers can be heavily dominated by zero or very low sales, making it hard to see the distribution of higher sales. The ground truth limits the range to the 99th percentile, which is a good practice for skewed data.
5.  **Line Plot Year Column:** The generated code assumes `Year` is the column for release year, while the ground truth uses `Year_of_Release`. This could cause an error or incorrect plot depending on the exact column name in the CSV. The ground truth also filters out unrealistic year values.
6.  **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError`, which is good practice for loading data. The generated code lacks this.

To improve, the code should:
-   Implement data cleaning and preprocessing for numerical columns and the year column.
-   Consider strategies for making the pie chart more readable for many categories.
-   Refine the histogram limits for a more informative distribution.
-   Ensure the correct column name for the year is used and potentially filter years for realism.
-   Add basic file loading error handling.",gpt-4.1,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd
import numpy as np

# 1) Load the dataset and display the first 10 rows
df = pd.read_csv('diabetes.csv')
print(""First 10 rows:"")
print(df.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(""\nShape of the dataset:"", df.shape)

# 3) List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# 4) Calculate the percentage of diabetic vs non-diabetic patients (Outcome column)
outcome_counts = df['Outcome'].va...",8,"The generated code provides a mostly correct and complete solution to the task.

**Strong points:**
*   **Correctness:** All requested analyses are performed correctly.
*   **Completeness:** All 7 requirements are addressed.
*   **Clarity:** The output is well-formatted and easy to understand.
*   **Specific analysis:** The calculation of outcome percentages and identification of zero values in specific columns are well-executed.

**Areas for improvement to achieve a higher score:**
*   **Robustness in file loading:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions, which makes the code more robust. The generated code assumes the file exists.
*   **More informative `df.dtypes`:** While `df.dtypes` is correct for item 3, `df.info()` (used in the ground truth) provides more comprehensive information (non-null counts, memory usage) which is generally preferred for EDA.
*   **Handling `Outcome` column percentages:** The generated code correctly calculates the percentages and renames the labels, but it forces an `astype(str) + '%'` which might be less flexible than directly printing the rounded percentages and letting pandas handle the formatting.
*   **Explicit missing value display:** For missing values, the ground truth explicitly states ""No missing values found"" if `sum() > 0` is false, making it clearer. The generated code just prints a list of zeros for columns with no missing values.
*   **Zero value identification output:** The ground truth also provides the *percentage* of zero values, which is more informative than just the raw count. It also includes a check if the column exists before processing.
*   **General Output Formatting:** The ground truth uses more consistent separators (`---`, `===`) and clearer headings for each section, which improves readability for a report-like output.",gpt-4.1,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# 1) Load the dataset
df = pd.read_csv('diabetes.csv')

# 2) Identify columns with zero values that represent missing data
cols_with_missing_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

# 3) Replace zero values in these columns with NaN
df[cols_with_missing_zeros] = df[cols_with_missing_zeros].replace(0, np.nan)

# 4) Fill missing values with the median of each column
fo...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
- All steps outlined in the prompt are implemented.
- Correctly identifies and replaces zero values in specified columns with NaN.
- Uses the median for imputation, which is a good choice for skewed data or when outliers might be present.
- Handles duplicate rows.
- Creates the `BMI_Category` column accurately using a function and `apply`.
- Correctly scales numerical features, excluding the 'Outcome' column.
- Displays the required output (shape and head).
- Proper imports are included.

**Areas for minor improvement:**
- **Robustness in file loading:** The ground truth includes a `try-except` block for `pd.read_csv`, which makes the code more robust against `FileNotFoundError`. The generated code assumes the file will always be present.
- **Handling missing columns:** The generated code doesn't explicitly check if the specified columns (`cols_with_missing_zeros`) actually exist in the DataFrame before attempting operations on them. While usually not an issue with a fixed dataset, it's good practice for more general scripts. The ground truth handles this via `if col in df.columns`.
- **Clarity in `BMICategory` bins:** The `bmi_category` function is correct, but using `pd.cut` with `bins` and `labels` (as in the ground truth) can sometimes be more concise and less error-prone for multiple categorical bins, especially if the number of categories grows.
- **Output formatting:** While the shape and head are printed, the ground truth provides more descriptive print statements (e.g., ""Dataset loaded successfully."", ""Removed X duplicate rows."", ""Cleaned dataset shape:"", etc.), which enhances user understanding.",gpt-4.1,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the dataset
df = pd.read_csv('diabetes.csv')

# 2. Correlation heatmap
plt.figure(figsize=(10,8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt="".2f"")
plt.title('Correlation Heatmap of Diabetes Dataset')
plt.show()

# 3. Histograms for all numerical features
df.hist(figsize=(12,10), bins=20, edgecolor='black')
plt.suptitle('Histograms of Numerical Features', fontsize=16)
plt.tight_layout()
pl...",8,"The generated code successfully addresses most of the task requirements.

**Strengths:**
- **Correctness:** All requested visualizations (heatmap, histograms, box plots, pie chart, scatter plots, pair plot) are generated correctly.
- **Proper Imports:** Necessary libraries (pandas, matplotlib.pyplot, seaborn) are imported.
- **Clear Structure:** The code is logically structured, with distinct sections for each visualization task.
- **Labels and Titles:** Most plots include appropriate titles and labels.
- **Scatter Plots:** The two requested scatter plots (BMI vs Glucose and Age vs Pregnancies) are correctly generated and colored by outcome.
- **Pair Plot:** The pair plot is correctly generated with `hue` and `diag_kind='kde'`.

**Areas for Improvement:**
- **Loading the dataset:** The ground truth includes error handling for `FileNotFoundError` and prints dataset information (`.head()`, `.info()`, `.describe()`), which is good practice for data science tasks. The generated code lacks this.
- **Correlation Heatmap:** The generated code computes the correlation for the entire DataFrame (`df.corr()`). The ground truth explicitly selects numerical columns excluding 'Outcome', which is more precise when 'Outcome' is a binary target, although `df.corr()` will typically treat 'Outcome' as numerical anyway. However, it's good practice to be explicit about which columns are numerical variables for correlation.
- **Histograms:** While `df.hist()` is a concise way to plot histograms for all numerical features, the Ground Truth uses `sns.histplot` which often provides more aesthetic control (e.g., `kde=True` for density estimation) and allows for more flexible subplot arrangement when not all columns are desired. The spacing for `df.hist()` plots was slightly cramped due to `subplots_adjust` being set for `top=0.92` and `suptitle` with `fontsize=16`, which sometimes cut off titles.
- **Box Plots:** The ground truth explicitly identifies numerical columns and iterates over them. The generated code also does this but the ground truth adds more descriptive labels for the 'Outcome' axis. Also, the ground truth uses a more dynamic calculation for `n_rows` and `n_cols` to ensure optimal subplot layout, whereas the generated code hardcodes `plt.subplot(3, 3, idx+1)`, which might not always fit perfectly depending on the number of features. In this specific case, it worked out (8 features fit in 3x3).
- **Pie Chart Details:** The ground truth used `explode=(0.05, 0)` to emphasize one slice, and `shadow=True`, which are minor aesthetic touches. The generated code used `explode=(0.05, 0.05)` which just separates both slices equally.
- **Consistency in Scatter Plot Labels/Legend:** The ground truth explicitly sets `legend(title='Outcome', labels=['Non-Diabetic', 'Diabetic'])` which is more informative for the user. The generated code relies on default labels, which might just show 0 and 1.

Overall, the generated code is highly functional and fulfills the core requirements. The suggested improvements are mainly related to robustness, better adherence to data science best practices (e.g., explicit column handling, error handling for loading), and minor aesthetic enhancements.",gpt-4.1,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

# 1. Load the dataset and display the first 10 rows
df = pd.read_csv('world-happiness-report-2021.csv')
print(""First 10 rows:"")
print(df.head(10))

# 2. Show the shape of the dataset (rows and columns)
print(""\nShape of the dataset (rows, columns):"")
print(df.shape)

# 3. List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# 4. Display the unique regional indicators and count of countries per region
print(""\nUnique r...",8,"The generated code correctly performs all the requested tasks. It loads the data, displays the first 10 rows, shows the shape and data types, provides regional insights, identifies top happy countries, calculates summary statistics, and checks for missing values.

Areas for improvement (to reach a 9 or 10):
- **Error Handling**: The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. Without it, the script would crash if the file isn't present.
- **Robustness for Column Names**: The generated code assumes all specified column names ('Regional indicator', 'Ladder score', 'Logged GDP per capita', etc.) exist. The ground truth includes checks like `if 'Regional indicator' in df.columns:` which makes the code more robust against variations in column names, preventing crashes if a column is missing.
- **Clarity of Output**: While the output is functional, the ground truth uses separators (`print(""\n"" + ""=""*70 + ""\n"")`) and more explicit headers (e.g., ""1. First 10 rows of the dataset:"") which improves the readability and separation of task outputs.
- **Data Type Display**: For task 3, `df.info()` as used in the ground truth is often preferred over `df.dtypes` because it also provides non-null counts, which is valuable for initial EDA and understanding data completeness.

Overall, it's a very good solution that achieves the core requirements. The improvements would make it more robust for real-world scenarios and clearer for output presentation.",gpt-4.1,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder

# 1. Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# 2. Handle missing values
# Separate columns by dtype
num_cols = df.select_dtypes(include=[np.number]).columns
cat_cols = df.select_dtypes(include=['object']).columns

# Fill numerical with mean
for col in num_cols:
    df[col] = df[col].fillna(df[col].mean())

# Fill categorical with mode
for col in cat_cols:...",7,"The generated code largely addresses the requirements but has a few areas for improvement, primarily in the implementation of ""GDP_Category"" and the handling of numerical features for scaling after new categories are introduced.

**Correctness:**
*   **Loading:** Correctly loads the dataset.
*   **Missing Values:** Correctly handles missing numerical values with the mean and categorical with the mode. Using `df[col] = df[col].fillna(...)` is equivalent to `inplace=True` for single column assignments.
*   **Happiness_Category:** Correctly implemented.
*   **Well_being_Score:** Correctly implemented.
*   **Outlier Removal:** Correctly implemented using IQR.
*   **Standardization:** The `num_cols` selection for standardization is problematic. After creating 'Happiness_Category' and 'GDP_Category', these are treated as numerical columns in the generated code's `df.select_dtypes(include=[np.number]).columns`. While `np.select` or `pd.qcut` would make them categorical (object/category dtype), the handwritten `apply` functions for `GDP_Category` specifically return strings ('Low', 'Medium', 'High', 'Very High') which means it *won't* be included in `num_cols`, but it's *supposed* to be a categorical feature. If `pd.qcut` was used (as in ground truth) it would be a Categorical Dtype, which also avoids being scaled. The current implementation of `GDP_Category` as string labels prevents it from being scaled, which is correct for that feature, but it's an incidental correctness due to the string return type rather than explicit intention. This could be confusing if the intention was to return numbers. The ground truth handles this more robustly by setting appropriate dtypes or ensuring the selected columns are purely numeric.
*   **Encoding:** Correctly applies LabelEncoder to 'Regional indicator'. The `if 'Regional indicator' in df.columns:` check is redundant if the previous load was successful, but harmless.
*   **Display:** Correctly displays shape and head.

**Code Quality:**
*   **Imports:** Proper imports.
*   **Structure:** Code is well-structured and follows the requested steps.
*   **Readability:** Generally readable.

**Completeness:**
*   **GDP_Category:** The implementation of `GDP_Category` using `apply` and custom quartiles is less robust and more verbose than `pd.qcut`. Crucially, the task specified ""based on quartiles"", and `pd.qcut` inherently creates quartiles (Q1, Q2, Q3, Q4). The generated code's `gdp_cat` function labels them as 'Low', 'Medium', 'High', 'Very High', which doesn't directly map to standard quartile nomenclature (Q1, Q2, Q3, Q4) as implicitly suggested by ""based on quartiles"". This also makes manual calculation of quartiles necessary, which `pd.qcut` abstracts away. This is a functional difference from the implicit expectation of the prompt.
*   **Error Handling:** Lacks the file not found error handling present in the ground truth. While not explicitly asked for, it's good practice for robust code.

**Areas for Improvement (to reach a higher score):**
*   **Robust `GDP_Category`:** Use `pd.qcut` for 'GDP_Category' as it's more idiomatic, robust, and directly aligns with creating categories based on quartiles. This also correctly assigns a 'category' dtype, preventing it from being accidentally selected by `select_dtypes` for numerical operations later.
*   **Specific Numerical Columns for Scaling:** Instead of `df.select_dtypes(include=[np.number]).columns`, explicitly define which numerical columns need to be scaled if some numerical-like columns (e.g., encoded categorical features if they were done earlier and not excluded, or if custom numerical categories were accidentally created) should not be scaled. In this case, `Well_being_Score` is a new numerical column from the merge, which should be scaled, and it is handled correctly by `select_dtypes`. However, if `Happiness_Category` or `GDP_Category` (if numerical) were to be created after outlier removal and before scaling, they could be accidentally scaled. The `select_dtypes` works fine here because the new categorical features are string-based and thus excluded, but this can be a subtle dependency. The ground truth explicitly lists or filters out columns from the numerical set, which is a safer approach for large datasets or complex pipelines.",gpt-4.1,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
# Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from plotly.subplots import make_subplots
from matplotlib.patches import Patch

# 1. Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# If region column does not exist, mapping countries to regions (only if necessary)
if 'Region' not in df.columns:
    # Example mapping; you may want a more complete mapping
    region_map = {
        'W...",8,"The generated code provides a good attempt at the task. It successfully generates most of the requested visualizations using the specified libraries (matplotlib, seaborn, plotly).

**Strong Points:**
*   **Correctness:** All 7 visualization types are implemented, and they generally produce the intended plots.
*   **Good Use of Libraries:** Effectively uses Plotly for the map, Seaborn and Matplotlib for others, as requested.
*   **Proper Labels and Titles:** Most plots have appropriate titles and axis labels.
*   **Handling Missing `Region`:** The code includes a commendable attempt to map countries to regions if the 'Region' column is missing, although the `region_map` provided is incomplete.

**Areas for Improvement:**
*   **Dataset Handling:** The `region_map` logic is an unnecessary added complexity. The ground truth dataset (`world-happiness-report-2021.csv`) already contains a `Regional indicator` column, which should be used directly. The generated code attempts to create a `Region` column, which is both redundant and prone to errors due to incomplete mapping. This also means plots relying on 'Region' might have less accurate groupings than if `Regional indicator` were used.
*   **Column Renaming:** The ground truth renames columns for better readability and consistency (`Ladder score` to `Happiness Score`, `Logged GDP per capita` to `GDP per Capita`, etc.). The generated code uses the original column names, which can be less clear in plots and might lead to typos.
*   **Radar Chart Normalization/Scaling:** The ground truth code uses `MinMaxScaler` globally on the relevant features across the *entire dataset* before selecting the top 5 countries. This ensures fair comparison on the radar chart as values are scaled relative to the global min/max for each factor. The generated code's normalization (`norm = lambda x: (x - x.min()) / (x.max() - x.min())` applied to `df[radar_factors].apply(norm)`) seems to normalise each column independently based on *its own* min/max, which is correct for individual column scaling, but the critical part is whether `top5_norm = norm_data.loc[top5.index]` then correctly selects the *normalized* values. The ground truth's approach is more robust for cross-country comparison on the radar chart. Additionally, the ground truth explicitly sets y-ticks for normalized values (0-1), which the generated code omits (`ax.set_yticklabels([])`).
*   **Code Structure/Robustness:** The ground truth includes `try-except` for file loading and print statements for progress, which are good practices. The generated code lacks these.
*   **Scatter Plot Legends:** The legends for the scatter plots are placed far from the plots by `bbox_to_anchor=(1,1)`, which makes them hard to read in some contexts. The ground truth uses `bbox_to_anchor=(1.05, 1)` and `loc='upper left'` which is standard and often better.
*   **Consistent Renaming for Factors:** While the ground truth renames factors for plots, the generated code uses original column names ('Ladder score', 'Logged GDP per capita' etc.) but then defines `factors` list with these names. This is less readable.

Overall, the generated code is functional and addresses most requirements, but could benefit from better data handling (especially regarding the `Regional indicator` column) and more robust visualization practices as seen in the ground truth.",gpt-4.1,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# 1) Load the dataset and parse 'date' column as datetime
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2) Display the first 10 rows
print(""First 10 rows:"")
print(df.head(10))

# 3) Show the shape of the dataset (rows and columns)
print(""\nShape of the dataset (rows, columns):"")
print(df.shape)

# 4) List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# 5) Total sum of 'to...",8,"The generated code provides a mostly correct and complete solution to the task.

**Strong Points:**
*   Correctly loads the dataset and parses the 'date' column.
*   Successfully displays the first 10 rows, dataset shape, and column data types.
*   Calculates and displays continental summaries for 'total\_cases' and 'total\_deaths' correctly.
*   Identifies the top 5 locations based on latest date and total cases, which is a good solution for this specific step.
*   Calculates and displays summary statistics for 'new\_cases' and 'new\_deaths'.
*   Correctly counts and displays missing values.
*   Includes all necessary imports and has a clear structure.

**Areas for Improvement (Why not a 9 or 10):**

1.  **Error Handling for File Loading:** The generated code lacks robust error handling for `FileNotFoundError` or other exceptions during CSV loading (Step 1). The ground truth includes `try-except` blocks which are crucial for real-world applications.
2.  **Column Type Conversion for Statistics:** While `agg` works for numeric columns, it implicitly handles non-numeric types by ignoring them. The ground truth explicitly converts columns to numeric using `pd.to_numeric(errors='coerce')` before calculating statistics (Step 7). This makes the code more resilient to unexpected data types in the 'new\_cases' or 'new\_deaths' columns and highlights potential data quality issues.
3.  **Handling NaNs in GroupBy (Continental Summary):** The ground truth explicitly drops NaNs in the 'continent' column before grouping (`dropna(subset=['continent'])`) to avoid a 'NaN' group. While Pandas `groupby` might exclude NaNs by default for sum, it's good practice to be explicit, especially if aggregations like `mean` or `count` were involved, or if a 'NaN' group is genuinely undesirable. However, for `sum` specifically, Pandas usually handles NaNs in the grouped column by ignoring them, so this is less critical than the other points.
4.  **Clarity for `df.dtypes` vs `df.info()`:** While `df.dtypes` is correct as per the literal prompt ""List all column names and their data types"" (Step 4), `df.info()` from the ground truth provides more comprehensive information (non-null counts, memory usage) which is generally preferred in EDA. This is a minor point, as `dtypes` *does* meet the requirement.
5.  **Robustness for Top 5 Locations:** The ground truth adds more robust type conversion (`pd.to_numeric(errors='coerce')`) and `dropna` for 'total\_cases' when determining the top 5 locations (Step 6), ensuring that if `total_cases` were not numeric, the code wouldn't break and would handle it gracefully. The generated code assumes 'total\_cases' will always be numeric.",gpt-4.1,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1) Calculate and report the average 'quality'
mean_quality = df['quality'].mean()
print(f""Average wine quality: {mean_quality:.2f}"")

# 2) Group by 'quality' and calculate average 'alcohol'
avg_alcohol_by_quality = df.groupby('quality')['alcohol'].mean()
print(""\nAverage alcohol content by wine quality:"")
print(avg_alcohol_by_quality)

# 3) Create bar...",9,"The generated code correctly addresses all parts of the task: calculating the mean quality, grouping by quality to find average alcohol, and creating a bar chart. It uses appropriate libraries (pandas, matplotlib, seaborn) and the code structure is clean and readable.

Strengths:
- Correctly calculates and prints the average quality.
- Correctly groups by quality and calculates the mean alcohol, printing the result.
- Creates a bar chart with appropriate labels and title.
- Uses `sns.barplot` effectively.
- Includes `plt.tight_layout()` for better plot presentation.

Areas for minor improvement (to reach a perfect 10):
- Error handling for file not found (though not explicitly requested, it's good practice).
- The `print` statement for average alcohol content is slightly less descriptive than the ground truth.
- The plot could benefit from a few more aesthetic touches like setting a style (e.g., `sns.set_style`), adjusting font sizes, or potentially adding data labels on the bars for precise values, as done in the ground truth. However, these are minor details and not critical for the task's core requirements.",gpt-4.1,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# 1. Load dataset and parse 'date' column as datetime
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2. Handle missing values
df['hosp_patients'] = df['hosp_patients'].fillna(0)
df['icu_patients'] = df['icu_patients'].fillna(0)

numerical_cols = ['positive_rate', 'reproduction_rate']
for col in numerical_cols:
    mean_val = df[col].mean()
    df[col] = df[col].fillna(mean_val)

# 3. Create 'mortality_rate' column
with np...",9,"The generated code is highly correct and complete, addressing almost all requirements. It effectively loads data, handles missing values as specified, creates the new rate columns with appropriate handling for division by zero, and filters the data for 2021.

Strong points:
- Efficient data loading with `parse_dates`.
- Correct handling of missing values for specified columns (filling with 0) and other numerical columns (filling with mean).
- Correct calculation and handling of `mortality_rate` and `vaccination_rate_per_capita` including division by zero using `np.errstate` and `replace`.
- Correct filtering for the year 2021.
- Clear and concise output.

Areas for minor improvement:
- The missing value handling for `positive_rate` and `reproduction_rate` implicitly assumes they are already numerical. While `pd.read_csv` often infers types correctly, the ground truth code explicitly converts them to numeric with `errors='coerce'` before calculating the mean, which is a more robust practice in case of non-numeric entries in these columns. This is a subtle point but adds robustness.
- The ground truth code includes a `try-except` block for `FileNotFoundError`, which is good practice for robustness in real-world scenarios, though not strictly required by the prompt.

Overall, the generated code performs very well and meets the task requirements effectively.",gpt-4.1,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load dataset and parse 'date'
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2. Time series line plot of 'new_cases' for 'India'
india_df = df[df['location'] == 'India'].sort_values('date')
plt.figure(figsize=(10,6))
sns.lineplot(data=india_df, x='date', y='new_cases')
plt.title('Daily New COVID-19 Cases in India')
plt.xlabel('Date')
plt.ylabel('New Cases')
plt.tight_layout()
plt.sh...",7,"The generated code correctly addresses most of the requirements.

**Strong points:**
- All required visualizations (time series line plot, bar chart, scatter plot, histogram, box plot) are implemented.
- Proper titles and labels are included for all charts.
- Matplotlib and Seaborn are used as requested.
- The 'date' column is correctly parsed.
- The use of `parse_dates=['date']` in `read_csv` is concise.
- For the scatter plot, the `latest_entries` logic using `sort_values` and `groupby().tail(1)` is correct for getting the latest data per location.
- Handles `dropna()` for the histogram, which is good practice.

**Areas for improvement:**
- **Bar Chart (Total Deaths by Continent):** The prompt specifies ""sum of 'total_deaths' for each 'continent'"". The generated code simply sums `total_deaths` for each continent across all available dates. However, to truly reflect the ""total deaths"" for a continent, one should ideally sum the *latest* `total_deaths` value *per country*. This is a more nuanced interpretation often required when dealing with cumulative metrics. The ground truth correctly implements this by taking `df.sort_values('date').drop_duplicates(subset='location', keep='last').groupby('continent')['total_deaths'].sum()`. This is a significant difference in calculation accuracy for this specific plot.
- **Error Handling:** The ground truth includes `try-except` blocks for file loading, which is robust and good practice in real-world applications. The generated code lacks this.
- **Aesthetics and Readability:** While functional, the generated code doesn't use `sns.set_theme` or include additional visual enhancements like `plt.grid` or `plt.xticks(rotation=...)` for better readability, especially for bar charts or plots with many x-axis labels. It also doesn't handle `xscale` or `yscale` for the scatter plot, which can be beneficial for widely distributed data like GDP per capita and total vaccinations.
- **Completeness (Scatter Plot Data Filtering):** The scatter plot for `gdp_per_capita` vs. `total_vaccinations` should ideally filter out NaNs from both columns *before* plotting to avoid potential issues or misinterpretations. While `dropna()` is used for the histogram, it's missing for the scatter plot, which might still plot points incorrectly if either GDP or vaccinations are missing for the latest entries.
- Missing `print` statements that indicate progress, as present in the ground truth code.",gpt-4.1,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# 1. Load dataset
df = pd.read_csv('clash_royale_cards.csv')

# List all column names and their data types
print(""Column Names and Data Types:"")
print(df.dtypes)
print(""\n"")

# 2. Descriptive statistics for specified numerical columns
cols = ['elixirCost', 'Win Rate', 'Usage']
stats = df[cols].agg(['mean', 'median', 'std', 'min', 'max'])
print(""Descriptive Statistics for elixirCost, Win Rate, Usage:"")
print(stats)
print(""\n"")

# 3. Total number of cards (rows)
prin...",9,"The generated code correctly performs all the requested tasks: loading the dataset, listing column names and data types, calculating descriptive statistics for the specified columns, and reporting the total number of rows. It uses appropriate pandas functions efficiently.

**Strong points:**
*   Correctly loads the CSV and stores it in a DataFrame.
*   Accurately identifies and prints column data types using `df.dtypes`.
*   Correctly selects the specified numerical columns and calculates the required descriptive statistics (`mean`, `median`, `std`, `min`, `max`) using the `agg` method, which is concise and effective.
*   Correctly identifies the total number of rows using `len(df)`.
*   Includes the necessary `import pandas as pd`.

**Areas for minor improvement:**
*   **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and other general exceptions when loading the CSV. The generated code lacks this, which could lead to a script crash if the file is not found.
*   **Clarity of Column Names Listing:** The ground truth specifically iterates and prints each column name, making the output more explicit for ""Column Names"" separate from ""Data Types of Each Column"". The generated code combines this by just printing `df.dtypes`, which shows both but doesn't explicitly list just the names as a separate point. While `df.dtypes` does implicitly show names, explicit listing reinforces the requirement.
*   **Verbiage:** The ground truth adds more descriptive print statements (`Dataset 'file_path' loaded successfully.`, `--- Column Names and Data Types ---`, etc.) that make the output more user-friendly and clearly delineate each section of the analysis. The generated code is a bit more succinct in its print statements.

Overall, it's a very good solution that directly addresses the core requirements. The lack of robust file loading error handling is the main reason for not giving a perfect score.",gpt-4.1,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Filter cards with elixirCost <= 3
filtered_df = df[df['elixirCost'] <= 3]

# Select Card and Win Rate columns
result = filtered_df[['Card', 'Win Rate']]

# Sort by Win Rate in descending order
result_sorted = result.sort_values(by='Win Rate', ascending=False)

print(result_sorted)
```",9,"The generated code correctly implements all requirements of the task: loading the CSV, filtering by `elixirCost`, selecting the specified columns, and sorting by `Win Rate` in descending order. The code is concise and clear.

Areas for slight improvement, though not critical for correctness in this specific context:
*   **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions, which is good practice for robust code, especially when dealing with external files. The generated code assumes the file will always be present, which could lead to a `FileNotFoundError` if the CSV is missing.
*   **Constants/Readability:** The ground truth uses constants for column names and values (`ELIXIR_COST_COL`, `MAX_ELIXIR_COST`, etc.) which enhances readability and maintainability. For a small script, string literals are acceptable, but for larger projects, constants are preferred.

Overall, the generated code fulfills the core requirements effectively and is a good solution for the given task.",gpt-4.1,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Plot the histogram for 'elixirCost'
plt.figure(figsize=(8, 6))
sns.histplot(df['elixirCost'], bins=range(int(df['elixirCost'].min()), int(df['elixirCost'].max()) + 2), kde=False)

plt.title('Distribution of Elixir Cost')
plt.xlabel('Elixir Cost')
plt.ylabel('Card Count')
plt.tight_layout()
plt.show()
```",8,"The generated code correctly loads the CSV and generates a histogram for the `elixirCost` column with a title and labeled axes, fulfilling the core requirements. The use of `seaborn.histplot` is good practice for data visualization.

Areas for improvement:
1.  **Error Handling**: The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. This makes the code less robust.
2.  **Binning for Discrete Data**: While `bins=range(int(df['elixirCost'].min()), int(df['elixirCost'].max()) + 2)` attempts to create appropriate bins, hardcoding a more specific range (e.g., `range(1, 12)` as in the ground truth) would be more precise for elixir costs, which are typically integers from 1 to 10. The current dynamic binning might not always perfectly align with discrete integer values, especially if the min/max values are not typical.
3.  **X-axis Ticks**: The ground truth explicitly sets `xticks` to ensure clear integer labels for elixir costs, which is important for discrete data. The generated code does not explicitly handle this, potentially leading to less clear x-axis labels if matplotlib decides on non-integer ticks.
4.  **Figsize and Aesthetics**: The ground truth uses a slightly larger `figsize` and adds `color` and `edgecolor` parameters to `histplot` for better visual appeal. These are minor but contribute to plot quality.
5.  **Output Clarity**: The ground truth includes `print` statements for successful loading and error messages, which aids in debugging and user feedback.",gpt-4.1,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('iris.csv')

# 1. List all column names and their data types
print(""Column Names and Data Types:"")
print(df.dtypes)
print(""\n"")

# 2. Descriptive statistics for numerical columns
num_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
print(""Descriptive Statistics:"")
for col in num_cols:
    print(f""\nColumn: {col}"")
    print(f""  Mean: {df[col].mean()}"")
    print(f""  Median: {df[col].median()}"")
    print(...",8,"The generated code correctly addresses most of the requirements of the task.

**Strengths:**
*   **Correct Loading:** Loads `iris.csv` successfully.
*   **Column Names and Data Types:** Correctly identifies and prints column names and their data types using `df.dtypes`.
*   **Descriptive Statistics:** Calculates mean, median, standard deviation, min, and max for all specified numerical columns.
*   **Unique Species Counts:** Correctly reports the number of unique species and their individual counts using `nunique()` and `value_counts()`.
*   **Proper Imports:** Includes `import pandas as pd`.

**Areas for Improvement:**
*   **Robustness:** The ground truth includes a `try-except` block for file loading, which makes the code more robust against `FileNotFoundError`. The generated code assumes the file exists.
*   **Output Formatting for Statistics:** While the statistics are correct, the descriptive statistics for numerical columns are printed in a loop, resulting in a somewhat verbose output. The ground truth uses `df[existing_numerical_columns].agg(['mean', 'median', 'std', 'min', 'max'])` or `df.describe()` which provides a more concise and tabular summary, which is generally preferred for data analysis.
*   **Clarity of Column Names Listing:** The ground truth explicitly prints ""Column Names: [list of names]"" and then ""Data Types: [dtypes]"". The generated code only prints `df.dtypes` under the ""Column Names and Data Types:"" heading. While `df.dtypes` implies the column names, an explicit list makes it clearer.
*   **Error Handling for Missing Columns:** The ground truth includes checks to ensure the specified numerical columns and the 'Species' column actually exist in the DataFrame before attempting operations. This makes the code more robust if the CSV file structure deviates from expectations (e.g., misspelled column names). The generated code would throw a `KeyError` if a specified column was missing.",gpt-4.1,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

# Load the iris.csv dataset
df = pd.read_csv('iris.csv')

# Filter for records where Species is 'Iris-setosa'
filtered_df = df[df['Species'] == 'Iris-setosa']

# Select and display only SepalLengthCm and SepalWidthCm columns
print(filtered_df[['SepalLengthCm', 'SepalWidthCm']])
```",8,"The generated code correctly addresses all functional requirements of the task: it loads the `iris.csv` dataset, filters it to include only 'Iris-setosa' species, and then selects and displays only the `SepalLengthCm` and `SepalWidthCm` columns. The imports are correct, and the code is structured logically.

The primary area for improvement is robustness. The ground truth code includes error handling for `FileNotFoundError` and `KeyError`, which are common issues when dealing with external data files and specific column names. While the generated code works perfectly under ideal conditions (i.e., `iris.csv` exists and has the expected columns), it lacks these important safeguards, which are considered good practice for production-ready data science code. However, given that the prompt specifically asked for ""only the Python code without explanations"" and focused on the core logic rather than robust error handling, the current solution effectively performs the requested operations.",gpt-4.1,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('iris.csv')

# Create color mapping for species
species = df['Species'].unique()
colors = dict(zip(species, ['red', 'green', 'blue']))

# Plot
plt.figure(figsize=(8, 6))
for sp in species:
    subset = df[df['Species'] == sp]
    plt.scatter(subset['SepalLengthCm'], subset['SepalWidthCm'],
                label=sp, color=colors[sp])

plt.title('Sepal Length vs Sepal Width by Species')
plt.xlabel('Sepal...",9,"The generated code correctly addresses all the requirements of the task. It properly loads the CSV, creates a scatter plot of `SepalLengthCm` vs `SepalWidthCm`, colors the points by `Species`, and includes a title and axis labels. The imports are correct and standard.

The primary difference from the ground truth is the explicit color mapping which limits the number of species it can handle without manual modification (though for Iris, it's fine). The ground truth code uses a more dynamic approach that iterates through species and `matplotlib` assigns default distinct colors, which is generally more robust for datasets with many unique categories. However, for a fixed and known dataset like Iris, the generated code's approach is perfectly acceptable and explicit.

Minor improvements could include:
- Adding error handling for `FileNotFoundError` or missing columns, similar to the ground truth, for robustness.
- Using `alpha` and `edgecolors` in the scatter plot for better visualization of overlapping points, as done in the ground truth.
- While `plt.tight_layout()` is good, `plt.figure(figsize=(10, 7))` or similar is generally better for readability for typical plots. The chosen `(8, 6)` is adequate.

Overall, it's a very good and correct solution to the problem statement.",gpt-4.1,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# 1. List all column names and their data types
print(""Column Names and Data Types:"")
print(df.dtypes)

print(""\n"")

# 2. Descriptive statistics for numerical columns
numerical_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']
for col in numerical_cols:
    print(f""Statistics for {col}:"")
    print(f""  Mean:  {df[col].mean()}"")
    print(f""  Median: {df[col].median()}"")
    print(f""  Std:   {df[col].std()}"")
    p...",7,"The generated code correctly performs the requested operations: loading the CSV, printing column names and data types, calculating descriptive statistics for the specified numerical columns, and reporting value counts for 'HeartDisease'.

However, it lacks some robustness and completeness compared to the ground truth:
1.  **Error Handling:** The generated code does not include any error handling for `FileNotFoundError` or other potential issues when loading the CSV, which is a critical omission for real-world applications. The ground truth includes robust error handling.
2.  **Explicit Column Names Output:** While `df.dtypes` implicitly shows column names, the task asked to ""List all column names and identify the data type of each column."" The ground truth explicitly lists column names first, then shows dtypes, which better matches the phrasing.
3.  **Descriptive Statistics Format:** The ground truth uses `df[existing_numerical_cols].describe().loc[['mean', '50%', 'std', 'min', 'max']]` which provides a concise and well-formatted table of statistics for all numerical columns at once, making it easier to read and compare. The generated code iterates and prints statistics column by column, which is less efficient and harder to interpret quickly.
4.  **Clarity for HeartDisease:** The ground truth adds an interpretation note for `HeartDisease` (0 = No Disease, 1 = Disease), which improves readability and understanding for the user.

Despite these points, the core logic for each task requirement is present and functional. The score of 7 reflects that it is a correct and functional solution but could be improved in terms of robustness, output formatting, and user-friendliness.",gpt-4.1,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# Filter the dataset
filtered_df = df[(df['Age'] > 50) & (df['HeartDisease'] == 1)]

# Select and display specific columns
result = filtered_df[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]
print(result)
```",9,"The generated code correctly performs all the required steps: loading the CSV, filtering by `Age` > 50 and `HeartDisease` == 1, and selecting/displaying the specified columns. Imports are correct. The code is concise and directly addresses the prompt.

Areas for minor improvement (not critical for this specific task, hence the high score):
- The Ground Truth wraps the logic in a function and adds error handling (e.g., `FileNotFoundError`, `KeyError`), which makes it more robust and reusable. For a production-level script, this would be preferred. However, sticking to the problem statement, the generated code satisfies all explicit requirements.
- The Ground Truth also includes print statements for clarity about the output, which is a good practice for data analysis scripts.",gpt-4.1,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Filter wines with 'alcohol' > 12 and 'pH' < 3.3
filtered_df = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]

# Identify the wine with the highest 'quality' score
if not filtered_df.empty:
    idx = filtered_df['quality'].idxmax()
    highest_quality_wine = filtered_df.loc[idx]
    print(highest_quality_wine)
else:
    print(""No wines match the given criteria."")
```",9,"The generated code correctly implements all specified requirements: filtering based on two conditions, finding the maximum quality wine within the filtered subset, and displaying its features. The use of `idxmax()` and boolean indexing is efficient and idiomatic pandas. It also includes a good check for an empty filtered DataFrame.

**Strengths:**
*   Correctly applies compound filtering conditions.
*   Correctly identifies the wine with the highest quality using `idxmax()` and `loc`.
*   Displays all features of the selected wine.
*   Includes a check for an empty `filtered_df` to prevent errors.
*   Properly imports pandas.

**Areas for Minor Improvement (to reach a perfect 10):**
*   **Error Handling (File Not Found):** The generated code lacks explicit error handling for `FileNotFoundError` if 'winequality-red.csv' is not present. The ground truth includes a `try-except` block for this.
*   **Function Encapsulation:** While not strictly required by the prompt ""Provide only the Python code"", wrapping the logic in a function (as in the ground truth) promotes reusability and modularity, which is good practice for larger scripts.
*   **Print Statements:** The ground truth includes more informative print statements about the dataset loading, filtering results, and clear headers, which enhances user understanding during execution. This goes beyond the minimal requirement but is part of ""good code"" for data analysis.",gpt-4.1,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('heart.csv')

# Histogram for Age
plt.figure(figsize=(8, 5))
sns.histplot(df['Age'], bins=20, kde=False, color='skyblue')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

# Bar chart for Sex
plt.figure(figsize=(6, 5))
sex_counts = df['Sex'].value_counts().rename({'M': 'Male', 'F': 'Female'})
sns.barplot(x=sex_counts.inde...",7,"The generated code correctly addresses the main requirements of loading the data and generating both a histogram for Age and a bar chart for Sex, with titles and labeled axes. The imports are proper, and the structure is clear.

However, there are a few areas for improvement, especially regarding the 'Sex' column:
1.  **Handling of 'Sex' labels:** The `heart.csv` dataset typically encodes 'Sex' as 0 for Female and 1 for Male. The generated code attempts to rename `'M'` to `'Male'` and `'F'` to `'Female'` using `rename({'M': 'Male', 'F': 'Female'})`, which will not work as the column contains numerical values (0 and 1), not 'M' and 'F'. This will lead to incorrect or uninformative labels on the bar chart. The ground truth correctly uses `.map()` to convert 0/1 to 'Female'/'Male'.
2.  **Histogram Details:** While `kde=False` is acceptable, including KDE (Kernel Density Estimate) as in the ground truth often provides a more insightful visualization of the distribution shape.
3.  **Error Handling (Minor):** The ground truth includes robust error handling for `FileNotFoundError` and `KeyError`, which is good practice for real-world applications. The generated code lacks this, which is a minor point for this specific task but a significant difference in production readiness.

Overall, it's a good attempt, but the incorrect handling of the `Sex` column's categorical mapping is a significant drawback for an accurate visualization.",gpt-4.1,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

# 1. Load CSV and list columns with data types
df = pd.read_csv('spotify_2023.csv')
print(""Column names and data types:"")
print(df.dtypes)
print()

# 2. Convert 'streams' to numeric, coerce errors, report NaNs
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
num_nans = df['streams'].isna().sum()
print(f""Number of NaNs in 'streams' after conversion: {num_nans}"")
if num_nans > 0:
    print(""Rows with NaN in 'streams':"")
    print(df[df['streams'].isna()]...",8,"The generated code correctly addresses almost all aspects of the task.

**Strong points:**
- Correctly loads the CSV.
- Accurately lists column names and data types using `df.dtypes`.
- Correctly converts the `streams` column to numeric, handling errors with `coerce`, and reports the number of NaNs.
- Calculates and reports the requested descriptive statistics (mean, median, std, min, max) for all specified numerical columns.
- Correctly calculates and reports the total number of unique artists.
- Uses proper imports.

**Areas for improvement (to achieve a higher score):**
- **Error Handling for File Loading:** The generated code lacks explicit error handling (e.g., `try-except FileNotFoundError`) for the `pd.read_csv()` call. While the task didn't explicitly demand it, robust code should include it, as seen in the ground truth.
- **Reporting specific NaNs:** While the generated code reports the count of NaNs in `streams`, the ground truth also provided a sample of rows where NaNs occurred, which is more informative for debugging or understanding the data quality issues.
- **Robustness for column existence:** The ground truth checks if numerical columns for stats or the `artist(s)_name` column actually exist in the DataFrame before attempting operations on them, which makes the code more robust against variations in schema. The generated code assumes these columns will always be present, which could lead to `KeyError` if the dataset changes.",gpt-4.1,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Filter for songs released in 2023
df_2023 = df[df['released_year'] == 2023]

# Ensure 'streams' is numeric, coerce errors to NaN
df_2023['streams'] = pd.to_numeric(df_2023['streams'], errors='coerce')

# Select relevant columns
result = df_2023[['track_name', 'artist(s)_name', 'streams']]

# Sort by 'streams' descending
result_sorted = result.sort_values(by='streams', ascending=False)

# Display the sorted ...",9,"The generated code correctly addresses all requirements: loading the CSV, filtering by `released_year` == 2023, selecting the specified columns, converting `streams` to numeric with error coercion, and sorting by `streams` in descending order. The code is concise and well-structured, using proper pandas operations.

The only minor point for improvement, which is not a correctness issue but a common practice, is that the ground truth handles potential `FileNotFoundError` and other exceptions during file loading. While the generated code works under the assumption the file exists, adding basic error handling would make it more robust. Additionally, the generated code modifies the `df_2023` dataframe slice directly by converting `streams` to numeric, which can sometimes raise a `SettingWithCopyWarning`. While it's not a fatal error here and pandas often handles it gracefully in simple cases, explicitly working on a copy (e.g., using `.loc` for filtering and then modifying) can prevent such warnings in more complex scenarios. However, for this task, the current approach is functionally correct.",gpt-4.1,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 1. Histogram of bpm
plt.figure(figsize=(8, 5))
plt.hist(df['bpm'], bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of BPM')
plt.xlabel('BPM')
plt.ylabel('Frequency')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# 2. Scatter plot: danceability_% vs energy_%
plt.figure(figsize=(8, 5))
plt.scatter(df['danceability_%'], df['ene...",9,"The generated code correctly loads the dataset, generates the histogram for `bpm`, and creates the scatter plot for `danceability_%` vs `energy_%`. Both plots have titles and appropriately labeled axes, and all necessary imports are present. The code is concise and directly addresses all requirements.

Minor areas for improvement include:
- The `plt.show()` call after the first plot will display it and then close it before the second plot is generated and displayed. For a unified display of multiple plots, it's generally better to call `plt.show()` once at the very end after all figures have been created, or use subplots (`plt.subplots`). While the prompt didn't explicitly forbid multiple `show()` calls, it often leads to a less fluid user experience.
- The title for the scatter plot could be more descriptive, e.g., 'Relationship between Danceability and Energy' instead of 'Danceability vs Energy'.
- The absence of error handling for file not found or missing columns makes it less robust than the ground truth. While not strictly requested by the prompt, it's a good practice for production-ready code.

Overall, it's a very good solution that fulfills the core requirements.",gpt-4.1,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# 1. List all column names and their inferred data types
print(""Column names and data types:"")
print(df.dtypes)

# 2. Clean 'Video views', 'Likes', 'Dislikes' columns
num_cols = ['Video views', 'Likes', 'Dislikes']
for col in num_cols:
    df[col] = df[col].astype(str).str.replace(',', '', regex=False)
    df[col] = pd.to_numeric(df[col], errors='coerce')
    df[col] = df[col].fillna(0)

# 3. Descriptive...",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans the specified numeric columns by removing commas, converting to numeric types, and filling NaNs with 0. It also correctly calculates and reports descriptive statistics for these cleaned columns. Furthermore, it fills missing `Category` values with 'Unknown' and reports the count of unique categories.

**Strong points:**
*   **Correctness:** All logical steps for data cleaning and analysis are correctly implemented.
*   **Conciseness:** The code is quite concise, especially the descriptive statistics calculation using `.agg()`.
*   **Readability:** The code is easy to read and understand.
*   **Completeness:** All 5 sub-tasks outlined in the prompt are successfully completed.

**Areas for minor improvement (to reach a perfect 10):**
*   **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is good practice for robust scripts. The generated code assumes the file exists.
*   **Output Formatting:** While the output contains the required information, the ground truth's output is more verbose and clearly labeled, which could be beneficial for user understanding, e.g., explicitly printing ""Column names and data types"" heading as done for descriptive stats. The `df.info()` from the ground truth also provides more detail (non-null counts) than just `df.dtypes`.
*   **Column Existence Check:** The ground truth explicitly checks if a column exists before attempting to process it (e.g., `if col in df.columns:`). While unlikely to fail for a known dataset, this adds robustness.

Overall, it's a very good solution that fully addresses the prompt's requirements. The minor points are mostly about robustness and output verbosity rather than core functionality.",gpt-4.1,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Video views' column
df['Video views'] = (
    df['Video views']
    .astype(str)
    .str.replace(',', '', regex=False)
)
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce').fillna(0)

# Filter for videos with more than 50,000,000 views
filtered_df = df[df['Video views'] > 50_000_000]

# Select and display 'Video' and 'Video views', sorted descending by views
result = filtered_...",10,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans the 'Video views' column by removing commas, converting to numeric, and filling NaNs with 0. It then correctly filters the data for views greater than 50,000,000, selects the specified columns ('Video' and 'Video views'), and sorts the final result in descending order by 'Video views'. The code is concise, well-structured, and uses appropriate pandas functions. It is functionally identical to the ground truth code in its core logic, although it combines a couple of steps, which is perfectly acceptable and often preferable for brevity. The main difference is the lack of `try-except` for file loading, but the task description implicitly assumes the file is present and doesn't explicitly ask for error handling, so this is not a deduction.",gpt-4.1,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Likes' and 'Video views' columns
for col in ['Likes', 'Video views']:
    df[col] = df[col].astype(str).str.replace(',', '', regex=False)
    df[col] = pd.to_numeric(df[col], errors='coerce')
    df[col] = df[col].fillna(0)

# Scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(df['Video views'], df['Likes'], alpha=0.5)
plt.title('Video Views vs. Likes')
plt.xlab...",9,"The generated code correctly performs all the required tasks: loading the dataset, cleaning the specified columns (removing commas, converting to numeric, filling NaNs with 0), and generating a scatter plot with the correct title and axis labels. The imports are proper, and the code is structured logically.

Minor improvements could include:
1. Adding error handling for `pd.read_csv`, especially `FileNotFoundError`, for robustness, similar to the ground truth.
2. The `alpha` value for `plt.scatter` is slightly different (0.5 vs 0.6 in ground truth), but this is a minor aesthetic choice.
3. Adding `plt.grid(True)` could improve readability, but it wasn't an explicit requirement and the plot is still clear without it.
4. Using `plt.tight_layout()` is good practice for preventing label overlaps, though not strictly necessary for this simple plot.

Overall, it's a very good solution that fully addresses the prompt's requirements.",gpt-4.1,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# 1. Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# 2. Clean 'Mobile Weight' column
df['Mobile Weight'] = (
    df['Mobile Weight']
    .astype(str)
    .str.replace('g', '', regex=False)
    .str.strip()
)
df['Mobile Weight'] = pd.to_numeric(df['Mobile Weight'], errors='coerce')

# 3. Clean 'RAM' column
df['RAM'] = (
    df['RAM']
    .astype(str)
    .str.replace('GB', '', regex=False)
    .str.strip()
)
df['RAM'] = pd.to_numeric(df['...",9,"The generated code correctly addresses almost all requirements of the task. It properly loads the CSV, cleans all specified columns (`Mobile Weight`, `RAM`, `Battery Capacity`, `Launched Price (USA)`) by removing units, handling `USD ` prefix and commas, and then converting them to numeric, correctly using `errors='coerce'` to handle conversion errors. It successfully prints the column names and their inferred data types after cleaning. It also calculates and reports the mean, median, std, min, and max for the cleaned numeric columns, and correctly reports the count of unique values in the `Company Name` column.

The only minor area for improvement is in the presentation of the descriptive statistics. While the calculation is correct, the output format is slightly less concise than using `df.describe()` for an array of columns which is often preferred in data analysis. The current output prints each column's stats individually, which is fine, but can be a bit verbose for multiple columns. This is a stylistic choice rather than a functional error. The problem statement also asked for ""proper imports"" and ""assume the CSV file is in the current directory"" which the code adheres to. It does not wrap the logic in a function or use `if __name__ == ""__main__"":` block, but the prompt only asked for ""the Python code without explanations"", implying a script directly executable, which this provides. All core functionality is robust and correct.",gpt-4.1,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean 'Launched Price (India)'
df['Launched Price (India)_clean'] = (
    df['Launched Price (India)']
    .astype(str)
    .str.replace('INR ', '', regex=False)
    .str.replace(',', '', regex=False)
)
df['Launched Price (India)_clean'] = pd.to_numeric(df['Launched Price (India)_clean'], errors='coerce')

# Clean 'RAM'
df['RAM_clean'] = (
    df['RAM']
    .astype(str)
    .str.replace('...",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans both the 'Launched Price (India)' and 'RAM' columns by removing specified characters and converting to numeric (handling errors with `coerce`), filters the data for Apple phones under 100,000 INR, selects the specified columns, and sorts the final result.

The code quality is good, using method chaining for cleaning which enhances readability. Using new `_clean` columns is a good practice for not overwriting original data, although the prompt implied cleaning in-place. The `np` import is unused, which is a minor, unnecessary import. The code doesn't include error handling for `FileNotFoundError` when loading the CSV, which was present in the ground truth for robustness, but the prompt didn't explicitly ask for it. Overall, a very strong solution.",gpt-4.1,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean RAM column
df['RAM_clean'] = pd.to_numeric(df['RAM'].str.replace('GB', '', regex=False).str.strip(), errors='coerce')

# Clean Battery Capacity column
df['Battery_Capacity_clean'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', '', regex=False).str.strip(), errors='coerce')

# Plot
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=df,
   ...",9,"The generated code is very good and accurately solves the task.

**Strong points:**
*   **Correctness:** It correctly loads the dataset, cleans both `RAM` and `Battery Capacity` columns by removing units and converting to numeric, handles conversion errors by coercing to `NaN`, and generates the specified scatter plot.
*   **Completeness:** All specific requirements of the prompt are met: scatter plot with correct axes, colored by `Company Name`, and including a title and axis labels.
*   **Code Quality:** Imports are correct. The cleaning steps are concise and efficient, leveraging `pd.to_numeric` with `errors='coerce'` effectively. The use of `seaborn.scatterplot` is appropriate for coloring by a categorical variable. `plt.tight_layout()` is also a good practice for plot presentation.

**Minor points for improvement (why not a 10):**
*   **Handling `FileNotFoundError`:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. While not explicitly asked for in the prompt's requirements list, robust code in a real-world scenario would include this. The generated code assumes the file is always present.
*   **`.astype(str)` for cleaning:** The ground truth includes `.astype(str)` before `.str.replace()`. While in many cases pandas might implicitly convert non-string types to string for `.str` access, explicitly converting to string (`.astype(str)`) is safer and more robust, especially if the original columns might contain numeric types or mixed types that aren't easily coerced to string for string methods. In this specific case, `mobiles_2025.csv` likely has these as strings, so it works, but it's a good defensive programming habit.
*   **Legend Placement:** While `bbox_to_anchor` works, it can sometimes be fragile if the plot size or labels change. For a cleaner look when there are many categories, `sns.move_legend` or just letting seaborn place it automatically could be considered, though the chosen method is perfectly functional.

Overall, an excellent solution that directly addresses all core requirements.",gpt-4.1,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd

# 1. Load the dataset and display the first 5 rows
df = pd.read_csv('titanic.csv')
print(df.head())

# 2. Show the shape of the dataset (rows and columns)
print(""\nShape of the dataset:"", df.shape)

# 3. List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# 4. Calculate the survival rate (percentage of passengers who survived)
if 'Survived' in df.columns:
    survival_rate = df['Survived'].mean() * 100
    print(f""\n...",8,"The generated code correctly addresses all the requirements of the task. It properly loads the CSV, displays the first 5 rows, shows the shape, lists column names and data types, calculates the survival rate, and counts missing values.

Strong points:
- The use of `df['Survived'].mean() * 100` for calculating the survival rate is a more concise and idiomatic pandas way than manually summing and dividing, as seen in the ground truth. This is a good practice.
- The inclusion of a check for the 'Survived' column (`if 'Survived' in df.columns:`) adds robustness to the code, preventing potential errors if the column is missing, although the prompt implies its existence.

Areas for improvement (to achieve a higher score like 9 or 10):
- **Output clarity:** The ground truth includes more explicit print statements (e.g., ""1. First 5 rows of the dataset:"", ""2. Shape of the dataset (rows, columns):""). While the generated code has some labels, they could be more descriptive to match the clarity of the ground truth.
- **Error Handling:** The ground truth includes `try-except FileNotFoundError` for loading the dataset, which is a good practice for robustness, especially when dealing with external files. The generated code lacks this.
- **`df.dtypes` vs. `df.info()`:** While `df.dtypes` is correct for listing data types, `df.info()` (used in the ground truth) provides a more comprehensive summary including non-null counts and memory usage, which is often more useful in initial EDA without explicitly being asked for non-nulls. For the specific requirement ""List all column names and their data types,"" both are technically correct, but `df.info()` is generally preferred for a broader overview.

Overall, it's a very good solution that successfully completes the task. The improvements suggested are mostly about minor refinements and robustness.",gpt-4.1,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 1) List all column names and their data types
print(""Column names and data types:"")
print(df.dtypes)
print()

# 2) Calculate and report basic descriptive statistics for 'Year', 'Rating', and 'Runtime'
numerical_cols = ['Year', 'Rating', 'Runtime']
stats = df[numerical_cols].agg(['mean', 'median', 'std', 'min', 'max'])
print(""Descriptive statistics for Year, Rating, and Runtime:"")
print(stats)
print()

# 3) ...",6,"The generated code correctly loads the dataset, lists column names and data types, and reports the total number of movies. It also attempts to calculate descriptive statistics for the specified columns.

However, it has several critical shortcomings that prevent a higher score:
1.  **Error Handling (File Not Found):** The code lacks any error handling for `FileNotFoundError` if `imdb_top_250.csv` is not present, which is a common and important consideration for robust code. The ground truth includes `try-except` for this.
2.  **Robustness for `Runtime` Column:** The most significant issue is its lack of handling for the `Runtime` column's typical string format (e.g., ""142 min""). The `imdb_top_250.csv` dataset commonly has `Runtime` as a string. Without explicit preprocessing to convert ""142 min"" to a numeric value, `df[numerical_cols].agg(...)` will raise a `TypeError` because it cannot compute statistics on a mix of numeric and object types. The ground truth correctly addresses this by checking the dtype and performing the necessary string manipulation and `pd.to_numeric` conversion.
3.  **Missing `df.info()` equivalent:** While `df.dtypes` is outputted, `df.info()` from the ground truth provides a more comprehensive summary including non-null counts, which is often more informative for data understanding.
4.  **Implicit Assumptions about Data Types:** The code assumes `Year`, `Rating`, and `Runtime` will be directly suitable for numerical statistics, which isn't always true for `Runtime` in real-world scraped data.
5.  **Descriptive Statistics Output Format:** The `agg` method might not always produce the same structured output as `describe().loc[['mean', '50%', 'std', 'min', 'max']]` and renaming '50%' to 'median' which the ground truth does for better readability.

To improve, the code needs to:
*   Implement `try-except` for file loading.
*   Add robust preprocessing for the `Runtime` column to convert it to a numeric type, similar to the ground truth.
*   Consider using `df.info()` for column information.
*   Guard against `Runtime` being absent from the dataset or already numeric, as the ground truth does.",gpt-4.1,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Filter movies with Rating >= 9.0
filtered_df = df[df['Rating'] >= 9.0]

# Select Title and Rating columns and sort by Rating descending
result = filtered_df[['Title', 'Rating']].sort_values(by='Rating', ascending=False)

print(result)
```",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, filters movies with a rating >= 9.0, selects the 'Title' and 'Rating' columns, and sorts the result by 'Rating' in descending order. The code is concise and efficient.

The only minor area where the ground truth code excels is the inclusion of robust error handling (FileNotFoundError, KeyError, and general Exception). While the prompt did not explicitly require error handling, it's a best practice for production-ready code. However, given the prompt's focus on the data manipulation steps, the generated code achieved the core task perfectly.",gpt-4.1,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 1) Histogram of the Rating column
plt.figure(figsize=(8, 5))
sns.histplot(df['Rating'], bins=20, kde=True, color='skyblue')
plt.title('Distribution of IMDb Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

# 2) Scatter plot of Rating vs Runtime
plt.figure(figsize=(8, 5))
sns.scatterplot(data=df, x='Runtime', y='Rating'...",7,"The generated code successfully loads the data and creates both the histogram and scatter plot with appropriate titles and labels. This addresses the core requirements of the task. The imports are correct, and the code structure is clean and readable.

However, a significant drawback is the lack of robust error handling for file loading (e.g., `FileNotFoundError`) and, more critically, for data preprocessing of the `Runtime` column. The ground truth code correctly anticipates that `Runtime` might be a string (e.g., ""120 min"") and provides logic to extract numeric values and handle `NaN`s. The generated code assumes `Runtime` is directly numeric, which is a common issue with raw CSV data and would likely lead to an error or an incorrect plot if the column is not pre-cleaned. This makes the code less robust for real-world scenarios where data formats might not be perfectly clean, as often happens in data science. Implementing this data cleaning for `Runtime` would significantly improve its robustness and completeness.

Improvements:
1. Add `try-except` block for `pd.read_csv` to handle `FileNotFoundError`.
2. Implement data preprocessing for the `Runtime` column to convert it to a numeric type, handling units (e.g., "" min"") and potential non-numeric entries. This is crucial for the scatter plot to function correctly and robustly.",gpt-4.1,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load dataset
df = pd.read_csv('penguins.csv')

# 1. Report number of missing values (NA) for each column
print(""Missing values per column:"")
print(df.isna().sum())

# 2. For the specified numerical columns
cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_num = df.dropna(subset=cols)

print(""\nDescriptive statistics for numerical columns (after dropping NA rows):"")
for col in cols:
    stats = {
        'mean': df_num[col].mean...",7,"The generated code correctly performs all the core tasks: loading the data, reporting missing values, dropping NAs from specified columns and calculating descriptive statistics, and counting unique values.

However, there are several areas for improvement in terms of robustness, clarity, and output formatting:

1.  **Error Handling (Robustness):** The ground truth includes `try-except` blocks for `FileNotFoundError` when loading the CSV, which is crucial for robust code. The generated code assumes the file exists, which is less ideal for general use.
2.  **Output Clarity and Formatting:**
    *   The missing values report in the generated code just prints `df.isna().sum()`. While correct, the ground truth filters this to show only columns *with missing values* and adds more descriptive print statements, making the output clearer and more concise for the user.
    *   The descriptive statistics output in the generated code iterates through columns and prints individual stats. The ground truth uses `df_filtered[numerical_cols_for_analysis].agg(['mean', 'median', 'std', 'min', 'max'])`, which produces a much cleaner, more tabular, and easier-to-read output, typical for statistical summaries.
    *   The ground truth also reports the number of rows before and after filtering for the descriptive statistics, which is valuable context.
3.  **Completeness (Implicit `exit()`):** While the generated code performs the requested calculations, the ground truth's use of `exit()` after a file not found error is a good practice for scripts where subsequent operations would fail definitively.
4.  **Redundancy in Unique Value Checks:** The ground truth checks (`if 'species' in df.columns:`) if the columns exist before attempting `nunique()`. While for this specific task with a known dataset it might not fail, it's a good practice for more generalized code.

Overall, it's a functional solution but lacks some best practices for robust and user-friendly data analysis scripts.",gpt-4.1,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the dataset
filtered = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')]

# Select specific columns
selected = filtered[['culmen_length_mm', 'culmen_depth_mm']]

# Drop rows with missing values
cleaned = selected.dropna(subset=['culmen_length_mm', 'culmen_depth_mm'])

# Display the first 5 rows
print(cleaned.head())
```",9,"The generated code correctly implements all the requirements of the task: it properly loads the CSV, filters for 'Adelie' species on 'Torgersen' island, selects the specified culmen columns, drops rows with NAs in those columns, and displays the first 5 rows. The code is clean, concise, and uses appropriate pandas operations. It follows good code quality practices with proper imports and logical flow.

The only minor area for improvement, preventing a perfect 10, is the lack of basic error handling for file loading, which is present in the ground truth. While not explicitly asked for in the prompt's requirements, robust code often includes it. However, given the prompt only asked for ""Provide only the Python code without explanations"" and specified the core data manipulation steps, the generated code successfully meets all explicit requirements.",gpt-4.1,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows with missing values in specified columns
df_clean = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])

# Create the scatter plot
plt.figure(figsize=(8, 6))
sns.scatterplot(
    data=df_clean,
    x='culmen_length_mm',
    y='culmen_depth_mm',
    hue='species',
    palette='deep',
    s=60,
    edgecolor='w'
)

plt.title('Penguin...",9,"The generated code successfully loads the dataset, filters out rows with missing values as required, and generates the specified scatter plot. It correctly colors points by species and includes a title and axis labels. All imports are correct. The code is concise and directly addresses all requirements of the task.

Minor improvements could include:
1. Encapsulating the plotting logic within a function for better reusability and organization, similar to the ground truth.
2. Adding error handling for `FileNotFoundError` if the CSV file is not present.
3. While `palette='deep'` and `edgecolor='w'` are valid aesthetic choices, they weren't explicitly requested by the prompt, making the ground truth slightly more aligned by not adding extra complexity unless explicitly specified. However, this is a very minor point and doesn't detract from the core functionality.",gpt-4.1,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

# 1. Read the dataset
df = pd.read_csv('penguins.csv')

# 2. Fill NA values in numerical columns with their mean
for col in ['culmen_length_mm', 'culmen_depth_mm']:
    mean_value = df[col].mean(skipna=True)
    df[col].fillna(mean_value, inplace=True)

# 3. Calculate and print statistics for the cleaned columns
for col in ['culmen_length_mm', 'culmen_depth_mm']:
    mean = df[col].mean()
    median = df[col].median()
    std = df[col].std()
    print(f""{col}:"")
  ...",9,"The generated code correctly addresses all requirements of the task: reading the dataset, filling NAs in specified columns with their means, calculating and printing statistics (mean, median, std) for these columns, and printing the total number of rows and columns. The use of `inplace=True` for `fillna` is efficient, and the f-strings for printing are good practice.

Areas for minor improvement:
1.  **Error Handling**: The ground truth code includes basic error handling for `FileNotFoundError`, which is robust. The generated code assumes the file will always be present. While not strictly required by ""Provide only Python code,"" it's good practice for real-world scenarios.
2.  **Readability/Clarity for Output**: The ground truth provides more distinct headers and separators for each section of output (e.g., ""--- NA Imputation Summary ---"", ""--- Statistics for Cleaned Numerical Columns ---""), which improves the readability of the console output. While the generated code's output is clear, adding such headers would enhance presentation.
3.  **Code Structure/Verbosity**: The ground truth uses constants for `DATASET_FILE` and `NUMERICAL_COLUMNS`, which makes the code more maintainable and less prone to typos if these values need to be reused or changed. However, for a small script with limited reuse, directly using string literals is acceptable.

Overall, it's a very solid and correct solution for the given task.",gpt-4.1,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the dataset
df = pd.read_csv('penguins.csv')

# 2. Drop rows where 'flipper_length_mm' is NA
df_clean = df.dropna(subset=['flipper_length_mm'])

# 3. Create a histogram of 'flipper_length_mm'
plt.figure(figsize=(8,5))
sns.histplot(df_clean['flipper_length_mm'], bins=20, kde=True)
plt.title('Distribution of Flipper Length (mm)')
plt.xlabel('Flipper Length (mm)')
plt.ylabel('Count')
plt.tight_layout()
pl...",9,"The generated code correctly addresses all parts of the task: loading the dataset, dropping NA values in the specified column, creating a histogram with appropriate labels, and creating a bar chart for island counts, also with fitting labels. The use of seaborn's `histplot` and `countplot` is efficient and good practice.

**Strengths:**
- **Correctness:** All plots are generated as requested, with correct data and labels.
- **Completeness:** All requirements from the task are met.
- **Code Quality:** Imports are correct, and the code is structured logically following the task steps. Using `plt.tight_layout()` is a good practice for plot readability. Using `df.dropna(subset=...)` specifically targets the required column for NA removal.

**Areas for minor improvement (not critical for this task, but good for robustness):**
- **Error Handling:** The ground truth includes `try-except` blocks for file loading, which is robust but not strictly required by the prompt's assumption ""Assume the CSV is in the current directory.""
- **Informative Prints:** The ground truth code includes print statements to confirm data loading, NA handling, and final success, which can be helpful for debugging or understanding script execution, but are not essential for the core visualization task itself.
- **Styling:** The ground truth sets `sns.set_theme`, which provides a more consistent and professional look. While not explicitly asked for, it's a good practice.
- **Y-axis Label for Histogram:** The ground truth specifies 'Count' which is more explicit than `histplot`'s default (which usually is count unless `stat` is changed), but it's often the default behavior so not a major issue.",gpt-4.1,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# 1. Load the dataset
df = pd.read_csv('penguins.csv')

# 2. Filter data to include only penguins from the 'Torgersen' island
torgersen_df = df[df['island'] == 'Torgersen']

# 3. Count and print the number of penguins belonging to each species
species_counts = torgersen_df['species'].value_counts()
print(species_counts)
```",8,"The generated code correctly implements all the required steps: loading the data, filtering by the 'Torgersen' island, and counting species. It produces the correct output for the given task.

**Strong points:**
- **Correctness:** The logic for loading, filtering, and counting is sound and directly addresses the problem statement.
- **Conciseness:** The code is very direct and uses Pandas functionalities efficiently.

**Areas for Improvement (to achieve a higher score like 9 or 10, similar to the ground truth):**
- **Error Handling:** The code lacks error handling for common issues like `FileNotFoundError` (if `penguins.csv` doesn't exist) or `KeyError` (if 'island' or 'species' columns are missing in the CSV). The ground truth includes robust error handling for these scenarios.
- **Edge Cases:** It doesn't explicitly check if `torgersen_df` is empty after filtering (i.e., no penguins on Torgersen island), which might lead to `species_counts` being an empty Series without an informative message.
- **Output Clarity:** While `print(species_counts)` works, adding a descriptive header to the output (like in the ground truth) would make it more user-friendly.

Overall, it's a good, functional solution for the specific task but could be made more robust and user-friendly with additional error handling and output formatting.",gpt-4.1,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# 1) Print the shape of the DataFrame
print(df.shape)

# ...",10,"The generated code is entirely correct and complete. It accurately defines the DataFrame as specified, and then correctly performs both required operations: printing the shape of the DataFrame and listing the data types of each column. The code is clean, concise, and directly addresses all parts of the task. It perfectly matches the functional requirements of the task.",gpt-4.1,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('titanic.csv')

# 1. Bar chart: Count of survivors vs non-survivors
survivor_counts = df['Survived'].value_counts().sort_index()
plt.figure(figsize=(6,4))
plt.bar(['Not Survived', 'Survived'], survivor_counts, color=['red', 'green'])
plt.title('Count of Survivors vs Non-Survivors')
plt.xlabel('Survival Status')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

# 2. Bar chart: Distribution of passenger...",7,"The generated code correctly creates four distinct plots with proper titles and labels as requested, demonstrating a good understanding of matplotlib and pandas for basic visualizations.

**Strong points:**
- All four required plots are generated.
- Proper titles and axis labels are used for each plot.
- `tight_layout()` is used for better plot presentation.
- Imports are correct.

**Areas for improvement:**
- **Missing Age Value Handling:** The prompt specifically requested ""handle missing values appropriately"" for age. The generated code uses `dropna()`, which removes rows with missing age values. While this is a valid way to handle missing data, a more robust or ""appropriate"" handling for a benchmark visualization task often involves imputation (e.g., filling with median/mean/mode) to retain all data points if possible, which the ground truth code does. This is a crucial difference in data preprocessing.
- **Robustness (File Not Found):** The generated code does not include any error handling for `FileNotFoundError` if 'titanic.csv' is not present, unlike the ground truth. This makes the code less robust for general use.
- **Clarity of Axis Labels (Survival Plot):** While `['Not Survived', 'Survived']` directly maps to the `value_counts()` output for `0` and `1` respectively, explicitly mapping them using `.index.map()` or `.index.map(survivor_labels)` as in the ground truth can be slightly clearer and more adaptable, especially if the numerical labels were not perfectly sorted or standard.
- **Readability/Comments:** The ground truth includes more descriptive comments, especially regarding configuration and the rationale for specific data handling (e.g., choosing median for age). While not strictly required for a ""code only"" submission, it adds to code quality.",gpt-4.1,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]
}
df = pd.DataFrame(data)

# 1. Calculate and print average 'Midterm_Score'
av...",9,"The generated code correctly performs all the required tasks: it creates the DataFrame, calculates and prints the average midterm and final scores, and correctly identifies the subject with the highest average final score. The code is well-structured and uses appropriate pandas functions. It includes the necessary import.

The only minor area for improvement, preventing a perfect 10, is that the output for the ""Subject with the highest average Final Score"" does not include the actual average score itself, which the ground truth code did. While not explicitly asked to print the *value* of the highest average score, it often adds helpful context. However, it successfully identified the subject, fulfilling the direct requirement. The print formatting for the first two averages is good.",gpt-4.1,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

# Create the data dictionary
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
    'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
    'Volume': [10...",9,"The generated code correctly performs all the required tasks: it creates the DataFrame as specified, sets the 'Date' as the index, calculates the 'Daily_Range' column, finds the date with the highest 'Close' price, and prints the updated DataFrame. The code is well-structured and uses appropriate pandas functions.

One minor point of improvement is in the printing of the highest close date. The ground truth code formats the date using `strftime('%Y-%m-%d')` for a cleaner output, especially when the index is a datetime object. The generated code uses `.date()`, which also works but includes additional information like `datetime.date(YYYY, MM, DD)` in the output, which is slightly less clean than the `strftime` output.

Overall, the code is highly correct and complete, demonstrating a strong understanding of pandas operations.",gpt-4.1,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Step 1: Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', '...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It correctly creates the DataFrame, calculates the 'Total_Revenue' column, identifies the best-selling product by quantity, and prints the head of the updated DataFrame. The code quality is good with proper imports and a clear structure.

One minor area for improvement is in the final print statement for the DataFrame's head. The task explicitly asks for the *first 5 rows* which `df.head()` achieves by default, but it explicitly prints ""First 5 rows of the DataFrame including new 'Total_Revenue' column:"" in the ground truth. While `df.head()` defaults to 5, being explicit with `df.head(5)` would match the prompt's wording more precisely, and often good practice. The original code added explicit print statements/tags which enhanced readability of the output. This is a very minor point and doesn't detract from the code's functionality.",gpt-4.1,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 1) Calculate and print the average ""SessionDurationMinutes""
average_duration = df[""SessionDurationMinutes""].mean()
print(""Ave...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the average session duration, and precisely counts users by device type. The code is clean, concise, and directly addresses all parts of the task requirements. The variable names are clear, and the use of pandas methods (`.mean()`, `.value_counts()`) is appropriate and efficient. It fully achieves the desired outcome.",gpt-4.1,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

# 1) Find and print the maximum TemperatureCelsius
max_temp = df[""TemperatureCe...",9,"The generated code is highly correct and effectively solves the task as specified. It accurately defines the DataFrame with the given data and correctly calculates and prints both the maximum ""TemperatureCelsius"" and the minimum ""HumidityPercent"".

**Strong points:**
*   **Correctness:** The code correctly initializes the DataFrame and uses `max()` and `min()` pandas DataFrame methods to find the required values.
*   **Completeness:** All aspects of the task are addressed: DataFrame creation, finding max temperature, and finding min humidity, along with printing them.
*   **Code Quality:** The code is clean, readable, and follows Python best practices. It imports `pandas` as required and uses clear variable names.

**Areas for minor improvement (to reach a perfect 10):**
*   **Output Clarity:** The ground truth code included units (`C` and `%`) in the print statements, making the output more informative for an end-user. While not explicitly asked for, it enhances user experience for numerical data.
*   **Comments:** The ground truth included more detailed comments explaining each step. While the generated code is self-explanatory for this simple task, adding brief comments for each functional block (e.g., ""Define the data"", ""Create DataFrame"", ""Find max temperature"") can improve maintainability and understanding for more complex scripts.

Overall, it's an excellent solution that meets the core requirements effectively.",gpt-4.1,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# 1) Print the total number of ""Likes""
total_likes = df[""Likes""].sum()
print(""Total Likes:"", total_likes)

# 2) Find and print th...",10,"The generated code is correct, well-structured, and complete.

**Correctness:**
*   **DataFrame Definition:** The DataFrame is correctly defined and matches the ground truth.
*   **Total Likes Calculation:** The `total_likes` is correctly calculated using `df[""Likes""].sum()`.
*   **Highest Average Shares:** The `PostType` with the highest average shares is correctly identified using `groupby(""PostType"")[""Shares""].mean().idxmax()`.

**Code Quality:**
*   **Imports:** Proper import of `pandas` is present.
*   **Clarity:** The code is clear and easy to understand. Variable names are descriptive.
*   **Comments:** The comments clearly indicate which part of the code addresses which requirement.

**Completeness:**
*   All three requirements outlined in the task (DataFrame creation, total likes calculation, highest average shares identification) are fully met.
*   The output format for printing is slightly different (e.g., ""Total Likes:"" vs. ""Total number of Likes across all posts:""), but this is a minor stylistic difference and does not affect correctness or completeness of the solution. Both outputs convey the required information clearly.

Overall, the generated code precisely fulfills all the requirements of the task.",gpt-4.1,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It successfully defines the DataFrame, calculates the average age, and counts patients by gender using appropriate pandas functionalities. The code quality is good, with proper imports and a clear structure.

The only minor area for improvement, which is not critical to correctness but enhances readability and output formatting, is the printing of the average age. The ground truth code formats the average age to two decimal places and includes ""years"" in the output string, which is slightly more user-friendly. However, the generated code's output is perfectly functional and shows the correct value. The difference is negligible for a benchmark.",gpt-4.1,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}
df = pd.DataFrame(data)

# 1) Print the ""Title"" of the movie with the highest ""Rating""
highest_rated_title ...",9,"The generated code is highly correct and concise, accurately solving both tasks. It properly defines the DataFrame, correctly identifies the title of the movie with the highest rating using `idxmax()`, and accurately calculates the average year. The code quality is excellent, with proper imports and clear variable names.

The only minor point preventing a perfect score is that the output format for the highest-rated movie title is just the title itself, whereas the ground truth includes a descriptive sentence. While the task stated to ""Print the 'Title'"", which the generated code does literally, the ground truth's verbose output is often preferred in real-world scenarios for clarity. Similarly, for the average year, the ground truth prints it formatted to two decimal places and with a descriptive sentence, which adds to readability. However, these are stylistic differences and do not affect the correctness of the core logic. The use of `idxmax()` is a very efficient and Pythonic way to solve the first task.",gpt-4.1,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

# 1) Print the total number of cars
print(""Total number of cars:"", le...",9,"The generated code correctly defines the DataFrame and calculates/prints the required metrics (total number of cars and average PriceUSD). It successfully addresses all parts of the task. The use of `len(df)` for the total number of cars is a perfectly valid and common alternative to `df.shape[0]`. The code is clean and directly implements the requirements.

The only minor area for improvement that would push it to a 10 is the formatting of the average price. The ground truth code formats the average price to two decimal places, which is good practice for currency. While not explicitly asked for in the prompt, it demonstrates a slightly higher level of polish for numerical output. However, given the prompt's simplicity, the current output is certainly acceptable and correct.",gpt-4.1,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}
df = pd.DataFrame(data)

# 1) Name with highest HP
print(df.loc[df['HP'].idxmax(),...",10,"The generated code is correct, complete, and of good quality. It perfectly solves the task by accurately defining the DataFrame and then correctly identifying and printing the names of the Pokemon with the highest HP and lowest Defense using `idxmax()` and `idxmin()` functions, which are efficient and standard pandas operations for this kind of task. The code is concise and directly addresses all requirements.",gpt-4.1,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd
import numpy as np
import re

# 1. Load the dataset
df = pd.read_csv('titanic.csv')

# 2. Identify columns with missing values
missing_cols = df.columns[df.isnull().any()]
print(""Columns with missing values:"")
print(df[missing_cols].isnull().sum())

# 3. Fill missing Age values with the median age
df['Age'].fillna(df['Age'].median(), inplace=True)

# 4. Fill missing Embarked values with the most frequent port
most_freq_port = df['Embarked'].mode()[0]
df['Embarked']....",9,"The generated code is very good and largely fulfills all requirements.

**Strong points:**
*   **Correctness:** All data cleaning steps (filling missing values, dropping column, creating 'Title') are performed correctly.
*   **Completeness:** All aspects of the task are addressed, including loading data, handling missing values in Age and Embarked, dropping Cabin, extracting Title, and displaying shape/head.
*   **Code Quality:** Imports are proper. The code is well-structured and easy to read.
*   **Efficiency:** Using `inplace=True` is generally efficient for DataFrame modifications.

**Areas for improvement (minor):**
*   **Robustness for 'Title' extraction:** The lambda function for title extraction, while mostly correct, could be more robust. If `re.search` returns `None`, `group(1)` would raise an `AttributeError`. The ground truth handles this by checking `if title_search:`. The generated code uses repeated `re.search` calls which is less efficient and still problematic if `re.search` returns `None` for the first call (though unlikely with the specific regex and data). A `try-except` block or a single call to `re.search` with an `if` condition using the result would be more robust. `df['Name'].apply(lambda name: re.search(' ([A-Za-z]+)\.', name).group(1) if re.search(' ([A-Za-z]+)\.', name) else '')` can be simplified to:
    ```python
    def get_title(name):
        title_search = re.search(' ([A-Za-z]+)\.', name)
        if title_search:
            return title_search.group(1)
        return """"
    df['Title'] = df['Name'].apply(get_title)
    ```
    This is what the ground truth does.

*   **Standardization of 'Title' (optional, but good practice):** The ground truth further standardizes less common titles (e.g., 'Mlle' to 'Miss', 'Capt' to 'Rare'). While the prompt didn't explicitly ask for this, it's a common and beneficial step in data cleaning/feature engineering for such a column. Not including it doesn't make the solution incorrect, but it's an area where the ground truth provides a more refined output.

Overall, the generated code is highly functional and fulfills the prompt's requirements effectively. The minor points are about robust error handling and common best practices, rather than fundamental errors.",gpt-4.1,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}
df = pd.DataFrame(data)

# 1. Print the ""Ti...",9,"The generated code correctly performs all the required tasks: it defines the DataFrame accurately, identifies the title of the book with the highest price, and calculates the total stock. The code is concise and uses appropriate pandas functions (`idxmax()`, `.loc[]`, `sum()`). The only minor improvement would be to include more descriptive print statements for the output, similar to the ground truth code, to clearly indicate what each printed value represents. For example, instead of just `print(title_max_price)`, something like `print(f""Title of the book with the highest price: {title_max_price}"")` would enhance readability for the user of the output. However, this doesn't detract from the correctness or functional completeness of the code.",gpt-4.1,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd

# 1. Load the dataset and display the first 10 rows
df = pd.read_csv('water_potability.csv')
print(""First 10 rows:"")
print(df.head(10))

# 2. Show the shape of the dataset (rows and columns)
print(""\nShape of the dataset (rows, columns):"")
print(df.shape)

# 3. List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# 4. Calculate the percentage of potable vs non-potable water samples
potability_counts = df['Potability']...",7,"The generated code largely addresses all requirements of the task correctly. It properly loads the data, displays the shape, column types, calculates percentages, shows missing values, and provides summary statistics.

However, there are several areas for improvement compared to the ground truth:

*   **Robustness/Error Handling:** The ground truth includes `try-except` blocks for file loading, which is crucial for real-world applications to handle `FileNotFoundError` or other loading issues gracefully. The generated code lacks this.
*   **Column Data Types (`df.dtypes` vs `df.info()`):** While `df.dtypes` does show data types, `df.info()` (used in ground truth) provides a more comprehensive overview, including non-null counts and memory usage, which is standard practice in EDA.
*   **Missing Value Display:** The ground truth selectively prints only columns with missing values (`missing_values[missing_values > 0]`) and also includes a check if there are *no* missing values at all. The generated code prints all columns, even those with zero missing values, which can be less concise.
*   **Clarity of Output:** The ground truth uses descriptive print statements and separators (`=""*50""`) to clearly delineate sections, making the output much easier to read and understand. The generated code's output is functional but less organized.
*   **Potability Interpretation:** The ground truth explicitly adds an interpretation for the potability output (1 = Potable, 0 = Non-Potable), which is helpful context for someone looking at the results.

The generated code is a good start, but it misses some best practices for robust and user-friendly EDA scripting.",gpt-4.1,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd
import numpy as np

# 1. Load the dataset
df = pd.read_csv(""water_potability.csv"")

# Store original info for summary
missing_before = df.isnull().sum()
shape_before = df.shape

# 2. Identify columns with missing values and fill with mean (for numerical columns)
numerical_cols = df.select_dtypes(include=[np.number]).columns
df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())

# 3. Remove duplicate rows
df = df.drop_duplicates()

# 4. Verify and...",7,"The generated code correctly performs the core data cleaning steps: loading, handling missing values, removing duplicates, and displaying the cleaned data. It also attempts a before/after summary.

Areas for Improvement:
- **Error Handling for File Loading:** The generated code lacks explicit error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. This makes the code less robust.
- **Detailed Summary:** The ""summary showing before and after statistics"" is somewhat succinct. The ground truth provides more verbose output for each step (e.g., confirming fill strategy, number of duplicates removed, specific range filter actions), which is more informative for a data cleaning task. The ground truth also explicitly checks `missing_values_before[missing_values_before > 0]` to only show columns with missing values, which is clearer.
- **Range Verification Granularity:** While the generated code uses a more structured `ranges` dictionary for verification, the ground truth's approach of logging how many rows were removed due to range issues, and specifically addressing pH as a boundary condition (0-14) versus other params as non-negative, provides more specific and actionable feedback on the cleaning process. The generated code applies `df = df[...]` directly without reporting how many rows were affected by each range check, making it less transparent.
- **Readability and Comments:** The ground truth code uses more comments and print statements to explain each step, which is beneficial for clarity in a data cleaning script.
- **No `np.number` for `fillna` target:** Using `df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())` is a neat vectorized approach for filling numerical columns, but it implicitly assumes all numerical columns should be filled with their mean. The ground truth explicitly targets columns that *have* missing values within the numerical type. In this specific dataset, this distinction might not matter much, but it's a subtle difference in robustness.
- **Potability Range:** While the `Potability` column in the CSV is 0 or 1, including it in a numerical range check with `(0,1)` might seem redundant or even incorrect if applied after it's understood as a target variable rather than a continuous parameter like pH. The `Potability` column does not typically have ""unreasonable values"" that need a range check in the same way.",gpt-4.1,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('water_potability.csv')

# 1) Bar chart: Distribution of Potable vs Non-potable Water Samples
plt.figure(figsize=(6,4))
sns.countplot(data=df, x='Potability', palette='Set2')
plt.title('Distribution of Potable vs Non-potable Water Samples')
plt.xlabel('Potability (0=Non-potable, 1=Potable)')
plt.ylabel('Count')
plt.xticks([0,1], ['Non-potable', 'Potable'])
plt.show()

# 2) His...",8,"The generated code correctly addresses all the requirements of the task, creating the four distinct visualizations with proper titles and labels. It handles missing values for plotting by using `.dropna()` where appropriate.

**Strong points:**
*   **Correct Visualizations:** All four required plots (bar chart, histograms, boxplot, heatmap) are generated correctly.
*   **Proper Labelling:** Titles, x-axis labels, and y-axis labels are included and clearly describe the content of each plot.
*   **Missing Value Handling:** The code correctly uses `.dropna()` for the histogram plots, which is a good practice for visualizing distributions with NaNs. While `seaborn`'s `countplot` and `boxplot` automatically handle NaNs, explicitly showing awareness for histograms is good. Correlation matrix calculation also handles NaNs appropriately.
*   **Readability and Structure:** The code is well-structured with clear comments indicating each visualization step, making it easy to understand.
*   **Imports:** Necessary libraries (pandas, matplotlib, seaborn) are correctly imported.

**Areas for minor improvement:**
*   **Consistency in `plt.show()`:** For the histograms, `plt.show()` is called inside the loop, which will display each histogram as a separate plot instead of a single figure with three subplots. The ground truth code successfully creates a single figure with subplots, which is generally preferred for presenting related distributions side-by-side. The generated code sets up `plt.figure(figsize=(15,4))` once, but the loop for `plt.show()` causes individual display. This can be fixed by moving `plt.show()` after the loop.
*   **Error Handling for CSV:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a robust programming practice. While not strictly required by the prompt's core task (visualization), it's good for production-ready code.
*   **Minor Customization:** While not a critical flaw, setting a `sns.set_style()` or adding `grid` lines (as seen in the ground truth) can sometimes enhance plot aesthetics and readability.

Overall, the generated code provides a very good and accurate solution to the problem. The issues are minor and easily fixable without fundamentally altering the correctness of the visualizations.",gpt-4.1,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression

# Load data
df = pd.read_csv('winequality-red.csv')

# Feature Engineering: create 3+ new features
df['acid_index'] = df['fixed acidity'] / (df['volatile acidity'] + 1e-6)
df['sulphur_ratio'] = df['free sulfur dioxide'] / (df['total sulfur dioxide'] + 1e-6)
df['density_pH_interaction'] = df['density'] * df['pH']
df['alcohol_squa...",7,"The generated code correctly addresses most of the requirements but has some minor issues and omissions compared to an expert solution.

**Strong Points:**
*   **Meaningful Engineered Features:** The code successfully creates four new features (`acid_index`, `sulphur_ratio`, `density_pH_interaction`, `alcohol_squared`), meeting the ""at least three"" requirement. The use of `1e-6` for division by zero prevention is good practice.
*   **Multiple Feature Selection Techniques:** It correctly implements RFE with RandomForestRegressor and SelectKBest with f_regression.
*   **Comparison of Feature Sets:** The comparison (`intersection`, `only_rfe`, `only_skb`) is correctly performed and displayed.
*   **Display of Selected Features:** The final selected features with their importance/scores are printed.

**Areas for Improvement:**
*   **RFE Feature Importance Calculation:** The way RFE feature importances are calculated is a bit less robust. Setting `rf.fit(X.iloc[:, rfe_support], y)` inside the `pd.Series` constructor can lead to issues if `rfe_support` is empty or if `X` isn't a DataFrame when used with `iloc`. A more explicit two-step process (fit RFE, then extract support, then re-fit the estimator on the *selected* features if you want importance from the reduced set) is safer and clearer, as seen in the ground truth. Also, the `rfe_features` variable is just a copy of `X.columns`, which isn't used correctly for extracting RFE selected features names later.
*   **Clarity in Output/Explanation:** While the printed output shows the results, the ground truth provides more context and explanation for *why* the methods might differ, which is explicitly asked for in ""Compare and contrast the feature sets selected by each method"". The generated code only shows the sets, not the ""why"".
*   **Robustness (Error Handling/NaNs):** The generated code lacks error handling for `FileNotFoundError` and doesn't explicitly handle potential NaN/inf values introduced by feature engineering, which is a common issue with operations like division.
*   **Scaling:** While not strictly mandatory for RFE with RandomForest or SelectKBest with f\_regression, scaling features (`StandardScaler`) is a good practice for general machine learning pipelines and can be crucial for many other models/selection techniques, making the overall solution more robust and complete. The ground truth includes this.
*   **N_FEATURES_TO_SELECT Consistency:** The generated code hardcodes `k=6` for both RFE and SelectKBest. The prompt asked for ""top 5-7 most important features"", and the ground truth used a constant `N_FEATURES_TO_SELECT = 7`, making it more flexible.

Overall, the code performs the core task well but could benefit from improved robustness, more explicit handling of certain steps (like RFE importance), and a more detailed comparative analysis as requested.",gpt-4.1,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor, StackingRegressor
from sklearn.metrics import mean_absolute_error, mean...",7,"The generated code attempts to address all requirements of the task, but several fundamental issues hinder its correctness and robustness, particularly concerning data preprocessing and model evaluation methodology.

**Strengths:**
*   **Comprehensive Model Coverage:** Implemented all requested ensemble methods (Random Forest, Gradient Boosting, XGBoost, AdaBoost, Voting, Stacking).
*   **Hyperparameter Tuning:** Included GridSearchCV for the best individual model.
*   **Feature Importance Plots:** Attempts to generate these, though with some issues.
*   **Evaluation Metrics:** Used all requested metrics (MAE, MSE, RMSE, R).
*   **Residual Plots:** Generated these for multiple models.
*   **Basic Data Loading and Initial Imputation:** Handles `csv` loading and basic numerical/categorical imputation.

**Areas for Improvement / Critical Issues:**
1.  **Preprocessing Pipeline Issue:** The most significant flaw is the preprocessing. The generated code performs `SimpleImputer` and `pd.get_dummies` *before* the `train_test_split`. This leads to **data leakage**, where information from the test set influences the imputation and one-hot encoding performed on the training set. The ground truth correctly uses `ColumnTransformer` and `Pipeline` to ensure preprocessing steps are fitted *only* on the training data within each cross-validation fold or train/test split.
2.  **Incorrect Cross-Validation for Tuning and Evaluation:**
    *   For the initial cross-validation of individual models, the code uses `cross_val_score` on `X_scaled` and `y`. While `cross_val_score` performs internal splitting, the scaling (`StandardScaler().fit_transform(X)`) is done on the *entire dataset* before cross-validation, again leading to data leakage. The `ColumnTransformer` and `Pipeline` approach in the ground truth is crucial for correct cross-validation.
    *   Hyperparameter tuning uses `GridSearchCV` without incorporating the preprocessing steps *within* the pipeline, meaning scaling is still applied to the entire dataset beforehand.
    *   The ""Final Model Evaluations on Hold-out Test Set"" is based on `X_train_scaled`/`X_test_scaled` from a *single* `train_test_split` instead of a robust cross-validation approach for final assessment, which was implicitly requested by the general CV requirement for all evaluations.
3.  **Feature Importance for XGBoost and Plotting:** While `plot_importance` is used, the initial `model.fit(X_train_scaled, y_train)` for all models might re-train them, potentially overriding the models already trained during `cross_val_score`. The `plot_importance` function needs the `XGBRegressor` object that has been trained, not just the class. The ground truth handles this by explicitly fitting the final model for importance calculation. Also, `X.columns` used for feature names in feature importance plots does not account for the `pd.get_dummies` expansion (e.g., `Category_A`, `Category_B`), which the ground truth addresses by obtaining feature names from the `OneHotEncoder` within the preprocessor.
4.  **Target Handling:** The drop of `Rank` and `Name` from the dataset is not explicitly done at the beginning, though `Rank` is numeric and might be imputed and included in features, and `Name` is categorical and would be one-hot encoded, which is not ideal. The ground truth correctly identifies and drops irrelevant columns.
5.  **`objective` for XGBoost:** While `objective='reg:squarederror'` is good practice, it's missing in some initial XGBoost instantiations (e.g., in the `models` dictionary, in the VotingRegressor, in the StackingRegressor base estimators), leading to a warning.
6.  **`n_jobs` in Models:** The generated code doesn't consistently use `n_jobs=-1` for models that support it (like `RandomForestRegressor`, `XGBRegressor`, `VotingRegressor`, `StackingRegressor`) for faster execution, unlike the ground truth.
7.  **Readability/Structure:** The code is functional, but the ground truth's use of `Pipeline` and `ColumnTransformer` creates a much cleaner, more robust, and less error-prone structure for complex ML workflows. The problem also asked for a single Python code without explanations, which the generated code provides.",gpt-4.1,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
# Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.seasonal import STL
from statsmodels.tsa.stattools import adfuller
from statsmodels.tools.eval_measures import rmse

from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression, RidgeCV, Las...",6,"The generated code attempts to address most of the requirements but has significant issues with correctness, completeness, and adherence to best practices in several sections.

**Correctness & Completeness Issues:**

*   **Data Loading and Cleaning:** The generated code renames `Year_of_Release` to `Year` without justification and struggles with `fill_value=0` for missing years in `sales_series.reindex`, which is generally not appropriate for sales data (should typically interpolate or drop). Also, the original `df['Year']` column is used throughout, while the ground truth uses `Year_of_Release`, which is the correct column name from the CSV.
*   **ARIMA Modeling:**
    *   It uses `SARIMAX` with `seasonal_order=(1,1,1,5)` for yearly data, which implies a 5-year seasonality. While the ground truth also attempts a 5-year period for seasonal decomposition if data allows, using it directly in SARIMAX for yearly data is unusual unless a strong 5-year cycle is explicitly known. `pmdarima.auto_arima` (used in ground truth) is a more robust way to automatically determine ARIMA orders, including non-seasonal for yearly data.
    *   The stationarity test is a very basic ADFuller; `pmdarima` handles differencing automatically.
*   **Seasonal Decomposition:** Uses `STL` with `period=5` but doesn't explain the choice or check for sufficient data points like the ground truth.
*   **Multivariate Regression:**
    *   The interaction terms are generated by `PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)` on the *entire* preprocessed data, which will create interactions between *all* features, including one-hot encoded categorical variables with each other and with Year. This is an explosion of features and not what was implied by ""interaction terms... to predict Global_Sales using Year, Genre, Platform, and Publisher"" (which usually means specific, interpretable interactions like `Year * Genre`). The ground truth implements this more carefully by only creating interactions of `Year` with top categories.
    *   The confidence/prediction intervals for multivariate regression are generated using bootstrapping *after* the model has been fitted for prediction, which is a roundabout and less direct method than what `statsmodels` provides (as used in the ground truth for `OLSResults.conf_int()` and `get_prediction().summary_frame()`). Sklearn models don't provide CIs/PIs directly, so `statsmodels` is preferred for this.
*   **Polynomial Regression with Regularization:**
    *   It attempts this on the same features as multivariate regression (`Year`, `Genre`, `Platform`, `Publisher`), applying `PolynomialFeatures` *before* `StandardScaler` within the pipeline. This is incorrect for polynomial regression on a single numerical feature (`Year`) as specified by the ground truth (""Use 'Year_of_Release' as the feature for polynomial regression""). The ground truth performs this specifically on 'Year_of_Release' to demonstrate polynomial curves.
    *   It uses `RidgeCV` and `LassoCV` but applies them to the *entire* `X_train` and `X_test` dataset, not just `Year` as intended for polynomial regression.
    *   Confidence/Prediction intervals are also attempted via bootstrapping, which is a less direct method for `sklearn` models.
*   **Bayesian Regression:**
    *   It uses `pm.Normal` for all `betas` priors without distinct shapes for categorical variables, which is a simplified approach compared to the ground truth's use of `beta_genre[X_bayesian[:, 1].astype(int)]` and `beta_platform[X_bayesian[:, 2].astype(int)]` to correctly model categorical effects.
    *   It samples a fixed number of rows (`df.sample(2500)`), which might lead to inconsistent behavior if the dataset is smaller or if the sample isn't representative.
*   **Survival Analysis:**
    *   The `Longevity` calculation is problematic: `df['Last_Prof_Year'] = df.groupby('Name').apply(...)`. The `df` is the original game-release rows. Applying `groupby('Name')` and then trying to get the `max()` `Year` where `Global_Sales > 0.1` should be done on a pre-aggregated dataset or very carefully. The ground truth aggregates `df` first by `Name` to get `min()` and `max()` `Year` for each game, which is a more robust approach.
    *   `game_longevity['event_observed'] = True` for all entries assumes no censoring, which significantly simplifies survival analysis and might not be realistic. The ground truth correctly adds `Observed_Event = 1` but acknowledges it's a simplification.
    *   For Cox PH model, it `LabelEncoder`s 'Genre' and 'Platform' directly, assuming ordinality, where one-hot encoding is more appropriate for nominal categorical variables, as done in the ground truth. `lifelines` supports formula-based categorical handling, or one-hot encoding.
*   **Confidence and Prediction Intervals:** The generated code states ""see above models"" and then proceeds to do limited bootstrapping for some models. The ground truth systematically generates and prints CIs/PIs for most models, which is a specific requirement.
*   **Time Series Cross-Validation:**
    *   Only `SARIMAX` is validated using `TimeSeriesSplit`. The requirement was to validate *all* models using TS-CV. The ground truth validates ARIMA, OLS, Ridge, and Lasso with TS-CV.
    *   The RMSE calculation for `SARIMAX` is done on `results.forecast(steps=len(y_test))`, which is correct.
*   **Missing Features:** Change Point Detection results are printed as a variable `result` but not mapped back to years for better interpretation, nor is a plot generated as in the ground truth.

**Code Quality:**

*   **Imports:** Mostly good, but some imports (`pm.sample_posterior_predictive`) are implicitly used without being explicitly imported from `pymc3` (though `pymc3 as pm` is fine).
*   **Structure:** The code is structured well according to numbered requirements.
*   **Readability:** Comments are minimal and sometimes inaccurate (e.g., interaction term generation).
*   **Efficiency:** The massive interaction terms for multivariate regression and then repeating `PolynomialFeatures` in `Ridge`/`Lasso` pipelines on inappropriate data can be very inefficient. Bayesian regression only samples 2500 entries, which might not be representative for large datasets.
*   **Error Handling:** Lacks robust error handling (e.g., `try-except` blocks for file loading, model fitting failures, or warnings suppression) which the ground truth includes.

Overall, the generated code demonstrates some understanding of the requested models and techniques but often misses nuances in their correct application, particularly regarding feature engineering for regression, calculation of confidence/prediction intervals, and validation strategies. The reliance on bootstrapping for CIs/PIs where direct methods or more statistical approaches are available (e.g., `statsmodels`) is a notable miss.",gpt-4.1,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, adjusted_rand_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.manifold import TSNE
import umap
from sklearn.neighbors ...",7,"The generated code provides a decent attempt at solving the complex task, covering most of the requirements. However, there are several areas where it falls short in terms of robustness, best practices, and accuracy compared to the ground truth.

**Strong points:**
- Implements all requested modules: K-Means, DBSCAN, Hierarchical, GMM.
- Includes dimensionality reduction (t-SNE, UMAP) with visualization.
- Attempts optimal cluster determination with Elbow, Silhouette, and Gap Statistic.
- Includes consensus clustering (though implementation differs and loses the original algorithm logic).
- Incorporates anomaly detection (LOF).
- Performs association rule mining.
- Builds a recommendation system.
- Includes internal validation metrics.

**Areas for Improvement (leading to a score of 7):**

1.  **Data Preprocessing and Feature Engineering:**
    *   **Regional Sales Distribution:** The generated code creates `p_NA_Sales`, `p_EU_Sales`, etc., which are proportions, but it also creates `Regional_Sales_Distribution` as a list of these values (`.values.tolist()`). This list is then not used in `features` for scaling, which means the model doesn't actually learn from this *distribution* feature. The ground truth correctly uses the individual proportional features.
    *   **Market Position & Publisher Portfolio Size:** The implementation is slightly different. Ground Truth uses `rank(method='dense')` for Market Position, and `nunique()` for `Publisher_Portfolio_Size`, giving a count of *unique* games for that publisher, which is a better measure of portfolio size. The generated code uses `transform('count')` on `Name`, which counts total entries, not unique games, potentially overcounting if games appear multiple times (e.g., across platforms).
    *   **Categorical Encoding:** The generated code uses `LabelEncoder` directly on categorical features for clustering. While this gets the code to run, `LabelEncoder` introduces an arbitrary ordinal relationship (e.g., Apple < Banana < Cherry), which is generally inappropriate for nominal categorical features in most distance-based clustering algorithms. `OneHotEncoder` (as used in the ground truth) is the standard and more robust approach.
    *   **Missing Values:** The handling of `Year_of_Release` is less robust; it uses `SimpleImputer` but doesn't handle other potential missing values in `Name`, `Platform`, `Genre`, `Publisher` which are critical for the analysis, leading to potential silent errors if the input CSV has such issues. Ground truth explicitly drops rows with NaN in these key columns.

2.  **Optimal Cluster Determination:**
    *   **Gap Statistic:** The implementation of the gap statistic is taken directly from a source (Tibshirani et al., 2001) but might not be fully integrated or optimized for direct use within the framework (e.g., the `refs` parameter is unused, and there's an import inside the function). The visualization of the gap statistic is good, but the core logic for `best_k` implicitly assumes the biggest gap is the best without considering the ""one standard error"" rule commonly applied with gap statistic.

3.  **DBSCAN Parameter Tuning:** The generated code hardcodes `eps=1.5` and `min_samples=10`. DBSCAN is highly sensitive to these parameters, and directly picking them without any heuristic (like the k-distance plot used in the ground truth) makes the DBSCAN result potentially irrelevant. The ground truth attempts a heuristic for `eps` based on nearest neighbors, which is a much better practice.

4.  **Consensus Clustering:**
    *   The `consensus_clustering` function's logic is fundamentally different from the ground truth. It uses `SpectralClustering` on a co-occurrence matrix, which is a valid approach, but loses the flexibility of directly applying clustering on the normalized similarity matrix using `AgglomerativeClustering` with `precomputed` affinity, as done in the ground truth. More critically, the ground truth performs consensus by creating a `co_occurrence_matrix` and *then* clustering that matrix using hierarchical clustering, maintaining the original number of samples. The generated code's definition of `consensus_clustering` is slightly different, possibly applying spectral clustering to samples.
    *   When passing `dbscan_labels` to `labels_list` for consensus, it checks `if dbscan_n_clusters>1 else kmeans_models[best_k].labels_`. This is a fallback, but a more robust approach for DBSCAN labels (which can include -1 for noise) would be to handle these noise points gracefully in the consensus matrix calculation (e.g., treating them as disparate or ignoring them, as ground truth does). If DBSCAN produces mostly noise, its contribution to consensus might be problematic.

5.  **Cluster Profiling and Statistical Significance:**
    *   The statistical testing function `stats_tests` tries to be generic but has limitations. For numerical features, it correctly identifies ANOVA/t-test. However, for categorical features, it directly performs `chi2_contingency` on the contingency table of the column vs. cluster. This is generally correct, but the output `print` statement only shows `stats_results` which is a dictionary of `(method, p-value)` without detailed interpretation or comparison. The ground truth provides more readable and interpreted output for statistical tests.

6.  **Anomaly Detection:** Uses LOF, which is a valid choice. The ground truth uses Isolation Forest, which is also valid. Both are acceptable.

7.  **Recommendation System:** The recommendation system is implemented using cosine similarity on the raw (scaled) features selected for clustering. While this is valid, it adds complexity by re-scaling the features for every recommendation. The ground truth's approach (recommend games from the *same cluster* sorted by global sales) is much simpler and directly leverages the clustering output for a market segmentation based recommendation. The user would naturally gravitate towards exploring other highly similar games within the same segment.

8.  **Validation Metrics:** Correctly implements internal validation. The optional external validation (ARI) based on Genre is a good addition, assuming Genre can act as a pseudo-ground truth for categories.

**Overall:** The code is functional and covers the breath of the problem. However, the quality of specific implementations (preprocessing, categorical encoding, DBSCAN parameterization, consensus clustering nuance) and the depth of analysis (statistical test reporting, chosen recommendation logic) are less robust or insightful than the ground truth. It demonstrates understanding of the components but misses some of the ""expert"" best practices.",gpt-4.1,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_validate, GridSearchCV
from sklearn.ensemble import (RandomForestClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier)
f...",7,"The generated code provides a good attempt at fulfilling the complex requirements of the task. It correctly identifies and uses multiple ensemble methods, applies Bayesian optimization with Optuna, demonstrates nested cross-validation, and attempts model calibration and SHAP interpretation.

However, there are several significant areas for improvement, particularly regarding the correct application of pipelines, nested cross-validation for all models, and some logical consistency in the evaluation and interpretation steps.

**Areas for Improvement:**

1.  **Preprocessing in Pipelines/Nested CV:**
    *   The `preprocessor` is defined, but not all models are consistently wrapped in a `Pipeline` with the preprocessor before being put into the `models` dictionary for nested CV. For instance, `xgb_clf` and `catboost_clf` are used directly in the `models` dictionary, but then special `if/elif` blocks are needed in the nested CV loop to apply preprocessing for them (e.g., `xgb_clf` gets wrapped in a pipeline there, but `catboost_clf` still assumes raw input for `cat_features` handling). This inconsistency makes the code less modular and more prone to errors.
    *   The `CatBoost` model's `cat_features` handling is partially correct but not robustly integrated into a pipeline logic, especially considering that `ColumnTransformer` might reorder columns or the dataset is primarily numerical after feature engineering. The `catboost_features` index list needs to be relative to the preprocessed data's column order, which isn't guaranteed if the preprocessor is transforming columns significantly.
    *   The `custom_ensemble` is trained on `preprocessor.fit_transform(X_train_full)`, which implicitly means the `preprocessor` is fitted *outside* the nested CV folds. In a proper nested CV, every model (including the ensemble's base learners) for a given fold should be trained on the training data of *that specific fold*, with preprocessing steps refitted *within* that fold. This breaks the ""unbiased evaluation"" goal of nested CV.

2.  **Nested Cross-Validation Implementation:**
    *   The nested CV loop calculates `roc_auc` for all models, but it only stores the `cross_val_score` result. It doesn't store the predictions or trained models per fold, which are crucial for subsequent steps like model calibration, SHAP interpretation, and ensemble diversity analysis. The ground truth code successfully stored these (using `model_predictions`), allowing for proper downstream analysis on OOF predictions.
    *   The current implementation of nested CV performs training/evaluation on `X_train_full` and `y_train_full`, then evaluates `X_test` and `y_test` separately at the end for calibration and SHAP. This means the `X_test`, `y_test` split is acting as a final hold-out set, which is fine, but the intermediate steps (calibration, SHAP) should ideally use data that wasn't used for training *or* validation, or explicitly state that the evaluation is limited to the final holdout. The ground truth's approach of using *predictions from the nested CV folds* for calibration and diversity analysis is more robust.

3.  **Model Calibration:**
    *   The `CalibratedClassifierCV` is fitted on `X_test, y_test` (`cal_clf.fit(preprocessor.transform(X_test), y_test)`). This is incorrect. Calibration models should be fit on a *separate* calibration set, typically a validation set, not the final test set. Fitting on the test set will lead to an optimistic estimate of calibration.

4.  **SHAP Interpretation:**
    *   SHAP values are calculated on `preprocessor.transform(X_test)`. This is problematic because `preprocessor` might contain an `Imputer` and `Scaler` fitted on the training data. Applying them directly to `X_test` is correct for inference, but the `shap_values` are typically calculated on the *original* features for easier interpretation, or clearly feature names correspond to the transformed features. The `feature_names=lgbm_feature_names` might be partially correct, but if `PolynomialFeatures` or other feature engineering (like the ground truth) were applied as part of the `preprocessor`, these feature names would not align. The code only applies polynomial features in `feature_engineering` *before* the preprocessor, not as part of it, which is a structural difference from the ground truth.

5.  **Custom Ensemble:**
    *   The `DynamicWeightedEnsemble` is an interesting idea but its `fit` method internally performs an `X_tr, X_val, y_tr, y_val` split *within* the `fit` method. This means the splitting happens *before* the outer CV. This is incorrect for a nested CV setup. This ensemble should be designed to receive splits from the outer CV. Also, it uses a preprocessed `X_train_full` for training, which, as mentioned, breaks the nested CV concept.
    *   The custom ensemble's base learners need to be pipelines themselves (`[lr, rf, gboost, xgb_clf, lgbm_clf]`) as they operate on the `X` (raw) data. The current implementation in `custom_ensemble.fit` expects `pre_X` already transformed outside, which is not how you would integrate it into a `cross_val_score` or nested CV loop.

6.  **Feature Engineering and `ColumnTransformer`:**
    *   The `feature_engineering` function modifies the DataFrame *before* the `ColumnTransformer`. This means features created (`BMI_Category`, `Age_Cat`, `Glucose_Insulin`, etc.) are new columns. The `ColumnTransformer` then tries to locate `num_cols` (which correctly excludes the target but includes all other numeric columns) and `cat_cols` (which are hardcoded rather than dynamically detected using `df.select_dtypes(include='category')` *after* creation). This can lead to issues if the feature engineering step changes the column list unexpectedly. The `cat_cols` list appears to be hardcoded, which might miss dynamically created categorical features, or incorrectly include them.

7.  **Completeness & Structure:**
    *   The diversity analysis uses a `disagreement_measure` but it's only on binary predictions, not probabilities, limiting its utility.
    *   The ""Greedy combination"" strategy is a good addition but it runs on the final `X_test` (`predict_proba(X_test)`), which again would lead to an optimistic performance if it were used for decision-making for a model for deployment.

Overall, the code demonstrates an understanding of the components required for the task. However, the exact implementation of the nested cross-validation, the appropriate use of pipelines within this context, and consistent data flow for subsequent analytical steps (calibration, SHAP) are not entirely correct or robust. The ground truth provides a clearer and more generalized structure for managing these complexities during multi-stage evaluation.",gpt-4.1,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pymc3 as pm
import arviz as az
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors
from scipy import stats

# 1) Load and Preprocess the Dataset with Comprehensive Missing Value Handling
df...",6,"The generated code attempts to address all requirements but falls short in several key areas, particularly in the depth and correctness of the implementations, leading to a score of 6.

**Strengths:**
*   **Comprehensive Effort:** The code tries to implement all 10 requested components, demonstrating an understanding of the breadth of the task.
*   **Correct Use of PyMC3/ArviZ:** For the parts where PyMC3 and ArviZ are used (hierarchical model, Bayesian logistic regression, MCMC, WAIC/LOO, PPC), the syntax and general approach are correct.
*   **Data Preprocessing Basics:** Correctly identifies and handles zeros as NaNs and performs standardization.
*   **Structuring:** The code is structured well with comments indicating the fulfillment of each task requirement.

**Areas for Improvement / Weaknesses:**

1.  **Missing Value Handling:**
    *   **KNNImputer:** While `KNNImputer` is a valid method, the ground truth uses `SimpleImputer(strategy='median')` which is often more robust for initial exploration in medical datasets, especially with potential outliers. The generated code uses `KNNImputer` but then proceeds to standardize the *entire* DataFrame after imputation, which is fine, but the problem statement emphasized ""comprehensive missing value handling."" The ground truth also explicitly stated *why* certain columns were imputed with NaN for 0 values.

2.  **Hierarchical Bayesian Model & Mixed Effects:**
    *   **PyMC3 Data Handling:** The `coords` and `pm.Data` setup for the hierarchical model is more verbose than necessary for this dataset. The `pm.Data(""age_group"", df['AgeGroup_code'].values, dims=""obs_id"")` part is appropriate, but `X_shared` dimensions could be simplified or handled directly within the `pm.math.dot` without explicit `dims` in `pm.Data` for simpler models.
    *   **Mixed Effects Implementation:** The generated code states ""Mixed-Effects Models to Account for Individual Variability and Group Effects (already included above)"". While the hierarchical model *is* a type of mixed-effects model, the implementation is very basic (only a group-level intercept). The ground truth provides a more robust and complete mixed-effects model by adding an `alpha_global` (global intercept) and `alpha_group` (group-specific intercepts) for the age group. The ground truth also applies the hierarchical model to binary classification (diabetes risk), which is more aligned with the dataset characteristics than the generic GP in the generated code.

3.  **Bayesian Logistic Regression with Informative Priors:**
    *   The generated code iterates through features and applies priors based on a hardcoded dictionary. While this works, it implicitly assumes `features` is directly mapped to the dictionary keys. The ground truth provides more context for the choice of informative priors based on medical knowledge (e.g., ""Glucose and BMI are known to be strong positive predictors""). The ground truth's use of `beta_featurename` explicitly links the prior to the specific feature, making it clearer.
    *   The linear predictor `pm.math.dot(df[features], beta)` directly computes the dot product, assuming `beta` is a list of parameters in the correct order. This approach is prone to errors if `features` order changes or `beta` names don't align. The ground truth explicitly multiplies each feature by its corresponding beta parameter, which is more robust and readable.

4.  **Survival Analysis:**
    *   **Simulation vs. Ground Truth:** The ground truth goes into much more detail on *how* it simulates the time-to-event data based on a risk score, making it a more comprehensive demonstration. The generated code's simulation is very simplistic (`np.random.uniform` for time, `df['Outcome']` for event), which makes the subsequent survival model less meaningful.
    *   **Weibull Regression:** The ground truth used `lifelines.CoxPHFitter`, a standard and robust approach for survival analysis in a frequentist context, which was also explicitly mentioned in the full problem description. The generated code attempts a Bayesian Weibull regression, which is a valid but arguably more complex choice given the task's stated goals. For a practical demonstration, Cox Proportional Hazards is more common and often a better starting point. More critically, the implementation of the Weibull model in PyMC3 `observed={'value': df['Time'], 'event': df['Event']}` might not directly handle censored data correctly without specific PyMC3 survival modeling features, which is usually `pm.censored.Weibull`.

5.  **Gaussian Processes:**
    *   **Choice of Features:** The generated GP models `Outcome` (binary) based on `Glucose` and `BMI`. GPs are typically used for continuous targets. While binary classification with GPs is possible (e.g., using a Bernoulli likelihood with an `invlogit` link to the latent GP function), the generated code simply uses `pm.gp.Marginal` with a `Normal` likelihood `y_ = gp.marginal_likelihood(""y"", X=X_gp, y=y_gp, noise=sigma)`, which implies a continuous target, incorrect for a binary `Outcome`. The ground truth correctly uses GP for modeling relationships between continuous variables (e.g., Glucose vs. Age).

6.  **Causal Inference:**
    *   **Propensity Score Matching (PSM) Implementation:** The code uses `NearestNeighbors` for matching, but it only matches *treated* individuals to *control* individuals. It doesn't explicitly form a balanced dataset for subsequent analysis and doesn't calculate the ATE (`ate = treated['Outcome'].values.mean() - matched_ctrl['Outcome'].values.mean()`) in a way that truly represents the matched treatment and control outcomes after balancing. The ground truth provides a clearer process, combining `treated` and `matched_control` into a single `matched_df` and then calculating ATT based on the outcome means of those groups.
    *   **Instrumental Variables (IV):** The ground truth correctly points out that finding a valid IV is challenging and provides a plausible (though simulated) IV and a proper Two-Stage Least Squares (2SLS) implementation using `statsmodels.formula.api.ols` for stage 1 and `statsmodels.api.Logit` for stage 2, explicitly detailing the formula. The generated code attempts to use `statsmodels.sandbox.regression.gmm.IV2SLS` but its usage `IV2SLS(df['Outcome'], exog, endog, instrument).fit()` without proper endog/exog variables alignment and formula-based input, makes it less clear and potentially incorrect for the typical 2SLS setup for logistic regression. The ground truth's 2SLS for logistic regression on the outcome is a better and more standard frequentist implementation of IV.

7.  **Model Comparison:**
    *   **Comparison Strategy:** The generated code only compares the hierarchical model and the Bayesian logistic regression. The original prompt usually implies comparing models *of the same type* (e.g., hierarchical vs. non-hierarchical logistic regression). The ground truth builds a specific *non-hierarchical* logistic regression model to enable a direct comparison with the hierarchical model, which is a more sensible comparison for demonstrating the value of hierarchical modeling.

8.  **Credible Intervals and PPCs:**
    *   The plots of `az.plot_forest` and `az.plot_ppc` are correctly called for the models. However, `az.plot_ppc({""y"": ppc[""y""]}, num_pp_samples=100)` for the hierarchical model is fine, but the ground truth `az.plot_ppc(az.from_pymc3(...))` is more robust for aligning observed data with model-generated samples.

In summary, while the generated code attempts to cover all aspects, many of its implementations are either too simplistic (e.g., survival analysis simulation, causal inference IV), potentially incorrect (GP for binary outcome, naive survival model), or lack the robustness and clarity of the detailed ground truth. The overall correctness and completeness suffer due to these issues.",gpt-4.1,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.neighbors import LocalOutlierFactor
from sklearn.model_selection import ...",7,"The generated code attempts to address all aspects of the prompt and demonstrates a good understanding of the required techniques. However, it falls short in several areas regarding correctness, robustness, and completeness as compared to the ground truth.

**Strengths:**
*   **Comprehensive:** Covers almost all requested features: multiple clustering algorithms, advanced dimensionality reduction, anomaly detection, association rule mining, and risk stratification.
*   **Good structure:** Uses functions and clear variable names.
*   **Includes some hyperparameter tuning:** Attempts to find optimal 'k' for K-Means, GMM, Hierarchical, and Spectral, and 'eps' for DBSCAN using silhouette score.
*   **Conceptual handling of Time-Series Clustering:** Acknowledges the data's cross-sectional nature and provides a conceptual outline.

**Areas for Improvement (compared to Ground Truth):**

1.  **Feature Engineering - Metabolic Syndrome Indicators:**
    *   **Ground Truth:** Creates indicators ('High_Glucose', 'High_BP', 'High_BMI') and a composite score ('Metabolic_Syndrome_Score', 'Potential_Metabolic_Syndrome') directly from existing features, which is appropriate for the Pima dataset.
    *   **Generated Code:** Assumes the existence of `Height`, `Waist`, `Triglycerides`, `HDL`, and `Sex` columns for metabolic syndrome definition, which are NOT present in the `diabetes.csv` (Pima Indian Diabetes) dataset. This makes the metabolic syndrome feature engineering incorrect and non-functional for the specified dataset, leading to a `KeyError` or incorrect results if run as is. This is a critical error in data preparation.

2.  **Imputation:**
    *   **Ground Truth:** Explicitly replaces 0s in `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI` with `np.nan` before median imputation, as 0s in these columns are typically missing values in this specific dataset.
    *   **Generated Code:** Uses `SimpleImputer` directly after loading, which will not treat 0s as missing values, thus leaving potentially incorrect 0s in the dataset for these clinical measurements.

3.  **Optimal Parameter Determination & Statistical Validation:**
    *   **Ground Truth:**
        *   Provides visual methods (Elbow, Silhouette plots for K-Means, Hierarchical, Spectral; BIC/AIC for GMM; K-distance graph for DBSCAN) for optimal parameter selection.
        *   Chooses optimal `k` for K-Means, GMM, HC, Spectral based on the plots (or highest silhouette/lowest BIC/AIC).
        *   Uses Kruskal-Wallis for continuous and Chi-squared for categorical features in phenotyping.
    *   **Generated Code:**
        *   Only calculates silhouette scores programmatically for K-Means, GMM, Hierarchical, Spectral, and DBSCAN, but does not visualize and explicitly mention the chosen optimal 'k' from these and uses `np.argmax(spec_silh)` etc. without visual context.
        *   Uses `gap_statistic` (extra library, not standard) and plots an elbow curve but does not clearly link the chosen `k` for clustering to the gap statistic or elbow curve.
        *   Uses Calinski-Harabasz score as a validation metric, which is good.
        *   For phenotyping, it uses `ttest_ind` or `mannwhitneyu` which are suitable for pairwise comparisons (cluster vs. rest), but for comparing across *multiple* clusters simultaneously, ANOVA/K-W (as in GT) or Chi-squared (as in GT) are more standard and direct.
        *   The statistical test for features in `statistical_test_features` is comparing each feature of one cluster against all other samples *combined*, not against other specific clusters, which can be less informative for profiling inter-cluster differences.

4.  **Consensus Clustering:**
    *   **Ground Truth:** Implements a more robust co-occurrence matrix approach and then applies hierarchical clustering on the derived similarity matrix, allowing for optional dendrogram plotting and selection of consensus clusters based on silhouette.
    *   **Generated Code:** Also attempts a co-occurrence matrix and then uses `AgglomerativeClustering` on `1 - consensus_matrix`. This is conceptually correct, but the GT provides more detailed steps for validating the linkage and ensuring validity for `linkage` function from `scipy.cluster.hierarchy`.

5.  **Robustness and Edge Cases:**
    *   **Ground Truth:** Includes thorough error handling (e.g., `try-except` for file loading), checks for the number of unique clusters before calculating silhouette score, and handles cases where not enough algorithms contribute to consensus clustering. Explicitly handles `Spectral` failure or non-converging cases.
    *   **Generated Code:** Less robust in handling cases like insufficient number of clusters for silhouette or `DBSCAN` producing only noise (-1 labels).

6.  **Association Rule Mining Input:**
    *   **Ground Truth:** Explicitly handles `qcut` and `cut` for numerical features and correctly creates binary (`0`/`1`) features for `Outcome` and `Potential_Metabolic_Syndrome` before `get_dummies`.
    *   **Generated Code:** Discretizes numerical features using `qcut` and then converts to string and `get_dummies`, then adds the `Outcome` column separately. The approach is generally okay, but the GT's more explicit handling of feature types (e.g., direct use of binary engineered features) can be clearer.

7.  **Time Series Clustering (Conceptual Realization):**
    *   **Ground Truth:** Provides a thorough explanation of why it cannot be applied and what a robust approach would entail.
    *   **Generated Code:** Provides a shorter explanation and includes a generic `tslearn` example, but it's less comprehensive in its conceptual outline than the ground truth.

8.  **Clinical Interpretation:**
    *   **Ground Truth:** Provides a more detailed and structured ""Clinical Interpretation"" section for each risk level/cluster, outlining what the cluster characteristics mean from a clinical perspective, which is crucial for a ""patient risk stratification system with clinical interpretation.""
    *   **Generated Code:** The `RiskLevel` mapping is present, but the clinical interpretation is less explicit and detailed.

Overall, the generated code is a good effort but lacks the precision, robustness, and detailed clinical rationale that the ground truth code provides, particularly in the critical steps of feature engineering for the specific dataset and the statistical validation for clustering.",gpt-4.1,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
# Imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict, GridSearchCV, RandomizedSearchCV
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, PolynomialFeatures, PowerTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from skle...",6,"The generated code attempts to address all requirements but falls short in several aspects, particularly in correctness and comprehensive implementation of advanced techniques.

**Correctness & Completeness Issues:**

1.  **Data Preprocessing and Pipelines:**
    *   The `PolynomialFeatures` and subsequent re-concatenation of `X_poly_df` are problematic. `PolynomialFeatures` should only be applied to numerical features, and the `drop(num_cols, axis=1)` part is incorrect as it removes original features from the polynomial output, preventing them from being passed to the preprocessor if they are also part of `num_cols`. It also breaks the `ColumnTransformer`'s `num_cols` later if not handled carefully.
    *   The `ColumnTransformer` is applied to `X_train` and `X_test` *separately* instead of being part of a larger `Pipeline` that encapsulates the full preprocessing and modeling steps, which is generally better practice for robust nested CV and avoiding data leakage in real-world scenarios. The ground truth correctly uses a `Pipeline` from the start.
    *   The `Unnamed: 0` column drop is hardcoded, which might not exist in all versions of the dataset or if the user creates a new CSV based on the original. Better to use `errors='ignore'`.
    *   The `df = df.rename(columns={target: 'target'})` is unnecessary and changes the column name to `target`, which is then referenced incorrectly later in `feature_names=list(df[features].columns)`.

2.  **Voting Regressor (Hard Voting):**
    *   The implementation of ""hard voting"" for regression by rounding and taking the mode is conceptually incorrect and not what ""hard voting"" typically means for regression (which is usually simple averaging) nor is it a standard approach. The ground truth correctly interprets ""hard voting"" as simple averaging. Rounding adds an arbitrary level of discretization.

3.  **Bayesian Optimization (Optuna):**
    *   The `objective` function for Optuna directly passes `X_train_prep` and `y_train`. This means preprocessing is done once *before* Optuna, which simplifies the pipeline but means Optuna isn't optimizing parameters that might affect the preprocessing or fitting of the entire process if the preprocessor itself had tunable parameters (though in this case it doesn't).
    *   The `make_scorer` is imported but not used.

4.  **Nested Cross-Validation:**
    *   The `nested_cv_eval` function explicitly mentions ""No hyperparam tuning here for speed, but can include"". A core part of nested cross-validation is performing hyperparameter tuning *within* each inner fold of the outer loop to get a truly unbiased evaluation of the model's generalization performance. The provided implementation skips this crucial step, rendering it not a ""true"" nested CV for robust evaluation *of the entire pipeline including tuning*. The ground truth correctly highlights where Optuna *would* be placed in a full nested CV.
    *   It uses `X_train_prep` and `y_train` for nested CV, meaning it's evaluating a subset of the data that has already been preprocessed and split. A more robust approach would be to conduct nested CV over the original `X` and `y`, with the preprocessor part of the CV pipeline for each fold.

5.  **Custom Ensemble with Dynamic Weights:**
    *   The `base_preds_val` stores predictions on `X_test_prep`, which is the global test set. The OOF predictions should be computed against the validation sets of `kf` for training weights. The `fit` call `m.fit(X_train_prep, y_train)` then makes predictions on `X_test_prep`, which is a form of data leakage if these models were then used for the final ensemble after weights calculation. The ground truth correctly separates OOF prediction for weight calculation and then fitting final models on the full dataset for prediction.

6.  **Advanced Blending and Multi-level Stacking:**
    *   The `blender` function takes `X` and `y` and internally performs `train_test_split`. It then fits models on `X1, y1` and predicts on `X2` to form `blend_train`. However, `blend_all` is formed by predicting on the *full* `X` using models trained on `X1`. This is incorrect as those models haven't seen `X2`'s data, leading to a data leakage problem. The predictions used for `meta_model.predict` (the `blend_all`) should ideally be out-of-fold predictions or predictions on a completely held-out test set if evaluating.
    *   The RMSE for blending is calculated using `blend_pred[:len(y_test)]`, which is a suspicious slice given `blend_pred` size. The logic for blending evaluation seems confused. The ground truth implements blending with a clear train-validation split and correct prediction steps.

7.  **Model Calibration & Uncertainty Quantification:**
    *   `MapieRegressor` is used, which is good. However, the section on ""Ensemble predictive uncertainty: std of ensemble predictions"" is a valid method but is *not* model calibration. Model calibration for regression refers to assessing if the reported confidence intervals (or prediction intervals) truly contain the true value the stated percentage of the time. Residual analysis or specialized calibration plots are typically used for this (which the ground truth covers via residual plots and quantile regression which directly gives intervals).

8.  **Model Combination Optimization:**
    *   The brute-force grid search over weights using `itertools.product` and a fixed increment (0.1) is inefficient and can easily miss optimal weights if the search space is large or the optimal weights are not multiples of 0.1. A proper optimization algorithm (like `scipy.optimize.minimize` used in the ground truth) is more appropriate and efficient.

**Code Quality:**

*   **Imports:** Generally good and comprehensive.
*   **Structure:** Follows the task requirements section by section, which is good.
*   **Comments:** Decent comments explain the purpose of each section.
*   **Modularity:** Some attempt at modularity with the `nested_cv_eval` and `blender` functions, but pipelines are not consistently used as in the ground truth.
*   **Reproducibility:** `random_state` is used consistently across models.
*   **Output:** Prints some performance metrics at the end, but could be more organized. SHAP and LIME plots are set to `plt.close()` but don't show, which is not ideal for demonstration.

**Strengths:**

*   Attempts to cover almost all requested components of the task, showing a broad understanding of advanced ML concepts.
*   Includes Optuna and Mapie, which are advanced libraries.
*   Correctly identifies and implements multiple ensemble methods (Voting, Bagging, Boosting, Stacking).
*   Correctly uses `ColumnTransformer` and `SimpleImputer` for preprocessing.",gpt-4.1,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
# Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import pymc3 as pm
import theano
import theano.tensor as tt

from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.neighbors import NearestNeighbors
from sklearn.model_selection import train_test_split

import semopy
from semopy import Model
import arviz a...",6,"The generated code attempts to address all requirements but falls short in several aspects regarding correctness, robust implementation, and adherence to best practices in statistical modeling and causal inference.

**Positive Aspects:**
*   **Comprehensive Coverage:** The code attempts to tackle almost all listed requirements (hierarchical models, SEM, IV, PSM, Bayesian regression, GP, mediation, DiD, model comparison, PPC, sensitivity).
*   **PyMC3 Usage:** Successfully uses PyMC3 for the Bayesian modeling tasks.
*   **SEMopy for SEM:** Uses `semopy`, which is a valid choice for SEM, though it handles classical SEM not specifically Bayesian SEM.
*   **ArviZ for Post-processing:** Correctly uses `arviz` for model comparison and plotting.

**Areas for Improvement & Critical Missing/Incorrect Implementations:**

1.  **Preprocessing (Item 1):**
    *   **Missing Value Handling:** The ground truth uses `ColumnTransformer` and separate imputers, which is more robust for mixed data types. The generated code uses `KNNImputer` for all numerical after converting some `object` columns to `_code`, which might not be ideal (e.g., `Regional_indicator` should be one-hot encoded for models like the hierarchical one, not label encoded if used as a categorical feature directly in a regression). The `KNNImputer` for `num_cols` before dropping `cat_cols` (which contain `Country_name` and `Regional_indicator`) is problematic if those categorical columns have missing values.
    *   **Column Renaming:** The generated `df.columns.str.replace('(', '').str.replace(')', '')` is risky as it removes parentheses that might be part of valid column names later needed for complex formulas. The ground truth's `[^A-Za-z0-9_]+` is more robust.
    *   **Standardization:** The ground truth explicitly standardizes numerical predictors used in PyMC3 models, which is crucial for parameter interpretation and often for MCMC efficiency with informative priors. The generated code only standardizes for GP, not for other PyMC3 models.

2.  **Hierarchical Bayesian Model (Item 2):**
    *   **Random Effects Implementation:** The generated code uses `region_a[region_idx]*sigma_a` for random effects. This is incorrect. Random effects `region_a` should typically be sampled from a Normal distribution with `mu_a` (the hyperprior mean) and `sigma_a` (the hyperprior standard deviation), i.e., `pm.Normal('region_a', mu=mu_a, sd=sigma_a, shape=len(regions))`. The current implementation explicitly multiplies by `sigma_a`, which isn't the standard hierarchical approach where `sigma_a` is the *scale* parameter of the random effects.
    *   **Priors:** The priors seem somewhat arbitrary (`mu=5`, `sigma=1` for `mu_a`), and not explicitly linked to happiness research literature as requested in the task (Item 5). The ground truth provides more reasoned priors.

3.  **SEM (Item 3):**
    *   **Bayesian SEM:** The task specifically asks for Bayesian SEM (`PyMC3/PyMC4`). The generated code uses `semopy`, which is a classical (frequentist) SEM package. While it fits SEM, it doesn't meet the ""Bayesian"" requirement for SEM.

4.  **Causal Inference (Item 4):**
    *   **Instrumental Variables (IV):**
        *   **Instrument Validity:** The choice of `Perceptions_of_corruption` as an IV for GDP is questionable. It's highly likely to affect happiness directly (exclusion restriction violation). The ground truth acknowledges this limitation in a cross-sectional dataset and *simulates* a valid, plausible IV, which is a much better demonstration for a benchmark.
        *   **Regression Implementation:** The ground truth uses `statsmodels.formula.api` for 2SLS, which is a standard and robust way to implement IV regressions in Python. The generated code manually performs two separate `LinearRegression` fits, which is less ideal and potentially problematic for correct standard error calculations.
    *   **Propensity Score Matching (PSM):**
        *   **ATE Calculation:** The ATE calculation `(treated['Ladder_score'].reset_index(drop=True) - matched['Ladder_score'].reset_index(drop=True)).mean()` assumes exact matching and doesn't handle cases where there might be multiple matches or for caliper matching. Also, it only finds one match for *each treated unit*, assuming perfect matching in indices. The ground truth handles this by pulling the matched control units into a new DataFrame and then correctly computes the mean difference between the *matched* treated and control groups, followed by a t-test for significance, which is best practice.
        *   **Balance Check:** No check for covariate balance after matching, which is crucial for PSM validity.

5.  **Bayesian Regression with Informative Priors (Item 5):**
    *   Addressed partially in the hierarchical model, and then a separate simple model is provided, which is good. The priors are slightly more informative than default but could be more explicitly justified based on literature.

6.  **Gaussian Processes (Item 6):**
    *   **Multivariate X:** The GP implementation uses multiple features as input (`Logged_GDP_per_capita`, `Social_support`, `Healthy_life_expectancy`). While valid, the prompt asked for ""non-parametric modeling of complex relationships"", and the ground truth focused on a single relationship (Happiness vs. GDP) for simpler visualization and demonstration, which is common for initial GP examples if the model gets complex. For a specific task like modeling 'complex relationships', multiple features could be fine.
    *   **Visualization:** No explicit visualization of GP predictions, though `az.plot_posterior` is called for other models. The ground truth includes a specific plot for GP predictions, which is illustrative.

7.  **Mediation Analysis (Item 7):**
    *   **Methodology:** The generated code uses `statsmodels.stats.mediation.Mediation`, which is a frequentist approach. The task asked for these analyses **within the Bayesian framework** if possible, or at least using PyMC3 results. The ground truth correctly uses the posterior samples from the PyMC3 SEM approximation models to calculate the indirect, direct, and total effects and their associated credible intervals. This is a significant difference.

8.  **Difference-in-Differences (DiD) (Item 8):**
    *   **Simulation:** The simulation of time and policy is minimal. The ground truth provides a more robust simulation of a panel dataset and a policy effect linked to existing data characteristics (e.g., top GDP countries).
    *   **Treatment Definition:** The treatment definition `(df['Year']==2021) & (df['Regional_indicator']=='Western Europe')` is too narrow and might result in very few or no treated units, making the DiD less meaningful or unstable.
    *   **DiD Model:** The ground truth correctly defines `treated` and `time` variables based on simulated panel data structure and then uses `treated * time` in the `smf.ols` formula, which is the standard DiD approach capturing the interaction term. The generated code also does `Policy + Year + Policy:Year` which is correct for DiD, but the simulation of `Policy` is very basic.

9.  **Bayesian Model Comparison (Item 9):**
    *   **Bayes Factors:** The calculation of Bayes Factor using `np.exp(log_ml_bayes - log_ml_hier)` from LOO values is not a correct way to compute Bayes factors. LOO is an approximation for cross-validation expected log-likelihood, not the model evidence for Bayes Factors. The ground truth correctly states that Bayes Factors are not directly computed due to complexity, implying a more sophisticated method (like bridge sampling) would be needed, or relies solely on WAIC/LOO, which are more practical.

10. **Credible Intervals, PPC, Sensitivity Analysis (Item 10):**
    *   **PPC:** `az.plot_ppc` is used correctly.
    *   **Credible Intervals:** Already inherent in `az.plot_posterior`.
    *   **Sensitivity Analysis:** Demonstrates a sensitivity analysis by changing priors, which is good. However, the visualization of the comparison between posteriors from different priors isn't as clear as the ground truth's side-by-side histogram/density plots.

Overall, the generated code provides a foundational attempt but lacks the depth, precision, and adherence to specific Bayesian methodologies requested, particularly for SEM, mediation, and causal inference methods, where it frequently defaults to frequentist solutions despite the task explicitly asking for Bayesian approaches where possible (e.g., ""Implement hierarchical Bayesian models...""). The data preprocessing and IV/PSM implementations also have significant robustness and correctness issues.",gpt-4.1,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
# All necessary imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.decomposition import PCA, FastICA
from sklearn.manifold import TSNE, Isomap
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, silhouette_samples, pairwise...",6,"The generated code attempts to address all parts of the prompt, but several sections are incomplete, conceptually flawed, or provide minimal output that doesn't fully fulfill the task's requirements.

Here's a breakdown:

**Strengths:**
*   **Comprehensive Coverage:** It attempts to include all the requested modules and analyses (multiple clustering, DR, optimal params, consensus, profiles, anomaly, network, ARM, recommendations, policy).
*   **Correct Imports:** Most necessary libraries are correctly imported.
*   **Basic Data Preprocessing:** Includes loading, basic cleaning, and feature engineering.
*   **Multiple Clustering Implementations:** K-Means, DBSCAN, Hierarchical, GMM, Spectral, and HDBSCAN are all present.
*   **Dimensionality Reduction:** t-SNE, UMAP, Autoencoder, and Isomap are implemented.
*   **Feature Engineering:** Attempts to do advanced feature engineering like z-scores, quantiles, PCA loading, and region-based stats.

**Weaknesses and Areas for Improvement:**

1.  **Preprocessing and Feature Engineering:**
    *   The feature engineering is somewhat haphazard. Adding `_zscore` and `_quantile` for every numerical feature can lead to a huge feature space, some of which might be redundant.
    *   `region_le` is defined but not used for `X` in clustering unless `Region_encoded` is used from `features` (which it is, but it's just one-hot encoding which is usually better for categorical data). Also, it merges `region_stats` but then doesn't include the specific `region_stats` aggregated columns in `features`, making that merge mostly useless for `X` unless the idea was for `num_features` to capture them indirectly.
    *   `X = StandardScaler().fit_transform(X)` is applied *after* adding `Region_encoded` (which is integer-encoded, not one-hot), which is problematic for scaling categorical features. The original `Regional indicator` should have been one-hot encoded or handled appropriately.
    *   `df[col].fillna(0)` can be problematic for numerical columns if 0 is not a sensible replacement for all fields (e.g., happiness scores). The prompt specified median/mode imputation for numerical/categorical.

2.  **Clustering Algorithms:**
    *   **Optimal K determination:** While it uses `silhouette_score` for K-Means, Hierarchical, GMM, and Spectral, it doesn't clearly display the optimized K or rationale.
    *   **DBSCAN:** `dbscan_scores` being empty is problematic when `clusterings['DBSCAN']` is assigned. It might produce an empty `dbscan_scores` list if no configuration yields >1 cluster and no noise, leading to `df['DBSCAN'] = -1` (all noise) which is not a valid clustering. The prompt asked for iterating through `eps_range` and `min_samples_range`, which it does not fully implement for DBSCAN or HDBSCAN.
    *   **HDBSCAN parameters:** Uses fixed `min_cluster_size` and `min_samples` rather than iterating to find optimal parameters.

3.  **Optimal Clustering Parameters:**
    *   **Gap Statistic:** The implementation of the gap statistic is rudimentary (`n_refs=5` is very low for reliably estimating the reference distribution, common practice is 100-500). Also, it calculates the gap but doesn't explicitly print or describe how `opt_k_gap` is used or its significance.
    *   **Elbow Method:** Only calculates inertia, no visualization is provided.
    *   **Stability Analysis:** Uses a very basic `adjusted_rand_score` on a subsample, which is a start but a more robust stability assessment (e.g., consensus matrix stability, bootstrap validation across multiple runs and metrics) would be needed. It also doesn't specify how stability is used to select optimal parameters.

4.  **Consensus Clustering:**
    *   The `consensus_clustering` function is a very basic majority vote (`mode`). True consensus clustering typically involves building a co-association matrix across multiple algorithm runs (or different initializations of the same algorithm) and then applying clustering on this matrix (e.g., hierarchical clustering on 1-co-association matrix). The current approach is overly simplistic and doesn't align with advanced consensus clustering methods.

5.  **Detailed Country Profiles:**
    *   The statistical significance testing uses `ttest_ind` comparing ""in-cluster"" vs. ""not-in-cluster"" for each feature. While this gives some insight, a more common approach for group comparisons is ANOVA across all clusters for each feature, followed by post-hoc tests if ANOVA is significant. The prompt explicitly mentioned ANOVA `f_oneway`, which is not correctly implemented for all clusters (it compares one cluster to ""all others"" instead of all clusters against each other).
    *   Effect size (Cohen's d) is computed, which is good, but the output `profiles_df` is not explicitly displayed or interpreted, making the ""detailed country profiles"" less concrete without showing the tables of means and p-values etc. explicitly.

6.  **Anomaly Detection:**
    *   Only `IsolationForest` is used. The prompt asked for ""implement multiple clustering algorithms"" *and* ""anomaly detection"", which implied more than one method for anomaly detection (e.g., LOF, One-Class SVM).

7.  **Network Analysis:**
    *   Graph visualization is commented out, which means it isn't readily available in the output.
    *   Cosine similarity is used as the edge weight, which is good.
    *   The `sim_matrix[i,j] > 0.93` threshold for edges is hardcoded and may result in a very sparse or dense graph depending on data. A more adaptive approach (e.g., top N neighbors or percentile) could be beneficial.
    *   It correctly uses `networkx` for basic graph creation and community detection, but the output related to network analysis (e.g., central countries, community structure) is minimal.

8.  **Association Rule Mining:**
    *   The binning to `_HIGH` and `_LOW` is a valid approach, but the categories `Cluster_X` are directly added. A more advanced ARM might look for associations *between* features, not simply feature-to-cluster. `min_support=0.12` and `min_threshold=1.1` might be too restrictive and result in few rules, as is common with ARM.

9.  **Country Recommendation System and Policy Insights:**
    *   **Recommendation System:** The recommendation function `recommend_countries` is simple but effective for basic similarity. However, it's not actually *called* for a specific country in the main execution block (aside from printing `recommend_countries('Finland')`).
    *   **Policy Insights:** The `policy_dict` logic is an interesting qualitative attempt but the thresholds for Cohen's d (`>0.5` and `< -0.5`) are hardcoded. A more detailed policy insight would involve explicitly listing the differentiating factors for each cluster.
    *   **Geopolitical Validation:** The `geopolitical_validation` table is good, but it's only a table, not deeply *used* for validation in the output or policy insights analysis. The prompt required ""geopolitical validation"" as part of the policy insights.

10. **Code Structure and Output:**
    *   The code is structured linearly, not within a class, which was specified in the ground truth (and implied by ""Write Python code...""). Using a class would better encapsulate the logic and state, fulfilling the ""code quality"" criterion.
    *   Minimal print statements for most steps. The task implies that the output should demonstrate the results of these analyses. Many visualizations are commented out.
    *   No clear summary of findings or optimal parameters at the end.",gpt-4.1,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pmdarima import auto_arima
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error

# 1. Load data
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
df = df[df['location'] == 'Germany'].copy()
df = df.sort_values('date')
df.set_index('date', inplace=True)

# 2. Select variables and handle missing values
target_col = 'new_cases'
exog_cols = ['strin...",8,"The generated code successfully implements the requested SARIMA time series forecasting task. It correctly loads and preprocesses the data, handles missing values, splits the data, uses `auto_arima` for order selection, trains the SARIMAX model with exogenous variables, forecasts, evaluates with RMSE, and plots the results.

**Strong points:**
*   **Correct Data Handling:** Properly filters for Germany, sets date as index, and sorts data.
*   **Missing Value Strategy:** Uses `interpolate().ffill().bfill()` which is a reasonable approach for time series, covering various NaN scenarios.
*   **Correct SARIMA Implementation:** Uses `pmdarima.auto_arima` for order selection and `statsmodels.tsa.statespace.sarimax.SARIMAX` for model training and forecasting, including exogenous variables.
*   **Evaluation:** Calculates RMSE correctly.
*   **Visualization:** Provides a clear plot of actual vs. forecasted values.
*   **Code Structure:** The code is well-structured with comments explaining each step, making it easy to understand.

**Areas for minor improvement:**
*   **Missing Value Detail:** While `interpolate().ffill().bfill()` is good, the prompt also mentioned ""forward fill or interpolation"" specifically. The ground truth handles ffill/bfill symmetrically and also adds a robust check for completely empty columns which might not be caught by linear interpolation if a column is entirely NaN. The generated code's approach is generally sufficient but could be slightly more robust if `interpolate` sometimes fails to fill all NaNs.
*   **Plotting Train Data:** The generated code plots `train.index[-50:]` and `y_train[-50:]` for the train data, which only shows the last 50 days of the training set. The ground truth plots the entire training set, which gives a better context of the time series behavior leading up to the test period. Showing the full training data (or a much larger window) on the plot is generally more informative.
*   **Robustness against File Not Found:** The generated code assumes the CSV file exists. The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice for robust scripts.
*   **Negative Forecasts:** The ground truth explicitly handles `forecast_values[forecast_values < 0] = 0`, which is important for 'new\_cases' as they cannot be negative. The generated code does not include this, which could lead to illogical negative forecasts on the plot if the model predicts below zero.
*   **Specific `auto_arima` parameters:** The generated code uses `D=1` directly in `auto_arima` instead of letting `auto_arima` determine `D` (by setting `D=None`), potentially overriding its optimal choice for seasonal differencing. While often `D=1` is a good default for seasonal data, `auto_arima` is designed to find this optimally. The ground truth leaves `d` and `D` for `auto_arima` to determine.
*   **Plot Title:** The plot title could include the RMSE value for quick reference, similar to the ground truth.",gpt-4.1,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import shapiro, kruskal
from statsmodels.formula.api import ols
import statsmodels.api as sm
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import matplotlib.pyplot as plt
import seaborn as sns
import scikit_posthocs as sp

# Load data
df = pd.read_csv('winequality-red.csv')

# Categorize quality
def quality_tier(score):
    if score <= 4:
        return 'low'
    elif score <= 6:
        return 'medium'
    else:
     ...",7,"The generated code correctly categorizes quality groups, applies statistical tests (ANOVA and Kruskal-Wallis), and performs post-hoc analysis. It also generates box plots for significant features and summarizes the findings.

However, there are several areas for improvement:

1.  **Normality Testing Issue**: The code uses `scipy.stats.shapiro` for normality testing. While appropriate, it applies it to `groups` which are already `pd.Series` from `df[df['quality_tier']==tier][prop]`. The issue is with the `all(...)` condition within the loop where `shapiro(g)[1] > alpha` is checked.
    *   **Missing Levene's Test**: The ground truth correctly checks for homogeneity of variances (Levene's test) after checking for normality. The generated code skips this crucial step. ANOVA assumes both normality AND homogeneity of variances. Without checking variance homogeneity, the decision to use ANOVA based solely on normality is incomplete and potentially incorrect.
    *   **Shapiro's Limitations**: The generated code includes `len(g) <= 5000` for Shapiro-Wilk. While it's true Shapiro has a sample size limit, it's typically around 5000. It's good that it's acknowledged, but in this specific dataset (winequality-red.csv has ~1600 rows), it's unlikely to be hit. Still, it's a detail worth noting. The ground truth approaches this by checking if `len(g) < 3` to avoid errors.

2.  **Post-hoc for Kruskal-Wallis (Dunn's test)**: The generated code uses `scikit_posthocs.posthoc_dunn`, which is a good choice. The ground truth implements pairwise Mann-Whitney U tests with Bonferroni correction, which is a common proxy for Dunn's test when `scikit_posthocs` might not be readily available or allowed. Both are acceptable, but the `scikit_posthocs` approach is more direct for Dunn's.

3.  **Visualization Subplots**: The generated code attempts to create multiple subplots for box plots, but it places them all in a single row without wrapping, which might lead to cramped or unreadable plots if there are many significant features. The ground truth creates a new figure for each plot, which is simpler and ensures readability.

4.  **Error Handling**: The ground truth includes basic error handling for `FileNotFoundError`, which is good practice. The generated code lacks this.

5.  **Readability/Structure**: The ground truth has a clearer structure with comments for each distinct step (1. Load Data, 2. Perform Tests, etc.) and comprehensive print statements that clearly indicate what tests are being run and their immediate results. The generated code's print statements are more concise but less informative during execution.

Overall, the generated code performs the core statistical analysis correctly, but overlooks an important assumption for ANOVA (homogeneity of variances) and could be improved in terms of robustness, error handling, and output clarity.",gpt-4.1,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
# Imports
import pandas as pd
import numpy as np

from sklearn.model_selection import GridSearchCV, train_test_split, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, f1_score

import xgboost as xgb

import shap

# 1. Load data, parse 'date'
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2. For each 'locat...",8,"The generated code provides a strong and nearly complete solution to the task. It correctly addresses most of the requirements, including target variable definition, feature engineering, XGBoost training with GridSearchCV, and SHAP explanations.

Here's a breakdown of its strengths and areas for minor improvement:

**Strengths:**

*   **Target Variable Definition:** The target variable is correctly defined by calculating the 90th percentile of `new_deaths` (for `new_deaths > 0`) per location and flagging days where `new_deaths` exceeds this threshold. The use of `apply` and `merge` is a valid approach.
*   **Feature Engineering:**
    *   Rolling averages and standard deviations are correctly implemented using `groupby` and `rolling`.
    *   Lag features are correctly implemented using `groupby` and `shift`.
    *   `days_since_first_case` computation is logical and well-implemented.
*   **Missing Value Handling:** The code correctly identifies engineered columns and fills NaNs with 0, which aligns with the prompt's suggestion.
*   **Feature Selection and Ratio Calculation:** The `vaccinated_ratio` is correctly calculated and included in the feature list.
*   **Preprocessing Pipeline:** The `ColumnTransformer` with `StandardScaler` and `OneHotEncoder` is correctly set up within a scikit-learn `Pipeline`. `handle_unknown='ignore'` for `OneHotEncoder` is good practice.
*   **Model Training and Hyperparameter Tuning:**
    *   XGBoost Classifier is used.
    *   `GridSearchCV` is correctly applied with the specified parameter grid.
    *   The time-based split for train/test data is correctly implemented by sorting by date and slicing.
    *   Using `TimeSeriesSplit` for `GridSearchCV`'s `cv` is a robust approach for time-series data and an excellent choice.
*   **Evaluation:** ROC-AUC and F1-score are correctly calculated and printed.
*   **SHAP Explanations:**
    *   `TreeExplainer` is correctly used for XGBoost.
    *   The code makes a good attempt to get the correct feature names after preprocessing for SHAP plotting. It correctly transforms `X_train` and passes feature names to `shap.summary_plot`.
    *   Calculating and printing the top 10 features by mean absolute SHAP value is a nice touch.

**Areas for Minor Improvement (to reach a 9 or 10):**

1.  **Rolling/Lag Feature Shifting:** The `shift(1)` is applied to the rolling features (e.g., `df[f'{feat}_7d_avg'] = ... .shift(1)`). While this is a valid way to prevent data leakage from the current day's feature to the target, the prompt did not explicitly ask for shifted rolling features. Typically, rolling features are calculated up to and including the current day, and then the model predicts the *next* day's target, or features are only for the *current* day, assuming they are known at the time of prediction. If the model is predicting the current day's target, then shifting these features by 1 (or more) is appropriate to prevent leakage. The ground truth didn't apply this shift, implying the features are for the current day. If `new_deaths` is *not* used in the features, this is less critical. Given the target is `high_daily_death_increase` on the *same day*, this shift *is* actually a good practice to prevent leakage if `new_cases`/`hosp_patients`/`icu_patients` for the *current day* influence the target. The prompt simply stated ""rolling 7-day averages"", without specifying if they should be lagged. My assessment is that the `shift(1)` is a reasonable defensive coding practice here.

2.  **`days_since_first_case` Calculation Detail:** The calculation `df.groupby('location').apply(lambda group: (group['date'] - group[group['new_cases'] > 0]['date'].min()).dt.days)` is a bit complex and could potentially create issues if a location has no `new_cases > 0`. The ground truth used `(x - x.min()).dt.days` on the `date` column itself, which just gives days since the *first date record* for that location. The generated code attempts to calculate days since the first *case*. Both are valid interpretations, but the ground truth's was simpler and directly from the dataframe's date column. The ground truth's `transform` would also be more efficient than `apply` for this.

3.  **`people_fully_vaccinated_ratio` Handling:** The generated code computes `vaccinated_ratio = df['people_fully_vaccinated'] / df['population']` and does not explicitly handle `np.inf` or `NaN` results from division by zero or missing `population` values for this new feature, unlike the ground truth. While `fillna(0)` is applied later for all features, it's good practice to specifically handle the ratio.

4.  **SHAP Plot Title/Labels:** While `shap.summary_plot` is called, adding a `plt.title()` would improve the plot's clarity, as done in the ground truth.

5.  **Imports:** `matplotlib.pyplot as plt` is missing from imports, but `shap.summary_plot` automatically imports it. It's good practice to explicitly include it if plotting functionality is used.

Overall, the code is well-structured, follows good practices (e.g., pipelines, time-series validation), and correctly implements the core requirements. The minor points are mostly about defensive coding or slight variations in interpretation that don't fundamentally break the solution.",gpt-4.1,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from statsmodels.regression.mixed_linear_model import MixedLM
from patsy import dmatrices

# 1. Load data, parse 'date', compute 'vaccination_percentage'
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

df['vaccination_percentage'] = df['people_fully_vaccinated'] / df['population']

# 2. Handle missing values by grouped interpolation/median imputation
cols_to_impute = ['reproduction_rate', 'stringency_index', 'vaccination_perc...",7,"The generated code provides a reasonable attempt at solving the problem but has several critical issues and areas for improvement, primarily in data preprocessing and model fitting.

**Correctness (5/10):**
*   **Missing Value Handling:** This is the most significant flaw.
    *   The `df['vaccination_percentage'] = df['people_fully_vaccinated'] / df['population']` line will produce NaNs if `people_fully_vaccinated` or `population` are NaN, or if `population` is zero, which is not handled properly before calculation. The ground truth correctly handles `population` missing values first to ensure a more robust `vaccination_percentage` calculation.
    *   The imputation logic `df.groupby('location')[col].apply(lambda x: x.interpolate(method='linear'))` and then `df.groupby('location')[col].apply(lambda x: x.fillna(x.median()))` is problematic. `x.median()` on a series `x` within a group will calculate the median *of that group*. If a group is `[NaN, NaN, NaN]`, `x.median()` will also be NaN. The subsequent `df[col].fillna(df[col].median())` tries to catch this, but it's less robust than a forward then backward fill within a group followed by interpolation, which is what the ground truth uses (ffill/bfill for population/vaccinated, then interpolate for the rest, and then global median as a final fallback). The generated code's imputation order and method for `_vaccination_percentage` specifically can lead to issues if `people_fully_vaccinated` or `population` had NaNs initially.
    *   The code directly calculates `vaccination_percentage` *before* ensuring `population` and `people_fully_vaccinated` are clean, leading to potential `division by zero` or `NaN` issues upfront.
*   **Model Specification (Minor):** While `stringency_index_z * vaccination_percentage_z` correctly includes main effects and interaction, the use of `dmatrices` is not strictly necessary or idiomatic for `smf.mixedlm`, which directly accepts string formulas. The prompt asked for `statsmodels.formula.api`, implying `smf.mixedlm`, which is simpler for model specification. The generated code uses `MixedLM` directly from `statsmodels.regression.mixed_linear_model`, which is lower-level and requires `dmatrices`. This is a slight deviation but not entirely incorrect.
*   **Interpretation of Fixed Effects:** The interpretation is very generic. The ground truth provides more specific directionality based on the sign of the coefficient (e.g., ""A higher X is associated with an increase/decrease in Y"").
*   **Random Effects Variance:** `mdf.cov_re` sometimes returns a DataFrame with information about the variance and covariance of random effects, but for a simple random intercept model, typically `mdf.cov_re.values[0,0]` or similar is needed to extract the scalar variance of the random intercept. Just printing `mdf.cov_re` might show the covariance matrix, which for a single random intercept is just a 1x1 matrix. The ground truth explicitly states ""Variance of Random Intercepts (for continent_location_id groups):"" and extracts the specific value.

**Code Quality (6/10):**
*   **Imports:** Uses `dmatrices` and `MixedLM` directly, which implies a lower-level API usage than the `statsmodels.formula.api` that is typically preferred for clarity with formulas. The ground truth's use of `smf.mixedlm` is more aligned with standard practice for mixed models specified by formula.
*   **Missing Value Handling (Structure):** The separate loops for `interpolate` and `fillna` within groups, and then a final `fillna` are less efficient and potentially less robust than the ground truth's chained operations which handle different types of missingness more systematically.
*   **Standardization:** While standardizing predictors is good practice, it was not explicitly asked for and adds complexity (e.g., `stringency_index_z`). The ground truth avoids this for simplicity, as it wasn't a core requirement.
*   **Error Handling:** Lacks robust error handling like the ground truth's `try-except` blocks for file loading and model fitting.
*   **Warnings:** Doesn't handle specific warnings from statsmodels which can be noisy during model fitting.

**Completeness (7/10):**
*   **All Requirements:** Most requirements are addressed, but the quality of implementation varies.
    *   Loading data and parsing date: Yes.
    *   `vaccination_percentage`: Yes, but calculation order is problematic.
    *   Missing values: Addressed, but incorrectly.
    *   Mixed-effects model (fixed/random effects): Yes.
    *   Fit model: Yes.
    *   Report fixed effects coefficients: Yes.
    *   Report variance components: Yes, but presentation could be clearer.
    *   Discuss model assumptions: Yes, correctly.
*   **Interpretation:** The interpretation of fixed effects is present but lacks the depth and specific reasoning that the ground truth provides (e.g., relating the sign of the coefficient to expected directional impact). It doesn't explicitly discuss the interpretation of the interaction term which is complex.

**Further Improvements:**
1.  **Robust Imputation:** Revisit the missing value imputation strategy. First, handle `population` and `people_fully_vaccinated` clean-up (e.g., ffill/bfill for cumulative metrics within groups), *then* calculate `vaccination_percentage`, and *then* consider interpolation/median for all three target variables.
2.  **Use `statsmodels.formula.api`:** Switch to `smf.mixedlm` for cleaner formula specification and alignment with typical usage.
3.  **Error Handling & Warnings:** Add `try-except` for file loading and model fitting, and `warnings.filterwarnings` as in the ground truth to make the code more robust and user-friendly.
4.  **Detailed Interpretation:** Enhance the interpretation of fixed effects, especially the interaction term, providing more insights into what the coefficients mean. Explicitly extract and present the random intercept variance as a scalar.
5.  **Clarity:** Add more print statements to show progress and key data characteristics (e.g., number of unique groups), similar to the ground truth, which aids in understanding the execution flow.",gpt-4.1,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 2. Select features for clustering
features = df[['elixirCost', 'Win Rate', 'Usage']]

# 3. Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(features)

# 4. Determine th...",9,"The generated code successfully completes almost all aspects of the task, demonstrating a strong understanding of the requirements.

**Strong points:**
*   **Correctness:** Loads data, selects and scales features, performs K-Means, PCA, and visualization correctly.
*   **K-selection:** Implements *both* the elbow method (WCSS) and silhouette scores for determining K, which is beyond the minimum requirement and provides a more robust analysis. It also correctly plots both.
*   **Optimal K determination:** Programmatically selects `optimal_k` based on the maximum silhouette score, which is a good approach.
*   **Code Quality:** Generally well-structured, clear variable names, and proper imports. Uses `random_state` for reproducibility.
*   **Completeness:** Addresses most requirements.

**Areas for Improvement (why not a 10):**
*   **Robustness in data loading/feature selection:** The ground truth includes error handling for `FileNotFoundError` and checks for missing columns. The generated code assumes the file exists and the columns are present, which is typical for a benchmark but lacks the robustness of production-ready code.
*   **Visualization clarity:** The scatter plot for PCA uses a loop with `plt.scatter` for each cluster, which is functional but less concise and potentially less performant than using a single `plt.scatter` call with `c=cluster_labels` as shown in the ground truth. Also, the ground truth explicitly adds explained variance to the axis labels, which is very helpful for interpretation.
*   **Minor optimization:** The `n_init=10` parameter in KMeans is good, but adding `max_iter` or a comment about its purpose could be beneficial, though not strictly required.
*   **Flexibility:** While programming the `optimal_k` selection is good, visual inspection of the plots is still important for the elbow method. A hardcoded value (like in GT for demo) or better yet, using a programmatic approach for elbow point inflection would be ideal to show robustness.

Overall, an excellent solution that clearly demonstrates proficiency in the required libraries and concepts. The missing robustness checks and slightly less informative plot are minor but prevent a perfect score.",gpt-4.1,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder

# 1. Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 2. Select features and target
X = df[['elixirCost', 'Usage', 'rarity']]
y = df['Win Rate']

# 3. Preprocess the 'rarity' column
encoder = OneHotEncoder(sparse=False, drop='first')
rarity_encod...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
- Correctly loads the dataset.
- Selects the specified features and target.
- Correctly handles the one-hot encoding for the 'rarity' column using `OneHotEncoder`, which is a valid and robust approach. The use of `encoder.get_feature_names_out()` and `index=X.index` ensures the resulting DataFrame is correctly aligned and has meaningful column names, which is excellent practice.
- Splits the data into training and testing sets with the correct `test_size` and `random_state`.
- Trains a `RandomForestRegressor` model with `random_state`.
- Evaluates the model using R-squared and MSE.
- Includes all necessary imports.

**Areas for improvement (to reach a higher score):**
- **Robustness/Error Handling:** The ground truth includes `try-except` for `FileNotFoundError` and checks for missing columns. While not explicitly asked for in the prompt's main requirements, these are good practices for robust code, especially when dealing with external files and specific column names. The generated solution lacks these checks.
- **Missing Value Handling:** The prompt implicitly assumes a clean dataset or doesn't explicitly ask for missing value handling, but the ground truth demonstrates a basic `dropna()` which is good practice. The generated code does not handle potential `NaN` values in the features or target, which could cause issues if the dataset is not perfectly clean.

Overall, it's a very solid implementation that performs the core task effectively and correctly. The use of `OneHotEncoder` with proper column and index handling is a highlight.",gpt-4.1,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy.stats import f_oneway

# 1. Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 2. Identify the unique rarity categories
rarity_categories = df['rarity'].unique()
print(""Rarity categories:"", rarity_categories)

# 3. Prepare data for the ANOVA test: lists of Win Rates by rarity
win_rates_by_rarity = [df[df['rarity'] == rarity]['Win Rate'].dropna() for rarity in rarity_categories]

# 4. State hypotheses
print(""Null hypothesis (H0): The mean Win R...",7,"The generated code correctly performs the ANOVA test and interprets the results. It includes the necessary imports, loads the data, identifies unique categories, states hypotheses, prints the F-statistic and p-value, and interprets the outcome based on the significance level.

However, there are a few areas for improvement:

1.  **Error Handling for File Loading:** The generated code does not include any error handling for `FileNotFoundError` when trying to load the CSV file. The ground truth includes `try-except` blocks for robust file loading.
2.  **Data Cleaning:** The generated code assumes 'Win Rate' is clean and numeric. It uses `.dropna()` on the individual series, which is good for handling missing values for that specific column, but it doesn't explicitly convert 'Win Rate' to numeric with `errors='coerce'` or handle potential non-numeric entries, which could lead to errors if the column isn't clean. The ground truth explicitly uses `pd.to_numeric` and also drops `NaN`s in `rarity`.
3.  **Robustness Check for `rarity` categories:** The ground truth checks if there are at least two unique rarity categories and if each group has at least two observations, which are crucial conditions for a valid ANOVA test. The generated code does not perform these checks, which could lead to errors or misleading results if the data doesn't meet these requirements.
4.  **Formatting of p-value:** The p-value is printed in scientific notation (`.4e`). While technically correct, for interpretation purposes, `.4f` or similar decimal formatting (as in the ground truth) is often more readable, especially if the p-value is not extremely small.
5.  **Unnecessary Print Statement:** `print(""Rarity categories:"", rarity_categories)` was not requested.

Overall, it's a good attempt that covers the core requirements, but it lacks the robustness and error-checking present in the ground truth, which are important for real-world data science tasks.",gpt-4.1,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# 1. Load the dataset
df = pd.read_csv('iris.csv')

# 2. Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidth...",9,"The generated code is highly correct and complete, successfully addressing all requirements of the task. It correctly loads the dataset, defines features and target, encodes the target variable, and splits the data appropriately. All three specified models are trained with the correct parameters, and their performance metrics (accuracy, macro-averaged precision, recall, F1-score) are correctly calculated and reported. The identification of the best model based on F1-score is also accurate.

Strong points:
- All required imports are present.
- Data loading and preprocessing (feature selection, target encoding) are performed correctly.
- Data splitting uses the correct `test_size`, `random_state`, and `stratify` parameters.
- All three specified models are initialized with the correct parameters.
- Loop for training and evaluation is well-structured.
- All required performance metrics are calculated using `average='macro'`.
- `zero_division=0` is a good practice for `precision_score`, `recall_score`, and `f1_score`, although not strictly necessary for this dataset.
- The best performing model is correctly identified and printed based on macro F1-score.
- The output formatting is clear and easy to read.

Areas for minor improvement (not affecting the score significantly):
- The Ground Truth explicitly uses `max_iter=200` for `LogisticRegression` to avoid convergence warnings. While the generated code also includes this, if the prompt did not specify it and the default caused a warning, that could be a minor point. However, in this case, it matches the ground truth.
- The variable `le` for LabelEncoder is a bit terse; `label_encoder` (as in Ground Truth) is slightly more descriptive, though `le` is common.
- The output for reporting could be explicitly prefaced with a title like ""--- Model Performance Comparison ---"" as in the Ground Truth for better structure, but the current output is still clear.",gpt-4.1,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. Load the dataset
df = pd.read_csv('iris.csv')

# 2. Select the features
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
X = df[features]

# 3. Standardize the features
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

# 4. Apply K-Means clustering
kmeans = ...",8,"The generated code correctly performs all requested steps: loading data, selecting and standardizing features, applying K-Means clustering, performing PCA, and generating two scatter plots.

The code is well-structured, uses proper imports, and clearly addresses each requirement of the task.

Areas for minor improvement:
1.  **`random_state` for PCA**: While not strictly incorrect, `random_state` for PCA is generally not needed as PCA is deterministic unless specific random initializations are involved (e.g., randomized PCA). Setting it here doesn't cause harm but isn't common practice.
2.  **Color maps**: The `cmap='Set1'` is fine, but the ground truth used `viridis` which is often preferred for continuous data, though for discrete labels, `Set1` or `Paired` can be good. This is a stylistic choice.
3.  **Color bar for K-Means plot**: The ground truth code included a color bar for the K-Means plot, which enhances readability by showing which colors correspond to which cluster labels. The generated code omits this.
4.  **Customizing color bar for True Species**: For the true species plot, the ground truth explicitly mapped the numerical labels back to the original species names in the color bar, which is a significant usability improvement for interpreting the plot. The generated code's color bar would only show numerical labels (0, 1, 2) which is less informative.
5.  **Error Handling**: The ground truth included basic error handling for `FileNotFoundError` when loading the CSV, which is good practice.

Overall, the code is very good and functional for the task, with minor polish needed for the plotting elements.",gpt-4.1,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# Load dataset
df = pd.read_csv('iris.csv')

# Feature engineering
df['PetalArea'] = df['PetalLengthCm'] * df['PetalWidthCm']
df['SepalArea'] = df['SepalLengthCm'] * df['SepalWidthCm']

# Define features and target
feature_cols = [
    'SepalLengthCm',
    'SepalWidthCm',
    'PetalLengthCm',
    'PetalWidthCm',
...",10,"The generated code is excellent. It correctly loads the dataset, performs the required feature engineering by creating `PetalArea` and `SepalArea`. It correctly defines `X` to include both original and new features and `y` as the `Species` column, which is then correctly encoded. The Gradient Boosting Classifier is initialized with the specified parameters, and 5-fold cross-validation is performed successfully. Finally, the mean cross-validated accuracy is reported in the correct format. The code is well-structured, uses proper imports, and fully addresses all requirements of the task.",gpt-4.1,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# 1. Load the dataset
df = pd.read_csv('heart.csv')

# 2. Define features and target
feature_cols = ['Age', 'Sex', 'C...",9,"The generated code is highly correct, complete, and well-structured, successfully addressing almost all requirements of the task.

**Strong points:**
*   **Correct Data Loading and Feature Selection:** Correctly loads the `heart.csv` dataset and defines `X` and `y` as specified, including all required features and the target 'HeartDisease'.
*   **Correct Preprocessing:** Accurately identifies and applies `OneHotEncoder` to categorical features and `StandardScaler` to numerical features using `ColumnTransformer` within a `Pipeline`.
*   **Correct Data Splitting:** Implements `train_test_split` with the correct `test_size=0.2`, `random_state=42`, and `stratify=y`, ensuring proper data division.
*   **Correct Model Training:** Uses `RandomForestClassifier` with the specified `n_estimators=100` and `random_state=42`.
*   **Correct Evaluation Metrics:** Calculates and prints all required evaluation metrics: accuracy, macro-averaged precision, recall, F1-score, and ROC-AUC.
*   **Pipeline Usage:** Effectively utilizes `Pipeline` to streamline preprocessing and model training, which is best practice.

**Minor Area for Improvement (Not critical for this task, but good practice):**
*   **Numerical Features Definition:** The `numerical_features` list was manually defined by listing all features *except* the categorical ones. While correct for this specific problem, a more dynamic approach (as used in the ground truth) would be `numerical_features = [col for col in features if col not in categorical_features]`. This helps prevent subtle errors if feature lists change or if there are many features. However, for the given, fixed list of features, the direct enumeration is also acceptable.
*   **`handle_unknown` in `OneHotEncoder`:** The generated code uses `OneHotEncoder(drop='first')`. While `drop='first'` is often used to avoid multicollinearity, the prompt did not specifically ask for it, and the ground truth used `handle_unknown='ignore'`. For robust deployment where unseen categories might appear, `handle_unknown='ignore'` is generally safer than dropping columns, as `drop='first'` does not inherently handle unknown categories. However, for a benchmark where the test set is derived directly from the training set, `drop='first'` might be a valid choice to prevent multicollinearity without explicitly hindering performance, and it's not a direct violation of the original prompt (which only asked for OHE, not specific parameters). The ground truth's `handle_unknown='ignore'` is arguably a slightly more robust default if not specified otherwise.

Overall, the code is very good and demonstrates a strong understanding of machine learning pipelines and preprocessing.",gpt-4.1,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. Load dataset
df = pd.read_csv('heart.csv')

# 2. Select specified numerical features
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']
X = df[features]

# 3. Standardize features
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

# 4. Deter...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Correctness:** The code successfully loads the data, selects the specified features, standardizes them, determines K using both WCSS and silhouette scores (visualizing them), applies K-Means with the specified parameters, performs PCA, and visualizes the results.
- **Code Quality:** Imports are proper, and the code structure is logical, following the steps outlined in the task.
- **Completeness:** All major requirements are met.

**Areas for Minor Improvement (for a perfect score):**
- **Robustness (File Not Found):** The generated code does not include error handling for `FileNotFoundError` when loading `heart.csv`. The ground truth includes a `try-except` block, which is good practice for robust code.
- **PCA `random_state`:** While `random_state` is often used in PCA for reproducibility in algorithms that involve randomness (like SVD solvers, especially randomized SVD), for standard PCA on dense matrices, it typically doesn't have an effect unless the solver chosen is randomized. The ground truth omits `random_state` for `PCA`, which is technically accurate as it's often not needed for deterministic PCA unless specific algorithms are used. Its inclusion here is not incorrect but also not strictly necessary for standard PCA.
- **Plotting Aesthetics:** While functional, the plotting in the generated code could be slightly improved with `plt.grid(True)` as in the ground truth for better readability of the elbow and silhouette plots. Also, the legend for the scatter plot could specify the cluster numbers more clearly if `label=f'Cluster {i}'` was directly used in the loop, which it does. The ground truth adds `edgecolors='w', linewidth=0.5` to the scatter plot points, which often makes them stand out better, but this is a minor aesthetic point.
- **Comments/Print Statements:** The ground truth includes more verbose print statements and comments explaining each step, which is good for readability and debugging in a benchmark context. The generated code is more concise but could benefit from a few more contextual prints.

Overall, it's a very good solution that successfully completes the core task. The missing file error handling is the most notable omission for robustness.",gpt-4.1,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report

# 1. Load data
df = pd.read_csv('winequality-red.csv')

# 2. Create binary target variable
df['target'] = (df['quality'] >= 7).astype(int)

# 3. Drop original 'quality' column
df = df.drop('quality', axis=1)

# 4. Handle...",9,"The generated code correctly creates the binary target, preprocesses the data (including scaling), splits the data, trains a logistic regression model, evaluates performance, and identifies important features. The imports are proper, and the code structure is logical.

The minor areas for improvement are:
1.  **Missing Value Handling:** The generated code uses `df.dropna()`. While it handles missing values, the ground truth code includes a more robust check and median imputation in cases where missing values might be present, which is generally better practice than just dropping rows, especially if missing values are sparse. For this specific dataset, `dropna()` works because there are no missing values, but a more generalized solution would account for this.
2.  **Explicit DataFrames for Scaled Data:** The ground truth converts the scaled NumPy arrays back into Pandas DataFrames (`X_train_scaled_df`, `X_test_scaled_df`) before fitting and predicting. This is not strictly necessary for the model fitting itself (as `fit` and `predict` accept NumPy arrays), but it's crucial for correctly associating the `lr.coef_[0]` with the original feature names when reporting feature importance. The generated code directly uses `X.columns` with `lr.coef_[0]` which works, but it implicitly relies on the order of columns being preserved after scaling, which `StandardScaler` does by default. Making it explicit with a DataFrame ensures robustness and readability for feature importance, especially if more complex preprocessing steps were involved.
3.  **Comments/Readability:** While the ground truth is instructed to include explanations, the generated code is a bit sparse on comments. Adding a few more comments to delineate sections (e.g., ""Feature Scaling Begins,"" ""Model Training"") would improve readability for future maintenance, even though the task description limits explanations in the output.

Overall, the generated code fully delivers on the core requirements of the task.",gpt-4.1,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression

# 1) Load the dataset
df = pd.read_csv('heart.csv')

# 2) Boxplot of Oldpeak for HeartDisease=0 and 1
plt.figure(figsize=(8, 6))
sns.boxplot(x='HeartDisease', y='Oldpeak', data=df)
plt.title('Distribution of Oldpeak by HeartDisease Status')
plt.xlabel('HeartDisease')
plt.ylabel('Oldpeak')
plt.show()

# 3) Logistic regression: HeartDisease ~ O...",8,"The generated code correctly addresses most aspects of the task, demonstrating a good understanding of data loading, visualization, statistical modeling, and data manipulation.

**Strong Points:**
*   **Correctness:**
    *   Loads the dataset correctly.
    *   Generates a box plot for `Oldpeak` by `HeartDisease`.
    *   Performs logistic regression correctly with `Oldpeak` as the sole predictor.
    *   Calculates and prints the coefficient and odds ratio.
    *   Correctly discretizes `Oldpeak` using `pd.qcut` into three bins.
    *   Calculates the proportion of `HeartDisease`=1 within each bin.
*   **Completeness:** Addresses all major requirements of the prompt.
*   **Code Quality:** The code is clean, readable, and uses standard libraries effectively.

**Areas for Improvement (to reach a higher score):**
*   **Box Plot Customization:** The ground truth added `xticks` labels (No Heart Disease, Heart Disease), a more descriptive `xlabel`, `ylabel`, and a `grid` to the box plot. This makes the visualization more informative and professional. The generated code's plot is functional but lacks these enhancements.
*   **Interpretation of Odds Ratio:** The prompt explicitly asked to ""interpret its meaning in the context of odds ratios."" While the odds ratio is calculated and printed, a textual interpretation explaining what a value of `exp(coef)` means (e.g., ""for every one-unit increase in Oldpeak, the odds of heart disease are multiplied by X"") is missing. The ground truth provides a robust interpretation based on the sign of the coefficient.
*   **Error Handling/Robustness:** The ground truth includes `try-except` blocks for `FileNotFoundError` when loading the CSV and for `ValueError` during `pd.qcut` (in case of duplicate quantiles). This makes the code more robust to common issues. While not strictly required for a basic execution, it's a good practice, especially in a benchmark scenario.
*   **Output Clarity:** The ground truth provides more descriptive `print` statements throughout the execution, indicating which step is being performed (`--- Box plot displayed: Distribution of Oldpeak by Heart Disease Status ---`). This improves the user experience when running the script.
*   **X and y shape for Logistic Regression:** While `logreg.fit(X, y)` works with `.values` in this case, `sklearn` typically expects `X` to be 2D (`df[['Oldpeak']]`) and `y` to be 1D (`df['HeartDisease']`). The generated code's `X = df[['Oldpeak']].values` does create a 2D array, which is correct, but directly passing `df[['Oldpeak']]` is often preferred as it maintains column names and allows for more complex DataFrame operations (though not strictly necessary here).
*   **Displaying Proportions:** While the proportions are printed, the ground truth also provides an optional visualization (bar plot) of these proportions, which is a good addition for understanding the results visually.

The generated solution is solid and fulfills the core requirements, but could benefit from more detailed interpretation, better plot aesthetics, and enhanced robustness for a perfect score.",gpt-4.1,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

# 1. Load data
df = pd.read_csv('spotify_2023.csv')

# 2. Convert 'streams' to numeric
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams'])

# 3. Create binary 'is_popular' target var...",9,"The generated code successfully completes almost all aspects of the task, demonstrating good understanding of data preprocessing, model training, and evaluation.

**Strong Points:**
*   **Correctness:** All core requirements are met: `streams` conversion and dropping NaNs, `is_popular` creation using the 75th percentile, correct feature selection, one-hot encoding, train-test split, Logistic Regression training, and reporting F1 (macro), ROC AUC, and confusion matrix.
*   **Clarity and Structure:** The code is well-structured with comments indicating each step as per the prompt.
*   **Feature Engineering:** Properly handles numerical and categorical features, using `OneHotEncoder` and combining them correctly.
*   **Evaluation Metrics:** Accurately calculates and prints the required evaluation metrics (F1 macro, ROC AUC, Confusion Matrix).
*   **Reproducibility:** `random_state=42` is correctly used for `train_test_split`.

**Areas for Minor Improvement (for a perfect score):**

*   **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code assumes the file exists.
*   **Direct `pd.get_dummies` vs. `OneHotEncoder` for combined df:** While `OneHotEncoder` works, for quick and simple one-hot encoding directly on a DataFrame in a pipeline-like flow where features are already selected, `pd.get_dummies` is often more straightforward and was used in the ground truth. The approach here involves `X_num.values` and creating a new DataFrame `X_df` which is a bit more verbose than necessary, but functionally correct.
*   **No `stratify=y` inside train_test_split:** The prompt required `stratify=y` for `train_test_split` which ensures the class distribution of `is_popular` is maintained in both training and testing sets. The generated code missed this detail but was added in the prompt (and then corrected in the evaluation). **Correction:** After reviewing the generated code again, `stratify=y` *is* included in the `train_test_split` call. My apologies for the oversight. This strengthens the score.

Overall, an excellent solution that correctly implements all major requirements.",gpt-4.1,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. Load data
df = pd.read_csv('spotify_2023.csv')

# 2. Select audio features and drop rows with NaNs in these features
features = [
    'danceability_%', 'energy_%', 'valence_%',
    'acousticness_%', 'instrumentalness_%',
    'liveness_%', 'speechiness_%', 'bpm'
]
df_selected = df[features].dropna()

# 3. Stan...",9,"The generated code is highly correct and addresses almost all requirements effectively.

**Strong points:**
*   **Correct Feature Selection and NaN Handling:** Correctly identifies and selects the specified audio features and uses `.dropna()` to remove rows with NaNs.
*   **Correct Standardization:** Uses `StandardScaler` as required.
*   **Correct K-Means Application:** Applies K-Means with the correct parameters (K=4, `random_state=42`, `n_init='auto'`).
*   **Correct PCA Application:** Applies PCA to 2 components with `random_state=42`.
*   **Correct Visualization:** Creates the scatter plot of the two principal components, correctly coloring points by cluster labels and adding a title and labels.
*   **Correct Mean Calculation:** Accurately calculates and displays the mean values of the original (unstandardized) selected audio features for each cluster. This is crucial and correctly implemented by attaching labels to the *original* `df_selected` data.
*   **Good Imports:** All necessary imports are present.

**Areas for minor improvement (why it's not a 10):**
*   **Missing Error Handling for File:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code assumes the file will always be present.
*   **Potentially Misleading `dropna()`:** While `df[features].dropna()` correctly handles NaNs, it creates a new DataFrame `df_selected`. If the original `df` were to be used for something else later, it would still contain the rows with NaNs. However, for this specific task, `df_selected` is the only part of `df` used subsequently, so it's functionally correct for the problem constraints. It's just a slightly less explicit way of saying ""create a new df with features and no NaNs."" The ground truth's `df_features = df[audio_features].copy()` followed by `df_features.dropna(inplace=True)` is slightly more explicit about modifying an extract.
*   **No print statements for NaN drop count:** The ground truth included print statements showing how many rows were dropped due to NaNs, which is good for user feedback during data preprocessing. This was not explicitly requested but is a nice addition for clarity.

Overall, these are minor points, and the generated code fully solves the core technical requirements of the problem.",gpt-4.1,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. Load data
df = pd.read_csv('spotify_2023.csv')

# 2. Convert `streams` to numeric (coerce errors to NaN)
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# 3. Filter for streams > 10,000,000 and 2010 <= released_year <= 2023, drop NaN streams
filtered = df[
    (df['streams'] > 10_000_000) &
    (df['released_year'] >= 2010) &
    (df['released_year'] <= 2023)
].dropna(subset=['streams'])

# 4. Group by released_ye...",9,"The generated code correctly addresses almost all requirements of the task. It loads the data, correctly converts the `streams` column to numeric with error coercion, filters the data based on streams and `released_year` (2010-2023), groups by year, calculates the average audio features, and then generates the required line plots with titles and labels.

**Strong points:**
- Correctly handles `streams` conversion and filtering.
- Implements all necessary filtering conditions.
- Correctly groups and calculates averages.
- Generates all three required plots with appropriate titles and labels.
- Uses `plt.tight_layout()` for better plot presentation.

**Areas for improvement:**
- The filtering logic for `streams` and `released_year` is combined in a single step with `dropna(subset=['streams'])`. While this works for the given parameters, the ground truth's approach of `dropna` *before* the numerical `streams` filter is slightly more robust if there were other non-numeric `streams` values *after* conversion that somehow evaluate to `NaN` but are not caught by the initial `pd.to_numeric` (unlikely for this specific scenario but good practice). The current approach essentially filters `NaN` streams *after* the `> 10M` check, which means any `NaN` values, if they exist for some reason *after* the numeric conversion, would still be rows. The task specified dropping `NaN` `streams` *after conversion* and *before* filtering by `streams` > 10M, which the ground truth does by dropping NaNs and *then* applying streams > 10M filter. In the generated code, the NaN drop (`.dropna(subset=['streams'])`) happens last, meaning `streams` columns that are NaN would be `False` for the `>10_000_000` condition and thus mostly excluded anyway, but it's a subtle difference.
- There's no error handling for `FileNotFoundError` when loading the CSV, which is a good practice for user-facing scripts.
- The plots lack explicit grid lines and fixed x-ticks rotation which can improve readability, as seen in the ground truth. The color choices are also just default or simple assignments; the ground truth uses more distinct thematic colors.
- `reset_index()` on `avg_features` before plotting is good practice for DataFrame usage, but plotting directly from `yearly_avg_features.index` (as done in GT) also works and is equivalent. These are minor stylistic choices.

Overall, the generated code is highly functional and correct for the given task.",gpt-4.1,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder

# 1. Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# 2. Clean numeric columns
for col in ['Likes', 'Dislikes', 'Video views']:
    df[col] = df[col].astype(str).str.replace(',', '', regex=False)
    df[col] = pd.to_numeric(df[col], errors='coerce...",9,"The generated code is largely correct and addresses most of the requirements.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly handles comma removal and conversion to numeric for the specified columns, including filling NaNs with 0.
*   Correctly extracts the year from `published` and treats it as a numeric feature, handling NaNs correctly.
*   Correctly fills missing `Category` values with 'Unknown' and applies one-hot encoding using `OneHotEncoder`.
*   Correctly defines `X` and `y` using the specified columns and concatenate them properly.
*   Correctly splits the data into training and testing sets with the specified `test_size` and `random_state`.
*   Correctly trains a `LinearRegression` model.
*   Correctly evaluates the model using R-squared and MAE.
*   Includes all necessary imports.

**Areas for minor improvement:**
*   **Type Coercion for Cleaned Numeric Columns:** While `fillna(0)` makes the columns numeric, the prompt implies that after cleaning the numeric columns (`Likes`, `Dislikes`, `Video views`), they should be treated as numeric (e.g., `astype(int)`). The generated code performs `fillna(0)` which results in a float column when there's an original NaN, but the ground truth converts them to `int` explicitly, which is a slightly cleaner representation for counts. This is a minor point, as `LinearRegression` can handle floats.
*   **Redundant `reset_index(drop=True)`:** When concatenating `df[['Dislikes', 'Video views', 'published_year']]` and `category_encoded_df`, `reset_index(drop=True)` is applied to both. `df` already has a default index, and `category_encoded_df` was created with `index=df.index`, so their indices should align by default, making `reset_index` unnecessary. While it doesn't harm, it adds complexity.
*   **Column Names for `OneHotEncoder`:** The `OneHotEncoder` `get_feature_names_out` method automatically prefixes the column name, so `encoder.get_feature_names_out(['Category'])` is good, but `df.columns` would have been `Category` rather than `Category_`. The output is what's expected, so this is fine.

Overall, the code is very close to perfect and successfully meets the requirements. The points for improvement are minor stylistic or efficiency considerations.",gpt-4.1,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. Load dataset
df = pd.read_csv('youtube_trendings.csv')

# 2. Clean 'Video views'
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)

# 3. Ensure 'published' column (year) is numeric/integer
df['published'] = pd.to_numeric(df['published'], errors='coerce').astype('Int64')

# 4. Group by 'published' year
grouped = df.groupby('published')

# 5a. Average cleaned 'Video views' per yea...",7,"The generated code addresses most of the requirements but has a significant issue with the `published` column.

**Strengths:**
*   **Correct Cleaning of 'Video views':** Successfully removes commas, converts to numeric, and fills NaNs with 0 in a concise manner.
*   **Correct Grouping and Metric Calculation:** Accurately groups by year and calculates both average views and unique video counts.
*   **Correct Plotting:** Generates two subplots with appropriate titles and labels as requested.
*   **Proper Imports:** Includes necessary libraries (`pandas`, `matplotlib.pyplot`).

**Areas for Improvement / Issues:**

*   **`published` Column Handling (Major Issue):** The prompt states ""Ensure the `published` column (year) is treated as a numeric or integer type."" and implies that the `published` *year* needs to be extracted. The dataset `youtube_trendings.csv` typically contains a `publishedAt` column with full datetime strings (e.g., '2020-08-14T20:20:12Z'), not just a 'published' column with only the year. The generated code attempts to convert a non-existent 'published' column directly to numeric. This will likely result in a `KeyError` if a column named `publishedAt` exists, or `NaN`s if a column named `published` (but with non-numeric data) exists, leading to incorrect or empty plots. The ground truth code correctly parses `publishedAt` to extract the year.
*   **Robustness:** The code assumes the file exists and doesn't include any error handling for file not found or other data loading issues, which is good practice in a real-world scenario (though not explicitly asked for in a minimal example, it's a minor point against ""quality"").

To achieve a higher score, the handling of the `published` (likely `publishedAt`) column needs to be corrected to extract the year from the datetime string.",gpt-4.1,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
import string

# Predefined English stop words list
stopwords = set(""""""
a about above after again against all am an and any are aren't as at be because been before being below between both but by
can't cannot could couldn't did didn't do does doesn't doing don't down during each few for from further had hadn't has hasn't
have haven't having he he'd he'll he's her here here's hers herself him himself his how how's i i'd i'll ...",8,"The generated code correctly addresses most of the requirements.

**Strong Points:**
- **Correctness:** All steps (loading, `title_word_count`, `Video views` cleaning, correlation, filtering by year, word processing, most common words) are largely implemented correctly.
- **`title_word_count`:** Correctly calculates word count.
- **`Video views` cleaning:** Handles commas, converts to numeric, and fills NaNs with 0 as required.
- **Correlation:** Calculates and prints the correlation.
- **Word Processing:** Correctly handles lowercasing, punctuation removal, and stop word removal using the provided list.
- **Most Common Words:** Correctly identifies the 5 most common words.
- **Self-contained Stopwords:** Providing a predefined list of stopwords is a good alternative to NLTK, making the code more portable.

**Areas for Improvement:**
- **Year Extraction for Filtering:** The `extract_year` function used `re.search(r'(\d{4})', str(s))`. While this might work for some date formats, it's brittle. The ground truth's `pd.to_datetime` is far more robust for parsing diverse date formats and extracting the year reliably. If the `published` column contains non-standard year formats (e.g., '23 instead of '2023'), or complex date strings, this regex might fail or extract incorrect years. `pd.to_datetime` handles a much wider range of formats.
- **Handling `dropna()` on `filtered['Video']`:** In the loop for word processing, `filtered['Video'].dropna()` is used. While it's generally good practice to handle NaNs, the previous steps (`astype(str)`) should ensure there are no NaNs in the string representation of titles. If any `Video` entries were truly NaN, `astype(str)` would turn them into 'nan', which might then be processed incorrectly by `title.split()`. It's better to ensure consistency in type upfront or explicitly handle the 'nan' string if it's a possibility from the initial data. The ground truth ensures string conversion without explicitly dropping.
- **Robustness in `pd.read_csv`:** The ground truth includes a `try-except FileNotFoundError` block, which is good practice for user-facing scripts to handle missing files gracefully. The generated code assumes the file exists.
- **Clarity of Output:** The output for correlation is good, but for the most common words, the ground truth provides ""word: count occurrences"" which is slightly more descriptive than ""word: count"". This is minor.",gpt-4.1,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score

# 1. Load data
df = pd.read_csv(""mobiles_2025.csv"")

# 2. Clean `Launched Price (USA)` and create target
def clean_price(s):
    if pd.isna(s):
        return np.nan
    # Remove 'USD ', commas, attempt to convert
    s_clean = s.replace('USD ', '').replace(',', '').strip()
    try:...",8,"The generated code addresses most requirements and implements a nearly correct solution.

**Strong points:**
*   Correctly loads the data.
*   Correctly implements the `is_expensive_usa` target, including handling NaNs in the price and ensuring the resulting target is not NaN.
*   Successfully cleans all specified numerical features (`Mobile Weight`, `RAM`, `Battery Capacity`, `Screen Size`) and extracts MP values for camera features using `re.findall`.
*   Applies one-hot encoding for `Company Name` and `Processor`.
*   Correctly defines `X` and `y` and performs a final NaN drop.
*   Splits data with the correct test size and `random_state`, using `stratify`.
*   Trains `RandomForestClassifier` with specified parameters.
*   Evaluates with both F1-score (macro) and ROC AUC score.
*   Includes all necessary imports.

**Areas for Improvement / Minor Issues:**

1.  **Handling of `Launched Price (USA)` NaN drop:** The generated code directly drops rows where `is_expensive_usa` is `NaN`. This is correct based on the prompt's requirement ""Drop rows where `is_expensive_usa` is NaN."" However, the ground truth code converts the `is_expensive_usa` column to `int` after dropping NaNs, which is good practice for binary target variables. The generated code also does `astype(int)` but later in the `df_clean` block.
2.  **Feature selection for `X` in step 5:** The `feature_cols` list correctly identifies all individual features that should be in `X`. However, when concatenating `X`, the code uses `df[feature_cols[:7]]` for numerical and `df_cat` for categorical. This implies `df_cat` columns are not explicitly listed in `feature_cols` when `X` is constructed, despite `list(df_cat.columns)` being added to `feature_cols`. While `pd.concat` might align columns correctly by name for `X`, it's generally safer and more explicit to build `X` from a dataframe that already contains all desired original and one-hot encoded columns, or to explicitly list all columns when concatenating. The ground truth explicitly creates `X` as a subset of the `df_encoded` which already contains features and encoded columns.
3.  **Final NaN handling (Step 5):** The generated code concatenates `X` and `y` then drops NaNs and then re-assigns `X_clean` and `y_clean`. This is functionally correct, but the use of `df_clean` as an intermediate copy before re-assigning back to `X_clean` and `y_clean` is a bit less direct than the ground truth which processes `X` and `y` directly or combines them temporarily.
4.  **`Launched Year`:** The task implicitly expects `Launched Year` to be used as a numerical feature. The generated code correctly includes it in `feature_cols` and `X`.
5.  **Robustness (Minor):** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is a good practice for robustness in real-world scenarios, though not strictly required by the prompt's core task.

Overall, the generated code is very good and produces a working solution that aligns well with the requirements. The minor points are mostly about code style/structure and minor optimizations, rather than functional errors.",gpt-4.1,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Step 1: Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Step 2: Define the cleaning function
import re

def clean_price(price):
    if pd.isna(price):
        return np.nan
    try:
        # Remove currency symbols and spaces
        cleaned = re.sub(r'(PKR|INR|CNY|USD|AED|\s+)', '', str(price))
        # Remove commas
        cleaned = cleaned.replace(',', '')
        # Convert to float
        return float(cleaned)
    except:
     ...",7,"The generated code correctly loads the data, cleans the specified price columns, calculates the average global price, identifies the top 5 models, and displays them as requested. The use of regex for cleaning is efficient.

However, there are a few areas for improvement:
1.  **New Columns for Cleaned Data**: The generated code creates new columns like `Launched Price (Pakistan) (Cleaned)` for the cleaned data. While functional, the task implicitly suggests overwriting the original columns with cleaned values, which is how the ground truth approaches it. Creating new columns adds unnecessary clutter to the DataFrame if not explicitly required.
2.  **Error Handling for File Not Found**: The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robust code. The generated code lacks this.
3.  **Clarity of `clean_price` Function**: The regex `re.sub(r'(PKR|INR|CNY|USD|AED|\s+)', '', str(price))` effectively removes currency symbols and spaces. However, it also removes any whitespace *within* the number, which might be an issue if there were unexpected spaces in a number (e.g., ""1 200""). While unlikely for prices, it's slightly less precise than replacing symbols then commas and finally stripping. The ground truth's iterative `replace` and `strip` is more explicit about what's being removed and in what order.
4.  **Redundant Import**: The `import re` is correctly placed inside the `clean_price` function definition, but it's generally best practice to place all imports at the top of the script for clarity and to avoid re-importing.",gpt-4.1,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# 2) Clean columns
def clean_ram(val):
    try:
        return float(str(val).replace('GB','').strip())
    except:
        return pd.NA

def clean_battery(val):
    try:
        return float(str(val).replace('mAh','').strip())
    except:
        return pd.NA

def clean_price(val):
    try:
        return float(str(val).replace('USD','').replace(' ',''...",8,"The generated code correctly addresses the prompt's requirements. It loads the data, cleans the specified columns (RAM, Battery Capacity, Launched Price (USA)), handles conversion errors by setting to NaN and dropping rows, filters for Apple products, groups by launched year, and generates comparative box plots for each year with subplots, proper titles, and y-axis labels.

Strengths:
- **Correctness:** The core logic for cleaning, filtering, grouping, and plotting is sound and achieves the desired outcome.
- **Modularity in cleaning:** Using separate functions for cleaning each column is a good practice for readability and reusability, though it's less performant than vectorized operations.
- **Proper use of `dropna`:** The use of `dropna` on the subset of cleaned columns correctly handles the requirement to drop rows where any of these are NaN.
- **Clear plotting:** The generation of subplots for each year and each metric is well-implemented, with appropriate titles and y-axis labels. `plt.tight_layout` is also used effectively.

Areas for Improvement:
- **Efficiency of Cleaning:** The use of `apply` (via `map` internally) with custom Python functions (`clean_ram`, `clean_battery`, `clean_price`) can be significantly slower than vectorized pandas string operations and `pd.to_numeric` with `errors='coerce'`. The ground truth demonstrates a more efficient and idiomatic pandas approach for cleaning.
- **Original Column Overwrite:** Instead of creating new `_Clean` columns, it would be more memory-efficient and aligned with typical pandas workflows to overwrite the original columns after cleaning, similar to the ground truth. This also avoids potential confusion about which column to use.
- **Error Handling for File Loading:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is a good practice for robust scripts.
- **Specific `fill_value` for `pd.NA`:** While `pd.NA` functions correctly, `np.nan` is more commonly used and broadly compatible in data science workflows, especially when dealing with numeric columns. The `errors='coerce'` in `pd.to_numeric` also naturally results in `np.nan`.
- **Title Redundancy:** The subplot titles include the year again (e.g., `RAM Distribution (year)`), which is somewhat redundant given the `suptitle` already specifies the year for all subplots. While not a major issue, it could be slightly more concise.
- **Color choices for plots:** The chosen colors `""#007aff""`, `""#1ecbe1""`, `""#dbe0e3""` are quite distinct but the last one `""#dbe0e3""` for price distribution is very light and might not be easily visible on some backgrounds. Using a more contrasting color for all plots or a default seaborn palette might be better.",gpt-4.1,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
# Imports
import pandas as pd
import numpy as np
from sklearn.model_selection import (train_test_split, StratifiedKFold,
                                     GridSearchCV, RandomizedSearchCV,
                                     learning_curve, validation_curve, cross_val_score)
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
from sklearn.pipeline import Pipeline
from sklearn.compo...",7,"The generated code attempts to address most of the requirements and demonstrates an understanding of advanced ML concepts. However, there are significant gaps and inconsistencies regarding the application of the preprocessing pipeline, feature engineering, and model evaluation, which prevent it from fully meeting the ""expert code evaluator"" standard set by the ground truth.

**Strengths:**
*   **Comprehensive Imports:** Includes most necessary libraries.
*   **Attempt at Advanced Preprocessing:** Tries custom feature engineering (Name, Cabin), multiple imputation strategies (KNN, IterativeImputer).
*   **Multiple Base Models & Tuning:** Implements Random Forest, Gradient Boosting, XGBoost, SVM, and Neural Network with `RandomizedSearchCV`.
*   **Ensemble Methods:** Includes Stacking and Voting classifiers.
*   **Feature Selection:** Attempts `SelectKBest`, `RFECV`, `SelectFromModel`.
*   **Evaluation Metrics:** Calculates various metrics (accuracy, precision, recall, F1, ROC AUC).
*   **Visualization:** Includes plotting for learning curves, validation curves, and SHAP.
*   **Interpretability:** Attempts SHAP.
*   **Statistical Tests:** Includes `ttest_rel`.

**Weaknesses and Areas for Improvement:**

1.  **Preprocessing Pipeline Inconsistency:** This is the most critical flaw.
    *   The `preprocess_data` function is applied globally to the `data` DataFrame *before* train-test split. This causes data leakage as information from the test set is used in feature engineering (e.g., `Title` value counts or `mode` imputation within the preprocessing function).
    *   The multiple imputation strategies (`data_simple`, `data_knn`, `data_mice`) are performed directly on the entire dataset and then `data_mice` is chosen. This also causes data leakage and means the imputer is not actually part of a robust pipeline that would prevent this.
    *   The core issue is that `ColumnTransformer` and `Pipeline` are *not* used for the overall preprocessing (feature engineering, imputation, scaling, OHE, polynomial features). Instead, preprocessing steps are performed globally before model training, which is fundamentally incorrect for rigorous machine learning. The ground truth correctly defines a `Pipeline` that encapsulates `FeatureEngineerTransformer`, `IterativeImputer`, `StandardScaler`, `PolynomialFeatures`, and `OneHotEncoder`, ensuring that these transformations are applied consistently to train/test sets and during cross-validation *without* data leakage.
    *   `PolynomialFeatures` is applied globally to `X` (after some manual OHE), but scaling (which is often crucial for polynomial features) is not consistently applied before or with it in a pipeline.
    *   Text features (`Name`) are processed by custom functions (`name_text_transform`) but NOT passed through a `ColumnTransformer` or `Pipeline` to ensure correct handling during cross-validation or testing, especially if `CountVectorizer` was intended to be used (it's imported but not leveraged).

2.  **Feature Engineering Completeness:**
    *   The `FeatureEngineerTransformer` in the ground truth is a much more robust approach, encapsulating all name/cabin/family features into a scikit-learn compatible transformer that fits into the main pipeline. The generated code has custom functions applied outside a pipeline.
    *   The ground truth also extracts `Deck` from `Cabin` and ensures `Fare_Per_Person`, which the generated code misses or implements differently (`HasCabin`, `Cabin_Letter`). No treatment for `Fare==0` besides filling it with NaN is robust and for `Fare_Per_Person` is completely missing.

3.  **Feature Selection Application:**
    *   Feature selection is done on `X` (after some preprocessing) *before* the models are tuned or evaluated, which is okay for demonstration, but for robust comparison, the feature selection should ideally be part of a `Pipeline` or applied to the data that goes into cross-validation.
    *   The impact of different feature selection techniques is ""compared"" by evaluating a `top_model` (presumably one of the individual tunned models) on these pre-selected feature sets. This is less rigorous than having the feature selection step as part of the overall pipeline being evaluated for each model or ensemble during CV. The ground truth, while also doing external feature selection for demonstration, explicitly states how it would be integrated (or shows its impact on a base model).

4.  **SHAP and LIME:**
    *   SHAP is applied only to `RandomForest` and uses `X` (which is already processed). The ground truth specifically pulls the `XGBClassifier` from its pipeline and applies SHAP to `X_test_processed_df`, ensuring consistency with the fully preprocessed data pipeline.
    *   LIME is commented out and not fully implemented, and if implemented, it doesn't correctly use the full pipeline for prediction during explanation, which is crucial for LIME's local interpretability. The ground truth correctly defines a `predict_proba_for_lime` function that uses the full `stacking_classifier` pipeline on *raw* data, allowing LIME to perturb features correctly.

5.  **Model Evaluation Detail:**
    *   The `model_scores` function for cross-validation re-initializes `StratifiedKFold` multiple times and doesn't explicitly mention `n_jobs=-1` for `cross_val_score` which can slow down execution.
    *   Learning and validation curves are only for a ""top individual model"" rather than an ensemble, or a customizable choice. The ground truth shows both, and specifically shows a good approach for a validation curve on a parameter within a pipeline.
    *   Statistical significance testing is only between Stacking and Voting classifiers, not a broader comparison as in the ground truth.

6.  **Code Structure and Readability:**
    *   Many steps are just raw scripts without being encapsulated in functions or classes (e.g., the global preprocessing, the multiple imputation attempts). The ground truth employs `BaseEstimator, TransformerMixin` for custom `FeatureEngineerTransformer`, which is a much cleaner and scikit-learn standard way to handle custom preprocessing steps within a `Pipeline`.
    *   The `use_label_encoder=False` for XGBoost should be `enable_categorical=True` for newer XGBoost versions, though it works in older versions. The ground truth used `eval_metric='logloss'` which is fine, but newer XGBoost versions are generally more robust with categorical features.

The generated code provides a reasonable start, but the lack of a robust, leakage-free preprocessing pipeline and the inconsistent application of various stages (feature engineering, imputation, feature selection, interpretation) undermine its claim to be ""expert"" level. It requires significant refactoring to match the consistency and rigor of the ground truth.",gpt-4.1,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# 1. Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 2. Define features X using 'Year' and 'Runtime'
X = df[['Year', 'Runtime']]

# 3. Define the target y using 'Rating'
y = df['Rating']

# 4. Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

...",7,"The generated code correctly addresses most of the requirements: loading data, defining features and target, splitting data (with correct test size and random state), training a Linear Regression model, and evaluating it using R2 and MAE. The imports are proper, and the code structure is clear and sequential.

However, a key limitation which prevents a higher score is the lack of robustness and data preprocessing. The `Runtime` column in the `imdb_top_250.csv` dataset typically includes ' min' (e.g., '142 min'), which needs to be cleaned (e.g., `df['Runtime'].str.replace(' min', '', regex=False).astype(float)`) before it can be used as a numerical feature. The generated code assumes `Runtime` is directly numeric, which will likely lead to an error or incorrect results if the column is not pre-processed. Similarly, it doesn't handle potential non-numeric values in 'Year' or 'Rating' columns or missing values (NaNs) that might arise from data loading or type conversion. While the prompt did not explicitly ask for robust data cleaning, it's a critical step for real-world data science tasks and affects the correctness of the model.

To improve, add data cleaning steps for `Runtime` and robust error handling for data loading and potential NaN values after type conversion.",gpt-4.1,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 1. Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 2. Select features
features = df[['Rating', 'Runtime']].copy()

# 3. Standardize features
scaler = StandardScaler()
X_std = scaler.fit_transform(features)

# 4. K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
labels = kmeans.fit_predict(X_std)
df['Cluster'] ...",7,"The generated code largely addresses the requirements but falls short on the data cleaning aspect, which is crucial for the 'Runtime' column.

**Strengths:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Feature Selection:** Correctly selects 'Rating' and 'Runtime'.
*   **Standardization:** Correctly applies StandardScaler.
*   **K-Means Application:** Correctly applies K-Means with specified parameters (K=3, n_init='auto', random_state=42).
*   **Scatter Plot:** Generates a scatter plot with correct axes and cluster coloring.
*   **Cluster Mean Calculation:** Correctly calculates and prints mean 'Rating' and 'Runtime' for each cluster.

**Areas for Improvement:**
*   **Data Cleaning for 'Runtime':** The most significant missing piece is the crucial data cleaning for the 'Runtime' column. The `imdb_top_250.csv` dataset's 'Runtime' column is typically in a string format like ""142 min"". The generated code assumes it's directly numeric, which will lead to a `TypeError` or `ValueError` when `df['Runtime']` is used in `plt.scatter` or when calculating means, as it cannot perform calculations on non-numeric strings. The ground truth code correctly handles this by converting 'Runtime' to numeric, removing ' min'. This omission is a major functional flaw.
*   **Handling `FileNotFoundError`:** While not explicitly requested, good practice dictates handling `FileNotFoundError` for the CSV, as demonstrated in the ground truth.
*   **Feature Copy (`.copy()`):** While `features = df[['Rating', 'Runtime']].copy()` is not strictly necessary for this specific case (as `fit_transform` returns a new array and `df['Cluster']` creates a new column), it's good practice to prevent `SettingWithCopyWarning` if subsequent modifications were to be made directly to `features` without explicitly reassigning it, but it doesn't affect the correctness here. The ground truth doesn't use `.copy()` because it extracts, scales, and then directly adds the cluster labels back to the original `df`.

To achieve a higher score, the generated code *must* include robust data cleaning for the 'Runtime' column to ensure the plotting and calculations work correctly.",gpt-4.1,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import nltk

# Ensure necessary NLTK resources are downloaded
nltk.download('punkt')
nltk.download('stopwords')

# 1. Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 2. Create title_length
df['title_length'] = df['Title'].astype(str).apply(len)

# 3. Pearson correlation between title_length and Rating
corr = df['title_length'].corr(df['Rat...",7,"The generated code largely addresses the requirements but has several areas for improvement.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly creates `title_length`.
*   Attempts to calculate and print correlation.
*   Correctly handles lowercase conversion and tokenization.
*   Successfully removes stopwords.
*   Correctly finds and prints the 10 most common words and their frequencies.
*   Includes necessary imports and handles NLTK downloads.

**Areas for Improvement:**

1.  **Correlation Calculation Robustness:** The generated code directly calculates `df['title_length'].corr(df['Rating'])`. This assumes `Rating` is already numeric and doesn't handle potential non-numeric values gracefully (e.g., using `pd.to_numeric` with `errors='coerce'`) or missing values (NaNs). The ground truth specifically addresses this by converting `Rating` to numeric and dropping NaNs before correlation, which is more robust for real-world data.
2.  **Punctuation Removal:** The punctuation removal `title = ''.join(char if char.isalnum() or char.isspace() else '' for char in title)` is less robust than using regex (`re.sub`). It only keeps alphanumeric characters and spaces. While it works for this specific task, `re` offers more concise and powerful pattern matching for more complex scenarios. The `string` module (which was imported but not directly used in the punctuation removal logic) is often used in conjunction with `str.translate` or `str.maketrans` for simple single-character punctuation replacement, but `re` is generally preferred for comprehensive non-alphanumeric character removal as seen in the ground truth.
3.  **DataFrame Integration for Preprocessing:** The preprocessing logic is applied in a `for` loop, iterating directly over the `Title` column and appending to `all_tokens`. While functionally correct, it's less ""pandastic"" than applying transformations to a new DataFrame column (e.g., `df['processed_title']`, `df['tokenized_title']`, `df['filtered_title']` as seen in the ground truth). Using DataFrame columns for intermediate steps makes the code more aligned with pandas best practices, easier to debug, and potentially more efficient for very large datasets due to vectorized operations.
4.  **Clarity of Output:** The correlation output is `Pearson correlation (title_length vs Rating): {corr}`, which is fine. The common words output is `10 most common words in movie titles:\n{word}: {freq}`. The ground truth provides slightly more specific formatting (`'word': count`).

To achieve a higher score, the generated code should incorporate the robust `Rating` handling for correlation and embrace more ""pandastic"" approaches for text preprocessing within the DataFrame.",gpt-4.1,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# 1) Load the dataset
df = pd.read_csv(""penguins.csv"")

# 2) Define features and target
num_features = ['culmen_length_mm', 'culmen_depth_mm', ...",7,"The generated code correctly performs most of the requested tasks and demonstrates a good understanding of the data preprocessing steps.

**Strengths:**
*   **Correct Imputation:** Uses `SimpleImputer` for numerical (median) and a manual loop for categorical (mode) imputation, which is generally correct.
*   **One-Hot Encoding:** Correctly applies `OneHotEncoder` and merges the features.
*   **Scaling:** Correctly applies `StandardScaler` to numerical features.
*   **Model Training and Evaluation:** Uses `RandomForestClassifier` and `cross_val_score` as requested.
*   **Target Encoding:** Correctly uses `LabelEncoder` for the target variable.

**Areas for Improvement / Issues:**

1.  **Handling of Missing `island` Values (Critical but Edge Case):** The task specified: ""if a row still has NA in `island` after mode imputation, e.g. if all were NA, it can be dropped, or a placeholder like 'Unknown' can be used if mode is NA itself. For this task, if mode imputation results in NA for island, drop those rows.)"".
    *   The generated code's logic for `island` imputation is `if not mode_value.empty and pd.notna(mode_value[0]): X[col] = X[col].fillna(mode_value[0]) else: # If mode itself is NA, only possible for 'island'. Drop these rows. X = X[X[col].notna()]`.
    *   While this attempts to handle the 'all NA' case for `island`, it fails if *all* values for `island` are missing. In such a scenario, `mode_value` would be an empty Series, causing `pd.notna(mode_value[0])` to raise an `IndexError`.
    *   The ground truth explicitly checked for `X['island'].isnull().all()` to handle this edge case correctly by dropping all rows if `island` was entirely NA. The generated code's approach for the `else` block `X = X[X[col].notna()]` would not work correctly in this specific 'all NA' case because `X[col].notna()` would be all `False`, resulting in an empty DataFrame.
    *   Additionally, the logic `X = X[X[col].notna()]` *always* gets applied if `mode_value` is empty or `pd.isna(mode_value[0])`. This isn't quite right. The instruction was to drop rows if imputation *results in NA* for island, which means if the *mode itself* was `NaN` (highly unlikely for string columns) or if the column was entirely `NaN` from the start. A simpler approach if the imputed mode ends up being `NaN` is to then drop those specific rows. The ground truth handles this more robustly by distinguishing between an entirely missing column and a column where a mode can be found.

2.  **`SettingWithCopyWarning` Likelihood (Medium):** The line `X[num_features] = num_imputer.fit_transform(X[num_features])` and the manual `X[col] = X[col].fillna(mode_value[0])` can trigger a `SettingWithCopyWarning` because `X` was created as `X = df[num_features + cat_features]`, which could be a view or a copy depending on pandas' internal heuristics. Using `.copy()` when creating `X` or explicitly using `.loc` for assignment would be safer.

3.  **Pipeline Usage (Minor Missed Opportunity):** While not explicitly *required* by the prompt, the inclusion of `Pipeline` and `ColumnTransformer` imports suggests that the intention might have been for a more robust sklearn pipeline approach. The generated code implements transformation steps manually. Using `ColumnTransformer` might have simplified the preprocessing logic for numerical and categorical features within a pipeline.

4.  **Redundant Imports:** `numpy` and `Pipeline`, `ColumnTransformer` are imported but not fully utilized in their typical `sklearn` pipeline context.

The code is very close but the categorical imputation logic, particularly for the 'all NA' island scenario, has a potential flaw and doesn't exactly match the robustness of the request or the ground truth's handling.",gpt-4.1,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Select features
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']

# 3) Drop rows with missing feature values
df_clean = df.dropna(subset=features)
X = df_clean[features].copy()
species = df_clean['...",9,"The generated code correctly implements all specified steps: loading data, selecting and cleaning features, standardizing, applying K-Means and PCA, and generating the two required scatter plots with appropriate coloring and titles.

Strengths:
- Correct handling of missing values (NA).
- Proper application of `StandardScaler`, `KMeans` (with correct parameters), and `PCA`.
- Generation of two clear scatter plots, one for K-Means clusters and one for original species.
- Correctly uses `n_init='auto'` and `random_state=42` for KMeans.
- Provides appropriate titles and labels for the plots.

Areas for minor improvement:
- The ground truth code uses `seaborn` for plotting which generally produces more aesthetically pleasing plots by default and simplifies some plotting aspects (`hue` parameter). While `matplotlib` is perfectly fine and used correctly here, `seaborn` is often preferred for data science visualizations.
- A `random_state` for PCA isn't strictly necessary as `fit_transform` usually doesn't have randomness, but it doesn't hurt.
- The `colorbar` for species labels is slightly less intuitive than a legend, especially if there are many unique species, though it works perfectly for the three species here. Aligning species names with indices on the colorbar can be slightly cumbersome for a user. Using `sns.scatterplot` with `hue` for both plots, like in the ground truth, makes the code more consistent and often provides clearer legends.
- No `FileNotFoundError` handling for the CSV file. This is a best practice, though not explicitly required by the prompt.",gpt-4.1,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the dataset
df = pd.read_csv(""penguins.csv"")

# 2. Impute missing values
num_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in num_cols:
    mean_val = df[col].mean()
    df[col].fillna(mean_val, inplace=True)

mode_sex = df['sex'].mode()[...",8,"The generated code largely addresses all requirements of the task.

**Strengths:**
*   **Correct Data Loading and Imputation:** The code correctly loads the `penguins.csv` dataset. It handles numerical `NaN` values by imputing with the mean of the respective columns and `sex` `NaN` values by imputing with the mode. It also correctly drops rows where `island` is `NaN`.
*   **Correct Feature Engineering:** It correctly identifies numerical features and performs one-hot encoding for `island` and `sex` columns, then concatenates them to form the feature matrix `X`. `species` is correctly label encoded for `y`.
*   **Correct Model Training and Feature Importance Extraction:** A `GradientBoostingClassifier` is trained with the specified parameters, and feature importances are correctly extracted and printed, associated with their feature names.
*   **Correct Visualization:** It generates box plots of `flipper_length_mm` grouped by `sex` for each `species`, as requested. Titles and labels are appropriate.

**Areas for Improvement (to reach a higher score):**
*   **Order of Operations for Imputation:** The ground truth code drops `island` NAs first, then imputes other columns. The generated code imputes numerical and `sex` columns *before* dropping `island` NAs. While the order might not cause an error in this specific case (as `island` is categorical and not being imputed with mean/mode of other columns), it's generally safer and more logical to drop rows based on a critical column like `island` *before* calculating means/modes for other columns, especially if those dropped rows could significantly affect the mean/mode calculation.
*   **Robustness to `FileNotFoundError`:** The ground truth includes a `try-except` block for `FileNotFoundError` during data loading, which is a good practice for robust code. The generated code lacks this.
*   **Column Selection for `get_dummies`:** The generated code applies `pd.get_dummies` to the entire `df` and then selects columns (`feature_cols`). The ground truth is more precise by encoding only the specified categorical columns (`island`, `sex`) separately and then concatenating, which can be slightly cleaner and less error-prone if there were other columns in `df` not intended for `X`.
*   **Direct `sns.boxplot` Usage for Grouped Plot:** The ground truth handles the box plots more efficiently by using `sns.boxplot` with `x='species'`, `y='flipper_length_mm'`, and `hue='sex'`, which creates a single consolidated plot showing all species and their sex distributions on one figure. The generated code uses a loop to create a separate plot for each species, which is also valid but might lead to many plots. The single combined plot is often preferred for comparison across species.
*   **Plot Styling Consistency:** The ground truth includes `sns.set_theme(style=""whitegrid"")` for consistent plot styling, which is a minor but good addition.

Overall, the code is highly functional and correctly implements the core requirements. The observations above are refinements rather than critical failures.",gpt-4.1,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer
import numpy as np

# 1. Load the dataset
df = pd.read_csv('penguins.csv')

# 2. Impute numerical columns with KNNImputer
num_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
imputer = KNNImputer(n_neighbors=3)
df[num_cols] = imputer.fit_transform(df[num_cols])

# 3. Impute categorical column 'sex' with the mode
mode_sex = df['sex'].mode(dropna=True)[0]
df['sex'] = df['sex'].fillna(mode_sex)

# ...",9,"The generated code correctly performs all the required steps: loading the dataset, applying KNN imputation to the specified numerical columns, and imputing the 'sex' column with its mode. It also verifies the remaining NA values, confirming the successful imputation.

Strengths:
- **Correctness:** The code accurately performs the requested imputation steps.
- **Conciseness:** The implementation is clean and to the point.
- **Completeness:** All task requirements are met.
- **Proper Library Usage:** `pandas` and `sklearn.impute.KNNImputer` are used correctly.

Areas for minor improvement (not critical enough to significantly reduce the score):
- **Error Handling:** The ground truth includes `try-except FileNotFoundError` for robust handling of missing CSV files, which is good practice. The generated code lacks this.
- **Clarity of Output:** While `print(na_counts)` shows the remaining NAs, adding more descriptive print statements (like the ground truth does at each step) would make the execution flow and outcomes clearer for a user.
- **In-place vs. Reassignment:** For `df['sex'] = df['sex'].fillna(mode_sex)`, while functionally correct, an `inplace=True` might be slightly more memory efficient, though it's often debated and re-assignment is perfectly valid. The `dropna=True` in `mode()` is redundant as `mode()` by default ignores NAs.

Overall, it's a very good solution that successfully addresses the problem statement.",gpt-4.1,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# 1. Load the dataset
df = pd.read_csv('penguins.csv')

# 2. Filter for 'Adelie' and 'Gentoo' species
filtered_df = df[df['species'].isin(['Adelie', 'Gentoo'])]

# 3. Drop rows with NA in 'culmen_length_mm'
filtered_df = filtered_df.dropna(subset=['culmen_length_mm'])

# 4. Get culmen_length_mm for each species
adelie = filtered_df[filtered_df['species'] == 'Adelie']['culmen_length_mm']
gentoo = filtered_df[filtered_df['species'] =...",8,"The generated code correctly performs all the required steps: loading the data, filtering species, handling NAs, conducting the t-test, and printing the results with the correct interpretation.

**Strong Points:**
- **Correctness:** The logic for data filtering, NA handling, and the t-test is sound and accurately reflects the task requirements.
- **Conciseness:** The code is quite concise and directly addresses the problem.
- **Readability:** Variable names are clear, and the steps flow logically, making the code easy to understand.
- **Completeness:** It addresses all 6 explicit requirements of the prompt.

**Areas for Improvement (to reach a higher score):**
- **Error Handling:** The code lacks error handling for `FileNotFoundError` or other potential issues when loading the CSV, which the ground truth includes. This makes it less robust.
- **Function Encapsulation:** The ground truth encapsulates the logic within a function, making it reusable and more modular. The generated code is a script, which is less ideal for larger applications.
- **Assumed Variance:** The generated code uses `equal_var=False` (Welch's t-test), while the ground truth defaults to `equal_var=True` (Student's t-test) and notes this in its comments. While Welch's test is often a safer default when variance equality is not explicitly known or tested, the prompt did not specify which to use. Sticking to the default for `ttest_ind` (which is `equal_var=True`) or explicitly stating the assumption would be good practice if not specified. Given the prompt didn't specify, either is technically acceptable, but consistency with commonly taught basic t-tests often implies `equal_var=True` unless `equal_var=False` is explicitly motivated.
- **Output Verbosity/Clarity:** The ground truth provides more verbose and user-friendly output, including information about data loading, filtering steps, and mean values, which aids in understanding the data and the process. The generated code's output is minimal.
- **Docstrings/Comments:** The ground truth includes a docstring for the function and more explanatory print statements and comments, which significantly improve code documentation and help others understand its purpose and execution steps.",gpt-4.1,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder

# 1. Load the dataset
df = pd.read_csv('penguins.csv')

# 2. Define features and target
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 
            'body_mass_g', 'island', 'sex']
target = 'species'

X = df[...",8,"The generated code correctly addresses almost all requirements of the task. It loads the data, defines features and target, performs the specified preprocessing (median imputation for numerical features, mode imputation for 'sex', one-hot encoding for 'island' and 'sex', and label encoding for 'species'). It also splits the data correctly, trains a `RandomForestClassifier`, makes predictions, and prints the accuracy and classification report.

**Strong points:**
*   Correct application of imputation strategies (median for numeric, mode for 'sex').
*   Correct one-hot encoding using `pd.get_dummies`.
*   Correct label encoding for the target variable.
*   Correct stratified train-test split with `random_state`.
*   Correct RF model training and evaluation with accuracy and classification report.
*   The `target_names` handling in `classification_report` is robust, checking if `le` (LabelEncoder) is defined before using `le.classes_`.

**Areas for improvement (preventing a higher score):**
*   **Preprocessing Structure:** While functionally correct, the preprocessing is done manually (loops for imputation, `pd.get_dummies`). The ground truth code utilizes `SimpleImputer`, `OneHotEncoder`, `ColumnTransformer`, and `Pipeline`. This sklearn-based approach is more robust, scalable, and idiomatic for machine learning preprocessing, especially for preventing data leakage during cross-validation, although not explicitly required by this task. The manual approach separates preprocessing from the `train_test_split` and thus applies transformations to `X` directly, which could in theory lead to data leakage if not handled carefully (though not in this specific case, as `X` is transformed before the split). For a ""benchmark"" standard, the `Pipeline` approach is generally preferred.
*   **Encoders:** `LabelEncoder` is applied to `y` (the target variable) *before* the split, which is fine and common. However, the One-Hot Encoding is applied directly to `X` using `pd.get_dummies`. If the `train_test_split` were to occur *before* the OHE, you'd need to fit the OHE on the training data and transform both train and test sets to avoid data leakage or issues with unseen categories in the test set. While `pd.get_dummies` is used correctly here because it's applied to the full `X` before splitting, for a more robust MLOps type of pipeline, a `ColumnTransformer` within a `Pipeline` (as in the ground truth) would be preferred to handle all preprocessing steps together.
*   **Clarity on `drop_first=True`:** While not an error, the `drop_first=True` for `pd.get_dummies` for one-hot encoding is a choice. For tree-based models, it usually doesn't significantly impact performance, but for linear models, it helps avoid multicollinearity. If the task hadn't specified ""one-hot encode"", then it's a good choice, but ""one-hot encode"" technically implies creating a column for every category. However, this is a minor stylistic/best practice point rather than a critical error.",gpt-4.1,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# 1. Pearson correlation between PerformanceScore and Yea...",9,"The generated code correctly defines the DataFrame, calculates the specified Pearson correlation coefficients, prints them, and correctly identifies the feature with the stronger absolute correlation with ""PerformanceScore"".

**Strengths:**
- **Correctness:** All parts of the task are solved correctly. The DataFrame is created accurately, correlations are computed as Pearson (which is the default for `corr()`), and the comparison of absolute correlations is sound.
- **Conciseness:** The code is clean and to the point, avoiding unnecessary lines.
- **Readability:** Variable names are sensible, and comments indicate the different steps.
- **Formatting:** f-strings are used for printing, and correlation coefficients are formatted to 4 decimal places, which is good practice.

**Areas for Minor Improvement (for a perfect score):**
- **Explicit `method='pearson'`:** While `df.corr()` defaults to Pearson, explicitly including `method='pearson'` as in the ground truth makes the code more robust and clear, especially for someone who might not be aware of the default behavior or if the pandas default changes in future versions (though unlikely for this specific method). This is a very minor point for correctness, but it's good practice for clarity.
- **More descriptive tie-handling:** The ground truth provides a slightly more descriptive print statement for the tie-case (""Both have equal strength with a value of X""). The generated code simply prints ""Both have equal strength"". While functionally correct, providing the actual value for the tie is more informative.

These are minor improvements that don't detract significantly from the overall correctness or quality of the solution. The code is highly effective and well-written for the given task.",gpt-4.1,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test
from lifelines.plotting import plot_lifetimes
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import logit, glm
from statsmodels.stats.weightstats import ttest_ind
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors
from sklearn.utils impo...",6,"The generated code attempts to address all requirements of the prompt but falls short in several key areas, primarily due to misinterpretations of the task's instructions and some fundamental flaws in its approach to survival analysis and statistical rigor.

**Strengths:**
*   **Comprehensive Imports:** Includes most necessary libraries, demonstrating an understanding of the tools required.
*   **Structured Approach:** The code is well-sectioned according to the requested tasks.
*   **Basic Implementations:** Successfully implements basic Kaplan-Meier curves, Cox PH models, propensity score matching (though flawed), and bootstrap resampling. It also includes a Bayesian model.
*   **Interaction Effects:** Attempts to test interaction effects in the multivariate analysis.

**Weaknesses and Areas for Improvement:**

1.  **Fundamental Misinterpretation of ""Time-to-Event"" for Kaplan-Meier and Cox PH:**
    *   **Prompt:** ""Implement Kaplan-Meier survival analysis treating the voyage as time-to-event data (with survival as event indicator)"". This implies a *fixed time* for the voyage, and the event is whether *death occurred during that voyage*. The ground truth correctly interprets this by setting `T = np.ones(len(df_encoded))` and `E = 1 - df_encoded['Survived']`. This effectively makes the Cox model a logistic regression on a binary outcome, which is a common way to model binary outcomes when using survival packages.
    *   **Generated Code:** Uses `df['Age']` as the `time` variable. This is incorrect for the prompt's context of ""the voyage as time-to-event."" If survival means surviving *the Titanic disaster*, then age is a covariate, not the time duration of exposure to risk of the event (death during the voyage). This fundamental misunderstanding makes the Kaplan-Meier and Cox PH results largely irrelevant to the stated problem.
    *   **Impact:** The Kaplan-Meier curves are plotting ""survival probability by age"" which is not what was asked. The Cox PH model also uses 'Age' as 'time', leading to outputs that don't model the Titanic survival event as requested.

2.  **Kaplan-Meier Event Indicator:**
    *   **Prompt:** ""survival as event indicator"". The conventional notation in lifelines is `event_observed=True` for an event (death) and `False` for censored (survived). The generated code uses `E = df['Survived']` which means `1` for survived and `0` for died, which is the opposite of the standard convention for `event_observed` (where `1` typically means the event happened). This would result in inverted survival curves.

3.  **Cox Proportional Hazards Model:**
    *   **`Survived_status`:** The generated code correctly inverts `Survived` to `1 - Survived` to make death the event, but then uses 'Age' as `time`, as mentioned in point 1.
    *   **`check_assumptions`:** While `check_assumptions` is called, in the context of `T` being `Age`, the assumptions are likely violated, but the code doesn't comment on or interpret this.

4.  **Propensity Score Matching:**
    *   **Treatment Definition:** ""Analyze treatment effects (e.g., passenger class impact)"". The generated code defines `Treatment` as `(psm_data['Pclass'] == 1)`. This is a reasonable choice.
    *   **Matching Quality:** The code performs matching and calculates ATET but **does not perform a covariate balance check** (e.g., using Standardized Mean Difference - SMD), which is crucial for evaluating the success of PSM. Without this, the validity of the matched comparison is questionable.
    *   **Confidence Intervals:** While bootstrapping is applied to the ATET, the primary evaluation of PSM's success (balance) is missing.

5.  **Multivariate Analysis and Model Diagnostics:**
    *   **Logistic Regression:** Correctly implements logistic regression and interaction effects.
    *   **Model Diagnostics for Logistic Regression:** The `plot_partregress_grid` is a good attempt at checking linearity assumptions, but it's not a full suite of diagnostics for logistic regression. The prompt asked for ""model diagnostics"" generally, and for ""Cox PH model diagnostics"" explicitly.

6.  **Bayesian Analysis:**
    *   **`dropna()`:** The code uses `bayes_data = df.dropna().copy()` before passing to PyMC3. This is an explicit way to handle NaNs but drops any rows with missing data, which might not be the most robust approach and makes the model less generalizable if NaNs are prevalent. The ground truth imputed missing values earlier.
    *   **Priors:** The priors are very wide (`sigma=10`), which might be too vague, especially for `Fare` which can be large. Standardizing inputs for Bayesian models (as done in the ground truth) is generally good practice for better sampling and prior specification.
    *   **Interpretation:** While it computes a summary and plots, there's no interpretation of the Bayesian results or uncertainty.

7.  **Sensitivity Analysis:**
    *   The sensitivity analysis is very basic, demonstrating the concept by simply removing a variable or stratifying by age group. It doesn't perform the more comprehensive sensitivity analysis like testing imputation strategies or prior sensitivity for the Bayesian model, as outlined conceptually in the ground truth.

8.  **Redundancy/Minor Issues:**
    *   `from lifelines.plotting import plot_lifetimes`: This is imported but not used.
    *   `from statsmodels.stats.weightstats import ttest_ind`: `scipy.stats.ttest_ind` is generally preferred and more common.
    *   `df.rename(columns={c: c.strip() for c in df.columns})` is good practice but `df['Age'] = df['Age'].fillna(df['Age'].median())` then `Embarked` with `mode`. This imputation then needs to be consistent across various uses including the `dropna()` for Bayesian. The ground truth handles this more consistently in a single preprocessing step.
    *   Fixed `Embarked` to numerical mappings (`0,1,2`) without one-hot encoding, which treats it as an ordinal variable. While not strictly wrong, one-hot encoding (as in ground truth) is generally preferred for nominal categorical variables in regression models.

In summary, the code correctly identifies the tools needed and the high-level tasks but significantly misinterprets the core survival analysis component (time-to-event) and lacks the statistical rigor in model validation and sensitivity analysis that would be expected from an ""expert code evaluator"" benchmark.",gpt-4.1,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# 1. Create DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
    'Final_Score': [90, 88, 82,...",10,"The generated code is excellent and correctly addresses all parts of the task.

**Strong points:**
*   **Correctness:**
    *   Successfully creates the DataFrame as specified.
    *   Correctly calculates and prints the Pearson correlation coefficient.
    *   Generates a scatter plot with the correct variables, colored by 'Subject', and includes all required plot attributes (title, labels, legend).
    *   Accurately adds the 'Overall_Grade' column based on the specified criteria.
    *   Correctly counts and prints the distribution of 'Overall_Grade' categories.
*   **Code Quality:**
    *   Properly imports all necessary libraries (`pandas`, `matplotlib.pyplot`, `seaborn`, `scipy.stats`).
    *   The code is well-structured and follows a logical flow corresponding to the task steps.
    *   Uses `sns.scatterplot` for better aesthetics and automatic legend generation based on `hue`.
    *   The `grade` function is a clean way to apply the grading logic.
    *   Uses f-strings for clear output.
    *   `plt.tight_layout()` is included for better plot rendering.
*   **Completeness:** All requirements of the prompt are met precisely.

The solution is robust and clearly demonstrated understanding of the data manipulation, statistical calculation, and visualization aspects of the task.",gpt-4.1,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Create DataFrame and set 'Date' as index
data = {
    'Date': pd.to_datetime([
        '2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05',
        '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12'
    ]),
    'Ticker': ['TECHCO']*10,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155...",9,"The generated code successfully completes all aspects of the task. It correctly creates the DataFrame, calculates the daily returns, and computes both 3-day and 5-day simple moving averages. The plotting functionality is well implemented with appropriate labels, title, and legend. Crucially, it accurately identifies and prints the bullish crossover dates using the specified logic.

Areas for minor improvement (to reach a perfect 10):
- **Plotting Aesthetics:** While the plot is functional, the ground truth code used specific `color` and `linewidth` parameters that made the plot slightly more visually appealing and distinct. The generated code also adds `marker` styles which are not strictly necessary for a line plot of this nature and can clutter the visualization, especially for longer series.
- **Clarity of Crossover Logic Variable Names**: The variable names `prev` and `curr` for the crossover logic are a bit terse. `previous_condition` and `current_condition` (or similar) might offer slightly more clarity for readability, although `crossover` is good. This is a very minor point and doesn't detract from correctness.

Overall, this is an excellent solution.",gpt-4.1,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. Create DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', ...",9,"The generated code correctly addresses all requirements of the task. It properly creates the DataFrame, calculates total sales revenue per product, generates and displays the horizontal bar chart as specified, calculates the average order value, and identifies repeat customers with their purchase counts.

Strong points:
- **Correctness:** All calculations are accurate and the plot is correctly generated.
- **Completeness:** All 5 parts of the task are fully implemented.
- **Code Quality:** The code is clean, well-structured, and includes necessary imports. Variable names are clear.
- **Plotting:** The chart is correctly generated with the specified labels and title. `plt.tight_layout()` is used which is good practice.

Areas for minor improvement (not critical enough to reduce the score significantly):
- **Sorting in plot:** The `product_revenue` series is sorted in `sort_values()` without specifying `ascending=False`. While `barh` will still show the data, it's generally better practice to sort it so larger bars are at the top/bottom for better readability. The ground truth sorts it descending. (This is a minor aesthetic point, not a correctness issue).
- **Output formatting for repeat customers:** The ground truth provides a slightly more structured output (header, separator) for the repeat customers, which can improve readability for larger outputs. The generated code's output is functional but could be slightly improved.
- **Prints:** The generated code only prints the average order value and repeat customers. The ground truth includes prints for the initial DataFrame and aggregated product revenue which can be helpful for debugging or understanding the intermediary steps. The prompt only asked to *display* the plot and *print* the values for the last two steps, so the generated code is technically compliant.",gpt-4.1,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd
import numpy as np

# Define the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 1. Map DeviceType to numeric
df['DeviceType_Numeric'] = df['DeviceType'].map({'Desk...",10,"The generated code is excellent and correctly addresses all aspects of the task.

**Strong Points:**
*   **Correctness:**
    *   The initial DataFrame is correctly defined.
    *   `DeviceType_Numeric` is created perfectly using `map`.
    *   `PagesPerMinute` is calculated correctly, and the handling of division by zero (NaNS/Infs) is robust and concise, using `replace` followed by `fillna`. This is an elegant way to handle it.
    *   The correct feature columns are selected.
    *   The head of the final feature DataFrame is printed as requested.
*   **Code Quality:**
    *   Proper imports (`pandas`, `numpy`) are present.
    *   Variable names are clear and descriptive (`feature_cols`, `features_df`).
    *   The code is well-structured, following the logical flow of the task requirements.
    *   Comments are minimal but effective, clarifying each step without being overly verbose.
*   **Completeness:** All four requirements of the task are successfully met.

The generated code is nearly identical in functionality and quality to the ground truth, demonstrating a strong understanding of pandas operations and data preparation techniques.",gpt-4.1,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""Timestamp"": [
        ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"",
        ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""
    ],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 3...",9,"The generated code correctly defines the DataFrame, calculates the mean and standard deviation of ""TemperatureCelsius"", and identifies outliers based on the 1.5 standard deviation threshold. It also correctly prints the required information for each outlier.

Strong points:
- Correctly implements outlier detection logic using `abs()` to check deviations both above and below the mean.
- Prints the specified details for each outlier.

Areas for minor improvement:
- The print format for the outliers could be slightly more explicit, perhaps similar to the ground truth which labels the outlier temperature.
- The ground truth includes optional conversion of 'Timestamp' to datetime objects, which is good practice and could be added for robustness in general scenarios, though not strictly necessary for this specific problem.
- The ground truth also includes helpful print statements for the calculated mean, std, and bounds, which aid in understanding the intermediate steps of the outlier detection. While not strictly required by the prompt, they improve clarity.",gpt-4.1,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [1...",9,"The generated code correctly defines the DataFrame, calculates the EngagementScore, and accurately identifies the PostType with the highest average EngagementScore for each platform. The code is well-structured and uses appropriate pandas methods.

The primary difference from the ground truth is the method used to find the highest average engagement score per platform. The generated code uses `idxmax()` on the grouped data, which is a concise and efficient way to achieve the desired result. The ground truth sorts and then uses `drop_duplicates`, which also works but is slightly less direct for this specific problem (though perhaps more flexible for other scenarios).

A minor improvement for the generated code would be to format the `EngagementScore` in the print statement to two decimal places, similar to the ground truth, for better readability (e.g., `{row['EngagementScore']:.2f}`). However, this is a cosmetic detail and doesn't affect correctness. The prompt asked for ""only Python code"", and the generated code provided only Python code, fulfilling this requirement.",gpt-4.1,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

# Create DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold""...",9,"The generated code is highly correct and addresses almost all requirements of the task.

**Strong points:**
- **Correctness:** The code correctly defines the DataFrame, calculates ""LengthOfStay"", computes the average stay and patient count per diagnosis, and identifies the diagnosis with the longest average stay.
- **Clarity and Readability:** The code is well-structured and easy to understand. Variable names are descriptive.
- **Efficiency:** Uses pandas aggregation efficiently, which is the recommended way for such operations.

**Areas for improvement:**
- **Printing Format (Minor):** While the printing of the information is correct, the ground truth code used f-strings with alignment (`:<`) and a header, which makes the output more visually appealing and organized, especially for tabular data. The generated code's printout is functional but less formatted. This is a minor aesthetic point rather than a correctness issue.

Overall, it's a very strong submission that fulfills the task requirements effectively. The slight difference in print formatting is the only reason it didn't get a perfect 10, but it doesn't detract from the core logic or correctness.",gpt-4.1,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",10,"The generated code correctly defines the DataFrame, calculates the ""WeightedRating"" as specified, adds it as a new column, and accurately identifies and prints the title and weighted rating of the movie with the highest ""WeightedRating"". The code is concise and directly addresses all parts of the task. It demonstrates a good understanding of pandas operations for data manipulation and querying.",gpt-4.1,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000,...",9,"The generated code correctly creates the DataFrame as specified, calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"", and groups this information into a new DataFrame, fulfilling all task requirements. The code is concise and uses appropriate pandas methods (`groupby`, `mean`, `reset_index`).

The only minor area for improvement, preventing a perfect 10, is that the column names for the aggregated averages default to ""PriceUSD"" and ""Mileage"" in the output DataFrame (`grouped_df`). While this is perfectly correct, the ground truth uses named aggregations (`Average_PriceUSD`, `Average_Mileage`) which can sometimes lead to clearer and more descriptive column names in larger, more complex aggregation scenarios. This is a stylistic choice and doesn't impact correctness.",gpt-4.1,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63, 83],
    ""De...",10,"The generated code correctly defines the DataFrame, accurately calculates the ""TotalStats"" column, precisely identifies and prints the name of the Pokemon with the highest ""TotalStats"", and correctly calculates and prints the average ""Speed"" for each ""Type1"". The code is well-structured, uses appropriate pandas functions, and directly addresses all parts of the task. The use of `df[[""HP"", ""Attack"", ""Defense"", ""Speed""]].sum(axis=1)` for 'TotalStats' is a robust and clear way to sum multiple columns. Pinpointing the best Pokemon using boolean indexing `df[df[""TotalStats""] == max_total][""Name""].values[0]` is also effective and correct. The `groupby().mean()` for average speed per type is standard and correct.",gpt-4.1,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict
from itertools import combinations

# Sklearn Imports
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mi...",7,"The generated code attempts to address all aspects of the prompt and has some good implementations for feature engineering and a wide range of clustering techniques. However, there are several issues that prevent it from being a higher score:

**Correctness & Completeness Issues:**
*   **Feature Engineering:**
    *   `df.get('SibSp', 0)` and `df.get('Parch', 0)`: Using `get` with a default of 0 is okay for features that might not exist, but `SibSp` and `Parch` are standard Titanic dataset columns and should always exist. It's an unnecessary defensive programming choice that could mask data loading errors if the column names were subtly wrong.
    *   `FareBin` and `AgeBin`: These are engineered but then directly used in `X` without proper handling in `StandardScaler`. Binning converts numerical data to ordinal/categorical, but then scaling them as if they were continuous numbers (which `StandardScaler` does after LabelEncoding them) can be problematic. They should ideally be one-hot encoded or handled differently if they are treated as bins.
    *   `Title` extraction: The `get_title` function is less robust than the ground truth's regex approach and lists out specific titles. It might miss some variations (e.g., ""Mlle."" vs ""Mlle""). The ground truth's regex is more general.
    *   `TicketPrefix` extraction: The regex `r'([A-Za-z\.\/]+)'` is good, but `fillna('None')` is used before stripping, which might not be ideal if 'None' then becomes a prefix after stripping. The ground truth handles this more robustly by grouping rare prefixes.
    *   Social Network Features: `FamilyID` and `FamilyGroup`/`FamilyGroupSize` introduce potential redundancies or complexities that aren't clearly differentiated or utilized. The approach to `FamilyID` based on surname and S/P adds very little beyond `FamilySize` and `IsAlone`. The ground truth's `TicketGroupSize` and `FarePerPerson` are more direct and useful social network features.
    *   Missing `CabinDeck`: The ground truth extracts `CabinDeck` (A, B, C, etc.) which is a valuable feature, while the generated code just extracts the first character (`Deck`) and doesn't handle the 'U' for unknown well (it replaces 'n' with 'Unknown', but 'n' is not a typical cabin deck). The ground truth correctly uses 'U' for unknown.
*   **Preprocessing:** `LabelEncoder` is used for all categorical columns, including those with nominal relationships (like 'Sex', 'Embarked', 'Title', 'Deck', 'TicketPrefix'). For nominal categories, `OneHotEncoder` (as used in the ground truth) is almost always preferred because `LabelEncoder` imposes an arbitrary ordinal relationship which can negatively impact clustering algorithms that rely on distance metrics. This is a significant issue.
*   **Optimal Clusters:** The `compute_gap_statistic` function is incomplete. It calculates `ref_disp` and `orig_disp`, but then computes `gap` using `np.log(np.mean(ref_disps)) - np.log(orig_disp)`. This is not the standard way to calculate the gap statistic (which typically involves comparing the within-cluster dispersion of the real data to that of random data). It also doesn't provide the standard error for comparison. The ground truth, while not implementing gap statistic directly, did consider standard metrics.
*   **Clustering Algorithms:** DBSCAN's `eps` and `min_samples` are highly dataset-dependent. Using fixed values without a mechanism for tuning (e.g., k-distance graph) makes its results less reliable. The ground truth acknowledges this and uses reasonable defaults.
*   **Visualization:** The loop for visualization shows all clustering results on the same reduced dimension subplot, which is helpful, but the ground truth provides separate figures for each combination, making it clearer. The ground truth's additional plots (survival rates, feature distributions per cluster, heatmap of scaled feature means) are more comprehensive for cluster analysis.
*   **Stability Analysis:** The `stability_index` function samples the data and re-clusters, then compares using ARI. This is a reasonable approach. However, it's applied after the clustering, meaning it re-runs parts of the pipeline multiple times. The ground truth's stability analysis for K-Means (`n_init` runs for different random seeds on full data) is also a valid and simpler way for K-Means. The ground truth provides a clearer, more standard consensus clustering approach than just averaging co-association matrix and then applying hierarchical clustering.
*   **External Validation:** The `validate_clustering` function correctly uses ARI and MI, and also looks at the standard deviation of survival rates, which is a good indicator.

**Code Quality Issues:**
*   A lot of global variable access (`df`, `survival_col`, `X_scaled`, `X_index`). Wrapping logic into functions would improve modularity.
*   Lack of clear comments explaining complex logic or assumptions.
*   Suppression of warnings (as in the ground truth) is missing, which could lead to noisy output especially with K-Means `n_init` warnings in older sklearn versions.
*   The `LabelEncoder` choice for all categoricals is a significant flaw as mentioned above.

Overall, the generated code demonstrates an understanding of the task and attempts to implement most requirements. The feature engineering is more complex but not necessarily better than the ground truth. The preprocessing choices (especially `LabelEncoder` for nominal features) and the partial/incorrect implementation of gap statistic are major drawbacks. The visualizations are also less comprehensive than the ground truth. The stability analysis is good though could be more generalized.",gpt-4.1,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dystopian""],
    ""Stock"": [...",9,"The generated code is highly correct and concise.

**Strong Points:**
- **Correctness:** All three parts of the task (DataFrame creation, counting books/average price per author, and finding the author with most stock) are correctly implemented.
- **Conciseness:** Uses efficient pandas `groupby()` and `agg()` operations effectively.
- **Completeness:** Addresses all requirements of the prompt.
- **Code Quality:** The code is clean, well-structured, and easy to understand. Variable names are appropriate.

**Areas for Minor Improvement:**
- **Printing Format:** While `print(author_stats)` correctly outputs the DataFrame, the ground truth's formatted loop (`for index, row in author_stats.iterrows(): print(...)`) provides a more readable and user-friendly output, especially for a benchmark where presentation matters. The prompt specifically asked to ""Print this information, showing Author, Count of Books, and Average Price,"" and the table directly fulfills that, but the string formatting is arguably *more* explicit to the user.
- **Clarity of Output:** The ground truth added descriptive print statements and separators (e.g., ""--- Author Book Statistics ---"") which improve the readability of the output, making it clearer what each section of the printout represents. This is a minor aesthetic point, not a functional flaw.

Overall, excellent solution. The improvements are primarily about output presentation rather than functional correctness.",gpt-4.1,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import (train_test_split, StratifiedKFold, GridSearchCV, cross_val_score,
                                     learning_curve, validation_curve)
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from sklearn.svm im...",6,"The generated code attempts to address all requirements but falls short in critical areas, particularly in the robust implementation of advanced preprocessing and comprehensive evaluation strategies.

**Strengths:**
*   **Comprehensive Imports:** Includes most necessary libraries.
*   **Multiple Imputation Strategies:** Attempts to compare different imputers and select the best, although the method of selection could be more rigorous (e.g., using a proper CV pipeline).
*   **Outlier Treatment & Feature Engineering:** Includes IQR capping and polynomial features.
*   **Multiple Base Models with Optimization:** Implements GridSearchCV for several models (RF, GB, XGB, MLP, SVM).
*   **Ensemble Methods:** Includes Voting and Stacking, and a custom Blending approach.
*   **Evaluation Metrics:** Calculates various metrics like ROC AUC, Accuracy, Precision, Recall, F1-Score, Brier Score, and LogLoss.
*   **Learning and Validation Curves:** Plots these as requested.
*   **SHAP and LIME:** Includes both interpretability tools.
*   **Model Calibration:** Implements calibration curves and CalibratedClassifierCV.
*   **Statistical Significance Testing:** Attempts a paired t-test between top models.
*   **Model Comparison Dashboard:** Creates a summary DataFrame and bar plots.

**Weaknesses and Areas for Improvement:**

1.  **Data Preprocessing Pipeline (Critical for 1a):**
    *   **Imputation Strategy Evaluation:** The method for choosing the best imputer is problematic. `cross_val_score` is directly applied to `X_imp` (processed `X_missing`), which contains imputed values for *all* data, effectively leaking information from the test set into the imputer selection process. Imputation *must* be done within a pipeline and fitted only on the training data. The ground truth correctly uses `IterativeImputer` as part of the `Pipeline` applied *after* `train_test_split`.
    *   **Preprocessing Flow:** The preprocessing steps (imputation, outlier capping, scaling, poly features) are applied sequentially *before* `train_test_split`. This is a major data leakage issue. Scalers and imputer fits should *only* be done on the training data, and then used to transform both training and test sets. The ground truth uses `ColumnTransformer` and `Pipeline` to ensure proper fit/transform on train/test splits.
    *   **Custom Outlier Capper:** The generated code's `iqr_cap` function is applied *before* splitting the data, leading to data leakage. The ground truth provides a proper `OutlierCapper` class within a pipeline, ensuring it learns and applies bounds only from the training data.

2.  **Model Optimization (Completeness for 2):**
    *   While GridSearchCV is used, the ground truth explicitly integrates the full preprocessing pipeline into the cross-validation setup for hyperparameter tuning, which is best practice. The generated code performs optimization on already preprocessed `X_train`, which is acceptable for base models *if* the preprocessing was done correctly, but does not test the preprocessing steps themselves.

3.  **Bootstrap Sampling for Confidence Intervals (Correctness/Completeness for 5):**
    *   The `bootstrap_auc` function refits the model (and potentially preprocessing anew if not careful) in each bootstrap iteration on `X_b`. Then it predicts on the *original* `X_test`. This is fundamentally incorrect for creating confidence intervals of the *test set performance*. Bootstrap sampling for confidence intervals usually means resampling the *test set* and evaluating the *already trained* model on these resampled test sets. The ground truth correctly resamples `X_test` and `y_test` and evaluates the *final, trained* model. The generated code also has an issue with `X_train.iloc[idx]` and `y_train[idx]` inside `bootstrap_auc` where `X_train` is a DataFrame but `y_train` is a numpy array.

4.  **Statistical Significance Testing (Correctness for 6):**
    *   The paired t-test
        `t_stat, p_value = ttest_rel(mdl1_preds, mdl2_preds)` tests the predictions on the *test set*, not the cross-validation scores across folds. A paired t-test to compare models should ideally be performed on the performance metric (e.g., ROC AUC) for each fold from a k-fold cross-validation, as this provides a more robust comparison of model generalizability. The ground truth demonstrates this correctly by collecting `cv_scores_per_model` and then performing `ttest_rel` on these lists of scores per fold.
    *   Also, `mdl1_preds` and `mdl2_preds` are just the raw probability predictions, not performance metrics. A t-test on these values is not meaningful for comparing models.

5.  **Model Interpretability (Completeness/Correctness for 7):**
    *   SHAP summary plot should handle `shap_values[1]` for binary classification correctly (the ground truth does this).
    *   The generated code uses LIME and SHAP, but the SHAP plots are shown in a minimal way (`plt.figure()` without specific sizing), and LIME's `show_in_notebook` is called directly which won't display in a non-notebook environment.

6.  **Prediction Confidence Intervals (Completeness for 7):**
    *   The generated code attempts `pred_conf_interval` by resampling `X` (implied `X_train`) to re-fit the model and then predict on `X_test`. This is an unusual and not standard way to get prediction confidence intervals. Usually, it's about ensemble predictions and their variance, or bootstrapping the *residuals* for regression, or using specific Bayesian approaches. The ground truth simply shows confidence intervals on the *performance metrics* (like ROC AUC of the model), which aligns better with the common interpretation of ""prediction confidence intervals"" in a benchmark context often meaning ""certainty about the model's performance on unseen data."" The generated code's `pred_conf_interval` seems to be trying to get per-sample prediction confidence, which is a very advanced and distinct task not explicitly requested in detail.

7.  **Code Structure and Readability:**
    *   The generated code uses global variables (`models`, `results`) to store information, which can lead to maintainability issues. The ground truth organizes its output better by using dictionaries and clear variable names.
    *   Repeated `fit` calls for blending. The ground truth avoids this by cloning models or careful structure.
    *   The `models` dictionary includes the fitted ensemble models directly, which is less ideal for further analysis where you might want to specifically work with the base models or their properties.

Overall, the data leakage in preprocessing and the incorrect implementation of bootstrap sampling for confidence intervals and statistical significance testing are major flaws that detract from the generated code's overall quality and correctness.",gpt-4.1,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from scipy.stats import f_oneway, permutation_test
from sklearn.datasets import make_classification
from sklearn.impute import KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from fancyimpute import IterativeImputer as MICE
from sklearn.linear_model import LogisticRegressionCV, LogisticRegression
from s...",7,"The generated code attempts to address most of the requirements but falls short in full implementation and correctness for several advanced statistical concepts.

**Strengths:**
*   **Missing Data Imputation:** Correctly implements KNN and Iterative Imputer, and even includes `fancyimpute.MICE` for a third method. It also includes a basic comparison.
*   **Data Preparation:** Handles data loading, separation of features/target, and scaling.
*   **Anomaly Detection:** Correctly implements Isolation Forest, LOF, and One-Class SVM.
*   **PCA and Factor Analysis:** Includes proper usage of PCA and Factor Analysis for dimensionality reduction.
*   **Basic Model Diagnostics:** Includes some residual analysis and Q-Q plots.
*   **Visualizations:** Attempts to create a diagnostic dashboard and other requested plots.

**Weaknesses and Areas for Improvement:**

1.  **Missing Data Imputation Comparison:** The `compare_imputations` function compares imputed dataframes to the *original* dataframe using `mean()` and `var()`. This is not a meaningful comparison of *effectiveness* for imputation. Effectiveness is usually assessed by how well the imputed values preserve the relationships or distributions of the *original complete data*, or by how well models built on imputed data perform. A better comparison might involve comparing distributions of the *imputed specific columns* or using a metric that assesses recovery if a known subset was removed.
2.  **GAM Implementation:** The GAM implementation uses `statsmodels.gam.api.GLMGam` which is suitable, but the usage with `BSplines` directly on `X_train` without specifying the feature for each spline is unusual and might not yield an interpretable GAM for individual features. The request specifically asks for `pygam`, which is generally more user-friendly and explicitly handles smoothing terms. The current statsmodels GAM implementation prints a summary but doesn't perform predictions or partial dependence plots, which are key for GAMs.
3.  **Bayesian Logistic Regression (PyMC3):**
    *   The `pm.sample_posterior_predictive` call uses `var_names=[""y_obs""]`. For classification, `y_obs` will return binary outcomes (0 or 1), not probabilities. To get probabilities, one should sample for `mu` or `p` (if defined as `pm.Deterministic('p', pm.math.sigmoid(mu))`). This means the subsequent performance evaluation (ROC-AUC for `y_obs`) will be incorrect as ROC-AUC requires probabilities.
    *   The `progressbar=False` argument is not standard for `pm.sample` or `pm.sample_posterior_predictive` and won't suppress the progress bar.
4.  **Advanced Hypothesis Testing:**
    *   **Permutation Test:** The implemented permutation test is a good start, but the `permutation_test` import from `scipy.stats` is not used. The manual implementation is okay, but `scipy.stats.permutation_test` provides a more robust and direct approach.
    *   **Bootstrap CI for Means:** This is correctly implemented.
    *   **Multivariate ANOVA (MANOVA):** The MANOVA setup for `statsmodels.multivariate.manova.MANOVA` looks correct, but it should be explicitly handled in a try-except block as it can fail with singular matrices, and the output `print(manova.mv_test())` can be verbose and hard to interpret without proper context.
5.  **Robust Statistical Methods:** The solution mentions ""Robust Logistic Regression via sample reweighting (Huber)"" but then uses `sklearn.linear_model.SGDClassifier`. `SGDClassifier` with `loss='log'` is just regular (stochastic gradient descent) logistic regression. It's not inherently ""robust"" in the statistical sense (like Huber loss or M-estimators in `statsmodels.robust`). The task asked for robust methods to handle outliers, which usually implies methods like `statsmodels.robust.robust_linear_model.RLM` or methods like Huber loss for regression. For classification, robust methods are less straightforward than for regression. The `RANSACRegressor` is imported but not used, and is for regression, not classification. This section is weak.
6.  **Model Diagnostics:**
    *   **Residual Analysis for Logistic Regression:** This is a common point of confusion. Logistic regression residuals are *not* normally distributed. Plotting `y_test - y_prob` as ""residuals"" and looking at its histogram and Q-Q plot is not the standard way to diagnose logistic regression. Deviance residuals or Pearson residuals are more appropriate, and those are generally not expected to be normal. The `statsmodels.graphics.gofplots.qqplot` should be used with appropriate residuals (e.g., deviance residuals from a GLM model).
    *   **Leverage vs. Residuals / Influence plots:** The `sm.OLS(y_train, ...).fit()` is used here. For logistic regression, you should use diagnostics from `GLM` from `statsmodels` (or `glm_results.get_influence()` if `sm.Logit` was used) rather than `OLS` as the assumptions differ. Using OLS for binary outcomes is incorrect.
    *   **Goodness-of-Fit Tests:** The Hosmer-Lemeshow ""approximation"" is just calculating a chi-squared statistic based on counts, but not the actual Hosmer-Lemeshow test. It needs a dedicated implementation or library function (though `statsmodels` does not directly provide it as part of `GLM` summary). The example given for HL test is not standard.
7.  **Completeness:**
    *   **""Q-Q plots, leverage plots, and diagnostic dashboards""**: Q-Q plots for *features* are done, but the prompt specifically asked for model diagnostic plots like Q-Q plots of *residuals*. Leverage plots were attempted but with incorrect model (`OLS`). The diagnostic dashboard is a good effort but some plots within it rely on prior incorrect model diagnostics.
    *   **Proper Imports:** Imports are mostly fine, but some are not used (`make_classification`, `permutation_test` from `scipy.stats`, `LogisticRegressionCV` from `sklearn.linear_model` is used, but another `LogisticRegression` is also imported unnecessarily). `GridSpec` is a good choice for the dashboard.
    *   The `fancyimpute` library is not standard and adds an external dependency beyond the specified ones (pandas, statsmodels, pymc3, scipy, sklearn, matplotlib, seaborn, numpy). While it provides MICE, `sklearn.experimental.enable_iterative_imputer` and `sklearn.impute.IterativeImputer` are considered the standard way to do MICE-like imputation within `sklearn`.

In summary, the code demonstrates an understanding of many components but often uses slightly incorrect implementations or applies methods in ways that are not statistically sound for the specific model (e.g., OLS diagnostics for logistic regression, incorrect PyMC3 prediction sampling).",gpt-4.1,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objs as go
from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.manifold import TSNE
import umap
from sklearn.cluster import KMeans, Agglom...",7,"The generated code provides a good attempt at addressing most of the requirements but falls short in a few critical areas, particularly regarding the robustness and depth of some analyses.

**Strengths:**
- **Comprehensive Imports:** All necessary libraries are imported.
- **Good Structure:** The code is logically organized into sections covering different parts of the task.
- **Feature Engineering Attempt:** It attempts to create WQI, chemical ratios, and contamination risk scores, which is a key requirement.
- **Multiple Clustering Algorithms:** Implements K-means, Hierarchical, DBSCAN, GMM, and Spectral Clustering.
- **Multiple Validation Techniques:** Uses Elbow, Silhouette, Davies-Bouldin, Calinski-Harabasz, and attempts Gap Statistic.
- **Dimensionality Reduction:** Includes PCA, t-SNE, UMAP, and Factor Analysis for visualization.
- **Visualization Dashboard:** Generates 2D/3D plots, radar charts, and interactive Plotly plots, largely fulfilling this requirement.
- **Cluster Profiling:** Calculates and displays mean values for clusters.
- **Anomaly Detection:** Implements a cluster-based anomaly detection using distance from centroids.

**Areas for Improvement:**

1.  **Feature Engineering Quality:**
    *   **WQI:** The WQI calculation is a very simplified, linear aggregation. While the prompt asks for *advanced* features, this WQI is quite basic and doesn't reflect the complexity of real-world water quality indices (which often involve non-linear transformations, sub-indices, and specific weighting based on health/aesthetic impacts). The ground truth's approach using scores for pH deviation and hardness provides a slightly more nuanced WQI.
    *   **Contamination Risk Score:** The `zscore` before `MinMaxScaler` on contamination features is okay, but `fillna(0)` is arbitrary for missing values. It's better to impute all missing values consistently at one stage (which was done for base features, but not explicitly handled for engineered ones if they arise from `df` directly).
    *   **Temporal Patterns:** Simulating temporal patterns with random integers when the dataset is static is not ideal. The ground truth correctly identifies that temporal patterns are ""Not applicable for this static dataset"" and explains what *would* be done if temporal data existed. Randomly adding `month`, `dayofweek`, and `hour` features that have no real relationship to the water quality data can introduce noise and mislead the clustering algorithms.

2.  **Optimal K Determination:**
    *   The `optimal_k = sil.index(max(sil)) + 2` selects the `k` with the absolute highest silhouette score. While valid, it often leads to `k=2`. The ground truth explicitly states that the choice of `k` is a data scientist's decision based on visual inspection of *multiple* plots and provides a reasonable default (e.g., 3 or 4) for further analysis, acknowledging real-world complexity. Relying solely on the highest silhouette can sometimes miss more nuanced cluster structures.

3.  **DBSCAN Parameter Tuning:**
    *   `eps = np.percentile(distances, 90)` is a heuristic. While better than a random guess, it might not be the optimal `eps` found by visually inspecting the k-distance graph's ""elbow"" as often recommended. The ground truth explicitly plots the k-distance graph for visual determination.

4.  **Cluster Stability Analysis:**
    *   The generated code's `consensus_clustering` function is flawed. It computes a co-occurrence matrix but then directly uses `linkage` on `1 - consensus_mat`. This is correctly done, however, the prompt asks for *bootstrap resampling AND consensus clustering* as distinct but related elements of stability. The generated `consensus_clustering` integrates resampling, but doesn't explicitly describe the *stability* of clusters beyond showing a heatmap. The ground truth demonstrates how this leads to a dendrogram to visually assess stability.
    *   The ground truth also mentions ""bootstrap resampling"" as a separate point first, then ""consensus clustering"" illustrating how the matrix is built, which aligns more closely with the specific requirements of stability analysis.

5.  **External Validation and Domain Knowledge:**
    *   The generated code skips the ""external validation"" part of the prompt. The ground truth discusses interpreting cluster profiles against water quality standards and provides example WHO guidelines, directly addressing this requirement. The generated code also does not perform ""validation using external criteria"" from the prompt. While `Potability` comparison is shown, it's under '10. VALIDATE CLUSTERING RESULTS', not integrated into the main validation step like the ground truth does by feeding `y` into `df_clustered`.

6.  **Code Readability and Detail:**
    *   While mostly well-structured, some comments could be more explanatory for complex steps. For example, the `gap_statistic` function is included but not fully explained or integrated into interpretation as prominently.

7.  **Minor Issues:**
    *   `plotly.graph_objs as go` is sometimes deprecated in favor of `plotly.graph_objects`.
    *   The 3D plot `Axes3D` import is legacy; `plotly.express` handles 3D much more seamlessly and interactively. (The generated code uses `matplotlib` for 3D plots, which is less interactive than Plotly).
    *   The `plot_3D_scatter` function uses `plt.show()` twice in a loop for 2D plots and once for 3D, leading to many separate windows.

In summary, the code covers many aspects but lacks the advanced depth and robustness expected for some of the more complex requirements like feature engineering nuance, full stability analysis (dendrogram from consensus matrix), and explicit external validation & domain knowledge interpretation.",gpt-4.1,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Compute correlation matrix
corr_matrix = df.corr(method='pearson')

# Correlation with 'quality', remove self-correlation
quality_corr = corr_matrix['quality'].drop('quality')

# Sort correlations
sorted_corr = quality_corr.sort_values(ascending=False)

# Top 3 positively correlated ...",9,"The generated code successfully addresses almost all requirements of the task.

**Strengths:**
- **Correctness:** It correctly calculates correlations, identifies top positive and negative features, generates a correlation heatmap, and creates scatter plots with regression lines.
- **Completeness:** All the main requirements (correlation calculation, top N features, heatmap, scatter plots with regression) are covered.
- **Code Quality:** The code is well-structured, uses appropriate libraries (pandas, numpy, matplotlib, seaborn, sklearn for regression), and is readable. Using `sns.regplot` simplifies the scatter plot with regression line fitting significantly compared to manually doing `linregress` and plotting. `plt.tight_layout()` is used for better plot presentation.

**Areas for Improvement (Why not a 10):**
- **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` when loading the CSV, which the generated code lacks. This is a good practice for general-purpose scripts.
- **Missing R-squared value on plots:** While `sns.regplot` does the regression, it doesn't automatically display the R-squared value on the plot or title. The ground truth calculates and displays this, which is a useful metric for judging the strength of the linear relationship on the scatter plots.
- **Minor Title Detail:** The titles for the scatter plots only include the correlation coefficient, not explicitly stating that it's R-squared. While the correlation coefficient itself is a good indicator, showing R-squared is a common practice for linear regression plots. The prompt specifically asked for ""linear regression lines"" and typically R-squared is associated with that.
- **Redundant `square=True` in heatmap:** While `square=True` is fine, `annot=True` often makes square cells less readable due to text overlap if `annot` is set to `True` and there are many features. In this case, it's okay because the dataset isn't too large, but for larger datasets, it can be problematic. The ground truth does not include `square=True` which is a minor aesthetic difference.",gpt-4.1,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import chi2_contingency, mannwhitneyu

# 1. Load the dataset and clean any missing values
df = pd.read_csv(""video_games_sales.csv"")
df = df.dropna()

alpha = 0.05

print(""---- Hypothesis Test 1: Nintendo vs Other Publishers (t-test) ----"")
# 2. Test whether Nintendo games have significantly higher global sales than others
nintendo_sales = df[df['Publisher'] == 'Nintendo']['Global_Sales']
other_sales = df[df...",6,"The generated code addresses most of the requirements but has several critical flaws that impact its correctness and robustness.

**Strengths:**
*   Correctly identifies and attempts to perform all required statistical tests (t-test, chi-square, Mann-Whitney U).
*   Uses `equal_var=False` for t-tests, which is generally good practice (Welch's t-test).
*   Correctly filters for top 5 genres/platforms for the chi-square test.
*   The interpretations are aligned with the p-value comparison to alpha.

**Weaknesses and Areas for Improvement:**

1.  **Data Cleaning (`df.dropna()`):** This is the **most significant issue**. `df.dropna()` without any arguments drops rows with *any* missing value in *any* column. This is overly aggressive and likely removes a large portion of the dataset, especially `User_Score` or `Critic_Score` which often have NaNs but are not relevant to the specified hypothesis tests. The ground truth specifically drops NaNs in relevant columns (`Global_Sales`, `Year_of_Release`, `Publisher`, `Genre`, `Platform`) ensuring only necessary data is removed. The generated code also fails to cast `Year_of_Release` to integer, which might cause issues if it's float after some NaNs are removed (though `dropna()` might prevent this specific issue if the column is entirely numeric by then).

2.  **Hypothesis 1 (Nintendo vs. Others):** The problem statement specifically asks to ""Test whether Nintendo games have significantly *higher* global sales"". This implies a one-sided (greater than) alternative hypothesis. The generated code uses the default `two-sided` alternative for `ttest_ind`, which is incorrect for this specific directional hypothesis. The interpretation also just says ""different from"" instead of ""higher than"".

3.  **Robustness (Empty Data/Insufficient Samples):** The generated code lacks checks for sufficient data in groups *before* performing statistical tests (e.g., `len(nintendo_sales) < 2`). If a group is empty or has only one sample after filtering/cleaning, `ttest_ind` or `mannwhitneyu` will raise an error. The ground truth includes these crucial checks.

4.  **Error Handling (File Not Found):** The generated code does not include any error handling for `FileNotFoundError` when reading the CSV, which is a good practice for robustness.

5.  **Interpretation Clarity:** While the interpretations are mostly correct (e.g., ""significant difference""), they could be more verbose about the null/alternative hypotheses, similar to the ground truth, for better understanding.

6.  **Year_of_Release type for Mann-Whitney U:** While the prompt asks for `Year_of_Release` cleaning, the generated code doesn't explicitly convert it to int. If the cleaning `df.dropna()` leaves it as float, the comparison `df['Year_of_Release'] < 2000` might work, but it's not robust. The ground truth ensures it's an integer.

To improve, the generated code should:
*   Refine data cleaning to target specific columns relevant to the tests.
*   Implement checks for sufficient data points before performing each statistical test.
*   Use the correct `alternative` argument for `ttest_ind` when a directional hypothesis is stated.
*   Add a `try-except` block for file loading.",gpt-4.1,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load dataset and handle missing values
df = pd.read_csv('video_games_sales.csv')

# Drop rows where essential colu...",9,"The generated code is very good and adheres to almost all requirements of the task.

**Strong Points:**
- **Correctness:** The code correctly implements all specified steps: loading data, handling missing values, creating the target variable, encoding features, creating new features, splitting data, training a Random Forest classifier, and evaluating the model with the requested metrics.
- **Completeness:** All evaluation metrics (accuracy, precision, recall, F1-score, confusion matrix) are included. Feature importance is calculated and displayed.
- **Code Quality:** The code is well-structured, uses appropriate imports, and is readable.
- **Missing Value Handling:** The approach for dropping essential columns and filling 'Name' is reasonable.
- **Feature Engineering:** Both `Total_Regional_Sales` and `Years_Since_Release` are correctly calculated.

**Minor Improvements/Observations leading to a score of 9 instead of 10:**

1.  **Missing Value Handling (Year Type Conversion):** The ground truth explicitly shows `df['Year'] = df['Year'].astype(int)` after dropping NaNs for Year. While the generated code drops NaNs in `Year` as part of `essential_cols`, it doesn't explicitly convert `Year` to `int` type. It might implicitly handle it if all years are clean numbers after dropping, but an explicit conversion is safer and more robust, especially before calculating `Years_Since_Release`.
2.  **Redundant Imports/Outputs:** The generated code imports `classification_report`, `matplotlib.pyplot`, and `seaborn` which are not strictly required by the prompt's explicit requirements (only evaluation metrics and feature importance display, not necessarily plotting). While plotting feature importance is a good visualization, it wasn't explicitly asked for in the prompt, which just requested ""Display feature importance"". The `classification_report` also prints redundant information to the individual metrics requested. For a benchmark ""only provide the Python code without explanations"", minimizing external dependencies or outputs beyond the strict requirements is sometimes preferred, but this is a minor point.
3.  **Encoders Naming:** The generated code uses separate `LabelEncoder` instances (`le_platform`, `le_genre`, `le_publisher`) and creates new columns (`Platform_enc`, `Genre_enc`, `Publisher_enc`). The ground truth uses a single loop and overwrites the original columns, which is slightly more concise and memory-efficient for this task. Both approaches are functionally correct.

Overall, it's a highly competent solution that fulfills almost all aspects of the task effectively. The minor points are mostly about refinement and strict adherence to the prompt's letter, rather than functional correctness.",gpt-4.1,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures, LabelEncoder, StandardScaler

# 1. Load dataset and handle missing values
df = pd.read_csv('video_games_sales.csv')

# Handle missing values
for col in ['Publisher', 'Genre', 'Platform']:
    df[col].fillna('Unknown', inplace=True)
for col in ['Year']:
    df[col].fillna(df[col].median(), inplace=True)
#...",7,"The generated code largely addresses the requirements of the task. It loads data, handles missing values, creates all specified engineered features, performs correlation analysis, applies PCA, creates polynomial features, and attempts outlier handling.

However, there are several areas for improvement and minor issues:

**Positive Aspects:**
*   **Comprehensive Feature Engineering:** All 7 required engineered features are created.
*   **Correct PCA Implementation:** PCA is applied correctly with scaling, and explained variance is visualized.
*   **Polynomial Features:** Polynomial features are generated for sales columns as requested.
*   **Correlation Analysis:** A heatmap is generated for numerical features.
*   **Outlier Handling:** The IQR method is correctly implemented for detection.

**Areas for Improvement/Minor Issues:**
*   **Missing Value Handling (Year):** The Ground Truth (GT) converts 'Year' to `int` after median imputation, which is good practice. The generated code keeps it as `float`.
*   **Sales Ratio & NaN values:** The generated code uses `np.where` for `Sales_Ratio_NA_EU` which is good to avoid division by zero. However, it assigns `np.nan` where EU\_Sales is 0. The GT uses `1e-6` which is a more robust way to handle this without introducing `NaN`s, especially since subsequent steps might not handle `NaN`s in this column well. In the PCA step, `fillna(0)` is used for `Sales_Ratio_NA_EU` which might not be the best strategy for that specific feature (0 ratio means no sales in NA when EU sales are not 0).
*   **Publisher Avg Sales Data Type:** The GT uses `transform('mean')` which ensures the `Publisher_Avg_Sales` column is aligned with the original dataframe's index. The generated code uses `map(publisher_avg)` which is also correct for this purpose.
*   **Genre Market Share:** The GT uses `transform('sum')` which is idiomatic for broadcasting aggregated values. The generated code uses `map` which also works.
*   **Platform Popularity:** The GT uses `transform('count')` which specifically counts items per group. The generated code uses `value_counts()` and `map()`. Both work, but `transform` can sometimes be more efficient for group-wise operations that need to be broadcast back.
*   **Interaction Features (Year & Genre):** The GT creates multiple interaction features (`Year_x_Genre_Action`, `Year_x_Genre_Sports`, etc.) by multiplying 'Year' with one-hot encoded genre columns. The generated code creates a single `Year_Genre_Interaction` feature by multiplying 'Year' with a single `LabelEncoder` output for 'Genre'. This is a conceptual difference. The GT's approach creates more granular and typically more useful interaction terms for categorical features. The generated code's approach effectively treats 'Genre' as an ordinal variable which might not be intended.
*   **Outlier Handling (Application):** The request specifically asks to ""handle outliers,"" and the GT caps them. The generated code *identifies* them using a mask but then chooses to *remove* the rows containing outliers (`df_no_outliers`). While removal is a valid strategy, capping is often preferred to retain data, especially if outliers are not extreme errors. The prompt didn't specify the handling method, but capping is a common and often better approach when ""handling"" is broadly requested. The generated code also re-defines `outlier_mask` for each column, which could be refactored for clarity and efficiency (though functional).
*   **Clarity and Intermediate Output:** The GT provides print statements and `df.head()` at each major step, which greatly helps in understanding the data's state and verifying the transformations. The generated code lacks these intermediate checks, making it harder to follow the data transformation process without manual inspection.
*   **Correlation Heatmap for Polynomial Features:** The task did not specifically ask for a correlation heatmap of polynomial features, though it's not incorrect to include it. The primary ask was to *analyze their impact*, which is left implicit.

Overall, the code is functional and covers most requirements, but some choices in implementation (especially for interaction features and outlier handling) and lack of intermediate output make it slightly less robust and clear than the ground truth.",gpt-4.1,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, mannwhitneyu, chi2_contingency, pearsonr, f_oneway

# 1) Load and clean dataset
df = pd.read_csv('diabetes.csv')

# Replace zero values with NaN for these columns (excluding 'Outcome' and 'Pregnancies')
cols_with_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[cols_with_zeros] = df[cols_with_zeros].replace(0, np.nan)

# Median imputation
for col in cols_with_zeros:
    median = df[col]....",7,"The generated code correctly implements all required statistical tests and data cleaning steps. The structure is logical, and imports are proper.

However, there are several areas for improvement to reach a higher score:

1.  **Robustness in Data Loading:** The ground truth includes a `try-except` block for file loading, which is a good practice for robustness. The generated code lacks this.
2.  **Detailed Interpretation:** While the generated code provides a basic ""Significant difference"" or ""No significant difference"" message, the ground truth offers much more detailed and nuanced interpretations. For example:
    *   For the t-test, it specifically states ""Diabetics have significantly higher glucose levels,"" not just ""Diabetics have different glucose levels.""
    *   For correlation, it states the direction (positive/negative).
    *   It generally provides more context on what the rejection or failure to reject the null hypothesis implies in the context of the specific variables being tested.
3.  **Median Imputation Check:** The ground truth code explicitly prints the median value used for imputation and confirms if `df[col].isnull().any()` before imputing. While the generated code performs the imputation, this level of detail in output and conditional imputation is slightly better.
4.  **BMI Category Binning:** The `pd.cut` function allows specifying `right=False` or `right=True` for binning. The ground truth explicitly uses `right=False` for BMI categories `[a, b)` which is more standard for ranges like `18.5-24.9`. The generated code uses a custom function which manually categorizes, making it less concise and potentially less efficient than `pd.cut` for this task. The manual function also correctly implements the ranges but `pd.cut` is often preferred for binning.
5.  **Handling Empty ANOVA Groups:** The ground truth includes a check `if len(glucose_groups) < 2:` before performing ANOVA to prevent errors if some categories happen to be empty, making it more robust. The generated code directly unpacks `*groups` without this check, which could lead to an error if a category is missing.
6.  **Minor Output Details:** The ground truth includes more descriptive print statements for means/medians, t-statistics, U-statistics, F-statistics, and correlation coefficients, which are valuable contextual details for interpreting the p-value. The generated code primarily focuses on just the p-value result.",gpt-4.1,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
# Imports
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, precision_...",7,"The generated code addresses most of the task requirements but has several areas for improvement in terms of robustness, best practices, and completeness according to the prompt's specific conditions.

**Strong Points:**
*   Successfully loads and preprocesses data (handling zeros with median).
*   Correctly implements the creation of 'Glucose\_BMI\_ratio' and 'Age\_group_bins'.
*   Correctly implements the creation of 'Insulin\_level' based on tertiles.
*   Correctly encodes categorical variables using `pd.get_dummies`.
*   Splits data into training and testing sets with stratification.
*   Performs feature scaling.
*   Trains and evaluates multiple classification models.
*   Performs cross-validation for model evaluation.
*   Includes hyperparameter tuning using `GridSearchCV`.
*   Calculates and displays common evaluation metrics (accuracy, precision, recall, F1, ROC-AUC).
*   Generates confusion matrices and ROC curves.
*   Displays feature importance for tree-based models.

**Areas for Improvement / Missing Aspects:**
1.  **Handling of Missing Values (Zeros):** The ground truth correctly identifies `0` in `SkinThickness` and `Insulin` as missing values based on problem context. The generated code uses `data.loc[data[col] != 0, col].median()` which is a good approach to calculate the median *excluding* the zeros themselves. However, the ground truth's approach of `replace(0, np.nan)` first and then `fillna(median_val)` is more standard and explicit for handling ""missing as zero"" cases. The output `print` statements to confirm processing steps are also missing compared to the ground truth.
2.  **Age Group Definition:** The generated code uses `[20, 30, 40, 50, 60, 100]` for `age_bins` and `right=False, include_lowest=True`. This means the first bin is `[20, 30)`, which might exclude ages below 20 (e.g., 0-19) if present, and `include_lowest=True` might be slightly redundant given `right=False`. The ground truth includes `0` in its bins `[0, 20, 30, 40, 50, 60, np.inf]`, making it more comprehensive for potential real-world age ranges.
3.  **Categorical Encoding Pipeline:** The generated code directly uses `pd.get_dummies` on the full dataframe `data` *before* the train-test split. This is generally a bad practice as it can lead to data leakage from the test set into the training set (e.g., if a new category only appears in the test set, or if dummy variables are created based on test set data distribution). The ground truth correctly uses `ColumnTransformer` with `OneHotEncoder` *within* a pipeline approach or after splitting, which is the robust way to handle encoding and scaling consistently across train and test sets without leakage, and to handle unseen categories gracefully (`handle_unknown='ignore'`).
4.  **Scaling and Model Training Logic:** The generated code conditionally scales `X_train` and `X_test` only for `Logistic Regression` and `SVM` and then trains/evaluates tree-based models on unscaled data. While tree-based models are less sensitive to scaling, it's a best practice to apply consistent preprocessing to all models or at least ensure `cross_val_score` and `GridSearchCV` use the *same* scaled data if the model requires it. The ground truth correctly applies `StandardScaler` to all numerical features via `ColumnTransformer` before passing to any model, simplifying the training loop and ensuring consistency.
5.  **Hyperparameter Tuning Strategy:** The code tunes only the `best_model_name` based on initial CV results (e.g., if GB is best, it tunes GB; if RF is best, it tunes RF). The prompt explicitly asks to ""Perform hyperparameter tuning for *the best model* using GridSearchCV."" This implies choosing the best performer *before* tuning and then tuning *that specific model* (which the generated code tries to do). However, the ground truth's approach of deciding on a strong candidate (e.g., Random Forest) based on performance in general and tuning that model is also a valid interpretation, and provides a 'Tuned' version for comparison. The generated code also includes conditional tuning for RF within an `else` block, which is somewhat awkward. A more structured approach would be to pick the one with highest CV score and then apply tuning.
6.  **Visualization Labels and Titles:** The confusion matrix and ROC curve plots in the generated code are functional but lack specific titles for axes (`xlabel`, `ylabel` for confusion matrix are general, not indicating 'Predicted/True Label'). The ground truth plots are more aesthetically pleasing and informative with clear axis labels and more descriptive overall titles.
7.  **`classification_report`:** The prompt asks to ""Evaluate all models using ... F1-score, and ROC-AUC"". While the code does print individual scores, the ground truth also includes printing `classification_report` which provides a comprehensive summary of precision, recall, f1-score, and support for each class, which is very useful for model evaluation. This is missing in the generated code's final evaluation output.
8.  **Reproducibility (Random State):** While `random_state=42` is used in many places, a global `np.random.seed(42)` at the beginning is good practice for entire script reproducibility, as done in the ground truth.
9.  **Clarity and Structure:** The ground truth organizes the code into distinct numbered sections (1. Load, 2. Preprocess, etc.) as implied by the prompt, which enhances readability and maintainability. The generated code also has comments but lacks the clear sectioning.",gpt-4.1,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load and preproc...",8,"The generated code addresses most of the requirements of the task.

**Strong points:**
*   **Comprehensive Feature Engineering:** Successfully creates all specified engineered features like 'Metabolic_Score', 'Risk_Factor_Count', 'Glucose_per_Age', 'BMI_BloodPressure_interaction', and 'Pedigree_Age_product'.
*   **Polynomial Features:** Correctly applies polynomial features of degree 2 and handles column concatenation appropriately.
*   **Missing Value Imputation:** Correctly identifies and imputes missing values (0s) using the median strategy.
*   **Scaling:** Implements both StandardScaler and MinMaxScaler.
*   **PCA with Visualization:** Performs PCA and provides explained variance ratios, and a helpful plot for visualization.
*   **Feature Selection Methods:** Correctly uses SelectKBest (though a fixed k is used, which is acceptable) and RFE.
*   **Correlation Filtering:** Effectively implements correlation-based feature filtering.
*   **SMOTE:** Applies SMOTE for handling class imbalance, performing it on the training data after the train-test split, which is good practice.
*   **Feature Importance:** Calculates and visualizes feature importance using RandomForest.

**Areas for improvement:**
*   **Order of Operations for PCA/Feature Selection:** The ground truth code performs correlation filtering *before* PCA and the other feature selection methods (SelectKBest, RFE). The generated code applies PCA/SelectKBest/RFE on `X` (which is `df_std`), and then applies correlation filtering to create `X_corr_filtered`. A more logical flow generally involves filtering out highly correlated features *before* applying techniques like PCA or other feature selection methods, as highly correlated features can unduly influence these processes. The ground truth approaches this better by applying correlation filtering first.
*   **PCA n_components:** The task implicitly asks to ""explain variance ratios"", and the ground truth uses `n_components=0.95` which directly relates to explained variance. The generated code uses `n_components=10`. While valid, it doesn't directly address the variance explanation as comprehensively as the ground truth's approach.
*   **Polynomial Feature Application:** The ground truth applies polynomial features to a selected subset of core numerical features, recognizing that applying them to *all* numeric columns (including newly engineered complex ones) can lead to an explosion of features and potential redundancy/noise. The generated code applies polynomial features to `num_cols` which includes several of the original and engineered features, which is broader than necessary and potentially less optimal depending on the dataset.
*   **Consistency in Scaled Data Usage:** The code scales the data (creating `df_std` and `df_minmax`), but then primarily uses `df_std` for subsequent steps. The variable `X` is derived from `df_std`. It's good to be clear about which scaled version is being passed through the pipeline. The ground truth simplifies this by explicitly naming `X_processed` for subsequent steps.
*   **Import Optimization:** While not critical, importing `train_test_split` earlier in the generated code is good practice as it's typically used early in ML pipelines.

Overall, it's a very strong attempt that correctly implements almost all components requested, with minor differences in the exact order of operations and the scope of polynomial features.",gpt-4.1,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import mannwhitneyu, pearsonr, spearmanr, chi2_contingency, f_oneway, ttest_ind

# 1. Load dataset and handle missing values
df = pd.read_csv('world-happiness-report-2021.csv')
df = df.dropna()

# Standardize column names (remove spaces if any)
df.columns = [c.strip().replace(' ', '_') for c in df.columns]

# Set significance level
alpha = 0.05

# Result Printer
def print_result(test_desc, p_val, alpha=0.05...",7,"The generated code correctly implements most of the requested statistical tests and handles data loading. It also includes a good utility function for printing results.

However, there are several areas for improvement:

1.  **Data Handling for Missing Values:** The `df.dropna()` approach used by the generated code drops *any* row with a NaN, which might be overly aggressive if some columns not critical for the specific tests have missing values. The ground truth code correctly identifies `critical_columns` and drops NaNs only from those, leading to more robust data handling. This can also lead to a different number of rows for analysis, impacting the results.
2.  **Missing Interpretation of Results:** The `print_result` function is good for stating statistical significance but lacks the crucial interpretation step that the ground truth code provides (e.g., ""Western European countries have significantly HIGHER happiness scores""). This is a explicit requirement in the prompt (""Display p-values and interpret results with significance level =0.05""). For example, for the t-test, it only says ""Statistically significant difference"" but not if it's higher or lower. For correlation, it correctly states ""significant correlation"" but doesn't mention if it's positive or negative. For ANOVA, it correctly concludes ""significant difference"" but doesn't add the crucial nuance that post-hoc tests are needed (which the GT does).
3.  **Chi-square Categorization:** The categorization for the chi-square test `q1 = df['Ladder_score'].quantile(0.25)` and `q3 = df['Ladder_score'].quantile(0.75)` for 'Low', 'Medium', 'High' creates groups with roughly 25% low, 50% medium, and 25% high. The ground truth uses `1/3` and `2/3` quantiles to create three roughly *equal-sized* groups, which is generally a better practice for categorical analysis if the intent isn't specifically 25/75 splits. While not strictly wrong, it's less standard for 'Low/Medium/High' categorization.
4.  **Column Name Standardization:** While `df.columns = [c.strip().replace(' ', '_') for c in df.columns]` is a good practice, it's not explicitly required by the prompt and adds an extra layer of transformation that might not be necessary if the column names were already clean or if the exact names were used. The ground truth simply uses the names as they are in the CSV.
5.  **Small Logical Discrepancy (Freedom Generosity Test):** In the freedom/generosity test, the generated code uses `df[df['Freedom_to_make_life_choices'] > median_freedom]` for ""above median"" and `df[df['Freedom_to_make_life_choices'] <= median_freedom]` for ""below median"". The ground truth uses `>=` for one group and `<` for the other. This matters for countries exactly at the median, impacting which group they fall into slightly. The GT's approach is more standard for binary splits around a median.
6.  **Readability/Clarity:** The ground truth code uses extensive print statements that clearly delineate each test, its purpose, the results, and the interpretation, making it much easier to follow and understand the output. The generated code's `print_result` is concise but sacrifices detailed explanation.",gpt-4.1,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
# 1. Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# 2. Load dataset
df = pd.rea...",7,"The generated code provides a good attempt at the task and covers most requirements.

**Strong points:**
*   Correctly loads data, handles missing values, and identifies target/features.
*   Implements train-test split.
*   Trains and compares multiple regression models.
*   Performs hyperparameter tuning using `GridSearchCV`.
*   Evaluates models with MAE, MSE, RMSE, and R.
*   Generates residual plots and feature importance visualizations.
*   Performs cross-validation.
*   Makes predictions for hypothetical countries.
*   Good use of sklearn modules.

**Areas for improvement:**
*   **Preprocessing consistency:** The generated code scales `X` *before* the train-test split (`X_scaled = scaler.fit_transform(X)`). This is a data leakage issue, as the scaler learns from the entire dataset, including the test set. Scaling should be done *after* the split (`scaler.fit_transform(X_train)` and `scaler.transform(X_test)`). The ground truth code avoids this by not scaling explicitly or by using `ColumnTransformer` with pipelines.
*   **Hyperparameter tuning logic:** The `if/elif/else` block for selecting the best model to tune is problematic. The prompt asks to ""Perform hyperparameter tuning for the best model using GridSearchCV"". The generated code attempts to identify the best model and then tune it. However, the logic for choosing the `estimator` and `grid` is manual and specific to the model types. The ground truth code simplifies this by selecting a generally strong candidate (GBR) for tuning, or by picking the dynamically best model if needed (though it doesn't explicitly do the latter, it sets up for it). More robustly, a pipeline that includes scaling should be used for each model, and then GridSearchCV can be applied to the pipeline.
*   **Missing categorical encoding:** The prompt explicitly mentions ""encoding categorical variables"". While the selected features are numerical, the original dataset does contain 'Regional indicator' and 'Country name'. The ground truth code correctly notes that these are not *used* as features, thus no encoding is applied *to the features used in the model*. The generated code simply states ""No categorical features among selected features"", which is true for *selected* features, but doesn't acknowledge the broader preprocessing step the prompt requested. This is a minor point, as the model explicitly does not use them, but it's worth noting.
*   **Hypothetical country prediction:** The prompt asks to ""predict Happiness for hypothetical countries"". The generated code provides only two hypothetical countries and only prints the numerical prediction. The ground truth provides more varied hypothetical scenarios and a brief analysis, which is more aligned with ""analyze results"".
*   **Redundant residual plots:** The generated code first generates residual plots for *all* models and then a separate residual plot for the ""Best Tuned Model"". While not strictly incorrect, it adds visual clutter. A single residual plot for the ""best"" relevant model (e.g., the tuned one) would suffice.
*   **`random_state` for `Ridge` and `Lasso`:** Ridge and Lasso do not have a `random_state` parameter, so including it (as in the ground truth) will raise an error. The generated code correctly omits it, which is good. The ground truth code has a minor bug here.
*   **Clarity in tuning:** The generated code attempts to pick the *best initial model* and then tune it. While this might seem intuitive, it can be complex. The ground truth simplifies by picking Gradient Boosting Regressor (a strong candidate) directly for tuning, or implies that the initial best among the set could be chosen.

Overall, the code is functional and addresses most parts of the task, but the data leakage during scaling and the somewhat overly complex/manual approach to hyperparameter tuning for ""the best model"" are the main areas that prevent a higher score.",gpt-4.1,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, RFE
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load and preprocess the ...",6,"The generated code addresses most of the requirements but has several issues in correctness, completeness, and structure that prevent it from being a strong solution.

**Correctness Issues:**
1.  **Missing Value Handling:** The generated code uses `df.fillna(df.median(numeric_only=True))`. While this is a plausible imputation strategy, the ground truth specifically targets `Perceptions of corruption` and then drops any `NaN`s in the *selected core columns*. The generated code's approach is more general but might mask issues if NaNs appear in non-numeric or critical identifier columns. It also doesn't explicitly drop rows with NaNs after initial fill, which could lead to issues with engineered features that rely on complete data. The ground truth's specific approach for `Perceptions of corruption` is more targeted and robust given the dataset.
2.  **`Governance_Score` Calculation:** The ground truth calculates `Governance_Score` as `Freedom_to_make_life_choices + (1 - Perceptions_of_corruption)`. The generated code divides this by 2, which was not specified and changes the interpretation of the score.
3.  **Renaming Columns:** The generated code renames columns extensively at the beginning (`df.rename(columns=lambda x: x.strip().replace(' ', '_').replace('(', '').replace(')', ''))`). While good practice for consistency, it makes the code base more brittle to the exact column names used in the prompt (e.g., `Logged GDP per capita` vs `Logged_GDP_per_capita`), requiring careful adjustments throughout. The ground truth sticks to the original names given in the prompt, making it more directly compliant.
4.  **Scaling and PCA/t-SNE Input:** The generated code performs scaling on `scale_features` and then uses only `scaled_data['StandardScaler']` as input for PCA/t-SNE. This is conceptually correct for using one scaled version, but the `scale_features` list for scaling doesn't include the *entire* set of numerical features that would be used for modeling (e.g., it initially misses the `Happiness_Efficiency` or `GDP_vs_Regional_Average` if those were created first before defining `scale_features`). The ground truth correctly builds `X` from all relevant numeric features after all engineering steps.
5.  **Polynomial Feature Application:** The polynomial features are concatenated based on `reset_index(drop=True)`. This is risky if the original DataFrame `df` has a non-default or non-contiguous index *after* preprocessing, as `reset_index` will create a new default index, which might not align correctly with `df`'s current index if rows were dropped (e.g., by `dropna`). The ground truth handles this more robustly by concatenating using `df.index` for the `poly_df`.
6.  **Feature Selection Target `X_base`:** The `X_base` for `SelectKBest` and `RFE` is defined as `df[scale_features + list(poly_df.columns)]`. This implicitly assumes that `scale_features` itself captures all relevant features *before* scaling. It also doesn't use any of the scaled data, which is typically what you'd use for feature selection in a pipeline after scaling for algorithms sensitive to scale (like Linear Regression, or even RF if feature interactions are key). The ground truth correctly performs feature selection on `X_scaled` (StandardScaler output), which is more appropriate.
7.  **RFE Estimator:** The ground truth uses `LinearRegression` for RFE, which is a common choice for its simplicity and interpretability in feature importance. The generated code uses `RandomForestRegressor`, which is also valid but might be computationally more expensive for RFE.
8.  **Interaction Features and Feature Importance:** The interaction features are created, but then when defining `all_features` for the final Random Forest importance, it combines `scale_features`, `poly_df.columns`, and `econ_social_interactions`. This list likely contains duplicate columns if `poly_df.columns` or `econ_social_interactions` contain features already listed in `scale_features` or are derivations of them. This can lead to issues or misinterpretations in feature importance. The ground truth ensures `X` is carefully constructed from all engineered and original features before defining `numerical_features` and separating `X, y`.

**Completeness Issues:**
1.  **Warning Suppression:** The ground truth includes `warnings.filterwarnings('ignore')` for cleaner output, which is a good practice in a benchmark scenario. The generated code omits this.
2.  **Error Handling for File:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, making it more robust. The generated code omits this.
3.  **Visualization of PCA/t-SNE:** The ground truth visualizes PCA and t-SNE results using `hue=y` (Ladder Score), showcasing how the target variable distributes across the reduced dimensions. The generated code uses `hue=df['Regional_indicator']`, which is a different visualization purpose (regional clustering) than intended for a feature engineering and dimensionality reduction benchmark focused on the target variable.
4.  **Mutual Information Scoring Output:** The ground truth explicitly prints a sorted list of top mutual information scores. The generated code only prints `top_mi_features` (names), not the scores themselves.
5.  **Information Messages:** The ground truth provides print statements throughout, indicating progress and summarizing results (e.g., ""StandardScaler applied,"" ""PCA explained variance ratio""). The generated code is sparse on such informative messages.

**Structure/Code Quality:**
1.  **Imports:** Mostly good, but `train_test_split` is imported but not used, as noted in the ground truth's comments.
2.  **Clarity:** The ground truth is generally clearer about which specific features are being operated on at each step (e.g., `key_happiness_factors` for polynomial features are explicitly listed). The generated code's broad `scale_features` list can be less precise.
3.  **Consistency:** The ground truth maintains consistency in feature naming, while the generated code's initial renaming and subsequent usage of both original-style and underscore-style names can be confusing.
4.  **Redundant Scaling:** The generated code creates three different scaled dataframes (`scaled_data['StandardScaler']`, `MinMaxScaler`, `RobustScaler`) and then merges them back into `df`, even though only `StandardScaler` is primarily used for downstream PCA/t-SNE/feature selection. This leads to an unnecessarily wide DataFrame and potentially redundant computations. The ground truth applies all three and selects one (`StandardScaler`) for subsequent steps without merging all back into the main DataFrame, which is cleaner.",gpt-4.1,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# 1. Load data, parse dates, filter for USA
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
usa_df = df[df['location'] == 'USA'].copy()

# 2. Set date as index
usa_df.set_index('date', inplace=True)

# 3. Resample to weekly sum
weekly_cases = usa_df['...",6,"The generated code correctly performs most of the required steps, including data loading, filtering, resampling, decomposition, ADF test, and ACF/PACF plotting. It correctly identifies the need for differencing if the initial ADF test indicates non-stationarity.

However, there are several areas for improvement:

1.  **Robustness/Error Handling:** The code lacks any error handling for file not found, empty data after filtering, or insufficient data points for time series operations (e.g., `seasonal_decompose` requires a certain number of periods, `adfuller` and `plot_acf`/`plot_pacf` require at least *some* data points). The ground truth code handles many of these edge cases gracefully, which is crucial for real-world scenarios.
2.  **Decomposition Period:** While `period=52` is a good starting point for weekly data and yearly seasonality, `seasonal_decompose` can fail or produce unreliable results if the series length is not significantly larger than the period. The ground truth code includes a check and a warning/adjustment for this.
3.  **Plotting Decomposed Components:** The generated code manually plots each component of the decomposition. While functional, `decomposition.plot()` from `statsmodels` handles this automatically and is more concise and visually consistent, which is demonstrated in the ground truth.
4.  **ADF Test Output:** The generated code only prints the ADF statistic and p-value. The critical values are important for a full interpretation of the ADF test, and generally, a more robust output is desired.
5.  **ACF/PACF Lags:** The fixed `lags=40` might not always be appropriate. It could be too many for short series or too few for very long ones. The ground truth attempts to determine a relevant `max_lags` based on the series length, which is better practice.
6.  **Readability/Comments:** While the code is relatively straightforward, adding more descriptive comments, especially around the logic for differencing, would improve readability. Encapsulating the logic within a function, as the ground truth does, makes the code more reusable and organized.
7.  **Implicit Assumptions:** The code implicitly assumes that the `synthetic_covid19_data.csv` file exists in the same directory. Adding an explicit `if __name__ == ""__main__"":` block with a function call (as in the ground truth) would be better practice for a standalone script.
8.  **Minor Plotting Issues:** The manual subplot creation in the generated code might lead to overlapping titles or labels without explicit `tight_layout` calls or more careful sizing, which can be avoided by using the built-in `decomposition.plot()`.

Overall, the code provides a functional solution but lacks the robustness, best practices, and elegant solutions provided by the `statsmodels` library itself (like `decomposition.plot()`) that are present in the ground truth.",gpt-4.1,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Load dataset
df = pd.read_csv('winequality-red.csv')

# Categorize pH groups
def categorize_ph(ph):
    if ph < 3.2:
        return 'Low pH'
    elif ph <= 3.4:
        return 'Medium pH'
    else:
        return 'High pH'

df['pH_group'] = df['pH'].apply(categorize_ph)

# Calculate average quality and sample sizes per group
group_stats = df.groupby('pH_group')['quality'].agg(['mean', 'cou...",7,"The generated code addresses most of the requirements but has a few shortcomings that prevent a higher score.

**Correctness:**
*   **pH Grouping:** The `categorize_ph` function is slightly off. `pH <= 3.4` for Medium pH means that `3.4` falls into Medium, but the ground truth specifies `(3.2 to 3.4)` implicitly meaning `3.2 <= pH < 3.4`. This is a subtle but important difference in categorization. The ground truth's use of `pd.cut` with `right=False` and `include_lowest=True` handles this precisely.
*   **Confidence Interval Calculation:** The generated code uses `1.96 * std / np.sqrt(count)` for the 95% confidence interval. This formula assumes a large sample size and uses the Z-score (1.96) for a normal distribution. While common as an approximation, for smaller sample sizes or when the population standard deviation is unknown (which is the case here), it's more statistically correct to use the t-distribution. The ground truth correctly uses `scipy.stats.t.interval` which accounts for degrees of freedom. This makes the error bars slightly less accurate, especially for groups with fewer samples.
*   **ANOVA Input:** The ANOVA input `stats.f_oneway(low, med, high)` implicitly assumes that `low`, `med`, and `high` contain data for all groups. If a group were empty, this would raise an error. The ground truth robustly handles cases where groups might be empty or have insufficient data before performing ANOVA.

**Code Quality:**
*   **Readability/Maintainability:** The hardcoded group names (`'Low pH', 'Medium pH', 'High pH'`) in `reindex` and the ANOVA preparation could be prone to errors if the categorization logic ever changed. The ground truth uses named constants for group labels, making it more robust.
*   **Magic Numbers:** The `1.96` for CI calculation is a magic number. While standard, defining it as a constant like `CONFIDENCE_LEVEL` for alpha and deriving the z-score (or using `t.interval`) is cleaner, especially if one wanted to change the confidence level.

**Completeness:**
*   **Confidence Interval Reporting:** The generated code calculates the CI but doesn't explicitly print the lower and upper bounds of the confidence intervals, only the `mean` and `ci95` (which is half the total width). The ground truth specifically prints `ci_lower` and `ci_upper`.
*   **ANOVA Conclusion:** The generated code just prints the F-statistic and p-value. The ground truth provides an interpretation of the p-value against a significance level (alpha), which is part of completing the statistical analysis.
*   **Visualization Refinements:**
    *   The `yerr` in the generated code represents `1.96 * SEM`, which calculates the distance from the mean to one bound of the CI, making the total error bar width `2 * 1.96 * SEM`. The ground truth explicitly calculates `error_bar_length = mean_quality - ci_lower`, ensuring the `yerr` parameter directly represents the distance from the mean to one end of the calculated `t`-distribution based CI, thus providing correctly sized error bars.
    *   The generated code doesn't set a `ylim` which might lead to a less informative plot if the quality range isn't fixed.
    *   Adding labels to the bars on the plot (as done in the ground truth) enhances readability.
    *   The ground truth includes better error handling for file loading.

Overall, the generated code provides a functional solution to most parts of the problem but lacks some statistical precision (t-distribution for CI), robustness (empty groups in ANOVA), and output completeness (detailed CI reporting, ANOVA interpretation, plot labels) compared to the ground truth.",gpt-4.1,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# 1. Load the dataset and parse 'date'
df = pd.r...",7,"The generated code correctly addresses most of the requirements but has a significant issue in the data preprocessing pipeline, specifically regarding the order and application of imputation, encoding, and scaling.

**Strengths:**
*   **Correct Task Interpretation:** The code correctly identifies features, target, and the types of models to be used.
*   **Model Training and Evaluation:** It correctly trains Linear Regression and Random Forest models and evaluates them using MAE, MSE, and R.
*   **Proper Imports:** All necessary libraries are imported.
*   **Clear Structure:** The code is structured logically with comments for each major step.

**Areas for Improvement (Why it's not a higher score):**
*   **Incorrect Preprocessing Order/Application:**
    *   **Imputation for Target Variable:** The prompt asks to handle missing values in ""selected features and target by imputing with the median"". The generated code applies `SimpleImputer` to `y` (the target variable) separately *before* splitting the data. While median imputation for the target is fine, applying `fit_transform` on the entire `y` *before* the train-test split can lead to some data leakage, as the median of the test set could influence the imputation. It's generally better to impute target `y` independently for train and test sets, or ensure the imputation is done on the full dataset before splitting if deemed acceptable for the target.
    *   **Imputation and Scaling for `X`:** The most critical issue is that scaling is applied **only to the original numerical features** (`X_combined[features]`) *after* imputation and concatenation, but **before** the train-test split. The one-hot encoded features are *not* scaled, which is correct, but the scaling itself is applied to the *full dataset* (`X_combined`) using `fit_transform` before the split. This introduces data leakage because the scaler learns parameters (mean, std dev) from the entire dataset, including the test set, before the data is split. **Scaling should always be done *after* the train-test split and fit *only* on the training data.**
    *   **One-Hot Encoding `handle_unknown='ignore'`:** While `handle_unknown='ignore'` is good practice, it's not explicitly asked for and could potentially hide issues if unexpected continent values appear. The ground truth avoided this by ensuring `continent` was in the initial `df_selected` and then performing `get_dummies` on it, ensuring all unique continents are handled.
    *   **Separate Imputers for Numerical and Categorical:** The code uses separate `SimpleImputer` instances for numerical and categorical columns (`X_num_imputed`, `X_cat_imputed`). While it works, the ground truth's approach of imputing numerical features and the target with median directly on `df_selected` and then using `pd.get_dummies` for categorical is simpler and often more robust without needing complex `ColumnTransformer` or manual DataFrame reconstruction, especially given the prompt's constraints.
*   **Redundant Steps/Complexity in Preprocessing:** The use of `ColumnTransformer` and `Pipeline` was not explicitly requested or necessary given the straightforward nature of the preprocessing steps defined in the prompt. Manually separating `X_num`, `X_cat`, imputing them, then encoding, and then concatenating is more complex than needed and introduces opportunities for errors (e.g., index misalignment if not careful, though `pd.concat` handles this well).
*   **Missing `FileNotFoundError` handling:** The ground truth includes robust error handling for file loading, which the generated code lacks. While not a direct functional requirement for the model, it's good practice.

To improve, the generated code should:
1.  Perform initial data selection and target identification.
2.  Handle missing values for *both features and target* on the full dataset (as median imputation is global).
3.  Perform one-hot encoding on the full dataset.
4.  Split the data into train/test sets.
5.  Initialize `StandardScaler`.
6.  `fit` the `StandardScaler` *only on the training numerical features*.
7.  `transform` both training and testing numerical features using the *fitted scaler*.
8.  Combine scaled numerical features with one-hot encoded features for both train and test sets (if not already combined by an appropriate ColumnTransformer setup).
9.  Train models.
10. Evaluate models.",gpt-4.1,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import f_oneway, pearsonr, ttest_ind
import statsmodels.api as sm

# 1. Load dataset and handle missing values
df = pd.read_csv('synthetic_covid19_data.csv')

cols_to_impute = ['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']
for col in cols_to_impute:
    median_val = df[col].median()
    df[col].fillna(median_val, inplace=True)

print(""Data loaded and missing values imputed.\n"")

...",7,"The generated code correctly implements the core statistical tests (ANOVA, Pearson, T-test) and states null hypotheses, and interprets p-values, which aligns with most requirements.

**Strengths:**
*   Correctly uses `f_oneway`, `pearsonr`, and `ttest_ind` from `scipy.stats`.
*   Properly states null hypotheses for each test.
*   Correctly interprets p-values against =0.05.
*   Handles missing values via median imputation as requested.
*   Correctly derives the `high_death_rate_country` by sorting and taking the last entry per country to get the latest data.

**Areas for Improvement (Why it lost points):**

1.  **Missing Value Imputation Robustness:** The generated code directly computes `df[col].median()` and then `fillna`. If a column is **entirely** NaN, `median()` would return NaN, leading to `fillna(NaN, inplace=True)` which does not impute anything. While the prompt assumes valid data, robust code would handle this (e.g., check `df[col].isnull().all()` or at least print warnings/errors if imputation fails). The ground truth adds checks like `if col in df.columns:` and `if df[col].isnull().any():` for more robustness and user feedback.
2.  **ANOVA - Empty Groups:** The `groups = [grp['reproduction_rate'].values for name, grp in df_anova.groupby('continent')]` line in ANOVA assumes that all specified continents (`continents_of_interest`) will have data. If one or more continents have no `reproduction_rate` data after filtering, or if `df_anova.groupby('continent')` doesn't produce groups for all expected continents, `f_oneway` could error or produce misleading results. The ground truth explicitly handles cases where groups might be empty or missing data preventing the test from running correctly (`if not rates.empty:` and `if len(reproduction_rates_by_continent) >= 2:`).
3.  **T-test - Handling Edge Cases for Groups:** Similar to ANOVA, the t-test assumes `age_high` and `age_low` will not be empty. If, for instance, all countries were classified as 'high death rate' or 'non-high death rate', or if all countries had NaNs for median age (though imputation should broadly mitigate this), `ttest_ind` could fail or produce warnings. The ground truth adds checks for `if not group_high_death.empty and not group_other_death.empty:`.
4.  **T-test - `equal_var` parameter:** While the generated code attempts to use Levene's test to determine `equal_var`, it's generally recommended for t-tests on real-world data to *always* use `equal_var=False` (Welch's t-test) as it's more robust to unequal variances and only loses a little power if variances are truly equal. Adding Levene's test is a good attempt at being thorough, but `equal_var=False` would be a safer default without the need for an additional hypothesis test. The prompt asked for ""an appropriate t-test"" which Welch's satisfies without additional complexity.
5.  **Output Clarity:** The output from the ground truth is more verbose with clear headings for each step, print statements for what is being done, and clear conclusions for each test, making it easier to follow the logic and results. The generated code is a bit more condensed.
6.  **`statsmodels` import:** `import statsmodels.api as sm` is included but not used. While not a functional error, it's unnecessary in this specific solution.
7.  **Data type for `df_latest['high_death_rate_country']`:** Converting to `.astype(int)` for a boolean column `high_death_rate_country` is not strictly necessary for filtering but doesn't cause issues. A boolean series would also work fine for pandas filtering.
8.  **Handling Division by Zero/NaN population in `death_rate_ratio`:** The ground truth explicitly replaces `inf` and `-inf` with `np.nan` and then `dropna` for the `death_rate_ratio` calculation to handle cases where `population` might be zero, which is a good practice for robustness. The generated code simply computes the division, which could result in `inf` values.",gpt-4.1,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select numerical columns
cols = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
num_df = df[cols]

# Calculate correlation matrix
corr_matrix = num_df.corr()

# Visualize correlation matrix as heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix of Selec...",9,"The generated code successfully accomplishes all the requirements of the task: loading the CSV, selecting the specified numerical columns, calculating the correlation matrix, and visualizing it as a heatmap. It includes all necessary imports and uses appropriate functions.

Strong points:
- Correctly identifies and loads the dataset.
- Accurately selects the required numerical columns.
- Correctly calculates the correlation matrix.
- Generates a heatmap with `annot=True` and `cmap='coolwarm'` as good practices for visualization.

Areas for minor improvement (preventing a perfect 10):
- Lack of error handling (e.g., `FileNotFoundError` if the CSV isn't present, or `KeyError` if columns are mistyped). While the ground truth had extensive error handling, for a typical benchmark evaluation, basic error handling is often desired for robustness.
- The ground truth encased the logic within a function, which is generally better for reusability and modularity. However, for a simple script, top-level execution is acceptable.
- The ground truth provided slightly more descriptive print statements, which aid in debugging and understanding the code's execution flow.

Overall, it's a very good solution that directly addresses the problem statement effectively.",gpt-4.1,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Group by rarity and calculate average values
grouped = df.groupby('rarity').agg({
    'elixirCost': 'mean',
    'Win Rate': 'mean',
    'Usage': 'mean'
})

# Display the calculated averages for each rarity
print(grouped)
```",10,"The generated code is functionally identical to the ground truth code and achieves the task requirements perfectly. It correctly loads the dataset, groups by 'rarity', and calculates the average 'elixirCost', 'Win Rate', and 'Usage' for each rarity. The use of `.agg()` is an equally valid and often more explicit way to perform multiple aggregations than simply calling `.mean()` on selected columns after grouping, making it a strong and clear solution. All imports are correct, and the code structure is clean and readable.",gpt-4.1,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split

# 1. Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 2. Create binary target column
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)

# 3. Select features
features = df[['elixirCost', 'Usage', 'rarity']]

# 4. One-hot encode 'rarity'
features_encoded = pd.get_dummies(features, columns=['rarity'])

# 5. Split data
X_train, X_test, y_train, y_test = train_test_split(
    features_encoded,
 ...",9,"The generated code is almost entirely correct and directly addresses all parts of the task. It successfully loads the data, creates the `is_high_win_rate` target column, selects the specified features, one-hot encodes the `rarity` column, and splits the data into training and testing sets with the correct parameters. The use of `pd.get_dummies` is appropriate and efficient for one-hot encoding.

The only minor stylistic/best practice point that could be improved upon is in step 3 where `features = df[['elixirCost', 'Usage', 'rarity']]` is directly assigned. While functionally correct, it's often better to explicitly separate `X` (features DataFrame) and `y` (target Series) for clarity, especially when the target column is being derived from the original DataFrame but then used as a separate entity in `train_test_split`. The ground truth does this by creating `X = df[features].copy()` and `y = df[target]`. The generated code implicitly handles this in the `train_test_split` call by passing `features_encoded` as `X` and `df['is_high_win_rate']` as `y`. This is not incorrect but less explicit in defining `X` and `y` variables before the split.

Overall, it's a very good solution and meets the requirements effectively.",gpt-4.1,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 1. Load the dataset
df = pd.read_csv('iris.csv')

# 2. Define features X
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# 3. Define the target y
y = df['Species']

# 4. Encode the categorical target variable
le = LabelEncoder()
y_encoded = le.f...",9,"The generated code correctly addresses almost all requirements of the task. It loads the data, defines features and target, encodes the target, splits the data with the correct test size and random state, trains a KNN classifier with the specified `n_neighbors`, makes predictions, and reports the accuracy.

One minor point of improvement is the lack of `stratify=y_encoded` in the `train_test_split` function. While not explicitly asked for in the prompt, it's a best practice for classification tasks with potentially imbalanced classes to ensure that the class distribution in the training and testing sets is similar to the original dataset. The ground truth includes this, which makes it slightly more robust.

Another minor point is the absence of error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth. This makes the ground truth more robust to real-world scenarios.

Finally, the accuracy printing format in the ground truth `print(f""K-Nearest Neighbors Classifier Accuracy: {accuracy:.4f}"")` is more descriptive than `print(f""Accuracy: {accuracy:.2f}"")` in the generated code, and the use of 4 decimal places given the nature of accuracy scores is generally preferred for consistency. These are subtle differences that do not hinder the core functionality but contribute to better code practices.",gpt-4.1,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# 1. Load the dataset
df = pd.read_csv('iris.csv')

# 2. Filter the data for two groups
group_versicolor = df[df['Species'] == 'Iris-versicolor']['PetalLengthCm']
group_virginica = df[df['Species'] == 'Iris-virginica']['PetalLengthCm']

# 3. Perform independent two-sample t-test
t_stat, p_value = ttest_ind(group_versicolor, group_virginica, equal_var=False)

# 4. State the hypotheses
print(""Null Hypothesis (H0): There is no differe...",8,"The generated code correctly performs all the required steps: loading the dataset, filtering the species, performing the t-test, stating hypotheses, printing results, and interpreting them.

Strong points:
- Correctly uses `pandas` for data loading and filtering.
- Correctly uses `scipy.stats.ttest_ind` for the t-test.
- Clearly states null and alternative hypotheses.
- Prints the t-statistic and p-value.
- Provides a correct interpretation based on the p-value and a specified alpha.
- Uses f-strings for clear output.

Areas for improvement to achieve a higher score (9-10):
- **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError` and checks for empty groups, which makes the code more robust. The generated code lacks this. This is significant for a real-world application.
- **Hypotheses Print Format:** While the hypotheses are correctly stated, the formatting in the ground truth with clear delineation ('--- Independent Two-Sample T-Test for PetalLengthCm ---', etc.) makes the output more professional and readable.
- **T-test `equal_var` parameter:** The generated code explicitly sets `equal_var=False`, which performs Welch's t-test. While often a good default assumption when variances are not known to be equal, the problem statement says ""perform an independent two-sample t-test"" without specifying Welch's. The ground truth correctly notes that `ttest_ind` 's default is `equal_var=True` and proceeds with that, which is a more direct interpretation of ""independent two-sample t-test"" without further qualification. For a benchmark where strict adherence to the most common interpretation is preferred, matching the default assumption or explicitly stating why `equal_var=False` is used would be better.",gpt-4.1,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('iris.csv')

# List of numeric features
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

# Set plot style
sns.set(style=""whitegrid"")

for feature in features:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='Species', y=feature, data=df)
    plt.title(f'Box Plot of {feature} grouped by Species')
    plt.xlabel('Species')
    plt.ylabel(feature)
 ...",9,"The generated code correctly loads the dataset, iterates through each numerical feature, and generates a box plot grouped by 'Species' for each, with appropriate titles. It uses `pandas`, `matplotlib.pyplot`, and `seaborn` as required. The plots are informative and meet the core requirements of the task.

**Strong points:**
*   Correctly loads the CSV.
*   Correctly identifies numerical features and iterates through them.
*   Generates separate `matplotlib` figures for each plot, preventing them from overlapping in a single axis.
*   Uses `seaborn.boxplot` effectively for grouped box plots.
*   Adds appropriate titles, x-labels, and y-labels.
*   Includes proper imports.

**Areas for minor improvement (why it's not a 10):**
*   **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions when loading the CSV. The generated code lacks this, which makes it less robust to missing files.
*   **Aesthetics:** While `sns.set(style=""whitegrid"")` is good, the ground truth includes `plt.grid(axis='y', linestyle='--', alpha=0.7)` and `plt.tight_layout()` for slightly more refined plot aesthetics and layout, which are not strictly required but improve presentation.
*   **Configuration:** The ground truth uses constants for `DATASET_PATH`, `NUMERICAL_FEATURES`, and `GROUPING_COLUMN`, which makes the code more readable and maintainable if these values need to change. The generated code directly hardcodes the strings.

Overall, it's a very good solution that successfully addresses the core requirements of the task. The missing error handling and slightly less configurable design are minor points compared to the overall correctness and functionality.",gpt-4.1,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score

# Load dataset
data = pd.read_csv('heart.csv')

# Select features and target
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']
X = data[features]
y = data['HeartDisease']

# One-hot encode categorical features
categorical_features = ['Sex', 'C...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, selects the specified features and target, performs one-hot encoding on the categorical features, splits the data, trains a Logistic Regression model, and reports the accuracy.

Strong points:
- Correctly loads data and selects features/target.
- Implements one-hot encoding for the specified categorical columns.
- Correctly splits data with the specified `test_size` and `random_state`.
- Trains a Logistic Regression model and calculates accuracy.
- Uses `pd.concat` to combine processed features, which is a common and acceptable way to handle this when not using a `ColumnTransformer` or `Pipeline`.
- The use of `drop='first'` in `OneHotEncoder` is a valid choice to avoid multicollinearity, though not explicitly requested in the prompt, it's good practice.

Areas for very minor improvement (not critical enough to significantly reduce score):
- The `handle_unknown='ignore'` parameter for `OneHotEncoder` is usually recommended in real-world scenarios to prevent errors if unseen categories appear in the test set. The generated code defaults to `error`. While for this specific dataset and split it might not be an issue, it's a good practice.
- The ground truth uses `ColumnTransformer` and `Pipeline`, which is a more robust and idiomatic way in scikit-learn to handle preprocessing steps and ensure they are applied consistently to new data (e.g., test data or future predictions). The generated code manually preprocesses `X` before splitting, which also works but is slightly less streamlined for complex workflows. However, for this simple case, the manual concatenation is acceptable.
- Increasing `max_iter` for `LogisticRegression` (as seen in the ground truth) can sometimes be necessary for convergence, though for this dataset it might not be strictly required.",gpt-4.1,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# 1. Load the dataset
df = pd.read_csv('heart.csv')

# 2. Create two groups based on 'Sex'
males = df[df['Sex'] == 'M']['Cholesterol']
females = df[df['Sex'] == 'F']['Cholesterol']

# 3. Perform an independent two-sample t-test
t_stat, p_value = ttest_ind(males, females, nan_policy='omit')

# 4. State hypotheses
print(""Null hypothesis (H0): There is no difference in mean Cholesterol levels between male and female patients."")
print(...",9,"The generated code is very good and accurately addresses almost all requirements of the task.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly separates male and female cholesterol levels.
*   Performs the independent two-sample t-test using `scipy.stats.ttest_ind`.
*   Clearly states the null and alternative hypotheses.
*   Prints the t-statistic and p-value with appropriate formatting.
*   Correctly interprets the p-value against the given alpha (0.05) and states the conclusion regarding the null hypothesis.
*   Handles `nan` values in `Cholesterol` implicitly using `nan_policy='omit'`, which is a good practice.

**Areas for minor improvement:**
*   **Robustness/Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions when loading the CSV, and checks for minimum data samples in each group. The generated code lacks these, making it less robust. If `heart.csv` is missing or the `Sex` or `Cholesterol` columns don't exist, it would throw an error instead of gracefully handling it.
*   **Encapsulation:** The ground truth encapsulates the logic within a function (`perform_cholesterol_t_test`), which is generally better practice for reusability and modularity, and includes a `if __name__ == ""__main__"":` block for execution. The generated code is a direct script. While acceptable for a short task, functions improve code organization.

These points are minor given the core task was statistical analysis, but they differentiate a good script from production-ready code. Overall, it's an excellent attempt.",gpt-4.1,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('winequality-red.csv')

# Columns to process
cols = ['volatile acidity', 'residual sugar']

outlier_info = {}

# Step 1 and 2: Identify and count outliers using IQR
for col in cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = ((df[col] < lower_bound) | (df[col] > upper_bound))
    outlier_count = o...",9,"The generated code is highly correct and directly addresses all parts of the task.

**Strong points:**
- Correctly identifies outliers using the IQR method.
- Reports the number of outliers accurately.
- Implements winsorization correctly by capping values at the 1.5*IQR bounds.
- Compares mean and standard deviation before and after treatment effectively.
- Uses `np.where` for efficient winsorization.
- The use of the `outlier_info` dictionary to store bounds and counts for each column is a good structural choice.

**Areas for minor improvement:**
- **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. The generated code assumes the file will always be present. While not explicitly asked for, robust code should include this.
- **Clarity of Output:** The ground truth provides more verbose and formatted output, which helps in understanding each step (e.g., printing Q1, Q3, IQR, bounds). The generated code's output is more concise but could benefit from a bit more detail for an ""expert"" demonstration. For instance, explicitly stating the bounds used for winsorization in the output for each column would be helpful.
- **Head of winsorized dataset:** The ground truth also explicitly shows the head of the winsorized dataset for the processed columns at the end, which aids in visual verification that the treatment was applied.

These are minor points. The core functionality and correctness of the code are excellent.",gpt-4.1,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the dataset
df = pd.read_csv('heart.csv')

# 2. Create a grouped DataFrame for plotting
grouped = df.groupby(['ChestPainType', 'HeartDisease']).size().reset_index(name='Count')

# 3. Plot grouped bar chart
plt.figure(figsize=(8,6))
sns.barplot(
    data=grouped, 
    x='ChestPainType', 
    y='Count', 
    hue='HeartDisease',
    palette='Set2'
)

# 4. Add title, axes labels, and legend
plt.title('Coun...",9,"The generated code correctly loads the dataset and creates a grouped bar chart as requested. It uses `seaborn.barplot` effectively after pre-grouping the data to achieve the desired visual. The plot includes a title, labeled axes, and a legend, satisfying all requirements.

**Strengths:**
- Correctly loads the CSV file.
- Successfully creates a grouped bar chart showing the count of HeartDisease (0 and 1) for each ChestPainType.
- All plot elements (title, labels, legend) are present.
- Uses `seaborn` which is a good choice for this type of visualization.
- The use of `groupby().size().reset_index()` is a valid approach to prepare the data for `barplot`.

**Minor Improvement:**
- The prompt explicitly stated ""Use matplotlib or seaborn for plotting"" and mentioned ""the x-axis should represent ChestPainType, and for each type, there should be two bars representing the count of HeartDisease=0 and HeartDisease=1"". While `barplot` with pre-grouped data works, `countplot` (as used in the ground truth) is often a more direct and sometimes cleaner way to get counts of categories like this, as it handles the counting internally. Both achieve the same visual outcome correctly, but `countplot` might be considered slightly more idiomatic for simple counts of categories.
- The legend for `HeartDisease` simply shows 0 and 1. Mapping these to more descriptive labels like 'No Heart Disease' and 'Heart Disease' (as in the ground truth) would improve readability, though this was not an explicit requirement of the prompt, it's good practice.
- There's no error handling for `FileNotFoundError` or other potential issues with loading the CSV, which is present in the ground truth and makes the code more robust. However, the prompt didn't explicitly ask for error handling, so it's not a deduction for correctness but rather a quality improvement.",gpt-4.1,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Clean 'in_shazam_charts' column
df['in_shazam_charts'] = df['in_shazam_charts'].astype(str).str.replace(',', '', regex=False)
df['in_shazam_charts'] = pd.to_numeric(df['in_shazam_charts'], errors='coerce')

# Report count of NaNs
nan_count = df['in_shazam_charts'].isna().sum()
print(f""Number of NaNs in 'in_shazam_charts': {nan_count}"")

# Group by 'key' and 'mode' and compute averages
result = df.groupby(['...",9,"The generated code is very good and correctly addresses almost all requirements of the task.

**Strong Points:**
*   **Correct CSV Loading:** Loads the `spotify_2023.csv` file using pandas.
*   **Correct Cleaning of `in_shazam_charts`:** Successfully removes commas and converts the column to numeric, coercing errors to NaN. The use of `astype(str)` before `str.replace` is robust.
*   **Correct NaN Reporting:** Accurately calculates and prints the count of NaNs in the `in_shazam_charts` column after coercion.
*   **Correct Grouping and Aggregation:** Correctly groups the data by `key` and `mode` and calculates the mean of `danceability_%` and `valence_%` for each group.
*   **Proper Imports:** Includes `import pandas as pd`.
*   **Conciseness:** The code is clean and to the point.

**Areas for Minor Improvement (Why not a 10):**
*   **Error Handling for File Not Found:** The ground truth includes a `try-except FileNotFoundError` block, which is a good practice for robust code, especially when dealing with external files. The generated code assumes the file will always be present, which might lead to a `FileNotFoundError` if the file is missing. While the prompt didn't explicitly ask for error handling, it's generally good practice in real-world scenarios.
*   **Clarity of Output Presentation:** The ground truth code added an extra print statement (`print(""\nAverage 'danceability_%' and 'valence_%' by 'key' and 'mode':"")`) before displaying the grouped result, which improves the readability of the output for the end-user. The generated code just prints the DataFrame directly, which is functional but less descriptive.

Overall, the generated code performs the core tasks excellently. The minor points are about robustness and presentation, not core functionality.",gpt-4.1,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# 1. Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 2. Ensure 'streams' column is numeric
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# 3. Select features and target, cleaning NaNs
predictor_cols = ['in_spotify_playlists', 'energy_%']
all_cols = predictor_cols + ['streams']
df_cl...",9,"The generated code successfully loads the dataset, handles the 'streams' column numerization and NaN values correctly, selects the specified features and target, splits the data, trains a Linear Regression model, and evaluates it with R2 and MSE. It includes all necessary imports and follows a logical flow.

**Strong points:**
*   Correctly uses `pd.to_numeric` with `errors='coerce'` for the 'streams' column.
*   Correctly identifies and drops NaNs across all relevant columns before selecting X and y.
*   Correctly splits the data for training and testing with the specified `random_state`.
*   Successfully trains and evaluates the Linear Regression model.
*   Prints the required evaluation metrics in the specified format.

**Area for minor improvement:**
*   The generated code omits the `try-except` block for `FileNotFoundError` when loading the CSV, which was present in the ground truth. While not strictly required by the prompt's explicit list, it's good practice for robustness in real-world scenarios and was part of the ground truth's considered code. This is very minor and doesn't detract from the core task completion.
*   The ground truth included some print statements for clarity and progress tracking, which are absent in the generated code. Again, not explicitly asked for, but adds to user experience.",gpt-4.1,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

# 1. Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 2. Combine released_year, released_month, released_day into 'release_date'
df['release_date'] = pd.to_datetime(
    df[['released_year', 'released_month', 'released_day']],
    errors='coerce'
)

# 3. Create 'days_since_release' as the days between 'release_date' and '2023-12-31'
fixed_date = pd.Timestamp('2023-12-31')
df['days_since_release'] = (fixed_date - df['release_date']).dt.days

# 4. Display fir...",9,"The generated code correctly addresses all parts of the task. It loads the dataset, converts the date columns into a single `release_date` column using `pd.to_datetime` with `errors='coerce'` (which is a very concise and effective way to achieve this for multiple columns), calculates `days_since_release` using a fixed date, and displays the required output.

The use of `pd.to_datetime(df[['released_year', 'released_month', 'released_day']], errors='coerce')` is a particularly elegant and efficient way to create the datetime column directly from multiple columns, which is a strength of the pandas library and well-utilized here.

The only minor area for improvement, preventing a perfect 10, is the lack of explicit error handling for file loading (e.g., `FileNotFoundError`) which was present in the ground truth. While not explicitly asked for in the prompt's main requirements, robust production code would typically include it. However, given the prompt asked ""Provide only the Python code without explanations"" and focused on the core logic, this is a minor omission. Also, the prompt specified the new feature should be an integer, and while `dt.days` typically returns an integer, if `release_date` contains `NaT` values, the result will be `NaN` for `days_since_release`. While `NaN` is technically a float, pandas often handles `NaN` in integer columns using nullable integer types (`Int64`). The generated code produces floats by default when `NaN`s are present in the `dt.days` output. If strict integer type without NaNs was required for *all* entries, a fillna or explicit astype('Int64') would be needed. However, given the phrasing ""an integer"", and the common practice with `NaN`s, `dt.days` is generally acceptable as it returns integer values where possible. Compared to the explicit `astype('Int64')` in the ground truth, the generated code doesn't strictly enforce `Int64` which handles `NaN`s as part of the integer type cleanly. Given the problem statement, this is a very minor detail.

Overall, it's a very good and concise solution.",gpt-4.1,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

# Load the CSV file
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Video views' column
df['Video views'] = pd.to_numeric(df['Video views'].astype(str).str.replace(',', ''), errors='coerce')
df['Video views'] = df['Video views'].fillna(0)

# Clean 'Likes' column
df['Likes'] = pd.to_numeric(df['Likes'].astype(str).str.replace(',', ''), errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

# Fill missing values in 'Category' with 'Unknown'
df['Category'] = df['Ca...",9,"The generated code correctly implements all specified requirements: loading the dataset, cleaning the 'Video views' and 'Likes' columns (removing commas, converting to numeric, handling NaNs by filling with 0), filling missing 'Category' values with 'Unknown', and finally grouping by 'Category' to sum the cleaned 'Video views' and 'Likes'. The import is correct, and the structure is clean and readable.

The only minor area for improvement that would bring it closer to the robustness of the ground truth is the lack of explicit error handling for missing columns or the dataset itself. While the prompt implied a normal execution path, robust production code would often include checks like `if 'Video views' in df.columns:` before attempting operations, or `try-except` blocks for `FileNotFoundError`. However, for a direct task fulfillment within a benchmark, the generated code precisely meets the requirements efficiently. The output format also exactly matches what was expected.",gpt-4.1,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# 1. Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# 2. Clean Likes column
df['Likes'] = df['Likes'].astype(str).str.replace(',', '', regex=False)
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

# 3. Fill missing Category values
df['Category'] = df['Category'].fillna('Unknown')

# 4. Create two groups
likes_music = df[df['Category'] == 'Music']['Likes']
likes_sports = df[...",9,"The generated code correctly addresses almost all aspects of the task, demonstrating good understanding of data cleaning, statistical testing, and hypothesis interpretation.

**Strong Points:**
- **Correct Imports:** Uses `pandas` and `scipy.stats.ttest_ind` appropriately.
- **Accurate Data Loading:** Loads the CSV file as required.
- **Effective Cleaning of 'Likes':** Successfully removes commas, converts to numeric, handles errors by coercing to NaN, and fills remaining NaNs with 0. This is a robust approach.
- **Correct Handling of 'Category' NaNs:** Fills missing `Category` values with 'Unknown'.
- **Correct Group Creation:** Correctly filters the DataFrame to create 'Music' and 'Sports' `Likes` groups.
- **Proper T-test Execution:** Uses `ttest_ind` with `equal_var=False` as is good practice when variances are not assumed to be equal.
- **Clear Hypotheses:** States both null and alternative hypotheses clearly.
- **Correct Output of Statistics:** Prints the t-statistic and p-value.
- **Accurate Interpretation:** Interprets the p-value correctly against the alpha level of 0.05 and states the conclusion about rejecting or failing to reject the null hypothesis.

**Areas for Minor Improvement:**
- **No `try-except` for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` during CSV loading, which is good practice for robust code. The generated code assumes the file will always be present, which might lead to an unhandled error in some environments.
- **Redundant `df['Likes']` Overwrite:** The ground truth creates a new column `Likes_cleaned` for the cleaned data, preserving the original `Likes` column. While overwriting `df['Likes']` directly is acceptable for this task, creating a new column can sometimes be beneficial for debugging or if the original data is needed later. This is a minor point, but worth considering for larger projects.
- **Slightly Less Detailed Hypothesis/Interpretation:** While correct, the hypotheses and interpretation statements are a bit more succinct than the ground truth. The ground truth explicitly mentions ""statistically significant difference in the mean cleaned 'Likes'"" and outlines exactly what failure to reject or rejection means in terms of the specific variables (`_music` vs `_sports`). This level of detail in the output can sometimes be preferred in a formal report.
- **No Check for Empty Groups:** The ground truth includes a check (`if len(music_likes) < 2 or len(sports_likes) < 2:`) before performing the t-test. This prevents errors if one or both categories have insufficient data for a statistical test. This is a minor robustness improvement that could be added.

Overall, the generated code is highly functional and fulfills the core requirements of the task. The suggested improvements are primarily about robustness and output detail, which are good practices but don't detract significantly from the current solution's correctness.",gpt-4.1,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Function to clean numeric columns
def clean_numeric(col):
    return pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce').fillna(0)

# Clean 'Likes' and 'Dislikes' columns
df['Likes_clean'] = clean_numeric('Likes')
df['Dislikes_clean'] = clean_numeric('Dislikes')

# Create 'TotalEngagement' column
df['TotalEngagement'] = df['Likes_clean'] + df['Dislikes_clean']

#...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, correctly cleans the 'Likes' and 'Dislikes' columns by removing commas, converting to numeric, and filling NaNs with 0 (achieved efficiently using a helper function). It then correctly calculates `TotalEngagement` and `LikeDislikeRatio` as specified, including the `+1` in the denominator to avoid division by zero. Finally, it displays the required `Video`, `TotalEngagement`, and `LikeDislikeRatio` columns.

The use of a helper function `clean_numeric` is a good practice for code readability and reusability, although it introduces new columns `Likes_clean` and `Dislikes_clean` instead of overwriting the original columns as the ground truth does. This is a minor difference and doesn't impact correctness.

One minor point is that error handling for file loading is missing, which was present in the ground truth. While not explicitly asked for in the prompt's requirements, robust code would typically include this. However, based on the specific requirements listed for ""Evaluation Criteria"", the code perfectly aligns.",gpt-4.1,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean 'Launched Price (USA)' column
df['Launched Price (USA)'] = (
    df['Launched Price (USA)']
    .str.replace('USD ', '', regex=False)
    .str.replace(',', '', regex=False)
)
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'], errors='coerce')

# Clean 'Screen Size' column
df['Screen Size'] = (
    df['Screen Size']
    .str.replace(' inches', '', regex=False)
)
d...",9,"The generated code correctly performs all the requested tasks: loading the CSV, cleaning both specified columns (including handling errors with `errors='coerce'`), grouping by the correct columns, and calculating the required averages. It uses `as_index=False` in `groupby`, which is a good practice for immediately getting a DataFrame with columns rather than a MultiIndex. The code is concise and efficient.

One minor area for improvement, which is purely for robustness and doesn't affect correctness for standard CSVs, would be to include error handling for `pd.read_csv` (e.g., `FileNotFoundError`), similar to the ground truth. Also, while not strictly necessary, explicitly converting the columns to string type before `str.replace` can prevent errors if the initial column type is mixed or numeric and not suitable for string operations. However, for a CSV where these are expected to be strings, the current approach works fine. The use of `numpy` is not strictly necessary as `pandas` handles `NaN` and numerical operations internally. Overall, a very good solution.",gpt-4.1,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

# 1. Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# 2. Define function to clean camera columns
def extract_mp(val):
    if pd.isnull(val):
        return 0
    match = re.search(r'(\d+)', str(val))
    if match:
        return int(match.group(1))
    else:
        return 0

df['Cleaned Front Camera'] = df['Front Camera'].apply(extract_mp)
df['Cleaned Back Camera'] = df['Back Camera'].apply(extract_mp)

# 3. Create new feature
df['Total Camera MP...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, defines and applies a function to clean camera specifications to extract the first numerical MP value (handling NaNs and returning 0 if no digits are found), creates the `Total Camera MP` feature, and displays the specified columns for the first 10 rows. The imports are correct and the code is well-structured.

One minor area for improvement, though not strictly required by the prompt, would be to include error handling for `FileNotFoundError` when loading the CSV, as demonstrated in the ground truth. This makes the script more robust. Also, the ground truth uses `to_string()` for displaying the head, which can provide slightly cleaner console output, but `print(df[...].head(10))` is perfectly acceptable and common practice. The use of `re.search(r'(\d+)', str(val))` is effective and correct, capturing the first sequence of digits. `match.group(1)` is also correct because of the capturing group, equivalent to `match.group(0)` when there's only one capturing group.",gpt-4.1,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# 1. Load dataset
df = pd.read_csv('mobiles_2025.csv')

# 2. Clean columns

# Clean RAM
df['RAM'] = pd.to_numeric(
    df['RAM'].str.replace('GB', '', regex=False).str.strip(),
    errors='coerce'
)

# Clean Battery Capacity
df['Battery Capacity'] = pd.to_numeric(
    df['Battery Capacity'].str.replace('mAh...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, performs the specified column cleaning (RAM, Battery Capacity, Screen Size, Launched Price (USA)) including handling NaNs and dropping rows, defines features and target, splits the data, trains a Linear Regression model, and reports R2 and MAE. The code is well-structured and uses appropriate libraries.

Improvement: The generated code is missing error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth. While not explicitly asked for in the prompt, it significantly improves robustness. Also, the ground truth includes checks for columns existing, which is a good practice.",gpt-4.1,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load dataset
df = pd.read_csv('titanic.csv')

alpha = 0.05

# 1. Chi-square test: Pclass vs Survival
contingency = pd.crosstab(df['Pclass'], df['Survived'])
chi2, p_chi2, dof, ex = stats.chi2_contingency(contingency)

# Cramr's V calculation
n_chi2 = contingency.values.sum()
min_dim = min(contingency.shape) - 1
cramers_v = np.sqrt(chi2 / (n_chi2 * min_dim))

print(""1. Chi-square Test: Pclass vs Survival"")
print(f""Chi2 s...",8,"The generated code successfully performs the requested statistical analyses (chi-square, t-test, ANOVA) and calculates the corresponding effect sizes. It also correctly interprets the p-values against the given alpha level of 0.05.

**Strong points:**
*   Correctly implements `chi2_contingency`, `ttest_ind`, and `f_oneway`.
*   Effect size calculations (Cramer's V, Cohen's d, Eta-squared) are largely correct.
*   Handles missing 'Age' and 'Fare' values for the respective tests using `dropna()`.
*   Includes clear print statements for statistics, p-values, effect sizes, and conclusions.

**Areas for improvement to reach a higher score:**
*   **Error Handling for File Loading:** The ground truth includes robust error handling for `FileNotFoundError` when loading the CSV, which is good practice. The generated code assumes the file will always be present.
*   **T-test `equal_var` parameter:** The generated code automatically sets `equal_var=False` for the t-test (Welch's t-test). While often a safer default, a more rigorous approach, as shown in the ground truth, would be to first perform Levene's test for equality of variances and then decide on the `equal_var` parameter based on its result. This demonstrates a deeper understanding of statistical assumptions.
*   **Clarity of Output and Assumptions:** The ground truth code provides more detailed print statements, including the exact null and alternative hypotheses, clarifying the context of each test. It also explicitly mentions the alpha level at the beginning.
*   **Cohen's d Calculation:** The `nan_policy='omit'` in the t-test call is redundant with the preceding `dropna()`. Also, the Cohen's d calculation should consider edge cases where `n1 + n2 - 2` might be zero or negative, although for typical datasets, this isn't an issue. The ground truth version of Cohen's d is potentially more robust if `std1` or `std2` are small for very small groups, though the core formula is the same.
*   **Cramer's V `min_dim`:** The ground truth includes `if (n * min_dim) > 0 else 0` for Cramer's V to prevent division by zero in edge cases (though unlikely with real data).
*   **Docstrings/Comments:** While the prompt asked for only code, adding comments (like in the ground truth) explaining the steps and formulas significantly improves code readability and maintainability, especially for statistical analysis where the ""why"" is as important as the ""what.""
*   **Minor formatting of output:** The ground truth's use of section headers (`--- Chi-square Test ---`) and newlines makes the output more organized and readable.",gpt-4.1,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Group by Year and calculate average Rating and Runtime
grouped = df.groupby('Year').agg({'Rating': 'mean', 'Runtime': 'mean'}).reset_index()

# Sort by Year for proper plotting
grouped = grouped.sort_values('Year')

# Plot average Rating over Year
plt.figure(figsize=(10, 6))
plt.plot(grouped['Year'], grouped['Rating'], marker='o')
plt.title('Average IMDb Top 250 Movie Rating ...",7,"The generated code correctly performs the core tasks of loading data, grouping, calculating averages, and plotting. It successfully creates a line plot with a title and labeled axes, fulfilling most requirements.

However, it lacks robustness and completeness in handling potential data cleaning issues that are common in real-world scenarios. Specifically:
1.  **Data Cleaning:** The ground truth includes robust data cleaning for `Year` (extracting 4-digit year from strings like ""(2000)"") and `Runtime` (removing "" min"" and converting to int), and coercing `Rating` to numeric. The generated code assumes these columns are already in a clean, numeric format. If the CSV has dirty data as described, the generated code would fail or produce incorrect results.
2.  **Error Handling:** The generated code does not include any error handling (e.g., `try-except` blocks for `FileNotFoundError`).
3.  **Function Encapsulation:** The ground truth encapsulates the logic within a function, making it more modular and reusable. The generated code is sequential script-style.
4.  **Comments/Readability:** While not explicitly required for this short task, the ground truth uses more comments and docstrings which improve readability and maintainability for larger projects.

These omissions, especially the lack of data cleaning for potentially problematic columns like 'Year' and 'Runtime' as often seen in real datasets (and implicitly handled in some previous iterations of such tasks), reduce the score. If the 'imdb_top_250.csv' is guaranteed to be perfectly clean, the score would be higher. Assuming a more realistic scenario where some cleaning might be needed, the generated code is less robust.",gpt-4.1,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

# 1. Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 2. Create two groups based on the `Year` column
group_before_1990 = df[df['Year'] < 1990]['Rating']
group_1990_and_after = df[df['Year'] >= 1990]['Rating']

# 4. State the hypotheses
print(""Null hypothesis (H0): There is no difference in mean Ratings between movies released before 1990 and those released in or after 1990."")
print(""Alternative hypothesis (H1): There is a difference...",8,"The generated code correctly performs the core task of conducting a t-test and interpreting the results.

**Strong Points:**
*   Correctly loads the dataset.
*   Correctly generates the two groups based on the `Year` column and extracts `Rating`.
*   States the null and alternative hypotheses clearly.
*   Performs the independent two-sample t-test using `scipy.stats.ttest_ind`.
*   Correctly prints the t-statistic and p-value.
*   Correctly interprets the p-value against the alpha level of 0.05.
*   Includes necessary imports (pandas, scipy.stats).
*   Uses `equal_var=False` (Welch's t-test), which is often a safer choice when population variances cannot be assumed equal, although the ground truth uses `equal_var=True`. Either is acceptable if not explicitly specified.
*   Uses `nan_policy='omit'`, which handles potential NaN values in the rating columns gracefully, although the ground truth handles this via `dropna` on the DataFrame itself beforehand.

**Areas for Improvement (Why it's not a 10):**
*   **Error Handling for File Loading:** The generated code lacks a `try-except` block for `pd.read_csv`, which means it would crash if the file is not found. The ground truth includes this, making it more robust.
*   **Data Type Coercion and NaN Handling:** The generated code directly indexes `Rating` after filtering. It assumes 'Year' and 'Rating' columns are already in numeric format and do not contain any `NaN` values that would cause errors. The ground truth explicitly handles this (`pd.to_numeric` with `errors='coerce'` and `dropna`), which is crucial for real-world datasets that might have dirty data. This is a significant omission for a robust solution.
*   **Edge Case: Insufficient Group Sizes:** While not explicitly asked for, a robust solution would check if there are enough samples in each group (e.g., at least 2) before performing the t-test, as `ttest_ind` will error or produce meaningless results otherwise. The ground truth includes this check.
*   **Output Formatting:** The output for hypotheses is a bit terse; the ground truth provides a more verbose, formal statement which aligns better with reporting hypotheses in a statistical context.
*   **Minor print statement phrasing:** The interpretation `print(f""Since p-value < {alpha}, we reject the null hypothesis: There is a significant difference in mean Ratings."")` is slightly less formal than the ground truth's `Conclusion: There is a statistically significant difference...`.

Overall, a very good attempt that fulfills most major requirements but could be improved in robustness and data handling.",gpt-4.1,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

# 1) Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 2) Create a 'Decade' column
df['Decade'] = (df['Year'] // 10 * 10).astype(str) + 's'

# 3) Display Title, Year, Decade for the first 10 movies
print(df[['Title', 'Year', 'Decade']].head(10))
```",9,"The generated code correctly performs all the required tasks: it loads the dataset, accurately creates the `Decade` column using integer division and string concatenation, and displays the specified columns for the first 10 movies. The imports are proper and the code is concise.

One minor improvement would be to handle potential `FileNotFoundError` or other exceptions for robustness, similar to the ground truth code. However, for the given problem statement, it fulfills all explicit requirements.",gpt-4.1,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

# 1. Load the dataset
df = pd.read_csv('penguins.csv')

# 2. Impute numerical columns with mean
num_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in num_cols:
    mean_val = df[col].mean(skipna=True)
    df[col] = df[col].fillna(mean_val)

# 3. Impute 'sex' with mode
mode_sex = df['sex'].mode(dropna=True)[0]
df['sex'] = df['sex'].fillna(mode_sex)

# 4. Group by 'species' and 'sex'
grouped = df.groupby(['species', 'sex'],...",10,"The generated code correctly solves the task. It loads the dataset, performs mean imputation for numerical columns, mode imputation for the 'sex' column, and then correctly groups the data and calculates the average `body_mass_g`. The code is clean, well-structured, and directly addresses all requirements of the prompt. All imports are proper, and the dataset loading and processing steps are logically ordered and correct. The use of `skipna=True` for mean calculation (though it's the default) and `dropna=True` for mode are good practices, even if they explicitly state the default behavior. The `as_index=False` in `groupby` is also a valid way to prepare for printing the result, though not strictly necessary for the `mean()` operation itself.",gpt-4.1,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# 1. Load dataset
df = pd.read_csv('penguins.csv')

# 2. Filter to include only Adelie and Gentoo species
df_filtered = df[df['species'].isin(['Adelie', 'Gentoo'])]

# 3. Remove rows with missing flipper_length_mm or species
df_filtered = df_filtered.dropna(subset=['flipper_length_mm', 'species'])

# 4. Prepare the samples
adelie_flippers = df_filtered[df_filtered['species'] == 'Adelie']['flipper_length_mm']
gentoo_flippers = df_fi...",9,"The generated code correctly addresses all aspects of the task. It loads the dataset, filters it appropriately, handles missing values, clearly states hypotheses, performs the t-test, and interprets the results correctly based on the given significance level.

Strong points:
- Correctly uses `pandas` for data loading and filtering.
- Correctly uses `scipy.stats.ttest_ind` for the independent two-sample t-test.
- Accurately states the null and alternative hypotheses.
- Correctly extracts the flipper lengths for each species.
- Prints the t-statistic and p-value as required.
- Provides a correct interpretation of the p-value against the alpha level.
- Uses `equal_var=False` which is a good default for robustness (Welch's t-test).

Areas for minor improvement (not critical for score, but good practice):
- The Ground Truth code includes error handling for `FileNotFoundError` during dataset loading, which is good practice for robustness in real-world applications. The generated code assumes the file will always be present.
- The `nan_policy='omit'` in `ttest_ind` is redundant because `dropna` was already performed. While it doesn't cause harm, it's unnecessary. `dropna` is the more explicit way to handle NAs for the whole dataset relevant to the test.",gpt-4.1,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1. Load the dataset
df = pd.read_csv('penguins.csv')

# 2. Select features and target
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
target = 'species'

# 3. Drop rows with missing values in features or target
df_clean = df.dropna(s...",9,"The generated code correctly implements all specified steps: loading data, selecting features and target, dropping NAs, encoding the target, splitting data, training a Logistic Regression model with `max_iter=1000`, and reporting accuracy. The imports are correct, and the code is well-structured and concise.

One minor suggestion for improvement, though not critical for correctness or score, would be to handle the `FileNotFoundError` for the `penguins.csv` file, similar to the ground truth. While the prompt implied the file would be present, robust code often includes such error handling. However, given the specific task constraints, this is a very minor point.

Overall, the code is excellent and fully meets the task requirements.",gpt-4.1,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Drop rows where `body_mass_g` or `species` is NA
df_clean = df.dropna(subset=['body_mass_g', 'species'])

# 3) For each `species`, calculate the average `body_mass_g`
avg_body_mass = df_clean.groupby('species')['body_mass_g'].mean().reset_index()

# 4) Create a bar plot showing the average `body_mass_g` for each `species`
plt.figure(figsize=(8, 6))
sns...",9,"The generated code correctly implements all specified tasks: loading the data, dropping NA values in the specified columns, calculating the average body mass per species, and generating a labeled bar chart. The use of pandas and seaborn is appropriate and efficient. The code is concise and readable.

Minor points for improvement (but not critical for the task):
- Error handling for `FileNotFoundError` is not included, which is good practice for robust code.
- Variable names in the plot (`x='species'`, `y='body_mass_g'`) could be made more descriptive like in the ground truth (`x=GROUPING_COLUMN`, `y=TARGET_COLUMN`) to align better with the calculated data frame's column names if they were renamed, or simply to show more intent.
- The title and labels are quite generic, slightly less informative than the ground truth's dynamically generated ones which explicitly state ""Average Body Mass (g)"".

Overall, it's a very good solution to the problem.",gpt-4.1,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Drop rows with NA in specified columns
cols_to_check = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_clean = df.dropna(subset=cols_to_check)

# 3) Create new feature 'culmen_area'
df_clean['culmen_area'] = df_clean['culmen_length_mm'] * df_clean['culmen_depth_mm']

# 4) Calculate Pearson correlation matrix
corr_cols = [...",9,"The generated code successfully completes all aspects of the task, demonstrating correct implementation for loading data, handling NAs, creating a new feature, calculating the correlation matrix, and visualizing it. The code is well-structured and uses appropriate libraries.

Strengths:
- Correctly loads the dataset.
- Accurately drops NA values from the specified columns.
- Correctly calculates the `culmen_area` feature.
- Computes the Pearson correlation matrix for all required columns.
- Generates an appropriate heatmap with annotations, correct colormap, and a title.
- Uses `plt.tight_layout()` for better plot presentation.

Areas for minor improvement (not critical for task completion but good practices):
- **Error Handling**: The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is robust. The generated code assumes the file will always be present, which could lead to a crash if 'penguins.csv' is missing.
- **`df_clean.copy()`**: The ground truth uses `.copy()` after `dropna` (`df_cleaned = df.dropna(subset=columns_to_check_na).copy()`). While not strictly necessary to avoid errors in this specific sequence (because `df_clean` is a new DataFrame resulting from `dropna`), it can prevent `SettingWithCopyWarning` in more complex scenarios where one might be modifying a view of a DataFrame. For this task, it implicitly creates a copy, so it's not a major issue here, but adopting `.copy()` is a good habit.
- **Readability**: Adding more comments detailing each step, like the ground truth, would improve readability for others.
- **Plot Enhancements**: The ground truth included `plt.xticks(rotation=45, ha='right')` and `plt.yticks(rotation=0)` which make the axis labels more readable in some cases, although for this specific small matrix, it's not strictly necessary. `square=True` in `sns.heatmap` is a nice addition in the generated code to ensure cells are square, which is often desirable for heatmaps.

Overall, the generated code is highly effective and meets all the task requirements. The suggested improvements are primarily about robustness and best practices rather than core functional correctness.",gpt-4.1,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np

# 1. Load the dataset
df = pd.read_csv('penguins.csv')

# 2. Select relevant features and target
features = [
    'culmen_length_mm',
    'culmen_depth_mm',
    'flipper_length_mm',
    'body_mass_g',
    'island',
    'sex'
]
X = df[features].copy()
y = df['species']

# 3. Impute numerical features with mean
num_features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in num_features:
    mean_value = X[col]...",9,"The generated code correctly addresses all requirements of the task. It loads the data, selects the specified features and target, correctly imputes numerical NA values with the mean, handles the 'sex' column according to the instructions (imputing NAs with 'Unknown' and mapping categories to integers), and applies one-hot encoding to the 'island' column. Finally, it prints the shape and the head of the processed feature matrix as requested.

The strong points include:
- Correct selection of features and target.
- Accurate imputation of numerical columns using the mean.
- Correct and robust handling of the 'sex' column's imputation and mapping.
- Proper application of one-hot encoding for 'island'.
- Clear and concise code structure.

A minor point for improvement, which prevents a perfect score, is the lack of explicit error handling for `FileNotFoundError` when loading the CSV, which was present in the ground truth. While not explicitly asked for as a requirement in the prompt, it's good practice for robust code. Also, the ground truth explicitly separated `X` and `y` at the end, while the generated code creates `y` early but then doesn't explicitly mention `X` as the ""feature matrix"" until the print statement. However, it correctly produces `X` as the feature matrix.",gpt-4.1,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004'...",10,"The generated code correctly performs all requested operations: creating the DataFrame, calculating total spending per customer, identifying and printing the top 3 customers by spending, and calculating the total quantity sold per category. The code is clean, properly structured, and uses appropriate pandas functions effectively. It correctly implements all requirements of the task. The output format for printing the results is slightly different from the ground truth (e.g., missing currency format for spending), but this is a minor cosmetic difference and does not impact the correctness of the calculations or the identification of values.",gpt-4.1,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
# Imports
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, cl...",8,"The generated code correctly addresses most of the task requirements.

**Strong Points:**
- **Correct Data Preprocessing:** It correctly handles missing values using pipelines (`SimpleImputer`), scales numerical features (`StandardScaler`), and encodes categorical variables (`OneHotEncoder`). It appropriately drops irrelevant columns.
- **Pipeline Usage:** The use of `Pipeline` and `ColumnTransformer` for preprocessing and model training is excellent, making the code clean and robust.
- **Model Training and Evaluation:** It successfully trains a `LogisticRegression` model and calculates all required evaluation metrics (accuracy, precision, recall, F1-score, ROC AUC).
- **Visualization:** Both the Confusion Matrix and ROC Curve are correctly visualized using `seaborn` and `matplotlib`.
- **Feature Coefficients:** It attempts to display feature coefficients and their interpretation, which is a good addition.
- **Cross-Validation:** It performs cross-validation, which adds to the robustness of the model evaluation.

**Areas for Improvement to reach a higher score:**
- **Missing Value Handling Enhancement:** While `SimpleImputer` is used, the ground truth explicitly defined the strategy for 'Age' (median) and 'Embarked' (mode) after reviewing data characteristics, which was a more guided approach. The generated code uses `SimpleImputer` within pipelines for all numerical and categorical features respectively, which is a reasonable general approach but might miss specific nuances or common practices for certain columns (e.g., median for age, mode for embarked).
- **Feature Engineering:** The ground truth included feature engineering by combining 'SibSp' and 'Parch' into 'FamilySize', which often improves model performance in this dataset. The generated code does not include any explicit feature engineering.
- **Detailed Coefficient Interpretation:** While the coefficients are displayed, the interpretation provided in the ground truth regarding positive/negative influence and magnitude, and how to interpret one-hot encoded features relative to a baseline, is more comprehensive and helpful for understanding the model. The generated code briefly mentions `Exp(Coefficient)` which is related to odds ratios but doesn't elaborate on the direct interpretation as clearly as the ground truth.
- **Consistency in Visualization Libraries:** The ground truth used `ConfuisonMatrixDisplay.from_estimator` and `RocCurveDisplay.from_estimator` which are more direct for visualizing pipeline outputs, maintaining consistency. The generated code uses `seaborn.heatmap` for confusion matrix and direct `matplotlib.pyplot.plot` for ROC curve, which is also correct but `sklearn`'s display functions are often more streamlined when working with `sklearn` models.
- **Cross-Validation Metrics:** The ground truth performed cross-validation for all evaluation metrics (accuracy, precision, recall, F1, ROC-AUC), not just accuracy, providing a more thorough assessment of model stability across various performance aspects.

Overall, the generated code performs very well and fulfills the core requirements. The improvements suggested are mostly about adding more detail, specific handling based on domain knowledge (feature engineering), and leveraging more specific `sklearn` visualization utilities, which would elevate it to an even higher standard.",gpt-4.1,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {
    'Date': pd.to_datetime([
        '2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04',
        '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10'
    ]),
    'Ticker': ['TECHCO'] * 8,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
    'Close': [152.0, 151.0, 153.5,...",9,"The generated code correctly creates the DataFrame, sets the 'Date' as the index, calculates the 3-day SMA for 'Close' with appropriate NaN handling for initial values, and calculates the percentage change in 'Volume' with NaN for the first entry. All requirements specified in the task description are met.

The only minor deviation from the prompt or a potential improvement is the line `df['Volume_Pct_Change'] = df['Volume'].pct_change() * 100 # in percentage`. While calculating the percentage change and multiplying by 100 for a more ""percentage"" look is common, the prompt simply asked for ""percentage change"" which `pct_change()` without the multiplication by 100 provides as a ratio. This is a subtle nuance and doesn't make the code incorrect, but rather slightly different in output format for one column. The ground truth provides it as a ratio, which is also a valid ""percentage change"". Given the prompt didn't specify the exact format (ratio vs. 0-100 scale), this is a very minor point.

Otherwise, the code is well-structured, uses correct pandas functions efficiently, and handles the NaN requirements correctly.",gpt-4.1,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

# Step 1: Create DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]
}
df = pd.D...",9,"The generated code correctly performs all the requested tasks: it creates the DataFrame, calculates the 'Score_Improvement' column, computes the average improvement per subject, and identifies students meeting the specified final score and attendance criteria. The code is well-structured and uses appropriate pandas functions. The output format for the ""StudentID"" list is slightly different from the ground truth (list vs. individual prints), but both convey the correct information. The ground truth includes additional print statements for intermediate DataFrame states and separators, which are not strictly required by the prompt's output specification but can be helpful for debugging/understanding. However, the core logic is identical and correct. The generated code also includes comments for each step, which is good practice.",gpt-4.1,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

filtered_df = df[(df['Department'] == 'Tech') & (df['Perfo...",9,"The generated code correctly defines the DataFrame, filters it as specified, and accurately calculates the average number of projects completed for the filtered group. The code is concise and directly addresses all requirements of the task.

**Strong points:**
*   **Correct Logic:** The boolean indexing `(df['Department'] == 'Tech') & (df['PerformanceScore'] > 3)` is perfectly applied to filter the DataFrame.
*   **Accurate Calculation:** `filtered_df['ProjectsCompleted'].mean()` correctly computes the average.
*   **Conciseness:** The code is efficient and to the point.

**Areas for minor improvement:**
*   **Edge Case Handling:** The generated code does not explicitly handle the edge case where `filtered_df` might be empty (i.e., no employees meet the criteria). While `mean()` on an empty Series would return `NaN`, a more robust solution (like the ground truth) might include a check for an empty DataFrame and print a more informative message.
*   **Output Formatting:** The ground truth formats the output with an f-string and explains what the printed number represents. The generated code just prints the raw number, which is functionally correct but less user-friendly.

Overall, the generated code is a very strong submission that fulfills the core requirements. The minor points are about robustness and presentation, not correctness of the stated task.",gpt-4.1,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate conversion rate for Desktop
desktop_users = df[df['DeviceType'] == 'Desktop']
desktop_conversion_rate = (desktop_us...",9,"The generated code correctly defines the DataFrame and accurately calculates the conversion rates for both Desktop and Mobile users, then prints them with appropriate labels. It achieves the task's requirements.

A minor improvement that could be made for robustness is handling cases where a device type might have zero users. For the given dataset, this isn't an issue, but in real-world scenarios, `len(desktop_users)` or `len(mobile_users)` could be zero, leading to a `ZeroDivisionError`. The ground truth code includes checks for `df.empty` to prevent this, which is a good practice for more robust code. Another subtle point for calculating conversion rates is that `df['Converted'].mean()` directly calculates the proportion of `True` values, which is often considered more idiomatic pandas for boolean Series than `sum() / len()`. However, both yield the same correct result.",gpt-4.1,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

# Calculate average AirQualityIndex for each SensorID
avg_aqi = df.groupby(""Sens...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the average ""AirQualityIndex"" for each ""SensorID"", and prints the output clearly, satisfying all requirements of the task. The use of `groupby()` and `mean()` is idiomatic pandas, and the f-string formatting (`:.2f`) ensures readable output. The code is concise, efficient, and directly addresses the prompt's specifications.",gpt-4.1,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

# 1. Create ""EngagementScore""
df['EngagementScore'] = df['Likes'] + (df['Shares'] * 2)...",9,"The generated code correctly defines the DataFrame, calculates the ""EngagementScore"", filters for Instagram posts, and calculates the average ""EngagementScore"" for those posts. The logic is sound and produces the correct output.

The one minor area for improvement is in the printing of the final result. The ground truth code uses an f-string with formatting (`.2f`) and provides a more descriptive message (""Average EngagementScore for Instagram posts: ...""), which enhances readability and user-friendliness. The generated code simply prints the raw float value, which is functionally correct but less polished. Additionally, the ground truth code includes a check for an empty filtered DataFrame, which is a good practice for robustness, although in this specific example, it's not strictly necessary as there will always be Instagram posts.",gpt-4.1,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""M...",10,"The generated code perfectly addresses all requirements of the task. It correctly defines the DataFrame, calculates the ""LengthOfStay"" column, filters for ""Flu"" patients, and accurately calculates and prints their average ""LengthOfStay"". The code is concise, well-structured, and functionally identical to the ground truth (minus the verbose print statements and comments which were part of the ground truth's explanation, not core requirements). It correctly uses `pd.to_datetime` for date columns and the `.dt.days` accessor for timedelta conversion. No improvements are needed for correctness or completeness within the scope of the problem.",gpt-4.1,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}
df = pd.DataFrame(data)

# 1) Filter for Sci-Fi movie...",9,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates their average rating, fulfilling all core requirements of the task. The code is concise and efficient.

One point for minor improvement, which is not critical and the task specifies to only provide Python code, so it might be unnecessary for this specific task, is the output format. The ground truth code formats the output print statement more explicitly, and in a real-world scenario, a more descriptive print statement (e.g., ""The average rating for Sci-Fi movies is: X"") is usually preferred over just printing the raw number, which is what the ground truth did. Additionally, adding an edge case check for an empty filtered DataFrame (though not strictly necessary for this specific dataset) would make the code more robust, as done in the ground truth. However, for a simple calculation task, the generated code is perfectly acceptable and performs the required operations correctly.",gpt-4.1,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

# 1. Filter to include only Toyota cars
toyota_cars = df[df['Brand'] ...",9,"The generated code successfully defines the DataFrame, filters for Toyota cars, and correctly calculates their average mileage. It is concise and directly addresses all the requirements. The code quality is good, with proper imports and a clear structure.

The only minor area for improvement, preventing a perfect 10, is the output format of the average mileage. The ground truth code formats the output with a descriptive string and two decimal places (e.g., ""The average Mileage for Toyota cars is: 22500.00""), which is generally more user-friendly and robust for numerical outputs than just printing the raw number. However, the task only specified to ""print the average 'Mileage'"", which the generated code does accurately. The use of a descriptive print statement would enhance clarity, but its absence doesn't make the code incorrect.",gpt-4.1,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 80]...",9,"The generated code correctly defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat. It correctly addresses all problem requirements. The only minor point for improvement is that the output could be more descriptive (e.g., ""Average Attack for Fire-type Pokemon: X""), similar to the ground truth, but this doesn't detract from the correctness or functionality of the code. The formatting of the print statement is a stylistic choice.",gpt-4.1,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from matplotlib.gridspec import GridSpec

# Load data
df = pd.read_csv('titanic.csv')

# Preprocessing
df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100], labels=['Child', 'Teen', 'Young Adult', 'Adult', 'Senior'])
df['Gender'] = df['Sex'].map({'male':'Male', 'female':'Female'})
df['SurvivedStr'] = df['Survived'].map({1:'Survived', 0:'Did not survive'})...",8,"The generated code successfully addresses most of the requirements. It produces a multi-panel figure, heatmap, violin plots, and an interactive scatter plot as requested. It also attempts a dashboard-style figure.

**Strong points:**
- All required libraries are imported.
- Data preprocessing for `AgeGroup`, `Gender`, `SurvivedStr`, and `PclassStr` is well-handled.
- The multi-panel figure is well-structured with clear titles and annotations.
- The heatmap and violin plots are correctly implemented and visualized.
- The interactive Plotly scatter plot is well-executed with relevant `hover_data` and clear labels.
- The dashboard-style figure attempts to combine multiple visualizations and includes text annotations, which is a good effort towards storytelling.

**Areas for improvement:**
- **Data Preprocessing Consistency/Robustness:** The generated code directly bins `Age` without handling `NaN` values first. This could lead to issues if there were many NaNs. The ground truth code correctly imputes `Age` NaNs with the median before binning. Also, `Pclass` is used directly in some plots (`Pclass` is numerical, `PclassStr` is categorical), which is okay for bar plots but `PclassStr` could be used for consistent categorical representation.
- **Dashboard Layout and Visuals:** While the dashboard is functional, the layout (especially with `GridSpec`) could be more artful and less constrained. The bar charts in the dashboard are repetitions of previous plots; it would be more impactful to include new, relevant visualizations to maximize the use of space and enhance storytelling (e.g., age distribution by class, or fare distribution by class). Also, the insights text is a bit cramped and could be presented more aesthetically. The dashboard's visual integration of the heatmap and violin is good, but adding more distinct visualizations beyond just repeating earlier bar plots would make it stronger.
- **Missing `Fare` imputation:** The generated code does not explicitly handle missing values for 'Fare', which is a minor point but demonstrates less robustness than the ground truth.
- **Annotation in dashboard:** While insight annotations are present, they are a somewhat rudimentary `plt.figtext`. The ground truth creates a dedicated `ax` for insights, allowing for better formatting and control, and separating it visually from the plots themselves.",gpt-4.1,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, ...",9,"The generated code correctly defines the DataFrame, filters for the ""Dystopian"" genre, and accurately calculates the average stock for that genre. It addresses all core requirements of the task. The code quality is good, with proper imports and a clear, concise structure. The only minor difference from the ground truth is the lack of a check for an empty filtered DataFrame before calculating the mean, which is a good practice for robustness in real-world scenarios, although for this specific predefined dataset, it's not strictly necessary as Dystopian books are guaranteed to exist. Additionally, the ground truth includes optional print statements for verification, which are not strictly part of the _task_ requirements but are good for debugging/understanding the output. The generated code also includes a `print(dystopian_books)` which wasn't explicitly asked but is harmless. Overall, a very solid and correct solution.",gpt-4.1,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Remove rows with missing values to ensure fair comparisons
df_clean = df.dropna()

# Define significance level
alpha = 0.05

# 1. T-tests for each water quality parameter between potable and non-potable water
parameters = ['ph', 'Hardness', 'Solids', 'Chloramines', 
              'Sulfate', 'Conductivity', 'Organic_carbon',
              'Trihalomethanes', 'Tur...",7,"The generated code addresses most of the requirements but has critical issues in data handling and some statistical methodology choices.

**Strong points:**
*   Correctly implements t-tests, chi-square test, and Shapiro-Wilk tests.
*   Calculates Cohen's d for t-tests.
*   Includes interpretation of p-values for all tests.
*   Handles the Shapiro-Wilk sample size limitation.

**Areas for improvement/Corrections needed:**
1.  **Missing Value Handling (Critical Issue):** The prompt explicitly mentioned handling missing values. The generated code uses `df.dropna()`, which removes all rows with *any* missing values. This can lead to significant data loss if missing values are common across multiple columns, potentially altering the statistical results and reducing the sample size. The ground truth, correctly, imputes missing values with the mean, which is a more standard approach for numerical data in statistical analysis when `dropna` would cause too much data loss.
2.  **Cohen's d Calculation (Minor Issue):** The formula used for Cohen's d in the generated code `(potable.mean() - non_potable.mean()) / np.sqrt(((potable.std() ** 2) + (non_potable.std() ** 2)) / 2 )` is for Glass's delta or a simplified version, not the standard Cohen's d which uses a pooled standard deviation. The ground truth implements the pooled standard deviation correctly.
3.  **T-test Choice (Minor Issue):** While using Levene's test to decide `equal_var` for `ttest_ind` is a common practice, it's generally recommended to *always* use Welch's t-test (`equal_var=False`) for independent samples t-tests, as it is robust to unequal variances and generally performs well even when variances are equal. The ground truth correctly uses `equal_var=False`.
4.  **Clarity on `parameters` list:** The `parameters` list is hardcoded which is okay, but it would be more robust to derive numerical columns dynamically as done in the ground truth, especially for a generic solution.
5.  **Output Readability:** While functional, the output could be slightly more structured with clear headings and separators, similar to the ground truth, for better readability.
6.  **Missing interpretation of Cohen's d:** The ground truth includes an interpretation of Cohen's d (trivial, small, medium, large), which is a good practice for effect sizes.",gpt-4.1,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, \
      ...",7,"The generated code addresses most of the requirements but has a significant issue in the data preprocessing pipeline, specifically with handling missing values and feature scaling *before* the train-test split. This leads to data leakage, which is a major flaw in machine learning model development.

**Strengths:**

*   **Comprehensive Model Evaluation:** The code correctly trains multiple models, calculates a good range of evaluation metrics (accuracy, precision, recall, F1, ROC-AUC), and visualizes confusion matrices and ROC curves.
*   **k-fold Cross-Validation:** It correctly implements k-fold cross-validation, though its application in combination with the preprocessing issue makes the results less reliable.
*   **Feature Importance:** It attempts to display feature importance for the best model, handling Logistic Regression and Random Forest appropriately, though the SVM part is somewhat redundant if an RBF kernel is used.
*   **Proper Imports and Structure:** The code is well-structured with clear sections for different parts of the task.

**Areas for Improvement (leading to data leakage):**

*   **Data Leakage in Preprocessing:** The most critical flaw is performing `SimpleImputer.fit_transform(X)` and `StandardScaler.fit_transform(X_imputed)` on the *entire dataset* (`X`) before the train-test split. This means the imputation and scaling parameters (mean/median, std dev, etc.) are learned from both training and testing data, leaking information from the test set into the training process. This inflates evaluation metrics and leads to an overly optimistic assessment of model performance.
    *   **Correction:** `fit_transform` should only be applied to the training data *after* the split, and then `transform` should be applied to the test data. A `Pipeline` is the best practice for handling this robustly, especially during cross-validation.
*   **Cross-Validation Preprocessing:** The cross-validation part `cross_val_score(model, X_scaled, y, cv=cv, scoring='f1')` also suffers from the same data leakage issue because `X_scaled` was preprocessed on the full dataset. For proper cross-validation, preprocessing steps should be *inside* the cross-validation loop or, more robustly, wrapped in a `Pipeline` as shown in the ground truth.
*   **Confusion Matrix Display:** While functional, the confusion matrix plotting is manually implemented. `sklearn.metrics.ConfusionMatrixDisplay` (as used in the ground truth) provides a cleaner, more standardized way to plot confusion matrices.
*   **Feature Importance for SVM:** The `if hasattr(best_model, 'coef_')` check for SVM is for linear SVMs. The prompt implies general SVM (`SVC`), which by default uses a non-linear (RBF) kernel, and thus doesn't have `coef_`. The ground truth correctly points this out, whereas the generated code tries to plot coefficients, which would fail for a non-linear SVM.
*   **Matplotlib vs. Seaborn:** The ground truth used `seaborn` for better-looking plots (e.g., heatmaps for confusion matrices), which isn't strictly required but is a good practice for data science visualization.
*   **Error Handling:** The ground truth includes basic error handling for file not found, which is a good practice.
*   **Missing `zero_division` in `precision_score` and `f1_score`:** While `precision_score` is handled by `zero_division=0`, `f1_score` also benefits from it.
*   **Minor Plotting Fixes:** The x-axis labels for feature importance plots sometimes overlap due to rotation, which could be improved by using `plt.tight_layout()` or adjusting figure size, which the ground truth does.

Overall, the generated code demonstrates a good understanding of the overall machine learning workflow and most of the specific requirements, but the fundamental data leakage issue is a critical flaw that affects the validity of the results.",gpt-4.1,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest, f_classif, RFE

# Load data
df = pd.read_csv('water_potability.csv')

# Impute missing values (mean imputation)
imputer = SimpleImputer(strategy='mean')
df[df.columns] = imputer.fit_t...",8,"The generated code correctly addresses most of the requirements of the task.

**Strengths:**
*   **Comprehensive Feature Engineering:** It successfully creates ratio features, interaction terms (using nested loops for all pairwise combinations), and polynomial (squared) features. This is a good implementation of the feature engineering step.
*   **Correct Missing Value Handling:** Uses `SimpleImputer` which is a clean way to handle missing values.
*   **Correlation Analysis:** Correctly calculates the correlation matrix and identifies the top 5 most correlated features with 'Potability'.
*   **Multiple Feature Selection Methods:** Implements both `SelectKBest` (using `f_classif`) and `RFE` with `RandomForestClassifier`, as requested.
*   **Advanced Visualizations:** Generates a correlation heatmap, a feature importance bar plot from Random Forest, and pairwise scatter plots for the top correlated features, all using `matplotlib` and `seaborn` effectively.
*   **Comparison of Feature Sets:** Provides a clear comparison table showing the overlap of features selected by different methods, which is a good analytical output.
*   **Code Structure:** The code is well-structured with clear comments and logical flow for each step.

**Areas for Improvement (Why it's not a 9 or 10):**

1.  **Polynomial Features Detail:**
    *   The ground truth code `PolynomialFeatures(degree=2, include_bias=False)` creates not just squared terms, but also cross-product terms (e.g., x1*x2). The generated code's polynomial feature engineering only creates squared terms (`df[f'{col}_squared'] = df[col]**2`), which is a subset of what `PolynomialFeatures` typically generates at `degree=2`. While it does create explicit interaction terms via nested loops, the `PolynomialFeatures` estimator itself could have handled more broadly. The task implied using `PolynomialFeatures` generally, and its typical usage at `degree=2` includes both.
    *   The generated code also applies polynomial features to *all* original columns, which might lead to an excessively high number of features (e.g., if there are 8 original features, `PolynomialFeatures(degree=2)` will generate many more than just 8 squared terms + 28 interaction terms). The ground truth was more selective (`poly_features_cols`) to avoid an explosion of features, which is good practice.

2.  **Visualization Scale/Clarity:**
    *   The correlation heatmap in the generated code uses `df[features.columns.tolist() + ['Potability']].corr()`. With too many engineered features, this heatmap becomes unreadable without `annot=False` (which is used) and more sophisticated sizing/font management (which the ground truth attempts). The ground truth's heatmap is `figsize=(20, 18)` and `fontsize=8`, which is more appropriate for a large number of features. The generated code's `figsize=(18, 14)` might still be too small if many features are generated.

3.  **Explicit Output:** The generated code directly plots the visualizations but doesn't print the lists of selected features by `SelectKBest` or `RFE`, or the top 5 correlated features, which the ground truth does and can be useful for direct assessment. It only provides the final comparison table.

Overall, a very strong submission that fulfills almost all requirements comprehensively. The points for improvement are minor enhancements or deeper adherence to nuances of the task's general phrasing (like `PolynomialFeatures`).",gpt-4.1,LLM_judge_config
