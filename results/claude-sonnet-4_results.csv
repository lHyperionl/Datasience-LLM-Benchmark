task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1) List all column names and identify data types
print(""Column names and data types:"")
print(df.dtypes)
print(""\n"" + ""=""*50 + ""\n"")

# 2) Basic descriptive statistics for numerical columns
print(""Descriptive statistics for numerical columns:"")
print(df.describe())
print(""\n"" + ""=""*50 + ""\n"")

# Additional descriptive statistics
print(""Additional statistics:"")
for col in df.select_dtype...",7,"The generated code addresses most of the requirements but has a few shortcomings compared to the ground truth and the full prompt.

**Strengths:**
*   **Loads CSV Correctly:** Successfully loads the `winequality-red.csv` file.
*   **Column Names and Data Types (Partial):** It prints `df.dtypes` which lists column names and their pandas data types.
*   **Descriptive Statistics:** `df.describe()` is used, which correctly provides mean, std, min, max, and quartiles for numerical columns. It then correctly iterates through numerical columns to print individual statistics.
*   **Total Rows and Columns:** Correctly reports the total number of rows and columns using `df.shape`.

**Areas for Improvement / Missing Aspects:**
*   **Data Type Identification (Clarity):** The prompt asked to ""identify the data type of each column (numerical, categorical, etc.)"". While `df.dtypes` shows pandas types, the ground truth explicitly maps these to ""Numerical"" or ""Categorical/Ordinal"" for clearer identification, which the generated code lacks in its initial output. The ""Column analysis"" section *attempts* classification but isn't as direct as the ground truth's initial type identification.
*   **Categorical/Ordinal Identification (Accuracy):** The ""Column analysis"" section in the generated code tries to identify categorical/ordinal columns based on `object` dtype or `unique_values <= 10`. While a reasonable heuristic, it incorrectly classifies all columns in `winequality-red.csv` (which are all numerical, though 'quality' is often treated as ordinal) as `Likely categorical/ordinal` if their unique count is low. The ground truth explicitly states that all columns are numerical and provides a nuanced comment about 'quality' being treated as ordinal. The generated code's current heuristic for ""Likely categorical/ordinal"" might misclassify truly numerical but discrete columns.
*   **Readability/Presentation:** The use of `print(""\n"" + ""=""*50 + ""\n"")` separating sections is less informative than the descriptive headers used in the ground truth (e.g., ""--- Column Names and Identified Data Types ---"").
*   **Robustness:** The ground truth includes error handling for `FileNotFoundError`, which is good practice for common issues. The generated code does not.
*   **Redundancy in Descriptive Statistics:** While breaking down individual statistics is good, using `df.describe()` first already provides all required statistics (mean, median/50th percentile, std, min, max, 25th/75th percentiles). The subsequent loop to print the same statistics individually is somewhat redundant and makes the output longer than necessary for the prompt's request.",claude-sonnet-4,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 1) Display the first 10 rows
print(""First 10 rows:"")
print(df.head(10))
print(""\n"")

# 2) Show the shape of the dataset
print(""Dataset shape (rows, columns):"")
print(df.shape)
print(""\n"")

# 3) List all column names and their data types
print(""Column names and data types:"")
print(df.dtypes)
print(""\n"")

# 4) Display the top 10 games by Global_Sales
print(""Top 10 games by Global_Sales...",8,"The generated code correctly addresses most of the requirements and demonstrates a good understanding of Pandas for EDA.

**Strengths:**
*   **Correctness:** All requested tasks are performed, and the output is largely correct.
*   **Conciseness:** Uses `df.nlargest()` which is a clean way to get top N rows.
*   **Clarity:** Outputs are clearly labeled with print statements.
*   **Additional Insights:** Provides `nunique()` for unique platforms/genres and total/percentage missing values, which are good additions for EDA.

**Areas for Improvement (Why not a 10):**
*   **Robustness (File Not Found):** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is a good practice for robust code. The generated code assumes the file will always be present.
*   **Output Formatting:** The ground truth uses `df.info()` for column names and dtypes, which provides more comprehensive information (non-null counts, memory usage) than `df.dtypes`.
*   **Detail in Missing Values:** While it shows total missing values and percentage, the ground truth explicitly filters to show only columns with *any* missing values, which can be more informative for large datasets where many columns have zero missing values.
*   **Top Games Columns:** The task asked for specific columns for the top 10 games (`Name`, `Platform`, `Genre`, `Publisher`, `Global_Sales`), but the generated code only included `Name`, `Platform`, `Global_Sales`.
*   **No Exit on Error:** In a real script, if the file isn't found, subsequent operations will fail. The ground truth exits gracefully.
*   **Unnecessary Import:** `import numpy as np` is included but not used in the provided code.",claude-sonnet-4,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values in the Year column by filling with median year
df['Year'].fillna(df['Year'].median(), inplace=True)

# Remove games with missing Publisher information
df = df.dropna(subset=['Publisher'])

# Create a new column 'Years_Since_Release'
df['Years_Since_Release'] = 2023 - df['Year']

# Remove duplicate games based on Name and Platform
df = df.drop_duplicates(subset=[...",8,"The generated code correctly addresses most of the requirements. It loads the data, handles missing 'Year' values, removes missing 'Publisher' data, creates the `Years_Since_Release` column, removes duplicates, filters low-selling games, and displays the final shape and head.

However, there are a few areas for improvement:
1.  **Robustness for 'Year' column**: The ground truth converts the 'Year' column to numeric with `errors='coerce'` before filling NaNs. This handles cases where 'Year' might contain non-numeric strings, which the generated code doesn't explicitly account for. The ground truth also converts 'Year' to `int` after filling, which is good practice.
2.  **Robustness for 'Global_Sales'**: Similar to 'Year', `Global_Sales` might not always be purely numeric. The ground truth converts it to numeric with `errors='coerce'` before filtering and drops rows where it became NaN. The generated code assumes `Global_Sales` is already numeric and doesn't handle potential non-numeric entries or NaNs in that column, which could lead to errors or unexpected filtering behavior if the column isn't clean.
3.  **Years_Since_Release validation**: The ground truth adds a check `max(0, x)` for `Years_Since_Release` to ensure values are not negative, which can happen if a game's release year is in the future relative to `CURRENT_YEAR` or if the median year is unusually high. While less common, it's a good practice for robustness.
4.  **No error handling**: The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions, making it more robust. The generated code does not include this.
5.  **Output Messages**: The ground truth provides more informative print statements throughout the process, showing the shape after each step, which is useful for debugging and understanding the data transformation.

Overall, it's a good attempt that covers the core logic but could be improved in terms of robustness and comprehensive error handling.",claude-sonnet-4,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 1) Bar chart showing top 10 gaming platforms by total global sales
plt.figure(figsize=(12, 6))
platform_sales = df.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False).head(10)
plt.bar(platform_sales.index, platform_sales.values)
plt.title('Top 10 Gaming Platforms by Total Global Sales')
plt.xlabel('Platform')
plt.ylabel('Global Sales...",7,"The generated code correctly addresses most of the task requirements by creating the specified visualizations (bar, pie, histogram, scatter, line). It uses `matplotlib` and `pandas` appropriately, and includes proper titles and labels for the charts.

However, there are a few areas for improvement:
1.  **Robustness in Data Loading/Handling:** The generated code lacks error handling for the `df = pd.read_csv('video_games_sales.csv')` call, which the ground truth includes using a `try-except` block for `FileNotFoundError`. This makes the code less robust.
2.  **Data Cleaning/Preprocessing:** The ground truth includes essential data cleaning steps for `Year_of_Release` (converting to numeric, handling NaNs) and ensuring sales columns are numeric. The generated code assumes the data is clean and directly uses it. This can lead to errors or incorrect plots if the raw data is not perfectly clean (e.g., if 'Year' is not numeric or contains NaNs, or if sales columns have non-numeric entries).
3.  **Specific Plot Issues:**
    *   **Line Plot (Games per Year):** The generated code uses `df['Year'].value_counts()`. The dataset column is actually `Year_of_Release`, not `Year`. The ground truth correctly uses `df['Year_of_Release']` and also includes important pre-processing for this column and filtering of years to make the plot more meaningful (e.g., filtering out years outside a typical range).
    *   **Histogram:** The ground truth includes a `range` parameter for the histogram to focus on the majority of sales (e.g., up to the 99th percentile), which is a good practice for highly skewed data like sales. The generated code plots the full range, which might not be as informative due to extreme outliers.
    *   **Pie Chart:** The ground truth handles the ""other"" category for genres with very few games to improve readability of the pie chart. The generated code plots all genres, which could result in a cluttered pie chart if there are many minor genres.

Overall, it's a good attempt that gets the basic visualizations right. The ground truth provides more robust and nuanced data handling and plotting techniques that are common in data science tasks.",claude-sonnet-4,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('diabetes.csv')

# 1) Display the first 10 rows
print(""First 10 rows:"")
print(df.head(10))
print(""\n"" + ""=""*50 + ""\n"")

# 2) Show the shape of the dataset
print(f""Dataset shape: {df.shape}"")
print(f""Rows: {df.shape[0]}, Columns: {df.shape[1]}"")
print(""\n"" + ""=""*50 + ""\n"")

# 3) List all column names and their data types
print(""Column names and data types:"")
print(df.dtypes)
print(""\n"" + ""=""*50 + ""\n"")

# 4) Cal...",7,"The generated code correctly addresses most of the requirements and produces the expected output for the given task. It loads the data, displays the head, shape, dtypes, missing values, and summary statistics accurately. The calculation for the percentage of diabetic vs non-diabetic patients is also correct.

However, there are a few areas for improvement:

1.  **Robustness in File Loading:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions during file loading, making the code more robust. The generated code assumes the file will always be present, which is not ideal for a general solution.
2.  **`df.info()` vs `df.dtypes`:** For task 3, `df.info()` provides more comprehensive information (including non-null counts) which is generally preferred in EDA over just `df.dtypes` as used in the generated code. While `df.dtypes` is correct, `info()` is better.
3.  **Specific Columns for Zero Values:** Task 7 specifically asks to ""Identify any columns with zero values *that might represent missing data*."" The ground truth thoughtfully defined `COLUMNS_TO_CHECK_FOR_ZERO_AS_MISSING` because some columns (like `Pregnancies` or `Outcome`) can legitimately have zero values and don't represent missing data. The generated code simply checks *all* columns for zeros, which includes `Pregnancies` and `Outcome` and might lead to misinterpretation of ""missing data."" For example, 0 pregnancies or an outcome of 0 (non-diabetic) are valid values, not missing data.
4.  **Clarity of Output:** The ground truth includes more descriptive print statements and section headers (`--- 1. First 10 rows...`, `--- 2. Shape of the dataset...`) which significantly improves the readability and understanding of the output for an exploratory analysis. The generated code's print statements are less informative.
5.  **`df.isnull().sum()` interpretation:** For missing values, if there are no missing values, `df.isnull().sum()` still prints a list of all columns with 0 as the count. The ground truth code handles this by printing ""No missing values found"" if the total sum is zero, which is cleaner.

Overall, it's a good attempt, but lacks some of the finer details and robustness present in the ground truth for a comprehensive EDA script.",claude-sonnet-4,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Replace zero values with NaN for specific columns
columns_with_zero_issues = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_with_zero_issues] = df[columns_with_zero_issues].replace(0, np.nan)

# Handle missing values by filling with median
for col in columns_with_zero_issues:
    df[col].fillna(df[col].median(), inpl...",7,"The generated code correctly addresses most of the requirements.

**Strong points:**
*   Correctly loads the dataset.
*   Accurately identifies and replaces zero values in specified columns with NaNs.
*   Correctly fills missing values with the median of their respective columns.
*   Correctly removes duplicate rows.
*   Creates the `BMI_Category` column accurately using a custom function.
*   Applies `StandardScaler` to numerical features.
*   Displays the cleaned dataset's shape and first 5 rows.

**Areas for Improvement (Why it lost points):**
*   **Robustness (Loading Data):** The generated code lacks error handling for `FileNotFoundError` when loading the CSV. The ground truth includes a `try-except` block, which makes it more robust.
*   **Dynamic Numerical Feature Selection:** The generated code hardcodes `numerical_features`. While these are likely correct for this dataset, a more robust solution (like the ground truth) would dynamically select all numerical columns, excluding the target variable (`Outcome`), making it more adaptable to datasets with varying column names or structures.
*   **BMI Category Bins:** The `categorize_bmi` function uses `18.5 <= bmi < 25` and `25 <= bmi < 30`. This is slightly off from the standard definitions for 'Normal' and 'Overweight' where the upper bound is typically inclusive or the categorization handles the exact boundary point differently using `pd.cut` right-inclusive `right=True` which is more standard. The prompt specifies ""18.5-24.9"" and ""25-29.9"", which `pd.cut` with `right=True` handles more precisely than simple `<` comparisons for bins. The use of `pd.cut` in the ground truth is more idiomatic and less error-prone for binning.
*   **Informative Output:** The generated code provides minimal print statements. The ground truth provides more verbose output, indicating success or warnings for various steps (e.g., ""Dataset loaded successfully"", ""Removed X duplicate rows"", ""Scaled numerical features""). This is good practice for data cleaning scripts.
*   **Commentary/Structure:** The ground truth code breaks down each step with clear section comments, which enhances readability and understanding of the code's purpose. The generated code has some comments but less structured.",claude-sonnet-4,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from matplotlib.gridspec import GridSpec

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Set style
plt.style.use('default')
sns.set_palette(""husl"")

# 1. Correlation heatmap
plt.figure(figsize=(12, 8))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
            square=True, linewidths=0.5)
plt.title('Correlation Heatmap of Diabetes ...",9,"The generated code successfully creates all the requested visualizations with proper titles and labels, demonstrating a good understanding of both `matplotlib` and `seaborn`.

**Strong points:**
- All 7 visualization types are correctly implemented.
- Proper use of `matplotlib.pyplot` and `seaborn` for plotting.
- Appropriate use of `plt.figure`, `plt.subplot`, `plt.title`, `plt.xlabel`, `plt.ylabel`, and `plt.show` for structuring and labeling plots.
- Correct use of `hue` in scatter plots and pair plots to differentiate by 'Outcome'.
- Good use of `figsize` for better plot readability.
- The use of `select_dtypes(include=[np.number])` for identifying numerical columns is robust.
- Added colorbars for scatter plots which was a nice touch.

**Areas for improvement:**
- **Inconsistency in `Outcome` labeling:** For box plots and scatter plot colorbars, the labels are `(0=No, 1=Yes)` or `(0=No Diabetes, 1=Diabetes)`, whereas the pie chart uses `Non-Diabetic` and `Diabetic`. It would be better to be consistent across all plots, possibly using the more descriptive labels for the legend and axis ticks where appropriate.
- **Set Style:** The ground truth sets `sns.set_style(""whitegrid"")` which is generally better for readability than `plt.style.use('default')` coupled with `sns.set_palette(""husl"")`. The default style often lacks gridlines which can be helpful.
- **Box Plot Feature Columns:** The generated code excludes 'Outcome' from `feature_cols` for box plots, which is correct. However, for the heatmap and histograms, it uses `df.select_dtypes(include=[np.number]).columns`, which will include 'Outcome'. While `corr()` on `Outcome` is fine, making `numerical_cols` consistent (i.e., excluding `Outcome` if only statistical features are desired) would be slightly cleaner, aligning more with the ground truth's `numerical_cols = df.columns.drop('Outcome')`. For this task, it's a minor point as `Outcome` is numerical.
- **Code Organization:** The ground truth includes `print` statements for dataset loading and info, which helps verify the initial load and dataset structure. While not strictly a visualization requirement, it's good practice for debugging and understanding the data.
- **Redundant Import:** `from matplotlib.gridspec import GridSpec` is imported but not explicitly used in the provided code snippet.
- **Pair Plot Diagonal:** The pair plot uses `diag_kind='hist'`, while the ground truth uses `diag_kind='kde'`. While `hist` is acceptable, `kde` often provides a smoother representation of the distribution. It's a stylistic choice rather than a correctness issue.",claude-sonnet-4,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# 1) Display the first 10 rows
print(""First 10 rows of the dataset:"")
print(df.head(10))
print(""\n"" + ""=""*50 + ""\n"")

# 2) Show the shape of the dataset
print(f""Dataset shape: {df.shape}"")
print(f""Number of rows: {df.shape[0]}"")
print(f""Number of columns: {df.shape[1]}"")
print(""\n"" + ""=""*50 + ""\n"")

# 3) List all column names and their data types
print(""Column names and data ...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Correctness:** All primary tasks (loading data, display head, shape, dtypes, happiness factors, top 10, missing values) are implemented correctly.
- **Clarity and Readability:** The code is well-structured with clear comments indicating each task.
- **Completeness:** It performs all requested EDA steps.
- **Use of Pandas:** Effectively uses Pandas functions for data manipulation and analysis.

**Areas for Improvement:**
- **Robustness (File Handling):** The generated code does not include error handling for `FileNotFoundError` when loading the CSV, which is crucial for robust scripts. The ground truth includes a `try-except` block for this, which is good practice.
- **Detailed `df.info()` vs. `df.dtypes`:** While `df.dtypes` lists data types, `df.info()` (used in the ground truth) provides a more comprehensive summary, including non-null counts, which is often more useful for initial EDA.
- **Output Verbosity for Shape:** The generated code prints the shape in three lines (total shape, rows, columns). While correct, printing just `df.shape` is usually sufficient and more concise, as `df.info()` already provides row counts.
- **`Regional Indicator` Column Check:** Similar to the ground truth, adding a check to ensure `Regional indicator` and other key columns exist before attempting to access them would make the code more robust against variations in dataset schema.
- **Separators:** The separators `""=""*50` are shorter than `""=""*70` in the ground truth, which is a minor aesthetic point but can impact readability in some console environments.",claude-sonnet-4,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from scipy import stats

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Handle missing values
numerical_cols = df.select_dtypes(include=[np.number]).columns
categorical_cols = df.select_dtypes(include=['object']).columns

for col in numerical_cols:
    df[col].fillna(df[col].mean(), inplace=True)

for col in categorical_cols:
    df[col].fillna(df[col].mode()...",8,"The generated code correctly addresses most of the requirements.

**Strong points:**
*   **Missing values:** Correctly handles missing numerical values with the mean and categorical with the mode.
*   **Derived features:**
    *   Creates 'Happiness\_Category' using `pd.cut` with appropriate bins.
    *   Creates 'GDP\_Category' using `pd.cut` and quartiles, which is a good approach.
    *   Calculates 'Well\_being\_Score' accurately.
*   **Outlier removal:** Implements IQR method correctly for 'Ladder score'.
*   **Standardization:** Uses `StandardScaler` and applies it to the correct set of numerical features.
*   **Encoding:** Correctly applies `LabelEncoder` to 'Regional indicator'.
*   **Display:** Prints the shape and head as requested.
*   **Imports:** Includes all necessary libraries.

**Areas for improvement:**
*   **Error Handling (Loading):** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the dataset, which is a good practice for robustness. The generated code lacks this.
*   **`GDP_Category` (`pd.qcut` vs `pd.cut`):** While `pd.cut` with custom quartile bins works for `GDP_Category`, `pd.qcut` (as used in the ground truth) is a more direct and common way to create quantile-based bins, and often handles edge cases (like identical values at quartile boundaries) more gracefully. The generated code's definition of bins for `GDP_Category` might not correctly handle cases where the first bin starts at 0, and the first quantile is greater than 0, resulting in `NaN` values for data points between 0 and the first quartile. `pd.qcut` automatically determines the appropriate bins.
*   **`Happiness_Category` (`np.select` vs `pd.cut`):** Both `np.select` (Ground Truth) and `pd.cut` (Generated) are valid for creating `Happiness_Category`. `pd.cut` is often slightly cleaner for binning numerical data into predefined categories.
*   **Code Clarity/Comments:** The ground truth includes more comments and print statements that explain each step, which improves readability and debuggability in a multi-step data preprocessing script. The generated code is more concise but could benefit from similar clarity.
*   **Numerical Features for Standardisation:** The generated code hardcodes the `numerical_features` list. While this works, a more dynamic approach like `df.select_dtypes(include=np.number).columns.tolist()` (as in the ground truth for identifying numerical columns) followed by excluding non-scalable columns (if any, like 'Country name' if it were included in the numerical `select_dtypes` initially) is often more robust to schema changes. In this specific case, the hardcoded list is mostly fine, but it seems to have missed 'Country_name', 'Regional_indicator' columns which are not numerical and should not be scaled. It also missed 'Well_being_Score' which is a newly created numerical feature. *Correction: 'Well_being_Score' *was* included in the hardcoded list for standardization, so that aspect is correct.* The hardcoded list specifically includes ""Ladder score"", ""Logged GDP per capita"", ""Social support"", ""Healthy life expectancy"", ""Freedom to make life choices"", ""Generosity"", ""Perceptions of corruption"", and ""Well_being_Score"". These are indeed the core numerical features that should be scaled. `Country name` and `Regional indicator` are correctly excluded as they are not numerical. The list `df.select_dtypes(include=np.number).columns.tolist()` would capture all these numerical features, and exclude the non-numerical ones naturally. So the hardcoded list is functionally correct.

Overall, it's a very good attempt that fulfills most of the requirements. The minor points are mostly about robustness, best practices, and slight preferences in method choice (`pd.qcut` is generally preferred for quantiles).",claude-sonnet-4,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.offline as pyo
import warnings
warnings.filterwarnings('ignore')

# Set style for matplotlib/seaborn
plt.style.use('default')
sns.set_palette(""husl"")

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Clean column names
df.columns = df.columns.str.strip()...",9,"The generated code is highly comprehensive and addresses almost all requirements of the task. It successfully creates a wide variety of visualizations using the specified libraries (Plotly, Matplotlib, Seaborn) with appropriate titles and labels.

**Strong Points:**
*   **Comprehensive Visualizations:** All requested visualization types (world map, bar chart, heatmap, scatter plots, box plots, radar chart) are implemented.
*   **Proper Library Usage:** Utilizes `plotly.express` for the map, `seaborn` and `matplotlib` for most statistical plots, and `plotly.graph_objects` for the radar chart, demonstrating good library selection.
*   **Good Formatting and Labels:** Titles, axis labels, and legends are generally clear and well-placed.
*   **Column Renaming:** The code includes a robust column renaming mechanism, handling cases where old names might not exist, which is good practice.
*   **Scatter Plot Enhancements:** Adds trend lines to scatter plots, which is a nice touch for showing relationships.
*   **Heatmap Masking:** Uses `np.triu` for the heatmap mask, improving readability by removing redundant data.
*   **Radar Chart Normalization:** Correctly normalizes data for the radar chart, which is crucial for meaningful comparison.
*   **Additional Information:** Includes summary statistics at the end, providing extra value.

**Areas for Improvement (Minor):**
*   **Radar Chart Features:** The ground truth included 'Perceptions of corruption' in its radar chart features, while the generated code excluded it. While not a critical error, consistency with the ground truth's chosen factors would have been ideal.
*   **Scatter Plot Hue (Minor):** The ground truth's scatter plots used `hue='Regional indicator'`, allowing for quick visual grouping by region. The generated code uses `c=df['Happiness_Score']`, which colors based on happiness score within each plot, still informative but different from the ground truth's approach.
*   **File Not Found Handling:** The ground truth included a `try-except` block for `FileNotFoundError` when loading the CSV. While the task didn't explicitly demand error handling, it's a good practice for robust code.
*   **Matplotlib `plt.figure` and `plt.show()` placement:** Some `plt.show()` calls are immediately after a single plot, when `plt.tight_layout()` might benefit from being called before `plt.show()` to ensure all subplot elements are correctly adjusted, especially when multiple plots are expected.

Overall, the generated code is excellent and meets the requirements effectively. The minor points are mostly about stylistic choices or robustness features that were present in the ground truth but not explicitly demanded by the prompt.",claude-sonnet-4,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# 1) Load the dataset and parse the 'date' column as datetime objects
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

# 2) Display the first 10 rows
print(""First 10 rows:"")
print(df.head(10))
print(""\n"")

# 3) Show the shape of the dataset (rows and columns)
print(f""Dataset shape: {df.shape}"")
print(""\n"")

# 4) List all column names and their data types
print(""Column names and data types:"")
print(df.dtypes)...",8,"The generated code correctly addresses most of the requirements.

**Strong points:**
*   Successfully loads the dataset and parses dates.
*   Correctly displays the first 10 rows and dataset shape.
*   Shows column names and dtypes effectively using `df.dtypes` (though `df.info()` from ground truth provides more detail including non-null counts, `dtypes` is sufficient for the prompt).
*   Correctly calculates total cases/deaths by continent.
*   The logic for identifying the top 5 locations based on the *latest* date is efficient and correct (`idxmax()` is a good approach here).
*   Correctly calculates and displays basic summary statistics for `new_cases` and `new_deaths`.
*   Correctly counts and displays missing values.
*   Includes all necessary imports (`pandas`).

**Areas for improvement:**
*   **Robustness in loading (1):** The ground truth includes `try-except` blocks for file loading, which is a good practice for robustness in real-world scenarios. The generated code assumes the file exists.
*   **Column Type Handling (5, 6, 7):** The generated code assumes `total_cases`, `total_deaths`, `new_cases`, and `new_deaths` are already numeric and handles potential non-numeric values implicitly (e.g., `sum()` might work with numeric strings, but `.mean()` etc. would fail or give incorrect results if mixed types are present). The ground truth's explicit `pd.to_numeric(errors='coerce')` makes the code significantly more robust to dirty data, especially in the context of `total_cases` for the top locations and summary statistics. This is a common issue with synthetic data that might have numbers stored as strings.
*   **Output Clarity (7):** While the summary statistics for `new_cases` and `new_deaths` are calculated correctly, they are printed separately. The ground truth combines them into a single `agg` call, which provides a more compact and readable output table, especially for comparing statistics across different metrics.
*   **Error Handling/Column Existence (General):** The ground truth checks for the existence of columns before attempting operations like grouping or calculating statistics. While less critical for perfectly structured synthetic data, this makes code more resilient if columns are missing or misspelled, preventing runtime errors.",claude-sonnet-4,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1) Calculate and report the average quality
mean_quality = df['quality'].mean()
print(f""Average quality of red wines: {mean_quality:.2f}"")

# 2) Group by quality and calculate average alcohol content
quality_alcohol = df.groupby('quality')['alcohol'].mean()
print(""\nAverage alcohol content by quality:"")
print(quality_alcohol)

# 3) Create bar chart
pl...",8,"The generated code correctly addresses all three main requirements of the task: calculating the mean quality, grouping by quality to find average alcohol content, and creating a visualization. It uses appropriate libraries and follows common data science practices.

However, there are a few minor areas for improvement that prevent a higher score:
1.  **Error Handling:** The generated code lacks robust error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth. While not explicitly asked for, it's good practice for production-ready code.
2.  **Visualization Aesthetics/Clarity:** The chosen `plt.bar` directly works, but the ground truth's use of `seaborn.barplot` (which is often preferred for categorical bar charts) and additional styling (palette, specific font sizes, `tight_layout`, and especially data labels on bars) makes the visualization significantly more informative and aesthetically pleasing. The ground truth example also includes a `sns.set_style(""whitegrid"")` for a nicer default look.
3.  **Completeness of Bar Chart Requirement:** While it creates a bar chart, the prompt asked for ""a bar chart visualizing the relationship..."". The ground truth's chart is much better equipped for this visualization due to the use of `seaborn` and the added labels, making it easier to read and interpret precise values.",claude-sonnet-4,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from datetime import datetime

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Parse the 'date' column as datetime objects
df['date'] = pd.to_datetime(df['date'])

# Handle missing values in 'hosp_patients' and 'icu_patients' by filling with 0
df['hosp_patients'] = df['hosp_patients'].fillna(0)
df['icu_patients'] = df['icu_patients'].fillna(0)

# Fill missing values in numerical columns with mean
numerical_cols = ['positive_ra...",8,"The generated code correctly addresses most of the requirements. It loads the data, parses the 'date' column, handles missing values for 'hosp_patients' and 'icu_patients' with 0, and for 'positive_rate' and 'reproduction_rate' with the mean. It also correctly calculates 'mortality_rate' and 'vaccination_rate_per_capita', handling division by zero effectively using `np.where`. Filtering for 2021 data and displaying the shape and head are also correct.

Areas for improvement:
1.  **Robustness for mean calculation:** The code for filling missing numerical values with the mean doesn't explicitly convert columns to numeric before calculating the mean, which *could* lead to errors if the columns were not inferred as numeric by `pd.read_csv`. The ground truth includes `pd.to_numeric` for this, making it more robust. While pandas often infers correctly, explicitly ensuring numeric types is good practice.
2.  **Error Handling:** The ground truth code includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.
3.  **Use of `inplace=True` or reassignmentconsistency:** In the missing value handling, the ground truth uses `inplace=True` for 'hosp_patients' and 'icu_patients' but then reassigns for 'positive_rate' and 'reproduction_rate'. The generated code consistently reassigns, which is generally considered better practice in modern pandas as it avoids potential `SettingWithCopyWarning` and is often more explicit. However, the ground truth's approach is not strictly wrong. It's a minor stylistic point.
4.  **`datetime` import:** `from datetime import datetime` is imported but not used directly in the code, as `pd.to_datetime` handles the conversion. It's not harmful but unnecessary.
5.  **Clarity for Division by Zero Handling:** While `np.where` is efficient for handling the 0-division case, the subsequent `.replace([np.inf, -np.inf, np.nan], 0)` is still necessary if non-zero divisions result in `inf` or if (less likely with `np.where`) `NaN`s persist from original data. The ground truth separates `inf` and `NaN` replacement, which can sometimes be clearer, but the generated code's combined `replace` is functional.",claude-sonnet-4,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# Load the dataset and parse the date column
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

# Set up the plotting style
plt.style.use('default')
sns.set_palette(""husl"")

# 1) Time series line plot of 'new_cases' for India
plt.figure(figsize=(12, 6))
india_data = df[df['location'] == 'India'].sort_values('date')
plt.plot(india_data['date'], in...",7,"The generated code largely addresses the requirements of the task with generally correct implementations for each visualization type. However, there are significant improvements that could be made to bring it closer to an expert-level solution.

**Strong Points:**
*   **Correctness:** All five requested visualization types are generated correctly.
*   **Data Loading and Parsing:** Handles `date` parsing correctly.
*   **Titles and Labels:** All plots include appropriate titles and axis labels.
*   **Imports:** Includes necessary libraries (`pandas`, `matplotlib.pyplot`, `seaborn`).

**Areas for Improvement:**

1.  **Bar Chart (Total Deaths by Continent):** The logic for calculating `total_deaths` for continents is flawed. It sums *all* entries for 'total_deaths' for a given continent, which is incorrect. 'total_deaths' is a cumulative metric, so summing it over all dates will vastly inflate the numbers. The correct approach, as demonstrated in the ground truth, is to take the *latest* `total_deaths` value for each *country* (location) and then sum these latest values by continent. This is a critical logical error.
2.  **Scatter Plot (GDP per Capita vs. Total Vaccinations):**
    *   **Data Selection for Latest Entry:** While `df.loc[df.groupby('location')['date'].idxmax()]` correctly gets the latest entry for each location, it doesn't explicitly filter out `NaN` values for `gdp_per_capita` or `total_vaccinations`. This can lead to points at (NaN, value) or (value, NaN) which Matplotlib typically ignores, but a robust solution would explicitly drop them.
    *   **Visualization Quality:** It uses `plt.scatter` directly without leveraging Seaborn's `scatterplot`, which offers more advanced features like `hue` or `size` based on other columns (e.g., continent, population), making the visualization more informative. The ground truth example used `hue='continent'` and `size='population'`, which is a better application of the data.
    *   **Scales:** `gdp_per_capita` and `total_vaccinations` often have skewed distributions, and using log scales (as in the ground truth) can significantly improve the interpretability of the scatter plot by spreading out clustered points and compressing outliers. The generated code uses linear scales.
3.  **Visualization Style and Aesthetics:**
    *   The `sns.set_palette(""husl"")` is a good start, but the plots generally lack the polish and visual enhancements found in the ground truth (e.g., `sns.set_theme(style=""whitegrid"")`, specific color choices, grid lines, `alpha` values, `edgecolor` for histogram).
    *   Using `plt.plot` for the time series instead of `sns.lineplot` doesn't inherently reduce correctness but misses an opportunity to use Seaborn's capabilities for potentially richer plots (though `plt.plot` is perfectly acceptable for a simple line plot).
4.  **Error Handling:** The generated code lacks error handling for file not found or parsing issues, which is present in the ground truth and is a good practice for robust code.
5.  **Redundancy/Clarity:** The comment numbers (1, 2, 3, 4, 5) don't match the original prompt's numbering (2, 3, 4, 5, 6), which could cause confusion.

**Conclusion:** The code provides a functional solution to the core plotting requirements. However, the logical error in the bar chart calculation for `total_deaths` per continent, the lack of more insightful features in the scatter plot, and overall less polished aesthetics, along with the absence of error handling, prevent it from achieving a higher score.",claude-sonnet-4,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 1) List all column names and identify data types
print(""Column names and data types:"")
print(df.dtypes)
print()

# 2) Calculate basic descriptive statistics for numerical columns
numerical_columns = ['elixirCost', 'Win Rate', 'Usage']

for col in numerical_columns:
    if col in df.columns:
        print(f""Descriptive statistics for {col}:"")
        print(f""Mean: {df[col].mean()}"")
        print(f""Med...",7,"The generated code correctly loads the dataset, identifies column data types, calculates descriptive statistics for the specified numerical columns, and reports the total number of rows.

**Strong points:**
*   Correctly uses pandas for data loading and manipulation.
*   Accurately extracts column data types using `df.dtypes`.
*   Correctly calculates mean, median, standard deviation, min, and max for the specified columns.
*   Handles the case where a specified numerical column might not exist by checking `if col in df.columns`.
*   Correctly identifies the total number of rows using `len(df)`.

**Areas for improvement:**
*   **Error Handling for File Not Found:** The ground truth includes robust error handling for `FileNotFoundError` and other general exceptions during CSV loading, which is crucial for production-ready code. The generated code does not include this.
*   **Output Formatting:** While the output contains all required information, the ground truth's output is consistently formatted with clear headings (e.g., ""--- Column Names and Data Types ---"", ""--- Descriptive Statistics for Numerical Columns ---""), making it more readable and user-friendly. The generated code's output is a bit more fragmented.
*   **Efficiency for Statistics:** The ground truth uses `df[existing_numerical_cols].agg(['mean', 'median', 'std', 'min', 'max'])`, which is a more concise and often more efficient way to calculate multiple statistics for multiple columns simultaneously compared to iterating and calculating each statistic individually as in the generated code.
*   **Listing Column Names Separately:** The task asked to ""List all column names and identify the data type of each column."" The generated code outputs `df.dtypes` which shows both. However, the ground truth explicitly lists column names first, then the dtypes, which matches the ""list all column names"" part more directly. While `df.dtypes` does implicitly list them, a separate explicit list is slightly better for clarity based on the prompt.

The code is generally correct and functional but lacks the robustness and stylistic improvements found in the best practice solution provided in the ground truth.",claude-sonnet-4,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

df = pd.read_csv('clash_royale_cards.csv')
filtered_df = df[df['elixirCost'] <= 3]
result = filtered_df[['Card', 'Win Rate']].sort_values('Win Rate', ascending=False)
print(result)
```",9,"The generated code correctly performs all the required operations: loading the CSV, filtering by `elixirCost` <= 3, selecting 'Card' and 'Win Rate' columns, and sorting by 'Win Rate' in descending order. It's concise and achieves the task efficiently.

**Strong points:**
*   **Correctness:** All steps are executed accurately to produce the desired output.
*   **Conciseness:** The code is very compact and uses method chaining effectively.
*   **Completeness:** All specific requirements of the prompt are met.

**Areas for improvement (minor):**
*   **Error Handling:** The code lacks error handling for `FileNotFoundError` or other potential issues during CSV loading, which is present in the ground truth. While not explicitly asked for in the prompt's requirements for *this specific task*, it's good practice for robust code.
*   **Readability/Maintainability for Complex Tasks:** For more complex filtering or column selections, breaking down the steps (like in the ground truth) can sometimes improve readability and make debugging easier, though for this simple task, the chaining works well.
*   **Configuration:** The ground truth code uses constants (`ELIXIR_COST_COL`, `MAX_ELIXIR_COST`, etc.) for column names and filter values, which makes the code more maintainable if these values change in the future. The generated code hardcodes these strings and numbers, which is less flexible.

Overall, it's a very good solution to the given task. The missing error handling and configuration are minor points that don't detract significantly from its ability to solve the core problem as stated.",claude-sonnet-4,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Create histogram for elixirCost distribution
plt.figure(figsize=(10, 6))
plt.hist(df['elixirCost'], bins=10, edgecolor='black', alpha=0.7)
plt.title('Distribution of Elixir Cost')
plt.xlabel('Elixir Cost')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)
plt.show()
```",8,"The generated code correctly loads the CSV and generates a histogram for the `elixirCost` column with a title and labeled axes using matplotlib. It fulfills all the stated requirements.

**Strengths:**
*   **Correctness:** The code successfully performs the task of loading data and plotting the histogram.
*   **Completeness:** All requirements (load CSV, histogram for `elixirCost`, title, labeled axes, proper imports) are met.
*   **Readability:** The code is concise and easy to understand.
*   **Appropriate Imports:** Only necessary libraries (pandas, matplotlib) are imported.

**Areas for Improvement (minor):**
*   **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and missing columns. The generated code lacks explicit error handling, which could make it less robust in a real-world scenario where the file might not exist or the column might be misspelled.
*   **Plot Customization:** While `matplotlib.pyplot.hist` works, using `seaborn.histplot` (as in the ground truth) often produces more aesthetically pleasing plots with less effort and offers more options for statistical plots. The `bins=10` is a reasonable default, but explicitly setting bins to cover integer elixir costs (e.g., `range(1, 12)` as in the ground truth) can make the histogram more accurate for discrete data.
*   **Y-axis Label:** ""Frequency"" is correct, but ""Number of Cards"" (as in the ground truth) might be slightly more descriptive for this specific dataset.",claude-sonnet-4,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('iris.csv')

# 1) List all column names and identify data types
print(""Column names and data types:"")
print(df.dtypes)
print()

# 2) Calculate descriptive statistics for numerical columns
numerical_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

print(""Descriptive statistics for numerical columns:"")
for col in numerical_columns:
    print(f""\n{col}:"")
    print(f""Mean: {df[col].mean():.2f}"")
    prin...",8,"The generated code correctly performs all the requested tasks: loading the dataset, identifying column data types, calculating descriptive statistics for the specified numerical columns, and reporting unique species counts. The imports are proper, and the code structure is clear and readable.

Areas for improvement to achieve a higher score:
1.  **Robustness**: The generated code assumes `iris.csv` exists. The ground truth includes a `try-except` block for `FileNotFoundError`, which makes the code more robust.
2.  **Efficiency for Descriptive Stats**: While the loop approach for descriptive statistics is functionally correct, `df[numerical_columns].describe()` or `df[numerical_columns].agg(['mean', 'median', 'std', 'min', 'max'])` would be more idiomatic and often more efficient/concise for computing multiple statistics across multiple columns in pandas, as shown in the ground truth.
3.  **Output Clarity**: The ground truth uses `df.columns.tolist()` to specifically list column names *separate* from data types, which was part of the prompt's first requirement (""List all column names and identify the data type of each column.""). The generated code implicitly shows column names with data types via `df.dtypes` but doesn't explicitly list *only* the column names.
4.  **Error Handling for Columns**: The ground truth includes checks to ensure `numerical_columns` and `Species` columns actually exist in the DataFrame before attempting operations on them, which adds robustness the generated code lacks (e.g., if a column name was misspelled).",claude-sonnet-4,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('iris.csv')

# Filter for Iris-setosa species
iris_setosa = df[df['Species'] == 'Iris-setosa']

# Select and display only SepalLengthCm and SepalWidthCm columns
result = iris_setosa[['SepalLengthCm', 'SepalWidthCm']]
print(result)
```",8,"The generated code correctly addresses the prompt by loading the dataset, filtering for 'Iris-setosa', and selecting/displaying the specified columns. It demonstrates proper use of pandas for these operations. The code is concise and clear.

However, it lacks error handling (e.g., `FileNotFoundError`, `KeyError`), which is present in the ground truth and is crucial for robust production code. While the prompt only asked for the Python code and assumed the CSV is in the current directory, comprehensive error handling makes the code more robust. The ground truth also explicitly defines `DATA_FILE`, which can improve maintainability if the file name needs to change or be parameterized.

To improve, add robust error handling for `FileNotFoundError` and `KeyError` as seen in the ground truth.",claude-sonnet-4,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('iris.csv')

# Create scatter plot
plt.figure(figsize=(8, 6))
for species in df['Species'].unique():
    species_data = df[df['Species'] == species]
    plt.scatter(species_data['SepalLengthCm'], species_data['SepalWidthCm'], 
               label=species, alpha=0.7)

plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.title('Sepal Length vs Sepal Width by Species')
plt.legend()
plt....",9,"The generated code correctly loads the dataset, creates a scatter plot of `SepalLengthCm` vs `SepalWidthCm`, colors the points by `Species`, and includes a title and axis labels. All required imports are present.

**Strong points:**
*   Correctly uses pandas to load the CSV.
*   Correctly uses matplotlib to create the scatter plot.
*   Successfully iterates through unique species to color points accordingly.
*   Includes a title, x-axis label, y-axis label, and a legend as requested.
*   The code is concise and readable.

**Minor areas for improvement (which would earn a 10):**
*   The ground truth explicitly states ""Include proper imports (pandas, matplotlib.pyplot) and assume the CSV file is in the current directory."" but the generated code does not explicitly define the file path as a constant, nor does it include error handling for `FileNotFoundError` or missing columns, which is a robustness feature present in the ground truth. While not strictly ""required"" by the prompt's output format, it's good practice for real-world code.
*   The axis labels ""Sepal Length (cm)"" and ""Sepal Width (cm)"" are slightly different from the column names ""SepalLengthCm"" and ""SepalWidthCm"" specified in the prompt as the axis labels, which the ground truth code used directly. This is a minor stylistic difference, but for precise adherence to variable names in labels, using the exact column names might be preferred based on the task description.",claude-sonnet-4,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('heart.csv')

# 1) List all column names and identify data types
print(""Column names and data types:"")
print(df.dtypes)
print(""\n"")

# 2) Calculate descriptive statistics for specified numerical columns
numerical_columns = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']

print(""Descriptive statistics for numerical columns:"")
for col in numerical_columns:
    print(f""\n{col}:"")
    print(f""Mean: {df[col].mean():.2f...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
*   **Correctness:** It successfully loads the `heart.csv` dataset, prints column names and data types, calculates the required descriptive statistics (mean, median, standard deviation, min, max) for the specified numerical columns, and correctly reports the value counts for the `HeartDisease` column.
*   **Completeness:** It addresses all parts of the prompt.
*   **Clarity and Readability:** The code is well-structured and easy to understand.

**Areas for improvement (preventing a higher score):**
*   **Error Handling:** The ground truth code includes robust `try-except` blocks for `FileNotFoundError` and general exceptions during file loading. The generated code lacks this, which is crucial for production-ready data science scripts. If `heart.csv` is not present, the script will crash.
*   **Output Formatting for Descriptive Statistics:** While the individual statistics are correct, the ground truth uses `df[existing_numerical_cols].describe().loc[['mean', '50%', 'std', 'min', 'max']]` which provides a cleaner, tabular output that is easier to compare across columns at a glance. The generated code prints each statistic for each column, which can become verbose if there are many numerical columns.
*   **Redundant Import:** `import numpy as np` is present but not used in the provided code. While not a major issue, it's good practice to only import necessary libraries.",claude-sonnet-4,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

df = pd.read_csv('heart.csv')
filtered_df = df[(df['Age'] > 50) & (df['HeartDisease'] == 1)]
result = filtered_df[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]
print(result)
```",9,"The generated code correctly addresses all the core requirements of the task: it imports pandas, loads the CSV, filters the data based on `Age > 50` and `HeartDisease == 1`, selects the specified columns, and prints the result. The code is concise and directly solves the problem.

Areas for minor improvement:
- While not explicitly requested by the prompt, adding a function wrapper would improve code organization and reusability, similar to the ground truth.
- Error handling (e.g., `FileNotFoundError`, `KeyError` if columns are missing) is absent. For a robust solution, especially in a production-like environment, this would be crucial. The prompt asked for ""only the Python code without explanations,"" so this might be considered outside the narrow scope, but it's good practice.
- The prompt specified ""Provide only the Python code without explanations,"" and the generated code perfectly adheres to that. The ground truth, however, includes extensive comments and a function definition which might be beyond what was strictly asked for in the prompt's final output format requirement (""Provide only the Python code"").

Overall, for a direct code solution to the prompt's requirements, it's very good.",claude-sonnet-4,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Filter wines with alcohol > 12% AND pH < 3.3
filtered_wines = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]

# Find the wine with the highest quality score from filtered subset
highest_quality_wine = filtered_wines[filtered_wines['quality'] == filtered_wines['quality'].max()]

# Display all features of this specific wine
print(highest_quality_wine)
```",7,"The generated code correctly performs the filtering and identifies the highest quality wine within the filtered subset. It also correctly displays the features of the identified wine.

However, there's a subtle but important issue in how it retrieves the ""highest quality wine."" The line `highest_quality_wine = filtered_wines[filtered_wines['quality'] == filtered_wines['quality'].max()]` will return *all* rows that have the maximum quality. The task asks to identify ""the wine"" (singular) with the highest quality. While pandas often handles this by showing the first one if multiple exist, `idxmax()` is the more idiomatic and precise way to get just one row with the maximum value, and explicitly states that it returns the index of the *first* occurrence.

Additionally, the code lacks proper error handling (e.g., `FileNotFoundError`), which is present in the ground truth and is good practice for robust code. It also doesn't wrap the logic in a function, making it less modular and reusable than the ground truth. The output format for the ""specific wine"" is a DataFrame, which is fine, but the ground truth's `to_string()` for a Series is often cleaner for a single record.",claude-sonnet-4,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('heart.csv')

# Create subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# 1) Histogram for Age distribution
ax1.hist(df['Age'], bins=20, edgecolor='black', alpha=0.7)
ax1.set_title('Distribution of Age')
ax1.set_xlabel('Age')
ax1.set_ylabel('Frequency')

# 2) Bar chart for Sex distribution
sex_counts = df['Sex'].value_counts()
ax2.bar(sex_counts.index, sex_count...",7,"The generated code correctly loads the dataset and creates both a histogram for 'Age' and a bar chart for 'Sex' distribution, fulfilling the core requirements of the task. Titles and labels are also present.

However, there are several areas for improvement to meet the quality and completeness of the ground truth:
1.  **Sex Labeling:** The generated code plots 'Sex' as 0 and 1. The ground truth explicitly maps these to 'Female' and 'Male', which is a much clearer and more informative visualization for the user.
2.  **Robustness:** The ground truth includes error handling (e.g., `FileNotFoundError`, `KeyError`), making it more robust in real-world scenarios. The generated code lacks this.
3.  **Plot Aesthetics/Information (Seaborn vs Matplotlib directly):** While the generated code uses `matplotlib.pyplot` directly for plotting, the ground truth leverages `seaborn` for potentially more aesthetic and informative plots (`sns.histplot` with `kde=True` and `sns.countplot` are generally preferred for statistical plots). The `sns.countplot` specifically handles categorical data counts very well.
4.  **Individual Plots vs Subplots:** The task implies creating separate plots for each visualization. While subplots are acceptable, separating them (as in the ground truth) often provides more flexibility for individual plot sizing and customization without affecting the other. The ground truth also explicitly sets figure sizes.
5.  **Small Details:** The ground truth adds minor improvements like `fontsize` for titles and labels, `grid lines`, and `color` palettes for better readability and presentation. The generated code also has a hardcoded color for the sex bar chart, which is less general than using a palette with `countplot`.",claude-sonnet-4,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 1) List all column names and identify data types
print(""Column names and data types:"")
for col in df.columns:
    print(f""{col}: {df[col].dtype}"")

# 2) Convert streams column to numeric and handle errors
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
nan_count = df['streams'].isna().sum()
print(f""\nNumber of NaN values in streams column after conversion: {nan_count}"")

# 3...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Correct CSV Loading:** Loads the `spotify_2023.csv` file successfully.
- **Column Info:** Correctly iterates and prints column names and their data types, fulfilling the first part of requirement 1.
- **`streams` Conversion:** Successfully converts the `streams` column to numeric, handling errors by coercing to `NaN`, and accurately reports the count of `NaN`s.
- **Descriptive Statistics:** Calculates and prints the mean, median, standard deviation, min, and max for each specified numerical column, fulfilling requirement 3.
- **Unique Artists:** Correctly counts and reports the total number of unique artists.
- **Proper Imports:** Includes necessary imports (`pandas`).

**Areas for Improvement (Why not a 10):**
- **Error Handling for File Loading:** The generated code lacks explicit error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. While the prompt implied the file exists, robust code should include this.
- **Output Format for Column Info:** While correct, the output for column names and data types is manually iterated. Using `df.info()` as in the ground truth is more idiomatic pandas and provides more comprehensive information (like non-null counts) more efficiently, which is generally preferred in such tasks. The prompt asked to ""list all column names and identify the data type of each,"" and `df.info()` does this along with useful additional context.
- **Descriptive Statistics Presentation:** The ground truth uses `df[numerical_cols_for_stats].describe().loc[['mean', '50%', 'std', 'min', 'max']]` which provides a much cleaner, tabular output for all statistics at once, making it easier to read and compare across columns. The generated code prints statistics for each column individually, which is less concise for multiple columns. The ground truth also explicitly checks `col in df.columns` before attempting to get stats, which is good practice. While `df[col]` would raise a KeyError if `col` doesn't exist, the loop structure implies it's handling this, but an explicit check is clearer.",claude-sonnet-4,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Filter for songs released in 2023
df_2023 = df[df['released_year'] == 2023]

# Select required columns
df_selected = df_2023[['track_name', 'artist(s)_name', 'streams']].copy()

# Convert streams to numeric, coercing errors to NaN
df_selected['streams'] = pd.to_numeric(df_selected['streams'], errors='coerce')

# Sort by streams in descending order
df_result = df_selected.sort_values('stre...",9,"The generated code successfully completes almost all tasks. It loads the CSV, filters by year, selects the correct columns, converts 'streams' to numeric with error coercion, and sorts descending.

Strong points:
- Correctly uses pandas functions for all operations (read_csv, filtering, column selection, to_numeric, sort_values).
- Addresses all specified requirements: loading, filtering, column selection, `streams` coercing to numeric, and sorting.
- The use of `.copy()` when selecting columns from `df_2023` is a good practice to avoid `SettingWithCopyWarning` later on, though it's not strictly necessary here as `pd.to_numeric` on a series usually doesn't trigger it when it's done directly on the column.

Area for minor improvement:
- While not explicitly an error in this specific case, the `pd.to_numeric` conversion of the `streams` column is done *after* filtering the DataFrame. Ideally, this conversion should happen on the full DataFrame *before* filtering. This ensures consistency if the filtering conditions later change, or if the `streams` column is used for other operations on the full dataset. This is a best practice for data type handling. The ground truth code handles this by converting `streams` early on the entire DataFrame.
- There's no robust error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. For a production-ready script, this is important. However, the prompt did not explicitly ask for error handling, so it's not penalized heavily.",claude-sonnet-4,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 1) Histogram for BPM distribution
plt.figure(figsize=(10, 6))
plt.hist(df['bpm'], bins=30, edgecolor='black', alpha=0.7)
plt.title('Distribution of BPM')
plt.xlabel('BPM')
plt.ylabel('Frequency')
plt.show()

# 2) Scatter plot for danceability vs energy
plt.figure(figsize=(10, 6))
plt.scatter(df['danceability_%'], df['energy_%'], alpha=0.6)
plt.title('Danceability vs Energy')
...",8,"The generated code correctly addresses all core requirements of the task: loading the CSV, generating a histogram for 'bpm', creating a scatter plot for 'danceability_%' vs 'energy_%', and including titles and labeled axes for both plots. The imports are correct and standard.

Areas for minor improvement:
- **Error Handling**: The generated code lacks error handling for `FileNotFoundError` when loading the CSV or `KeyError` if columns are missing, which is present in the ground truth and makes the code more robust.
- **Plot Display**: The generated code calls `plt.show()` after each plot. While this works for displaying separate plots, it's generally more common and often preferred to create all figures and then call `plt.show()` once at the end to display them all simultaneously. This is a stylistic preference, but makes the output less cluttered in an interactive environment.
- **Axis Label Detail**: The ground truth provides slightly more descriptive axis labels (e.g., 'BPM (Beats Per Minute)', 'Danceability (Percentage)') which adds more context.

Overall, the code is very good and directly solves the problem. The suggested improvements are mainly for robustness and minor stylistic/descriptive enhancements.",claude-sonnet-4,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# 1) List all column names and their inferred data types
print(""Column names and data types:"")
print(df.dtypes)
print()

# 2) Clean Video views, Likes, and Dislikes columns
columns_to_clean = ['Video views', 'Likes', 'Dislikes']

for col in columns_to_clean:
    if col in df.columns:
        # Remove commas and convert to numeric
        df[col] = df[col].astype(str).str.replace(',', '...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
- **Correctness:** The code correctly loads the dataset. It accurately cleans the specified numeric columns by removing commas, converting to numeric types (with `errors='coerce'`), and filling NaNs with 0. It also correctly calculates and prints the descriptive statistics for these columns. Missing values in the 'Category' column are correctly filled with 'Unknown', and the unique count of 'Category' values is reported.
- **Completeness:** All the core requirements stated in the task description are met.
- **Code Quality:** The code is well-structured, uses appropriate pandas functions, and is easy to read. It includes necessary imports and checks for column existence before processing.

**Minor Improvements:**
- **Error Handling for File Loading:** The ground truth code includes robust error handling for `FileNotFoundError` when loading the CSV, which is a good practice for real-world scenarios. The generated code assumes the file will always be present, which could lead to a crash if the file is missing. Adding a `try-except` block for `pd.read_csv` would make it more robust.
- **Clarity of Output:** While the output is correct, the ground truth's specific print statements for each step (e.g., ""--- Column Names and Inferred Data Types ---"") provide more context and clarity to the user. The generated code's output is slightly less verbose.
- **`df.dtypes` vs `df.info()`:** For listing column names and data types, `df.info()` provides more comprehensive information, including non-null counts and memory usage, which is often more useful for initial data inspection than just `df.dtypes`. However, `df.dtypes` fully satisfies the requirement of listing column names and their *inferred data types*.

Overall, this is a very good solution that successfully implements the core logic.",claude-sonnet-4,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

df = pd.read_csv('youtube_trendings.csv')

df['Video views'] = df['Video views'].astype(str).str.replace(',', '')
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce')
df['Video views'] = df['Video views'].fillna(0)

filtered_df = df[df['Video views'] > 50000000]

result = filtered_df[['Video', 'Video views']].sort_values('Video views', ascending=False)

print(result)
```",9,"The generated code correctly implements all the requirements of the task. It properly loads the CSV, cleans the 'Video views' column by removing commas, converting to numeric, and filling NaNs with 0. It then filters the data for views greater than 50 million, selects the required columns, and sorts them in descending order. The code is concise and efficient.

One minor improvement, as seen in the ground truth, would be to include error handling for `FileNotFoundError` when attempting to read the CSV. While not explicitly asked for in the prompt's requirements, it is good practice for robust code, especially when assuming a file is in the current directory. However, given the explicit instructions to ""Provide only the Python code without explanations"" and the primary focus on data manipulation steps, its absence doesn't detract significantly from the core task. The code is functionally equivalent to the ground truth in terms of data processing.",claude-sonnet-4,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the Likes column
df['Likes'] = df['Likes'].astype(str).str.replace(',', '')
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

# Clean the Video views column
df['Video views'] = df['Video views'].astype(str).str.replace(',', '')
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce')
df['Video views'] = df['...",9,"The generated code successfully loads the dataset, correctly cleans the 'Likes' and 'Video views' columns by removing commas, converting to numeric, and filling NaNs with 0. It also generates the required scatter plot with the specified title and axis labels. The imports are proper, and the code structure is logical and readable.

One minor point for improvement, not critical but good practice, would be to include error handling for the `pd.read_csv` call (e.g., `try-except` for `FileNotFoundError`), which the ground truth included. However, the prompt specifically asked for ""only the Python code without explanations"" and didn't explicitly demand robust error handling beyond the core data processing and plotting, so its omission doesn't severely detract from its correctness for the stated task. The generated code also omitted the `alpha` and `s` parameters in the scatter plot, which were present in the ground truth but not explicitly requested by the prompt. Overall, it perfectly fulfills the core requirements.",claude-sonnet-4,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean Mobile Weight column
df['Mobile Weight'] = df['Mobile Weight'].astype(str).str.replace('g', '', regex=False)
df['Mobile Weight'] = pd.to_numeric(df['Mobile Weight'], errors='coerce')

# Clean RAM column
df['RAM'] = df['RAM'].astype(str).str.replace('GB', '', regex=False)
df['RAM'] = pd.to_numeric(df['RAM'], errors='coerce')

# Clean Battery Capacity column
df['Battery Capacity'] = d...",9,"The generated code correctly addresses all requirements of the task. It loads the CSV, cleans the specified columns by removing units and special characters and converting them to numeric, handles conversion errors by coercing to NaN, lists data types, calculates descriptive statistics for the cleaned numeric columns, and reports the count of unique company names. The imports are correct, and the code structure is clear and sequential.

**Strong points:**
*   **Correct Cleaning Logic:** The `str.replace` and `pd.to_numeric(errors='coerce')` are correctly used for all specified columns. This handles potential errors by setting them to NaN, as required.
*   **Data Type Listing:** `df.dtypes` correctly lists the column names and their inferred data types after cleaning.
*   **Descriptive Statistics:** The loop iterates through the `cleaned_columns` and prints the individual mean, median, std, min, and max, fulfilling the requirement.
*   **Unique Count:** `df['Company Name'].nunique()` correctly provides the count of unique company names.
*   **Completeness:** All 7 requirements of the prompt are met.

**Minor areas for improvement (not affecting score significantly given the directness of the task):**
*   **Error Handling for File Loading:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions during `pd.read_csv`. While not explicitly requested in the prompt's output format, robust production code would include this.
*   **Output Formatting for Stats:** The ground truth used `df[existing_numeric_cols].describe().loc[['mean', '50%', 'std', 'min', 'max']]` which provides a more compact, table-like output for statistics, generally preferred for readability when multiple columns are involved, although the generated code's individual print statements are also acceptable.
*   **Chain calls for cleaning:** The `str.strip()` call after `replace` in the ground truth for ""Mobile Weight"", ""RAM"", and ""Battery Capacity"" is a good practice to remove leading/trailing whitespace that might be present after unit removal, although for simple numerical data, it might not be strictly necessary if the original data is clean. The generated code omitted this, but for the given dataset, it's unlikely to cause issues.",claude-sonnet-4,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the Launched Price (India) column
df['Launched Price (India)'] = df['Launched Price (India)'].astype(str).str.replace('INR ', '', regex=False)
df['Launched Price (India)'] = df['Launched Price (India)'].str.replace(',', '', regex=False)
df['Launched Price (India)'] = pd.to_numeric(df['Launched Price (India)'], errors='coerce')

# Clean the RAM column
df['RAM'] = df['RAM'].astype(str...",9,"The generated code correctly addresses all the requirements of the task: loading the CSV, cleaning both 'Launched Price (India)' and 'RAM' columns (including handling errors with `coerce`), filtering the data for 'Apple' and price less than 100,000, selecting the specified columns, and sorting by 'Launched Price (India)'.

The code is concise and directly implements the steps. The use of `.astype(str)` before string operations is a good practice to prevent errors if the column contains non-string types.

Areas for minor improvement (not critical enough to significantly reduce the score, but good practice):
1.  **Error Handling for File Loading:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions during file loading, which makes the script more robust. The generated code assumes the file will always be found.
2.  **Comments/Readability:** While succinct, adding comments to explain each major step (like in the ground truth) would improve readability for more complex tasks or larger codebases.
3.  **Intermediate Prints/Debugging aids:** The ground truth includes `print` statements to show progress and file shape, which can be useful for debugging and understanding what the script is doing, especially for larger datasets or when issues arise.
4.  **`numpy` import:** `import numpy as np` is included but `numpy` itself is not directly used in the code. While harmless, it's unnecessary in this specific context.

Overall, the generated code is a very strong and correct solution to the problem as stated.",claude-sonnet-4,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the RAM column
df['RAM'] = df['RAM'].str.replace('GB', '', regex=False)
df['RAM'] = pd.to_numeric(df['RAM'], errors='coerce')

# Clean the Battery Capacity column
df['Battery Capacity'] = df['Battery Capacity'].str.replace('mAh', '', regex=False)
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'], errors='coerce')

# Create scatter plot
...",9,"The generated code correctly loads the dataset, cleans the `RAM` and `Battery Capacity` columns (including error handling for numeric conversion), and creates a scatter plot with proper labels and title. It also correctly colors the points by `Company Name`.

Strong points:
- Correctly uses `pd.to_numeric` with `errors='coerce'` for robust cleaning.
- Properly sets x-axis, y-axis labels, and plot title.
- Successfully colors points by `Company Name` and creates a legend.
- Includes `plt.tight_layout()` for better plot presentation.

Areas for minor improvement (not critical enough to significantly deduct points):
- When cleaning columns, it overwrites the original columns (`df['RAM'] = ...`, `df['Battery Capacity'] = ...`). While functional for this specific task, it's often good practice in data analysis to create new cleaned columns (e.g., `df['RAM_cleaned']`) to preserve the original data, as done in the ground truth. This is a matter of coding style and data integrity practices, not correctness for this problem.
- The iteration for plotting (`for i, company in enumerate(companies): plt.scatter(...)`) is a valid approach, but `seaborn.scatterplot` (as used in the ground truth) can achieve the same result more compactly and often with better default aesthetics, especially for categorical `hue` mapping. However, using `matplotlib` directly is perfectly acceptable and achieves the required outcome.
- The `File Not Found` error handling present in the ground truth is a good addition for robustness in real-world scenarios but was not explicitly required by the prompt's core tasks.",claude-sonnet-4,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset and display the first 5 rows
df = pd.read_csv('titanic.csv')
print(""First 5 rows of the dataset:"")
print(df.head())
print()

# Show the shape of the dataset
print(f""Dataset shape: {df.shape}"")
print()

# List all column names and their data types
print(""Column names and data types:"")
print(df.dtypes)
print()

# Calculate the survival rate
survival_rate = (df['Survived'].sum() / len(df)) * 100
print(f""Survival rate: {survival_ra...",8,"The generated code correctly performs all the requested tasks: loading the dataset, displaying the first 5 rows, showing the shape, listing column names and data types, calculating the survival rate, and counting missing values. The code is clean and uses appropriate pandas functions.

Areas for improvement:
1.  **Error Handling for File Not Found:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is robust. The generated code assumes the file will always be present, which could lead to a crash if 'titanic.csv' is missing.
2.  **Output Clarity:** The ground truth adds descriptive print statements like ""1. First 5 rows of the dataset:"" and ""2. Shape of the dataset (rows, columns):"" which makes the output clearer and easier to follow, especially for someone unfamiliar with the task. The generated code's print statements are slightly less descriptive, though still understandable.
3.  **Data Type Display:** While `df.dtypes` is technically correct for showing data types, `df.info()` (used in the ground truth) provides more comprehensive information, including non-null counts and memory usage, which is often more useful in initial EDA without explicitly asking for it.
4. `np` import is not necessary as `numpy` functions are not explicitly used.

Overall, it's a very good attempt that meets the core requirements. The minor points relate to robustness and output presentation.",claude-sonnet-4,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 1) List all column names and their data types
print(""Column names and data types:"")
print(df.dtypes)
print(""\n"")

# 2) Calculate and report basic descriptive statistics for numerical columns
numerical_columns = ['Year', 'Rating', 'Runtime']

for col in numerical_columns:
    if col in df.columns:
        print(f""Statistics for {col}:"")
        print(f""Mean: {df[col].mean():.2f}"")
        ...",6,"The generated code correctly loads the dataset, lists column names and data types, and reports the total number of movies. However, it has a significant flaw regarding the `Runtime` column. It assumes `Runtime` is already numeric, which is often not the case in `imdb_top_250.csv` (it's typically 'XXX min'). This will lead to an error or incorrect statistics if `Runtime` is an object/string type and not handled (the `mean`, `median`, `std`, `min`, `max` calls will fail). The ground truth code explicitly handles this preprocessing. Additionally, the statistical reporting in the generated code is individual for each column, which is less concise than using `df.describe()` as in the ground truth. It also lacks error handling for `FileNotFoundError`.

Specific improvements:
1.  **Handle 'Runtime' column preprocessing**: Convert the 'Runtime' column from string (e.g., '142 min') to numeric before calculating statistics.
2.  **Robust statistics calculation**: Use `df[numerical_columns].describe().loc[['mean', '50%', 'std', 'min', 'max']]` for a more concise and correct summary of statistics.
3.  **Error Handling**: Add `try-except` block for `FileNotFoundError` when loading the CSV.",claude-sonnet-4,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

df = pd.read_csv('imdb_top_250.csv')
filtered_df = df[df['Rating'] >= 9.0]
result = filtered_df[['Title', 'Rating']].sort_values('Rating', ascending=False)
print(result)
```",9,"The generated code correctly implements all the requirements of the task: it loads the CSV, filters movies with a rating greater than or equal to 9.0, selects the 'Title' and 'Rating' columns, and sorts the results by 'Rating' in descending order. The code is concise and demonstrates a good understanding of pandas operations.

The only minor area for improvement that prevents a perfect score is the lack of error handling (e.g., `FileNotFoundError`, `KeyError`) which is present in the ground truth. While not explicitly asked for in the prompt, robust production-ready code typically includes such safeguards. However, for a data science benchmark focusing on task execution, the core logic is perfectly sound.",claude-sonnet-4,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create histogram for Rating distribution
plt.figure(figsize=(10, 6))
plt.subplot(1, 2, 1)
plt.hist(df['Rating'], bins=20, edgecolor='black', alpha=0.7)
plt.title('Distribution of Movie Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')

# Create scatter plot for Rating vs Runtime
plt.subplot(1, 2, 2)
plt.scatter(df['Runtime'], df['Rating'], alpha=0.6...",7,"The generated code correctly loads the CSV, creates both a histogram and a scatter plot with appropriate titles and labels. This addresses the core requirements of the task.

However, there's a significant issue regarding the `Runtime` column. The ground truth code includes robust preprocessing for `Runtime` because it is often in a string format like '120 min' in datasets such as imdb_top_250.csv. The generated code attempts to use `df['Runtime']` directly in the scatter plot, which will likely fail or produce incorrect results if the column is indeed a string. This oversight makes the scatter plot part of the solution not fully robust or correct for the likely state of the data.

Additionally, putting both plots into a single `plt.subplot(1, 2, X)` call makes them appear side-by-side, which wasn't explicitly requested and reduces the clarity of individual plots unless the figure size is very large. While not incorrect, it's a deviation from the ground truth's separate plot display and might not be the most readable default.

To improve, the code should:
1. Implement preprocessing for the `Runtime` column to convert it into a numeric format (e.g., extract minutes from strings like '120 min').
2. Consider displaying plots separately (`plt.show()` after each plot) for better readability, or explicitly managing the subplot layout and size if they are intended to be side-by-side.",claude-sonnet-4,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('penguins.csv')

# 1) Report missing values for each column
print(""Missing values per column:"")
print(df.isnull().sum())
print()

# 2) Drop rows with missing values in specified numerical columns and calculate statistics
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_clean = df.dropna(subset=numerical_cols)

print(""Descriptive statistics for numerical columns (af...",8,"The generated code correctly addresses most of the task requirements.

**Strong Points:**
*   Correctly loads the CSV using `pd.read_csv`.
*   Correctly reports missing values for each column using `df.isnull().sum()`.
*   Correctly identifies the four numerical columns and uses `dropna(subset=...)` to filter rows.
*   Calculates and reports mean, median, standard deviation, min, and max for the specified numerical columns on the filtered data.
*   Correctly counts and reports unique values for `species` and `island` columns using `nunique()`.

**Areas for Improvement:**
*   **Error Handling for File Loading:** The ground truth includes robust error handling for `FileNotFoundError` when loading the CSV. The generated code assumes the file will always be present, which is less resilient.
*   **Reporting Descriptive Statistics:** While the statistics are correct, the ground truth presents them in a more concise and readable table format using `df_filtered[numerical_cols_for_analysis].agg(['mean', 'median', 'std', 'min', 'max'])`, which is generally preferred for a data science report compared to iterating and printing each statistic individually.
*   **Clarity and Readability:** The ground truth uses more descriptive print statements and section headers, making the output clearer and easier to understand for someone reviewing the report.
*   **Unnecessary Import:** `import numpy as np` is present but not used. While minor, it's good practice to only import necessary libraries.",claude-sonnet-4,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

df = pd.read_csv('penguins.csv')
filtered_df = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')]
selected_df = filtered_df[['culmen_length_mm', 'culmen_depth_mm']]
cleaned_df = selected_df.dropna()
print(cleaned_df.head())
```",9,"The generated code correctly solves all aspects of the task, including loading the data, filtering by species and island, selecting specific columns, dropping NaNs, and displaying the first 5 rows. The code is concise and efficient.

One minor improvement, though not strictly required by the prompt, would be to explicitly specify the `subset` argument in `dropna()` to ensure only the `culmen_length_mm` and `culmen_depth_mm` columns are considered for dropping NaNs. While `dropna()` without `subset` on a DataFrame that *only* contains these two columns will achieve the same result, being explicit is good practice and more robust if the `selected_df` somehow ended up with other columns (though in this specific pipeline, it wouldn't).

Additionally, the ground truth includes error handling for `FileNotFoundError`, which is a good practice for robust code but was not explicitly asked for by the prompt. The generated code does not include this, which is acceptable given the prompt's simplicity. The `print(cleaned_df.head())` implicitly prints 5 rows, but `cleaned_df.head(5)` is slightly more explicit, though both achieve the same result.",claude-sonnet-4,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter out rows with missing values in the required columns
df_clean = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])

# Create scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_clean, x='culmen_length_mm', y='culmen_depth_mm', hue='species')
plt.title('Culmen Length vs Culmen Depth by Species')
plt.xlabel('Culmen Length ...",9,"The generated code successfully meets almost all requirements of the task. It correctly loads the CSV, filters out rows with NAs in the specified columns (`culmen_length_mm`, `culmen_depth_mm`, `species`), generates the scatter plot with `culmen_length_mm` vs `culmen_depth_mm`, colors points by `species`, and includes a title and axis labels. Imports are correct and the code is clean and concise.

The only minor point of improvement compared to the ground truth is the lack of robust error handling for `FileNotFoundError` when loading the CSV. While not explicitly asked for in the prompt's requirements, it's a good practice for production-ready code. Additionally, encapsulating the plotting logic within a function (as done in the ground truth) promotes reusability and modularity, which is generally considered better code quality, though the prompt didn't explicitly demand a function. However, for a direct comparison against the stated task requirements, the generated code is highly effective.",claude-sonnet-4,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd
import numpy as np

# Read the dataset
df = pd.read_csv('penguins.csv')

# Fill NA values with mean for culmen_length_mm and culmen_depth_mm
df['culmen_length_mm'].fillna(df['culmen_length_mm'].mean(), inplace=True)
df['culmen_depth_mm'].fillna(df['culmen_depth_mm'].mean(), inplace=True)

# Calculate and print statistics for culmen_length_mm
print(""culmen_length_mm statistics:"")
print(f""Mean: {df['culmen_length_mm'].mean():.2f}"")
print(f""Median: {df['culmen_length_m...",9,"The generated code correctly addresses all requirements of the task: it loads the CSV, fills NA values in the specified columns with their respective means, calculates and prints the mean, median, and standard deviation for these columns, and finally prints the total number of rows and columns. The use of `inplace=True` for `fillna` is efficient. The output formatting for statistics is clear and readable.

Areas for minor improvement (not critical enough to significantly lower the score):
*   **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when reading the CSV, which is good practice for robust code. The generated code assumes the file will always be present.
*   **Encapsulation/Modularity:** The ground truth defines `NUMERICAL_COLUMNS` as a list and iterates through it for NA imputation and statistics calculation. This makes the code more modular and easier to extend if more columns needed similar processing. The generated code explicitly calls out each column, which is fine for two columns but less scalable.
*   **`numpy` import:** `import numpy as np` is included but not used in the provided code. It's a minor unused import.

Overall, it's a very solid and correct solution for the given task.",claude-sonnet-4,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows where flipper_length_mm is NA
df = df.dropna(subset=['flipper_length_mm'])

# Create histogram of flipper_length_mm
plt.figure(figsize=(10, 6))
plt.hist(df['flipper_length_mm'], bins=20, edgecolor='black', alpha=0.7)
plt.title('Distribution of Flipper Length (mm)')
plt.xlabel('Flipper Length (mm)')
plt.ylabel('Frequency')
plt.show()

# Create bar c...",8,"The generated code correctly addresses all core requirements: loading the dataset, dropping NA values in `flipper_length_mm`, creating a labeled histogram for flipper length, and a labeled bar chart for island counts. The plots are generated and displayed.

**Strong points:**
*   Correctly uses pandas for data manipulation and matplotlib for plotting.
*   Successfully drops NA values from the specified column.
*   Both plots have correct titles and axis labels as requested.

**Areas for improvement (to achieve a higher score, closer to the ground truth):**
*   **Error Handling:** The ground truth includes `try-except` blocks for file loading, which makes the code more robust against `FileNotFoundError` and other general exceptions. This is a good practice for real-world scenarios.
*   **Plotting Library Usage:** While `plt.hist` and `plt.bar` directly fulfill the request, the ground truth leverages Seaborn (e.g., `sns.histplot`, `sns.countplot`) which often produces more aesthetically pleasing and informative plots with less code (e.g., `countplot` handles value counts and plotting automatically, `histplot` can add KDE). The ground truth also uses `sns.set_theme` for styling, which is a nice touch for professional plots.
*   **Clarity and Detail in Plots:** The ground truth adds minor enhancements like `kde=True` in `histplot`, `order` in `countplot` (to sort bars by count), `grid` lines, and `plt.tight_layout()` for better presentation.
*   **Informative Printing:** The ground truth includes print statements to inform the user about data loading success/failure, number of NA values found, and rows dropped, improving user experience and debugging.

Overall, the generated code is a solid solution that meets the specified output requirements, but lacks some of the robustness, advanced plotting features, and user-friendly feedback present in the ground truth.",claude-sonnet-4,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter data to include only penguins from 'Torgersen' island
torgersen_penguins = df[df['island'] == 'Torgersen']

# Count and print the number of penguins belonging to each species
species_counts = torgersen_penguins['species'].value_counts()
print(species_counts)
```",7,"The generated code correctly addresses the core requirements of loading the data, filtering by 'Torgersen' island, and counting species. Its conciseness is a positive aspect for this specific problem.

However, it lacks robustness compared to the ground truth. It assumes the `'penguins.csv'` file always exists, and the `'island'` and `'species'` columns are always present. This makes it less resilient to common real-world data issues. The ground truth code includes error handling for `FileNotFoundError` and checks for column existence, which makes it more production-ready and user-friendly.

Improvements: Add error handling for file not found and checks for the existence of required columns (`'island'`, `'species'`) to make the code more robust. Also, adding more descriptive print statements would enhance clarity.",claude-sonnet-4,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

print(df.shape)
print(df.dtypes)
```",9,"The generated code correctly defines the DataFrame, prints its shape, and lists the data types of all columns. It completely addresses all the requirements of the task. The code is concise and directly implements the required operations.

The only minor improvement, which is not strictly a requirement but good practice for clarity in output, would be to add descriptive print statements before `df.shape` and `df.dtypes` (e.g., ""Shape of the DataFrame:"" and ""Data types of each column:""). This makes the output more user-friendly, especially when running the code in an environment where output is not automatically labeled. However, given the prompt asked for ""only Python code"" and the core functionality is present, this does not detract significantly from correctness or completeness.",claude-sonnet-4,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('titanic.csv')

# 1) Bar chart showing count of survivors vs non-survivors
plt.figure(figsize=(8, 6))
survival_counts = df['Survived'].value_counts()
plt.bar(['Non-Survivors', 'Survivors'], [survival_counts[0], survival_counts[1]])
plt.title('Count of Survivors vs Non-Survivors')
plt.xlabel('Survival Status')
plt.ylabel('Number of Passengers')
plt.show()

# 2) Bar chart showing distribution of pass...",7,"The generated code successfully creates the four requested plots with appropriate titles and labels, demonstrating good understanding of visualization basics. It uses `matplotlib.pyplot` and `pandas` correctly.

However, there are a few areas for improvement:
1.  **Handling Missing Age Values**: For the age histogram, the generated code uses `df['Age'].dropna()`. While this handles missing values by removing them, the ground truth code fills missing values with the median (`fillna(median_age)`). Filling with the median is generally a better approach for histograms as it preserves more data and provides a more complete picture of the age distribution, rather than simply discarding rows with missing age. The prompt asked to ""handle missing values appropriately"", and dropping them might not always be the most appropriate strategy.
2.  **Dataset Loading Robustness**: The generated code assumes `titanic.csv` is present and doesn't include any error handling (`try-except`) for `FileNotFoundError`. The ground truth code includes robust error handling for this, which is good practice for real-world scripts.
3.  **Visualization Aesthetics**: While the plots are functional, they lack some aesthetic enhancements present in the ground truth, such as custom colors, grid lines, and `plt.tight_layout()` for better spacing, and horizontal x-tick labels (`plt.xticks(rotation=0)`).
4.  **X-axis Labels for Categorical Plots**: In plots 1 and 2, the generated code manually constructs the x-axis labels (`['Non-Survivors', 'Survivors']`, `['1st Class', '2nd Class', '3rd Class']`). While functional, using `value_counts().index` directly with a `map` or proper DataFrame plotting (`.plot(kind='bar')`) can be more robust and scalable, especially if categories change. The ground truth beautifully maps the survival status numbers to strings directly on the index.

The code is functional and addresses the core requirements but could benefit from more robust data handling, better visualization choices (like median imputation), and improved general code robustness and aesthetics.",claude-sonnet-4,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'], 
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'], 
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89], 
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93], 
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]}

df = pd.DataFrame(data)

# 1) Calculate and print the average...",9,"The generated code is highly correct and addresses all the requirements of the task. It properly imports pandas, creates the DataFrame, calculates the average midterm and final scores, and correctly identifies the subject with the highest average final score. The variable names are clear, and the overall structure is clean.

Minor points for improvement (which do not detract significantly from correctness or quality):
- The print statements could be slightly more descriptive or formatted (e.g., using `:.2f` for scores to ensure two decimal places, as in the ground truth). This is a stylistic choice and doesn't impact correctness.
- The ground truth code explicitly comments on each step, which adds to readability and maintainability for very complex tasks, but for this task, the generated code is clear enough without excessive commenting.",claude-sonnet-4,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']), 
        'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'], 
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0], 
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5], 
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0], 
        'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0], 
  ...",10,"The generated code is correct, complete, and well-structured. It successfully creates the DataFrame, sets the 'Date' as the index, calculates the 'Daily_Range' column, identifies and prints the date with the highest 'Close' price, and finally prints the full DataFrame including the new column. All requirements of the task are met. The code also correctly formats the printed date, although the ground truth uses `strftime('%Y-%m-%d')` which is more explicit. The generated code's output for the date is `2024-01-08 00:00:00`, which is also acceptable as a datetime object.",claude-sonnet-4,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107], 
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 
        'Quantity': [1, 2, 1, 1, 3, 1, 1], 
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150], 
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', ...",10,"The generated code is correct, complete, and well-structured. It successfully creates the DataFrame, calculates the 'Total_Revenue' column, correctly identifies the product sold the most in terms of quantity using `groupby().sum().idxmax()`, and then prints the first 5 rows of the modified DataFrame. All requirements of the task are met precisely. The code includes necessary imports and follows clear logical steps.",claude-sonnet-4,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 1) Calculate and print the average ""SessionDurationMinutes""
average_session_duration = df[""SessionDurationMinutes""].mean()
pr...",10,"The generated code is entirely correct, complete, and adheres to good code quality. It accurately defines the DataFrame, calculates the average session duration, and counts users by device type using appropriate pandas methods. The output format is clear and directly addresses all parts of the task. There's nothing to improve upon for this specific task.",claude-sonnet-4,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

max_temperature = df[""TemperatureCelsius""].max()
print(max_temperature)

min_hu...",9,"The generated code correctly defines the DataFrame and accurately finds the maximum temperature and minimum humidity. It correctly uses `pandas.DataFrame.max()` and `pandas.DataFrame.min()`. The only minor point of improvement is that the print statements do not include the descriptive text from the ground truth. While not strictly required by the prompt, adding context to the output makes the results more readable and user-friendly, which is generally good practice in data analysis tasks.",claude-sonnet-4,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# 1) Print the total number of ""Likes"" across all posts
total_likes = df[""Likes""].sum()
print(total_likes)

# 2) Find and print t...",9,"The generated code correctly defines the DataFrame, calculates the total likes, and identifies the post type with the highest average shares. All requirements of the task are met. The code is clean, concise, and uses appropriate pandas functionalities.

The only minor area for improvement, preventing a perfect 10, is the print statements. The output for total likes and the post type with the highest average shares are just the raw values. For a user, it would be much clearer if these outputs were accompanied by descriptive text, similar to the ground truth (e.g., `print(f""Total number of Likes across all posts: {total_likes}"")`). This improves readability and user experience. However, since the task only asks to ""print"" and doesn't specify the exact format of the print output, the current solution is fundamentally correct.",claude-sonnet-4,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",9,"The generated code correctly defines the DataFrame, calculates the average age, and counts patients by gender, fulfilling all task requirements. The code is clean and well-structured, reflecting good code quality. The only minor difference from the ground truth is the formatting of the average age output (not rounded to two decimal places) and a slightly different phrasing of the print statements. However, these are minor cosmetic differences and do not affect the correctness or completeness of the solution.",claude-sonnet-4,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}
df = pd.DataFrame(data)

print(df.loc[df['Rating'].idxmax(), 'Title'])
print(df['Year'].mean())
```",9,"The generated code correctly solves all aspects of the task with concise and efficient pandas operations.

**Strong Points:**
- **Correctness:** Both parts of the task (finding the highest-rated movie title and calculating the average year) are correctly implemented.
- **Conciseness:** `df['Rating'].idxmax()` is an elegant and efficient way to find the index of the maximum rating, which is then used with `.loc` to retrieve the title. This is often more efficient than filtering the entire DataFrame when only one value is needed.
- **Completeness:** All requirements from the prompt are met.
- **Readability:** Despite its conciseness, the code remains readable.

**Minor Area for Improvement (Not impacting score significantly):**
- **Output clarity:** The ground truth code provides more descriptive print statements (e.g., ""The title of the movie with the highest rating is: 'The Dark Knight'""). The generated code simply prints the title and then the average year on separate lines. While functionally correct, adding descriptive text makes the output more user-friendly, especially in a benchmark context where the output might be evaluated by humans. This is a minor stylistic point for a benchmark task.",claude-sonnet-4,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

print(f""Total number of cars: {len(df)}"")
print(f""Average price: ${df...",10,"The generated code is excellent. It correctly defines the DataFrame, prints the total number of cars, and calculates the average ""PriceUSD"" as required by the task.

**Correctness:**
*   The DataFrame `df` is created exactly as specified.
*   `len(df)` correctly calculates the total number of rows (cars).
*   `df['PriceUSD'].mean()` correctly calculates the average PriceUSD.
*   The f-strings correctly format the output, including two decimal places for the average price, which is good practice for currency.

**Code Quality:**
*   Proper import of pandas.
*   Clear and concise variable names (`data`, `df`).
*   The code is very readable and directly addresses the task requirements.

**Completeness:**
*   All requirements from the task description are met:
    *   DataFrame definition.
    *   Printing total number of cars.
    *   Printing average ""PriceUSD"".

The generated code is functionally identical to the core logic of the ground truth code, albeit slightly more concise in its printing statements, which is a positive attribute.",claude-sonnet-4,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}
df = pd.DataFrame(data)

print(df.loc[df[""HP""].idxmax(), ""Name""])
print(df.loc[df[...",9,"The generated code correctly defines the DataFrame and accurately identifies and prints the names of the Pokemon with the highest HP and lowest Defense. The imports are correct, and the code directly fulfills all requirements. The primary difference from the ground truth is the conciseness of the printing statements, which combine the `idxmax()`/`idxmin()` and `loc` operations into a single line. While this is perfectly functional and efficient, the ground truth's approach using intermediate variables `idx_highest_hp`, `pokemon_highest_hp`, etc., can sometimes improve readability for more complex operations or if those intermediate values were needed elsewhere. However, for this specific task, the generated code's conciseness is not a detriment to its quality or correctness. The ground truth also included comments for explanation, which were omitted in the generated code. Adding brief comments could further improve readability, although it's not strictly necessary for such straightforward operations.",claude-sonnet-4,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('titanic.csv')

# Identify columns with missing values
print(""Missing values per column:"")
print(df.isnull().sum())

# Fill missing Age values with the median age
df['Age'].fillna(df['Age'].median(), inplace=True)

# Fill missing Embarked values with the most frequent port
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# Drop the Cabin column due to excessive missing values
df.drop('Cabin', axis...",9,"The generated code correctly performs all the required data cleaning steps. It loads the dataset, identifies missing values (though the output format is slightly different from the ground truth's ""missing values before cleaning""), fills missing 'Age' and 'Embarked' values appropriately, drops the 'Cabin' column, and successfully extracts titles using a concise regex. The display of the shape and first 5 rows is also correct.

Strengths:
- Uses `df.isnull().sum()` to identify missing values.
- Correctly uses `median()` and `mode()[0]` for filling missing 'Age' and 'Embarked' values, respectively.
- Correctly drops the 'Cabin' column.
- Efficiently extracts titles using `str.extract` with a regex directly on the Series, which is a good pandas idiom.
- Correctly displays the shape and head of the cleaned DataFrame.

Areas for minor improvement (preventing a perfect 10):
- The `import numpy as np` is not strictly necessary for the code provided, as only pandas functionalities are used. While it doesn't harm, it's an unnecessary import.
- The title extraction, while correct for the prompt, doesn't include the standardization of less common titles (e.g., 'Mlle' to 'Miss', 'Capt' to 'Rare') as seen in the more robust ground truth. While not explicitly asked for by the prompt, it makes the 'Title' feature more useful, which the ground truth included as a good practice. However, the prompt only asked to ""Create a new column 'Title' extracted from the Name column (Mr., Mrs., Miss., Master., etc.)"", which the generated code fulfills.
- The output for ""Identify columns with missing values"" is more comprehensive in the generated code (`df.isnull().sum()`) compared to the ground truth which filtered for `>0`. Both are acceptable interpretations of ""identify"".",claude-sonnet-4,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}
df = pd.DataFrame(data)

# 1) Print the ""T...",9,"The generated code correctly defines the DataFrame, identifies the title of the book with the highest price, and calculates the total stock, fulfilling all task requirements. The code is concise and uses appropriate pandas methods (`idxmax`, `sum`).

The only very minor area for improvement, preventing a perfect 10, is the print statements. While they print the correct values, they lack descriptive text, making the output less user-friendly compared to the ground truth. For example, instead of just printing `The Hobbit`, it could print `The title of the book with the highest price is: The Hobbit`. This is a matter of output presentation rather than code correctness or functionality.",claude-sonnet-4,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Display the first 10 rows
print(""First 10 rows:"")
print(df.head(10))
print(""\n"" + ""=""*50 + ""\n"")

# Show the shape of the dataset
print(f""Dataset shape: {df.shape}"")
print(f""Rows: {df.shape[0]}, Columns: {df.shape[1]}"")
print(""\n"" + ""=""*50 + ""\n"")

# List all column names and their data types
print(""Column names and data types:"")
print(df.dtypes)
print(""\n"" + ""=""*50 + ""\n"")

# Calcula...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
- All requested functionalities are implemented (loading, shape, dtypes, potability percentage, missing values, summary statistics).
- Uses `pandas` effectively for data manipulation.
- Clear print statements for each section.
- The calculation and display of potability percentages are well-formatted.

**Areas for improvement:**
1.  **Error Handling (Critical omission):** The generated code lacks any robust error handling for `FileNotFoundError` or other potential issues when loading the CSV. The ground truth includes `try-except` blocks which are crucial for real-world scenarios. This is a significant point of failure.
2.  **Dataset File Naming:** It hardcodes `'water_potability.csv'` directly, whereas the ground truth uses a `DATASET_FILE` constant, which is a good practice for maintainability.
3.  **Column Names and Data Types Display:** The ground truth uses `df.info()`, which provides a more comprehensive overview including non-null counts, memory usage, and the total number of entries, in addition to data types. `df.dtypes` only shows the data types.
4.  **Missing Values Display:** The ground truth specifically prints only columns with missing values (`missing_values[missing_values > 0]`) and a message if no missing values are found. The generated code prints all columns, even those without missing values, which can be less informative for very wide datasets.
5.  **Unused Import:** `import numpy as np` is included but not used in the provided code. It's good practice to remove unused imports.
6.  **Redundant Shape Print:** It prints the shape twice, once as `df.shape` and then breaks it down as `Rows: {df.shape[0]}, Columns: {df.shape[1]}`. While not incorrect, it's slightly redundant.

The primary reason for not giving a higher score is the lack of error handling for file loading, which is a fundamental aspect of robust code.",claude-sonnet-4,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Store original statistics
original_shape = df.shape
original_missing = df.isnull().sum()
original_duplicates = df.duplicated().sum()

print(""BEFORE CLEANING:"")
print(f""Dataset shape: {original_shape}"")
print(f""Missing values:\n{original_missing}"")
print(f""Duplicate rows: {original_duplicates}"")
print(""\n"" + ""=""*50 + ""\n"")

# Fill missing values with mean for numerical columns
numerica...",7,"The generated code correctly implements most of the required steps: loading data, filling missing values with the mean, removing duplicates, and displaying the first 5 rows of the cleaned dataset. It also attempts to check reasonable ranges for water quality parameters.

However, there are several areas for improvement to meet the ""expert code evaluator"" standard:
1.  **Error Handling:** The ground truth includes `try-except FileNotFoundError` for loading the CSV, which is good practice for robust code. The generated code lacks this.
2.  **Before/After Summary:** While the generated code prints ""BEFORE CLEANING"" and ""AFTER CLEANING"", the ground truth provides a more structured and explicit comparison of changes (e.g., `rows_removed_due_to_range`, explicit statement about `No missing values remaining`). The generated code's summary `Rows removed: {original_shape[0] - cleaned_shape[0]}` is good, but it could be more descriptive about *what* caused the removals (duplicates vs. range issues).
3.  **Range Checking Granularity:** The ground truth identifies `numerical_cols_with_nan` more precisely and applies a loop for non-negative checks. The generated code individually checks each column for non-negativity, which is less efficient and harder to maintain if more columns are added or general rules apply. Also, the explicit `if 'ph' in df.columns:` for *every* parameter check is redundant if we assume these columns exist in the dataset, or a loop over a list of expected columns would be more concise and proper. The range check logic for numerical values should ideally be generalized.
4.  **Clarity of Output:** The ground truth includes more descriptive print statements regarding each step (e.g., ""Filling missing values:"", ""Removed X duplicate rows"", ""Removed Y rows due to out-of-reasonable-range values""). The generated code is more concise but less informative during execution.
5.  **Unnecessary Output:** The `df.describe()` at the end was not explicitly requested in the prompt, making the output slightly more verbose than necessary for the given task.

Overall, it's a good attempt that covers the core requirements, but it could benefit from more robust error handling, more generalized and concise data validation (range checking), and more detailed pre/post-cleaning summary reporting.",claude-sonnet-4,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Set up the plotting style
plt.style.use('default')
sns.set_palette(""husl"")

# Create a figure with subplots
fig = plt.figure(figsize=(15, 12))

# 1) Bar chart for potable vs non-potable distribution
plt.subplot(2, 3, 1)
potability_counts = df['Potability'].value_counts()
bars = plt.bar(['Non-Potable', 'Potable'], potability_counts....",7,"The generated code attempts to address all requirements of the task. It successfully generates four distinct visualizations: a bar chart, histograms for the specified features, a boxplot, and a correlation heatmap. It handles missing values by dropping NaNs for the relevant plots, which is appropriate for visualization.

However, there are several areas for improvement as compared to the ground truth:

1.  **Plot Organization:** The generated code attempts to put all plots into a single `2x3` subplot layout. While this is not explicitly forbidden by the prompt, it significantly compromises the readability and clarity of the individual plots, especially the heatmap, which benefits from having its own dedicated space. The ground truth separates each major visualization into its own figure, which is generally better practice for presentation and clarity. The inclusion of an ""Additional standalone correlation heatmap"" at the end suggests the model recognized this limitation but didn't integrate it seamlessly.
2.  **Redundancy in Histograms:** The prompt specifically asked for histograms for ""pH, Hardness, and Chloramines."" The generated code outputs three distinct histograms, which is correct, but the subplot layout dictates that these are squeezed into smaller spaces. The ground truth puts these three histograms in a single figure as subplots but uses a `1x3` layout, which is much better for their size and comparison.
3.  **Visualization Libraries:** While `matplotlib.pyplot` is used, the generated code often uses `plt.bar`, `plt.hist`, `plt.boxplot` directly instead of leveraging Seaborn for these common plot types (`sns.countplot`, `sns.histplot`, `sns.boxplot`). Seaborn often provides more aesthetically pleasing defaults and simplifies common plotting tasks, as demonstrated in the ground truth.
4.  **Aesthetics and Readability:**
    *   The manual setting of bar colors and adding text labels in the bar chart is less concise than using `sns.countplot` which handles these aspects well.
    *   The `mask` for the correlation heatmap is unnecessary given the prompt asked for a ""simple correlation heatmap for all numerical features,"" which implies a full heatmap. While masking is useful, it was not a direct requirement and adds complexity.
    *   The `plt.style.use('default')` and `sns.set_palette(""husl"")` are applied globally, which is fine, but the specific colors used for bars and boxplots are still set manually.
5.  **Missing File Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a robust coding practice. The generated code lacks this.

Overall, the generated code fulfills the basic functional requirements but could improve significantly in terms of best practices for data science visualization (especially when using Seaborn), plot organization for clarity, and code robustness.",claude-sonnet-4,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Separate features and target
X = df.drop('quality', axis=1)...",8,"The generated code provides a mostly correct and comprehensive solution to the problem, successfully implementing feature engineering and multiple feature selection techniques.

**Strengths:**
*   **Comprehensive Feature Engineering:** The code creates more than the required three new features, demonstrating good understanding of different types of feature engineering (ratios, interactions, polynomial). The `EPSILON` for division by zero is a good detail.
*   **Multiple Feature Selection Methods:** It correctly applies both RFE with RandomForest and SelectKBest with f\_regression, fulfilling the requirement. It also adds a third method (pure RandomForest feature importance) which enhances the analysis.
*   **Comparison and Contrast:** The code includes a thorough comparison of the feature sets selected by each method, identifying common, unique, and specific overlaps. This is a strong point.
*   **Display of Results:** The display of various results and rankings is clear and well-formatted.
*   **Consolidated Ranking:** The attempt to create a consolidated ranking across methods by normalizing scores is a very good addition, showing advanced understanding beyond the basic requirements.
*   **Good Code Structure:** The code is well-structured with clear comments and logical flow.

**Areas for Improvement:**
*   **Minor Issue with RFE and scaling:** In the RFE section, `rfe.fit(X_train, y_train)` is used with unscaled `X_train`, while `SelectKBest` uses `X_train_scaled`. While RandomForest is less sensitive to scaling, it's generally good practice to apply scaling consistently across methods, especially if comparing feature importances or model performances down the line. For RFE, if the estimator is sensitive to scaling (e.g., SVM, Logistic Regression), then scaling `X_train` would be crucial. For RandomForest specifically, it might not be a major issue, but it's a minor inconsistency.
*   **RFE Feature Importance Display:** The RFE results table correctly includes `rf_importance`. However, `rf.fit(X_train, y_train)` is called *before* `rfe.fit` and again *after* `rfe.fit` to get `feature_importance_rf`. The `feature_importance_rf` obtained from `rf.fit(X_train, y_train)` represents the importance on *all* features, not just the features selected by RFE. To truly get ""Importance (from RF on selected)"" as in the ground truth, you would need to retrain `rf` on *only* the features selected by RFE. The current `rf_importance` in the RFE results simply shows the general Random Forest importance, not specifically tied to the RFE-selected subset. This slightly deviates from the idea of RFE giving insight into how the estimator performs on the reduced feature set.
*   **The prompt asked for ""Provide only the Python code without explanations.""** The generated code includes an extensive amount of `print` statements that explain the process, which violates this specific instruction to some extent. While helpful for human understanding, it deviates from the strict output format required. My evaluation solely focuses on the quality of the Python code's logic and correctness.
*   **Unnecessary Imports:** `matplotlib.pyplot` and `seaborn` are imported but not used. While minor, it's good practice to only import necessary libraries.
*   **`train_test_split` unnecessary for task:** The prompt explicitly states that `train_test_split` is not strictly needed for ""feature selection"" (though good practice). The generated code uses it for all feature selection steps. While not incorrect, it slightly adds overhead that isn't strictly necessary for the asked-for feature selection *process* itself. However, it's not a critical flaw.

Overall, the code is robust and demonstrates strong proficiency in feature engineering and selection techniques. The minor issues are related to consistency and strict adherence to the prompt's output format, rather than fundamental correctness in the core task.",claude-sonnet-4,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
f...",6,"The generated code attempts to address most of the requirements but has significant issues in implementation and misses some key elements, leading to a score of 6.

**Correctness & Completeness Issues:**

*   **1. Preprocessing:** The use of `LabelEncoder` for features like 'Year' and 'Publisher' is problematic. 'Year' is a numerical feature and should not be label encoded. 'Publisher' and 'Platform' should be one-hot encoded for regression tasks, as label encoding implies an ordinal relationship that doesn't exist. The ground truth uses `OneHotEncoder` and `ColumnTransformer` which is the correct and more robust approach. There's also no preprocessor pipeline for the models, which means preprocessing steps are applied manually to `X_train` and `X_test` but not integrated into the model pipelines, making evaluation within cross-validation or GridSearchCV less robust.
*   **2. Cross-Validation:** The `cross_val_score` is used on `X_train` only, which is incorrect for evaluating the overall model performance. Cross-validation should typically be performed on the entire `X` dataset (or the training set if a separate holdout set is kept, but metrics should be averaged across folds). The ground truth correctly implements a manual KFold loop over the entire dataset `X` and `y` for individual models, voting, and stacking to get robust cross-validated metrics.
*   **3. Hyperparameter Tuning:** `GridSearchCV` is called on the raw model, not a pipeline. This means the scaling and encoding steps are not part of the `GridSearchCV` process for each fold, which is incorrect and can lead to data leakage or inconsistent preprocessing. The ground truth properly uses pipelines for tuning.
*   **4. Stacking Ensemble Implementation:** The custom `StackingRegressor` class is overly complex and attempts to re-implement `sklearn.ensemble.StackingRegressor` but does so in a way that doesn't align with `sklearn`'s typical pipeline integration. The data handling within the custom class (e.g., `X.iloc`) is not ideal for generic use. The ground truth directly uses `sklearn.ensemble.StackingRegressor`, which is the standard and correct way to implement it.
*   **5. Feature Importance:** The feature importance plot uses the `features` list which are the original encoded features, but the `StandardScaler` is applied. This means the feature mapping might not be entirely correct or understandable without knowing which original feature corresponds to which scaled column. More importantly, it doesn't correctly handle the one-hot encoded features' names (which the ground truth does using `get_feature_names_out`).
*   **6. Residual Plots:** The residual plots only show one type of plot. The ground truth includes both ""Residuals vs Predicted"" and ""Distribution of Residuals,"" which are both important for error analysis (though the generated code does add ""Actual vs Predicted"" and ""Error Distribution"" which are good additions).
*   **7. Model Evaluation:** Many of the metrics are calculated only after a single `train_test_split` on the `X_test` set. The prompt specifically asks for cross-validation evaluation for *each* model. While `cross_val_score` is used for R, the other metrics (MAE, MSE, RMSE) are not shown for each fold or averaged over CV for individual models. The ground truth explicitly calculates and averages all required metrics over 5 folds for every model.

**Code Quality:**

*   **Imports:** Mostly correct, but `BaseEstimator` and `RegressorMixin` are imported for a custom class that could have been avoided.
*   **Structure:** The flow is somewhat logical, but the manual `X_train_scaled`, `X_test_scaled` creates maintenance issues compared to a pipeline.
*   **Missing Values:** The handling of missing values is somewhat basic (`dropna`, `fillna`). The `SimpleImputer` used in the ground truth within a pipeline is more robust for numerical and categorical data.
*   **Redundancy:** The custom `StackingRegressor` is unnecessary given `sklearn`'s built-in functionality.

**Strengths:**

*   **Comprehensive Model Range:** Addresses Random Forest, Gradient Boosting, XGBoost, AdaBoost, Voting, and Stacking.
*   **Hyperparameter Tuning Attempt:** It attempts to perform hyperparameter tuning using GridSearchCV.
*   **Multiple Evaluation Metrics:** Includes MAE, MSE, RMSE, and R.
*   **Visualization Attempts:** Tries to create feature importance and residual plots.
*   **Clear Output:** Prints evaluation metrics well.",claude-sonnet-4,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Statistical modeling imports
from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder
from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV
from sklearn.metrics import mean_...",5,"The generated code attempts to address all requirements but falls short significantly in several critical areas, leading to incorrect or incomplete implementations for a data science benchmark.

**Major Issues & Incorrect Implementations (Primary Reasons for Failure):**

1.  **ARIMA Modeling:**
    *   **Incorrect `ARIMA` class import:** Uses `statsmodels.tsa.arima.model.ARIMA` which requires the `order` parameter in a specific tuple format `(p, d, q)`, not `(p, d, q, P, D, Q, s)`. It does not perform `auto_arima` functionality as requested by the ground truth and is a more brittle approach for finding the best model.
    *   **Fixed Order:** The `fit_arima_model` hardcodes `order=(1,1,1)`, which is unlikely to be the optimal order for the given data and doesn't implement finding the ""best"" order as the ground truth does using `pmdarima.auto_arima`.
    *   **Time Series Indexing:** The ARIMA model is built on an integer-indexed DataFrame (`yearly_sales.set_index('Year', inplace=True)`). While `statsmodels` can sometimes handle this, it's best practice to use a proper datetime index for time series analysis to ensure correct frequency handling and forecasting, as done in the ground truth.

2.  **Multivariate Regression with Interaction Terms:**
    *   **Feature Encoding:** Uses `LabelEncoder` for categorical features. While `LabelEncoder` can be used, it implicitly assigns ordinal relationships which is usually inappropriate for nominal categorical variables in regression; one-hot encoding (`pd.get_dummies`) is the standard practice, as used in the ground truth.
    *   **Interaction Term Creation:** Manually creates `Year_Genre`, `Year_Platform`, `Genre_Platform` by multiplying encoded numerical values. This is fundamentally incorrect. Interaction terms should be between individual dummy variables (or their original categorical representation). The ground truth correctly uses `statsmodels.formula.api` which handles this implicitly or explicit interaction syntax (e.g., `A:B`) with one-hot encoded variables. Multiplying label-encoded integers yields meaningless interactions.
    *   **Confidence/Prediction Intervals for LR:** The custom `linear_regression_ci` function is overly complex and likely buggy, given the interaction terms. Statsmodels provides direct methods (`get_prediction().summary_frame()`) for this, which are more robust. Sklearn's `LinearRegression` does not directly provide prediction intervals; the ground truth explains this limitation and provides an approximation.

3.  **Polynomial Regression with Regularization:**
    *   **Data Preparation:** The `PolynomialFeatures` are applied to the full feature set `X` (including encoded categorical variables and manual interaction terms `Year_Genre` etc.). Polynomial regression is typically applied to numerical features. Applying it to label-encoded categorical features is conceptually flawed and creates an explosion of meaningless features. The ground truth correctly applies it only to `Year_of_Release`.
    *   **Pipeline Setup:** The pipeline setup (`ridge_pipeline`, `lasso_pipeline`) is correct for the transformation process but the input (`X_train`) is still problematic due to the previous issue with categorical features.

4.  **Bayesian Regression (PyMC3):**
    *   **Data Scaling:** The example uses `X_train_scaled` which suffers from the same issues as the regression models (includes problematic label-encoded features and incorrect interactions).
    *   **Model Specification:** The Bayesian model definition `mu = alpha + pm.math.dot(X_train, beta)` assumes all `X_train` are continuous features. It does not correctly handle categorical features or interactions in a Bayesian context, especially when using label encoding. The ground truth correctly defines separate priors for categorical effects.
    *   **Confidence/Prediction Intervals:** While `pm.sample_posterior_predictive` is used, the interpretation and display of the intervals could be clearer.

5.  **Survival Analysis:**
    *   **'Game Longevity' Definition:** The definition `df_survival['longevity'] = np.random.exponential(scale=df_survival['Global_Sales']*2 + 1) + 1` and `event` are purely synthetic and random. The task explicitly asked to implement survival analysis to model 'game longevity' _(years a game remains profitable)_. The ground truth attempts a plausible, though simplified, definition based on the difference between the max and min year a game appeared in the dataset. The generated code completely ignores the actual data for this, making the survival analysis meaningless in the context of the task.
    *   **Feature Encoding for CoxPHFitter:** The Cox model is built using `Genre_encoded` and `Platform_encoded`. Again, `LabelEncoder` is problematic. `lifelines` can handle string categorical features directly if they are specified correctly, or requires one-hot encoding.
    *   **Confidence/Prediction Intervals:** No explicit presentation of confidence/prediction intervals for CoxPH, only a summary. The ground truth provides coefficient CIs and plots a predicted survival function.

6.  **Time Series Cross-Validation:**
    *   **ARIMA TS-CV:** The `time_series_cv_arima` function uses the `TimeSeriesSplit` on `ts_data` but then still hardcodes `order=(1,1,1)` instead of using the learned optimal order (`arima_model.order`) or re-fitting a full `auto_arima` within each CV fold (which would be very slow but more robust). This makes the cross-validation less valuable.
    *   **Regression TS-CV:** The `regression_cv` uses `cross_val_score` with `TimeSeriesSplit`. It applies `Ridge` and `Lasso` to `X_train_scaled`, which has already been scaled *once* before the CV split. For proper CV, scaling should happen *inside* each fold to prevent data leakage from the test set into the training set's scaling parameters. This is achieved using a `Pipeline` where scaling is part of the model. The ground truth properly re-scales within each fold or integrates it into a pipeline before splitting.

**Code Quality and Completeness:**

*   **Imports:** Generally good with specific submodules.
*   **Structure:** Attempts to organize by task point, which is good.
*   **Documentation/Explanation:** Lacks inline comments or print statements explaining interim steps, which makes it harder to follow than the ground truth.
*   **Confidence/Prediction Intervals:** While some attempts are made (e.g., `linear_regression_ci`), they are often incorrect or not presented for all requested models (e.g., CoxPH, polynomial regression).
*   **Validation:** Implementation of TS-CV is flawed as noted above.

**Overall:** The code demonstrates a basic understanding of the required libraries and tasks but fails to correctly implement key statistical methodologies, particularly concerning feature encoding for categorical variables, interaction term creation, proper application of polynomial features, definition of longevity in survival analysis, and correct cross-validation practices. The random generation of survival data is a critical failure point.",claude-sonnet-4,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score
fro...",7,"The generated code attempts to address most of the complex requirements of the task, demonstrating a good understanding of advanced analytics concepts. However, it falls short in several areas including robustness, correctness of certain implementations, and adherence to specific instructions.

**Strengths:**
- **Comprehensive Feature Engineering:** The `create_advanced_features` function goes beyond the requested features and creates a very rich set of new variables, which is excellent for deep analysis.
- **Multiple Clustering Algorithms:** Implements K-Means, DBSCAN, Hierarchical, and GMM as requested.
- **Dimensionality Reduction:** Correctly uses PCA, t-SNE, and UMAP for visualization.
- **Optimal Cluster Determination:** Implements Elbow, Silhouette, and Gap Statistic, which is a strong point.
- **Consensus Clustering:** Attempts to implement consensus clustering, though with some flaws (see weaknesses).
- **Anomaly Detection:** Uses Isolation Forest and LOF.
- **Association Rule Mining:** Correctly implemented with `mlxtend`.
- **Recommendation System:** Provides a clustering-based recommendation system.
- **Validation Metrics:** Includes internal (Silhouette, Davies-Bouldin, Calinski-Harabasz) and external (ARI, NMI) validation.
- **Visualization:** Generates a comprehensive plot, which is a good effort but lacks proper handling of various plot types and potentially meaningful insights.
- **Output:** Provides a summary and saves enriched data.

**Weaknesses and Areas for Improvement:**
1.  **Missing `ColumnTransformer` for Preprocessing:** The ground truth code correctly uses `ColumnTransformer` and `OneHotEncoder` for mixed data types (numerical and categorical). The generated code uses `LabelEncoder` for categorical features, which is problematic for clustering as it implies an ordinal relationship that doesn't exist. This is a significant correctness issue in preprocessing for clustering. The task specifically asked for handling varied data types.
2.  **`Regional_Sales_Distribution` Implementation:** The generated code attempts to create `Regional_Sales_Distribution` as multiple columns `df[sales_columns].div(total_sales, axis=0)`. The `div` operation with a series will create a DataFrame where the columns are the sales columns, not a single 'Regional_Sales_Distribution' feature. The ground truth correctly creates individual proportion columns (`NA_Sales_Prop`, etc.), which is a more standard way to represent a distribution feature.
3.  **DBSCAN Parameter Tuning:** While the code attempts to dynamically set `eps` using NearestNeighbors and a percentile, this is a very rough heuristic. DBSCAN often requires more sophisticated tuning (e.g., using a k-distance plot and visually identifying the elbow, or more robust automated methods). The chosen `min_samples=4` is arbitrary and not justified, and the `eps` calculation is potentially unstable. The ground truth explicitly plots the k-distance graph for visual inspection, which is a better practice for DBSCAN.
4.  **Consensus Clustering Flaws:**
    *   The `consensus_clustering` function hardcodes `algorithms = ['kmeans', 'hierarchical', 'gmm']`, completely omitting DBSCAN. If DBSCAN yields useful results, it should be included.
    *   The normalization of the `consensus_matrix` by `len(algorithms)` is correct, but then `fill_diagonal(consensus_matrix, 1.0)` is used, but the initial matrix is already symmetric and initialized with zeros, so `np.fill_diagonal` isn't strictly needed if the loops correctly handle `i==j` (they are not iterated, so diagonals will remain 0 unless explicitly set). The ground truth correctly handles setting diagonals.
    *   Using `squareform` directly on `distance_matrix` (which is `1 - consensus_matrix`) is correct for linkage if the matrix is indeed a distance matrix.
    *   The use of `fcluster(linkage_matrix, t=optimal_k, criterion='maxclust') - 1` implies that `optimal_k` obtained earlier is directly applicable to the consensus clustering, which may not always be true, as consensus clustering might naturally form a different optimal number of clusters. However, for benchmark comparison, using a consistent `k` across algorithms is often required.
5.  **Statistical Significance Testing in Cluster Profiling:** The generated code uses `stats.mannwhitneyu` for numerical features and `chi2_contingency` for categorical. While these are appropriate tests, they are performed against ""other data"" which means comparing one cluster to *all other* clusters combined. A more robust approach for multiple clusters is ANOVA for numerical and Chi-squared contingency across *all* clusters simultaneously (as done in the ground truth) to determine overall significance, then potentially performing post-hoc tests to identify specific cluster differences if the ANOVA/Chi-squared is significant.
6.  **Recommendation System:** The recommendation system's `recommend_by_preferences` logic for sales level (`df['Global_Sales'].quantile`) uses quantiles of the *entire* dataset. While this is one way, it would be more effective if it leveraged the cluster profiles (e.g., recommend games from clusters characterized by high sales, or based on the cluster of games matching specific preferences). The `similarity` calculation `1 / (1 + abs(cluster_games['Global_Sales'] - target_sales))` is simplistic; it could incorporate more features or use distances in the feature space.
7.  **Plotting Issues:** The comprehensive visualization function tries to put too many plots in one figure, leading to very small and difficult-to-read subplots (3x4 arrangement). The PCA feature importance plot only shows PC1, which might not be the most representative dimension. The sales distribution by cluster plot uses `log` scale, which is fine, but needs to be clearly labeled. Generally, several plots would benefit from better labeling, legends, and overall presentation.
8.  **Redundant Imports/Code:** Some imports (`cross_val_score`, `DecisionTreeClassifier`, `itertools`, `Counter`, `defaultdict`) are present but not used in the provided code.
9.  **Date Handling:** The ground truth code correctly identifies `Year_of_Release` and explicitly converts it to `int` and handles missing values more robustly. The generated code uses `Year` directly and just drops `Year` NaNs. This might not be as robust depending on the column name variant and potential non-numeric entries.
10. **Hardcoded `n_neighbors` for DBSCAN:** The `NearestNeighbors(n_neighbors=4)` and then `distances[:, 3]` is not ideal. It should be dynamic based on dimensionality for `min_samples` heuristic. The ground truth uses `2 * X_processed.shape[1]`.
11. **`Optimal_k` Selection:** The code selects `optimal_k = optimal_results['silhouette']`. While `silhouette_score` is a good metric, it's generally best practice to consider all three methods and perhaps discuss the discrepancies if they suggest different `k` values, rather than just picking one.

Overall, the code is ambitious and attempts to cover all requirements, but lacks the meticulous detail and robustness needed for a production-ready or expert-level solution in several key areas. The preprocessing for clustering (especially categorical features) and the statistical significance testing implementation are notable weaknesses compared to the ground truth.",claude-sonnet-4,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler, PolynomialFeatures
from sklearn.ensemble import (VotingClassifier, BaggingClassifier, AdaBoostClassifier, 
                             GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier)
from sklearn.linear_mo...",7,"The generated code provides a comprehensive attempt at fulfilling the task requirements, implementing many advanced techniques.

**Strengths:**
*   **Comprehensive Feature Engineering:** The `FeatureEngineer` class is well-structured and includes various types of feature creation (categories, ratios, transformations) and scaling.
*   **Multiple Ensemble Methods:** It correctly implements Voting (soft/hard), Bagging, Boosting (AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost), and Stacking.
*   **Bayesian Optimization with Optuna:** The `BayesianOptimizer` class is a good implementation of hyperparameter tuning using Optuna for XGBoost and LightGBM.
*   **Custom Ensemble with Dynamic Weight Assignment:** The `DynamicWeightedEnsemble` is an excellent demonstration of a custom ensemble, including a meta-learner and dynamic weight calculation.
*   **Model Calibration and Reliability Analysis:** The `ModelCalibrationAnalyzer` class correctly implements calibration curves and Brier score.
*   **Ensemble Diversity Analysis:** Includes `EnsembleDiversityAnalyzer` for calculating and visualizing disagreement and kappa diversity matrices, which is a strong point.
*   **SHAP Interpretation:** The `SHAPInterpreter` class correctly utilizes SHAP for model explainability.
*   **Visualization:** Includes plotting for ROC curves, Precision-Recall curves, and feature importance.

**Areas for Improvement (Why not higher score):**

1.  **Nested Cross-Validation for Hyperparameter Tuning:** The ground truth explicitly asked for nested cross-validation for *unbiased model evaluation*, which implies that hyperparameter tuning (like the Optuna step) should be *inside* the inner loop of a nested CV to prevent data leakage in parameter selection. The generated code performs Optuna tuning once *before* any CV for evaluation, and then uses those fixed parameters for the subsequent evaluation (which isn't nested CV for the *tuned* models, but rather just a train-test split evaluation or simple k-fold for some metrics). The `NestedCrossValidator` class is defined but **not actually used** in the main execution to evaluate the *tuned* models or ensembles with nested CV. It's set up to do GridSearchCV in its inner loop, but this logic is not applied to `xgb_optimized`, `lgb_optimized`, or the ensembles. This is a critical omission for ""unbiased model evaluation.""

2.  **Dataset Preprocessing (0s as NaN):** The ground truth specifically highlighted handling `0` values in certain critical features (Glucose, BloodPressure, SkinThickness, Insulin, BMI) as `NaN` and then imputing them. The generated code does not explicitly perform this crucial step, which can significantly impact model performance and data interpretation for this specific dataset. The `FeatureEngineer` class does not handle this.

3.  **CatBoost and Categorical Features:** The ground truth used `CatBoost` but specified `cat_features=None` because, after preprocessing, there were no explicit categorical features. The generated code uses `cb.CatBoostClassifier` but doesn't explicitly handle or identify categorical features, which could be an issue if the `FeatureEngineer` introduced new categorical columns (e.g., `BMI_Category`, `Age_Group`) without informing CatBoost of them (though CatBoost sometimes can infer, explicit handling is better). The ground truth's pre-processing also simplified the feature space by making everything numerical and scaling it, reducing the need for `cat_features`.

4.  **Redundancy/Efficiency:** The `NestedCrossValidator` class defines `GridSearchCV` inside, but Optuna is preferred for Bayesian optimization. Ideally, the nested CV should integrate the Optuna search for hyperparameter tuning in its inner loop for the relevant models.

5.  **Lack of Coherent Evaluation Loop:** The code evaluates individual models and then ensembles separately. While it calculates AUC on a single hold-out `X_test`, it doesn't clearly demonstrate the ""nested cross-validation for unbiased model evaluation"" across *all* models and ensembles, especially for the hyperparameter-tuned ones. The ground truth demonstrated a clear single loop for nested CV that evaluated all models consistently.

Overall, the code is very strong in implementing many individual advanced techniques, but the central ""nested cross-validation for unbiased evaluation"" and handling of missing values (0s) are not fully realized as intended by the problem statement, which hinders a higher score.",claude-sonnet-4,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Statistical and Bayesian libraries
import pymc3 as pm
import arviz as az
import theano.tensor as tt

# Machine learning libraries
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.impute import KNNImputer
from sklearn.neighbors import NearestNeighbors
from sklearn....",7,"The generated code provides a good attempt at meeting the complex requirements of the task. It successfully implements Bayesian modeling using PyMC3, including hierarchical structures, mixed-effects, and logistic regression with informative priors. MCMC sampling is integrated into these models. Survival analysis is performed with simulated data, and Gaussian processes are used for non-parametric modeling. Causal inference methods (PSM and a simplified IV) are also included. Bayesian model comparison via WAIC and LOO, and posterior predictive checks, are present.

However, there are several areas for improvement and some inaccuracies that prevent a higher score:

1.  **Correctness/Completeness of Mixed-Effects Model (Task 3):** The `mixed_effects_model` in the generated code is not a true mixed-effects model in the statistical sense (e.g., as implemented in the ground truth using group-specific intercepts or slopes that vary *around* a global mean). It defines `beta_fixed` for common effects and `beta_random` as group-specific *individual* effects rather than actual deviations from fixed effects. The ground truth correctly implements a hierarchical model which serves as a type of mixed-effects model by having group-specific intercepts (`alpha[group_idx]`) informed by a hyperprior (`mu_alpha`, `sigma_alpha`). The generated code's definition of `beta_random` as `mu=0, sigma=sigma_group, shape=n_groups` means each group gets an independent random effect, which is a start, but its integration with `beta_fixed` might not reflect standard mixed-effects formulations where `beta_fixed` applies to covariates across all groups, and `beta_random` allows for variation in *slopes or intercepts* per group. The ground truth's `hierarchical_model` is a better representation of a mixed-effects logistic regression as described.

2.  **Gaussian Processes (Task 7):** The generated code uses `sklearn.gaussian_process.GaussianProcessRegressor` which is a frequentist approach. The task explicitly asks for using Gaussian Processes for *non-parametric modeling of relationships between variables* in the context of the overall task which implies a Bayesian context (as it follows other PyMC3 models). The ground truth correctly implements this using `pymc3.gp`. This is a significant deviation from the spirit of the task's emphasis on Bayesian methods.

3.  **Causal Inference - Instrumental Variables (Task 8):** The generated code implements a frequentist Two-Stage Least Squares (2SLS) approach for IV. While it states it's simplified, the overall task context emphasizes Bayesian methods. The ground truth also uses a frequentist 2SLS, acknowledging the complexity of implementing a full Bayesian IV model within a comprehensive script, so this is a minor point compared to the GP approach.

4.  **Survival Analysis Simulation (Task 6):** While both generated and ground truth code simulate the time component, the ground truth's simulation links `time_to_event` to a calculated `Risk_Score` based on diabetes features, making it more plausible for demonstration. The generated code uses a simple exponential distribution and binomial for `observed`, which is less nuanced in its relation to the actual predictors in the dataset.

5.  **Data Preprocessing and Feature Engineering:** The generated code adds interaction terms and different age grouping. While potentially beneficial for modeling, the prompt primarily asked for standard preprocessing and ""comprehensive missing value handling,"" implying robustness. The ground truth stuck to simpler, more direct preprocessing as the models themselves were complex.

6.  **Code Structure and Execution:** The generated code puts most of the logic inside functions and then calls them sequentially, which is clean. However, the final `print(cph.summary)` might cause issues as `cph.summary` is not directly a string in `lifelines`; it's an object with a `print_summary()` method. The ground truth included `cph.print_summary()` which is correct.

Overall, the generated code demonstrates a strong understanding of PyMC3 and Bayesian modeling principles, covering most requirements. The key areas for improvement are replacing the scikit-learn GP with a PyMC3 GP and refining the mixed-effects model to be a more standard Bayesian mixed-effects formulation.",claude-sonnet-4,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering, OPTICS
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, silhouette_samples, adjusted_rand_score, normalized_mutual_inf...",7,"The generated code demonstrates a strong understanding of many advanced data science techniques and attempts to fulfill all requirements. However, there are several areas where it falls short in terms of direct implementation against the task description and best practices for this specific problem:

**Strengths:**
*   **Comprehensive Feature Engineering:** The `AdvancedFeatureEngineering` class is well-structured and creates many relevant metabolic and risk indicators.
*   **Multiple Dimensionality Reduction Techniques:** Correctly implements PCA, t-SNE, UMAP, and autoencoder for embeddings.
*   **Multiple Clustering Algorithms:** Covers K-Means, GMM, Hierarchical, Spectral, DBSCAN, and OPTICS.
*   **Clustering Parameter Optimization:** Attempts Elbow, Silhouette, and Gap Statistic for optimal K.
*   **Consensus Clustering:** Provides a decent implementation of co-occurrence matrix based consensus clustering.
*   **Patient Phenotyping:** Creates cluster profiles and performs statistical significance testing (ANOVA/Kruskal-Wallis).
*   **Anomaly Detection:** Correctly uses Isolation Forest and LOF.
*   **Association Rule Mining:** Implements feature discretization and Apriori/association rules.
*   **Risk Stratification:** Attempts to assign risk levels based on cluster diabetes rates.
*   **Visualization:** Includes a section for creating comprehensive plots, which is a good addition.

**Areas for Improvement / Missing Aspects:**
1.  **Missing Value Imputation:** The ground truth explicitly handles missing values by replacing 0s with NaNs and then imputing with median. The generated code only uses `fillna(df_engineered[numerical_cols].median())` *after* feature engineering. This order can be problematic, and the `0` values in sensitive columns like Glucose, BloodPressure, etc., are usually missing indicators, not actual zeros, and should be handled *before* feature creation. The ground truth's approach is more robust.
2.  **Explicit Optimal Parameter Determination for *All* Clustering Algorithms:** The task clearly asks to determine optimal clustering parameters for *multiple clustering algorithms*. The generated code primarily focuses on `k` (optimal_k) for K-Means/GMM/Hierarchical/Spectral, but it uses hardcoded or default values for DBSCAN (`eps=0.5`, `min_samples=5`) and OPTICS (`min_samples=5`). The ground truth calculates `min_samples` for DBSCAN/OPTICS and encourages visual inspection for `eps`, which is a better approach given the complexity.
3.  **Statistical Validation of Clustering Parameters:** While it attempts statistical validation for K, it doesn't explicitly link this to the clusters (e.g., using permutation testing or other formal validation methods for cluster stability beyond just K). The `silhouette_analysis` and `gap_statistic` are parameter search methods, not statistical validation of chosen clusters.
4.  **Time-Series Clustering:** The generated code's implementation of time-series clustering is very rudimentary and not applicable to the cross-sectional `diabetes.csv` dataset. It attempts to segment patients *by age*, which is not time-series clustering; it's just another form of segmentation. The ground truth correctly identifies that the dataset is cross-sectional and provides a conceptual explanation of how time-series clustering *would* be applied, which is more honest and appropriate given the data limitation. The prompt asked for ""time-series clustering if temporal patterns exist in the data"", which the generated code fails to accurately assess for this dataset while providing an unhelpful ""implementation.""
5.  **Clinical Interpretation of Risk Stratification:** The generated code defines risk levels (""Low"", ""Medium"", ""High"") but lacks the detailed textual clinical interpretation for each cluster as provided in the ground truth. This is a crucial aspect of patient segmentation analysis.
6.  **Code Structure and Execution Flow:** The generated code is well-modularized into classes, but it lacks the clear sequential execution flow with print statements and intermediate check-ins that make the ground truth more readable and demonstrative of the steps being performed. The ground truth provides explicit ""--- 1. Loading and Preprocessing Data ---"" headers, which aids in understanding.
7.  **Autoencoder Activation Function:** The autoencoder output layer `Dense(input_dim, activation='sigmoid')` might not be appropriate for scaled features (which can extend below 0 or above 1). A `linear` activation is generally more suitable for reconstructing `StandardScaler` outputs. The ground truth uses `linear`.
8.  **DBSCAN/OPTICS `eps` and `min_samples` determination:** While the ground truth also provides a placeholder for `eps` in DBSCAN (encouraging visual inspection), it's more explicit about how to estimate it (k-distance graph) and `min_samples` (2 * #features). The generated code uses arbitrary `eps=0.5, min_samples=5`.

Overall, a very good effort with significant overlap with the ground truth's capabilities, but some critical nuances outlined above prevent it from scoring higher.",claude-sonnet-4,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Core ML libraries
from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_regression, RFE, SelectFromModel
from sklearn.metrics import mean_squared_error, mean_ab...",8,"The generated code addresses most of the requirements of the task.

**Strong Points:**
- **Comprehensive Ensemble Methods:** Implements a wide range of ensemble techniques including Voting (hard and soft), Bagging with various estimators, and Boosting algorithms (AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost).
- **Advanced Feature Engineering:** Goes beyond basic preprocessing by including interaction, polynomial, ratio, statistical, and clustering features. This is a significant strength.
- **Bayesian Optimization (Optuna):** Correctly integrates Optuna for hyperparameter tuning.
- **Nested Cross-Validation:** Implements nested cross-validation, a crucial aspect for robust model evaluation and hyperparameter tuning.
- **Model Interpretation (SHAP & LIME):** Includes both SHAP and LIME for model interpretability, which is excellent.
- **Model Calibration & Uncertainty Quantification:** Attempts to address these complex aspects, specifically with `IsotonicRegression` for calibration and ensemble statistics for uncertainty.
- **Custom Ensemble with Dynamic Weight Assignment:** Successfully implements this, calculating weights based on cross-validation performance.
- **Blending and Multi-level Stacking:** Provides implementations for these advanced ensemble techniques.
- **Ensemble Diversity Analysis and Combination Optimization:** Calculates correlation and Q-statistics for diversity, and optimizes weights using `scipy.optimize.minimize`.
- **Code Structure and Readability:** The code is well-structured with functions for different logical parts, improving readability and maintainability.
- **Comprehensive Evaluation:** Provides a clear comparison of model performance metrics.

**Areas for Improvement / Minor Issues:**
- **Preprocessing Pipeline Integration (Minor):** While `RobustScaler` is applied, the extensive feature engineering is done *before* the train-test split and scaling in `main()`, which could lead to data leakage if not perfectly handled. Ideally, the `advanced_feature_engineering` function should be part of a `sklearn.pipeline.Pipeline` or `ColumnTransformer` to ensure transformation fit during training and evaluation. The `full_pipeline` or `preprocessor` concept from the ground truth is more robust.
- **`use_label_encoder` warning/deprecation (Minor):** The `use_label_encoder=False` and `eval_metric='rmse'` in XGBoost are good, but some older `sklearn` versions might still show a warning if `use_label_encoder` is deprecated. The generated code doesn't explicitly suppress this if a global warning filter isn't enough, but it does handle it.
- **SHAP/LIME on scaled data (Minor):** SHAP and LIME are performed on `X_test_scaled` and `X_train_scaled`. While mathematically correct, interpreting features that have been heavily transformed (e.g., polynomial, interaction, scaled) can be more challenging for a human. It's often preferred to interpret on original feature space if possible, or at least be explicit about the interpretation context. The ground truth attempts to get original feature names where possible after preprocessing.
- **Hard Voting for Regression (Minor):** The prompt specifically asks for ""hard and soft voting"". For regression, hard voting generally implies simple averaging (which `VotingRegressor` with `weights=None` or equal weights handles). The current code implements soft voting implicitly via `VotingRegressor` with default behavior and then custom weighted average. It might benefit from a clear ""hard voting"" demonstration.
- **KMeans n_clusters (Minor):** Using a fixed `n_clusters=5` for KMeans without trying to optimize it or relate it to dataset characteristics is a bit arbitrary.
- **`MultiLevelStacking` Implementation Details (Minor):** The `MultiLevelStacking` class's `predict` method for Level 2 combines `X.values` with Level 1 predictions without explicit scaling of `X` if it's not already within the model for Level 2, which could be inconsistent with `fit` or overall pipeline if `X` isn't scaled in `fit`. The ground truth's `full_pipeline` approach handles this more cleanly by including the preprocessor in the pipeline.

The code is generally very strong and implements almost all challenging aspects of the prompt. The issues are mainly refinement and best practices rather than critical failures.",claude-sonnet-4,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Statistical modeling
import pymc3 as pm
import arviz as az
import theano.tensor as tt
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.impute import KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.ensemble import RandomForestRegr...",6,"The generated code attempts to address all requirements of the task, which is commendable given the complexity. It includes implementations for hierarchical Bayesian modeling, SEM, causal inference methods (IV and PSM), Bayesian regression, Gaussian processes, mediation analysis, DiD, and model comparison. It also has a good preprocessing step and visualization.

However, there are several issues and areas for improvement, particularly concerning the *correctness and robustness* of some statistical implementations and the use of libraries as specified in the prompt:

**Areas for Improvement / Issues:**

1.  **PyMC3 Usage & Robustness:**
    *   **SEM with PyMC3:** The prompt explicitly asks for SEM using PyMC3 or PyMC4. The generated code uses `semopy`, which is a different library. While `semopy` is a valid SEM library, it deviates from the specified PyMC3 requirement. Furthermore, the `try-except` block around the `semopy` call suggests a lack of confidence or robustness in its direct integration/execution within the overall script. The ground truth correctly approximates SEM using a series of interconnected PyMC3 models.
    *   **PyMC3 Sampling:** The `target_accept` parameter in `pm.sample` (set to 0.95) is good practice, but the warning suppression for `UserWarning` might hide important sampling diagnostics if this isn't sufficient.
    *   **Insufficient Sampling for Comparison Models:** In `bayesian_model_comparison`, the sampling for `model1`, `model2`, and `model3` is very short (`1000` draws, `500` tune). This is unlikely to yield stable or reliable posterior distributions, especially for comparison. The hierarchical and informative prior models use `2000` draws and `1000 tune`, which is better but still potentially low for complex models.

2.  **Causal Inference - IV Implementation:**
    *   The IV implementation uses a very simple, potentially flawed instrument: `regional average GDP (excluding own country)`. While the intention is good, this is not a strong or robust instrument in a real-world setting without strong theoretical justification and validation. A more appropriate approach, as seen in the ground truth, would be to simulate a clearly exogenous instrument for demonstration purposes or state explicitly the limitations of finding a valid instrument in cross-sectional data. The use of `sklearn.linear_model.LinearRegression` directly for 2SLS lacks the diagnostics and standard errors typically provided by `statsmodels`, which the ground truth uses effectively.

3.  **Causal Inference - PSM Matching Logic:**
    *   The PSM matching implementation is quite basic. It iterates through treated units and finds *one* closest control. This is a 1:1 matching without replacement. While functional, it could be improved with k-nearest neighbors (k>1), matching with replacement, or more robust matching algorithms (e.g., genetic matching, optimal matching). More critically, the `caliper` is hardcoded and its effect isn't clear from the `matches` structure. The t-test for ATE is also not performed (though the ground truth does this).

4.  **Data Preprocessing and Variable Standardization:**
    *   The preprocessing attempts multiple imputation (KNN and IterativeImputer), but the combination method (averaging imputed values) is not a standard or statistically sound way to combine imputations. Proper multiple imputation involves running the analysis on *each* imputed dataset and then combining the results using Rubin's rules. This increases complexity, but the current approach could introduce bias.
    *   Numerical variables are standardized (`StandardScaler`), but `happiness` itself is not scaled. While not strictly necessary for regression, it can sometimes aid interpretation, especially when setting priors based on scaled predictors. This isn't a critical error, but worth noting. The ground truth applies standardization more consistently across predictors.

5.  **SEM using `semopy` vs. PyMC3/PyMC4:**
    *   This is a direct violation of the prompt's explicit requirement. The ground truth demonstrates how to approximate SEM using interconnected Bayesian models in PyMC3, which is the expected approach.

6.  **Mediation Analysis:**
    *   The mediation analysis is performed using `LinearRegression` (OLS), which is a classical approach. While correct, the prompt implies a Bayesian context for most tasks. A more integrated Bayesian mediation analysis would involve computing indirect/direct effects from the posterior distributions of the relevant parameters, as demonstrated in the ground truth.

7.  **Gaussian Processes:**
    *   The use of `sklearn.gaussian_process.GaussianProcessRegressor` is standard for GPs. The prompt asks for *PyMC3/PyMC4* for Bayesian models. While GPR can be used, GPs can also be implemented within PyMC3 for a fully Bayesian treatment (e.g., through `pm.gp`). The generated code does not do this, adhering to the specified library.

8.  **Difference-in-Differences:**
    *   The simulation of pre/post data is a reasonable approach for a cross-sectional dataset. The implementation with `LinearRegression` is fine for the DID estimator.

9.  **Bayesian Model Comparison:**
    *   The model comparison is performed using `az.compare` (WAIC), which is correct. However, the models being compared are limited to simple linear regressions, not necessarily the hierarchical models or other complex models developed, which could be part of a broader comparison.

10. **Code Structure and Readability:**
    *   The code is quite monolithic. Breaking down large functions into smaller helper functions with clear responsibilities, especially for the complex modeling steps, could improve readability and maintainability.
    *   Variable naming is generally good.

Overall, the code demonstrates a good understanding of *what* needs to be done, but the *how* for certain components (especially SEM, Bayesian GPs, and the depth of Bayesian inference for some causal parts) could be improved to better align with the explicit requirements and best practices for robust statistical modeling in a Bayesian context.",claude-sonnet-4,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE, LocallyLinearEmbedding, Isomap
from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual...",7,"The generated code attempts to address all requirements of the task, demonstrating a good understanding of advanced analytics concepts. However, there are several areas where the implementation falls short or could be improved for correctness, robustness, and completeness as per the detailed ground truth.

**Strengths:**
*   **Comprehensive Coverage:** Most of the requested advanced techniques (multiple clustering algorithms, dimensionality reduction, autoencoders, anomaly detection, network analysis, association rule mining, recommendation system) are included.
*   **Modular Design (Class-based):** Encapsulating the logic within a `HappinessAnalyzer` class is a good practice for organization and reususability.
*   **Feature Engineering:** Includes a decent attempt at advanced feature engineering with new derived features.
*   **Hyperparameter Tuning & Optimal K:** The `optimal_clustering_parameters` method attempts to implement methods like Silhouette, Gap Statistic, and Stability Analysis for determining optimal 'k', which is a crucial and advanced requirement.
*   **Consensus Clustering:** The implementation of consensus clustering using a co-association matrix is a strong point.
*   **Anomaly Detection:** Utilizes multiple anomaly detection algorithms (Isolation Forest, LOF, Elliptic Envelope, One-Class SVM) and even attempts a ""consensus anomaly"" approach.
*   **Network Analysis:** Uses `networkx` for graph creation and calculates basic network metrics.
*   **Association Rule Mining:** Properly uses `mlxtend` for Apriori and association rules.
*   **Geopolitical Validation:** Attempts a basic geopolitical validation by mapping countries to regions and performing a chi-squared test.
*   **Visualization & Reporting:** Includes basic visualization and a summarized report, which are helpful for interpreting results.

**Areas for Improvement / Missing Aspects (compared to Ground Truth):**
1.  **Preprocessing Robustness:**
    *   **Imputation:** While it handles numeric missing values with median, it doesn't explicitly handle categorical missing values (though the given dataset might not have them).
    *   **Regional Indicator Handling:** The ground truth correctly identifies `Regional indicator` as a categorical feature and one-hot encodes it, making it part of the features for clustering. The generated code attempts to use it only in the very end for geopolitical validation, missing its inclusion as a feature for clustering. This is a critical omission as region is a major differentiating factor for happiness.
    *   **Feature Selection for Clustering:** The ground truth carefully selects numerical features and one-hot encoded regions for clustering. The generated code creates many engineered features but then implicitly includes them all regardless of their relevance or potential multicollinearity with other features.
2.  **Optimal Clustering Parameter Determination:**
    *   **Elbow/Silhouette/Gap Stat Plots:** While the methods are internally calculated, the code does not generate separate plots for Elbow, Silhouette, and Gap Statistic to visually aid in optimal K determination, as hinted in the ground truth's description of 'optimal clustering parameters'. The ground truth explicitly states these are integrated into the clustering process.
    *   **Stability Analysis:** The stability analysis in the generated code uses `KMeans` with two different random states on bootstrapped data, then computes `adjusted_rand_score`. This is a valid approach, but the description of the ground truth implies a more general approach where the stability is assessed across multiple initializations or perturbations, not just for one specific algorithm.
3.  **DBSCAN/HDBSCAN Parameter Optimization:** The generated code attempts DBSCAN parameter optimization, but for HDBSCAN, it uses a fixed `min_cluster_size=5` without explicit optimization. The ground truth attempts a small loop for HDBSCAN `min_cluster_size`.
4.  **Autoencoder Implementation:**
    *   **Scaling for AE:** The generated code uses `minmax_scaled_data` for autoencoder training, which is appropriate for a sigmoid output, but it doesn't mention or show the conversion of the original `feature_cols` to the [0,1] range for this specific scaler setup. The Ground Truth makes this explicit.
    *   **`n_init` for KMeans:** The generated code specifies `n_init=10` for K-Means, which is good.
5.  **Consensus Clustering Details:**
    *   The ground truth uses `linkage(distance_matrix[np.triu_indices(n_samples, k=1)])` for hierarchical clustering on the distance matrix, which is more robust than `squareform` directly on `distance_matrix` (which assumes a Euclidean distance, not a similarity-derived distance). The ground truth also explicitly checks for valid clustering results (i.e., multiple clusters, not just one) before including them in consensus, making it more robust.
    *   The generated code relies on `self.optimal_params['kmeans_k_silhouette']` for the number of consensus clusters, which is reasonable, but the ground truth explicitly re-optimizes this *for consensus* using silhouette, rather than just reusing a K-Means optimal K, which is more precise.
6.  **Country Profiles:**
    *   **Statistical Significance:** The generated code uses `ttest_ind` between a cluster and ""other clusters."" This is a pairwise t-test, which isn't ideal for comparing means across *multiple* groups. The ground truth correctly uses `f_oneway` (ANOVA), which is the standard method for comparing means of a numerical variable across more than two groups (clusters).
    *   **Effect Size:** While it calculates Cohen's d, the interpretation/context might be simpler with Eta-squared as used in the ground truth, which is often standard for ANOVA.
    *   **Original Features:** The generated code performs profiling on features available within the `HappinessAnalyzer` class, which are derived. The ground truth explicitly goes back to the *original (unscaled) numerical features* for profiling, which makes the interpretation of mean values far more intuitive and actionable for policy insights.
7.  **Network Analysis Details:**
    *   **Similarity Metric:** The generated code uses `np.corrcoef` which computes the Pearson correlation coefficient. While a valid similarity measure, the ground truth uses `cosine_similarity`, which is often preferred for feature vectors.
    *   **Threshold:** The generated code applies a fixed threshold of 0.8. The ground truth uses `np.percentile(positive_similarities, 90)`, which dynamically sets the threshold based on the data, forming links only among the most similar pairs, leading to a sparser and more interpretable network.
    *   **Labels in Graph:** The ground truth draws labels for nodes in the network plot, which is crucial for interpretability. The generated code does not.
    *   **Community Detection:** The generated code uses `greedy_modularity_communities` but makes it error-tolerant without explaining why that might be needed.
8.  **Recommendation System & Policy Insights:**
    *   **Recommendation Type:** The generated code's recommendation system is more focused on ""best practices within a cluster,"" which is good. The ground truth provided a basic country recommendation system (countries in the same cluster) and then delves deeper into broader policy insights by comparing high and low-happiness clusters based on their average feature profiles.
    *   **Qualitative Analysis in Policy Insights:** The ground truth provides rich qualitative descriptions of policy insights derived from cluster comparisons, going beyond just listing features, which is essential for ""policy insights."" The generated code is more quantitative initially.
    *   **Geopolitical Validation Integration:** The ground truth explicitly integrates geopolitical validation *within* the policy insights section as a cross-check, which is a good contextualization. The generated code performs it as a separate step.
9.  **Visualizations:**
    *   **Clarity:** The autoencoder plot's legend might be missing/hard to interpret without explicit colorbars or labels.
    *   **Completeness:** The clustering comparison plot is good, but the overall visualization setup is less coherent than the ground truth which visualizes each DR embedding with clusters.
10. **Code Quality:**
    *   **Imports:** Some unused imports (`PCA`, `MLPRegressor`, `mutual_info_regression`, `chi2_contingency`, `kruskal`).
    *   **`n_init` warning fix:** The ground truth explicitly states `n_init=10` to avoid warnings with `KMeans`. The generated code does use `n_init=10`.
    *   `verbose=0` in `fit` calls is good for cleaner output.

In summary, the generated code is a solid attempt but lacks some of the finer details, robustness, and statistical rigor present in the ground truth, particularly in preprocessing, statistical validation of clusters, optimal parameter determination, network analysis, and the depth of policy insight generation.",claude-sonnet-4,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
from pmdarima import auto_arima
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# Load and prepare data
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])
germany_data = df[df['location'] == 'Germany'].copy()
germany_data.set_index('date', inplace=True)
germany_data.sort_index(...",8,"The generated code successfully implements the SARIMA forecasting task for 'new_cases' in Germany, incorporating exogenous variables and handling missing data. It meets most of the requirements: data loading and preparation, target and exogenous variable selection, missing value handling, train/test split, SARIMA model implementation with auto_arima for order selection, model training, forecasting, RMSE evaluation, and plotting.

Strong points:
- Correctly uses `pmdarima.auto_arima` for hyperparameter tuning.
- Correctly uses `statsmodels.tsa.statespace.sarimax.SARIMAX` for model fitting and forecasting with exogenous variables.
- Implements appropriate train/test splitting.
- Calculates RMSE and plots the results.
- Includes necessary imports and warning suppression.

Areas for improvement to reach a higher score:
- **Missing Value Handling Detail:** While the generated code attempts to handle missing values, the ground truth's approach using `ffill` then `bfill` for `new_cases` and `interpolate` then `ffill`/`bfill` for exogenous variables is more robust and explicitly handles potential NaNs at the beginning or end of the series for each column. The generated code applies `interpolate` directly on `exog_cols` after `ffill` only on the target, which might not fill all initial/trailing NaNs for exogenous variables. The ground truth also includes checks for remaining NaNs and robust fallbacks, which is good practice.
- **Handling of negative forecasts:** The ground truth explicitly clamps negative forecasts to 0, which is important for count data like 'new_cases'. The generated code does not include this logical post-processing step.
- **Error Handling/Edge Cases:** The ground truth contains more robust error handling, such as checking if the dataset file exists, if there's enough data for the test set, and if the DataFrame becomes empty after dropping NaNs. This makes the code more production-ready.
- **Plotting details:** The generated plot is good, but the ground truth includes plotting the training data, which provides more context to the time series plot.
- **Clarity in auto_arima parameters:** The ground truth specifies `start_p`, `start_q`, `max_p`, `max_q`, `start_P`, `start_Q`, `max_P`, `max_Q`, and `test='adf'`, giving `auto_arima` more guidance and making the process more explicit. The generated code uses default ranges, which might be fine but less explicit.",claude-sonnet-4,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import shapiro, kruskal, f_oneway
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scikit_posthocs import posthoc_dunn
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Define quality categories
def categorize_quality(score):
    if score <= 4:
        return 'Low'
    elif scor...",7,"The generated code largely addresses the requirements of the task. It correctly categorizes quality groups, performs statistical tests (ANOVA and Kruskal-Wallis), conducts post-hoc analysis, and generates visualizations.

However, there are several areas for improvement:

1.  **Normality Test Implementation**: The `check_normality` function is flawed. It groups the data by `quality_category` *before* checking normality, but then the actual `shapiro` call inside the loop is on `group`, which is just a single series of values for one property across all categories. It should be checking normality for *each quality group* (low, medium, high) for a given chemical property independently. The ground truth correctly checks `shapiro(low_quality_data)`, `shapiro(medium_quality_data)`, etc., which is the correct approach. The generated code also lacks a check for homogeneity of variances (Levene's test), which is a necessary assumption for ANOVA. The ground truth correctly includes this.
2.  **Handling of Empty/Insufficient Data**: The generated code checks if groups are empty (`len(low) == 0`). However, for `shapiro` and `f_oneway`/`kruskal` to function correctly, there should be a minimum number of samples (e.g., `shapiro` requires N >= 3, `f_oneway` needs at least 2 per group for meaningful comparison). The ground truth handles this more robustly by checking `len(data) < 3` for Shapiro-Wilk and `len(data) < 2` for pairwise comparisons.
3.  **Post-hoc for Kruskal-Wallis (Dunn's test)**: While the use of `scikit_posthocs.posthoc_dunn` is generally correct for non-parametric post-hoc, the ground truth's implementation of pairwise Mann-Whitney U with Bonferroni correction is also a widely accepted approach in the absence of a direct Dunn's test implementation. The generated code correctly uses `posthoc_dunn`.
4.  **Error Handling**: The generated code does not include error handling for `FileNotFoundError` when loading the CSV, unlike the ground truth.
5.  **Readability/Comments**: The ground truth includes more verbose comments and print statements, which makes the execution flow and decision-making process (e.g., why ANOVA or Kruskal-Wallis was chosen) much clearer.
6.  **Plotting Structure**: The box plot generation logic in the generated code for handling `fig, axes` arrangement can be overly complex for simple cases. The ground truth iterates through significant features and creates a new figure for each, which is simpler and makes sure each plot is clearly separated. While the generated code's grid approach is space-efficient, its implementation for single plots or specific numbers of plots could be cleaner.
7. **Alpha Level Consistency**: The ground truth defines `ALPHA = 0.05` globally and uses it consistently. The generated code hardcodes `0.05` in multiple places.",claude-sonnet-4,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import roc_auc_score, f1_score, classification_report
from sklearn.compose import ColumnTransformer
import xgboost as xgb
import shap
import warnings
warnings.filterwarnings('ignore')

# 1) Load data and parse date
df = pd.read_csv('synthetic_covid19_data.csv')
df['date...",7,"The generated code correctly addresses most of the requirements and successfully implements the major steps of the task.

**Strengths:**
*   **Data Loading and Sorting:** Correctly loads data and sorts by location and date, which is crucial for time-series operations.
*   **Feature Engineering (Rolling & Lag):** Implements rolling averages/stds and lag features largely as requested.
*   **Missing Value Handling:** Attempts to handle missing values, though with some minor issues (see below).
*   **Feature Selection:** Selects the specified features for modeling.
*   **Preprocessing (Scaling & OHE):** Uses `ColumnTransformer` for numerical scaling and one-hot encoding, which is good practice.
*   **Time-Based Split:** Correctly implements a time-based split for the test set.
*   **XGBoost Training & Tuning:** Uses `XGBClassifier` and `GridSearchCV` with `TimeSeriesSplit` for hyperparameter tuning.
*   **Evaluation:** Calculates ROC-AUC and F1-score as requested.
*   **SHAP Explanation:** Implements SHAP for feature importance and attempts to visualize it.

**Areas for Improvement to achieve a higher score (8+):**

1.  **Target Variable Definition:**
    *   **Crucial Error:** The definition of `high_daily_death_increase` in the generated code is incorrect based on the prompt. The prompt states: ""A 'high_daily_death_increase' is defined as a day where 'new_deaths' for a 'location' is in the top 10% of its own historical 'new_deaths' (excluding days with zero new deaths).""
    *   The generated code uses `group['new_deaths'] >= percentile_90`. This means if `new_deaths` is *exactly* the 90th percentile, it's considered `high`. More importantly, it seems to apply the percentile definition to *all* days including those with zero deaths, and also includes days with zero deaths if the 90th percentile happens to be zero (which is unlikely but mathematically possible if all 'new_deaths' > 0 were small numbers, or if `percentile_90` was calculated incorrectly). The prompt explicitly says ""excluding days with zero new deaths"" for the percentile calculation AND for the target definition (i.e., new_deaths must be > 0 and > percentile to be a target).
    *   The ground truth code correctly handles this: `((df['new_deaths'] > df['location_90th_percentile_deaths']) & (df['new_deaths'] > 0)).astype(int)`

2.  **`days_since_first_case` Calculation:**
    *   The generated `engineer_features` function calculates `days_since_first_case` based on `group[group['new_cases'] > 0]['date'].min()`. This is correct in principle, but `first_case_date` is a single value per group. The `days_since_first_case` should be `(group['date'] - first_case_date).dt.days` *for all rows in the group*. The current `if pd.notna(first_case_date):` branch ensures this happens, but the `else` branch `group['days_since_first_case'] = 0` might incorrectly set days to 0 if a location never had `new_cases > 0`. The ground truth calculates `(x - x.min()).dt.days` for each group, which is simpler and more robust, effectively calculating the days since the *earliest recorded date* per location, which is a common proxy for 'days since first case' when actual 'first case' events might not be perfectly marked. The prompt was a bit ambiguous here, but the ground truth's interpretation is safer.

3.  **Missing Value Handling Consistency:**
    *   The generated code handles `std_columns` and `lag_columns` specifically. `days_since_first_case` is filled with 0. Then, it attempts to fill ""other missing values"" by `X[numerical_features] = X[numerical_features].fillna(0)`. This scattered approach could be made more robust and consistent, especially for initial NaN values that might arise at the beginning of groups for rolling/lag features: `ffill` then `fillna(0)` is generally better than just `fillna(0)` or `ffill` alone for time-series features. For the 'days\_since\_first\_case' feature, `fillna(0)` is okay if the first case is `NaN`, but the GT `(x - x.min()).dt.days` handles that more naturally without a specific fill.
    *   The `dropna(subset=['high_daily_death_increase'] + feature_columns)` is overly aggressive. Filling missing values *before* selecting features is usually preferred, as `dropna` can remove many rows. The numerical `fillna(0)` *after* this `dropna` is somewhat redundant and misses applying fills to other potential NaNs before `feature_columns` were subset.

4.  **SHAP Plotting:**
    *   The SHAP plotting part is trying to isolate top 10 features for plotting: `shap_values[:, top_10_indices]` and `X_test[:, top_10_indices]`. While this is correct in principle, `shap.summary_plot` with `plot_type=""bar""` automatically handles `max_display` for you, so just calling `shap.summary_plot(explainer.shap_values(X_test_df_processed), X_test_df_processed, plot_type=""bar"", max_display=10)` (with `X_test_df_processed` being a DataFrame with proper column names) would lead to a more standard and easier-to-read plot. The generated code also didn't explicitly show the plot (`plt.show()`).

Overall, the code is well-structured and follows a logical flow. The main drawback is the critical error in the target variable definition, which would fundamentally change the problem being solved. Addressing that and refining the missing value strategy and days since first case logic would significantly improve the solution.",claude-sonnet-4,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import mixedlm
import warnings
warnings.filterwarnings('ignore')

# 1) Load data and calculate vaccination percentage
data = pd.read_csv('synthetic_covid19_data.csv')
data['date'] = pd.to_datetime(data['date'])
data['vaccination_percentage'] = data['people_fully_vaccinated'] / data['population']

# 2) Handle missing values with interpolation and median imputation grouped by location
def han...",7,"The generated code correctly implements the core requirements of loading data, performing mixed-effects modeling, and reporting results. However, there are several inaccuracies and suboptimal approaches in the data preprocessing and missing value handling sections that prevent a higher score.

**Strong Points:**
*   **Correct Model Specification:** The `mixedlm` formula for fixed and random effects (random intercepts for location nested within continent) is correctly implemented.
*   **Reporting:** Fixed effects coefficients, standard errors, p-values, and random effects variance components are correctly extracted and displayed.
*   **Interpretation:** Provides reasonable interpretations of the fixed effects, including the interaction term, and the random effects variance.
*   **Assumptions Discussion:** The discussion of model assumptions is comprehensive and aligns with typical statistical practice for mixed models.
*   **Code Structure:** The code is well-structured with clear comments and sections.

**Areas for Improvement / Errors:**

1.  **Missing Value Handling (Critical Flaw):**
    *   **Calculation of `vaccination_percentage`:** The generated code calculates `vaccination_percentage = data['people_fully_vaccinated'] / data['population']` *before* handling missing values in `people_fully_vaccinated` or `population`. If `population` or `people_fully_vaccinated` contain `NaN` or `0`, this initial calculation will result in `NaN` or `Inf` for `vaccination_percentage`, potentially propagating errors or creating more `NaN`s than necessary.
    *   **Order of Imputation:** The `handle_missing_values` function *first* interpolates `reproduction_rate`, `stringency_index`, and `vaccination_percentage`, and *then* fills remaining NaNs with the *location-wise median*. This is generally less robust than the ground truth's approach of filling `population` and `people_fully_vaccinated` first, then calculating `vaccination_percentage`, then interpolating, and *finally* using a global median fallback.
    *   **Grouped Imputation Logic:** The generated code's loop `for location in df['location'].unique():` and `df.loc[location_mask, 'reproduction_rate'] = ...` is less efficient and prone to `SettingWithCopyWarning` compared to the ground truth's use of `df.groupby('location')[col].transform(...)`. The `transform` method is specifically designed for group-wise operations that return a Series of the same length as the group, making it much cleaner and more performant.
    *   **Interpolation `limit_direction` and `limit_area`:** The ground truth used `limit_direction='both'` and `limit_area='inside'` in its interpolation, which is crucial for handling NaNs at the start/end of a series and only interpolating within observed data, respectively. The generated code's `interpolate(method='linear')` lacks these specifics, which could lead to missed NaNs if a series starts or ends with NaNs.

2.  **`statsmodels` Imports:** The generated code imports `statsmodels.api as sm` and then `from statsmodels.formula.api import mixedlm`. While not strictly an error, `smf.mixedlm` is usually preferred when using formula APIs, and `sm` isn't used for anything besides the `mixedlm` call, which is specifically imported from `formula.api`. The ground truth directly imports `smf` for formula-based models, which is more aligned with `statsmodels` conventions for `formula.api` usage.

3.  **Redundant `sm` import:** `import statsmodels.api as sm` is included, but `sm.mixedlm` is not used, only `mixedlm` is directly imported. This is minor but unnecessary.

4.  **`warnings.filterwarnings('ignore')`:** While it addresses the prompt's mention of suppressing warnings, it's a very broad suppression. The ground truth uses more targeted warning suppression for specific `statsmodels` messages, which is better practice for debugging and understanding potential issues.

5.  **Output Formatting:** The fixed effects reporting in the generated code attempts to manually format the table, which isn't as robust or clean as simply printing `result.summary().tables[1]` as done in the ground truth. The manual formatting can break if parameter names are very long or if the model includes many terms.

6.  **ICC Calculation:** While the calculation of ICC is a nice addition for interpreting random effects, it wasn't explicitly requested by the prompt.

Overall, the generated code produces the desired model and output, but its preprocessing steps are less robust and potentially problematic compared to the ground truth. The handling of missing values, in particular, needs significant refinement.",claude-sonnet-4,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select features for clustering
features = ['elixirCost', 'Win Rate', 'Usage']
X = df[features]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

#...",9,"The generated code correctly addresses almost all aspects of the task.

**Strong points:**
*   **Correctness and Completeness:** The code correctly loads data, selects and standardizes features, implements both the elbow method and silhouette scores for K selection, applies K-Means, performs PCA, and visualizes the results. This covers all the core requirements.
*   **Feature Selection:** The chosen features (`elixirCost`, `Win Rate`, `Usage`) are correct.
*   **Standardization:** `StandardScaler` is used correctly.
*   **K-Means and PCA:** `KMeans` and `PCA` are implemented and applied correctly.
*   **K Selection:** It provides two methods (Elbow and Silhouette), which is excellent, and then *programmatically* selects the optimal K based on the silhouette score, which is a good automated approach.
*   **Visualization:** The scatter plot correctly shows PCA components colored by cluster labels, and the axis labels properly include explained variance, which is a nice touch.
*   **Imports:** All necessary imports are present.
*   **`n_init=10`:** Correctly uses `n_init=10` to suppress warnings and ensure more robust clustering.

**Areas for minor improvement:**
*   **Error Handling for File Not Found:** The ground truth includes a `try-except FileNotFoundError` block for robust loading of the CSV, which is a good practice for production-ready code. The generated code assumes the file exists.
*   **Explicit K Choice Reporting:** While the code programmatically chooses `optimal_k`, for an expert evaluating the code's process, it could be beneficial to explicitly state which method was used to *choose* the `optimal_k` (e.g., ""Optimal K chosen based on highest silhouette score:"") near the plot, as the task implied using one to *justify* the choice, which often involves some human interpretation or explicit logic. The current print statement is good, but linking it more directly to the plots would be ideal.
*   **Clarity on Optimal K:** The prompt asks to ""determine an appropriate number of clusters (K) ... You can use the elbow method (plotting WCSS) or silhouette scores to justify your choice of K."" The generated code plots both, which is great, and then automatically selects `optimal_k` based on silhouette score. If the intention is for human justification, having a comment explaining the decision process after the plots would be beneficial, even if the code automates it. However, from a pure functionality standpoint, the auto-selection is perfectly fine.

Overall, this is a very strong submission that fulfills the requirements effectively. The automated K selection using silhouette score is a commendable addition.",claude-sonnet-4,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder
import numpy as np

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select features and target
features = ['elixirCost', 'Usage', 'rarity']
target = 'Win Rate'

# Prepare feature matrix and target vector
X = df[features].copy()
y = df[target]

...",9,"The generated code correctly addresses all requirements of the task. It loads the data, selects the specified features and target, correctly preprocesses the `rarity` column using `OneHotEncoder` (which is a valid and robust approach), splits the data, trains a `RandomForestRegressor` model, and evaluates it using R-squared and MSE. The imports are correct and sufficient.

Strong points:
- Correctly implements OneHotEncoder for the `rarity` column.
- Handles the concatenation of encoded features back with numerical features correctly.
- Uses `random_state` for reproducibility in both `train_test_split` and model training.
- Correctly calculates and prints R2 and MSE.

Areas for minor improvement (not critical enough to reduce the score significantly):
- The `try-except` block for file loading in the ground truth is more robust.
- The ground truth also includes checks for missing columns and handles potential `NaN` values, which makes it more robust in a real-world scenario. While the prompt didn't explicitly demand robust error handling for missing files/columns or `NaN`s, it's good practice.
- The `np` import is not used.
- The `print` statements are a bit less descriptive than the ground truth's, but perfectly acceptable given the prompt's simplicity.",claude-sonnet-4,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Identify unique categories in the rarity column
unique_rarities = df['rarity'].unique()
print(""Unique rarity categories:"", unique_rarities)

# Group win rates by rarity
rarity_groups = [df[df['rarity'] == rarity]['Win Rate'] for rarity in unique_rarities]

# Remove any groups with NaN values
rarity_groups = [group.dropna() for group in rarity_groups]

# State hypotheses
print(""...",7,"The generated code correctly performs the ANOVA test and interprets the results based on the significance level. It also properly states the hypotheses.

However, there are a few areas for improvement regarding robustness and explicit data cleaning:

1.  **Handling `Win Rate` Data Type and Missing Values More Explicitly:** The original `Win Rate` column might not be numeric, and `f_oneway` expects numeric inputs. The ground truth explicitly uses `pd.to_numeric` with `errors='coerce'` and `dropna` to handle potential non-numeric values and NaNs, making the code more robust. The generated code relies on `dropna()` after grouping, but this assumes `Win Rate` is already numeric or will be silently handled by `f_oneway`, which is not always safe.
2.  **`FileNotFoundError` Handling:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is important for user experience.
3.  **Edge Case - Insufficient Groups/Data:** The ground truth includes checks for cases where there aren't enough unique `rarity` categories or where groups have insufficient data for ANOVA (e.g., less than 2 observations), which prevents the ANOVA from failing or producing meaningless results. The generated code implicitly handles groups with NaNs by dropping them, but it doesn't explicitly check if groups become empty or too small after this.

Overall, the code is functional for ideal cases but could be more robust for real-world messy data scenarios.",claude-sonnet-4,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
df = pd.read_csv('iris.csv')

# Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task. It successfully loads the dataset, defines features and target, encodes the target, splits the data, trains the requested models, and reports the required metrics. It also correctly identifies the best model based on F1-score.

The significant strengths include:
- All required imports are present and correctly used.
- Data loading and feature/target selection are accurate.
- Label encoding is correctly applied.
- Data splitting uses the specified `test_size` and `random_state`.
- All three specified models are trained with their respective parameters.
- All four required metrics (accuracy, macro precision, macro recall, macro F1-score) are calculated and printed.
- The best model is correctly identified based on the highest F1-score.

One minor area for improvement or a subtle difference from the ground truth is the lack of `stratify=y_encoded` in the `train_test_split` call. While not explicitly requested in the prompt, stratifying the split for classification tasks (especially on imbalanced datasets, though Iris is fairly balanced) is a best practice to ensure that the class distribution in the training and testing sets is similar to the original dataset. The ground truth included this for robustness.

Another minor point is that the `LogisticRegression` model in the ground truth included `max_iter=200` to prevent potential convergence warnings, which is a good practice. The generated code uses default parameters, which might sometimes lead to convergence warnings on certain datasets (though unlikely for Iris).

Overall, these are minor omissions that do not fundamentally break the solution or lead to incorrect results for this specific dataset and task, but are considerations for slightly more robust and professional code. The solution is very good.",claude-sonnet-4,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('iris.csv')

# Select the features
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
X = df[features]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply K-Means clustering
kmeans = KMeans(n_...",8,"The generated code correctly performs all the core tasks: loading data, selecting and standardizing features, KMeans clustering, PCA, and generating two scatter plots. The use of `subplot` to display both plots side-by-side is a nice touch for comparison.

Areas for improvement to achieve a higher score:
1.  **Readability/Completeness - Plot Labels:** The `xlabel` and `ylabel` for the PCA plots are generic (`'First Principal Component'`, `'Second Principal Component'`). The ground truth uses `'Principal Component 1'` and `'Principal Component 2'`, which is more standard.
2.  **Completeness - Colorbar:** The ground truth code includes colorbars for both plots, which significantly aids in understanding the mapping between colors and cluster/species labels. The generated code lacks colorbars. Especially for the true species plot, a colorbar reverse-mapping to the actual species names would be beneficial.
3.  **Robustness:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust. This is missing in the generated code.
4.  **Minor Styling:** The ground truth uses `alpha=0.8` and `plt.grid(True)`, which can enhance plot aesthetics but are not strictly required by the prompt.

Despite these minor points, the code is functional and addresses the prompt's main requirements effectively.",claude-sonnet-4,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# Load the dataset
df = pd.read_csv('iris.csv')

# Create new features
df['PetalArea'] = df['PetalLengthCm'] * df['PetalWidthCm']
df['SepalArea'] = df['SepalLengthCm'] * df['SepalWidthCm']

# Define features X
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'PetalArea', 'SepalArea']]

# ...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task. It properly loads the data, creates the specified new features, defines `X` with both original and new features, encodes the target `y`, initializes the Gradient Boosting Classifier correctly, performs 5-fold cross-validation, and reports the mean accuracy.

The only minor points for improvement, which prevent a perfect score, are:
1.  **Missing `try-except` for file loading:** The ground truth includes a robust `try-except` block for `FileNotFoundError`, which is good practice for real-world scenarios. The generated code assumes the file will always be present, which is less robust.
2.  **Explicit scoring in `cross_val_score`:** While `accuracy` is the default scoring for classification tasks with `cross_val_score`, explicitly setting `scoring='accuracy'` makes the code clearer and more robust against potential future changes in defaults or when evaluating other metrics.
3.  **Missing print of individual fold scores:** The ground truth also prints the individual cross-validation scores for each fold, which can be useful for analysis, although not strictly asked for in the ""report the mean"" requirement.

Overall, the code is very good and fully functional for the given task.",claude-sonnet-4,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.compose import ColumnTransformer

# Load the dataset
df = pd.read_csv('heart.csv')

# Define features and target
feature_columns = ['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol'...",8,"The generated code correctly performs all steps outlined in the task. It loads the dataset, defines features and target, preprocesses categorical and numerical features using `ColumnTransformer` with `OneHotEncoder` and `StandardScaler`, splits the data, trains a `RandomForestClassifier`, and evaluates the model using the required metrics (accuracy, macro-averaged precision, recall, F1-score, and ROC-AUC).

**Strengths:**
*   Correctly identifies and separates numerical and categorical features for preprocessing.
*   Uses `ColumnTransformer` for integrated preprocessing, which is a good practice.
*   Correctly applies `StandardScaler` to numerical features and `OneHotEncoder` to categorical features.
*   Correctly splits data before fitting the preprocessor in a way that avoids data leakage into the test set (although it explicitly calls `fit_transform` on the entire `X` first, which is a potential minor issue, explained below).
*   Correctly instantiates and trains the `RandomForestClassifier` with the specified parameters (`n_estimators=100`, `random_state=42`).
*   Calculates all required evaluation metrics accurately.

**Areas for Improvement (Minor):**
*   **Preprocessing application:** The line `X_preprocessed = preprocessor.fit_transform(X)` applies the fitting of the preprocessor to the *entire* dataset `X` before splitting the data. While the `train_test_split` then creates `X_train` and `X_test` from this preprocessed `X`, it's generally best practice to split the data *before* any transformations so that the preprocessor is only `fit` on the training data. The ground truth code demonstrates the more robust approach by using a `Pipeline` which fits the preprocessor only on `X_train`. While for `StandardScaler` this might not lead to significant data leakage with a large dataset, for `OneHotEncoder` it's generally safer to fit only on the training set to ensure categories seen only in the test set don't influence the encoder mapping.
*   **`OneHotEncoder(drop='first')`:** The generated code uses `drop='first'` for `OneHotEncoder`. While this is often a good practice to avoid multicollinearity in linear models, it wasn't explicitly requested in the prompt. The ground truth uses `handle_unknown='ignore'`, which is more robust for unseen categories but doesn't drop the first column. For a Random Forest, multicollinearity is less of a concern, so both approaches are acceptable for this model, but it deviates slightly from a minimal interpretation of ""one-hot encoding"" which typically implies creating a column for every category. The prompt did not specify `drop='first'`, so `handle_unknown='ignore'` used in the ground truth is a safer, more general choice.
*   **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code omits this.

Overall, the code is very good and achieves the task goals. The improvements are minor considerations for production-level code or stricter adherence to best MLOps practices.",claude-sonnet-4,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('heart.csv')

# Select numerical features for clustering
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']
X = df[features]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# D...",9,"The generated code correctly performs all the required steps: loads data, selects and standardizes features, implements both elbow and silhouette methods to help choose K, applies K-Means with the specified parameters, performs PCA, and visualizes the results.

Strong points:
- All required libraries are imported.
- Correct features are selected and standardized.
- Both elbow method and silhouette scores are calculated and plotted for the specified K range.
- `n_init='auto'` and `random_state=42` are used for K-Means.
- PCA is correctly applied to reduce dimensionality to 2 components.
- The visualization correctly colors points by cluster labels and includes a title.
- The code handles the determination of `optimal_k` programmatically by selecting the K with the highest silhouette score, which is a reasonable approach.

Areas for improvement:
- **Missing file handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code does not include this.
- **Plotting aesthetics/labels:** While sufficient, the PCA plot could benefit from a legend distinguishing the clusters by number, similar to the ground truth, rather than just a color bar. The ground truth also explicitly adds `xticks` to the K-selection plots, which makes them slightly clearer.
- **Explicit `chosen_k` explanation:** The ground truth explicitly states which K was chosen and why. While the generated code programmatically selects `optimal_k`, a print statement indicating the chosen K would be a good addition for clarity, as done in the ground truth.
- **Informative print statements:** The ground truth includes more print statements confirming each major step (e.g., ""Dataset loaded successfully"", ""Features standardized""). While not strictly required by the prompt, these improve user feedback.",claude-sonnet-4,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Create binary target variable
df['quality_binary'] = (df['quality'] >= 7).astype(int)

# Separate features a...",9,"The generated code correctly addresses all requirements of the task. It properly creates the binary target, handles missing values (though using mean instead of median is a minor difference, still acceptable), scales features, splits data, trains a logistic regression model, evaluates performance, and identifies important features.

**Strong points:**
- Correctly implements all required steps.
- Uses `stratify=y` during `train_test_split`, which is good practice for imbalanced datasets common in binary classification.
- Correctly calculates and displays all requested metrics (accuracy, F1-score, classification report).
- Correctly identifies important features using coefficients and sorts them by absolute value.
- Handles potential warnings by `warnings.filterwarnings('ignore')` (though for a benchmark, this might hide useful information, it's a stylistic choice).

**Areas for minor improvement:**
- **Missing Value Imputation:** The ground truth code used `median` for imputation, which is generally more robust to outliers than `mean`. While `mean` is acceptable, `median` is often preferred for numerical features.
- **Reporting:** The ground truth code includes more descriptive print statements, which improves readability and understanding of each step's output. The generated code's print statements are more concise. The ground truth also adds an interpretation of coefficients, which is very helpful for understanding feature importance.
- **File Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust.
- **Model Parameters:** The ground truth explicitly sets `solver='liblinear'` and `max_iter=1000` for the Logistic Regression model, improving reproducibility and ensuring convergence in some cases. The generated code relies on default parameters.

Overall, the generated code is highly functional and correct for the task. The differences are minor enhancements that make the ground truth slightly more robust and user-friendly.",claude-sonnet-4,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.linear_model import LogisticRegression

# 1) Load the dataset
df = pd.read_csv('heart.csv')

# 2) Create box plot
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='HeartDisease', y='Oldpeak')
plt.title('Distribution of Oldpeak by Heart Disease Status')
plt.xlabel('Heart Disease (0=No, 1=Yes)')
plt.ylabel('Oldpeak')
plt.show()

# 3) Logistic regression
X = df[['Oldpeak']]
y = df['...",8,"The generated code correctly addresses most of the requirements. It loads the data, creates the box plot, performs logistic regression and reports the coefficient and odds ratio, and discretizes the `Oldpeak` column, then calculates the proportion of `HeartDisease`=1 in each bin.

Areas for improvement to get a higher score:
1.  **Error Handling (Data Loading):** The ground truth includes `try-except` for `FileNotFoundError`, which is good practice for robust code when dealing with external files. The generated code lacks this.
2.  **Interpretation of Odds Ratio:** The interpretation provided is a bit simplistic. It states that ""the odds of having heart disease increase by a factor of X."" This is true if the coefficient is positive. However, it doesn't account for the scenario where the coefficient (and thus the odds ratio) might be less than 1, implying a decrease in odds. The ground truth provides a more nuanced interpretation covering both positive and negative coefficients.
3.  **Visualization of Proportions:** The task implicitly asks to ""display"" the proportion, and the ground truth explicitly includes a visualization for this step, which is a good way to ""display"" numerical results visually. The generated code only prints the dataframe. While printing is a display, a plot would be more insightful for proportions.
4.  **Robustness of `pd.qcut`:** The ground truth includes a `try-except` block for `pd.qcut` to handle potential `ValueError` if `Oldpeak` has too many identical values, which can prevent unique quantiles. This makes the code more robust. The generated code assumes `qcut` will always work.
5.  **Print Statements/Readability:** The ground truth provides more descriptive print statements to guide the user through the output, which enhances the user experience.

Overall, it's a solid attempt that performs the core tasks accurately due to the prompt asking for ""Provide only the Python code without explanations"" it reduces the amount of print statements. The primary improvements would be around robustness, more thorough interpretation, and a more complete display of results.",claude-sonnet-4,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

# 1) Load data
df = pd.read_csv('spotify_2023.csv')

# 2) Convert streams to numeric and drop NaN rows
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams'])

# 3) Create binary target ...",7,"The generated code largely addresses the requirements but has a few issues that prevent it from being perfect. The most significant issue is related to how the features are handled, specifically after `dropna` and during `OneHotEncoder` and `pd.concat`.

**Correctness:**
*   **Streams conversion & dropna:** Correctly handles `streams` conversion to numeric and drops NaNs.
*   **is_popular:** Correctly creates the binary target based on the 75th percentile.
*   **Feature selection:** Correctly identifies numerical and categorical features.
*   **One-hot encoding:** Uses `OneHotEncoder` as requested, but `drop='first'` was not specified in the prompt for `pd.get_dummies` (which is typically the default behavior of `get_dummies` if not specified with `drop_first=False`, but `OneHotEncoder` with `drop='first'` does something slightly different by dropping one column per feature, which might conflict with expectation if `pd.get_dummies` without `drop_first` was assumed). The `drop='first'` argument should ideally be omitted for perfect adherence to the prompt which didn't specify dropping the first dummy, or `drop_first=False` should be set for `pd.get_dummies` equivalent.
*   **Data Splitting:** Correctly splits data with `test_size=0.2` and `random_state=42`. However, it misses `stratify=y` which is good practice for imbalanced classification tasks (like popularity based on 75th percentile) and is present in the ground truth.
*   **Model Training:** Correctly trains Logistic Regression with `max_iter=1000`.
*   **Evaluation:** Correctly calculates F1-macro, ROC AUC, and displays confusion matrix.

**Code Quality & Completeness:**
*   **Imports:** All necessary imports are present.
*   **Structure:** Code is well-structured and follows the steps outlined in the prompt.
*   **`OneHotEncoder` Index Alignment Issue:** This is the most crucial part. When `df.dropna(subset=numeric_features + categorical_features)` is called, it potentially changes the DataFrame's index. Then, `pd.get_dummies` or `OneHotEncoder` needs to be applied carefully. In the generated code, `encoded_categorical_df` is created using the original `df.index` which is correct, but then `reset_index(drop=True)` is called on both `df[numeric_features]` and `encoded_categorical_df` *before* concatenation. This can lead to misalignment if `df.dropna()` removed rows. The `reset_index(drop=True)` should ideally be applied *after* concatenation or not at all if the indices are already aligned. The ground truth avoids this by applying `pd.get_dummies` directly on `X = df[all_features].copy()` where `X` already has the correct index after `df.dropna(subset=['streams'])`.
    *   Example: If row 5 is dropped by `dropna(subset=...)`, the original `df.index` would still include 5, but the actual data for numerical features would shift. `pd.concat` with `reset_index(drop=True)` on both sides fixes this by re-aligning, but it's a roundabout way and can hide subtle issues if not handled meticulously. The simpler way (`X = pd.get_dummies(X, ...)` where `X` *is* the `df` subset already) is more robust.
*   **Missing `stratify` in `train_test_split`:** This is a minor but important missing detail for robustness, especially for imbalanced datasets.
*   **Error Handling:** Lacks the `try-except` for file loading present in the ground truth, which is good practice.
*   **Clarity of variable names:** Good.
*   **Comments:** Sparse compared to ground truth, but adequate for the task.

The central issue with the `OneHotEncoder` and subsequent `pd.concat` with `reset_index(drop=True)` is that it creates a slight risk of misaligning data if not fully understood, and it's less direct than applying `get_dummies` on a pre-selected `X` dataset. The ground truth method of applying `pd.get_dummies` directly on the `X` DataFrame (which itself was created after necessary `dropna` operations) is generally more robust as it implicitly handles index alignment. The `dropna(subset=numeric_features + categorical_features)` is also a step that might remove additional rows that were already valid after the initial `streams` cleaning; while not explicitly problematic given the structure, it adds another layer of data modification that needs careful index management.",claude-sonnet-4,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load data
df = pd.read_csv('spotify_2023.csv')

# 2) Select audio features and drop rows with NaNs
features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%', 
           'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm']
df_features = df[features].dropna()

# 3) Standardize features
sca...",8,"The generated code correctly addresses most of the requirements. It loads the data, selects the specified features, handles NaNs, standardizes the features, applies K-Means clustering with the correct parameters, performs PCA, and generates the scatter plot. It also calculates and displays the mean values of the original features per cluster.

However, there is one significant subtlety regarding the NaN handling and subsequent operations. When `df_features = df[features].dropna()` is executed, the `df_features` DataFrame has its index reset implicitly if the NaNs are not contiguous, or it loses rows. When `df_features['cluster'] = cluster_labels` is later performed, `cluster_labels` is a numpy array (index 0 to N-1 for the rows that passed the NaN check) and `df_features` still retains its original index from `df`. This can lead to misaligned cluster labels if the original `df_features` had non-contiguous index values after dropping NaNs, causing the cluster labels to be assigned to the *wrong* rows of the original data.

The ground truth correctly handles this by creating `df_original_for_analysis` from the *filtered* `df_features` (i.e., after dropping NaNs), ensuring the indices match for subsequent operations like adding cluster labels and calculating means.

**Specific areas for improvement:**
1.  **Correct NaN Handling & Index Alignment**: The most critical issue is that `df_features` after `.dropna()` might have a broken index (i.e., not a continuous 0 to N-1 range). When `cluster_labels` (which *is* a continuous 0 to N-1 array corresponding to `features_standardized`) is assigned to `df_features['cluster']`, if the index of `df_features` is not 0 to N-1, pandas will align by index. This *will* lead to `NaN` values in the 'cluster' column for rows that were dropped, and wrong assignments for others. The original values for mean calculation should be based on the exact same set of rows that were fed into the clustering algorithm. The ground truth uses `df_original_for_analysis = df_features.copy()` *after* `dropna()` which correctly preserves the exact data used for clustering for the mean calculation. The generated code implicitly assumes the indices will align, which is true for sequential dropping but not for subsequent assignment if indices are non-contiguous.
2.  **Plot labeling**: While `plt.xlabel` and `plt.ylabel` are present, they are generic (""First Principal Component"", ""Second Principal Component""). The ground truth provides slightly more informative labels (""Principal Component 1"", ""Principal Component 2"") which is a minor point.
3.  **Mean Calculation Display**: The mean calculation is correct, but the display format is a bit verbose with nested loops and repeated ""Cluster X - Mean values:"" lines. The ground truth uses `groupby().mean()` which is more concise and idiomatic pandas, presenting the results in a cleaner table format.

Overall, the code is functionally mostly correct and well-structured, but the index alignment issue after NaNs is a subtle but potentially impactful bug for the cluster mean calculation.",claude-sonnet-4,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1) Load data
df = pd.read_csv('spotify_2023.csv')

# 2) Convert streams to numeric (coerce errors to NaN)
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# 3) Filter for songs with streams > 10,000,000 and released_year between 2010-2023
# Drop rows with NaN streams after conversion
df = df.dropna(subset=['streams'])
df_filtered = df[(df['streams'] > 10000000) & 
                 (df['released_year'] >= 2010) & 
    ...",9,"The generated code correctly implements all the requirements of the task. It loads the data, correctly converts `streams` to numeric with error coercion, filters the data based on `streams` and `released_year` criteria, groups by `released_year`, calculates the average of the specified audio features, and generates three individual line plots with appropriate titles and labels.

**Strong points:**
*   Correctly handles the `streams` conversion and NaN dropping.
*   Accurate filtering logic for both `streams` and `released_year`.
*   Proper grouping and aggregation for calculating yearly averages.
*   Effective use of `matplotlib.pyplot.subplots` for creating multiple plots.
*   Plots have correct titles, labels, and show grid.

**Areas for minor improvement (not critical for score):**
*   **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code lacks this.
*   **Clarity in Filtering:** The generated code applies `dropna` to `df` and then filters `df` into `df_filtered`. It might be slightly clearer to apply `dropna` to `df` and then keep working with `df` or rename it, or ensure the filtering is done on the result of the `dropna`. The current approach works correctly, but the variable naming could be a tad more consistent (e.g., `df_filtered = df.dropna(subset=['streams'])` followed by `df_filtered = df_filtered[...]`).
*   **X-axis Ticks:** Ground truth code rotates x-axis labels (`plt.xticks(yearly_avg_features.index, rotation=45)`), which can be helpful if there are many years. The generated code implicitly handles this, but explicit rotation can improve readability.
*   **Plot Styling/Consistency:** While the plots are correct, adding consistent colors or markers explicitly for all plots (as shown in the ground truth) can make them visually cohesive. For example, using markers for all plots, and potentially different color choices (though 'orange' and 'green' are fine). The ground truth also uses `plt.style.use('seaborn-v0_8-darkgrid')` for better aesthetics.

Overall, the code is highly functional and accurate, meeting the core requirements of the prompt effectively. The points for improvement are mostly related to robustness and minor aesthetic choices.",claude-sonnet-4,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean numeric columns
for col in ['Likes', 'Dislikes', 'Video views']:
    df[col] = df[col].astype(str).str.replace(',', '')
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

# Ext...",8,"The generated code largely fulfills the requirements of the task.

**Strong Points:**
- **Correct Data Loading and Cleaning:** Successfully loads the CSV, removes commas, converts to numeric, and fills NaNs with 0 for 'Likes', 'Dislikes', and 'Video views'.
- **Correct Feature Engineering:** Correctly extracts the year from 'published' and treats it as a numeric feature. Handles missing 'Category' values by filling with 'Unknown' and then correctly applies one-hot encoding.
- **Correct Feature and Target Definition:** `X` and `y` are correctly defined using the specified columns.
- **Correct Model Training and Evaluation:** Uses `train_test_split` with the correct `test_size` and `random_state`, trains a `LinearRegression` model, and calculates both R2 and MAE as required.
- **Proper Imports:** All necessary libraries are imported.

**Areas for Improvement (Why not a 9 or 10):**
- **Robustness in Numeric Conversion:** While `fillna(0)` is applied, the prompt explicitly asked for ""fill resulting NaNs with 0"". The current code chain `pd.to_numeric(...).fillna(0)` achieves this, but it's good practice to ensure the final type is an integer if possible, like `astype(int)` used in the ground truth. This is a minor point as `float` handling works.
- **`OneHotEncoder` `drop='first'`:** The ground truth used `pd.get_dummies` which by default doesn't drop the first category. While `drop='first'` is often good practice to avoid multicollinearity, the prompt didn't specify it, and the ground truth's approach is simpler and directly meets the ""one-hot encode"" requirement without further assumptions. This is a stylistic/best-practice choice rather than a direct error in meeting the prompt.
- **Type Conversion for Cleaned Numerics:** The ground truth explicitly converts 'Likes', 'Dislikes', and 'Video views' to `int` after cleaning. The generated code leaves them as `float` due to `pd.to_numeric` and `fillna`. While functionally this might not break `LinearRegression`, it's a detail that could be more precise to the data's expected type (counts/integers).
- **Hardcoding column names:** The ground truth includes checks like `if col in df.columns:` and `if 'published' in df.columns:` which makes the code more robust if the dataset schema changes slightly. The generated code assumes all columns will be present, which is typical for a fixed dataset, but less robust.",claude-sonnet-4,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the Video views column
df['Video views'] = df['Video views'].astype(str).str.replace(',', '')
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce')
df['Video views'] = df['Video views'].fillna(0)

# Ensure published column is numeric
df['published'] = pd.to_numeric(df['published'], errors='coerce')

# Group by published year and calculate metrics
ye...",7,"The generated code successfully addresses most of the requirements. It correctly loads the dataset, cleans the `Video views` column, performs the grouping, and generates the two required plots with appropriate titles and labels.

Areas for improvement:
1.  **Handling of 'published' column**: The prompt specified the `published` column (year) should be treated as numeric. The assumption is that file contains `published` column for year itself. However, common dataset schema usually contains `publishedAt` column for full date e.g. `youtube_trendings.csv` in question contains `publishedAt`. Ground truth robustly extracts year from `publishedAt`. Generated code doesn't accommodate for this.
2.  **Robustness**: The generated code directly tries to load the CSV and doesn't include error handling for `FileNotFoundError` or other potential issues during data loading, which the ground truth does. This makes the generated code less robust.
3.  **Missing `numpy` import**: Although `numpy` is not explicitly used by name in the operations, `pandas` often relies on it internally for numeric operations. It's generally good practice to import `numpy` when working with `pandas` for numerical tasks. However, its absence doesn't cause an error in this specific case.
4.  **`Video views` vs `Video` column for `nunique`**: The `Video` column name is used for counting unique videos, which aligns with the prompt's request for ""unique `Video` titles"".
5.  **Plot formatting**: While the plots are correct, the ground truth includes minor enhancements like `grid(True)`, `marker='o'`, `linestyle='-'`, and explicit `xticks` for integer years, making the plots slightly more readable and professional. The ground truth also uses `ticklabel_format(style='plain', axis='y')` to prevent scientific notation on the y-axis for views, which could be an issue with very large numbers.",claude-sonnet-4,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords', quiet=True)

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Create title_word_count feature
df['title_word_count'] = df['Video'].str.split().str.len()

# Clean Video views column
df['Video views'] = df['Video views'].astype(str).str.replace(',', '').replace('', '0')
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce')....",8,"The generated code largely addresses all requirements of the task.

**Strong points:**
- Correctly loads the dataset.
- Accurately calculates `title_word_count`.
- Cleans `Video views` by removing commas and converting to numeric, handling NaNs correctly by filling with 0.
- Correctly calculates and prints the Pearson correlation coefficient.
- Filters the dataset for 2023 and 2024 published videos.
- Processes video titles by converting to lowercase, removing punctuation, and filtering stop words.
- Correctly finds and prints the 5 most common words.
- Includes all necessary imports and handles NLTK stopwords download silently.

**Areas for improvement:**
- **Robustness in `Video views` cleaning:** The line `df['Video views'] = df['Video views'].astype(str).str.replace(',', '').replace('', '0')` might not behave as expected if there are empty strings *after* replacing commas but *before* converting to numeric. `pd.to_numeric` with `errors='coerce'` followed by `fillna(0)` is generally more robust for handling various non-numeric string values, including empty strings, by turning them into NaN first. The ground truth's approach is more robust by first coercing to numeric and then filling NaNs.
- **Handling of potential `FileNotFoundError`:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code assumes the file will always be present.
- **`re.findall` for punctuation removal:** The regex `r'\b[a-zA-Z]+\b'` for extracting words effectively removes punctuation and numbers, which is good. However, it might be slightly less flexible than the ground truth's `re.sub(r'[^\w\s]', '', title)` if one intends to capture words that might contain internal apostrophes (e.g., ""don't"" would become ""don t""). For simple word counting and stop word removal, `r'\b[a-zA-Z]+\b'` is acceptable. The ground truth then specifically adds `.isalpha()` to further filter out any numeric strings that might have passed through, which is a good extra step.
- **Output formatting:** The output for the correlation and common words is correct but less descriptive than the ground truth (e.g., ""Pearson correlation between 'title_word_count' and 'Video views': 0.XXXX"" vs. ""Pearson correlation coefficient: 0.XXXX""). Also, the ground truth explicitly mentions ""occurrences"" for common words, which is clearer.
- **`title_word_count` creation:** While `df['Video'].str.split().str.len()` works, it implicitly creates a list of words for each title. `apply(lambda x: len(x.split()))` is functionally similar and often preferred for clarity, especially when intermediate transformation isn't needed. However, both are correct. The ground truth also explicitly casts to `astype(str)` which adds robustness against potential non-string entries in the 'Video' column. This is a minor but good practice.",claude-sonnet-4,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score
import re

# 1) Load data
df = pd.read_csv('mobiles_2025.csv')

# 2) Clean Launched Price (USA) and create binary target
df['Launched Price (USA)'] = df['Launched Price (USA)'].astype(str).str.replace('USD ', '', regex=False)
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace(...",8,"The generated code correctly implements almost all aspects of the task.

**Strong points:**
*   Successfully loads the data.
*   Correctly cleans `Launched Price (USA)` and creates `is_expensive_usa` binary target.
*   Handles errors in `pd.to_numeric` for price cleaning effectively.
*   Cleans `Mobile Weight`, `RAM`, `Battery Capacity`, and `Screen Size` as specified.
*   The `extract_mp` function correctly extracts the first numerical MP value for camera features and defaults to 0 if none is found.
*   Correctly one-hot encodes `Company Name` and `Processor`.
*   Defines features `X` and target `y` using the cleaned numerical and one-hot encoded features.
*   Handles NaNs by dropping rows with NaNs in `X` or `y` before splitting.
*   Splits data into training and testing sets with the correct `test_size` and `random_state`.
*   Trains a `RandomForestClassifier` with specified `n_estimators` and `random_state`.
*   Evaluates the model using F1-score (macro average) and ROC AUC score, printing the results.

**Areas for Improvement (to reach a higher score):**
*   **Target Creation Logic:** The line `df['is_expensive_usa'] = (df['Launched Price (USA)'] > 1000).astype(int)` directly converts `True`/`False` to `1`/`0`. This works, but it causes rows where `Launched Price (USA)` was `NaN` after cleaning to become `0` (False > 1000 is False, so `NaN` is implicitly converted to `0` or `False` then `0`). The ground truth specifically drops rows where `is_expensive_usa` is NaN (before `astype(int)`), which implies that unparseable prices should also lead to dropped rows, not `0`. The current code then implicitly drops them in the subsequent `dropna(subset=['is_expensive_usa'])` line, but the sequence matters if the intent was to explicitly handle the NaN `is_expensive_usa` as a separate drop condition for unparseable prices. The ground truth sets `NaN` explicitly for `is_expensive_usa` if the source price was `NaN` and then drops based on that.
*   **NaN Handling for Cleaned Features:** The generated code directly overwrites the original columns (`df['Mobile Weight']`, etc.) with cleaned numeric versions. While this is fine, the `dropna()` call for `X` and `y` at the end handles any `NaN`s introduced. The separate dropping strategy in the ground truth for `is_expensive_usa` is a bit more robust for that specific step but otherwise, the `dropna` at the end covers it.
*   **Data Copying:** The ground truth code starts by creating `df_cleaned = df.copy()` to avoid `SettingWithCopyWarning` and ensure original `df` is untouched. While `SettingWithCopyWarning` might not occur for all operations done here, using a copy is generally a good practice when performing multiple chained operations or modifications.
*   **Robustness:** The code assumes the CSV file exists. Adding a `try-except FileNotFoundError` block for `pd.read_csv` would make it more robust, as in the ground truth.
*   **Clarity on Column Overwrite:** Using new column names for cleaned features (e.g., `df_cleaned['RAM_cleaned']`) as done in the ground truth can sometimes improve clarity, especially in a complex pipeline, but overwriting in place is also a common practice and acceptable.

Overall, the code is very well-written, addresses the core requirements, and produces the expected output. The minor points are about slightly more robust or clearer practices.",claude-sonnet-4,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
import re

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Define function to clean price columns
def clean_price(price_str):
    try:
        if pd.isna(price_str):
            return np.nan
        # Remove currency symbols and commas
        cleaned = re.sub(r'[PKR|INR|CNY|USD|AED|,]', '', str(price_str))
        # Remove any remaining non-numeric characters except decimal points
        cleaned = re.sub(r'[^\d.]', '', cleaned)
     ...",8,"The generated code largely fulfills all requirements of the task.

**Strong Points:**
*   **Correct Loading:** Correctly loads the CSV file.
*   **Effective Price Cleaning:** The `clean_price` function is robust, handling `NaN` inputs and using `re.sub` to remove currency symbols and commas, converting prices to numeric. It correctly handles conversion errors by returning `NaN`.
*   **Correct Average Calculation:** Uses `df[price_columns].mean(axis=1, skipna=True)` which is the correct and efficient way to calculate the average price, considering only available data.
*   **Efficient Top N Selection:** Utilizes `df.nlargest(5, 'Average Global Price')` which is an efficient way to get the top 5 models by `Average Global Price`.
*   **Clear Display:** The output correctly displays the `Model Name` and `Average Global Price` for the top 5 models.

**Areas for Improvement:**
*   **Error Handling for File Loading:** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.
*   **Redundant Sorting in Display:** The final line `print(top_5.sort_values('Average Global Price', ascending=False))` is redundant because `nlargest` already ensures the result is sorted in descending order by the specified column. It doesn't harm correctness but is unnecessary.
*   **RegEx pattern for currency symbols:** While `re.sub(r'[PKR|INR|CNY|USD|AED|,]', '', str(price_str))` works, the regex pattern `[PKR|INR|CNY|USD|AED|,]` means ""match any single character that is 'P', 'K', 'R', '|', 'I', 'N', 'C', 'Y', 'U', 'S', 'D', 'A', 'E', or ','"". This is not typically how you remove multiple specific substrings. A more explicit (though possibly less concise) way would be to replace ""PKR"", then ""INR"", etc. The ground truth's iterative `replace()` approach is more explicit for removing specific multi-character strings. However, in this specific case, `re.sub` seems to have worked by sheer luck because none of the letters forming the symbols appear in numeric values. A more robust regex would be `r'PKR|INR|CNY|USD|AED|,'` without the `[]` around the whole thing, or using `|` directly within the `[]` as `[PKR|INR|CNY|USD|AED,]` would mean any single char that is P, K, R etc. The `re.sub(r'[^\d.]', '', cleaned)` line correctly removes non-numeric characters *after* the symbol removal, which implicitly fixes potential issues from the first `re.sub` if a currency symbol wasn't fully removed as a multi-character string.

Overall, the code is highly functional and correctly solves the problem. The suggested improvements are minor and related to robustness and slight optimizations rather than fundamental correctness issues.",claude-sonnet-4,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean RAM column
df['RAM_cleaned'] = df['RAM'].astype(str).str.replace('GB', '').str.strip()
df['RAM_cleaned'] = pd.to_numeric(df['RAM_cleaned'], errors='coerce')

# Clean Battery Capacity column
df['Battery_Capacity_cleaned'] = df['Battery Capacity'].astype(str).str.replace('mAh', '').str.strip()
df['Battery_Capacity_cleaned'] = pd.to_numeric(df['Battery_C...",7,"The generated code correctly addresses most of the task requirements. It loads the dataset, performs the necessary cleaning operations on RAM, Battery Capacity, and Launched Price, handles NaN values by dropping rows, filters for Apple products, and groups by launched year. It then generates box plots for each year with subplots for the three metrics.

**Strengths:**
- Correctly loads the CSV.
- Implements the cleaning steps for RAM, Battery Capacity, and Launched Price, including removing specified strings and converting to numeric with `errors='coerce'`.
- Correctly drops rows where the cleaned columns are NaN.
- Properly filters for 'Apple' products.
- Groups data by 'Launched Year' and iterates through each year to create plots.
- Uses subplots for each year, as requested.
- Plots have appropriate titles and y-axis labels.

**Areas for Improvement (Why not higher than 7):**
- **Plot quality/readability**: The `plt.boxplot` function is used directly instead of `seaborn.boxplot`. While not explicitly stated to use seaborn, the ground truth uses it. `seaborn.boxplot` generally produces more aesthetically pleasing plots and often handles data framing (like specifying `y` directly) more intuitively. The generated plots often have `xticklabels` set to generic ['RAM'], ['Battery'], ['Price'] which isn't necessary for a single box plot per subplot; removing `set_xticklabels` would make them cleaner.
- **Robustness**: The loading of the dataset does not include any error handling (`try-except FileNotFoundError`) which is good practice.
- **Naming Conventions**: While `_cleaned` suffixes are clear, using temporary columns for cleaned data when the original can be overwritten is less memory-efficient, though for this task, it's not a major issue. The ground truth directly overwrites the columns, which is a common practice.
- **Minor details**: `plt.tight_layout()` is called per year inside the loop, the ground truth defines a slightly more controlled `rect` for `tight_layout` with `fig.suptitle` to prevent overlapping. `import numpy as np` is included but not used.",claude-sonnet-4,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold, validation_curve, learning_curve
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassif...",6,"The generated code provides a decent attempt at solving the complex task, covering many of the requested components. However, it falls short in several areas, particularly regarding the structure and comprehensiveness of the preprocessing pipeline, and the integration of feature selection within model training.

**Strengths:**
*   **Comprehensive Imports:** All necessary libraries are imported.
*   **Feature Engineering:** Good attempt at advanced features like `Title`, `FamilySize`, `IsAlone`, `FarePerPerson`, `AgeGroup`, `HasCabin`, `CabinDeck`, `NameLength`, `TicketPrefix`, `TicketNumber`.
*   **Multiple Imputation:** Addresses the requirement for multiple imputation strategies (SimpleImputer, KNNImputer).
*   **Polynomial Features:** Correctly applies polynomial features.
*   **Multiple Base Models with Hyperparameter Tuning:** Implements RandomizedSearchCV for RandomForest, Gradient Boosting, XGBoost, SVM, and MLPClassifier.
*   **Ensemble Methods:** Includes both VotingClassifier and StackingClassifier.
*   **Comprehensive Evaluation:** Performs cross-validation, generates learning curves, and calculates standard classification metrics on the test set.
*   **Statistical Significance Tests:** Uses `ttest_rel` for statistical comparisons.
*   **Model Interpretability:** Attempts SHAP and LIME analysis.
*   **Feature Selection Comparison:** Compares impacts of SelectKBest, RFE, and SelectFromModel.

**Areas for Improvement / Weaknesses:**

1.  **Preprocessing Pipeline (Major Flaw):**
    *   The `advanced_feature_engineering` function is applied **before** splitting the data into train/test sets, leading to data leakage. Feature engineering (especially anything that learns from the data like `LabelEncoder` or `fillna` based on the whole dataset) *must* be done after the split or encapsulated within a scikit-learn pipeline.
    *   The `multiple_imputation` function is a standalone function applied to `X` (the full dataset) before the split, again causing data leakage.
    *   Polynomial features and scaling are also applied globally before the split/within standalone operations, rather than as part of a robust ColumnTransformer/Pipeline.
    *   The `ColumnTransformer` is imported but never explicitly used for the main data processing flow (`X_train_scaled`). Instead, separate scaling and imputer calls are made. This violates the ""proper structure"" aspect of code quality.
    *   `LabelEncoder` is used within `advanced_feature_engineering`. While it encodes categorical features, for new/unknown categories in test data, it will fail or treat them differently. `OneHotEncoder` (often preferred for tree-based models) is more robust for unseen categories and avoids implicit ordinality. The ground truth uses `OneHotEncoder` and `ColumnTransformer`.

2.  **Feature Selection Integration:**
    *   The feature selection methods are implemented as standalone functions that return transformed data (`X_train_kbest`, etc.). However, these transformed datasets are then largely *unused* in the subsequent model training and evaluation phase (tuned models and ensembles are trained on `X_train_scaled` which uses *all* features).
    *   The task explicitly asks to ""compare their impact"", which implies training *and evaluating* models (or pipelines) with and without feature selection, or with different feature selection techniques. The current implementation compares feature selection outputs by running a single `temp_model` at the very end, which is insufficient for a comprehensive comparison against the best models/ensembles built previously, which were trained on the full set of features. A proper comparison would integrate feature selection *into* the pipelines for specific models.

3.  **Reproducibility and Robustness:**
    *   Handling `PassengerId` and `Ticket` columns: The `advanced_feature_engineering` function handles `Ticket` features by extracting prefixes and numbers, but then drops `Name`, `Cabin`, `Ticket` (and potentially `PassengerId`) implicitly by not including them in `feature_columns`. The ground truth explicitly drops `Name`, `Cabin`, `Ticket` AFTER processing them, and drops `PassengerId` from the feature set. The generated code's approach to `TicketNumber` (`astype(float)`) and subsequent imputation/scaling for it might introduce issues if numbers are not purely numeric.
    *   The `Title` mapping is hardcoded. While functional, it might be brittle if input titles vary. The ground truth groups dynamically and handles rare/unseen titles better.

4.  **Model Interpretability (SHAP/LIME):**
    *   For SHAP, `explainer = shap.Explainer(best_model, X_train_scaled)` and `shap_values = explainer(X_test_scaled[:100])` is used. This `Explainer` (without specifying `TreeExplainer` or `KernelExplainer`) might struggle with non-tree models (like SVM or MLP) if they happened to be `best_model`. Also, `feature_names=X.columns` is passed to `summary_plot`, but `X` was the *raw* feature DataFrame *before* imputation and polynomial expansion. SHAP values are calculated on `X_test_scaled`, which has a different number of columns due to polynomial features. This mismatch means the SHAP plots will have incorrect feature labels. The ground truth correctly maps SHAP values to the names of the *processed* features (`processed_feature_names`).
    *   LIME uses `X.columns` as `feature_names` again, which is problematic for the same reason (mismatch with `X_train_scaled`). LIME also expects the `predict_proba` function to be able to transform new data. The generated code passes `best_model.predict_proba`, which works if `best_model` accepts `X_test_scaled` directly. However, if `best_model` itself is a pipeline (like the stacking/voting models defined in the ground truth), and LIME generates perturbed samples in the *original* feature space, the model won't know how to process them. The ground truth provides a wrapper `predict_proba_for_lime` that handles this correctly, ensuring the model's preprocessing pipeline is applied to LIME-generated samples.

5.  **Validation Curve:** The task explicitly asks for validation curves but the generated code does not include this in its output.

6.  **Code Structure and Readability:** The code combines many steps into large functions or global scripts, which can make it harder to follow and debug compared to a pipeline-driven approach as seen in the ground truth.

To score higher, the generated code would need to:
*   Implement `ColumnTransformer` and `Pipeline` for all preprocessing steps including feature engineering, imputation, scaling, and polynomial features to prevent data leakage and ensure a robust workflow.
*   Integrate feature selection logically into the model pipelines, allowing for a fair comparison of model performance *with* different feature selection strategies.
*   Ensure consistency in feature naming, especially for interpretability tools like SHAP and LIME, linking them back to the processed feature names.
*   Include validation curves.",claude-sonnet-4,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Define features and target
X = df[['Year', 'Runtime']]
y = df['Rating']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = LinearRegression()
model.fit(X_train, y_tr...",7,"The generated code correctly addresses all the requirements of the task: loading data, defining features and target, splitting data, training a Linear Regression model, and evaluating it with R-squared and MAE. The imports are correct, and the general structure is logical and follows the specified steps.

However, the code lacks robustness that is present in the ground truth. Specifically, it does not include any error handling for `FileNotFoundError` when loading the CSV, nor does it perform any data preprocessing for the 'Runtime' column (e.g., stripping ' min' and converting to numeric) or handling potential non-numeric values or NaNs in 'Year' and 'Rating'. Without this preprocessing, the model might fail or produce incorrect results if the 'imdb_top_250.csv' dataset contains runtime strings or non-numeric/missing values in 'Year' or 'Rating', which is a common scenario in real-world data. The ground truth specifically handles these common issues, leading to a more robust and production-ready solution.",claude-sonnet-4,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Select features for clustering
features = df[['Rating', 'Runtime']]

# Standardize the features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
cluster_labels = kmeans.fit_predi...",6,"The generated code correctly implements most of the requirements. It loads the data, selects features, standardizes them, applies K-Means with the specified parameters, and generates a scatter plot. It also attempts to calculate and print mean ratings and runtimes per cluster.

However, there are two significant issues:
1.  **Missing Data Cleaning/Preprocessing for 'Runtime':** The `imdb_top_250.csv` dataset typically has the 'Runtime' column as a string (e.g., ""142 min""). The generated code directly selects this feature without converting it to a numeric type, which will cause a `TypeError` or `ValueError` during standardization or mean calculation, or yield incorrect results if `pd.to_numeric` is implicitly called in a way that handles errors poorly. The ground truth explicitly handles this by removing "" min"" and converting to numeric.
2.  **Robustness for Missing Columns/Files:** While the task description implies the file and columns exist, a more robust solution (like the ground truth) would include `try-except` for `FileNotFoundError` and checks for column existence.

Improvements needed:
*   Add robust data cleaning for the 'Runtime' column to convert it to a numeric type before standardization and plotting.
*   Consider adding error handling for file loading and missing columns to make the code more robust, although this was not explicitly requested.
*   The printing of cluster means could be slightly cleaner using `groupby()` as in the ground truth, but the current loop approach is functionally correct.
*   The scatter plot axis labels could be more descriptive as in the ground truth (""Runtime (minutes)"").",claude-sonnet-4,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter

# Download required NLTK data
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create title_length feature
df['title_length'] = df['Title'].str.len()

# Calculate and print Pearson correlation coefficient
correlation = df['title_length'].corr(df['Ratin...",7,"The generated code correctly addresses most of the task requirements.

**Strong Points:**
*   Correctly loads the dataset.
*   Correctly creates `title_length`.
*   Attempts to calculate the correlation coefficient (though with a minor flaw).
*   Correctly downloads NLTK resources.
*   The preprocessing steps (lowercase, remove punctuation, tokenize, remove stopwords) are generally followed.
*   Correctly identifies and prints the 10 most common words and their frequencies.
*   Uses appropriate libraries and data structures (`pandas`, `nltk`, `collections.Counter`).

**Areas for Improvement (leading to a score of 7, not higher):**
*   **Correlation Calculation Issue:** The generated code directly calculates `df['title_length'].corr(df['Rating'])`. The ground truth code explicitly converts `Rating` to numeric and handles potential `NaN` values (`pd.to_numeric(df['Rating'], errors='coerce')` and `dropna`). While `Rating` in this specific dataset might be clean, robust code for data science tasks should always anticipate and handle non-numeric data or missing values when performing numerical operations like correlation, especially as it's a direct requirement from the prompt ""Ensure 'Rating' column is numeric"". Not doing so makes the correlation calculation potentially unreliable if the `Rating` column contains non-numeric entries.
*   **Punctuation Removal Robustness**: The generated code uses `''.join(char for char in title if char.isalnum() or char.isspace())` for punctuation removal. While this works for many cases, the `re` module (used in the ground truth) with regular expressions offers a more standard, powerful, and often more robust way to handle character filtering, especially for complex patterns or international characters. The prompt asked for `string` and `re` was implicitly used for robust punctuation removal in the ground truth. The generated code strictly uses `string` for its `isalnum` and `isspace` methods, which is compliant, but the regex approach is generally preferred for this kind of task.
*   **DataFrame Operations vs. Loop**: The processing of titles and filtering of stopwords is done via an explicit loop (`for title in df['Title']:`) and `all_words.extend(filtered_tokens)`. While functional, a more ""pandas-idiomatic"" way, as shown in the ground truth, would be to operate on DataFrame columns using `apply` for each step (`df['processed_title'] = df['Title'].str.lower()`, `df['processed_title'].apply(...)`, `df['tokenized_title'] = df['processed_title'].apply(word_tokenize)`), making the code cleaner and potentially more performant for very large datasets due to vectorized operations where possible. This also aligns slightly better with the prompt's implied manipulation of the `Title` column and a new `processed_title` column.
*   **Output Format for Correlation:** The output format for correlation is slightly different (`f""Pearson correlation coefficient between title_length and Rating: {correlation}""` vs. `f""Pearson correlation between title_length and Rating: {correlation:.4f}""`). The ground truth specifies `:.4f` for formatting. This is a minor detail but good to note.
*   **Error Handling for NLTK Downloads**: The prompt explicitly mentioned `try-except LookupError` for NLTK downloads in the ground truth, which the generated code omits, opting for `quiet=True` instead. While `quiet=True` suppresses output, it doesn't handle the case where the resource might *not* be found, which the `try-except` structure does. The ground truth method is more robust for dependency management.",claude-sonnet-4,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer

# Load the dataset
df = pd.read_csv('penguins.csv')

# Define features and target
feature_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
X = df[feature_cols].copy()
y = df['species'].copy()...",9,"The generated code is largely correct and addresses most of the requirements.

**Strong Points:**
- Correctly loads the data.
- Correctly defines features and target.
- Correctly identifies numerical and categorical features.
- Uses `SimpleImputer` with appropriate strategies (`median` for numerical, `most_frequent` for categorical).
- Correctly performs one-hot encoding using `pd.get_dummies`.
- Correctly scales numerical features using `StandardScaler`.
- Correctly label encodes the target variable.
- Initializes `RandomForestClassifier` with specified parameters.
- Correctly performs 5-fold cross-validation and prints the mean accuracy.

**Areas for Improvement (and why it's not a perfect 10):**
1.  **Imputation of `island` for all-NA case:** The task specifically states: ""if a row still has NA in `island` after mode imputation, e.g. if all were NA, it can be dropped... For this task, if mode imputation results in NA for island, drop those rows.""
    The generated code uses `SimpleImputer(strategy='most_frequent')` for both 'sex' and 'island'. While `SimpleImputer` handles cases where a column is entirely NA by filling with `np.nan` if the strategy results in an empty mode list, the `if X['island'].isna().any():` block then tries to drop rows where `island` is NA.
    However, the crucial part is that `SimpleImputer` might not behave *exactly* as intended for the ""if mode imputation results in NA for island, drop those rows"" scenario. If the entire column *was* NA, `SimpleImputer` would fill with `np.nan`, and the subsequent `isna().any()` check and dropping mechanism should work. But the problem states ""if mode imputation results in NA for island"", which implies a scenario where `mode()` itself returns empty (i.e., all NaNs). The ground truth specifically checks `if X['island'].isnull().all():` to handle this explicitly before imputation, which is safer. The generated code's approach relies on the `SimpleImputer` filling with `np.nan` (which it does for `most_frequent` if the column is all NaN) and then dropping it. This is slightly less robust than the explicit check for all-NA columns in the ground truth.

2.  **Handling missing values in target variable:** The ground truth code explicitly handles missing values in the target `y` by dropping corresponding rows from both `X` and `y`. While `penguins.csv` typically doesn't have missing `species`, a robust solution for a general classification task should account for this. The generated code does not include this check. This is a minor omission given the specific dataset, but good practice.

These points are minor, especially the target NA handling, as the `penguins.csv` dataset is usually clean for the target. The `island` imputation logic is sound but less explicit than the ground truth in handling the edge case of an all-NA column resulting in `np.nan` from `SimpleImputer`.",claude-sonnet-4,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Load the dataset
df = pd.read_csv('penguins.csv')

# Select the features
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
selected_data = df[features + ['species']].copy()

# Drop rows with missing values in the selected features
selected_data = selected_d...",8,"The generated code correctly implements all specified steps: loading data, selecting and cleaning features, standardizing, applying K-Means and PCA, and generating two scatter plots. The use of `subplot` to display both plots side-by-side demonstrates good understanding of visualization.

Areas for improvement:
1.  **Plotting:** While functional, the plotting for original species labels is less robust than using `seaborn`'s `hue` parameter, which automatically handles legends and color mapping. The manual color mapping for species requires hardcoding and might break if species names differ or new species are added.
2.  **Readability/Consistency for Plots:** Using `seaborn` for plotting generally leads to more aesthetically pleasing and consistent plots in data science contexts. The ground truth uses `seaborn.scatterplot` for both plots, which is a common and good practice.
3.  **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.
4.  **`n_init='auto'`:** While correctly set, the generated code doesn't explicitly mention `n_init='auto'` in the comment as a requirement, but it is present in the code.

Overall, the generated code is highly functional and addresses the core requirements of the task effectively. The identified areas for improvement are mostly related to best practices and minor robustness/aesthetic considerations rather than critical failures.",claude-sonnet-4,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Impute missing values
# Drop rows where island is NA
df = df.dropna(subset=['island'])

# Impute numerical columns with mean
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in num...",8,"The generated code correctly addresses most of the requirements.

**Strong Points:**
*   **Data Loading and Imputation:** Correctly loads the data, drops rows for NA `island`, imputes numerical columns with their mean, and `sex` with its mode.
*   **Feature Engineering:** Properly identifies numerical features and one-hot encodes `island` and `sex`. Correctly concatenates features to form `X` and label encodes `species` for `y`.
*   **Model Training and Feature Importance:** Successfully trains a `GradientBoostingClassifier` and extracts/prints feature importances, associating them with their names.
*   **Visualization Setup:** Correctly uses `matplotlib.pyplot` and `seaborn` for plotting.

**Areas for Improvement (to achieve a higher score):**

1.  **Plotting Structure for Box Plots:** The requirement was to ""For each `species`, create box plots to visualize the distribution of `flipper_length_mm` (using imputed data) grouped by the imputed `sex` column.""
    *   The generated code creates separate subplots for each species, plotting `flipper_length_mm` against `sex`. This is technically ""for each species,"" but a more common and often preferred visualization for comparing distributions across multiple categorical and continuous variables is a single box plot where `species` is on the x-axis, `flipper_length_mm` on the y-axis, and `sex` is used as a `hue` variable (as shown in the ground truth). This provides a more direct comparison of `flipper_length_mm` across all species and sexes within a single plot. While the generated code's approach is not strictly incorrect, it deviates from the typical interpretation of ""grouped by"" in such visualization tasks and results in multiple, less comparative plots.
    *   The titles on the subplots are just `Flipper Length Distribution for {species}`, not explicitly mentioning grouping by sex, although sex is on the x-axis. A more descriptive title would be better.
2.  **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice for robustness. The generated code lacks this.
3.  **`pd.get_dummies` `dtype` Parameter:** While `dtype=int` is not strictly necessary for functionality, it's good practice to ensure dummy variables are integers, which the ground truth does. The generated code omits this.",claude-sonnet-4,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer
import numpy as np

# Load the dataset
df = pd.read_csv('penguins.csv')

# Define numerical and categorical columns
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
categorical_col = 'sex'

# KNN imputation for numerical columns
knn_imputer = KNNImputer(n_neighbors=3)
df[numerical_cols] = knn_imputer.fit_transform(df[numerical_cols])

# Mode imputation for categorical column 'sex'
if c...",9,"The generated code correctly performs all the required steps: loading the dataset, applying KNN imputation to the specified numerical columns, and imputing the `sex` column with its mode. It also includes the verification step of printing the remaining NA values.

Strong points:
- Correctly imports necessary libraries.
- Clearly defines numerical and categorical columns.
- Applies KNNImputer with the correct `n_neighbors` value.
- Correctly uses `fillna()` with the mode for the 'sex' column, including a check for an empty mode result (though the `if sex_mode is not None` condition might be redundant if the `mode().empty` check prevents `sex_mode` from being `None` in the first place, it doesn't hurt).
- Prints the final NA counts as requested.

Areas for minor improvement:
- **Error Handling:** The ground truth code includes basic error handling for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.
- **Verification Clarity:** While it prints `df.isnull().sum()`, the ground truth adds more specific verification by printing NA counts for the directly imputed columns, which can be useful for debugging or detailed verification.
- **Comments/Print Statements:** The ground truth provides more verbose print statements and comments, explaining each step of the process. While not strictly required for functionality, it enhances readability and understanding, especially in a benchmark scenario where the task is to ensure compliance with specific steps.

Overall, it's a very good solution that successfully addresses the core requirements of the task. The missing error handling and slightly less verbose verification are minor points.",claude-sonnet-4,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Filter dataset to include only 'Adelie' and 'Gentoo' species
filtered_df = df[df['species'].isin(['Adelie', 'Gentoo'])]

# 3) Handle NA values in culmen_length_mm column by dropping rows with NAs
filtered_df = filtered_df.dropna(subset=['culmen_length_mm'])

# Separate the culmen_length_mm data for each species
adelie_culmen = filtered_df[filtered_df['species'] == 'Adelie...",9,"The generated code correctly addresses all requirements of the task. It loads the data, filters for the specified species, handles NA values in the correct column, performs the t-test, and interprets the results based on the p-value and significance level.

Strong points:
- All steps are correctly implemented.
- The use of `dropna(subset=[...])` is correct for handling `NA` values in a specific column.
- The t-test is performed correctly using `ttest_ind`.
- The interpretation of the p-value is accurate and clearly stated.
- The code is concise and readable.

Areas for minor improvement (not critical enough to significantly deduct points given the problem statement simply asked for ""Python code"" and not necessarily a function or robust error handling):
- **Error Handling:** The ground truth includes `try-except` blocks for file loading, which makes the code more robust against missing files. The generated code assumes the `penguins.csv` file will always be present. While the task stated ""Assume the CSV is in the current directory,"" robust code typically includes such handling.
- **Modularity:** Encapsulating the logic within a function (as in the ground truth) would make the code more reusable and testable.
- **Readability/Clarity:** Adding comments for each step, similar to the ground truth, could slightly improve readability, although the current code is quite clear.
- **Copy vs. View Warning:** While not explicitly an issue in this specific case due to immediate `dropna`, pandas sometimes throws `SettingWithCopyWarning` if modifications are made to a filtered DataFrame that is a ""view"" and not a ""copy"". Using `.copy()` after filtering (as in the ground truth) can prevent this common pitfall, though it's less critical here.
- **Assumptions in `ttest_ind`**: The `ttest_ind` function defaults to `equal_var=True` (Student's t-test). The ground truth implicitly mentions this assumption, which adds a layer of statistical understanding. The generated code uses the default, which is typically fine, but explicitly setting it or acknowledging it can be good practice.

Overall, the generated code fulfills the core requirements excellently. The minor improvements are more about best practices for production-ready or more robust code rather than direct task failures.",claude-sonnet-4,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Define features and target
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
target = 'species'

# 3) Preprocessing
#...",8,"The generated code successfully completes the task by loading the dataset, performing the specified preprocessing steps (median imputation for numerical features, mode imputation for 'sex', one-hot encoding for 'island' and 'sex', and label encoding for 'species'), splitting the data, training a Random Forest classifier, and evaluating its performance with accuracy and a classification report.

**Strong Points:**
- All requirements are met.
- Imputations are done correctly for numerical and categorical features.
- One-hot encoding is applied correctly.
- Target variable `species` is label encoded.
- Data splitting is correct, including stratification and `random_state`.
- Random Forest Classifier is trained with specified parameters.
- Evaluation metrics (accuracy and classification report) are printed.

**Areas for Improvement:**
- **Robustness in Preprocessing:** While the current approach of directly modifying `df` and then creating `df_encoded` works, using `ColumnTransformer` and `Pipeline` (as demonstrated in the ground truth) is generally more robust, clean, and scalable for complex preprocessing workflows. It prevents data leakage issues if preprocessing steps were derived from the full dataset before splitting.
- **Handling of `species` in `pd.get_dummies`:** The line `df_encoded = pd.get_dummies(df[features + [target]], columns=['island', 'sex'])` includes the target column `species` in the `get_dummies` call. While `get_dummies` correctly ignores `species` as it's not specified in `columns`, it's less explicit and potentially confusing to include it in the dataframe being one-hot encoded if it's not intended for encoding. A cleaner approach would be to only include the features (`df[features]`) for `get_dummies` and handle the target `y` separately.
- **Classification Report Readability:** The classification report is printed with numerical labels (0, 1, 2) rather than the original species names. The ground truth code correctly uses the `label_encoder.classes_` to provide readable class names in the report, which is a small but impactful detail for interpretability.",claude-sonnet-4,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

corr_performance_experience = df[""PerformanceScore""].corr...",9,"The generated code correctly defines the DataFrame, calculates the specified Pearson correlation coefficients, and prints them. It also correctly identifies the feature with the stronger absolute correlation with ""PerformanceScore"". The code is concise and directly addresses all parts of the task.

Minor improvements could include:
1.  Specifying `method='pearson'` for clarity, although `corr()` defaults to Pearson.
2.  Formatting the printed correlation coefficients to a few decimal places for better readability, similar to the ground truth.
3.  Handling the edge case where the absolute correlations are exactly equal (though this is unlikely with real-world data and not explicitly required by the prompt, it makes the code more robust).
4.  The print statement for the stronger feature could be slightly more descriptive, though it's functionally correct.

Overall, it's a very good solution that successfully completes all requirements.",claude-sonnet-4,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import chi2_contingency, fisher_exact
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test, multivariate_logrank_test
from lifelines.utils import concordance_index
import pymc3 as pm
import theano.tensor as...",6,"The generated code attempts to address all aspects of the prompt and demonstrates a good understanding of various statistical and machine learning techniques. However, there are some fundamental misunderstandings and implementation issues, particularly concerning the definition of time-to-event for survival analysis and the lack of comprehensive problem modeling as prescribed by the instructions.

**Major Issues and Areas for Improvement:**

1.  **Kaplan-Meier and Cox Proportional Hazards - Misunderstanding of ""Time-to-Event"":**
    *   **Crucial Flaw**: The prompt explicitly stated: ""Implement Kaplan-Meier survival analysis treating the voyage as time-to-event data (with survival as event indicator)"". The ground truth code correctly interprets this as setting a fixed time `T=1` for all observations, and `E` (event) as death. This transforms the survival analysis problem into one that evaluates factors associated with the *event of death during a fixed period* (the voyage), which effectively becomes a binary outcome regression if not for the CoxPH framework.
    *   **Generated Code's Error**: The generated code defines `SurvivalTime` as `df_encoded['PassengerId'] / df_encoded['PassengerId'].max()`. This is an arbitrary and meaningless proxy for time-to-event in the context of the Titanic disaster. Passenger ID has no bearing on the duration of a voyage. This fundamental error renders the K-M and CoxPH analysis based on this `SurvivalTime` variable largely meaningless for the task specified.
    *   **Impact**: Because of this, the core survival analysis (K-M, CoxPH) is incorrectly implemented in terms of its `T` variable. The K-M curves and CoxPH coefficients are based on an invalid time scale, making the results uninterpretable in the context of the problem.

2.  **Missing ""Advanced Statistical Tests"" (from prompt):**
    *   The prompt asked for ""multivariate analysis, interaction effects testing, and model diagnostics."" While the generated code attempts interaction effects and some diagnostics (C-index, PH assumption check), ""multivariate analysis"" usually implies techniques beyond simple Cox models, or a more rigorous approach to testing sets of variables.
    *   It also did not implement ""multivariate logrank test"" which is an import.

3.  **Bayesian Analysis (PyMC3):**
    *   The generated code uses `pm.Bernoulli('observed', p=p, observed=bayesian_data['Survived'].values)` which is standard for binary regression. This is acceptable, but the prompt specifically hinted at ""probabilistic modeling of survival factors"" which could have allowed for a more direct Bayesian CoxPH model if time-to-event was correctly defined, or a Bayesian survival model if `SurvivalTime` had real meaning. Given the incorrect `SurvivalTime`, a simple binary regression is defensible as a ""probabilistic model of survival factors"" but it's not truly a Bayesian *survival* model in the way *lifelines* does survival.

4.  **Propensity Score Matching:**
    *   The implementation is reasonable for basic PSM (matching one treatment to one control based on scores).
    *   However, it only calculates the ATE (Average Treatment Effect) on `Survived` (binary outcome) using simple mean difference and bootstrap. It doesn't analyze the effect on survival *time* (which is problematic anyway due to the incorrect `SurvivalTime`).
    *   The Ground Truth used Pclass 1 vs Pclass 3 for clearer contrast, which was a good choice compared to just Pclass 1 vs all others.
    *   It lacks covariate balance checking *after* matching, which is crucial for demonstrating successful matching. (The Ground Truth explicitly included this). A basic check like SMD would be expected.

5.  **Model Validation and Sensitivity Analysis:**
    *   The generated code includes some diagnostics (residuals for Cox, posterior predictive check for Bayesian, VIF).
    *   For sensitivity analysis, it attempts ""remove influential observations"" which is a valid technique.
    *   However, it misses the kind of sensitivity analysis hinted at in the prompt and shown in the ground truth: e.g., sensitivity to imputation strategy, or prior choices in Bayesian.

6.  **Code Quality & Structure:**
    *   Imports are mostly appropriate.
    *   Variable naming is generally clear.
    *   The use of `plt.subplot` is good for organizing plots.
    *   The data preprocessing and feature engineering are decent.

**Comparison with Ground Truth (Key Differences that lower score):**

*   **Time-to-Event Definition**: This is the most significant difference, leading to a fundamentally different (and incorrect in context) interpretation of survival analysis. The ground truth correctly interprets the ""voyage as time-to-event"" as a fixed time `T=1`.
*   **Propensity Score Matching Robustness**: Ground truth includes covariate balance check (SMD) post-matching, which is critical.
*   **Sensitivity Analysis Depth**: Ground truth demonstrates imputation strategy sensitivity and explicitly discusses prior sensitivity. Generated code's sensitivity analysis is less comprehensive in this regard.
*   **Problem Framing**: The ground truth carefully models the problem's interpretation of time and event which is key for survival analysis. The generated code deviates significantly here.

Overall, the generated code shows an effort to incorporate various advanced techniques but falls short on the precise understanding of the survival analysis problem as framed by the prompt, particularly the definition of ""time-to-event"". The reliance on `PassengerId` for `SurvivalTime` is a critical flaw.",claude-sonnet-4,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# Create the DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'], 
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'], 
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77], 
        'Final_Score': [...",8,"The generated code correctly addresses almost all requirements of the task.

**Strengths:**
*   **DataFrame Creation:** The DataFrame is created accurately as specified.
*   **Correlation Calculation:** The Pearson correlation coefficient is correctly calculated and printed.
*   **Grade Assignment:** The 'Overall_Grade' column is correctly added based on the specified conditions using `apply` and a custom function.
*   **Grade Distribution:** The count of students in each grade category is correctly calculated and printed.
*   **Plotting Basics:** The scatter plot for 'Attendance_Rate' vs 'Final_Score' is generated with correct axes and title. The use of `plt.figure(figsize=...)` and `plt.grid` is good practice.

**Areas for Improvement:**
*   **Scatter Plot by Subject:** While the generated code attempts to color points by subject using a loop and `plt.scatter`, it manually assigns a limited set of `colors = ['blue', 'red', 'green']`. The task implied coloring *based on* subject, and the ground truth demonstrated using `sns.scatterplot` with `hue='Subject'`, which automatically handles distinct colors for each unique subject value, and also automatically generates a legend. With 3 subjects defined in the data ('Math', 'Science', 'English'), the provided colors are sufficient; however, this approach is less robust if there were more than 3 subjects or if the subjects were different.
*   **Plot Legend:** The generated code places the legend *inside* the plot (`plt.legend()`), which can sometimes overlap with data points. The ground truth code used `bbox_to_anchor` to place the legend outside for better readability, which is a minor aesthetic improvement.
*   **Unused Import:** `seaborn` is imported but not used in the generated code for plotting. The task description mentioned it for ""potentially better aesthetics but not required for coloring."" While not used, `sns.scatterplot` would have been a more direct and robust way to achieve the color-by-subject requirement.
*   **Readability/Structure (Minor):** The ground truth included print statements for each step, which aids in understanding the execution flow in an interactive environment. This is not a strict requirement for correctness but improves the overall user experience of the code in a benchmark context.

The code is functional and solves the core problem, but the plotting section could be made more robust and idiomatic using `seaborn` as implied by the prompt.",claude-sonnet-4,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']), 
        'Ticker': ['TECHCO']*10, 
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0], 
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0], 
        'Low': [149.5, 150....",10,"The generated code accurately completes all aspects of the task. It successfully creates the DataFrame, calculates daily returns and both simple moving averages (3-day and 5-day). The plot is correctly generated with the specified title, legend, and labeled axes. Finally, it correctly identifies and prints the dates where the 3-day SMA crosses above the 5-day SMA, indicating bullish signals. The code is well-structured and uses appropriate pandas and matplotlib functionalities. The addition of `df['SMA_3_prev'] = df['SMA_3Day'].shift(1)` and `df['SMA_5_prev'] = df['SMA_5Day'].shift(1)` for identifying crossovers is a clear and effective way to implement the logic, making it easy to understand.",claude-sonnet-4,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107], 
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 
        'Quantity': [1, 2, 1, 1, 3, 1, 1], 
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150], 
        'CustomerID': ['CUST001', 'CUST...",9,"The generated code is highly correct and complete, addressing all aspects of the task.

**Strong points:**
- **Correctness:** All calculations (total sales revenue by product, average order value, repeat customers) are performed accurately.
- **Completeness:** Every part of the task prompt is addressed, including DataFrame creation, revenue calculation, plotting, average order value, and identifying repeat customers.
- **Code Quality:** The code is well-structured and uses appropriate pandas and matplotlib functions. Imports are correct.
- **Plotting:** The horizontal bar chart is correctly generated with the specified labels and title.

**Minor areas for potential improvement (mostly stylistic, not critical for correctness):**
- **Plot aesthetics:** While `plt.barh` is used correctly, the ground truth uses `product_revenue.plot(kind='barh')` which is a more idiomatic pandas plotting method that can sometimes lead to slightly cleaner code, especially when chaining. Labeling the x-axis for the plot as ""Total Revenue (USD)"" and the y-axis as ""Product"" would be slightly more descriptive, though ""Revenue"" and ""Product"" are acceptable.
- **Output formatting:** The average order value print statement is clear, but the ground truth adds more context (total revenue and unique orders). The repeat customer output is also good, but the ground truth's table-like formatting is a nice touch. These are minor presentation details.
- **No extra prints:** The generated code is very concise, which is generally good, but the ground truth included `print(df)` and other intermediate print statements which can be helpful for debugging or demonstrating intermediate steps in a benchmark scenario. This is a preference rather than a strict requirement.

Overall, an excellent solution that fulfills all requirements correctly and efficiently.",claude-sonnet-4,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd
import numpy as np

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Create DeviceType_Numeric feature
df['DeviceType_Numeric'] = df['DeviceType'].map({'Desktop': 0, 'Mobile':...",9,"The generated code is very good and correctly implements all the requirements of the task.

**Strong points:**
- **Correct DataFrame definition:** Accurately creates the initial DataFrame.
- **Correct `DeviceType_Numeric` creation:** Uses the `map` function with the specified mapping, which is efficient and clear.
- **Correct `PagesPerMinute` creation with division by zero handling:**
    - Performs the division correctly.
    - Handles `np.inf` and `-np.inf` by replacing them with 0.
    - Handles `NaNs` (including those that result from `0/0` or if `inf` was replaced by `NaN` first, though in this case `replace` to `0` directly handles infinities, and then `fillna(0)` handles any `NaN` that might have resulted from `0/0`). This is a concise and effective way to handle the edge cases.
- **Correct feature selection:** Selects all the required features into a new DataFrame.
- **Correct output:** Prints the `head()` of the final feature DataFrame as requested.
- **Code quality:** The code is clean, well-structured, and uses appropriate pandas and numpy functions.

**Minor area for improvement (why not a 10):**
The only very minor difference from the ground truth, which doesn't affect correctness but is good practice, is the order of replacing `inf` and `nan` for `PagesPerMinute`.
Ground Truth: `replace([np.inf, -np.inf], np.nan, inplace=True)` then `fillna(0, inplace=True)`. This ensures all problematic division outcomes (including `0/0` which results in `NaN`, and `x/0` which results in `inf`) are treated as `NaN` and then `NaN`s are filled by `0`.
Generated Code: `replace([np.inf, -np.inf], 0).fillna(0)`. This handles `inf` and `-inf` by directly replacing them with 0, and then handles `NaN` (which would only occur from `0/0` in this scenario) by filling them with 0. Both are functionally equivalent for the given data and requirements. The ground truth's approach might be slightly more robust if there were pre-existing NaNs or if the intermediate `NaN` state was desired for some reason, but for this specific problem, the generated code's approach is perfectly fine and slightly more concise.

Overall, an excellent solution.",claude-sonnet-4,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26, 45, 28]
}
df = pd.DataFrame(data)
...",9,"The generated code correctly defines the DataFrame, calculates the mean and standard deviation for temperature, and identifies outliers based on the 1.5xSTD threshold. It successfully prints the specified details for each outlier. The code is concise and directly addresses all requirements.

One minor improvement could be to explicitly convert the ""Timestamp"" column to datetime objects using `pd.to_datetime()`. While not strictly necessary for this specific outlier detection logic, it is good practice for time-series data and can prevent potential issues or enable more advanced time-based operations in other contexts. Also, adding a check for `if not outliers.empty:` before the loop to print outliers would make the code slightly more robust, although for this specific dataset and threshold, outliers are guaranteed. The output format for the printed outliers is correct but could be slightly more descriptive (e.g., indicating it's an ""outlier"").",claude-sonnet-4,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [10, 5, 25, 15, 8, 30, 9,...",10,"The generated code correctly defines the DataFrame, calculates the EngagementScore, and accurately identifies the PostType with the highest average EngagementScore for each platform. The method used to find the best `PostType` per `Platform` (`idxmax`) is efficient and correct. The output format is also consistent with the typical expected output for such a task, providing the required information clearly. The code is concise and well-structured, fulfilling all aspects of the prompt.",claude-sonnet-4,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu"", ""Injury"", ...",9,"The generated code is highly correct and fully addresses all the requirements of the task. It correctly defines the DataFrame, calculates ""LengthOfStay"", computes average stay and patient count per diagnosis, prints these details, and correctly identifies the diagnosis with the longest average stay.

**Strong points:**
- Correct implementation of DataFrame creation and `LengthOfStay` calculation.
- Efficient use of `groupby()` and `agg()` for calculating statistics.
- Correct renaming of columns after aggregation.
- Accurate identification of the diagnosis with the longest average stay.
- Clear and concise printing of the results.

**Minor areas for improvement (not affecting score significantly):**
- The print format for the diagnosis statistics could be made slightly more aligned or tabular for better readability, similar to the ground truth's use of f-string alignment.
- The generated code is not encapsulated within a function, which is a common practice for reusable code. However, the prompt only asked for ""Python code"" and did not explicitly require a function. The ground truth includes a function, which is good practice.",claude-sonnet-4,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",10,"The generated code correctly defines the DataFrame, accurately calculates the ""WeightedRating"" for each movie, adds it as a new column, and identifies and prints the ""Title"" and ""WeightedRating"" of the movie with the highest ""WeightedRating"".

- **Correctness:** The code is perfectly correct and produces the expected output. The formula for ""WeightedRating"" is implemented correctly, and the identification of the movie with the highest weighted rating is accurate.
- **Code Quality:** The code is clean, concise, and easy to understand. It uses pandas' vectorized operations effectively for calculating ""WeightedRating."" The printing of the final result is well-formatted.
- **Completeness:** All requirements of the task are fully addressed. The DataFrame is created, the new column is calculated and added, and the specific movie's details are printed as requested.

The generated solution is efficient and directly addresses all parts of the prompt without unnecessary complexity. It's a strong example of how to solve this type of data manipulation task using pandas.",claude-sonnet-4,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 10000, 8000]
}
df = pd....",10,"The generated code is correct, complete, and follows good coding practices. It successfully creates the DataFrame, calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"", and prints the resulting grouped DataFrame, fulfilling all aspects of the task. The use of double square brackets `[[""PriceUSD"", ""Mileage""]]` before `.mean()` is a concise and effective way to select multiple columns for aggregation, resulting in a clean and readable solution. While the ground truth code used named aggregations in `.agg()` for clearer output column names (`Average_PriceUSD`, `Average_Mileage`), the generated code's output using `PriceUSD` and `Mileage` as column names is also perfectly acceptable and directly reflects the average of those columns. It effectively achieves the desired outcome with concise and efficient pandas operations.",claude-sonnet-4,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63, 83],
    ""De...",9,"The generated code correctly performs all the required tasks: it defines the DataFrame, calculates the ""TotalStats"" column, identifies and prints the name of the Pokemon with the highest ""TotalStats"", and calculates and prints the average ""Speed"" for each ""Type1"". The code is concise and uses appropriate pandas functionalities.

The only minor area for improvement that prevents a perfect 10 is the lack of print statements to clearly label the output for each step, as seen in the ground truth. While the code produces the correct results, adding descriptive print statements would enhance readability and user understanding, especially in a benchmark scenario where clarity of output is important. However, functionally, it is entirely correct.",claude-sonnet-4,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, calinski_harabasz_score, adjusted_rand_score
from skl...",6,"The generated code attempts to address all requirements of the task, but several significant aspects are implemented incorrectly or not fully in line with standard practices for robust data science analysis.

**Areas for Improvement:**

1.  **Feature Engineering Scope and Quality:**
    *   While some feature engineering from the prompt is attempted (family size, title, ticket sharing), the implementation is contained within a class (`TitanicFeatureEngineer`) which is an unusual choice for a standalone script, and some choices are less robust. For example, `FamilySurvivalRate` mapping without careful handling of missing surnames or proper validation is fragile.
    *   The prompt specifically asked for ""socioeconomic indicators"" and ""text-based features from names and tickets."" The generated code's implementation of 'TicketPrefix' and 'CabinDeck' is basic (`str.extract`, `fillna('Unknown')`) and doesn't fully explore potential nuances or grouping as robustly as the ground truth.
    *   There's no explicit handling for 'FarePerPerson' with zero family size or division by zero, which could lead to issues.
    *   The `TitanicFeatureEngineer` class modifies `self.data` in place and returns `self`, which is a pattern that can lead to unexpected side effects if not handled with extreme care, especially if the original `df` is used elsewhere without copying.

2.  **Preprocessing and Scaling:**
    *   The generated code correctly uses `StandardScaler`, but it performs manual `fillna` for 'Age' and 'Fare' and then directly applies scaling to the selected `clustering_features`. The prompt mentioned using `ColumnTransformer` and `Pipeline` for proper preprocessing, which is a more robust and professional approach for handling different data types and transformations. The generated code does not use `ColumnTransformer` or `Pipeline` for preprocessing.
    *   `LabelEncoder` is used for several categorical features (`Sex`, `TicketPrefix`, `CabinDeck`, `Embarked`). While it works, `OneHotEncoder` is generally preferred for nominal categorical features when feeding into most clustering algorithms to avoid implying ordinality, unless the clustering algorithm inherently handles ordinality (which K-Means or GMM generally don't well). The ground truth correctly uses `OneHotEncoder`.

3.  **Clustering and Validation Metrics:**
    *   **Gap Statistic:** The implementation of `gap_statistic` is flawed. The reference datasets `ref_data` are generated using `np.random.uniform(X.min(axis=0), X.max(axis=0), X.shape)`. This is a simplification and doesn't correctly model the null hypothesis for the Gap Statistic, which typically involves permuting the data within columns or sampling from a uniform distribution over the hyperrectangle spanning the data, but more robustly, it involves PCA followed by random projection or using a more sophisticated null distribution. The simple uniform sampling between min/max can severely underestimate variability.
    *   **Optimal k selection:** The code attempts to pick an `optimal_k` using `stats.mode` from the results of different metrics. This is a reasonable heuristic, but the `min(optimal_k_gap, 8)` hardcoding for gap statistic is arbitrary and implies a lack of confidence in the gap statistic's result or an implicit domain knowledge not justified in the code.
    *   **DBSCAN parameters:** The DBSCAN parameters `eps=0.5, min_samples=5` are hardcoded without any justification or adaptive selection, which is a common pitfall. DBSCAN is highly sensitive to these parameters, and `0.5` is often too small for normalized data in higher dimensions. The ground truth code correctly points out the need for careful tuning.
    *   **Missing Metrics:** The prompt explicitly asks for ""gap statistic"" and ""calinski-harabasz index"", which are covered, but does not include Davies-Bouldin Index mentioned in the ground truth.

4.  **Cluster Stability Analysis and Consensus Clustering:**
    *   **Stability:** The `bootstrap_clustering` function for stability analysis is problematic. It calculates `silhouette_score(X, labels_original)` where `labels_original` are *predicted* on the original `X` after fitting on bootstrap samples. This is not a standard way to measure clustering stability. Stability usually involves comparing cluster assignments (e.g., using ARI) between runs, or comparing cluster structures. The prompt requested stability analysis and consensus clustering, and while attempts are made, the implementation is not robust. For instance, comparing `labels_original` against 'true' labels for stability is impossible without true labels. It should compare labels *between* bootstrap runs or compare how well the bootstrap clusters assign to the original cluster labels.
    *   **Consensus Clustering:** The ""simple consensus: majority vote"" is a basic approach and might not always be the most effective for consensus clustering, especially if algorithms have very different underlying assumptions. Robust consensus clustering techniques (like co-association matrices) are more involved but provide better results.

5.  **Visualizations:**
    *   The scatter plots for reduced dimensions incorrectly apply `dbscan_labels` directly within a loop without checking if DBSCAN found any meaningful clusters (i.e., not just noise).
    *   The `feature_importance` for clustering using `RandomForestClassifier` is an interesting idea but not directly a feature of clustering itself. It provides insight into which features best separate the *K-Means* clusters, which is a form of cluster interpretation, but not a direct clustering metric or property.

Overall, the code shows an understanding of the components required for the task but often implements them in a simplified, less robust, or occasionally incorrect manner compared to best practices or the specific details implied by ""expert"" level analysis.",claude-sonnet-4,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dystopian""],
    ""Stock"": [...",8,"The generated code correctly defines the DataFrame and performs the required aggregations.

**Strengths:**
- **Correct DataFrame Creation:** The DataFrame is created accurately.
- **Accurate Aggregations:** It correctly calculates the count of books and average price per author using `groupby()` and `agg()`.
- **Correct Stock Calculation:** It correctly identifies the author with the most books in stock by summing `Stock` and using `idxmax()`.
- **Conciseness:** The code is quite concise, especially the `agg` part for multiple statistics.

**Areas for Improvement (to reach a higher score):**
- **Output Format for Author Statistics:** The printed output for author statistics (Task 1 & 2) is a pandas DataFrame, which is functional but not explicitly formatted as requested in the task description (""showing Author, Count of Books, and Average Price"") as clearly as the ground truth's f-string loop. The ground truth uses `reset_index()` and then iterates to print in a more human-readable, sentence-like format. While `print(author_stats)` is valid, it doesn't match the *exact* presentation style implied by ""showing Author, Count of Books, and Average Price"" where Author is usually a direct field alongside the counts/averages without being solely an index.
- **Completeness of Output for Task 3:** While it identifies the author with the most books in stock, it doesn't print the *total stock value* for that author, which the ground truth does. This is a minor omission but makes the output less informative.
- **Clarity of Comments:** The generated code lacks comments, unlike the ground truth which uses them to explain each step, making the code more readable and maintainable. This is particularly important in data science contexts where steps can be complex.
- **Renaming Columns:** The generated code renames columns *after* aggregation (`author_stats.columns = ['Count of Books', 'Average Price']`). While it works, performing this within the `agg` function itself (as shown in the ground truth using `count_of_books=('BookID', 'count')`) can be more explicit and lead to fewer lines of code.",claude-sonnet-4,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import ttest_rel
import warnings
warnings.filterwarnings('ignore')

# Sklearn imports
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.model_selection import learning_curve, validation_curve
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, RobustScaler
from sklearn.impu...",6,"The generated code attempts to address all requirements of the task. However, it suffers from several issues hindering its correctness, robustness, and full functionality as specified.

**Strong Points:**
*   **Modularization:** Uses classes (`AdvancedPreprocessor`, `ModelOptimizer`, `EnsembleModels`, `AdvancedEvaluator`) which is good for organizing code.
*   **Advanced Preprocessing:** Includes multiple imputation strategies (mean, median, KNN), outlier detection (IQR, Z-score) and treatment (cap, remove), and clearly attempts advanced feature engineering (polynomial, interaction features).
*   **Model Optimization:** Correctly uses `BayesSearchCV` for optimization, which is an advanced choice over `GridSearchCV`.
*   **Ensemble Methods:** Implements `VotingClassifier` and `StackingClassifier`.
*   **Evaluation:** Includes `cross_val_score`, learning curves, and bootstrap sampling.
*   **Interpretability:** Attempts `SHAP` analysis.
*   **Calibration:** Includes probability calibration plots.
*   **Dashboard:** Attempts a Plotly-based dashboard for comparison.

**Areas for Improvement/Correction:**

1.  **Data Preprocessing and Pipelines (Critical):**
    *   **Overfitting Preprocessor:** The `AdvancedPreprocessor` is fit on `X` (the whole dataset) in `main()` *before* the `train_test_split`. This is a major data leakage issue. The preprocessor (especially scaling, imputation (if stateful), and feature selection) should be fit ONLY on the training data and then transform both training and testing data.
    *   **Feature Selection:** `SelectKBest` is applied within `fit_transform`. This needs to be part of a `Pipeline` or handled carefully to avoid data leakage during cross-validation or testing. It's usually a step after scaling and imputation.
    *   **Polynomial Features:** The interaction and polynomial feature creation is complex and might lead to an explosion of features before scaling or selection, potentially causing memory/performance issues or even numerical instability. The handling of feature names after polynomial features is not robust.
    *   **Imputation Strategy Averaging:** Averaging `imputed_dfs` for `multiple_imputation` is not a standard MICE (Multiple Imputation by Chained Equations) approach where you train models on each imputed dataset and then average the *results*. This custom averaging of imputed values is unusual and not proper multiple imputation. The ground truth's `IterativeImputer` is a better fit for MICE.
    *   **`fit_transform` return signature:** The `AdvancedPreprocessor.fit_transform` returns `X_processed, y` when `y` is not `None`, which is problematic as typically `fit_transform` only transforms `X`. Moreover, dropping rows for outlier removal makes `X` and `y` inconsistent if not handled carefully and returned together.

2.  **Model Optimization and Evaluation:**
    *   **`cross_val_score` usage:** `cross_val_score` is used for CV results, but for paired t-tests, you need the scores *per fold* for *each model* to perform the relative comparison from the same folds. The current `cross_val_score` results do not inherently provide this model-aligned fold-wise performance needed for `ttest_rel`. The ground truth explicitly handles this by re-running CV for the best model to collect fold scores.
    *   **Statistical Significance:** The `statistical_significance_testing` function attempts paired t-tests but relies on the `scores` from `cross_val_score`. If these scores are not aligned (i.e., not from the same folds for each model), the paired t-test is invalid.
    *   **Bootstrap Evaluation:** The bootstrap evaluation samples from `X` and `y` as `pd.Series` but then uses `.iloc` for `X_bootstrap` etc. It's not clear where `X` was converted to a DataFrame in the main loop or if it retains column names for the models. The logic seems to apply to the *training set* for bootstrap, not the final *test set* as typically done for robust final evaluation. The ground truth correctly bootstraps from the test set for confidence intervals there. Also, fitting model clones using `type(model)(**model.get_params())` is not robust for complex models or pipelines and can lose fitted estimators within pipelines. `sklearn.base.clone` is much safer.

3.  **SHAP Analysis:**
    *   **`explainer = shap.Explainer(model, X[:100])`**: This is problematic. `shap.Explainer` (the default) might not be ideal for all model types (e.g., tree models benefit from `shap.TreeExplainer`, linear models from `shap.LinearExplainer`). Also, passing only the first 100 samples might not provide a representative view of feature importance across the entire dataset.

4.  **Dashboard/Plotting:**
    *   **Subplot Limit:** The `plot_learning_curves` and `plot_calibration_curve` have a hardcoded `idx >= 6` break, meaning only the first 6 models will be plotted, which could exclude ensemble models or others that might be important.
    *   **`create_comprehensive_dashboard` - Plotly:** While using Plotly is advanced, the execution might be complex without a clear web environment, and the layout definition could be tricky for dynamic number of models. The bar plots don't show the confidence intervals.
    *   **Confidence Interval Plotting:** The bar plots in the dashboard don't explicitly show the bootstrapped confidence intervals visually, which was a specific request.

5.  **Code Quality & Robustness:**
    *   **Imports:** Many imports are present, some (like `RobustScaler`, `SelectKBest`, `BayesSearchCV`, `plotly`) are good additions. However, the presence of `itertools` without clear usage suggests unused imports.
    *   **Error Handling:** Lacks basic error handling (e.g., for file not found, like the ground truth).
    *   **Reproducibility:** `np.random.seed(42)` is missing for consistent results, especially important with functions like `np.random.choice` in bootstrap or `train_test_split`.
    *   **`get_feature_names_out()`:** The polynomial feature processing within the `AdvancedPreprocessor` might lead to issues with feature names if combined with Feature Selection. The way `feature_names` is set could be inconsistent after `SelectKBest` if not properly handled.
    *   **`RobustScaler` vs `StandardScaler`**: While `RobustScaler` is a valid choice, it's not the same as `StandardScaler` used in the ground truth. The problem explicitly asked for StandardScaler in some requirements, but the prompt's focus was ""advanced"" which RobustScaler could fit.

Overall, the generated code has strong intentions to meet the requirements but falls short on critical implementation details (especially around data leakage, proper statistical testing setup, and robust pipeline integration) that make it less reliable and accurate than the ground truth. It's a good starting point, but would require significant debugging and refactoring to be production-ready.",claude-sonnet-4,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Statistical and ML libraries
from scipy import stats
from scipy.stats import jarque_bera, shapiro, anderson, kstest
from statsmodels.stats.multitest import multipletests
from statsmodels.stats.anova import anova_lm
from statsmodels.formula.api import ols
from statsmodels.tsa.stattools import jarque_bera as jb_test
import statsmodels.api as sm...",7,"The generated code makes a strong effort to address all requirements of the complex task. It correctly implements multiple imputation techniques, various statistical models (including GAM and Bayesian), advanced hypothesis testing, and anomaly detection. It also attempts comprehensive diagnostics and visualizations.

However, there are several areas for improvement:

1.  **MICE Imputation:** The code claims to use `fancyimpute.MICE` but then immediately states ""fancyimpute not available, using IterativeImputer for MICE simulation"" and uses `RandomForestRegressor` as the estimator. While IterativeImputer can *simulate* MICE, relying on a specific `fancyimpute` import and then immediately failing it with fallback makes the MICE implementation unconvincing and less robust. The ground truth explicitly used `IterativeImputer` from sklearn and mentioned it's MICE-like. The generated code's comparison of ""MICE"" via IterativeImputer with RandomForest to ""Iterative"" via default BayesianRidge is a good step towards comparing imputation methods, but the naming and reliance on a potentially unavailable library are less ideal.

2.  **GAM Implementation:** The `statsmodels.gam.api.GLMGam` usage appears incorrect and incomplete. The formula `gam_model = GLMGam.from_formula('y_train ~ 1', data=pd.DataFrame({'y_train': y_train}), smoother=sm.gam.BSplines(np.arange(len(features)), df=4), family=sm.families.Binomial())` seems to attempt to fit a GAM with only an intercept and then map features to splines in a non-standard way. The `smoother` argument typically takes a design matrix related to the smooth terms. It doesn't use the actual `X_train_gam` features. The ground truth used `pygam`, which is specifically designed for GAMs and offers simpler syntax for smooth terms. This is a significant functional error for GAMs.

3.  **Bayesian Logistic Regression (PyMC3):**
    *   The `X_train.values` in `pm.math.dot(X_train.values, beta)` implicitly uses all columns of `X_train`, which is fine, but the feedback points out that the ground truth explicitly subset the features for simplicity.
    *   The model and sampling generally seem correct. The `return_inferencedata=True` is a good practice.

4.  **Robust Statistical Methods (`RLM`):** The ground truth correctly demonstrates `RLM` for a regression problem, stating that it's not for classification. The generated code discusses ""Robust scaling"" and ""Robust correlation"" but doesn't implement a robust regression model (`RLM`) or general linear model to handle outliers or non-normal distributions in the context of modeling, failing to directly address the ""robust statistical methods"" part as thoroughly as the ground truth. It tests for normality, but that's not the same as applying robust methods.

5.  **Model Diagnostics:**
    *   For residual analysis, the generated code switches to an OLS model for a continuous target (`ph`). While this demonstrates how to do residuals, it misses the point of performing diagnostics *for the trained logistic regression models* (as the ground truth did using `statsmodels.api.GLM` and its specific diagnostics for binary outcomes). This is a critical omission, as the primary models built were classification models.
    *   The ""leverage plots"" are just residual vs. fitted plots, not true leverage plots in the sense of hat values. The ground truth correctly used `statsmodels.graphics.regressionplots.influence_plot` which provides integrated influence diagnostics.

6.  **Hypothesis Testing:**
    *   The ""Multivariate ANOVA"" is implemented as a loop of univariate ANOVAs (`anova_lm`) for each feature against 'Potability', not a true Multivariate ANOVA (MANOVA) that considers multiple dependent variables simultaneously, which `statsmodels.multivariate.manova.MANOVA` directly provides (as shown in the ground truth). This is a conceptual misunderstanding of the task.
    *   Permutation tests and bootstrap CI are implemented correctly.

7.  **Visualizations:** The code attempts ambitious visualizations, but some are mislabeled or not directly what was requested.
    *   ""Leverage plots"" are `scatter(fitted_values, residuals)` which is a common residual plot, but not what `plot_leverage_resid2` or `influence_plot` from `statsmodels` (which produce more specific leverage plots) would generate.
    *   The Q-Q plots are good.
    *   The ""Diagnostic Dashboard"" attempt is commendable for its structure, but the visualization content doesn't fully match the requested types (e.g., specific leverage plots from `statsmodels`).

Overall, the code structure is good, and it attempts to cover all requirements. However, several key implementations (GAM, MANOVA, robust methods, specific model diagnostics for logistic regression) are either incorrect, incomplete, or deviate from the standard and more appropriate methods. The reliance on OLS for diagnostics when the primary models are logistic regression is a notable flaw.",claude-sonnet-4,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import...",8,"The generated code provides a comprehensive solution for the water quality clustering task, covering most of the requirements.

**Strong Points:**
- **Advanced Feature Engineering:** The code implements a good variety of engineered features, including water quality indices, chemical balance ratios, and contamination risk scores, which is crucial for a data science benchmark. The use of `zscore` for risk scores is a good practice.
- **Multiple Clustering Algorithms:** It successfully applies K-means, Hierarchical, DBSCAN, Gaussian Mixture Models, and Spectral Clustering.
- **Optimal Number of Clusters:** It uses multiple metrics (silhouette, elbow, Davies-Bouldin, Calinski-Harabasz) and correctly visualizes them. It also includes gap statistic, which is an advanced validation technique.
- **Dimensionality Reduction:** PCA, t-SNE, UMAP, and Factor Analysis are all included and demonstrated.
- **Cluster Stability Analysis:** A basic but functional bootstrap resampling for stability is implemented.
- **Water Quality Profiles:** Detailed cluster analysis including size, potability rate, and mean/std of features is performed.
- **Visualization Dashboard:** Generates a comprehensive dashboard with various plots (validation metrics, DR 2D plots, bar charts for cluster characteristics, and a heatmap). Interactive Plotly visualizations for 3D, feature distribution, and a radar chart are also well-implemented.
- **Anomaly Detection:** Includes both Isolation Forest and Local Outlier Factor for anomaly detection, which was a specific requirement.
- **External Validation:** Attempts to validate against WHO standards and includes statistical significance testing (ANOVA, Chi-square), which are valuable additions.
- **Code Quality:** Generally well-structured with proper imports, comments, and logical flow.

**Areas for Improvement:**
- **Temporal Patterns:** The task mentioned temporal patterns ""if applicable."" While the dataset is static, a note explaining this, as in the ground truth, would be good. The generated code does not explicitly address that temporal analysis is not applicable for this dataset.
- **Cluster Stability Implementation:** The stability analysis using bootstrap resampling is a bit simplistic. The `predict` on the original data after training on bootstrap may not accurately capture stability, especially if cluster assignments shift arbitrarily. The ground truth's use of co-occurrence matrix for consensus clustering is a more robust approach to stability.
- **DBSCAN Parameter Tuning:** DBSCAN's `eps` is set to a fixed value (0.5), which is often dataset-dependent and might not be optimal. The ground truth visualizes the k-distance graph for better epsilon selection, which is a best practice.
- **Factor Analysis `n_components`:** Factor analysis `n_components` was fixed at 5, which might be arbitrary. It's often determined by cumulative explained variance or parallel analysis.
- **Interactive Plotly Integration:** While plotly is used, the `make_subplots` for all interactive plots could be better organized. For instance, the feature distribution box plot adds all features to one subplot, making it crowded. Separate plots for each type of visualization within plotly could be clearer. The radar chart is separate from the `make_subplots` which is slightly inconsistent.
- **Minor Feature Engineering Issues:**
    - `zscore` inside feature engineering might lead to data leakage if not handled carefully during cross-validation, although for a single run clustering, it's less critical.
    - `WQI_approx` formula seems ad-hoc and not strongly related to common WQI calculation methods (which typically involve sub-indices and specific weighting). The ground truth's WQI is also simplified but attempts to normalize and combine scores more explicitly.
    - `pH_category` and `Hardness_category` are created and then immediately `.cat.codes` applied, converting them to numerical. This might not be ideal for all subsequent analysis methods, as they lose their categorical interpretation.
- **Consistency in Optimal K:** The code determines optimal K through various metrics but then hardcodes `optimal_k = 4`. A more programmatic way to pick the optimal K (e.g., based on the peak silhouette score or elbow point) or a clear statement about manual selection would improve clarity. The ground truth implies manual selection after plotting, which is acceptable.

Overall, the code is highly functional and demonstrates a strong understanding of the requirements for advanced analytics in this domain, making it a good choice for a benchmark. The improvements are mainly about robustness, best practices, and polishing minor details.",claude-sonnet-4,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
from sklearn.linear_model import LinearRegression
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Calculate correlations with quality
quality_correlations = df.corr()['quality'].drop('quality').sort_values(ascending=False)

# Get top 3 positive and negative correlations
top_positive = quality_correlations.head(3)
top_negative = quality_correlation...",9,"The generated code successfully addresses most aspects of the task. It correctly calculates correlations, identifies top positive and negative features, generates a correlation heatmap, and creates scatter plots with linear regression lines.

**Strengths:**
- Correctly identifies top positive and negative correlations.
- Generates a clear correlation matrix heatmap.
- Creates scatter plots for the most positive and negative features.
- Successfully fits and plots linear regression lines on the scatter plots using `sklearn.linear_model.LinearRegression`. This is a valid alternative to `scipy.stats.linregress`.
- Uses `reshape(-1, 1)` for the `LinearRegression` input, which is good practice.

**Areas for Improvement (minor):**
- **Robustness:** The ground truth includes error handling for file not found, which is a good practice for general scripts. The generated code assumes the file exists.
- **`pearsonr` import:** While `pearsonr` was imported, it wasn't explicitly used for calculating the correlations displayed or the regression. `df.corr()` is sufficient for the correlation matrix. This is a minor point as `df.corr()` is the more direct way to get the full matrix.
- **R-squared value on plots:** The ground truth code explicitly calculates and displays the R-squared value on the scatter plot titles/labels. The generated code only displays the correlation coefficient (`r`). While `r` is related, displaying `r^2` often provides more insight into the explained variance for regression plots. This was an explicit requirement in the ground truth for `linregress` output but not strictly for `sklearn` implementation. Given the prompt asked for regression lines, `R^2` is a standard metric.
- **Labeling regression lines:** The ground truth adds a label to the regression line itself (`label=f'Regression Line (R={r_value**2:.2f})'`) and includes a `plt.legend()`, which was not explicitly done in the generated code's scatter plots. This improves clarity.

Overall, the generated code is very good and directly addresses the core requirements. The minor points are mostly about adding robustness, more explicit performance metrics, and better plot labeling, which are polish rather than fundamental correctness issues.",claude-sonnet-4,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import chi2_contingency
import warnings
warnings.filterwarnings('ignore')

# Load and clean the dataset
df = pd.read_csv('video_games_sales.csv')
df = df.dropna(subset=['Global_Sales', 'Publisher', 'Genre', 'Platform', 'Year'])
df['Year'] = pd.to_numeric(df['Year'], errors='coerce')
df = df.dropna(subset=['Year'])

print(""Dataset shape after cleaning:"", df.shape)
print(""\n"" + ""=""*50)

# 1) Test whether Nint...",7,"The generated code provides a good attempt at the task, covering most of the requirements and successfully implementing the statistical tests. However, there are several areas where it falls short compared to the ground truth, particularly in data cleaning robustness, explicit interpretation of results, and adherence to specific test parameters (e.g., alternative hypothesis for Nintendo test).

**Strong Points:**
*   **Correct Statistical Tests:** All required statistical tests (t-test, chi-square, Mann-Whitney U) are correctly imported and used.
*   **Data Preparation for Tests:** It correctly identifies and prepares data subsets for each test.
*   **Chi-square filtering:** Correctly filters for top 5 genres and platforms before performing the chi-square test.
*   **Clear Output:** The output for each test is well-formatted, displaying p-values and a clear ""Significant"" or ""Not Significant"" message.
*   **Summary:** Provides a nice summary of all test results at the end.

**Areas for Improvement (compared to Ground Truth):**

*   **Robust Data Cleaning:**
    *   The ground truth handles `Year_of_Release` (renamed to `Year` in generated code) more robustly by explicitly converting to `int` after dropping NaNs and handling `User_Score` cleaning although not used in the specific tests. The generated code uses `astype(int)` which can fail if `NaN`s are still present, thus the `to_numeric(errors='coerce')` then `dropna` is a better approach, but it misses the `Year_of_Release` column name.
    *   **CRITICAL FLAW:** The generated code renames 'Year_of_Release' to 'Year' implicitly in its `dropna` and subsequent usage, but the original column name in the CSV is `Year_of_Release`. This means `df.dropna(subset=['Global_Sales', 'Publisher', 'Genre', 'Platform', 'Year'])` will fail if `Year` column doesn't exist, which it doesn't. And then `df['Year'] = pd.to_numeric(df['Year'], errors='coerce')` will create a new 'Year' column of NaNs if the original 'Year_of_Release' column was not processed first. This will lead to an empty or mostly empty DataFrame for the Mann-Whitney test, which is a major correctness issue. The ground truth explicitly uses `Year_of_Release`.
    *   The generated code's initial `df = df.dropna(subset=['Global_Sales', 'Publisher', 'Genre', 'Platform', 'Year'])` will remove all rows where 'Year' is NaN, which is implicitly what the ground truth does for `Year_of_Release`, but then `df['Year'] = pd.to_numeric(df['Year'], errors='coerce')` and `df = df.dropna(subset=['Year'])` is redundant and can lead to issues if 'Year' was not original column name.

*   **Interpretation of Results:**
    *   The prompt specifically asks for ""interpret results with significance level =0.05"". While the generated code states ""Significant at =0.05: Yes/No"", it lacks the explicit *linguistic interpretation* of the rejection or failure to reject the null hypothesis, and what that *means* in the context of the business problem. For example, the ground truth statement ""Nintendo games have significantly higher average global sales compared to all other publishers combined."" is the type of interpretation expected.
    *   The generated code doesn't explicitly state the Null and Alternative Hypotheses for each test, which is good practice for clarity and was done in the ground truth.

*   **Specificity of T-test (Nintendo):** The first t-test for Nintendo sales required `alternative='greater'` (higher sales). The generated code correctly includes this, which is excellent.

*   **Error Handling:** The ground truth includes `try-except FileNotFoundError` for loading the CSV, which is a good practice for robust code. The generated code lacks this.

*   **Mann-Whitney Test Labels:** The generated code uses `< 2000` and `>= 2000`, while the prompt stated ""before 2000"" and ""after 2000"". The ground truth used `<= 2000` and `> 2000`, which is a subtle but notable difference in interpretation of ""before"" and ""after"". The prompt phrasing itself is slightly ambiguous, but typically ""before 2000"" means up to and including 1999. The ground truth's `Year` column cleaning to `int` is also important here.

*   **Redundant Import:** `from scipy.stats import chi2_contingency` is redundant when `from scipy import stats` already provides it as `stats.chi2_contingency`.

Overall, the generated code is functional and addresses most parts of the prompt but lacks the robustness, detailed interpretation, and precise adherence to column names (e.g., Year_of_Release) present in the higher-quality ground truth. The `Year` column issue is a significant bug that would prevent the code from running successfully without modification to column names.",claude-sonnet-4,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values
df = df.dropna()

# Create binary...",8,"The generated code provides a mostly correct and complete solution to the task.

**Strong points:**
- **Correct Target Variable Creation:** Successfully creates the 'High_Sales' binary target variable as specified.
- **Correct Feature Engineering:** Accurately calculates 'Total_Regional_Sales' and 'Years_Since_Release'.
- **Correct Encoding:** Uses `LabelEncoder` for categorical features ('Platform', 'Genre', 'Publisher').
- **Correct Model Training:** Trains a `RandomForestClassifier` with 100 estimators.
- **Comprehensive Evaluation:** Calculates and prints accuracy, precision, recall, F1-score, and confusion matrix, which aligns well with the requirements.
- **Feature Importance:** Correctly extracts and displays feature importance.
- **Visualization:** Includes plots for Confusion Matrix and Feature Importance, which is a nice addition although not explicitly required, it enhances usability.
- **Stratified Split:** Uses `stratify=y` for `train_test_split`, which is good practice for imbalanced datasets and crucial here.

**Areas for improvement:**
- **Missing Value Handling:** The ground truth code handles missing values more robustly. The generated code uses `df.dropna()`, which drops *any* row with *any* NaN. This can lead to significant data loss if many columns have a few NaNs. The ground truth specifically drops NaNs in crucial columns (`Global_Sales`, `Year`, `Publisher`, regional sales) and converts 'Year' to `int` *after* dropping. The generated code also implicitly makes a decision by dropping NaNs in 'Year' (a numerical variable) but then includes 'Year' in `feature_columns` without explicit conversion to int, which might cause issues or unexpected behavior if it's not already numeric after `dropna()`.
- **Feature Selection for `X`**: The generated code adds `_encoded` suffix to the encoded columns and then uses these new columns (e.g., `Platform_encoded`) in `feature_columns`. This is a valid approach, but the ground truth simply overwrites the original columns, which is also fine and perhaps slightly cleaner if the original string columns are no longer needed. More importantly, the generated code *includes* 'Year' as a direct feature in `feature_columns` *in addition* to 'Years_Since_Release'. While not explicitly wrong, typically one would use `Years_Since_Release` as the primary time-based feature, as `Year` itself is often just an ordinal counter. The ground truth only uses `Years_Since_Release`.
- **Imports:** Additional imports like `matplotlib.pyplot` and `seaborn` were included for plotting, which was not explicitly asked for but is a good addition. `classification_report` was also imported, which provides a nice summary.
- **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code does not.",claude-sonnet-4,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values
df = df.dropna(subset=['Year'])
numeric_columns = ['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sale...",7,"The generated code addresses most of the requirements of the task but has some significant issues that prevent it from being a fully correct and high-quality solution.

**Strong points:**
- **Proper Imports:** All necessary libraries are imported at the beginning.
- **Missing Value Handling:** Attempts to handle missing values for both numerical and categorical columns.
- **Engineered Features:** Correctly implements the five specified engineered features: 'Sales_Ratio_NA_EU', 'Publisher_Avg_Sales', 'Genre_Market_Share', 'Platform_Popularity', and 'Sales_Momentum'.
- **Correlation Analysis:** Generates a correlation matrix and visualizes it with a heatmap, as requested.
- **PCA:** Applies PCA, calculates explained variance, and visualizes it.
- **Polynomial Features:** Correctly applies polynomial features of degree 2 to the specified sales columns.
- **Interaction Features:** Creates interaction features between 'Year' and 'Genre' using one-hot encoding.
- **Outlier Handling:** Implements the IQR method for outlier detection and removal/analysis.

**Areas for Improvement / Issues:**
1.  **Missing Value Handling (Correctness):**
    *   The `df.dropna(subset=['Year'])` is too aggressive. The task specifically asks to ""handle missing values"" which implies imputation, not dropping rows when a large number might be missing (e.g., Year). The ground truth uses `SimpleImputer(strategy='median')` for 'Year'.
    *   For `Platform`, `Genre`, `Publisher`, the code uses `fillna(df[col].mode()[0])`, which is acceptable but the ground truth uses `SimpleImputer(strategy='most_frequent')` which is a more robust way to handle this as it creates an imputer object that can be reused if needed.
    *   `Name` column missing value handling was missed in the generated code. While not directly used in numerical analysis, it's a part of robust data loading.

2.  **Feature Engineering (`Publisher_Avg_Sales` and `Genre_Market_Share`):**
    *   Using `.map(publisher_avg)` and `.map(genre_market_share)` after groupby operations is less efficient and prone to issues if there are new publishers/genres not present in the original grouping. Using `.transform()` as in the ground truth is the correct and more robust way to broadcast aggregated values back to the original DataFrame size without affecting its index or requiring mapping. This is a common pitfall.

3.  **Correlation Analysis (`numerical_features` definition):**
    *   The selected `numerical_features` for correlation analysis include `Genre_Encoded`, `Platform_Encoded`, and `Publisher_Encoded`. While these are numerical representations, 'Year' and these encoded features are typically not expected to be part of ""all numerical features"" for correlation in the same way sales figures are. The ground truth precisely selects `df.select_dtypes(include=np.number).columns.tolist()` and then removes `Rank`, which is a more accurate way to get *all true numerical features* without implicitly including encoded categorical ones unless specifically intended for that analysis. The task asks for correlation between ""all numerical features"". `Rank` was explicitly excluded in ground truth, but not in generated code's definition of `numerical_features`.

4.  **PCA (`numerical_features` for scaling):**
    *   Similar to correlation, using `Genre_Encoded`, `Platform_Encoded` as features for PCA alongside the actual sales and engineered features is not ideal, as PCA is meant for continuous numerical data to find dimensions. Encoding categorical features into integers can incorrectly imply ordinality for PCA purposes. One-hot encoding would be more appropriate if categories were to be included in PCA. The ground truth correctly excludes this by using `df.select_dtypes(include=np.number)` prior to PCA.

5.  **Polynomial Features (Placement and Impact Analysis):**
    *   The generated code applies polynomial features and immediately plots a heat map of original vs. polynomial features and `Global_Sales`. While this shows correlation, it doesn't clearly *analyze* their impact as requested. The ground truth code focuses on *creating* them and integrating into the main dataframe, leaving the specific analysis of their impact implicit (e.g., for model building later). The prompt asks to ""analyze their impact"" which might imply more than just a correlation heatmap among the features themselves, but perhaps their relation to a target variable, or their effect on model performance. The inclusion of `Global_Sales` in the correlation heatmap is a good step but not fully comprehensive.

6.  **Interaction Features (`Year_x_Genre`):**
    *   The interaction features are created (`df_interactions = pd.get_dummies(df['Genre'], prefix='Genre')`) but the `df_interactions` DataFrame itself is not `concatenated` back to the main `df`. This means the new interaction columns `Year_x_Genre_xyz` are added, but the base one-hot encoded columns `Genre_xyz` are not. While the interaction columns are the primary request, the ground truth adds the full set of one-hot encoded genre columns to the DataFrame, which is typically a good practice and simplifies downstream processes if those original one-hot features are needed.

7.  **Outlier Handling (Incompleteness/Clarity):**
    *   The code identifies and *removes* outliers from `Global_Sales` into a *new* DataFrame `df_no_outliers`. The task asks to ""Identify and *handle* outliers using IQR method"". While removing is a valid way to handle, *capping* (as done in the ground truth) is often preferred as it preserves the data points and their relationships while mitigating extreme values. The generated code also only applies outlier handling to `Global_Sales`, whereas the task implies applying it to numerical features prone to outliers, and the ground truth applies it to a broader list of relevant numerical features (`sales_cols`, `Sales_Ratio_NA_EU`, `Publisher_Avg_Sales`, etc.). The last output `df_no_outliers.shape` indicates a new DataFrame, not modification of the main `df`, which might not be desired for further analysis building on the full data.

8.  **Output and Structure:**
    *   The generated code includes additional ""summary statistics"" and ""feature importance"" analysis at the end which were not explicitly requested in the prompt. While potentially useful, they deviate from the explicit task requirements.
    *   There is less verbose printing throughout the process compared to the ground truth, which makes it harder to follow the data transformation stages.

Overall, the generated code successfully implements the core feature engineering and analysis steps but lacks some correctness in implementations (e.g., `transform` vs `map`, imputation strategy) and completeness in addressing all nuances of handling specific data types and columns as robustly as the ground truth.",claude-sonnet-4,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import chi2_contingency
import warnings
warnings.filterwarnings('ignore')

# Load dataset
df = pd.read_csv('diabetes.csv')

# Clean zero values by replacing with NaN and median imputation
columns_to_clean = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
for col in columns_to_clean:
    df[col] = df[col].replace(0, np.nan)
    df[col] = df[col].fillna(df[col].median())

# 1. T-test for gluco...",8,"The generated code correctly addresses most of the requirements and demonstrates good understanding of the statistical tests.

**Strengths:**
*   **Correct Imputation:** The median imputation for zero values is correctly implemented and applied to the specified columns.
*   **All Tests Performed:** All requested statistical tests (t-test, Mann-Whitney U, chi-square, Pearson correlation, ANOVA) are performed.
*   **Correct Test Application:** The tests are applied to the appropriate variables and groups as specified in the task (e.g., diabetic vs. non-diabetic, age groups vs. outcome, BMI categories vs. glucose).
*   **Interpretations:** The p-values are displayed, and a clear ""Significant: Yes/No"" or ""Reject H0/Fail to reject H0"" is provided based on the =0.05 level, fulfilling the interpretation requirement.
*   **Code Structure:** The code is well-structured with clear comments for each section.

**Areas for Improvement (Why not a 9 or 10):**
*   **T-test `alternative` parameter:** The ground truth code used `equal_var=False` (Welch's t-test), which is generally safer when variance assumptions are not checked. The generated code uses `alternative='greater'`. While this could be a valid one-sided hypothesis if specifically aiming to test *higher* glucose levels, the task phrasing ""significantly higher glucose levels"" implies a one-sided test, but the t-test itself typically tests for *any* significant difference unless specified. For a general ""significant difference"", a two-sided test (`alternative='two-sided'`) or explicit `equal_var=False` is often preferred. The ground truth specifically focused on ""significantly higher"" which `alternative='greater'` implies, but a two-sided test would also be acceptable for ""significantly higher"" if the interpretation accounts for the direction. The ground truth implicitly tested for *any* difference then concluded *specifically* higher based on the means. The generated code's `alternative='greater'` directly tests for *higher*, which is a good choice for the phrased question. My initial read skewed towards general difference, but ""significantly higher"" makes `alternative='greater'` appropriate.
*   **BMI Category Definition:** The `bins` for `BMI_Category` are slightly off compared to common BMI classifications and the ground truth. `bins=[0, 18.5, 24.9, 29.9, 100]` for a `right=True` (default for `pd.cut`) binning would define Normal as (18.5, 24.9], Overweight as (24.9, 29.9], etc. The ground truth used `[0, 18.5, 25, 30, np.inf]` with `right=False`, which correctly maps to `[0, 18.5)`, `[18.5, 25)`, `[25, 30)`, `[30, inf)`. This is a minor point, but consistency with standard definitions and the ground truth's logic would be better.
*   **Error Handling for File Loading:** The ground truth included a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.
*   **Detailed Interpretation:** While the code indicates significance, the ground truth provides a more verbose and human-readable interpretation of the results for each test, elaborating on what ""rejecting the null hypothesis"" means in the context of the variables being tested (e.g., ""diabetic patients have significantly higher glucose levels""). The generated code provides a concise summary, which is good, but lacks the extra interpretation.

Overall, it's a solid solution that correctly implements the core requirements. The improvements are mostly about robustness, minor definition precision, and more detailed output interpretation.",claude-sonnet-4,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confus...",6,"The generated code attempts to address most aspects of the task, but several critical issues prevent it from being production-ready or fully accurate.

**Strengths:**
* **Correct Data Loading and Missing Value Handling:** Correctly loads the CSV and handles missing values (zeros) by replacing them with the median in specified columns.
* **Feature Engineering:** Implemented the requested new features ('Glucose_BMI_ratio', 'Age_group', 'Insulin_level').
* **Multiple Models & Evaluation Metrics:** Trained and evaluated multiple classification models (Logistic Regression, Random Forest, SVM, Gradient Boosting) using the specified metrics (accuracy, precision, recall, F1-score, ROC-AUC).
* **Cross-Validation:** Performed cross-validation for model evaluation.
* **Visualization:** Included confusion matrices and ROC curves.
* **Feature Importance:** Attempted to display feature importance for tree-based models.

**Weaknesses and Areas for Improvement:**

1.  **Incorrect Categorical Encoding:** The primary and most significant flaw is the use of `LabelEncoder` for 'Age_group' and 'Insulin_level'. `LabelEncoder` assigns arbitrary numerical labels (e.g., 0, 1, 2) which implies an ordinal relationship that doesn't exist for these features (e.g., 'Young' is not ""less than"" 'Middle' in a numerical sense that a model can interpret meaningfully without one-hot encoding). This severely impacts model performance and interpretability, especially for non-tree-based models. The task explicitly asks for ""Encode categorical variables,"" and `OneHotEncoder` is the standard for non-ordinal features.
2.  **Inconsistent Preprocessing:** The code applies `StandardScaler` only to `X_train` and `X_test` for SVM, but not for other models (Logistic Regression, Random Forest, Gradient Boosting). All models (especially linear models and SVMs) benefit from scaling, and tree-based models can also sometimes benefit, though it's less critical. The Ground Truth code uses a `ColumnTransformer` and `StandardScaler` for all numerical features, ensuring consistent and correct preprocessing for all models.
3.  **Feature Drop for Training:** When preparing `X`, the code drops the original 'Age_group' and 'Insulin_level' columns, but then it *also* drops the `Age_group_encoded` and `Insulin_level_encoded` columns in the same line (`X = df.drop(['Outcome', 'Age_group', 'Insulin_level'], axis=1)` if you intended to use the encoded columns as features). This means the encoded features are not actually used in model training, further exacerbating the preprocessing issue. The corrected line should be `X = df.drop(['Outcome', 'Age_group', 'Insulin_level'], axis=1)` while the encoded features (`Age_group_encoded`, `Insulin_level_encoded`) are *kept* in `X`. However, if `X = df.drop(['Outcome', 'Age_group', 'Insulin_level'], axis=1)` was run, then the encoded columns `Age_group_encoded` and `Insulin_level_encoded` are retained by default, which means they are being used. The problem, as stated in point 1, is that they are `LabelEncoded` which is incorrect.
4.  **Inefficient Hyperparameter Tuning Logic:** The hyperparameter tuning block conditionally selects the best model *after* initial evaluation based on CV accuracy. A better approach would be to tune a pre-specified ""best candidate"" model (e.g., Random Forest or Gradient Boosting, which are generally strong performers) or to tune all models, then compare. The current approach is inflexible and adds unnecessary conditional logic for a benchmark.
5.  **Lack of `Pipeline` and `ColumnTransformer`:** The absence of `Pipeline` and `ColumnTransformer` (as used in the Ground Truth) makes the preprocessing inflexible and prone to errors. It forces manual scaling for specific models, leading to inconsistencies. A `ColumnTransformer` would correctly handle both numerical scaling and one-hot encoding within a single preprocessing step, simplifying the workflow and ensuring that the entire transformer is applied consistently during cross-validation and tuning.
6.  **`pd.qcut` Usage for Insulin_level:** While `pd.qcut` is used, the interpretation of `q=3` should directly correspond to 'low', 'normal', 'high' without manual bin definitions, as `pd.qcut` by default creates equal-sized quantiles. The manual `[0, insulin_tertiles.iloc[0], insulin_tertiles.iloc[1], np.inf]` introduces extra complexity and is less robust than letting `qcut` handle it directly. The Ground Truth's approach `pd.qcut(df['Insulin'], q=3, labels=['low', 'normal', 'high'])` is simpler and more robust.
7.  **Feature Importance for SVM and LR:** The `feature_importance` code block attempts to access `model.feature_importances_` which is only available for tree-based models. It doesn't handle the case for Logistic Regression or SVM (which have coefficients, not `feature_importances_`), though the `if model_name in tree_models:` check does prevent errors here. However, it means feature importance is not displayed for ALL relevant models.
8.  **Output Clarity:** The output from classification reports is missing, which is a valuable part of evaluation. The Ground Truth also includes a comprehensive summary table of all metrics for all models, which is more readable.

Overall, the code demonstrates an understanding of the task but falls short on implementing robust, standard, and correct machine learning practices for preprocessing and model comparison.",claude-sonnet-4,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.smote import SMOTE
impo...",8,"The generated code successfully addresses most of the requirements of the task, demonstrating good understanding of feature engineering, dimensionality reduction, and feature selection techniques.

**Strong Points:**
*   **Comprehensive Feature Engineering:** All requested engineered features ('Metabolic\_Score', 'Risk\_Factor\_Count', 'Glucose\_per\_Age', 'BMI\_BloodPressure\_interaction', 'Pedigree\_Age\_product') are created. The 'Risk\_Factor\_Count' is well-implemented using reasonable quantile-based thresholds.
*   **Correct Missing Value Handling:** The code correctly identifies and handles missing values (0s) in relevant columns by replacing them with median imputation.
*   **Polynomial Features:** Polynomial features are applied correctly using `PolynomialFeatures`, and the interaction\_only parameter is a good choice for avoiding redundant features.
*   **Dual Scaling:** Both StandardScaler and MinMaxScaler are applied as requested.
*   **PCA Implementation:** PCA is correctly applied, and the explained variance ratio is clearly visualized, showing a good understanding of how to determine the number of components for a desired variance.
*   **Effective Feature Selection:** Both `SelectKBest` and `RFE` are implemented and their selected features are printed, showing a good attempt to cover the requirements for feature selection.
*   **Correlation Filtering:** The correlation-based feature filtering is correctly implemented to remove highly correlated features, which is essential for model robustness.
*   **SMOTE for Imbalance:** SMOTE is applied to handle class imbalance, which is crucial for the `diabetes.csv` dataset.
*   **Feature Importance Evaluation:** Random Forest is used for feature importance evaluation, and the top features are visualized, fulfilling this requirement.
*   **Good Code Structure and Imports:** The code is well-structured with clear steps, and necessary imports are included.

**Areas for Improvement:**
*   **Polynomial Feature Integration:** While polynomial features are created and appended, `X_combined = pd.concat([X, poly_df], axis=1)` could lead to duplicate columns if `X` still contains the `poly_cols` themselves. The ground truth explicitly drops the original `poly_cols` from `X` before concatenation to prevent this. This is a subtle but important detail for clean feature sets.
*   **PCA on Filtered Data vs. Full Data:** The prompt implies a flow where PCA might be applied *after* other preprocessing/feature selection, but here it's applied on `X_scaled_standard` (the full scaled dataset before correlation filtering). The prompt's specific order for PCA after correlation filtering isn't strictly followed by this code (though the ground truth also has this slightly out of order relative to the prompt). Consistency in the feature set used for PCA vs. SelectKBest/RFE could be improved; the ground truth applies PCA on the correlation-filtered data, which makes more sense.
*   **RFE Estimator:** While a RandomForestClassifier is used as the estimator for RFE, a simpler model like Logistic Regression (as in the ground truth) is often preferred for RFE to explicitly select features based on a simple linear relationship, and then a more complex model like RandomForest can be used for final importance. This is a nuanced point but good practice.
*   **Verbosity/Prints:** The output could benefit from more structured print statements to indicate the current step and the state of the data (e.g., number of features after each step), similar to the ground truth, which fulfills this part of the hidden prompt requirement.
*   **Minor Plotting Redundancy:** The `plt.subplot(2, 1, 1)` and `plt.subplot(2, 1, 2)` for feature importance visualization is a bit redundant; a single bar plot for top features is generally sufficient for the task without explicitly showing 'original features importance' (which is mostly covered by the top features anyway).
*   **Error Handling for File Loading:** The ground truth includes a `try-except` block for `pd.read_csv`, which is good practice for robust code.

Overall, this is a strong submission that accurately implements the majority of the complex requirements. The minor points mentioned are more about stylistic choices or fine-tuning the order of operations for optimal workflow, rather than fundamental errors.",claude-sonnet-4,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import mannwhitneyu, f_oneway, chi2_contingency
import warnings
warnings.filterwarnings('ignore')

# Load dataset and handle missing values
df = pd.read_csv('world-happiness-report-2021.csv')
df = df.dropna()

# Clean column names
df.columns = df.columns.str.strip()

# 1) Test whether Western European countries have significantly higher happiness scores
western_europe = df[df['Regional indicator'] == 'Weste...",7,"The generated code addresses most of the requirements and performs the requested statistical tests. However, there are several key areas for improvement in terms of robust data handling, detailed interpretation, and alignment with the specific steps requested in the prompt.

Here's a breakdown:

**Strengths:**
*   **Correct Test Application:** The code correctly identifies and uses `t-test`, `Mann-Whitney U test`, `ANOVA`, `Pearson correlation`, and `Chi-square test`.
*   **Basic Data Loading/Cleaning:** It loads the data and handles missing values by dropping them.
*   **Structured Output:** P-values and significance are printed for each test, and a summary is provided.
*   **Appropriate Imports:** Necessary libraries are imported.

**Areas for Improvement:**

1.  **Missing Value Handling (Robustness):** The ground truth uses `df.dropna(subset=critical_columns, inplace=True)` which is more robust, as dropping all NaNs (`df.dropna()`) can lead to loss of valuable data if missing values are only in irrelevant columns. The generated code just uses `df.dropna()`.
2.  **Order of Operations and Numbering:** The generated code's numbering for the tests (1-6) does not match the prompt's numbering (1-7), which can be confusing.
3.  **T-test for Western Europe (Alternative Hypothesis):** The prompt asks if Western European countries have *significantly higher* happiness scores. The generated code uses `alternative='greater'` which correctly sets up a one-tailed test. The ground truth used `equal_var=False` (Welch's t-test) and then interpreted the means to determine ""higher"" or ""lower"", which is more robust for unequal variances, but didn't explicitly use `alternative='greater'` in the function call. While both approaches can lead to the right conclusion, the generated code's explicit `alternative='greater'` is good for the specific phrasing of ""significantly higher"". However, the prompt asked for `t-test`, not necessarily Welch's, so this is a minor difference.
4.  **GDP Comparison (Top/Bottom 25%):** The prompt specifies top 25% and bottom 25% of *happiest* countries. The generated code uses `df['Ladder score'].quantile(0.75)` and `df['Ladder score'].quantile(0.25)` to select countries. This defines the groups based on *thresholds* rather than directly selecting the top and bottom 25% by count after sorting, which is what the ground truth does. This could lead to slightly different group sizes and members if there are many countries with scores exactly at the quantile boundaries. For example, if many countries share the 0.75 quantile score, more than 25% might be included. The ground truth's `tail(num_25_percent)` and `head(num_25_percent)` is a more precise interpretation of ""top 25%"" and ""bottom 25%"".
5.  **Chi-square Happiness Categories:** The ground truth divides happiness into Low/Medium/High based on **equal quantiles (1/3, 2/3)**, aiming for roughly equal group sizes. The generated code uses `pd.cut(df['Ladder score'], bins=3, labels=['Low', 'Medium', 'High'])`, which primarily creates bins of equal *width* rather than equal *number of observations*. This can result in very skewed category distributions, which might not be ideal for a chi-square test.
6.  **T-test for Freedom vs. Generosity:** The prompt asks if generosity levels are ""significantly different"" (implying a two-tailed test), which the generated code correctly implements by default (`ttest_ind`). The ground truth uses `equal_var=False` (Welch's t-test) for robustness, which is generally a good practice when unsure about variance equality. The generated code doesn't specify `equal_var`, so it defaults to `True` (Student's t-test).
7.  **Interpretation of Results:** While the code prints ""Significant at =0.05: Yes/No"", it lacks the more detailed interpretive text that the ground truth provides (e.g., ""Western European countries have significantly HIGHER happiness scores..."" or ""There is no statistically significant difference...""). The prompt explicitly asks to ""Display p-values and interpret results with significance level =0.05"". This is a notable missing element.
8.  **Significance Level Definition:** The ground truth explicitly defines `alpha = 0.05` at the beginning and uses it consistently. The generated code hardcodes `0.05` in the print statements. Defining `alpha` as a variable is better practice.
9.  **Error Handling:** The ground truth includes `try-except FileNotFoundError`, which is good practice for data loading. The generated code does not.",claude-sonnet-4,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
warnings.filte...",7,"The generated code attempts to address all aspects of the task, but several issues prevent it from being a perfect solution, especially in correctness and completeness against the ground truth's specific requirements.

**Correctness Issues:**
*   **Missing Value Handling:** The generated code uses `df.fillna(df.mean(numeric_only=True))` for numerical columns and then attempts to fill categorical columns. However, the ground truth explicitly states to drop rows with missing values in the *specified target and feature columns*. The generated code's approach of imputing means can distort the data if there are many missing values, and it doesn't align with the ground truth's more straightforward handling.
*   **Categorical Variable Encoding:** The task description and ground truth clearly state that ""no categorical features are used in the model."" The generated code incorrectly assumes there might be categorical features in the specified `feature_columns` and attempts to use `LabelEncoder`. This is problematic because the task specifically asks for a set of numerical features. `LabelEncoder` is also generally not suitable for nominal categorical variables in regression; `OneHotEncoder` would be more appropriate if such features were present and intended to be used. The ground truth correctly identifies that no encoding is needed for the chosen features.
*   **Feature and Target Column Names:** The generated code includes complex logic for ""Alternative column names"" and ""Check and update feature column names."" While this might be robust for varied datasets, the prompt specifically listed the exact column names from the `world-happiness-report-2021.csv` ('Logged GDP per capita', 'Freedom to make life choices', 'Perceptions of corruption'). The ground truth uses these exact names, which is simpler and more accurate given the specific dataset. The generated code also includes `if col != 'Country name' and col != 'Regional indicator':` for LabelEncoder, implying it might process these, but they are not listed as features.
*   **Scaling:** The generated code applies `StandardScaler` for linear models but *not* for tree-based models (Random Forest, Gradient Boosting). While tree-based models are generally not sensitive to scaling, it's good practice to consider it, or at least be consistent with the ground truth if it intended a full pipeline. More importantly, it scales *after* `train_test_split` on `X_train` and `X_test`, which is correct. However, for cross-validation and hypothetical predictions, it makes assumptions about whether to use scaled or unscaled data that could lead to inconsistencies if not carefully managed. The ground truth typically handles this by either not scaling explicitly for the chosen features or using a `Pipeline` for robustness if needed.
*   **Hyperparameter Tuning:** The `GridSearchCV` logic for the best model is conditional and somewhat overly complex. The ground truth explicitly picks `GradientBoostingRegressor` for tuning, which is a good default choice for strong performance. The generated code first identifies the ""best"" model based on initial R, then sets up tuning for that specific model. This is an improvement in methodology but leads to a bulky `if/elif` block. Also, the `param_grid` for Ridge and Lasso is relatively small, and for tree models too, compared to what a thorough search might encompass.
*   **Cross-validation:** The generated code performs cross-validation but does so on `X_train_scaled` or `X_train` and *then* plots mean CV scores. The ground truth also explicitly implemented `KFold` and `cross_val_score` on the *full dataset* (`X`, `y`) for assessing model stability, which is a common and robust approach. The generated code's CV is mainly on the training set.

**Completeness Issues:**
*   **`KFold` implementation:** The ground truth specifically used `KFold` with `shuffle=True` and `random_state`. The generated code uses the default `cv=5` in `cross_val_score`, which is usually `StratifiedKFold` for classification or plain `KFold` without shuffling if not explicitly set (which might not be ideal for regression). It doesn't explicitly define `KFold` as requested.
*   **Error Handling for File:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice and part of robust code. The generated code lacks this.
*   **Assumptions about best model for tuning:** The ground truth explicitly identified the best model based on R2, but then stated it would tune the ""Gradient Boosting Regressor as it's often a strong performer and benefits from tuning."" This is a pragmatic choice. The generated code's logic to tune the ""best initial model"" is fine but could lead to tuning linear models which generally don't benefit as much from tuning compared to ensemble models.

**Code Quality:**
*   **Imports:** Imports are generally correct.
*   **Structure:** The code is structured logically with sections for loading, preprocessing, model training, evaluation, tuning, visualizations, and predictions.
*   **Readability:** The code is readable, but the conditional logic for scaling and hyperparameter tuning adds some complexity that the ground truth avoids by using a `Pipeline` (if scaling were consistent) or by making a deliberate choice about which model to tune.
*   **Warnings:** The `warnings.filterwarnings('ignore')` is generally discouraged as it can hide important messages.

Overall, the generated code produces a working solution that covers most requirements. However, the specific implementation details, especially around missing value handling, categorical encoding, and the precision in column selection (as per the prompt's exact feature names), deviate from the ground truth's more accurate and robust approach. The hyperparameter tuning and cross-validation implementations, while present, could be refined to better match the ground truth's comprehensive and robust nature.",claude-sonnet-4,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_regression, RFE, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mea...",9,"The generated code demonstrates a very strong understanding of the requirements and implements nearly all aspects of the task effectively.

**Strong points:**
*   **Comprehensive Feature Engineering**: Successfully creates all requested engineered features ('Happiness_Efficiency', 'Social_Wellness_Index', 'Governance_Score', 'Regional_Happiness_Rank', 'GDP_vs_Regional_Average') and additional interaction features.
*   **Polynomial Features**: Correctly applies polynomial features of degree 2 to key happiness factors.
*   **Multiple Scaling Methods**: Implements StandardScaler, MinMaxScaler, and RobustScaler as requested.
*   **Dimensionality Reduction**: Correctly applies PCA and t-SNE for dimensionality reduction and provides visualizations. PCA also includes calculation for 95% variance which is a good practice.
*   **Feature Selection**: Successfully uses SelectKBest (with `f_regression` and `mutual_info_regression`) and Recursive Feature Elimination (RFE).
*   **Correlation-based Filtering**: Implements a robust method for filtering highly correlated features.
*   **Mutual Information Scoring**: Correctly calculates and presents mutual information scores.
*   **Random Forest Feature Importance**: Validates feature importance using Random Forest and visualizes the results.
*   **Robustness**: Includes `try-except` for column name handling and adds small constant to avoid division by zero in `Happiness_Efficiency`.
*   **Good Visualizations**: Provides a wide range of relevant visualizations that clearly illustrate the results of different steps (feature importance, PCA variance, t-SNE, correlation heatmap, MI scores, KBest scores, PCA 2D projection, scaling comparison). This goes beyond just implementing the methods.
*   **Code Structure**: The code is well-structured, with clear comments and logical grouping of steps.

**Areas for minor improvement:**

*   **Data Preprocessing**: While `fillna` with median for all numeric columns is a good general approach, the ground truth specifically targets `Perceptions of corruption` and then drops NaNs for core columns, which might be a more precise handling based on the task description's context. The generated code's broad `fillna` might inadvertently ""fill"" NaNs in identifier columns if they were numeric.
*   **Column Name Handling Logic**: The `try-except` block for column names, while attempting to be robust, is complex and less direct than simply using the specified column names from the problem description. The prompt implies specific column names that should be handled as given. The ground truth handles this by listing relevant columns, which is simpler and less error-prone for the specified dataset.
*   **Consistency in `X` and `y` Handling**: When performing feature selection (e.g., RFE, KBest), the base estimator for RFE for the Ground Truth is `LinearRegression` as it's typically used for linear models, whereas the generated code uses `RandomForestRegressor`. While using `RandomForestRegressor` is perfectly valid, the prompt didn't specify which model to use for RFE.
*   **Missing `plt.show()`**: Some visualizations are generated but might not display immediately if not grouped, or if previous `plt.show()` calls are missing (though in this script, they are grouped and handled fairly well, but it's a common oversight). The ground truth ensures each visualization is explicitly shown. (Upon review, the generated code groups them into a single `plt.show()` at the end, which is also a valid approach for displaying multiple plots in one run.)
*   **`mutual_info_regression` for SelectKBest**: The ground truth explicitly uses `mutual_info_regression` for a `SelectKBest` step. The generated code also calculates MI scores, but only uses `f_regression` for `SelectKBest`. It might be beneficial to explicitly demonstrate `SelectKBest` with `mutual_info_regression` as well. (Upon review, the generated code *does* compute MI scores, but applies SelectKBest only with `f_regression`. The prompt asked for MI *scoring* and *SelectKBest*, implying MI scoring as a method for SelectKBest).

The issues are minor and mostly about fine-tuning or alternative interpretations, not critical failures. The generated code provides a comprehensive and high-quality solution to the task.",claude-sonnet-4,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import warnings
warnings.filterwarnings('ignore')

# 1) Load dataset, parse date column, and filter for USA
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])
usa_data = df[df['location'] == 'USA'].copy()

# 2) Set dat...",9,"The generated code successfully addresses most of the requirements. It correctly loads and filters the data, resamples it, performs time series decomposition, plots the components, conducts the ADF test, applies differencing if needed, and plots ACF/PACF.

**Strong Points:**
*   Correctly implements data loading, parsing, and filtering.
*   Accurately performs weekly resampling with `sum`.
*   Applies `seasonal_decompose` with a sensible period (52).
*   Correctly plots the decomposed components and ACF/PACF.
*   Implements the ADF test and conditional differencing logic.
*   Uses `plt.tight_layout()` for better plot presentation.

**Areas for Minor Improvement:**
*   **Error Handling:** The code lacks robust error handling for `FileNotFoundError` or cases where 'USA' data might be empty after filtering. The ground truth includes `try-except` blocks and checks for empty DataFrames, which is good practice for production-ready code.
*   **`period` for `seasonal_decompose`:** While 52 is a reasonable default for weekly data, it's good to consider edge cases where the dataset might not have 2 full years of data, which could cause `seasonal_decompose` to fail or produce less reliable results. The ground truth handles this with a warning and potentially adjusting the period.
*   **`dropna()` for stationary data:** While `stationary_data.dropna()` is used before plotting ACF/PACF, it's good practice to ensure that the series passed to `adfuller` and `plot_acf`/`plot_pacf` is already cleaned if `diff()` introduces `NaN` values. The current code does this mostly correctly, but explicit handling (like in the ground truth's `adf_test_results` function) can make it more robust.
*   **Encapsulation:** The ground truth encapsulates the logic within a function, making it more reusable and modular. This is not explicitly required by the prompt's formatting (""Provide only the Python code without explanations""), but it's a good code quality practice that could be seen as an improvement.
*   **Plot Titles:** The plots for decomposition could benefit from a main title, as demonstrated in the ground truth solution.
*   **`numpy` import:** `import numpy as np` is present but `np` is not used in the code. It can be removed.

Overall, the code is highly functional and fulfills the core requirements. The minor points are mostly about robustness, error handling, and best practices for production-level code.",claude-sonnet-4,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from scipy import stats
from scipy.stats import f_oneway
import seaborn as sns

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Categorize wines based on pH values
def categorize_ph(ph):
    if ph < 3.2:
        return 'Low pH'
    elif 3.2 <= ph <= 3.4:
        return 'Medium pH'
    else:
        return 'High pH'

df['pH_group'] = df['pH'].apply(categorize_ph)

# Calculate average quality for each pH...",8,"The generated code correctly addresses most of the task requirements.

**Strong points:**
- **Correct pH Categorization:** The use of a function `categorize_ph` effectively categorizes the pH values into the specified groups.
- **Average Quality Calculation:** Correctly calculates and prints the average quality for each pH group.
- **ANOVA Test:** Performs the ANOVA test correctly and prints the F-statistic, p-value, and a conclusion based on the significance level.
- **Confidence Interval Calculation:** The `calculate_ci` function correctly computes the 95% confidence intervals using the t-distribution, which is appropriate for sample means.
- **Visualization:** Creates a bar chart with error bars representing the calculated 95% confidence intervals, labeled appropriately.
- **Clarity:** The code is generally clear and well-commented, making it easy to understand each step.

**Areas for Improvement (Why not a 10):**
- **Slightly less robust pH grouping:** While effective, using `pd.cut` with predefined bins and labels (as in the ground truth) is generally more robust and concise for binning numerical data than a custom `apply` function, especially if there are many bins or complex boundary conditions (`include_lowest`, `right`). The current `categorize_ph` uses `3.2 <= ph <= 3.4`, which means 3.2 and 3.4 are included in ""Medium pH"", whereas the prompt implies `(3.2 to 3.4)` for medium and `>3.4` for high. The ground truth's `pd.cut(..., right=False)` correctly implements `[lower, upper)` for bins.
- **Hardcoded Group Order in Plotting:** The `ph_groups = ['Low pH', 'Medium pH', 'High pH']` list is hardcoded for plotting and CI calculation. If the data didn't contain all groups, or if the order was different, this could lead to issues. Using `pH_group` as a categorical type with a defined order (as in the ground truth) is more robust for maintaining consistent ordering across different operations (grouping, plotting).
- **Redundant Imports/Not Used:** `seaborn` is imported but not used. `from scipy import stats` and `from scipy.stats import f_oneway` is a bit redundant; `from scipy.stats import f_oneway, t, sem` would be cleaner or just `import scipy.stats as stats`.
- **Minor Plotting Detail:** The `plt.ylim(0, max(means) + max(cis) + 0.5)` might not always set the most intuitive y-limit if one group's CI is very large. While not a critical flaw, a more dynamic or fixed range could be better, depending on the data. Also, the error bars drawn are symmetric `yerr=cis`, but typically the CI from the mean would consist of a lower and upper bound, not necessarily symmetric `ci`. The ground truth calculates the specific CI range and error bar length from the mean to the lower/upper bound, which is more precise. However, for a quick statistical visualization, `yerr=cis` is acceptable, as `cis` here represents `h`, the half-width of the interval. `stats.sem` is good for calculating standard error.
- **Output of `avg_quality_by_ph` does not preserve order:** `df.groupby('pH_group')['quality'].mean()` will sort the groups alphabetically by default (`High pH`, `Low pH`, `Medium pH`) unless `pH_group` is explicitly made a categorical type with a specified order. This means the printed average quality might not match the order in the plot or the conceptual order of pH. The plotting code later re-establishes the correct order via the `ph_groups` list, but it's an inconsistency.",claude-sonnet-4,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load the dataset and parse 'date'
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

# Select features and t...",7,"The generated code correctly addresses most of the requirements, including loading data, selecting features, training two models (Linear Regression and Random Forest), and evaluating them. However, there are several issues that reduce its score:

1.  **Imputation of `continent`:** The generated code attempts to impute `continent` using `SimpleImputer(strategy='median')` because `df[features]` includes `continent` if it's considered. This is incorrect as 'continent' is a categorical variable and should not be median-imputed. The ground truth correctly handles numerical and categorical columns separately for imputation/encoding. **Correction Needed:** The `features` list should *not* include 'continent' for imputation. 'continent' needs to be handled separately by one-hot encoding *before* scaling.
2.  **Order of Operations (Scaling and Splitting):** The generated code scales the *entire* dataset `X` (`X_scaled = scaler.fit_transform(X)`) *before* splitting the data into training and testing sets. This leads to data leakage, where information from the test set influences the scaling applied to the training set. The `StandardScaler` should be `fit` only on the training data and then `transform` applied to both training and test data. The ground truth correctly applies scaling after the train-test split.
3.  **One-Hot Encoding Integration:** The way one-hot encoding is integrated is slightly disjointed. While `continent_encoded` is created, the original `continent` column might still be in `df[features]`, leading to potential type errors or incorrect imputation if it's treated as a numerical feature. The ground truth correctly adds `continent` to the initial `features` list, then separates numerical and categorical for handling, and finally merges them.
4.  **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is a good practice for robustness that the generated code misses.

Overall, the core logic for model training and evaluation is present, but the preprocessing steps have critical flaws related to data type handling and preventing data leakage.",claude-sonnet-4,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import f_oneway, pearsonr, ttest_ind
import statsmodels.api as sm
from statsmodels.stats.anova import anova_lm

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Handle missing values with median imputation for relevant columns
columns_to_impute = ['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']
for col in columns_to_impute:
    if col in ...",7,"The generated code correctly performs the three requested hypothesis tests: ANOVA, Pearson correlation, and an independent t-test. It loads the data, handles missing values via median imputation, states hypotheses, prints p-values, and draws conclusions based on the 0.05 significance level for each test.

However, there are a few areas for improvement:

1.  **Strict Adherence to Instructions (""Provide only the Python code without explanations."")**: The generated code includes print statements like ""1) ANOVA Test - Reproduction Rate across Continents"" which, while useful for clarity when run, contradict the instruction ""Provide only the Python code without explanations."" The ground truth manages to achieve the necessary output with only the statistical results and conclusions. This is a minor point but worth noting for strict adherence.
2.  **Robustness in Data Derivation for T-test**:
    *   The `groupby('location')` for the latest entry per country should probably be `groupby('country')` given the dataset context and the problem description implies a country-level analysis. While `location` might map to country in this dataset, `country` is safer if available and generally clearer. (Self-correction: The synthetic dataset `location` column *is* equivalent to `country` in this context, so `location` is functionally correct).
    *   The calculation of `death_rate = latest_data['total_deaths'] / latest_data['population']` does not handle potential division by zero or NaN values in 'population', which could lead to `inf` or `NaN` in the `death_rate` column, potentially affecting the `quantile` calculation or subsequent filtering. The ground truth handles this by replacing `inf/-inf` with `np.nan` and then dropping `NaN`s before quantile calculation.
    *   The `high_death_rate` and `low_death_rate` groups for the t-test call `.dropna()` which is good, but the initial imputation step should ensure that `median_age` is not NaN, so this particular `.dropna()` might be redundant if the imputation was perfect.
3.  **Imports**: `statsmodels.api as sm` and `anova_lm` are imported but not used. While not a functional error, it's unnecessary clutter. The ground truth only imports what's needed.
4.  **Clarity/Comments**: The prompt asks for ""only the Python code without explanations"". The generated code provides some descriptive print statements for the titles of tests and null hypotheses, which is good. The ground truth explicitly states the null hypotheses before the test, which is a good practice as per the prompt. The generated code also does this.

Overall, it's a solid attempt that addresses the core requirements. The issues are mainly about robustness, minor adherence to strict output format, and unnecessary imports rather than functional correctness.",claude-sonnet-4,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select numerical columns
numerical_cols = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
df_numerical = df[numerical_cols]

# Calculate pairwise correlation matrix
correlation_matrix = df_numerical.corr()

# Create heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
   ...",9,"The generated code correctly implements all specified requirements: loading the dataset, selecting the numerical columns, calculating the correlation matrix, and visualizing it with a heatmap. It includes the necessary imports and produces the expected output.

**Strengths:**
- **Correctness:** The code correctly performs all steps of the task.
- **Conciseness:** It is very direct and to the point.
- **Proper Imports:** All required libraries are imported.
- **Visualization:** The heatmap is generated with appropriate parameters (`annot=True`, `cmap='coolwarm'`).

**Areas for Minor Improvement (to reach a 10):**
- **Robustness:** The ground truth includes error handling (e.g., `FileNotFoundError`, `KeyError` for missing columns) and clear print statements indicating progress or issues. While not explicitly asked for in the prompt's ""provide only the Python code without explanations"" for the output, robust code generally includes such checks.
- **Function Encapsulation:** The ground truth encapsulates the logic within a function, which is good practice for reusability and modularity, especially in larger scripts.
- **Clarity of `center` argument:** While `center=0` is reasonable for a correlation matrix, explicitly setting `fmt="".2f""` in `sns.heatmap` as the ground truth does often provides clearer formatting of the annotations, rather than relying on default behaviors.

Overall, it's a strong solution that effectively addresses the core task.",claude-sonnet-4,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Group by rarity and calculate averages
rarity_stats = df.groupby('rarity').agg({
    'elixirCost': 'mean',
    'Win Rate': 'mean',
    'Usage': 'mean'
}).round(2)

# Display the results
print(rarity_stats)
```",10,"The generated code is correct, complete, and of good quality. It successfully loads the dataset, groups it by 'rarity', calculates the mean for 'elixirCost', 'Win Rate', and 'Usage', and displays the results. The use of `.agg()` is an equally valid and often more explicit way to perform column-specific aggregations compared to directly selecting columns before `.mean()`. The addition of `.round(2)` is a nice touch for readability, though not strictly required by the prompt, it doesn't detract from correctness.",claude-sonnet-4,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Create binary target column
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)

# Select features
features = ['elixirCost', 'Usage', 'rarity']
X = df[features].copy()
y = df['is_high_win_rate']

# Apply one-hot encoding to rarity column
encoder = OneHotEncoder(sparse_output=False, drop='first')...",8,"The generated code correctly performs all the required steps: loading data, creating the target column, selecting features, applying one-hot encoding, and splitting the data. The use of `OneHotEncoder` is acceptable, although `pd.get_dummies` (used in the ground truth) is often simpler for this specific task when dealing with DataFrames directly.

The points preventing a higher score are:
1.  **Complexity of One-Hot Encoding**: While technically correct, the implementation of `OneHotEncoder` is more verbose than necessary, especially when compared to `pd.get_dummies`. It involves creating an encoder, fitting/transforming, getting feature names, creating a new DataFrame, and then concatenating it back to the original features. `pd.get_dummies` handles this much more elegantly in a single line. The `drop='first'` argument also deviates from the implicit expectation of not dropping the first category if the ground truth doesn't specify it, although it's a common practice in OHE to avoid multicollinearity. The prompt did not specify setting `drop_first=True` (or equivalent).
2.  **Missing `try-except` for File Loading**: The generated code does not include the `try-except` block for handling `FileNotFoundError` when loading the CSV, which is a good practice for robust code, demonstrated in the ground truth.

Overall, it's a very good attempt that fulfills the core requirements, but could be slightly more concise and robust.",claude-sonnet-4,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv('iris.csv')

# Define features X
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Define target y
y = df['Species']

# Encode categorical target variable
label_encoder = LabelEncoder()
y_encoded = label_encoder...",9,"The generated code correctly implements all the requirements of the task: loading data, defining features and target, encoding the target, splitting the data with the correct test size and random state, training a KNN classifier with `n_neighbors=3`, and reporting the accuracy. The imports are correct and the code structure is logical.

The only minor difference from the ground truth is the omission of `stratify=y_encoded` in the `train_test_split` function. While not explicitly asked for, adding `stratify` is generally a best practice for classification tasks, especially with imbalanced datasets, to ensure that the class distribution in the train and test sets reflects the original dataset. However, for the Iris dataset, which is balanced, its omission does not significantly impact correctness. The ground truth also included error handling for `FileNotFoundError` and a more verbose print statement for accuracy, but these are minor stylistic differences and not core requirements of the task.",claude-sonnet-4,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('iris.csv')

# Filter the data to get two groups
versicolor = df[df['Species'] == 'Iris-versicolor']['PetalLengthCm']
virginica = df[df['Species'] == 'Iris-virginica']['PetalLengthCm']

# State the hypotheses
print(""Null Hypothesis (H0): There is no difference in mean PetalLengthCm between Iris-versicolor and Iris-virginica"")
print(""Alternative Hypothesis (H1): There is a significant difference in mean Pet...",9,"The generated code correctly performs all the required steps: loading the data, filtering for the two species, performing the t-test, stating the hypotheses, printing the t-statistic and p-value, and interpreting the result based on the given alpha. The code is well-structured and uses appropriate libraries.

**Strong points:**
- All requirements are met.
- Hypotheses are clearly stated.
- Interpretation of the p-value against alpha is correct.
- Uses standard libraries and functions effectively.

**Minor Improvements (to reach a perfect 10):**
- **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError` and checks for empty groups, which makes the code more robust. The generated code lacks these.
- **Clarity of Output:** The ground truth provides more verbose and descriptive print statements (e.g., ""Calculated T-statistic:"", ""Significance Level (alpha):"", separating sections with dashes), which improves readability for a user interpreting the output. The request mentions ""Clearly state the null and alternative hypotheses"" and ""Clearly state the null and alternative hypotheses"". While the generated code states them, the ground truth's formatting makes them stand out more.",claude-sonnet-4,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('iris.csv')

# Set up the plotting style
plt.style.use('default')
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
fig.suptitle('Box Plots of Iris Features by Species', fontsize=16)

# List of numerical features
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

# Create box plots for each feature
for i, feature in enumerate(features):
    row = i /...",9,"The generated code correctly loads the dataset and generates box plots for each numerical feature, grouped by species, with appropriate titles. This addresses the core requirements of the task.

**Strong Points:**
- Correctly uses pandas to load the CSV.
- Effectively uses seaborn's `boxplot` function.
- Creates plots grouped by `Species`.
- Adds titles to each plot, indicating the feature.
- Uses `plt.subplots` to arrange all four plots in a single figure, which is a good practice for comparison and presentation.
- `tight_layout()` is used for proper plot spacing.

**Areas for Improvement (minor):**
- **Error Handling:** The code lacks error handling for `FileNotFoundError` (e.g., if `iris.csv` is not present), which the ground truth includes. While the prompt states ""assume the CSV file is in the current directory,"" robustness is always a good practice in real-world scenarios.
- **Individual Plot Display:** The prompt implied ""create a box plot"" which could be interpreted as showing each plot independently (`plt.show()` after each plot, as in the ground truth). However, combining them into `subplots` is generally better for comparison, so this is more of an interpretation difference than a flaw. The ground truth's approach ensures each plot is shown sequentially/individually if preferred.
- **Title Detail:** The ground truth titles explicitly mention ""Distribution of [Feature] by Species"", while the generated code simply has ""[Feature] by Species"". Both are acceptable, but the former is slightly more descriptive.",claude-sonnet-4,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv('heart.csv')

# Select features and target variable
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']
X = df[features]
y = df['HeartDisease']

# One-hot encode categorical features
encoder = OneHotEncoder(dr...",7,"The generated code correctly addresses most of the task requirements: loading data, selecting features/target, splitting data, training a Logistic Regression model, and reporting accuracy. The one-hot encoding for categorical features is handled correctly by manually concatenating the processed features.

However, there are a few areas for improvement:
1.  **Preprocessing on Training Data vs. Test Data Separation:** While the manual concatenation works, it's generally better practice to encapsulate preprocessing steps within a `Pipeline` (as seen in the ground truth) or use a `ColumnTransformer`. The current approach applies `fit_transform` to `X[categorical_features]` before splitting the data, which means `fit` for the `OneHotEncoder` is done on the *entire* dataset. Ideally, `fit` should only happen on the training data to prevent data leakage. While `sparse_output=False` and `drop='first'` are used, the more robust way is to fit the transformer on `X_train` and then `transform` both `X_train` and `X_test`.
2.  **`drop='first'` in `OneHotEncoder`**: The task instructions did not specify `drop='first'` for one-hot encoding, which removes one category to avoid multicollinearity. While often a good practice, it wasn't a strict requirement, and the `ColumnTransformer` with `handle_unknown='ignore'` is a more direct interpretation of ""one-hot encoding"" without implicit column dropping, and handles unseen categories more robustly.
3.  **`max_iter` for `LogisticRegression`**: The default `max_iter` (100) for `LogisticRegression` is often sufficient, but for some datasets, especially with more features or complex relationships, it might issue a convergence warning. The ground truth preemptively increased `max_iter`, which is a good minor robustness improvement. The generated code did not.
4.  **Clarity and Readability**: While functional, using `ColumnTransformer` and `Pipeline` (as in the ground truth) often leads to cleaner, more maintainable, and less error-prone code for complex preprocessing workflows.

Overall, the code is functional and produces correct results for this specific problem, but could benefit from more robust and standard sklearn practices for preprocessing.",claude-sonnet-4,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('heart.csv')

# Create two groups based on Sex column
males = df[df['Sex'] == 'M']['Cholesterol']
females = df[df['Sex'] == 'F']['Cholesterol']

# Remove zero values (assuming they are missing/invalid data)
males = males[males > 0]
females = females[females > 0]

# State hypotheses
print(""Null Hypothesis (H0): There is no difference in mean cholesterol levels between males and females"")
print(""Alternative ...",8,"The generated code correctly addresses most of the requirements of the task. It loads the dataset, separates the `Cholesterol` data for males and females, states the hypotheses, performs the t-test, prints the t-statistic and p-value, and interprets the result based on the given alpha.

**Strong points:**
- Correctly uses `pandas` to load data.
- Correctly separates data into two groups based on 'Sex'.
- Correctly performs an independent two-sample t-test using `scipy.stats.ttest_ind`.
- Clearly states the null and alternative hypotheses.
- Prints the t-statistic and p-value with appropriate formatting.
- Correctly interprets the p-value against the alpha level and states the conclusion.

**Areas for improvement (why it's not a 10):**
1.  **Robustness/Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions during file loading, and checks for the existence of required columns. The generated code lacks these crucial error handling mechanisms, making it less robust.
2.  **`equal_var` parameter in `ttest_ind`:** The ground truth explicitly chose `equal_var=False` (Welch's t-test) which is generally more robust as it does not assume equal variances. The generated code uses the default `equal_var=True` (Student's t-test), which might be inappropriate if the variances are not equal. While the prompt didn't explicitly specify, in statistical practice, Welch's t-test is often preferred as a safer default.
3.  **Missing/Invalid Data Handling:** The generated code includes `males = males[males > 0]` and `females = females[females > 0]` to remove zero values, assuming they are missing/invalid. While this is a plausible assumption for cholesterol, the prompt did not specify this requirement; typically, `dropna()` (as used in ground truth) is more standard for handling missing values if `NaN`s are present. The ground truth focuses on `dropna()` which is the standard way to handle actual `NaN` values rather than assuming 0s are invalid data without explicit instruction.
4.  **Function Encapsulation:** The ground truth encapsulates the logic within a function (`perform_cholesterol_t_test`), making the code reusable and organized. The generated code is a script, which is acceptable for a direct task but less ideal for real-world applications where reusability is key. The prompt asked for ""only the Python code without explanations"" which might imply a script, but a function is generally better practice for modularity.",claude-sonnet-4,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Function to identify outliers using IQR method
def identify_outliers_iqr(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = (data < lower_bound) | (data > upper_bound)
    return outliers, lower_bound, upper_bound

# Function to winsorize outliers
def winsorize_outliers(data...",9,"The generated code correctly implements all the requirements of the task. It identifies outliers using the IQR method, reports the number of outliers, performs winsorization by capping values at the 1.5*IQR bounds, and compares the mean and standard deviation before and after treatment. The code is well-structured with clear functions for outlier identification and winsorization, which enhances readability and reusability. The use of `np.clip` for winsorization is efficient and concise.

Potential minor improvement:
- While not explicitly required for this specific task, adding a `try-except` block for `pd.read_csv` would make the code more robust against `FileNotFoundError`, as was present in the ground truth. This is a best practice for handling file operations.
- The output for the comparison could be slightly more explicit about which values correspond to which column during the report, although it's clear enough given the headings.",claude-sonnet-4,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('heart.csv')

# Create crosstab for grouped bar chart
ct = pd.crosstab(df['ChestPainType'], df['HeartDisease'])

# Create grouped bar chart
fig, ax = plt.subplots(figsize=(10, 6))
ct.plot(kind='bar', ax=ax, color=['skyblue', 'salmon'])

# Customize the plot
plt.title('Count of Heart Disease by Chest Pain Type', fontsize=14, fontweight='bold')
plt.xlabel('Chest Pain Type', font...",8,"The generated code correctly loads the dataset and creates a grouped bar chart as requested. It uses `pd.crosstab` effectively to prepare the data for plotting, which is a valid approach. The plot includes a title, labeled axes, and a legend, fulfilling most requirements.

**Strong points:**
- Correctly loads the CSV file.
- Creates a grouped bar chart showing the count of HeartDisease (0 and 1) for each ChestPainType.
- Includes a title, x-label, y-label, and legend.
- Uses both pandas for data manipulation and matplotlib/seaborn for plotting.
- Concise and readable.

**Areas for improvement:**
- **Robustness:** The code directly calls `pd.read_csv` without any error handling (e.g., `FileNotFoundError`), which is present in the ground truth.
- **Categorical order:** The ground truth explicitly orders `ChestPainType` categories (`TA`, `ATA`, `NAP`, `ASY`), ensuring consistent plotting. The generated code relies on the default alphabetical order or the order encountered in the CSV, which might not match the desired order.
- **Legend labels:** While the legend is functional, the ground truth explicitly maps 0 to 'No Heart Disease' and 1 to 'Heart Disease', making the legend more descriptive. The generated code uses numeric labels with parenthetical descriptions (`No Heart Disease (0)`), which is acceptable but slightly less clean than dedicated descriptive labels.
- **No function encapsulation:** The generated code is a script, whereas the ground truth encapsulates the logic in a function, which is generally better practice for reusability and modularity, especially in a benchmark context.",claude-sonnet-4,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Clean the in_shazam_charts column
df['in_shazam_charts'] = df['in_shazam_charts'].astype(str).str.replace(',', '')
df['in_shazam_charts'] = pd.to_numeric(df['in_shazam_charts'], errors='coerce')

# Report count of NaNs
nan_count = df['in_shazam_charts'].isna().sum()
print(f""Number of NaN values in in_shazam_charts: {nan_count}"")

# Group by key and mode, calculate averages
grouped = df.gr...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, cleans the `in_shazam_charts` column as specified, reports NaN counts, and then correctly groups the data by `key` and `mode` to calculate the average `danceability_%` and `valence_%`. The `astype(str)` conversion before `str.replace` is a good practice to handle potential mixed types. The use of `.agg()` for grouping is also perfectly valid.

A minor improvement could be to include a `try-except FileNotFoundError` block for `pd.read_csv` for robustness, similar to the ground truth, but this wasn't explicitly asked for in the prompt and doesn't detract from the core logic's correctness. The output format for the NaN count message is also slightly different but conveys the information clearly. Overall, very good and complete.",claude-sonnet-4,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Ensure streams column is numeric
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# Select features and target
X = df[['in_spotify_playlists', 'energy_%']]
y = df['streams']

# Drop rows with NaN values
data = pd.concat([X, y], axis=1)....",9,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, converts 'streams' to numeric, selects the correct features and target, handles NaNs, splits the data, trains a Linear Regression model, and reports R2 and MSE.

**Strong points:**
* Correct handling of `pd.to_numeric(errors='coerce')`.
* Correct selection of features (`X`) and target (`y`).
* Correct data splitting with `random_state=42` and `test_size=0.2`.
* Proper use of `LinearRegression`, `r2_score`, and `mean_squared_error`.
* The `dropna()` approach by concatenating `X` and `y` ensures that NaNs are dropped from all relevant columns, which is a clever way to handle the requirement.

**Areas for minor improvement:**
* **Error Handling for File Loading:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is good practice for robust code but not strictly required by the prompt's core logic. The generated code skips this, making it less robust in a real-world scenario but still functionally correct for the benchmark scenario where the file is assumed to exist. However, the prompt clearly states ""assume the CSV file is in the current directory"", so this isn't a critical omission for this specific evaluation.
* **Print Statements:** The ground truth includes more descriptive print statements regarding data loading, cleaning, and model training/evaluation, which enhances user understanding. The generated code provides only the final R2 and MSE. While the prompt asks for ""only the Python code without explanations"", the ground truth shows that including informative print statements *within* the code (not as separate explanations) is acceptable and improves clarity. This is a minor stylistic difference.

Overall, the generated code performs the requested task accurately and efficiently, demonstrating a strong understanding of the data science workflow. The `dropna` strategy could be slightly more explicit about which columns are being used for dropping, but its current implementation achieves the desired outcome.",claude-sonnet-4,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd
from datetime import datetime

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Combine released_year, released_month, and released_day into a single datetime column
df['release_date'] = pd.to_datetime(
    df[['released_year', 'released_month', 'released_day']], 
    errors='coerce'
)

# Create days_since_release feature
fixed_date = pd.to_datetime('2023-12-31')
df['days_since_release'] = (fixed_date - df['release_date']).dt.days

# Display the first 5 tr...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, successfully combines the year, month, and day into a `release_date` column, and correctly handles errors using `errors='coerce'`. It then accurately calculates `days_since_release` as an integer number of days between the release date and '2023-12-31'. Finally, it displays the requested columns for the first 5 tracks.

The code is concise and uses appropriate pandas functionalities like `pd.to_datetime` with a list of columns for date construction, which is a neat alternative to string concatenation.

Areas for minor improvement (not critical for the task benchmark but good practice):
- Adding error handling for file loading (e.g., `try-except FileNotFoundError`) would make the code more robust, though it wasn't explicitly asked for as a primary task goal.
- While `dt.days` returns an integer, the resulting column type in pandas might default to float if there are any NaNs. Explicitly casting to pandas' nullable integer type (`'Int64'`) as seen in the ground truth can be beneficial for strict type adherence, but for this specific task, it wasn't a strict requirement that the *column Dtype* be integer, only that the *value* stored for days be an integer. The generated code implicitly handles this acceptably for the task.",claude-sonnet-4,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean Video views column
df['Video views'] = df['Video views'].astype(str).str.replace(',', '')
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce')
df['Video views'] = df['Video views'].fillna(0)

# Clean Likes column
df['Likes'] = df['Likes'].astype(str).str.replace(',', '')
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Likes'] = df['Likes'].fillna(...",9,"The generated code correctly addresses all the requirements of the task. It properly loads the CSV, cleans ""Video views"" and ""Likes"" columns by removing commas, converting to numeric, and handling NaNs with 0. It also correctly fills missing ""Category"" values with 'Unknown' and performs the required grouping and sum aggregation.

**Strong points:**
- Correct use of `pd.read_csv`.
- Accurate cleaning logic for `Video views` and `Likes` columns using `str.replace`, `pd.to_numeric(errors='coerce')`, and `fillna(0)`.
- Correct handling of missing `Category` values with `fillna('Unknown')`.
- Proper use of `groupby()` and `agg()` to calculate the sum of `Video views` and `Likes` per category.
- Clear and concise code.

**Minor areas for improvement (not critical for this task but good practice):**
- The ground truth includes robust error handling for `FileNotFoundError` and checks for column existence. While the prompt implied the columns would exist and the file would be present, in a real-world scenario, these checks are crucial for more robust code. The generated code assumes the CSV file is present and the columns `Video views`, `Likes`, and `Category` exist, which is acceptable given the constraints of the prompt, but it's worth noting the absence of these checks compared to the ground truth.
- The `numpy` import is not strictly necessary as `pandas` handles all operations here.

Overall, it's a very good solution that directly answers the prompt's requirements effectively.",claude-sonnet-4,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats
import numpy as np

# 1) Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# 2) Clean the Likes column
df['Likes'] = df['Likes'].astype(str).str.replace(',', '')
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

# 3) Fill missing values in Category column
df['Category'] = df['Category'].fillna('Unknown')

# 4) Create two groups
music_likes = df[df['Category'] == 'Music']['Likes']
sports_l...",9,"The generated code correctly performs all the required steps: loading the data, cleaning the 'Likes' column, handling missing 'Category' values, creating groups, performing the t-test, stating hypotheses, and interpreting the results.

The code is well-structured and uses appropriate pandas and scipy functions.

Minor points for improvement:
- **T-test Equal Variance Assumption**: The ground truth uses `equal_var=False` for `ttest_ind`, which performs Welch's t-test and is generally more robust as it does not assume equal population variances. The generated code defaults to `equal_var=True`. While not explicitly stated in the prompt, it's a best practice for independent two-sample t-tests if not certain about variance equality.
- **Hypotheses Wording**: The hypotheses statements are slightly less formal than the ground truth's, but still convey the correct meaning. The ground truth provides a more precise mathematical representation (_music = _sports).
- **Error Handling**: The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. This makes the ground truth more robust.
- **Variable Naming**: The ground truth renames the cleaned 'Likes' column to 'Likes_cleaned' to avoid overwriting the original, which can be useful for debugging or if the original column is needed later. The generated code overwrites 'Likes' directly. This is a minor style point.

Overall, the code is highly functional and fulfills almost all requirements. Its only minor shortcomings are in robustness (no file error handling) and a slightly less robust statistical assumption for the t-test (default `equal_var=True`).",claude-sonnet-4,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean Likes column
df['Likes'] = df['Likes'].astype(str).str.replace(',', '')
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

# Clean Dislikes column
df['Dislikes'] = df['Dislikes'].astype(str).str.replace(',', '')
df['Dislikes'] = pd.to_numeric(df['Dislikes'], errors='coerce')
df['Dislikes'] = df['Dislikes'].fillna(0)

# Create TotalEng...",9,"The generated code correctly implements all the required feature engineering steps: loading the dataset, cleaning 'Likes' and 'Dislikes' (removing commas, converting to numeric, filling NaNs with 0), creating `TotalEngagement`, creating `LikeDislikeRatio` (with the +1 to avoid division by zero), and displaying the specified columns. The code is concise and directly addresses the prompt's requirements.

The only minor point of improvement compared to the ground truth is the lack of robust error handling for file loading (e.g., `FileNotFoundError`). While the prompt didn't explicitly ask for error handling, it's good practice for real-world scenarios. The ground truth includes this, making it marginally more robust. However, for a data science benchmark focusing on the core transformation logic, the generated code is excellent. The `numpy` import is also superfluous as it's not used.",claude-sonnet-4,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the Launched Price (USA) column
df['Launched Price (USA)'] = df['Launched Price (USA)'].astype(str).str.replace('USD ', '').str.replace(',', '')
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'], errors='coerce')

# Clean the Screen Size column
df['Screen Size'] = df['Screen Size'].astype(str).str.replace(' inches', '')
df['Screen Size'] = pd.to_numeric(df['Scree...",9,"The generated code correctly addresses all the core requirements of the task. It loads the CSV, cleans both specified columns by removing specific strings and converting to numeric, handling errors by coercing to NaN. It then correctly groups the data and calculates the mean for both cleaned columns. Finally, it displays the aggregated result.

**Strong points:**
* **Correctness:** The code correctly performs all stated transformations and aggregations.
* **Conciseness:** The cleaning steps are well-chaining `astype(str)`.str.replace().str.replace()`, making the code compact and readable.
* **Error Handling for numeric conversion:** Uses `errors='coerce'` as required, which is crucial for real-world data.
* **Proper Grouping and Aggregation:** Uses `groupby().agg()` effectively.

**Areas for minor improvement:**
* **Clarity of column names:** When cleaning, the code overwrites the original `Launched Price (USA)` and `Screen Size` columns. While this works, creating new columns like `Cleaned_Price` and `Cleaned_Screen_Size` (as in the ground truth) can sometimes improve readability and preserve original data if needed for other operations. This is a stylistic choice and doesn't affect correctness for this specific task.
* **Robustness with file loading:** It lacks the `try-except` block for `pd.read_csv`, which is good practice for production code to handle `FileNotFoundError` or other loading issues gracefully. However, the prompt did not explicitly ask for error handling during load, only for the cleaning specific columns.
* **Import `numpy`:** `numpy` is imported, but not used. While harmless, it's good practice to remove unused imports.

Overall, it's a very good solution that successfully accomplishes the task.",claude-sonnet-4,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Define function to clean camera columns
def clean_camera_specs(camera_str):
    if pd.isna(camera_str):
        return 0
    # Extract first sequence of digits
    match = re.search(r'\d+', str(camera_str))
    if match:
        return int(match.group())
    else:
        return 0

# Apply function to create cleaned camera columns
df['Cleaned Front Camera'] = df['Front Camera'].apply(clean_camera_...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, defines a function to clean camera specifications by extracting the first sequence of digits, handles NaN values gracefully, applies this function to create new cleaned columns, calculates the total camera MP, and displays the required columns for the first 10 rows.

Strengths:
- Correctly uses pandas for data loading and manipulation.
- Correctly uses `re` for extracting numerical values from strings.
- The `clean_camera_specs` function accurately extracts the first digit sequence and handles non-digit cases by returning 0, including `pd.isna` values.
- All derived columns (`Cleaned Front Camera`, `Cleaned Back Camera`, `Total Camera MP`) are correctly created.
- The final display of the first 10 rows for the specified columns is correct.

Areas for minor improvement (not critical for correctness, but good practice):
- The ground truth code includes error handling for `FileNotFoundError` when loading the CSV, which is robust. The generated code does not, meaning it would raise an error if the file isn't present.
- The ground truth code includes print statements to indicate the progress of the script (e.g., ""Dataset loaded successfully.""), which can be helpful for user experience in larger scripts.
- The ground truth code uses `to_string()` for display, which can sometimes provide cleaner console output than the default `print(df.head(10))` for dataframes.",claude-sonnet-4,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
import numpy as np

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean RAM column
df['RAM'] = df['RAM'].astype(str).str.replace('GB', '', regex=False)
df['RAM'] = pd.to_numeric(df['RAM'], errors='coerce')

# Clean Battery Capacity column
df['Battery Capacity'] = df['Battery Capacity'].astype(st...",10,"The generated code correctly addresses all requirements of the task. It loads the dataset, cleans the specified columns effectively by removing units and commas (for price) and converting them to numeric types while handling errors by coercing to NaN. It then correctly drops rows with NaN values in these specific columns before defining features and target variables. The data splitting, model training, and evaluation (R2 and MAE reporting) are all implemented accurately and according to the instructions. The imports are correct and complete. The code structure is logical and easy to follow.",claude-sonnet-4,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import chi2_contingency, ttest_ind, f_oneway
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
df = pd.read_csv('titanic.csv')

# Clean the data
df = df.dropna(subset=['Pclass', 'Survived'])
df_age = df.dropna(subset=['Age'])
df_fare = df.dropna(subset=['Fare'])

print(""="" * 60)
print(""STATISTICAL ANALYSIS OF TITANIC DATASET"")
print(""="" * 60)

# 1. Chi-square test: Association between Pc...",8,"The generated code correctly performs all the requested statistical tests (Chi-square, T-test, ANOVA) and calculates their respective effect sizes (Cramer's V, Cohen's d, Eta-squared). It also interprets the results based on a predefined alpha level (0.05) and displays the p-values, test statistics, and conclusions.

However, there are a few areas for improvement:
1.  **Data Cleaning/Handling Missing Values:** The generated code uses `dropna` on the full DataFrame initially (`df = df.dropna(subset=['Pclass', 'Survived'])`) and then creates `df_age` and `df_fare` by dropping NaNs specifically for 'Age' or 'Fare'. While this approach ensures that the specific analyses use clean data for their respective variables, the initial `dropna` on Pclass/Survived might be unnecessary or subtly different from the ground truth which cleans only for the relevant analysis. The ground truth handles `dropna` right before each specific test, ensuring the largest possible dataset for each test if other columns are irrelevant for that test.
2.  **T-test equal_var:** The generated code hardcodes `equal_var=False` for the t-test (Welch's t-test). While often a safer general choice, the ground truth code includes Levene's test to statistically decide whether equal variances can be assumed, which is a more robust and statistically sound approach to choosing between pooled and Welch's t-test. Including this adds a layer of statistical rigor.
3.  **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.
4.  **Clarity of Output:** While the output is clear, the ground truth code adds more narrative and specific hypotheses (H0/Ha) before each test, which is helpful for understanding the context of the statistical analysis.
5.  **Cramer's V calculation robustness**: The ground truth includes a check `if (n * min_dim) > 0 else 0` to prevent division by zero for Cramer's V calculation, which the generated code lacks. Similarly for Cohen's d, the ground truth has a `if (n1 + n2 - 2) > 0:` check.
6.  **Levene's Test for ANOVA**: Similar to the t-test, explicitly performing Levene's test before ANOVA (as in the ground truth) confirms the homogeneity of variances assumption, adding more rigor to the analysis. The generated code skips this.",claude-sonnet-4,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Group by Year and calculate averages
yearly_stats = df.groupby('Year').agg({
    'Rating': 'mean',
    'Runtime': 'mean'
}).reset_index()

# Create line plot for average Rating over Year
plt.figure(figsize=(10, 6))
plt.plot(yearly_stats['Year'], yearly_stats['Rating'], marker='o', linewidth=2)
plt.title('Average IMDb Rating Trend Over Years')
plt.xlabel('Year')
plt.ylabel('Av...",7,"The generated code correctly loads the data, groups by 'Year' and calculates the average 'Rating' and 'Runtime', and generates the required line plot with a title and labeled axes. This fulfills the main requirements of the task.

However, the code lacks robustness in handling potential data inconsistencies. The ground truth includes essential data cleaning steps for the 'Year' and 'Runtime' columns (e.g., extracting year from ""(2000)"", cleaning ' min' from 'Runtime', and handling potential non-numeric values). Without these steps, the generated code might fail or produce incorrect results if the 'imdb_top_250.csv' dataset contains dirty data, which is common in real-world scenarios. Also, the ground truth uses a function to encapsulate the logic, which is better for reusability and error handling. The generated code also does not handle potential `FileNotFoundError`.

Improvements for a higher score would include:
1.  Adding data cleaning steps for 'Year' (if it contains non-numeric strings) and 'Runtime' (to convert to numeric and handle units).
2.  Adding error handling for `FileNotFoundError` or other potential issues during data loading/processing.
3.  Encapsulating the logic within a function for better modularity and reusability.",claude-sonnet-4,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create two groups based on Year
group_before_1990 = df[df['Year'] < 1990]['Rating']
group_after_1990 = df[df['Year'] >= 1990]['Rating']

# Perform independent two-sample t-test
t_statistic, p_value = stats.ttest_ind(group_before_1990, group_after_1990)

# Print hypotheses
print(""Null Hypothesis (H0): There is no difference in mean ratings between movies released before 1990 and movie...",8,"The generated code correctly performs the core task of loading data, creating groups, performing a t-test, stating hypotheses, and interpreting the result. It effectively uses `pandas` and `scipy.stats` as required.

Areas for improvement to achieve a higher score:
1.  **Robustness (Data Loading):** The ground truth code includes error handling for `FileNotFoundError` and also explicitly converts `Year` and `Rating` columns to numeric types with error handling (`errors='coerce'`) and subsequent `dropna`. This makes the code more robust to unexpected data formats or missing files. The generated code assumes the CSV file exists and the columns are already in a usable format, which might not always be the case in real-world scenarios.
2.  **Hypothesis Wording:** While the generated code states the hypotheses, the ground truth provides a slightly more formal and complete phrasing (""statistically significant difference,"" ""mean 'Rating'""). This is a minor point but adds to clarity.
3.  **T-statistic and P-value formatting:** The ground truth formats the output to two or four decimal places, which makes the output cleaner and easier to read. The generated code prints the full precision.

Overall, it's a very good attempt that meets most requirements, but lacks the robustness and some minor output refinements seen in the ground truth.",claude-sonnet-4,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create Decade column
df['Decade'] = (df['Year'] // 10 * 10).astype(str) + 's'

# Display first 10 movies with Title, Year, and Decade columns
print(df[['Title', 'Year', 'Decade']].head(10))
```",9,"The generated code correctly loads the dataset, creates the `Decade` column using the specified logic, and displays the required columns for the first 10 entries. It uses proper pandas imports and directly addresses all parts of the task. The strong points are its conciseness and correctness for the given task.

Minor improvements could include:
- Error handling for `FileNotFoundError` or missing columns (though not explicitly asked for, it's good practice).
- If the 'Year' column were not guaranteed to be numeric, `pd.to_numeric` with `errors='coerce'` followed by `dropna` would make it more robust, as seen in the ground truth. However, for a standard dataset like this, it's usually clean.
- The output format for printing the first 10 rows is slightly different (`to_string(index=False)` vs default pandas `head()`) but both are acceptable for ""display"".",claude-sonnet-4,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Impute missing values for numerical columns with mean
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numerical_cols:
    df[col].fillna(df[col].mean(), inplace=True)

# Impute missing values for sex column with mode
df['sex'].fillna(df['sex'].mode()[0], inplace=True)

# Group by species and sex, calculate average body_mass_g
result = df.groupby(['species'...",10,"The generated code correctly implements all requirements of the task. It loads the dataset, performs mean imputation for the specified numerical columns, mode imputation for the 'sex' column, groups the data by 'species' and 'sex', and finally calculates and displays the average 'body_mass_g' for each group. The code is concise, efficient, and directly addresses the problem statement. Imports are correct and the structure is logical.",claude-sonnet-4,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter data to include only Adelie and Gentoo species
filtered_df = df[df['species'].isin(['Adelie', 'Gentoo'])]

# Remove rows where flipper_length_mm or species is missing
clean_df = filtered_df.dropna(subset=['flipper_length_mm', 'species'])

# Separate flipper lengths by species
adelie_flippers = clean_df[clean_df['species'] == 'Adelie']['flipper_length_mm']
gentoo_flippers = clean_d...",9,"The generated code correctly performs all the steps required by the task: loading the data, filtering, handling NAs, performing the t-test, stating hypotheses, printing results, and interpreting them. The code is clean and well-structured.

**Strong points:**
*   Correctly loads the CSV and handles file existence.
*   Accurately filters for 'Adelie' and 'Gentoo' species.
*   Properly removes rows with NA in 'flipper_length_mm' or 'species' from the filtered subset.
*   Correctly performs an independent two-sample t-test using `scipy.stats.ttest_ind`.
*   Clearly states the null and alternative hypotheses.
*   Prints the t-statistic and p-value with appropriate formatting.
*   Correctly interprets the results based on the p-value and a significance level of alpha = 0.05.

**Minor areas for improvement (to achieve a perfect 10):**
*   The ground truth includes error handling for `FileNotFoundError` when loading the CSV, which is robustness that the generated code lacks. While not explicitly asked for, it's good practice.
*   The `ttest_ind` function has a default `equal_var=True` assumption. While not specified in the prompt, using `equal_var=False` (Welch's t-test) is often a safer and more robust default in real-world scenarios unless assumptions of equal variances are known to hold. The generated code uses the default `equal_var=True`. For this specific problem's requirements, not specifying `equal_var` is acceptable as the prompt didn't mandate it, but it's a point of consideration for a perfect solution.
*   The ground truth also includes checks for empty groups after filtering and NA removal, which adds robustness to prevent errors if the dataset for some reason doesn't yield enough data points for the t-test. This is a minor omission but good practice.",claude-sonnet-4,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv('penguins.csv')

# Select features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]
y = df['species']

# Drop rows with missing values
data = pd.concat([X, y], axis=1)
data = data.dropna()
X =...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task. It loads the dataset, correctly selects features and target, handles missing values (though in a slightly less direct way than the ground truth by concatenating first), encodes the target, splits the data with the specified parameters, trains the Logistic Regression model with `max_iter=1000`, and reports the accuracy.

The only minor point of difference, not necessarily an error, is in the handling of `dropna()`. The ground truth explicitly creates a `df_selected` and then applies `dropna()` to it, ensuring that only rows with NAs in the *relevant* columns (features + target) are dropped. The generated code concatenates `X` and `y` *before* dropping, which effectively achieves the same goal in this specific case, but it's a slightly less explicit approach for targeting NAs in the specified columns only if there were other columns in the original `df`. However, given the problem statement specified ""drop any rows where `species` or any of the four selected feature columns have missing values"", and the `X` and `y` are extracted before concatenation, it perfectly adheres to this requirement.

The code quality is good with proper imports and a clear, sequential flow. It doesn't include the error handling for `FileNotFoundError` or print statements for each step, which are present in the ground truth for robustness and clarity, but these were not explicitly mandatory requirements beyond the core task steps.

Overall, it's a very strong submission.",claude-sonnet-4,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows where body_mass_g or species is NA
df_clean = df.dropna(subset=['body_mass_g', 'species'])

# Calculate average body_mass_g for each species
avg_body_mass = df_clean.groupby('species')['body_mass_g'].mean()

# Create bar plot
plt.figure(figsize=(8, 6))
bars = plt.bar(avg_body_mass.index, avg_body_mass.values)
plt.title('Average Body Mass by Penguin...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, handles NAs in the specified columns, calculates the average body mass per species, and generates a bar plot with a title and labeled axes.

**Strong points:**
*   **Correctness:** All functional requirements are met accurately.
*   **Conciseness:** The code is straight to the point and efficient.
*   **Clarity:** The variable names are descriptive, and the code flow is easy to follow.

**Areas for minor improvement (to reach a perfect 10):**
*   **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code lacks this.
*   **Plotting Library Choice:** While `matplotlib.pyplot.bar` works, the task description mentioned `seaborn` as an option. Using `seaborn.barplot` often produces aesthetically pleasing plots with less manual configuration (e.g., automatic handling of categorical x-axis, better default styling). The ground truth code utilizes `seaborn.barplot`.
*   **Detail in Plot Labels:** The ground truth code has slightly more detailed axis labels (`Average Body Mass (g)` vs `Average Body Mass (g)`) and a more dynamic title generation, though the generated code's labels are perfectly acceptable.
*   **Readability of `ylabel`:** `Average Body Mass (g)` would be preferable over `Average Body Mass (g)` for the y-axis label to clarify the units.",claude-sonnet-4,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows where specified columns are NA
df = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])

# Create new feature culmen_area
df['culmen_area'] = df['culmen_length_mm'] * df['culmen_depth_mm']

# Select columns for correlation analysis
correlation_columns = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_lengt...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, handles NA values in the specified columns by dropping rows, creates the new `culmen_area` feature, calculates the Pearson correlation matrix, and visualizes it as an annotated heatmap with the correct title. The code structure is clear and concise.

Minor improvements could include:
- Adding error handling for `FileNotFoundError` when loading the CSV, similar to the ground truth. While not strictly required by the prompt, it's good practice for robust code.
- Explicitly specifying `method='pearson'` in the `.corr()` call, although 'pearson' is the default. This improves readability and explicitness.
- The `center=0` argument in `sns.heatmap` is redundant here because `cmap='coolwarm'` already handles the centering around zero for correlation matrices.
- The `figsize` chosen (`10, 8`) is good, but slight adjustments to text alignment (like `plt.xticks(rotation=45, ha='right')`) as seen in the ground truth can sometimes improve readability for larger matrices, though it's not strictly necessary for this size.

Overall, it's a very good solution that successfully accomplishes the task.",claude-sonnet-4,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('penguins.csv')

# Select features and target
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
X = df[features].copy()
y = df['species']

# Impute numerical features with mean
numerical_features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for feature in numerical_features:
    X[feature].fillna(...",9,"The generated code largely fulfills all the requirements of the task.

**Strong Points:**
- **Correctness:** It successfully loads the data, selects the specified features, correctly imputes numerical NA values with the mean, imputes and maps the 'sex' column as required, and applies one-hot encoding to the 'island' column.
- **Completeness:** All steps outlined in the prompt are addressed.
- **Code Quality:** The code is clean, readable, and uses appropriate pandas functions (`fillna`, `map`, `get_dummies` or `OneHotEncoder`). It separates features (X) and target (y) clearly.

**Areas for Improvement (Minor):**
1.  **Implicit Assumption of 'species' presence:** While the prompt asks for `species` as the target, the generated code creates `y = df['species']` directly without explicit error handling if `species` were missing from the original `df`. The ground truth code handles this by explicitly listing `[target_variable]` in the `df_processed` creation. This is a minor point as the context implies `species` will always be there, but robust code would consider it.
2.  **`OneHotEncoder` usage for `island`:** While `OneHotEncoder` *can* be used, `pd.get_dummies` is often simpler and more idiomatic for one-hot encoding categorical columns within a DataFrame in pandas, especially when the task description doesn't explicitly require `sklearn` encoders. The ground truth uses `pd.get_dummies` which is slightly more concise for this specific task. The `drop='first'` argument in `OneHotEncoder` is good practice to avoid multicollinearity, but the prompt didn't explicitly ask for it; `pd.get_dummies` without `drop_first=True` would also be acceptable given the prompt.
3.  **Missing `numpy` import:** Although `numpy` isn't directly called, `pandas` often relies on it internally, and best practice sometimes includes importing it if complex numerical operations or NaN handling are explicitly performed (though not strictly necessary here).

Overall, it's a very good solution that successfully addresses the task requirements. The use of `OneHotEncoder` is a valid approach, just slightly more verbose than `pd.get_dummies` for this specific case.",claude-sonnet-4,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

# Create the DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107], 
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 
        'Quantity': [1, 2, 1, 1, 3, 1, 1], 
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150], 
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', ...",10,"The generated code correctly performs all the required tasks:
1. It correctly creates the DataFrame.
2. It correctly calculates the total spending for each 'CustomerID' by first creating a 'Total_Spent' column and then grouping by 'CustomerID' and summing.
3. It correctly identifies and prints the top 3 customers by total spending, sorted in descending order.
4. It correctly calculates and prints the total quantity of items sold for each 'Category'.
The code is well-structured, uses proper pandas operations, and includes necessary imports. The output format for printing top customers and category quantities is clear and provides the requested information.",claude-sonnet-4,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc, classification_report
from sklearn.impute import SimpleImputer
import seaborn as sns

# Load the dat...",7,"The generated code addresses most of the requirements but has several areas for improvement, particularly regarding best practices in preprocessing and evaluation.

**Strengths:**
*   Successfully loads data and performs basic missing value imputation.
*   Correctly splits data into training and testing sets with stratification.
*   Trains a Logistic Regression model.
*   Evaluates the model using all requested metrics (accuracy, precision, recall, F1-score, ROC-AUC).
*   Visualizes confusion matrix and ROC curve.
*   Calculates and displays feature coefficients.
*   Performs cross-validation for model stability.
*   Includes basic feature engineering (`FamilySize`, `IsAlone`).

**Areas for Improvement:**
*   **Preprocessing with Pipelines (Major Issue):** The ground truth uses `ColumnTransformer` and `Pipeline` for preprocessing (scaling and encoding). The generated code performs these steps manually after the train-test split `X_train_scaled = scaler.fit_transform(X_train)`, `X_test_scaled = scaler.transform(X_test)`. This is problematic for cross-validation where separate scaling should happen on each fold. The generated code scales `X_train` and `X_test` *once* directly, but then for cross-validation, it calls `cross_val_score(log_reg, X_train_scaled, y_train, ...)`, which means the `log_reg` model is receiving *already scaled* data. A `Pipeline` is essential here to correctly apply preprocessing within each cross-validation fold and to ensure proper separation of concerns. The ground truth handles this perfectly.
*   **Categorical Encoding (Major Issue):** The generated code uses `LabelEncoder` for 'Sex' and 'Embarked'. `LabelEncoder` assigns ordinal values (0, 1, 2...), which implies an ordered relationship that doesn't exist for these nominal categorical variables. This can negatively impact the Logistic Regression model, which assumes a linear relationship. `OneHotEncoder` (as used in the ground truth) is the correct approach for nominal categorical variables.
*   **Feature Interpretation:** The interpretation of coefficients `np.exp(row['Coefficient']):.3f} odds ratio` is only valid for one-hot encoded features relative to a baseline, or for continuous features (interpreting as a 1-unit change). Since `LabelEncoder` was used, and a mix of continuous and ordinal features are present, this interpretation may not be entirely accurate or clear, especially for 'Sex' and 'Embarked'. The ground truth's interpretation is more general and accurate given proper one-hot encoding.
*   **Cross-Validation Implementation:** Calling `cross_val_score` for each metric individually is redundant and less efficient than using `cross_validate` as in the ground truth, which can calculate multiple metrics in one pass. Also, the cross-validation is performed on `X_train_scaled` and `y_train` which is *part* of the data, not the full `X`, `y` dataset, making it less representative of overall model performance stability across the entire dataset.
*   **Visualization:** The generated code puts all plots in one figure using `subplot`, which can make them smaller and harder to read. Separate figures might be clearer, as in the ground truth.
*   **Missing Value Handling:** While it handles Age and Embarked, it also imputer Fare but the Ground Truth doesn't. Although, Fare has like 1 missing value so it doesn't hurt. Also, it drops 'Cabin' which is correct but the ground truth points out why.
*   **Random State:** The ground truth sets `np.random.seed(42)` at the very beginning for full reproducibility across all random operations, which is good practice. The generated code only sets it for `train_test_split` and `LogisticRegression`.

By addressing the pipeline-based preprocessing and correct categorical encoding, the code would be much more robust and aligned with standard data science practices.",claude-sonnet-4,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']), 
        'Ticker': ['TECHCO']*8, 
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0], 
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5], 
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0], 
        'Close': [152.0, 151.0, 153.5, 155...",9,"The generated code correctly creates the DataFrame, sets the index, calculates the 3-day SMA for 'Close' price, and calculates the daily percentage change for 'Volume', handling NaNs appropriately as required. All imports are correct and the code is well-structured.

The only minor discrepancy is in the 'Volume_Pct_Change' calculation where the generated code multiplies by 100 (`df['Volume'].pct_change() * 100`). While this presents the percentage as a whole number (e.g., 20.0 instead of 0.20), the `pct_change()` method itself returns a float representing the fractional change, which is typically how it's used in data analysis contexts unless explicitly asked for a whole number percentage. The prompt asked for ""percentage change"", which usually implies the fractional value. The ground truth provides the fractional value, which is generally more common and flexible for further calculations. This is a stylistic choice more than a functional error given the prompt's ambiguous phrasing of ""percentage change"", but the fractional representation is arguably more standard.",claude-sonnet-4,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'], 
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'], 
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91], 
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89], 
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]}...",9,"The generated code correctly addresses all parts of the task. It properly creates the DataFrame, calculates the 'Score_Improvement' column, groups by subject to calculate the average improvement, and filters students based on the specified criteria.

**Strong points:**
*   **Correctness:** All calculations and filtering operations are accurate.
*   **Completeness:** All three requirements of the task are met.
*   **Readability:** The code is clear and easy to understand.
*   **Conciseness:** It achieves the task efficiently without extraneous code.
*   **Output formatting:** The printing of average improvement is well-formatted with two decimal places.

**Areas for minor improvement:**
*   The printing of the high-performing students uses `.tolist()`, which is functional and acceptable, but the ground truth's loop with an `if not empty` check provides slightly more robust and user-friendly output if no students meet the criteria (it would print ""No students met both criteria"" instead of an empty list `[]`). However, given the prompt only asked to ""print the 'StudentID'"", `tolist()` implicitly handles cases where the result is an empty list, which is still correct.
*   The ground truth also included print statements for the intermediate steps of DataFrame creation and column addition, which wasn't strictly required by the prompt but can be good practice for debugging/verification. For this specific task's output requirements, the generated code's approach is perfectly fine.",claude-sonnet-4,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

filtered_df = df[(df[""Department""] == ""Tech"") & (df[""Perf...",9,"The generated code correctly performs all the required operations: defining the DataFrame, filtering it based on department and performance score, and calculating the average projects completed for the filtered group. The code is concise and demonstrates a good understanding of pandas operations.

The only minor point for improvement, which prevents a perfect score, is the lack of explicit handling for the edge case where `filtered_df` might be empty. While `mean()` on an empty DataFrame returns `NaN`, in a production setting, it's often good practice to add a check (like in the ground truth code) to provide a more informative message if no data meets the criteria, rather than just printing `NaN`. However, given the specific task, the current output of `NaN` is technically correct for that scenario. The output format also differs slightly from the ground truth (printing a raw number vs. a formatted string), but the core calculation is correct.",claude-sonnet-4,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

desktop_conversion_rate = (df[df[""DeviceType""] == ""Desktop""][""Converted""].sum() / len(df[df[""DeviceType""] == ""Desktop""])) * 100...",9,"The generated code correctly defines the DataFrame and accurately calculates and prints both conversion rates. The use of `.sum()` on the boolean 'Converted' series implicitly counts `True` values, which is an effective approach, and dividing by `len()` of the filtered DataFrame gives the correct proportion.

The code is concise and efficient.

However, it could be slightly improved by:
1.  **Readability**: While functional, the long lines for calculating conversion rates could be broken down or split into intermediate variables for better readability, similar to the ground truth which defines `desktop_users_df` and `mobile_users_df`.
2.  **Robustness**: The generated code does not handle the edge case where there might be no 'Desktop' or 'Mobile' users, which would lead to a `ZeroDivisionError` if `len()` evaluates to 0. The ground truth code includes checks for `df.empty` to prevent this. While not strictly necessary for *this specific dataset*, it's a good practice for more robust production code.

Overall, it's a very good solution that fulfills the requirements.",claude-sonnet-4,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

avg_air_quality = df.groupby(""SensorID"")[""AirQualityIndex""].mean()

for sensor_...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the average ""AirQualityIndex"" for each ""SensorID"", and prints the output in a clear and readable format. The code adheres to all requirements of the task. It's concise, efficient, and demonstrates a good understanding of pandas operations for grouping and aggregation.",claude-sonnet-4,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

df[""EngagementScore""] = df[""Likes""] + (df[""Shares""] * 2) + (df[""Comments""] * 3)

insta...",9,"The generated code correctly defines the DataFrame, calculates the ""EngagementScore"", filters for ""Instagram"" posts, and accurately calculates and prints their average ""EngagementScore"". The code is concise and directly addresses all requirements of the task.

One minor area for improvement would be to format the output of the average engagement score to two decimal places, similar to the ground truth. While not strictly a correctness issue, it's good practice for presenting numerical results. Additionally, consider adding a check for an empty `instagram_posts` DataFrame before calculating `.mean()` to prevent potential errors if no Instagram posts were present, though for this specific dataset it's not strictly necessary.",claude-sonnet-4,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",9,"The generated code correctly performs all the required tasks:
1. It defines the DataFrame accurately, including the correct conversion of date columns.
2. It calculates the ""LengthOfStay"" column correctly using `dt.days`.
3. It filters the DataFrame to include only ""Flu"" patients.
4. It calculates and prints the average ""LengthOfStay"" for these ""Flu"" patients.

The code is concise, imports necessary libraries, and follows a logical flow.

**Areas for minor improvement (to achieve a perfect 10):**
- **Readability of output:** The ground truth code includes f-string formatting (`print(f""\nAverage Length of Stay for Flu patients: {average_length_of_stay_flu:.2f} days"")`) which makes the output more user-friendly and clear about what is being printed. The generated code just prints the raw number, which is functionally correct but less informative.
- **Comments:** While the ground truth code includes extensive comments explaining each step (likely for instructional purposes), the generated code is self-explanatory enough for this simple task. However, adding a small comment before the `print(average_stay)` line could improve clarity for new users reading the code.
- **Copying filtered DataFrame:** The ground truth uses `.copy()` after filtering (`df[df[""Diagnosis""] == ""Flu""].copy()`) to explicitly create a new DataFrame and avoid a `SettingWithCopyWarning` if subsequent modifications were to be made to `flu_patients_df`. While not strictly necessary for this specific problem (as `flu_patients` is only used for calculating the mean and not modified), it's a good practice in pandas for filtered DataFrames.",claude-sonnet-4,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}
df = pd.DataFrame(data)

sci_fi_movies = df[df[""Genre...",9,"The generated code correctly performs all the required tasks: defining the DataFrame, filtering for Sci-Fi movies, and calculating their average rating. The code is concise and efficient.

The only minor area for improvement is in the output format. While `print(average_rating)` is correct, `print(f""The average rating for Sci-Fi movies is: {average_rating:.2f}"")` as seen in the ground truth provides a more user-friendly and formatted output, which is generally good practice for reporting results in data analysis. Additionally, it could be slightly more robust by including an explicit check for an empty filtered DataFrame before calculating the mean, although for this specific dataset and task, it's not strictly necessary as Sci-Fi movies are guaranteed to exist.",claude-sonnet-4,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

toyota_cars = df[df[""Brand""] == ""Toyota""]
average_mileage = toyota_ca...",9,"The generated code correctly defines the DataFrame, filters for Toyota cars, and accurately calculates their average mileage. It adheres to all requirements of the task. The code is concise and directly addresses the problem.

One minor point for improvement, which prevents a perfect 10, is that the output format of the average mileage is just the number itself (`42500.0`). The ground truth uses an f-string to provide more descriptive output, e.g., ""The average Mileage for Toyota cars is: 22500.00"". While not strictly asked for a specific print format, a more user-friendly output is generally preferred in practical scenarios. However, for a pure correctness and functionality evaluation, it's very good.",claude-sonnet-4,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 80...",9,"The generated code correctly performs all the required tasks: it defines the DataFrame accurately, filters for Fire-type Pokemon, and calculates their average Attack stat successfully. The code is concise and directly addresses the problem statement.

The only minor area for improvement, which prevents a perfect score, is the lack of a user-friendly print statement for the average attack. While `print(average_attack)` correctly outputs the value, the ground truth's `print(f""The average Attack stat for Fire-type Pokemon is: {average_fire_attack:.2f}"")` provides better context and formatting, which is generally good practice for outputting results in a user-facing way. Additionally, the ground truth includes an edge case check for an empty filtered DataFrame, which is robust but not strictly required by the prompt's data. Given the provided data, the `fire_pokemon` DataFrame will never be empty, so the absence of that check doesn't lead to an error in this specific scenario.",claude-sonnet-4,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.offline as pyo
import numpy as np
from matplotlib.patches import Rectangle
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
df = pd.read_csv('titanic.csv')

# Data preprocessing
df['Age_Group'] = pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100], 
                        labels=...",9,"The generated code successfully addresses all requirements of the task. It creates sophisticated visualizations, uses appropriate chart types, and combines plots effectively for storytelling.

**Strengths:**
- **Comprehensive Visualizations:** All requested visualization types (multi-panel, heatmap, violin plots, interactive scatter, dashboard) are implemented.
- **Good Use of Libraries:** Effectively uses matplotlib, seaborn, and plotly for different visualization needs, including interactive plots.
- **Detailed Preprocessing:** Includes relevant data preprocessing steps like creating age groups and mapping categorical values to more descriptive labels, which enhances readability and plotting.
- **Dashboard Design (Matplotlib):** The Matplotlib-based dashboard is well-structured using `gridspec`, includes descriptive titles, axes labels, and a dedicated section for key insights, demonstrating good storytelling. It also adds a summary table and correlation matrix, exceeding expectations slightly.
- **Interactive Plotly Dashboard:** The inclusion of a second Plotly-based interactive dashboard is a significant added value, showcasing advanced capabilities and user interaction.
- **Code Clarity and Readability:** The code is well-commented, and variable names are descriptive.
- **Error Handling (Implicit):** While not explicit with try-except for file loading, missing values for 'Age' are handled (though for 'Fare' it's implied by `dropna` for interactive plots, median fill would be better for consistency).

**Areas for Minor Improvement:**
- **Missing Value Handling:** The ground truth explicitly fills `Age` with median and `Fare` with median (though `Fare` might not have many missing values for Titanic data `df.isnull().sum()` often shows 2 missing for 'Embarked' and usually 1 for 'Fare' before 1899 datasets). The generated code handles `Age` `dropna` for some visualizations, which is acceptable but median imputation is generally robust.
- **Consistency in Data Preparation:** Some derived columns like `Age_Group` and `Survived_Label` are created at the beginning, but then `dropna` is used for `Age` directly in violin plots instead of relying on the imputed `Age` or `Age_Group` which could sometimes lead to slight inconsistencies if not managed carefully.
- **Heatmap Masking:** The generated heatmap correctly uses `unstack()` and `mask=survival_heatmap.isnull()` which is good, but the Ground Truth's `pivot_table` automatically handles structure and ensures all combinations are present if `aggfunc` is used.
- **Dashboard Insights Placement:** While good to have, the text insights in the Matplotlib dashboard are placed at the very bottom and are quite small. This is a minor aesthetic point, but larger text or a dedicated subplot for key takeaways might enhance readability.
- **Redundant Plotly Import:** `plotly.offline as pyo` is imported but not used.
- **Seaborn Style Setting:** `plt.style.use('seaborn-v0_8')` and `sns.set_palette(""husl"")` are fine, but `sns.set_style(""whitegrid"")` from the ground truth can often provide a cleaner aesthetic for scientific plots.

Overall, the generated code is excellent and provides a robust solution to the prompt. The inclusion of two dashboard styles (Matplotlib static and Plotly interactive) is particularly impressive.",claude-sonnet-4,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, 8...",9,"The generated code correctly defines the DataFrame, filters for the ""Dystopian"" genre, and accurately calculates the average stock for that genre. It addresses all core requirements of the task. The code is concise and directly implements the requested operations.

Minor points for improvement (not critical enough to significantly deduct from the score but worth noting for perfect robustness):

1.  **Output Format for Average Stock**: The ground truth uses `f""{average_dystopian_stock:.2f}""` to format the average stock to two decimal places. The generated code uses `f""{average_stock}""`, which might display more decimal places than typically desired for monetary or stock values. While not explicitly asked for, it's good practice.
2.  **Explicit Verification Prints**: The ground truth includes `print` statements for the initial and filtered DataFrames, which are very helpful for debugging and verifying intermediate steps during development. The generated code only prints the `dystopian_books` DataFrame and the final average.
3.  **Edge Case Handling (Empty DataFrame)**: The ground truth includes an `if not dystopian_books_df.empty:` check to handle the scenario where no Dystopian books are found, preventing potential issues or providing a more informative message. The submitted code would still calculate `NaN` if `dystopian_books` was empty, which is technically correct but less user-friendly. Given the specific dataset, this isn't an issue here, but for a general solution, it's a good practice.

Overall, it's a very good and functional solution.",claude-sonnet-4,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Remove rows with missing values for analysis
df_clean = df.dropna()

# Separate potable and non-potable samples
potable = df_clean[df_clean['Potability'] == 1]
non_potable = df_clean[df_clean['Potability'] == 0]

# Define numerical features for analysis
features = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sul...",7,"The generated code correctly performs most of the required statistical tests (t-tests, chi-square, Shapiro-Wilk) and calculates Cohen's d. It also includes good output formatting and a summary.

However, there are critical issues that prevent a higher score:
1.  **Missing Value Handling (Critical Flaw):** Instead of imputing missing values (e.g., with the mean as suggested in the ground truth and standard practice for these types of analyses), the code uses `df.dropna()`. This drastically reduces the dataset size and can lead to biased results, especially if missingness is not completely random. In this `water_potability.csv` dataset, 'ph', 'Sulfate', and 'Trihalomethanes' have a significant number of missing values (around 10-15%). Dropping these rows removes ~20% of the data, which is a major methodological error for statistical analysis.
2.  **Cohen's d Calculation (Potential Error):** The Cohen's d formula uses `pooled_std = np.sqrt(((len(potable_values) - 1) * potable_values.var() + (len(non_potable_values) - 1) * non_potable_values.var()) / (len(potable_values) + len(non_potable_values) - 2))`. This formula for pooled standard deviation is for *pooled variance* when *equal variances are assumed*. However, the t-test performed is `equal_var=False` (Welch's t-test), which does *not* assume equal variances. For Cohen's d with Welch's t-test, a common approach is to use the square root of the average of the variances, or a slightly more complex pooled standard deviation that accounts for unequal variances. The current calculation is inconsistent with the `equal_var=False` parameter of the t-test, potentially leading to inaccurate effect sizes.
3.  **Shapiro-Wilk Sample Size Limitation:** The code correctly acknowledges the Shapiro-Wilk limitation by sampling `min(5000, len(df_clean[feature].dropna()))`. This is a good pragmatic solution.
4.  **Additional Analysis:** The code includes ""Correlation analysis for potability"" (point-biserial correlation), which was not explicitly requested in the prompt. While not incorrect, it adds extra code beyond the scope of the prompt.
5.  **Hardcoded Features:** The `features` list is hardcoded. While it covers most numerical features, a dynamic approach (e.g., `df.select_dtypes(include=np.number).columns.drop('Potability')`) would be more robust to changes in the dataset.

To improve, the code should:
*   Implement proper missing value imputation (e.g., mean imputation) instead of dropping rows.
*   Review and correct the Cohen's d calculation to align with the `equal_var=False` assumption of the t-test.
*   Dynamically select numerical features for analysis.",claude-sonnet-4,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, roc_cu...",7,"The generated code addresses most of the requirements but has several key issues that prevent it from scoring higher.

**Strengths:**
- **Data Preprocessing:** Correctly handles missing values with `SimpleImputer` and performs feature scaling with `StandardScaler` after train-test split. However, the imputation strategy is mean instead of median (as in ground truth).
- **Multiple Models:** Implements Logistic Regression, Random Forest, and SVM as required.
- **Comprehensive Metrics:** Calculates and displays accuracy, precision, recall, F1-score, and ROC-AUC.
- **Visualizations:** Generates confusion matrices and ROC curves as requested.
- **K-fold Cross-Validation:** Performs k-fold cross-validation.
- **Best Model Selection:** Selects the best performing model based on ROC-AUC (ground truth used F1-score from CV, which is a better approach given the potential class imbalance).
- **Feature Importance:** Attempts to show feature importance for Random Forest and Logistic Regression.

**Areas for Improvement/Weaknesses:**

1.  **Data Leakage during Training/Scaling:**
    *   The `model.fit(X_train, y_train)` for Random Forest, and `model.fit(X_train_scaled, y_train)` for LR/SVM, suggests inconsistent scaling. Random Forest is a tree-based model that often doesn't *require* scaling, but if scaling is performed for others, it should be applied consistently or explicitly chosen not to be applied. More critically, the problem arises because `X_imputed` is created *before* the train-test split, leading to data leakage in the imputation step. The imputer should be fit *only* on the training data and then used to transform both train and test sets. The ground truth correctly handles this by applying imputation after train-test split within the pipeline for CV.
    *   Similarly, `StandardScaler` should be fit only on `X_train_imputed`. The generated code does `X_imputed` before `train_test_split`, then `scaler.fit_transform(X_train)` and `scaler.transform(X_test)`. While the scaling part is correct for train/test *after* the split, the `X_imputed` step is problematic.

2.  **Cross-Validation Implementation:**
    *   The cross-validation is performed only on `X_train_scaled`/`X_train` and not the full dataset `X_imputed`/`y`. Best practice for k-fold CV is to perform it on the *entire dataset* while ensuring that preprocessing steps (like imputation and scaling) are contained within a `Pipeline` inside the cross-validation loop to prevent data leakage. The generated code calculates `cross_val_score` directly on `X_train_scaled` or `X_train`, meaning it's only evaluating a subset of the data and not performing proper nested CV or full dataset CV with a pipeline. The ground truth correctly addresses this by creating a `Pipeline` for CV.
    *   The cross-validation only reports accuracy, not all required metrics (precision, recall, F1-score, ROC-AUC) as requested by the prompt for CV.

3.  **Missing Value Strategy:** The generated code uses `strategy='mean'` for `SimpleImputer`, whereas the ground truth uses `strategy='median'`. While both are valid, the prompt didn't specify, but median is often preferred for skewed data to be less impacted by outliers.

4.  **Minor Visualization Issues:**
    *   The ROC curves don't use `RocCurveDisplay.from_estimator`. While `roc_curve` and `auc` manually is fine, `RocCurveDisplay` is a cleaner way to achieve similar results and is often preferred in modern `sklearn`.
    *   The `ConfusionMatrixDisplay` (or `sns.heatmap` as used in GD) is not explicitly imported for the ground truth, but the ground truth uses `sns.heatmap`. The generated code correctly uses `sns.heatmap`.

5.  **Robustness (FileNotFound):** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes.

6.  **Feature Importance for SVM:** The generated code does not explicitly state that feature importance for SVM is not directly available, unlike the ground truth which explains it.

Overall, the code provides a functional solution but falls short on good MLOps practices, particularly concerning data leakage in preprocessing and robust cross-validation. The ground truth's use of a `Pipeline` for cross-validation on the full dataset is a critical differentiator for a high-quality solution.",claude-sonnet-4,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

# Load dataset
df = pd.read_csv('water_potability.csv')

# Handle...",9,"The generated code is highly comprehensive and addresses almost all requirements of the task.

**Strengths:**
- **Excellent Feature Engineering:** Goes beyond the basic requirements to create a wide array of meaningful features including ratios, interaction terms, polynomial features (squared/cubed), logarithmic, square root, and statistical aggregates (mean, std, max, min). This demonstrates a deep understanding of feature engineering.
- **Robust Missing Value Handling:** Correctly identifies and uses `SimpleImputer` for handling missing values, which is good practice.
- **Comprehensive Correlation Analysis:** Correctly calculates the correlation matrix and identifies the top 5 correlated features with Potability.
- **Multiple Feature Selection Methods:** Successfully implements both `SelectKBest` (F-classif) and `RFE` (with RandomForestClassifier), as requested. It also implicitly uses Random Forest's feature importance which is a good addition.
- **Advanced Visualizations:** Creates multiple informative plots as requested:
    - Correlation Heatmap (though limited to top features in the first subplot, a full heatmap is desired as per the prompt, which the ground truth provides).
    - Feature Importance Plot (from Random Forest).
    - SelectKBest scores plot.
    - Pairwise scatter plots for top correlated features.
    - **Excellent addition:** Overlap matrix, feature frequency, correlation distribution, and importance vs. correlation scatter plot are highly insightful and go beyond the prompt, adding significant value.
- **Clear Comparison of Feature Sets:** Provides a detailed comparison of selected features from different methods, including overlaps and unique features, which is excellent.
- **Good Code Structure and Readability:** The code is well-organized with comments and logical flow.
- **Includes `train_test_split`:** While not strictly needed for feature selection itself, it's good practice for subsequent model training and shows foresight.

**Areas for Improvement (Minor):**
- **Full Correlation Heatmap:** The prompt specifically asks for a correlation heatmap for *all features* (original + engineered). The generated code's first subplot shows a heatmap only for `top_features_for_heatmap`, which is a subset. While other visualizations are excellent, this misses a direct output requested. The ground truth provides a single large full heatmap.
- **Redundant `train_test_split` for Feature Selection:** While good practice for modeling, feature selection inherently happens on the full dataset (or X_train *if* the goal is to prevent data leakage for model training, but for just feature selection and visualization, the full dataset or X_train_scaled is fine). The current split means that `correlation_matrix` and `Potability_correlations` are calculated on `df_engineered` (which is the full dataset), but then `SelectKBest` and `RFE` are applied to `X_train_scaled`. This is not necessarily wrong, but for *identifying* general top features for exploration, applying it to the full dataset (after scaling) might be more straightforward as implied by the prompt (which just asks for *selection* not selection *for training*). The ground truth processes the full dataset `X_fe` for feature selection to meet the prompt's simplicity.
- **Pairwise Scatter Plot Implementation:** The current pairwise scatter plot tries to plot `feature` vs `Potability` separately using jitter, which is more of a univariate distribution view, rather than `feature1` vs `feature2` with `hue=Potability` as typical for `sns.pairplot`. The prompt implicitly asks for the latter, as 'pairwise scatter plots' usually refers to relationships between *features*. The ground truth uses `sns.pairplot` correctly for this.

Overall, the code is very strong, demonstrates deep understanding, and provides exceptionally good insights beyond the core requirements. The minor points are mostly about adhering strictly to the visualizations as traditionally interpreted or optimizing for the direct prompt.",claude-sonnet-4,LLM_judge_config
