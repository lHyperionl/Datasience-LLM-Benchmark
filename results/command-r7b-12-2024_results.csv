task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1. List all column names and identify data types
column_names = df.columns.tolist()
data_types = df.dtypes.apply(lambda x: 'Numerical' if pd.api.types.is_numeric_dtype(x) else 'Categorical').to_dict()
print(""Column Names:"", column_names)
print(""Data Types:"", data_types)

# 2. Numerical Columns Descriptive Statistics
numerical_cols = [col for col, dtype in data_types.items() if dtype == 'Numerical']
for c...",8,"The generated code correctly performs all the requested tasks: loading the CSV, identifying column types, calculating descriptive statistics for numerical columns, reporting dataset dimensions, and identifying categorical/ordinal columns.

**Strengths:**
- Correctly uses `pandas.read_csv` to load the data.
- Accurately identifies numerical and categorical data types using `pd.api.types.is_numeric_dtype`.
- Uses `df.describe()` for numerical columns, which efficiently provides all requested statistics (mean, median (50% quartile), std, min, max, quartiles).
- Correctly reports the total number of rows and columns.
- The use of list comprehensions for identifying column types and `df.describe()` for statistics is concise and pythonic.

**Areas for Improvement (to reach a perfect score):**
- **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError`, which is good practice for robust code. The generated code assumes the file will always be present, which could lead to a crash if it isn't.
- **Clarity of Output:** While the information is present, the ground truth's print statements are more descriptive (e.g., ""--- Column Names and Identified Data Types ---"", ""Successfully loaded..."") and use f-strings for better readability of the output, making it easier for a user to understand the different sections of the report. For example, instead of just `print(""Data Types:"", data_types)`, a more descriptive output for each column would be better, similar to the ground truth.
- **Categorical/Ordinal Nuance:** In the context of `winequality-red.csv`, all columns are numerically stored. The `quality` column is often treated as ordinal conceptually, but Pandas will identify it as numerical. The ground truth provides a helpful note regarding this specific dataset and the 'quality' column's conceptual ordinality, which adds valuable context. The generated code's current approach to categorizing based purely on `is_numeric_dtype` would correctly identify all columns as numerical for this specific dataset and thus report an empty list for `categorical_cols`, which is accurate based on Pandas dtypes but might be misleading for a human expecting 'quality' to be mentioned. However, the task asked for ""Identify any columns that appear to be categorical or ordinal"" and relying on dtype for ""appear to be"" is a reasonable interpretation.",command-r7b-12-2024,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
df = pd.read_csv('video_games_sales.csv')
print(df.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(df.shape)

# 3) List all column names and their data types
print(df.dtypes)

# 4) Display the top 10 games by Global_Sales
print(df.sort_values('Global_Sales', ascending=False).head(10))

# 5) Show the unique values in the Platform and Genre columns
print(df['Platform'].unique())
print(df['Genr...",7,"The generated code correctly performs most of the requested EDA tasks. It successfully loads the dataset, displays the head, shows the shape, lists column types, identifies top games by global sales, displays unique platform and genre values, calculates sales statistics, and finds missing values.

However, there are a few areas for improvement:

1.  **Robustness**: The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. This is crucial for a robust script.
2.  **Output Clarity**: The ground truth uses `print` statements with descriptive headers for each section, making the output much clearer and easier to understand. The generated code simply prints the output without much context, making it less user-friendly. For example, it doesn't explicitly state ""Top 10 Games by Global Sales"" before printing the DataFrame.
3.  **Specific Column Display for Top Games**: While the generated code correctly sorts and displays the top 10 rows, the ground truth specifically selects `[['Name', 'Platform', 'Genre', 'Publisher', 'Global_Sales']]` for display, which is a better practice for presenting concise results.
4.  **`df.info()` vs `df.dtypes`**: For listing column names and data types, `df.info()` as used in the ground truth often provides more comprehensive information (including non-null counts and memory usage), which is generally preferred in EDA over just `df.dtypes`.
5.  **Missing Value Clarity**: While `df.isnull().sum()` is correct, the ground truth specifically filters to show only columns with missing values and adds clarifying sentences, which improves the output's readability when there are many columns with no missing data.
6.  **Redundancy**: The ground truth adds conditional checks (e.g., `if 'Global_Sales' in df.columns:`) which are good practice in larger scripts, but for a standard EDA scenario where these columns are expected, it adds verbosity. The generated code is more concise in this aspect.

Overall, the generated code fulfills the technical requirements of the task but could benefit from improved output presentation, robustness, and slight refinements in the choice of Pandas methods for better EDA insights.",command-r7b-12-2024,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd

# 1) Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 2) Handle missing values in the Year column by filling them with the median year
median_year = df['Year'].median()
df['Year'].fillna(median_year, inplace=True)

# 3) Remove games with missing Publisher information
df.dropna(subset=['Publisher'], inplace=True)

# 4) Create a new column 'Years_Since_Release' that calculates how many years have passed since each game's release year
df['Years_Since_Rele...",8,"The generated code correctly addresses most of the requirements. It loads the dataset, handles missing 'Year' values using the median, removes entries with missing 'Publisher' information, creates the 'Years_Since_Release' column, removes duplicate games, filters out low-selling games, and displays the final shape and head of the DataFrame.

Areas for improvement to achieve a higher score:
1.  **Robustness for 'Year' column**: The ground truth code includes `pd.to_numeric(df['Year'], errors='coerce')` before calculating the median and filling NaNs. This is crucial as the 'Year' column might contain non-numeric strings, which would cause an error when `median()` or arithmetic operations are performed if not handled. The generated code assumes 'Year' is already numeric or can be implicitly coerced, which might not always be true.
2.  **Robustness for 'Global_Sales'**: Similar to 'Year', `Global_Sales` might also contain non-numeric data. The ground truth explicitly converts it to numeric and handles potential NaNs introduced during conversion. The generated code assumes 'Global_Sales' is numeric, which could lead to errors if it contains strings.
3.  **Data Type Conversion for 'Year'**: After filling missing 'Year' values, the ground truth explicitly converts the 'Year' column to `int` using `df['Year'].astype(int)`. This is good practice as median calculation might result in a float, and years are typically integers. The generated code misses this conversion.
4.  **Edge Case for 'Years_Since_Release'**: The ground truth includes `df['Years_Since_Release'] = df['Years_Since_Release'].apply(lambda x: max(0, x))` to ensure the calculated years are non-negative. This is good practice, especially if there are future release dates or incorrect year entries. The generated code does not include this.
5.  **Output Information**: The ground truth provides print statements at various stages to show the shape of the DataFrame after each major cleaning step, which is very helpful for debugging and understanding the data cleaning process. The generated code only shows the final shape.
6.  **Error Handling**: The ground truth includes a `try-except` block to handle potential `FileNotFoundError` or other unexpected errors, which adds robustness. The generated code lacks this.",command-r7b-12-2024,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 2) Create a bar chart showing the top 10 gaming platforms by total global sales
plt.figure(figsize=(10, 6))
top_platforms = df.groupby('Platform')['Global_Sales'].sum().reset_index()
top_platforms.sort_values(by='Global_Sales', ascending=False, inplace=True)
top_10_platforms = top_platforms.head(10)
plt.bar(top_10_platforms['Platform'], top_10_platforms['Global_Sales'...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being a robust and entirely correct solution.

**Positive Aspects:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Visualization Types:** All requested visualization types (bar, pie, histogram, scatter, line) are attempted.
*   **Basic Labels/Titles:** Most plots have basic titles and labels.

**Areas for Improvement/Corrections:**
1.  **Data Loading Robustness:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice. The generated code lacks this.
2.  **Data Cleaning:** This is a major missing piece.
    *   `Year_of_Release` (or `Release_Year` as used in the generated code) is likely not clean. The ground truth explicitly converts it to numeric and handles NaNs. The generated code implicitly assumes it's clean and directly tries to use `release_years.value_counts()`. This can lead to errors or incorrect plots if the column contains non-numeric values or NaNs in the index.
    *   Sales columns (`NA_Sales`, `EU_Sales`, `Global_Sales`) also need to be ensured as numeric, as done in the ground truth. This is critical for aggregations, histograms, and scatter plots.
    *   The scatter plot for `NA_Sales` vs `EU_Sales` includes a strange `plt.scatter(df.loc[df['NA_Sales'].isnull(), 'EU_Sales'], ...)` line. This attempts to plot points where `NA_Sales` is null, which is counterintuitive for showing a relationship between *both* sales figures, and also assumes `NA_Sales` could be null, which points back to the need for data cleaning. If there are NaNs, they should be handled (e.g., dropped) before plotting relationships between them.

3.  **Bar Chart:**
    *   The ground truth explicitly uses `.nlargest(10)` which is more direct for getting the top 10 after grouping. The generated code chains `reset_index()`, `sort_values()`, and `head(10)`, which is functionally correct but less concise.

4.  **Pie Chart:**
    *   The generated code only shows `head(5)` genres. The task asks for ""distribution of games across different genres,"" which implies all genres or a more thoughtful handling of ""Other"" categories for readability, as demonstrated in the ground truth. Restricting to just 5 might omit significant portions of the data.

5.  **Histogram:**
    *   The range of `Global_Sales` can be very skewed. The ground truth considers limiting the range (e.g., to the 99th percentile) or using a log scale for better visualization of the bulk of the data, which is a good analytical consideration missing here.

6.  **Scatter Plot:**
    *   The `plt.scatter(df.loc[df['NA_Sales'].isnull(), 'EU_Sales'], color='red', marker='x', s=100) # Show outliers` line is problematic. This is not how you typically show outliers in a scatter plot where you're visualizing the relationship between two variables. If `NA_Sales` is null, you can't plot both `NA_Sales` and `EU_Sales` for that point. If the intent was to highlight points based on some outlier detection algorithm, that logic is missing. This line will likely throw an error or plot unintuitive data if `NA_Sales` is not NaN.

7.  **Line Plot (Year Trend):**
    *   The generated code refers to `df['Release_Year']` while the dataset likely has `Year_of_Release`. This is a crucial inconsistency. A robust solution needs to handle this.
    *   The ground truth performs explicit type conversion `pd.to_numeric` and `astype(int)` for the year column, which is necessary. The generated code just uses `dropna()`, assuming the year column is numeric to begin with, which might not be true for all datasets.
    *   The ground truth also filters years (e.g., between 1980 and 2020) to remove potential data entry errors or extreme outliers, which is good practice for time series plots.

Overall, the generated code provides a basic framework but lacks the data cleaning, handling of potential errors, and some plotting best practices demonstrated in the ground truth. The scatter plot's outlier handling is incorrect. The inconsistency in column names (`Release_Year` vs `Year_of_Release`) is also a concern.",command-r7b-12-2024,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
data = pd.read_csv('diabetes.csv')
print(data.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(data.shape)

# 3) List all column names and their data types
print(data.info())

# 4) Calculate the percentage of diabetic vs non-diabetic patients (Outcome column)
outcome_counts = data['Outcome'].value_counts()
diabetic_percentage = (outcome_counts[1] / sum(outcome_counts)) * 100
non_diabetic_perc...",6,"The generated code addresses most of the requirements and demonstrates a good understanding of fundamental pandas operations. However, it lacks robustness, clarity in output, and some nuanced handling present in the ground truth.

**Strong points:**
*   Correctly loads the CSV and displays the first 10 rows.
*   Correctly shows the dataset shape.
*   Correctly lists column names and data types using `df.info()`.
*   Correctly calculates missing values.
*   Uses `describe()` for summary statistics.

**Areas for improvement:**
*   **Missing Error Handling (Criterion: Code quality):** The ground truth includes `try-except` blocks for file loading, which is crucial for robust code. The generated code will crash if `diabetes.csv` is not found.
*   **Output Clarity/Formatting (Criterion: Code quality):** The ground truth uses descriptive print statements and separators to clearly demarcate each section of the analysis. The generated code merely prints outputs without context, making it harder to read and understand which output corresponds to which task.
*   **Percentage Calculation for Outcome (Criterion: Correctness/Completeness):** The ground truth explicitly states the `value_counts(normalize=True)` approach, which is more direct and robust than manually calculating `(outcome_counts[1] / sum(outcome_counts))`. Also, the ground truth prints both counts and percentages, while the generated code only prints the calculated percentages, assuming a binary outcome. It doesn't handle cases where `outcome_counts[0]` or `outcome_counts[1]` might not exist, though unlikely for this specific dataset it's not robust.
*   **Identification of Zero Values (Criterion: Correctness/Completeness):** This is the biggest gap.
    *   The ground truth specifically defines `COLUMNS_TO_CHECK_FOR_ZERO_AS_MISSING` to exclude columns like 'Pregnancies' and 'Outcome' where zero can be a legitimate value. The generated code uses `data.eq(0).sum()` on *all* columns, which incorrectly flags 'Pregnancies' and 'Outcome' as having ""zero values that might represent missing data,"" leading to a misleading conclusion.
    *   The ground truth also prints the *count* and *percentage* of zeros for these suspected columns, which is much more informative than just listing the column names.
*   **Numerical Columns for `describe()` (Criterion: Completeness):** While `data.select_dtypes(include=['number']).columns` explicitly selects numerical columns for `describe()`, `data.describe()` by default already does this for numerical columns, making the explicit selection redundant in this specific case. It also prints `numerical_columns.to_list()` which wasn't requested.
*   **Imports (Criterion: Code quality):** The ground truth imports `numpy`, which is good practice if any `numpy` operations were to be performed (like the `np.round` for percentages). While not strictly necessary for the generated code, it's generally a common import in data science scripts.",command-r7b-12-2024,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# 1) Load the dataset
df = pd.read_csv('diabetes.csv')

# 2) Identify columns with zero values
zero_columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin']

# 3) Replace zero values in these columns with NaN
df[zero_columns] = df[zero_columns].replace(0, np.nan)

# 4) Handle missing values
imputer = SimpleImputer(strategy='median')
df_imputed = df...",3,"The generated code has several critical issues that make it largely incorrect for the given task.

1.  **Incorrect Handling of BMI Calculation (Critical Failure):** The prompt explicitly states to clean and preprocess the `diabetes.csv` dataset, which by standard convention when referring to Pima Indians Diabetes Dataset, *already contains a 'BMI' column*. The generated code *assumes* the absence of 'BMI' and attempts to calculate it using non-existent 'Weight' and 'Height' columns (`df['Weight'] / (df['Height'] ** 2)`), leading to a `KeyError`. This fundamentally misunderstands the dataset being worked with.
2.  **Incorrect Missing Value Handling (Critical Failure):**
    *   It lists `zero_columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin']`, *missing 'BMI'*. The prompt clearly states that 'BMI' should also be checked for zero values that represent missing data.
    *   The `SimpleImputer` application: `df_imputed = df.apply(lambda x: imputer.fit_transform(x.values.reshape(-1, 1)) if x.dtype in ['object', 'float64', 'int64'] else x)` and subsequent `df = df_imputed.apply(lambda x: x.values.flatten() if x.ndim > 1 else x)` is overly complex and incorrect. `SimpleImputer` is designed for use on numerical arrays, not directly on whole DataFrames in this manner. It will convert numerical columns into single-column DataFrames, which is not what's intended, and then flatten them. This also incorrectly attempts to impute 'object' (categorical) columns, which is not part of the task.
3.  **Missing `BMI_Category` Column Check (Correctness Issue):** Due to the incorrect `BMI` calculation, the `BMI_Category` creation will fail or produce incorrect results.
4.  **Inefficient Scaling and Data Reconstitution (Code Quality/Correctness):**
    *   It creates a separate DataFrame for scaled features (`df_scaled`) and then tries to `pd.concat` it back. This is overly complicated and prone to alignment errors if not done carefully. The standard approach is to directly assign back to the original DataFrame for the scaled columns.
    *   It performs `df.drop(['Weight', 'Height'], axis=1)` which reinforces the mistake about the dataset's structure.
    *   It only scales a hardcoded subset of columns (`['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']`), which might not cover all numerical features if the dataset had more. The ground truth correctly identifies all numerical columns *except* the target and scales them.
5.  **Lack of Robustness:** The code assumes the file exists and does not include error handling for `FileNotFoundError`, unlike the ground truth.
6.  **Missing Output Details:** The prompt asks for ""cleaned dataset's shape *and first 5 rows*"", not just `head()`.
7.  **Implicit Assumption of 'BMI' Column:** The code uses BMI for categorization and scaling without ensuring its existence or valid calculation first.

In summary, the code fails to correctly identify and handle the 'BMI' column as specified, uses an incorrect method for missing value imputation, and has structural issues in scaling and data manipulation.",command-r7b-12-2024,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
data = pd.read_csv('diabetes.csv')

# 2) Create a correlation heatmap
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# 3) Generate histograms for all numerical features
for feature in data.select_dtypes(include=['float64', 'int64']).columns:
    plt.figure()
    sns.histplot(data[feature], kde=True)
...",7,"The generated code correctly addresses most of the tasks and produces the required visualizations. However, there are several areas for improvement to achieve a higher score.

**Positive Aspects:**
- **Correctness:** All requested visualization types (heatmap, histograms, box plots, pie chart, scatter plots, pair plot) are produced.
- **Imports:** Necessary libraries are imported.
- **Basic Functionality:** The code successfully loads the data and generates plots.

**Areas for Improvement:**
- **Plot Formatting & Readability (Significant):**
    - **Separate Figures for Histograms and Box Plots:** The generated code creates a new `plt.figure()` and then `plt.show()` inside the loop for histograms and box plots. This results in each individual histogram/boxplot appearing in its own separate window, which is not ideal for analysis and visualization of multiple distributions or comparisons. The ground truth code uses subplots to arrange these on a single figure, making direct comparison easier.
    - **Titles and Labels:** While titles and labels are present, they lack the detail and clarity of the ground truth (e.g., box plot x-axis label for ""Outcome"" could specify ""Non-Diabetic/Diabetic"").
    - **Scatter Plot Titles:** The scatter plot titles are generic (""BMI vs Glucose"", ""Age vs Pregnancies"") and don't mention the coloring by outcome, which is a key aspect of these plots. The ground truth explicitly states ""Colored by Diabetes Outcome"".
- **Completeness/Robustness:**
    - **Dataset Loading:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, unlike the ground truth.
    - **Numerical Features Selection:** The ground truth explicitly identifies numerical features (`df.columns.drop('Outcome')`), which is a more robust approach than `select_dtypes(include=['float64', 'int64'])` as `Outcome` might also be an integer and included in `select_dtypes`.
    - **Pair Plot Features:** While the generated code includes the correct features for the pair plot, the ground truth uses `diag_kind='kde'` and better `suptitle` placement, enhancing the plot's quality.
- **Code Quality:**
    - **No Descriptive Prints:** The generated code doesn't include informative print statements (e.g., `df.head()`, `df.info()`) like the ground truth, which aids in initial data understanding.
    - **Consistency:** The ground truth sets a `sns.set_style` for better aesthetics across all plots.
    - **Tight Layout:** The ground truth frequently uses `plt.tight_layout()` to prevent labels and titles from overlapping, which is missing in the generated code and can lead to cluttered plots.

By addressing the plotting issues (especially grouping similar plots into subplots) and adding more descriptive labels and error handling, the code quality and user experience would significantly improve.",command-r7b-12-2024,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
df = pd.read_csv(""world-happiness-report-2021.csv"")
print(df.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(df.shape)

# 3) List all column names and their data types
print(df.info())

# 4) Display the unique regional indicators and count of countries per region
print(df[""Region""].value_counts())

# 5) Show the top 10 happiest countries based on Ladder score
print(df.nlargest(10, ""Ladder sc...",6,"The generated code addresses most of the task requirements but has several critical issues that prevent it from being production-ready or fully accurate.

**Correctness:**
*   **Task 3 (`df.info()` output):** `df.info()` prints information and returns `None`. The generated code `print(df.info())` will print the info correctly, but then also print `None` afterwards, which is not ideal.
*   **Task 4 (Column Name):** The code assumes ""Region"" for regional indicators, whereas the ground truth uses ""Regional indicator"". This is a crucial difference and will lead to an error if the column name is exactly ""Regional indicator"", which it is for the `world-happiness-report-2021.csv` dataset.
*   **Task 5 (Top 10):** `df.nlargest(10, ""Ladder score"")` will return all columns for the top 10 rows, while the task implicitly asks for just the country name and score (though `df.nlargest` is technically correct for ""show the top 10 happiest countries""). The ground truth specifically selects the relevant columns.
*   **Task 6 (Column Names):** The generated code uses ""GDP per capita"" for summary statistics when the correct column name is ""Logged GDP per capita"". This will cause a `KeyError`. The ground truth includes robust error handling for missing columns, which the generated code lacks.
*   **Error Handling:** The generated code lacks any error handling for file not found or missing columns, making it less robust than the ground truth.

**Code Quality:**
*   **Readability/Clarity:** The print statements are minimal (`print(df.head(10))`) without any descriptive text (e.g., ""1. First 10 rows...""). This makes the output less user-friendly compared to the ground truth which adds clear labels and separators.
*   **Robustness:** The complete lack of error handling (e.g., `try-except` for `FileNotFoundError` or checking for column existence) makes the code fragile. If the file is missing or a column name is slightly off, the program will crash.

**Completeness:**
*   Most tasks are attempted, but the incorrect column names for Task 4 and Task 6 lead to logical failures or errors.
*   The output format is not as clean or informative as the ground truth.

**Areas for Improvement:**
*   Implement `try-except` for file loading.
*   Verify column names before trying to access them (or at least use the correct ones).
*   Add descriptive `print` statements before each task output to make the results clear to the user.
*   Review `df.info()` usage for printing.
*   Consider being more specific in column selection for Task 5's output as the ground truth does.",command-r7b-12-2024,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer

# 1) Load the dataset
data = pd.read_csv('world-happiness-report-2021.csv')

# 2) Handle missing values
numeric_cols = data.select_dtypes(include=['int64','float64']).columns
imputer = SimpleImputer(strategy='mean')
data[numeric_cols] = imputer.fit_transform(data[numeric_cols])

categorical_cols = data.select_dtypes(include=['obje...",2,"The generated code has several critical issues that render it largely incorrect or incomplete for the given task.

1.  **Missing Value Handling (for categorical):** The line `data[categorical_cols] = data[categorical_cols].fillna(data[categorical_cols].mode().iloc[0])` is incorrect. `mode()` can return multiple modes, and `.iloc[0]` retrieves only the first *row* of the result, not necessarily the mode for each column. The ground truth correctly iterates through columns or uses `apply`.
2.  **Feature Name Mismatches:** The generated code assumes column names like `Happiness_Score`, `GDP_per_capita`, `Social_support`, `Healthy_life_expectancy`, `Freedom_to_make_life_choices`, `Ladder_score`, `Regional` which are not present in the `world-happiness-report-2021.csv` dataset. The actual column names are 'Ladder score', 'Logged GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Regional indicator'. This fundamental error means the code will fail to run or produce incorrect results right from the feature creation step.
3.  **Happiness_Category Logic:** The task specifies 'High: >6.5, Medium: 4-6.5, Low: <4'. The `pd.cut` bins `[0, 4, 6.5, 10]` and labels `['Low', 'Medium', 'High']` do not precisely match these conditions. Specifically, 4.0 would be `Low` in the generated code, but `Medium` in the ground truth/task.
4.  **GDP_Category Logic:** The task specifies 'based on quartiles'. The generated code uses fixed bins `[0, 10000, ...]` which is not quartile-based. Also, it has an empty `q=[]` argument, which is wrong. `pd.qcut` is the correct approach for quartiles.
5.  **Outlier Removal:** The logic for outlier removal is completely broken and syntactically incorrect: `data = data[pd.cut(data['Ladder_score'], bins=pd.Series([(data['Ladder_score'].quantile(0.25) - 1.5 * data['Ladder_score'].quantile(0.25), data['Ladder_score'].quantile(0.75) + 1.5 * data['Ladder_score'].quantile(0.75)]))['range']).category.apply(lambda x: x <= 'Upper')]`. This will likely raise a `KeyError` or `AttributeError` and certainly doesn't implement the IQR filtering correctly.
6.  **Standardization:** The standardization re-creates the DataFrame `data_scaled` and then attempts `data.join(data_scaled)`. This will not correctly replace the original numerical columns with their scaled versions and may lead to duplicate columns or incorrect merging, especially if the index is not carefully handled. The ground truth method of directly assigning back to the selected `df[numerical_features_to_scale]` is much cleaner and correct.
7.  **Label Encoding:** Similar to other features, the column name `Regional` is incorrect; it should be `Regional indicator`. The new encoded column `Regional_encoded` is created, but the task implies replacing or effectively using the encoded column, not necessarily keeping both the original `Regional indicator` and `Regional_encoded`. The ground truth replaces the original column.
8.  **Error Handling (File Loading):** The generated code lacks basic error handling for `FileNotFoundError` when loading the CSV, which is good practice.
9.  **Completeness:** Due to the critical errors in feature naming and logic, the code is incomplete in its functionality even if it runs with modified column names. The output display is correctly requested.",command-r7b-12-2024,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import correlation_matrix
import plotly.express as px
import plotly.graph_objs as go
import folium
from folium.plugins import HeatMap

# Load the dataset
data = pd.read_csv(""world-happiness-report-2021.csv"")

# 1) World map visualization showing happiness scores by country using Plotly
world_map = px.choropleth(data, locations=""iso_code"", color=""indicator"", hover_name=""coun...",5,"The generated code attempts to address all requirements but falls short in several key areas of correctness, completeness, and code quality.

**Correctness & Completeness Issues:**

*   **Dataset Loading and Column Names:** The generated code loads the dataset but immediately assumes column names like ""iso_code"", ""indicator"", ""country"", ""region"", ""happiness"", ""gdp_per_capita"", ""social_support"", ""life\_ expectancy"", ""freedom\_to\_make\_choices"", ""generosity"", ""perception\_of\_corruption"". The `world-happiness-report-2021.csv` dataset uses different column names (e.g., 'Ladder score', 'Regional indicator', 'Country name', 'Logged GDP per capita', 'Social support', 'Healthy life expectancy'). This leads to `KeyError` for almost all visualization attempts. The ground truth correctly renames columns for consistency.
*   **World Map Visualization (Plotly):** It incorrectly uses `locations=""iso_code""`, `color=""indicator""`, `hover_name=""country""`. The dataset requires `locations=""Country name""` and `color=""Ladder score""`. It also misses crucial `locationmode=""country names""` and `color_continuous_scale` for a meaningful map. The `update_geos(projection_color=""lightgray"")` and `projection_type` are also missing.
*   **Horizontal Bar Chart (Region):** Again, it incorrectly uses `region` and `happiness` column names. It also doesn't sort or use a specific palette like the ground truth.
*   **Correlation Heatmap:** The column names for the factors are incorrect (`gdp_per_capita`, `social_support`, etc.). This will cause `KeyError`.
*   **Scatter Plots:** `plt.figure(figsize=(12, 8))` creates a single figure, but then `sns.scatterplot` is called twice on the *same axes*, essentially overlaying two different relationships on one plot. This is not what ""scatter plots"" (plural) implies or what the ground truth does by creating separate subplots. The column names `gdp_per_capita`, `social_support`, and `ladder` are also incorrect.
*   **Box Plots:** Incorrect column names again (`happiness`, `region`). Uses `data.boxplot()` from pandas, which is less visually appealing and customizable than `sns.boxplot`.
*   **Radar Chart:** This is a major failure.
    *   It retrieves `top_5_countries` based on the incorrect `happiness` column.
    *   The radar chart implementation is fundamentally flawed. It tries to use `labels` as a DataFrame for plotting, but then attempts `ax.bar(labels.index, top_5_countries['happiness'])`, which creates a bar chart on polar coordinates for *happiness score only*, not a multi-dimensional ""profile"" of factors.
    *   It completely misses the crucial steps of defining `radar_features`, normalizing them using `MinMaxScaler`, and plotting each country's *scaled factor values* as lines on the radar chart.
    *   The `set_xticks` and `set_xticklabels` logic is also incorrect for a radar chart where features should be the labels.
*   **Missing Imports:** `sklearn.metrics.correlation_matrix` is imported but not used, and `MinMaxScaler` (essential for the radar chart) is not imported. `plotly.graph_objs` and `folium` imports are present, but `folium` is not used in the provided code snippet, and `plotly.graph_objs` is not necessary for `plotly.express`.
*   **General Code Quality & Formatting:**
    *   No informative print statements for progression or error handling (like the ground truth's `try-except`).
    *   Lack of consistent figure sizing (`plt.figure(figsize=...)` is used sometimes, but not set globally or consistently for all Matplotlib plots).
    *   Labels and titles are present but lack the specific formatting (e.g., fontsize, rotation) found in the ground truth.
    *   No `tight_layout()` calls for Matplotlib plots, which can lead to cropped labels.
    *   Missing legends where relevant (e.g., scatter plots with hue).

The code demonstrates an understanding of the types of plots requested but fails to implement them correctly due to fundamental errors in data access (column names) and the specific logic for certain complex plots like the radar chart and proper scatter plot layout.",command-r7b-12-2024,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# 1) Load the dataset and parse the 'date' column as datetime objects
df = pd.read_csv(""synthetic_covid19_data.csv"", parse_dates=['date'])

# 2) Display the first 10 rows
print(df.head(10))

# 3) Show the shape of the dataset (rows and columns)
print(df.shape)

# 4) List all column names and their data types
print(df.dtypes)

# 5) Calculate and display the total sum of 'total_cases' and 'total_deaths' grouped by 'continent'
print(df.groupby('cont...",6,"The generated code addresses most of the requirements but has several issues in correctness and robustness, leading to a lower score than ideal.

**Strong Points:**
- Correctly loads the data and parses dates.
- Correctly displays the first 10 rows and shape.
- Correctly counts and displays missing values.
- Uses `df.dtypes` for data types, which is acceptable, though `df.info()` from the ground truth is more comprehensive.
- Correctly calculates summary statistics using `.describe()`.

**Areas for Improvement / Errors:**

1.  **Missing `numpy` import:** While `import numpy as np` is present, it's not used anywhere in the code. It's a minor point but indicates unnecessary import.

2.  **Lack of error handling:** The generated code does not include any error handling for `FileNotFoundError` or other exceptions when loading the CSV, which is crucial for robust code.

3.  **Output formatting for sections 2, 3, 4, 5, 7, 8:** The ground truth includes clear print statements (e.g., ""--- First 10 rows of the dataset ---"") to delineate the output for each task section. The generated code lacks these, making the output less readable and harder to understand which output corresponds to which task. This deviates from ""Provide only the Python code without explanations"" implicitly meaning that the output should also be clear as a result of the code.

4.  **Task 5 (Continental Summary) - Redundant `reset_index()`:** While `reset_index()` doesn't break the code, it's often not necessary for `groupby().agg()` if you just want to view the grouped results, and it shifts 'continent' from an index back to a column. The ground truth keeps 'continent' as the index, which is often preferred for grouped summaries.

5.  **Task 6 (Top 5 locations with highest 'total_cases' based on the latest date) - Critical Error:** This section has a significant logical flaw and incorrect implementation:
    *   `df.groupby('location')['date'].transform('max')` correctly identifies the latest date per location.
    *   `df.set_index('date_latest').groupby('location')['total_cases'].transform('sum')` is incorrect.
        *   Setting `date_latest` as an index then grouping by `location` and transforming 'total_cases' by 'sum' will sum ALL `total_cases` values for a given location, regardless of date, and then assign this cumulative sum back to every row belonging to that location. The task requires the `total_cases` *at the latest date*, not the sum of all `total_cases` for that location. This approach fundamentally misunderstands the requirement.
        *   The subsequent `top_locations = df.sort_values('total_cases_latest', ascending=False).groupby('location').head(5)` will then sort based on this incorrect 'cumulative' `total_cases_latest` and then select the first 5 entries for *each* location (due to `groupby('location').head(5)`), which is definitely not the top 5 overall locations but rather 5 rows for each of the top locations based on this incorrect sum. The ground truth's approach using `drop_duplicates(subset=['location'], keep='last')` after sorting by date is the correct way to get the latest data per location.

6.  **Task 7 (Summary Statistics):** While `df[['new_cases', 'new_deaths']].describe()` provides mean, std, min, max, it *also* includes percentiles and count, which aren't explicitly requested. The ground truth's `agg(['mean', 'median', 'min', 'max', 'std'])` is more precise for the requested statistics. Also, `describe()` can include stats for non-numeric columns if not selected carefully, whereas the ground truth explicitly converts to numeric and handles NaNs for robustness.

The critical error in Task 6 significantly impacts the correctness of the solution as a whole, pulling the score down. Robustness (error handling) and clarity of output are also significant deficiencies.",command-r7b-12-2024,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Calculate and report the average 'quality' of the red wines
data = pd.read_csv(""winequality-red.csv"")
mean_quality = data[""quality""].mean()
print(f""Average Quality: {mean_quality:.2f}"")

# 2) Group the wines by their 'quality' score and calculate the average 'alcohol' content for each quality group
quality_groups = data.groupby(""quality"")[""alcohol""].mean().reset_index()

# 3) Create a bar chart visualizing ...",8,"The generated code correctly addresses all three requirements of the task: calculating the mean quality, grouping by quality to find average alcohol content, and generating an appropriate bar chart. The imports are correct, and the code structure is clear.

Areas for improvement (to reach a 9 or 10):
- **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError`. While not explicitly requested, robust production-ready code often includes such error handling for file operations.
- **Plot Enhancements:** The ground truth's plot has more detailed labels (e.g., ""Quality Score"" vs. ""Quality"", ""Average Alcohol Content (%)"" vs. ""Average Alcohol Content""), a larger font size for the title, and data labels on top of the bars for precise values. The `xticks(rotation=45)` in the generated code is unnecessary as the x-axis labels are simple integers and don't overlap. The `sns.set_style(""whitegrid"")` and `palette='viridis'` in the ground truth also add a nice touch to the visualization.
- **Reporting of results:** The ground truth provides more context around the print statements (e.g., ""1. Average 'quality' of red wines:"", ""2. Average 'alcohol' content for each 'quality' group:""). This makes the output clearer for the user.",command-r7b-12-2024,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# 1) Load the dataset and parse the 'date' column as datetime objects
data = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2) Handle missing values in 'hosp_patients' and 'icu_patients' by filling them with 0
# For other numerical columns, fill missing values with the mean of their respective column
data.fillna(0, inplace=True)
for col in data.select_dtypes(include=[np.number]):
    if col not in ['hosp_patients', 'icu_patien...",6,"The generated code has several issues that prevent it from fully meeting the requirements and demonstrating robust data handling.

**Correctness Issues:**

*   **Missing Value Handling (Critical):** The `data.fillna(0, inplace=True)` line at the beginning is fundamentally flawed. It fills *all* NaN values in the entire DataFrame with 0 *before* attempting to compute means for `positive_rate` and `reproduction_rate`. This means that if `positive_rate` had NaNs, they would be replaced by 0, and then the subsequent `fillna(data[col].mean())` would try to fill them with the mean of a column that no longer has NaNs (or has 0s where NaNs used to be), which is incorrect. The task specifically asked to fill `hosp_patients` and `icu_patients` with 0, and *other numerical columns* with their mean. The current logic inverts this or makes it impossible.
*   **Mortality Rate Calculation:** The `apply(lambda x: x if x >= 0 else 0)` after `fillna(0)` is redundant and also potentially problematic. `fillna(0)` handles both NaNs (from 0/0 or NaN in original data) and `inf` values (if `total_cases` was 0 and `total_deaths` was positive, resulting in `inf`, `fillna(0)` would not catch this directly, but `np.inf` isn't a `NaN` that `fillna` acts upon). The ground truth's approach of `replace([np.inf, -np.inf], 0)` followed by `fillna(0)` is more robust for explicitly handling both `inf`/`-inf` and `NaN` values that can arise from division. The provided `apply` function also doesn't multiply by 100 before the check, which is incorrect as the final result should be a percentage.
*   **Vaccination Rate Calculation:** Similar to mortality rate, `fillna(0)` doesn't correctly handle `np.inf` if `population` is 0 and `people_fully_vaccinated` is positive. The `apply(lambda x: x * 100 if x >= 0 else 0)` is a strange way to handle the percentage calculation; it should simply be `* 100` initially. The `if x >= 0 else 0` is not necessary if division by zero is handled properly by replacing `inf`/`NaN` directly.

**Completeness:**

*   **Mean Calculation for Numerical Columns:** The loop for filling missing values with the mean doesn't respect the initial `fillna(0)` for `hosp_patients` and `icu_patients`. It attempts to fill all numerical columns' NaNs with their mean, which goes against the specific instruction for `hosp_patients` and `icu_patients`. The ground truth explicitly handles these two columns first with `0`, then only the *other* specified numerical columns with their mean.
*   **Division by Zero Handling:** While `fillna(0)` can cover some aspects of `NaN` from `0/0`, it does not explicitly handle `inf` from `X/0` where `X != 0`. The ground truth's use of `replace([np.inf, -np.inf], 0)` is more comprehensive for this.

**Code Quality:**

*   The single `data.fillna(0, inplace=True)` at the beginning is a significant logical error in the flow of operations for handling missing values.
*   The use of `apply` for the percentage calculation and handling potential negative values (which shouldn't occur in these rates if inputs are non-negative) is less direct and robust than the ground truth's method.
*   The `try-except FileNotFoundError` block from the ground truth is a good practice for robust code, which is missing here.

To improve, the code needs to:
1.  Separate the missing value handling: first, specific columns (`hosp_patients`, `icu_patients`) with 0, then other specified numerical columns (`positive_rate`, `reproduction_rate`) with their mean, and potentially ensuring those columns are numeric before calculating the mean.
2.  Correctly handle `inf` values in rate calculations, typically using `np.inf` replacement, then `fillna(0)`.
3.  Ensure the multiplication by 100 for rates happens at the correct step in the calculation.",command-r7b-12-2024,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# 1) Load the dataset and parse the 'date' column
data = pd.read_csv(""synthetic_covid19_data.csv"")
data['date'] = pd.to_datetime(data['date'])

# 2) Create a time series line plot of 'new_cases' for the location 'India' over the entire period
plt.figure(figsize=(10, 6))
sns.lineplot(x=data['date'], y=data[data['location'] == 'India']['new_cases'], label='India')
plt.title('Time Seri...",6,"The generated code correctly addresses most of the plotting requirements but has several issues that reduce its accuracy and quality.

**Correctness:**
*   **Time Series Plot (India):** The `sns.lineplot` call is malformed (`y=data[data['location'] == 'India']['new_cases']` should ideally be `data=india_df, x='date', y='new_cases'`). While it might technically work, it's not the idiomatic or cleanest way to pass data to Seaborn, and it doesn't filter the x-axis for India correctly. The ground truth extracts a filtered dataframe first.
*   **Total Deaths Bar Chart:** The calculation of `total_deaths` for each continent is incorrect. Summing `total_deaths` directly will count many duplicate entries for countries that appear multiple times. To get the `sum of 'total_deaths' for each 'continent'`, one should take the *latest* `total_deaths` value for *each country* within that continent, and then sum those country values. The generated code simply sums all `total_deaths` rows, which is wrong. The ground truth correctly handles this by taking the latest data per location before grouping.
*   **Scatter Plot (GDP vs. Vaccinations):** The `keep='first'` in `drop_duplicates` for `latest_data` is incorrect. To get the latest entry, `keep='last'` should be used after sorting by date, as done in the ground truth. This means the scatter plot uses the *earliest* data points, not the *latest*.
*   **Error Handling:** Lacks basic error handling for file not found, which is essential for robust code.

**Code Quality:**
*   **Readability:** The `lineplot` call for India is less readable.
*   **Consistency:** The `sns.set_theme` is missing, leading to default plot styles.
*   **Plot Customization:** Lacks detailed customization (e.g., specific `figsize`, grid lines, rotations for x-ticks, usage of `tight_layout`, adding KDE to histogram, specific palettes, log scales for scatter plot, legend positioning) which improves clarity and presentation.
*   **Redundant Import:** `from datetime import datetime` is imported but not used.

**Completeness:**
*   All required plots are generated, but some are technically incorrect in their data aggregation/selection as noted above.
*   Titles and labels are present, but additional plot enhancements (like `xticks` rotation, `grid`, specific `figsize`, log scales, proper legend placement for scatter plot) are missing.
*   No `print` statements indicating plot generation progress for user feedback.",command-r7b-12-2024,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 1) List all column names and identify the data type of each column
columns = df.columns.tolist()
dtypes = df.dtypes.apply(lambda x: x.name).to_dict()
print(""Column Names:"", columns)
print(""Data Types:"", dtypes)

# 2) Numerical columns
numerical_columns = ['elixirCost', 'Win Rate', 'Usage']

for col in numerical_columns:
    stats = df[col].describe()
    print(f""Statistics for {col}:"")
    print(stats...",7,"The generated code correctly addresses the main requirements of the task: loading the CSV, identifying column names and data types, calculating descriptive statistics for the specified numerical columns, and reporting the total number of rows.

**Strong Points:**
- Proper use of `pandas` for data loading and manipulation.
- Correctly identifies column names (`df.columns.tolist()`).
- Correctly identifies data types (`df.dtypes`).
- Uses `describe()` method which conveniently provides the requested statistics (mean, median, standard deviation, min, max, though `describe()` also reports count, 25%, 50%, 75% percentiles).
- Correctly gets the total number of rows (`df.shape[0]`).

**Areas for Improvement to achieve a higher score:**
- **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and general exceptions during CSV loading, which is crucial for production-ready code. The generated code lacks this.
- **Specificity of Statistics:** The task specifically asked for mean, median, standard deviation, min, and max. While `df[col].describe()` provides these, it also provides other statistics (count, 25%, 50%, 75% quartiles) which were not explicitly requested. The ground truth uses `agg` to get *only* the requested statistics, which is more precise.
- **Output Format for Data Types:** The ground truth prints `df.dtypes` directly, which is a clean Series output. The generated code converts it to a dictionary with `apply(lambda x: x.name).to_dict()`, which is less conventional for simply listing data types and can be less readable than the default `df.dtypes` output for this specific need.
- **Clarity of Output:** The ground truth provides more verbose and self-explanatory print statements (e.g., ""--- Column Names and Data Types ---"", ""--- Descriptive Statistics for Numerical Columns ---"") making the output much clearer and easier to understand for a user. The generated code's print statements are more succinct but less descriptive.
- **Handling of Missing Columns:** The ground truth gracefully handles the scenario where one or more of the specified numerical columns might not be present in the dataset. The generated code would raise a `KeyError` if a column in `numerical_columns` was not found.",command-r7b-12-2024,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Filter the dataset to include only cards with an elixirCost less than or equal to 3
filtered_df = df[df['elixirCost'] <= 3]

# Select and display the Card name and Win Rate columns
selected_df = filtered_df[['Card', 'Win Rate']]
print(selected_df)

# Sort the resulting data by Win Rate in descending order
sorted_df = selected_df.sort_values(by='Win Rate', ascending=False)

# Display the sorted data
pr...",7,"The generated code correctly performs all the requested tasks: loading the CSV, filtering by `elixirCost`, selecting specific columns, and sorting by `Win Rate`. However, the code has minor issues in terms of output and robustness. It prints `selected_df` before sorting, which is redundant as the final sorted output is what's truly desired. Additionally, it lacks error handling for `FileNotFoundError` or missing columns, which is present in the robust ground truth. While it fulfills the core requirements, these additions would improve its quality and user experience.",command-r7b-12-2024,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
clash_royale_cards = pd.read_csv(""clash_royale_cards.csv"")

# Extract the 'elixirCost' column
elixir_costs = clash_royale_cards[""elixirCost""]

# Generate a histogram
plt.figure(figsize=(8, 6))
sns.histplot(elixir_costs, bins=20, kde=True, color=""blue"")
plt.title(""Distribution of Elixir Cost"")
plt.xlabel(""Elixir Cost"")
plt.ylabel(""Frequency"")
plt.xticks(rotation=45)
plt.show()",6,"The generated code correctly loads the CSV, generates a histogram for `elixirCost`, and includes a title and labeled axes. This fulfills the core requirements of the task.

However, there are a few areas for improvement:
- **Error Handling**: The generated code lacks explicit error handling for `FileNotFoundError` or other exceptions during CSV loading. The ground truth includes robust `try-except` blocks which are good practice for production-ready code.
- **Plot Customization**: While `20` bins are used, `elixirCost` is a discrete variable typically ranging from 1 to 10. `kde=True` (Kernel Density Estimate) is less appropriate for discrete data where `kde=False` would be more suitable to clearly show the distribution of distinct elixir values. The ground truth uses `bins=range(1, 12)` and `kde=False`, which provides a much clearer and more accurate visualization for discrete data.
- **`xticks` customization**: The `xticks` are rotated by 45 degrees which is not necessary for this, and the ground truth properly sets `xticks` to show integer values clearly.

While the generated code essentially solves the problem, it could be more robust and generate a more appropriate visualization with better understanding of the data's nature.",command-r7b-12-2024,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# Load the iris dataset
iris_data = pd.read_csv('iris.csv')

# 1) List all column names and identify data types
column_names = iris_data.columns.tolist()
data_types = iris_data.dtypes.to_dict()
print(f""Column Names: {column_names}\nData Types: {data_types}"")

# 2) Numerical column statistics
numerical_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
for col in numerical_columns:
    stats = iris_data[col].describe()
    print(f""Statistic...",8,"The generated code correctly addresses most of the task requirements.

**Strengths:**
*   **Correct CSV Loading:** Successfully loads the `iris.csv` dataset.
*   **Column Names and Data Types:** Correctly identifies and prints column names and their data types.
*   **Descriptive Statistics:** Calculates and prints the descriptive statistics (mean, std, min, max, 25%, 50%, 75% percentiles, count) for the specified numerical columns using `.describe()`.
*   **Unique Species Counts:** Correctly identifies the number of unique species and their counts.
*   **Proper Imports:** Includes `import pandas as pd`.

**Areas for Improvement (Why not a 10):**
*   **Robustness (File Loading):** The ground truth includes a `try-except` block for `FileNotFoundError` and other general exceptions, which makes the code more robust if the `iris.csv` file is not present. The generated code will simply raise an error and crash in this scenario.
*   **Specific Statistics for Numerical Columns:** While `describe()` provides all the requested statistics (mean, median, std, min, max), the prompt specifically listed these five. `.describe()` also provides count and percentiles, which are not explicitly asked for. The ground truth uses `.agg(['mean', 'median', 'std', 'min', 'max'])` which is more precise to the prompt's request for *basic descriptive statistics (mean, median, standard deviation, min, max)* as it only shows those specific metrics.
*   **Output Formatting:** The output formatting for numerical statistics is a loop that prints `describe()` for each column separately, which is functional but less concise and harder to read than the table format provided by the ground truth (which uses `df[existing_numerical_columns].agg(...)` or `df[numerical_columns].describe()` if the columns exist). The species counts are printed as a dictionary, which is slightly less readable than the Series format in the ground truth.
*   **Error Handling for Missing Columns:** The generated code does not check if the specified numerical columns actually exist in the DataFrame before trying to access them (e.g., `iris_data[col].describe()`). If a column is misspelled or missing, it will raise a `KeyError`. The ground truth includes checks for `existing_numerical_columns` and `species_column_name`.",command-r7b-12-2024,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

# Load the iris dataset
iris_data = pd.read_csv('iris.csv')

# Filter the dataset for 'Iris-setosa' records
setosa_records = iris_data[iris_data['Species'] == 'Iris-setosa']

# Select and display 'SepalLengthCm' and 'SepalWidthCm' columns
setosa_sepal_features = setosa_records[['SepalLengthCm', 'SepalWidthCm']]
print(setosa_sepal_features)
```",9,"The generated code correctly loads the `iris.csv` dataset, filters it to include only 'Iris-setosa' species, and then selects and displays the `SepalLengthCm` and `SepalWidthCm` columns. It properly utilizes pandas for these operations, directly addressing all specified requirements.

The code is concise and readable. The primary reason it doesn't get a perfect 10 is the lack of robust error handling for common issues like `FileNotFoundError` or `KeyError`, which the ground truth includes. While the task description didn't explicitly ask for error handling, it's good practice in real-world scenarios. However, for a simple benchmark task focusing on core data manipulation, it is highly effective.",command-r7b-12-2024,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the iris dataset
data = pd.read_csv('iris.csv')

# Create a scatter plot
plt.scatter(data['SepalLengthCm'], data['SepalWidthCm'], c=data['Species'])

# Add title and labels
plt.title('Iris Dataset Scatter Plot')
plt.xlabel('SepalLengthCm')
plt.ylabel('SepalWidthCm')

# Display the plot
plt.show()
```",6,"The generated code correctly loads the dataset, creates a scatter plot, and adds a title and axis labels. However, it fails on a critical requirement: coloring the points by species. The `c=data['Species']` argument in `plt.scatter` does not automatically map string labels to distinct colors as intended for categorical data in a simple scatter plot. It will likely throw an error or assign numerical values to the species names, resulting in a continuous color mapping instead of discrete colors for each species. To properly color by species, one typically iterates through unique species and plots each subset, or uses a plotting library like Seaborn which handles categorical coloring more directly. This oversight makes the plot not visually distinct by species as required.

Improvements needed:
1. Revise the coloring method to correctly distinguish each `Species` with a unique color. This could involve iterating through unique species, or using a library that supports categorical coloring directly (e.g., `seaborn.scatterplot`).
2. Add a legend to clearly indicate which color corresponds to which species.",command-r7b-12-2024,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

# 1) Load the dataset and get column names and data types
df = pd.read_csv(""heart.csv"")
column_names = df.columns.tolist()
column_dtypes = df.dtypes.to_dict()

print(""Column Names:"", column_names)
print(""Column Data Types:"", column_dtypes)

# 2) Descriptive statistics for numerical columns
numerical_columns = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']
for column in numerical_columns:
    print(f""Description statistics for {column}:"")
    print(df[column].describe...",7,"The generated code correctly addresses the core requirements of loading the CSV, identifying column names and types, calculating descriptive statistics for specified numerical columns, and reporting value counts for `HeartDisease`. However, there are areas for improvement.

**Strengths:**
*   **Correctness:** It correctly loads the data and performs the requested calculations (descriptive statistics, value counts).
*   **Imports:** Proper use of `pandas`.
*   **Readability:** The code is straightforward and easy to understand.

**Areas for Improvement:**
*   **Descriptive Statistics Output:** While it uses `describe()`, the output for descriptive statistics is printed for each column individually, which is less concise and harder to compare than a single table `describe()` would provide for multiple columns. The request specifically asked for `mean, median, standard deviation, min, max`. `describe()` provides more than that, and doesn't directly give 'median' explicitly (it's '50%'). The ground truth renames '50%' to 'median' and selects only the requested statistics, which is better.
*   **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError`, which is crucial for real-world applications. The generated code lacks this.
*   **Output Formatting:** The output formatting for column names and data types is less clear than the ground truth. The ground truth separates column names and data types and presents them more cleanly. The descriptive statistics output is also verbose in the generated code because it calls `.describe()` individually for each column.
*   **Completeness (Minor):** The instruction for descriptive statistics explicitly asked for `mean, median, standard deviation, min, max`. While `describe()` includes these, filtering and renaming as done in the ground truth would make the output more aligned with the specific request.

Overall, the code is functional and largely correct but could benefit from better output formatting, more precise statistic reporting, and essential error handling to be considered expert-level.",command-r7b-12-2024,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

# Load the dataset
data = pd.read_csv('heart.csv')

# Filter the dataset
filtered_data = data[(data['Age'] > 50) & (data['HeartDisease'] == 1)]

# Select and display the required columns
result = filtered_data[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]
print(result)
```",10,"The generated code is excellent. It correctly implements all requirements: loading the CSV, filtering by `Age > 50` and `HeartDisease == 1`, and selecting and displaying the specified columns (`Age`, `Sex`, `ChestPainType`, `Cholesterol`). The code is concise, uses proper pandas syntax, and includes the necessary import. It perfectly solves the task.",command-r7b-12-2024,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Filter the wines
filtered_df = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]

# Find the wine with the highest quality score in the filtered subset
highest_quality_wine = filtered_df.loc[filtered_df['quality'].idxmax()]

# Display all features of the wine with the highest quality score
print(highest_quality_wine)
```",8,"The generated code correctly performs all the required steps: filtering based on two conditions, identifying the wine with the highest quality in the filtered subset, and displaying its features. It uses appropriate pandas functions (`read_csv`, boolean indexing, `idxmax`, `loc`).

**Strong points:**
*   **Correctness:** Accurately implements all three steps of the task.
*   **Conciseness:** Achieves the task efficiently with minimal lines of code.
*   **Proper Imports:** Includes `pandas` as required.

**Areas for Improvement:**
*   **Error Handling:** The ground truth includes robust error handling (e.g., `FileNotFoundError`, `KeyError`, `try-except` blocks) which is crucial for real-world applications but is missing in the generated code.
*   **Edge Case Handling:** The generated code does not explicitly handle the case where `filtered_df` might be empty (i.e., no wines meet the criteria). `idxmax()` on an empty Series would raise an error. The ground truth handles this by checking `if filtered_wines.empty:`.
*   **Readability/Comments:** While the code is relatively straightforward, adding comments for each step (similar to the ground truth) would improve readability for more complex tasks.
*   **Function Encapsulation:** Encapsulating the logic within a function (as done in the ground truth) makes the code more reusable and modular.
*   **Explicit Output Formatting:** Printing `highest_quality_wine` (a pandas Series) is acceptable, but the ground truth's `to_string()` can be marginally better for ensuring full display without truncation for wider series in some environments. It's a minor point, but worth noting.",command-r7b-12-2024,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv(""heart.csv"")

# Histogram for Age distribution
plt.figure()
sns.histplot(data[""Age""], bins=30, kde=True)
plt.title(""Age Distribution"")
plt.xlabel(""Age"")
plt.ylabel(""Frequency"")
plt.show()

# Bar chart for Sex distribution
plt.figure()
sns.countplot(x=""Sex"", data=data)
plt.title(""Sex Distribution"")
plt.xlabel(""Sex"")
plt.ylabel(""Count"")
plt.show()
```",8,"The generated code correctly loads the dataset and generates both a histogram for Age and a bar chart for Sex distribution, with appropriate titles and labeled axes. This fulfills all the core requirements of the task.

Strengths:
- Correctly uses pandas to load the CSV.
- Successfully generates a histogram for 'Age' with KDE.
- Successfully generates a bar chart for 'Sex' distribution.
- Both plots have titles and labeled axes as required.
- Uses `matplotlib.pyplot` and `seaborn` effectively for plotting.

Areas for Improvement (minor):
- The `Sex` column in `heart.csv` typically uses numerical values (e.g., 0 for Female, 1 for Male). The ground truth code maps these numerical values to descriptive labels ('Male', 'Female') for better readability in the bar chart. The generated code plots the numerical values directly, which is less user-friendly. While not explicitly asked for, it's a best practice for clarity.
- The ground truth code includes error handling for `FileNotFoundError` and `KeyError`, which adds robustness. While not strictly required by ""Provide only the Python code without explanations,"" it's a good practice in real-world scenarios.",command-r7b-12-2024,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Get column names and data types
print(""Column names:"")
print(df.columns)
print(""\nData types:"")
print(df.dtypes)

# Convert 'streams' column to numeric
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# Report NaNs in the 'streams' column
print(""\nNaNs in 'streams' column:"", df['streams'].isna().sum())

# Calculate and report descriptive statistics for numerical columns
print(""\nDescriptive st...",7,"The generated code correctly addresses most of the task requirements. It loads the dataset, converts the `streams` column to numeric, reports NaNs, calculates descriptive statistics for the specified columns, and counts unique artists.

However, there are several areas for improvement:
- **Error Handling for File Loading:** The code lacks `try-except` for `FileNotFoundError` when loading the CSV, which is crucial for robustness.
- **Reporting Column Names/Types:** While `df.columns` and `df.dtypes` are printed, `df.info()` from the ground truth provides a more comprehensive and readable summary including non-null counts, which is more aligned with standard data profiling practices.
- **Descriptive Statistics Presentation:** The generated code calculates the statistics for each column individually using string formatting. The ground truth's use of `df[columns].describe().loc[['mean', '50%', 'std', 'min', 'max']]` is much more concise, readable, and scalable for multiple numerical columns. It also directly uses the `50%` index for median as output by `describe()`, which is more natural than explicitly calling `median()`.
- **Informative Prints:** The ground truth includes more descriptive print statements for each step, improving clarity and user experience. For example, it confirms the original and new dtypes of `streams` and provides a sample of rows with NaN `streams` values.
- **Column Existence Check:** The ground truth checks if numerical columns or `artist(s)_name` exist before attempting operations, making it more robust against variations in dataset structure.",command-r7b-12-2024,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Filter the dataset for songs released in 2023
df_2023 = df[df['released_year'] == 2023]

# Select the required columns and ensure streams are numeric
df_filtered = df_2023[['track_name', 'artist(s)_name', 'streams']].astype(
    {'streams': pd.to_numeric, 'streams': pd.to_numeric, 'streams': pd.to_numeric},
    errors='coerce'
)

# Sort the data by streams column in descending order
df_sorted = df_filtered....",7,"The generated code correctly loads the data, filters by year, selects columns, and sorts by streams. It covers almost all requirements.

However, there's a significant issue in the line:
`df_filtered = df_2023[['track_name', 'artist(s)_name', 'streams']].astype({'streams': pd.to_numeric, 'streams': pd.to_numeric, 'streams': pd.to_numeric}, errors='coerce')`

1.  **Redundant `astype` mapping**: Specifying `'streams': pd.to_numeric` multiple times in the dictionary for the same column is redundant and doesn't add value. A single entry is sufficient.
2.  **`astype` behavior**: While `astype` with `errors='coerce'` *can* work, `pd.to_numeric(df['streams'], errors='coerce')` applied directly to the series is a more common and often clearer way to handle the coercion of a single column, especially before filtering/selection if the original dataframe's `streams` column needs to be made numeric. In this case, `astype({'streams': float}, errors='coerce')` would have been a more idiomatic pandas way to convert to a numeric type, which would then implicitly use `to_numeric` under the hood.

The ground truth code addresses the `streams` coercion earlier, which is generally a better practice if that column is used for filtering or sorting later, ensuring its type is consistent from the start. Also, the generated code doesn't handle `FileNotFoundError` or other potential exceptions during CSV loading, which the ground truth does. This makes the generated code less robust for real-world scenarios.",command-r7b-12-2024,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Histogram for 'bpm' column
plt.figure(figsize=(8, 6))
df['bpm'].hist( bins=20, edgecolor='black')
plt.title('BPM Distribution')
plt.xlabel('BPM')
plt.ylabel('Frequency')
plt.show()

# Scatter plot for 'danceability_%' vs. 'energy_%%'
plt.figure(figsize=(10, 6))
df[['danceability_%', 'energy_%']].plot(kind='scatter', c='blue')
plt.title('Danceability vs. Energy')
plt.xlabel('D...",7,"The generated code correctly loads the dataset, generates a histogram for 'bpm', and a scatter plot for 'danceability_%' vs 'energy_%'. It also includes titles and labeled axes for both plots and proper imports.

However, there's a significant issue with the scatter plot generation. The line `df[['danceability_%', 'energy_%']].plot(kind='scatter', c='blue')` will produce an error because the `plot` method when called on a DataFrame with a list of columns expects one column for x and one for y in the `scatter` kind. It does not automatically infer the x and y columns from the list. The current version will try to plot `danceability_%` against its index and `energy_%` against its index, which is not the desired scatter plot between the two columns.

To fix this, the scatter plot should explicitly define x and y axes, like `df.plot(kind='scatter', x='danceability_%', y='energy_%', c='blue')` or `plt.scatter(df['danceability_%'], df['energy_%'], c='blue')`.

Additionally, the generated code calls `plt.show()` twice, once after each plot. While this works, it means the plots will appear sequentially and require user interaction to close the first one before viewing the second. The ground truth code correctly shows all plots at once at the end using a single `plt.show()`, which is generally better practice for multiple figures in a script.

The axis labels for the scatter plot have a typo: `energy_%%` should be `energy_%`.",command-r7b-12-2024,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd

# 1) Load the dataset and list all column names and their inferred data types
df = pd.read_csv('youtube_trendings.csv')
column_dtypes = df.dtypes.apply(lambda x: x.name if isinstance(x, pd. Madhatter) else str(x)).to_dict()
print(f""Column Names and Data Types:\n{column_dtypes}"")

# 2) Clean and type-cast the specified columns
df[['Video views', 'Likes', 'Dislikes']] = df[['Video views', 'Likes', 'Dislikes']].apply(pd.to_numeric, errors='coerce').fillna(0)

# 3) Cal...",5,"The generated code has several critical issues that prevent it from fully satisfying the task requirements.

1.  **Incorrect Cleaning of Numeric Columns**: The most significant flaw is in step 2. The code attempts to directly apply `pd.to_numeric` to the columns `['Video views', 'Likes', 'Dislikes']` without first *removing the commas*. The task explicitly states: ""remove any commas, convert them to numeric types"". For `youtube_trendings.csv`, these columns often contain commas (e.g., ""1,234,567""). `pd.to_numeric` will fail to convert these string representations directly, leading to `NaN` values, and then these `NaN`s are filled with 0. This results in incorrect numeric data after cleaning. The ground truth correctly handles this by `str.replace(',', '')` first.

2.  **Incorrect Data Type Reporting (Step 1)**: The code tries to report data types using `df.dtypes.apply(lambda x: x.name if isinstance(x, pd. Madhatter) else str(x))`. This is problematic. `pd.Madhatter` is not a standard pandas type or module, indicating a potential hallucination or misunderstanding. The correct way to get inferred data types is simply `df.dtypes` or, for a more comprehensive view, `df.info()`. The output format for column names and dtypes is also different from what `df.info()` would provide, which is generally preferred as it is a standard data exploration tool.

3.  **Missing `mean`, `median`, `std`, `min`, `max` in Descriptive Statistics**: While `df[column].describe()` is used, which `does` include these statistics, the output format is not explicitly presenting them as separate values (e.g., ""Mean: X, Median: Y""). The task asked to ""Calculate and report basic descriptive statistics (mean, median, std, min, max)"". The current output dumps the entire `describe()` series, which is less precise than requested.

4.  **No Error Handling for File Not Found**: The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is good practice. The generated code lacks this.

5.  **Lack of Intermediate Information/Print Statements**: The ground truth provides helpful print statements showing the progress of cleaning, initial and final NaN counts, etc. While not strictly required by the *output* of the task, it contributes to better code quality and understanding of its execution, especially in a ""benchmark"" scenario where clarity is valuable.

In summary, the code fundamentally fails on the cleaning step for numeric columns, which is a core requirement of the task. The data type reporting is also flawed.",command-r7b-12-2024,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Step 1: Clean 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '').astype(float)
df['Video views'] = df['Video views'].fillna(0.0)

# Step 2: Filter videos with more than 50,000,000 views
df = df[df['Video views'] > 50000000]

# Step 3: Select and display 'Video' and cleaned 'Video views'
df[['Video', 'Video views']] = df[['Video', 'Video views']]

# Step 4: Sort by cleaned '...",7,"The generated code largely addresses the requirements, but it has a crucial flaw in the 'Video views' column cleaning, specifically in handling conversion errors.

**Correctness:**
- **Issue:** The line `df['Video views'] = df['Video views'].str.replace(',', '').astype(float)`
  - This line assumes that all values can be converted to float after removing commas. If `Video views` contains non-numeric strings other than just commas, `astype(float)` will raise an error and crash the script.
  - The ground truth correctly uses `pd.to_numeric(..., errors='coerce')` which is robust as it converts non-numeric values to `NaN` instead of raising an error, making the subsequent `fillna(0)` effective in catching these cases.
- **Other aspects:** Filtering, sorting, and column selection are correctly implemented.

**Code Quality:**
- Imports are proper.
- Structure is logical and follows the steps outlined in the task.
- Variable naming is clear.

**Completeness:**
- All steps (loading, cleaning, filtering, selecting, sorting) are attempted.
- The `try-except` block for `FileNotFoundError` present in the ground truth is a good practice for robustness in real-world scenarios, which the generated code lacks. While not explicitly asked for, it's a mark of more complete and robust code.

**Areas for Improvement:**
- **Robust error handling for numeric conversion:** Change `astype(float)` to `pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce')` to prevent crashes when encountering truly unconvertible string values. This is the most significant fix needed.
- **`FileNotFoundError` handling:** Although not explicitly requested, adding a `try-except` block for `pd.read_csv` would make the code more robust against missing files.
- **Final display:** The generated code directly `print(df)` which is correct, but reassigning the filtered and sorted DataFrame back to `df` might not be ideal if one needed to preserve the original DataFrame or intermediate steps. The ground truth uses separate variables (`filtered_df`, `selected_columns_df`, `sorted_df`) which can sometimes be clearer for debugging or if further operations on intermediate states are needed. (This is a minor stylistic point, not a correctness issue).",command-r7b-12-2024,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the YouTube trending dataset
df = pd.read_csv('youtube_trendings.csv')

# Step 1: Clean the 'Likes' and 'Video views' columns
df['Likes'] = df['Likes'].str.replace(',', '').astype(float)
df['Video views'] = df['Video views'].str.replace(',', '').astype(float)

# Handle potential conversion failures by filling with NaN and then with 0
df['Likes'] = df['Likes'].fillna(0).astype(float)
df['Video views'] = df['Video views'].fillna...",7,"The generated code correctly performs the core tasks of loading the data, cleaning the specified columns, and generating the scatter plot with the requested title and labels. Proper imports are included.

However, there's a significant issue in the cleaning logic for handling conversion failures. The line `df['Likes'] = df['Likes'].str.replace(',', '').astype(float)` directly tries to convert to float. If there are non-numeric characters (after comma removal) that Pandas cannot automatically coerce to NaN, this `astype(float)` will raise an error, crashing the script. The task specifically requires that *if conversion fails for any value, treat that value as NaN*, which `pd.to_numeric(..., errors='coerce')` handles gracefully. The current implementation does not.

The subsequent `fillna(0).astype(float)` calls are redundant if the initial `astype(float)` works, and they are misplaced because the `errors='coerce'` behavior is missing. If the initial `astype(float)` fails, these lines will not even be reached.

While the plot is generated, there are no error handling mechanisms for file loading (e.g., `FileNotFoundError`), which is good practice. The plot also lacks visual enhancements like `alpha` or `s` for points, or a grid, but these were not explicitly mandatory for this specific task.

To improve, the most critical fix is to use `pd.to_numeric(..., errors='coerce')` for robust numeric conversion as specified in the prompt.",command-r7b-12-2024,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""mobiles_2025.csv"")

# Step 1: Clean the 'Mobile Weight' column
df['Mobile Weight'] = df['Mobile Weight'].str.replace('g', '').astype(float)
df['Mobile Weight'].fillna(df['Mobile Weight'].mean(), inplace=True)

# Step 2: Clean the 'RAM' column
df['RAM'] = df['RAM'].str.replace('GB', '').astype(float)
df['RAM'].fillna(df['RAM'].mean(), inplace=True)

# Step 3: Clean the 'Battery Capacity' column
df['Battery Capacity'] = df['Batter...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being a perfect solution.

**Correctness:**
*   **Conversion Errors:** The `astype(float)` without `errors='coerce'` will raise an error if any non-numeric value (after stripping units) is encountered, rather than setting it to NaN as required. This breaks the execution for 'Mobile Weight', 'RAM', and 'Battery Capacity'. Only 'Launched Price (USA)' uses `errors='ignore'`, which is not the correct parameter for `astype` (it should be `errors='coerce'` if used with `pd.to_numeric`).
*   **NaN Handling:** The task explicitly asks to handle conversion errors by ""setting to NaN"". The generated code then *fills* these NaNs with the mean using `fillna(... inplace=True)`. While this is a common data cleaning step, it was *not* requested by the prompt. The prompt only asked to handle conversion errors by setting to NaN, implying these NaNs should remain for the statistics calculation unless otherwise specified. Filling them changes the statistics.
*   **`df.info()` vs `df.dtypes`:** The task asks to ""List all column names and their inferred data types"", `df.info()` provides more than just the dtypes (memory usage, non-null counts), whereas `df.dtypes` is a direct output of what was requested.

**Code Quality:**
*   **Error Handling for File Loading:** No error handling for `pd.read_csv`, which means a `FileNotFoundError` would crash the script.
*   **Redundant `inplace=True`:** While not strictly 'bad quality', `inplace=True` is often discouraged in pandas as it makes chaining operations harder and can sometimes lead to unexpected behavior. Directly assigning the result of `fillna` back to the column is often preferred.
*   **Readability/Structure:** The code is a linear script. encapsulation into functions and better modularity (as seen in the ground truth) would improve readability and maintainability for larger tasks.

**Completeness:**
*   All steps are attempted, but step 1, 2, 3, and 4 are not correctly implemented regarding error handling (`errors='coerce'`) and the subsequent `fillna` was not requested.
*   Step 5 is completed, but `df.info()` is not the most direct or specific method to fulfill ""List all column names and their inferred data types"".
*   Step 6 calculates statistics, but they might be skewed due to the unsolicited `fillna` operation.
*   Step 7 is correctly implemented.

To improve, the code needs to correctly use `pd.to_numeric(..., errors='coerce')` for all relevant numeric conversions and remove the `fillna` steps since they were not requested by the prompt. Explicitly using `df.dtypes` for step 5 would also be a more precise match for the requirement.",command-r7b-12-2024,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the 'Launched Price (India)' column
df['Launched Price (India)'] = df['Launched Price (India)'].str.replace('INR ', '').str.replace(',', '').astype(float, errors='ignore')

# Clean the 'RAM' column
df['RAM'] = df['RAM'].str.replace('GB', '').astype(float, errors='ignore')

# Filter the dataset
filtered_df = df[(df['Company Name'] == 'Apple') & (df['Launched Price (India)'].notna()) & (df['Launched Pri...",8,"The generated code correctly loads the dataset, cleans the specified columns, filters the data, selects the required columns, and sorts the result.

**Strong points:**
- Correctly uses `pandas.read_csv`.
- Efficiently chains string cleaning methods(`.str.replace()`) for `Launched Price (India)`.
- Uses `astype(float, errors='ignore')` which is a good attempt at handling errors, though `pd.to_numeric(errors='coerce')` is generally more robust for this specific type of task as it explicitly handles invalid parsing to `NaN`.
- Correctly filters for 'Apple' and price `< 100000`.
- Correctly selects and sorts the columns in one line.

**Areas for improvement (to achieve a perfect score):**
- **Error Handling for `errors='ignore'` vs `errors='coerce'`:** The `astype(float, errors='ignore')` for cleaning columns is problematic. `errors='ignore'` will return the original (unparsed) value if conversion fails, instead of converting it to `NaN`. This means if a non-numeric string (that doesn't match the regex patterns) remains in the column, it will not be converted to float, potentially leading to errors later or incorrect filtering. The task explicitly stated ""Handle any conversion errors by setting to NaN,"" which is better achieved using `pd.to_numeric(errors='coerce')`.
- **Filtering Logic:** The filter `(df['Launched Price (India)'].notna())` was added in the generated code. While it's not strictly incorrect, the task implicitly states to filter by price `< 100000`, and if `pd.to_numeric(errors='coerce')` was used, `NaN` values would automatically fail the `< 100000` condition. So, this extra `notna()` check is redundant if `coerce` is used correctly, but necessary given `ignore` was used.
- **Robustness in String Cleaning:** The ground truth converts to `str` first (`astype(str)`) before applying string operations. While pandas `str` accessor generally handles non-string types for some operations, explicitly converting to string first is a more robust practice, especially for columns that might contain mixed types.
- **File Not Found Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for real-world scenarios. The generated code lacks this.
- **Clarity/Readability:** While concise, breaking down the cleaning steps (like in the ground truth for `Launched Price`) can sometimes improve readability and debuggability, especially for more complex cleaning. However, for these relatively simple operations, chaining is acceptable.",command-r7b-12-2024,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv(""mobiles_2025.csv"")

# Clean the 'RAM' column
df['RAM'] = df['RAM'].str.replace('GB', '').astype(float).fillna(pd.NA)

# Clean the 'Battery Capacity' column
df['Battery Capacity'] = df['Battery Capacity'].str.replace('mAh', '').astype(float).fillna(pd.NA)

# Create a scatter plot
plt.scatter(df['RAM'], df['Battery Capacity'], c=df['Company Name'])

# Set plot title and labels
plt.title('Relationshi...",6,"The generated code correctly loads the dataset and attempts to clean the `RAM` and `Battery Capacity` columns. It also creates a scatter plot with a title and axis labels.

However, there are several key issues that prevent a higher score:

1.  **Error Handling for `pd.read_csv`:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes using a `try-except` block.
2.  **Robust Cleaning for `RAM` and `Battery Capacity`:**
    *   `df['RAM'].str.replace('GB', '').astype(float)`: This approach is problematic because `astype(float)` will raise an error if any non-numeric characters remain after the replacement (e.g., if a value was `8GB and something else`). `pd.to_numeric(errors='coerce')` is the correct and robust way to handle conversion errors by turning them into `NaN`, as explicitly required by the prompt (""Handle any conversion errors by setting to NaN""). The current approach will fail if there are any non-numeric strings that just don't have 'GB' in them.
    *   `fillna(pd.NA)`: While this attempts to fill NaNs, `astype(float)` directly converts to `float64` where `NaN` is implicitly handled. `pd.NA` specifically refers to the new pandas missing data type, but using `np.nan` (or simply relying on `errors='coerce'`) is more standard when dealing with float types and `pd.NA` might not be fully compatible with all float operations in the same way `np.nan` is.
    *   The ground truth also explicitly converts to `str` first (`astype(str)`) to ensure the `.str` accessor works reliably and uses `.str.strip()` to remove leading/trailing whitespace, making the cleaning more robust.
3.  **Coloring by `Company Name`:** `c=df['Company Name']` in `plt.scatter` will likely result in an error or an unintuitive plot. `matplotlib.pyplot.scatter` expects `c` to be a color, sequence of colors, or a 1D array of values that can be mapped to colors via a colormap. It does not automatically handle categorical string data for coloring like Seaborn's `hue` parameter does. The ground truth correctly uses `seaborn.scatterplot` with the `hue` parameter, which is designed for this exact use case.
4.  **Clarity of Axis Labels:** The axis labels `RAM` and `Battery Capacity` are a bit too generic compared to `RAM (GB)` and `Battery Capacity (mAh)` in the ground truth, which specify the units as implied by the cleaning step.
5.  **Plot Enhancements:** The ground truth also includes better plot enhancements (larger figure size, `s` for marker size, `alpha` for transparency, a grid, and `tight_layout`), which contributes to better visualization but are not strictly required by the prompt for correctness.",command-r7b-12-2024,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 5 rows
titanic_data = pd.read_csv('titanic.csv')
print(titanic_data.head())

# 2) Show the shape of the dataset (rows and columns)
print(titanic_data.shape)

# 3) List all column names and their data types
print(titanic_data.info())

# 4) Calculate the survival rate (percentage of passengers who survived)
survival_rate = (titanic_data['Survived'].value_counts(normalize=True) * 100).round(2)
print(f""Survival Rate: {surviva...",8,"The generated code correctly performs most of the requested tasks.

**Strong points:**
*   **Correctness:** It correctly loads the data, displays head, shows shape, calculates missing values.
*   **Survival Rate Calculation (Alternative):** The `value_counts(normalize=True)` approach for survival rate is a valid and often more concise way to get percentages for categorical columns, especially if you wanted rates for both survived and not survived.
*   **Conciseness:** The code is quite concise.

**Areas for Improvement:**
*   **Output Clarity:** The generated code lacks the introductory `print()` statements (e.g., ""1. First 5 rows of the dataset:"") that were present in the ground truth. This makes the output less clear to follow step-by-step. While not strictly a correctness issue, it significantly impacts user experience for an EDA script.
*   **`df.info()` output:** When `titanic_data.info()` is called with `print()`, it prints the info and then `None` (because `info()` returns `None`). This is a minor stylistic issue.
*   **Survival Rate Display:** The survival rate line `print(f""Survival Rate: {survival_rate}%"")` will print the `value_counts` Series, not just the survival percentage (e.g., `Survived\n1    38.38\n0    61.62\nName: Survived, dtype: float64%`). The request was for ""percentage of passengers who survived"", implying a single number, not the rates for both survived and not survived. To explicitly get *only* the survival percentage using the generated approach, it would need to index the series: `print(f""Survival Rate: {survival_rate.loc[1]}%"")`.
*   **Error Handling:** The ground truth included basic error handling for `FileNotFoundError`. While not explicitly requested in the prompt, it's good practice for robust code, especially when dealing with external files.

Overall, it's a good attempt that covers the core requirements, but could be improved in terms of output formatting and precision for specific results.",command-r7b-12-2024,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 1) List all column names and their data types
print(""Column names and data types:"")
for column in df.columns:
    print(f""{column}: {df[column].dtype}"")

# 2) Calculate and report basic descriptive statistics
numerical_columns = ['Year', 'Rating', 'Runtime']
for column in numerical_columns:
    print(f""Descriptive statistics for {column}:"")
    print(df[column].describe())

# 3) Report the total number of m...",6,"The generated code addresses most of the requirements but has several shortcomings regarding robustness, completeness, and precision.

**Strengths:**
*   Correctly loads the CSV file.
*   Correctly identifies and prints column names and their data types (though the output format is slightly less concise than `df.info()`).
*   Correctly calculates and reports the total number of movies.
*   Correctly uses `df.describe()` for numerical columns.

**Areas for Improvement / Shortcomings:**

1.  **Robustness (File Handling):** The code lacks error handling for `FileNotFoundError`. If `imdb_top_250.csv` is not present, the script will crash. The ground truth includes a `try-except` block for this.
2.  **Preprocessing `Runtime`:** The most significant issue is the lack of handling for the `Runtime` column. In many datasets, `Runtime` is stored as a string (e.g., ""142 min""). The ground truth demonstrates how to convert this to a numeric type, which is crucial for `df.describe()` to work correctly on it. Without this, `df['Runtime'].describe()` on an object column will produce very different statistics (count, unique, top, freq) than numerical ones. The prompt specifically asks for numerical statistics for `Runtime`.
3.  **Specific Statistics:** The task asks for ""basic descriptive statistics (mean, median, standard deviation, min, max)"". While `df.describe()` provides these and more, the ground truth specifically filters for `mean`, `50%` (median), `std`, `min`, `max` and renames `50%` to `median` for clarity, adhering more strictly to the requested set of statistics. The generated code prints all of `df.describe()` output.
4.  **Clarity of Output:** The output for column data types is verbose (one line per column) compared to `df.info()`, which provides a concise summary and is more standard for data understanding.
5.  **Implicit Assumptions:** The code implicitly assumes `Year`, `Rating`, and `Runtime` will always be numerical and present. The ground truth builds in checks to handle cases where these columns might be missing or not numeric, removing them from the analysis list if problematic.

To improve, add robust file handling, implement preprocessing for the `Runtime` column to ensure it's numeric, and refine the descriptive statistics output to specifically match the requested metrics.",command-r7b-12-2024,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""imdb_top_250.csv"")

# Filter the dataset to include only movies with a Rating greater than or equal to 9.0
filtered_df = df[df[""Rating""] >= 9.0]

# Select and display the Title and Rating columns
result = filtered_df[[""Title"", ""Rating""]]
print(result)

# Sort the resulting data by Rating in descending order
sorted_result = result.sort_values(by=""Rating"", ascending=False)
print(sorted_result)
```",9,"The generated code correctly performs all the required operations: loads the CSV, filters by rating, selects the specified columns, and sorts in descending order. The imports are correct, and the code structure is logical and easy to understand.

Minor points for improvement:
- The task asked to ""display"" the selected columns and the sorted results. The generated code prints `result` then `sorted_result`. While both are displayed, typically the final sorted output is what's solely intended for display. Printing `result` before `sorted_result` is redundant. Removing the first `print(result)` would make it more concise.
- The ground truth includes robust error handling (FileNotFoundError, KeyError, general Exception), which is good practice for production-ready code, especially when dealing with file I/O and expected column names. The generated code lacks this, making it less robust. However, the prompt did not explicitly ask for error handling, so its omission doesn't detract significantly from the direct task fulfillment.",command-r7b-12-2024,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create a histogram for the Rating distribution
sns.histplot(df['Rating'], bins=20, kde=True)
plt.title('Distribution of Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# Create a scatter plot for Rating vs. Runtime
sns.scatterplot(x='Runtime', y='Rating', data=df)
plt.title('Relationship between Rating and Runtime')
plt.xlabel('Runtime...",7,"The generated code correctly loads the CSV, creates a histogram for 'Rating', and a scatter plot for 'Rating' vs. 'Runtime', including titles and labels as requested. The imports are proper.

However, it implicitly assumes the 'Runtime' column is already in a clean numeric format. The ground truth code includes robust preprocessing for the 'Runtime' column (extracting numeric values from strings like '120 min' and handling potential errors), which is a common requirement when dealing with real-world datasets like 'imdb_top_250.csv' where 'Runtime' is often a string. Without this, the scatter plot might fail or produce incorrect results if 'Runtime' is not purely numeric (e.g., '120 min'). This lack of robustness for a common data quality issue prevents a higher score. It also lacks error handling for `FileNotFoundError` for the CSV.",command-r7b-12-2024,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the penguins.csv dataset
df = pd.read_csv('penguins.csv')

# Report the number of missing values (NA) for each column
missing_values = df.isnull().sum()
print(missing_values)

# Drop rows with missing values in the specified columns
df = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])

# Calculate and report basic descriptive statistics for the numerical columns
numerical_columns = ['culmen_length_mm', 'culmen_de...",7,"The generated code correctly performs the core data manipulation tasks: loading the dataset, calculating NA counts, dropping NAs for specific columns, computing descriptive statistics, and counting unique values.

However, it falls short in terms of robustness and completeness compared to the ground truth.
1.  **Missing Value Reporting:** The generated code prints `df.isnull().sum()` which includes columns with zero missing values. The ground truth filters this to show only columns that actually have missing values, which is generally more informative.
2.  **Descriptive Statistics:** The `describe()` method provides more statistics than just mean, median, std, min, max (e.g., quartiles, count). While `describe()` is convenient, the prompt specifically requested mean, median, std, min, max. The ground truth uses `agg()` which precisely addresses this.
3.  **Error Handling:** The generated code lacks any error handling for `FileNotFoundError` when loading the CSV, which is crucial for robust code.
4.  **Clarity/Readability:** The ground truth includes more descriptive print statements and clear sectioning, making the output and logic easier to follow.
5.  **Intermediate Output:** The ground truth explicitly reports the number of rows before and after filtering, which provides valuable context for the `dropna` operation.

To improve, the code should:
-   Refine the missing value reporting to only show columns with actual missing values.
-   Use `agg()` for descriptive statistics to match the exact requirements more closely.
-   Add `try-except` blocks for file loading.
-   Include more informative print statements.",command-r7b-12-2024,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

# Load the penguins.csv dataset
penguins = pd.read_csv('penguins.csv')

# Filter the dataset
filtered_penguins = penguins[(penguins['species'] == 'Adelie') & (penguins['island'] == 'Torgersen')]

# Select only the 'culmen_length_mm' and 'culmen_depth_mm' columns
selected_columns = filtered_penguins[['culmen_length_mm', 'culmen_depth_mm']]

# Drop any rows with missing values
cleaned_data = selected_columns.dropna()

# Display the first 5 rows of the resulting DataF...",9,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, filters by species and island, selects the specified columns, drops rows with missing values, and displays the first 5 rows. The imports are proper, and the code structure is clear and logical.

The only minor deviation from the ground truth and a subtle point in the requirements is related to `dropna()`. The ground truth explicitly uses `subset=['culmen_length_mm', 'culmen_depth_mm']` for `dropna()`, ensuring only NA values in those specific columns are considered for dropping rows. The generated code uses `dropna()` without a `subset` argument. In this specific scenario, since `selected_columns` *only* contains `culmen_length_mm` and `culmen_depth_mm`, `dropna()` without `subset` will behave identically to `dropna(subset=...)`. However, for robustness and to strictly adhere to the prompt's implied precision (""if either `culmen_length_mm` or `culmen_depth_mm` has a missing value""), specifying the `subset` is generally better practice, even if it leads to the same outcome here. This is a very minor detail and doesn't affect the correctness of the output for this particular task given the preceding column selection.

One other minor point is the `print()` statement in the ground truth is more verbose (e.g., ""First 5 rows of the processed DataFrame:""), which is good for user clarity in a real application but not strictly part of the coding task itself. The generated code also accurately calls `head()` without an argument, which defaults to 5, perfectly matching the requirement to display the first 5 rows.",command-r7b-12-2024,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the penguins.csv dataset using pandas
penguins_df = pd.read_csv('penguins.csv')

# Create a scatter plot of `culmen_length_mm` (x-axis) versus `culmen_depth_mm` (y-axis)
# Color the points according to the `species` column
penguins_df[penguins_df.isnull().any(axis=1)].shape[0]  # Number of rows with missing values

# Ensure the plot has a title and labels for the x-axis and y-axis
plt.scatter(penguins_df['culmen_length_mm'], p...",4,"The generated code fails to meet several key requirements of the task.

1.  **Filtering for NA values (Critical Failure):** The prompt explicitly states: ""The plot should only include data points for which `culmen_length_mm`, `culmen_depth_mm`, AND `species` are all non-missing (not NA)."" The generated code completely misses this requirement. It loads the data and directly plots it without any `dropna()` calls on the specified columns. The line `penguins_df[penguins_df.isnull().any(axis=1)].shape[0]` merely calculates the number of rows with *any* missing value, but it does not *filter* the DataFrame used for plotting. This is a critical omission.

2.  **Coloring by Species (Incorrect Implementation):** While `c=penguins_df['species']` attempts to color by species, `matplotlib.pyplot.scatter` does not inherently know how to map categorical string values (like species names) to distinct colors automatically without further processing (e.g., mapping them to numerical categories or using a tool like seaborn which handles this natively). This will likely result in an error or an unusable plot if the species column contains string values, unless `matplotlib` has been updated to handle this directly, which is not standard. The ground truth leverages `seaborn.scatterplot` which is designed for this exact purpose with the `hue` parameter.

3.  **Use of Matplotlib vs. Seaborn (Missed Opportunity/Incorrect Choice):** While the prompt allowed either, and matplotlib could be made to work, `seaborn` is explicitly designed for statistical plotting and handles categorical coloring and legends much more gracefully. The generated code uses basic `matplotlib.pyplot.scatter` which is less suitable for the `hue` requirement without manual categorical-to-numeric mapping. Even if the plot renders, it won't automatically generate a legend for species.

4.  **Completeness (Missing Legend):** Due to the incorrect way of coloring by species, the plot generated by the code will lack a legend that maps species names to colors, which is crucial for interpretability and implicitly required when coloring by a categorical variable like 'species'.

In summary, the code loads the data and attempts a scatter plot with a title and labels, but critically fails on the data filtering and correctly coloring/legending by a categorical variable as specified in the prompt.",command-r7b-12-2024,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

# 1) Read the dataset
penguins = pd.read_csv('penguins.csv')

# 2) Fill NA values in numerical columns with the mean
penguins['culmen_length_mm'].fillna(penguins['culmen_length_mm'].mean(), inplace=True)
penguins['culmen_depth_mm'].fillna(penguins['culmen_depth_mm'].mean(), inplace=True)

# 3) Calculate and print summary statistics
print(""Culmen Length (mm):"")
print(f""Mean: {penguins['culmen_length_mm'].mean():.2f}, Median: {penguins['culmen_length_mm'].median():.2...",9,"The generated code correctly addresses all requirements of the task. It loads the `penguins.csv` dataset, correctly fills NA values in the specified numerical columns with their respective means, calculates and prints the mean, median, and standard deviation for these cleaned columns, and finally prints the total number of rows and columns.

**Strong Points:**
*   **Conciseness:** The `inplace=True` usage for `fillna` makes the code very succinct for the imputation step.
*   **Correctness:** All calculations and data manipulations are accurate as per the task description.
*   **Readability:** The print statements are clear and the code flow is logical.

**Areas for Minor Improvement (to reach a perfect 10, aligning more closely with best practices for robust code):**
*   **Error Handling:** The ground truth includes `try-except FileNotFoundError`, which is good practice for robust code that might be run in different environments or with missing files. The generated code assumes the file will always be present.
*   **Column Validation:** The ground truth checks if columns exist (`if col in df.columns:`). While not strictly necessary for this specific problem (as we know the columns exist in `penguins.csv`), it's a good practice for more generalizable code that might be used with different datasets where column names could vary or be misspelled.
*   **Reusability/Modularity:** Using variables for column names (`NUMERICAL_COLUMNS`) and iterating through them, as in the ground truth, would make the code more scalable and easier to modify if more columns needed similar processing. Currently, the `fillna` and statistics calculation steps are hardcoded individually for each column.",command-r7b-12-2024,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
penguins_df = pd.read_csv(""penguins.csv"")

# 2) Drop rows where `flipper_length_mm` is NA
penguins_df = penguins_df.dropna(subset=[""flipper_length_mm""])

# 3) Create a histogram of the `flipper_length_mm` column
plt.hist(penguins_df[""flipper_length_mm""], bins=30, alpha=0.7, color=""blue"")
plt.title(""Distribution of Flipper Length (mm)"")
plt.xlabel(""Flipper Length (mm)"")
plt.ylabel(""Frequency...",7,"The generated code correctly performs most of the required tasks.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly handles NA values in `flipper_length_mm` by dropping rows.
*   Successfully generates a histogram for `flipper_length_mm` with the correct title and x-axis label.
*   Attempts to create a bar chart for `island` counts with the correct title and labels.

**Areas for Improvement (why it didn't get a higher score):**

*   **Bar Chart for Island Counts:** The bar chart implementation for the `island` column is incorrect. `sns.barplot(x=""island"", y=""count"", data=penguins_df, palette=""viridis"")` will lead to an error because there is no 'count' column in the `penguins_df` DataFrame. The correct way to get counts of categorical data for a bar chart is to use `value_counts()` and then plot it (e.g., `penguins_df['island'].value_counts().plot(kind='bar')` or more idiomatically `sns.countplot(x='island', data=penguins_df)`).
*   **Histogram Y-axis Label:** While the x-axis and title are correct for the histogram, the y-axis is labeled ""Frequency"" when it should be ""Count"" or ""Number of Observations"" for `plt.hist` by default, or more specifically ""Count"" as in the ground truth.
*   **Robustness:** The code lacks error handling for `FileNotFoundError` when loading the CSV, which is a good practice for data loading steps.
*   **Code Style/Best Practices (Minor):** The histogram uses `plt.hist` directly, which is functional but `seaborn.histplot` (as in the ground truth) often provides more visually appealing and informative plots (e.g., with KDE). Similarly, for count plots, `seaborn.countplot` is generally preferred over manually calculating `value_counts()` and plotting. While not explicitly asked for, basic `plt.figure(figsize=...)` for better plot sizing would also enhance readability.
*   **Clarity of variable name:** `penguins_df` is slightly redundant compared to `df` if only one DataFrame is used.

Overall, it's a good attempt, but the critical error in generating the bar chart prevents it from being fully correct.",command-r7b-12-2024,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# 1) Load the dataset
penguins = pd.read_csv('penguins.csv')

# 2) Filter the data to include only penguins from the 'Torgersen' island
torgersen_penguins = penguins[penguins['island'] == 'Torgersen']

# 3) For this filtered subset, count and print the number of penguins belonging to each `species`
species_count = torgersen_penguins['species'].value_counts()
print(species_count)
```",8,"The generated code correctly solves the task by loading the dataset, filtering for 'Torgersen' island, and counting species occurrences. It is concise and directly addresses all requirements.

However, the code lacks robustness compared to the ground truth. Specifically:
- **Error Handling:** It does not include `try-except` blocks for `FileNotFoundError` when loading the CSV, which is crucial for real-world applications.
- **Column Existence Checks:** It implicitly assumes the 'island' and 'species' columns exist. If these columns were missing, the code would raise a `KeyError`, whereas the ground truth handles this gracefully.
- **Empty Filtered DataFrame Handling:** The code doesn't explicitly check if `torgersen_penguins` is empty after filtering. While `value_counts()` on an empty Series returns an empty Series which is not an error, a user might appreciate a more explicit message if no penguins are found on Torgersen island.

Despite these omissions in robustness, the core logic is sound and achieves the stated goal efficiently.",command-r7b-12-2024,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

print(df.shape)
print(df.dtypes)
```",10,"The generated code is correct, complete, and of good quality. It precisely defines the requested pandas DataFrame, accurately prints its shape, and correctly lists the data types of all columns as required by the task. The code is concise and directly addresses all requirements. There are no unnecessary additions or omissions. The generated code perfectly matches the essential operational parts of the ground truth code.",command-r7b-12-2024,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the Titanic dataset
titanic_df = pd.read_csv('titanic.csv')

# 1) Survivors vs Non-survivors Bar Chart
survivor_counts = titanic_df['Survived'].value_counts()
non_survivor_counts = 1 - survivor_counts

plt.bar(['Survivors', 'Non-Survivors'], [survivor_counts, non_survivor_counts])
plt.title('Survivor vs Non-Survivor Count')
plt.xlabel('Category')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

# 2) Passenger Class Distr...",4,"The generated code has significant issues that prevent it from correctly fulfilling the prompt's requirements.

1.  **Visualization 1 (Survivors vs Non-survivors):**
    *   `non_survivor_counts = 1 - survivor_counts` is fundamentally incorrect. It tries to subtract the `value_counts` Series from an integer `1`, which will result in an error or unexpected behavior, not the count of non-survivors. The prompt asked for ""count of survivors vs non-survivors"", which implies two distinct counts, not a survival rate.
    *   The bar chart plotting directly uses these incorrect `survivor_counts` and `non_survivor_counts` in a way that won't work as intended for a series.
    *   The `kind='bar'` method on the pandas Series (as used in the ground truth) is a much more idiomatic and concise way to plot value counts.

2.  **Visualization 3 (Passenger Age Histogram):**
    *   **Missing Value Handling:** The prompt specifically asked to ""handle missing values appropriately"" for age. The generated code uses `titanic_df.dropna(subset=['Age'])`, which *removes* rows with missing age values. While this is a form of handling, the ground truth and typically better practice for histograms is to *impute* missing values (e.g., with median or mean) to retain more data, especially since the task was about distribution, not a specific analysis requiring complete cases. Explicitly, the prompt asked to ""handle missing values appropriately"", and dropping them permanently for *all subsequent plots* without a proper reason is not always appropriate, especially when median imputation is a common and better method for histograms.
    *   **Histogram Plotting:** The plotting of the histogram is incorrect. It tries to use `age_counts.index` (which would be distinct ages) and `age_counts.values` (which would be the frequency of each distinct age) with `plt.hist`. `plt.hist` expects a 1D array of data points to *bin itself*. It does not take pre-counted values and their frequencies for the main data argument. `age_counts = titanic_df['Age'].value_counts()` is also not ideal for a histogram as it pre-counts distinct ages, losing the raw data distribution that `plt.hist` needs. The `bins=range(...)` and `xticks` are also overly complex and rigid, and `plt.hist` typically handles binning automatically or with a simple integer.

3.  **Overall Code Quality:**
    *   The code modifies the `titanic_df` in place by dropping rows for age, which might unexpectedly affect subsequent plots if they rely on the original dataset size or other columns that were removed. This is poor practice.
    *   Error handling for `FileNotFoundError` (as in the ground truth) is missing.
    *   Plot styling (figsize, grid, tight_layout, colors) is minimal compared to the ground truth, making the plots less clear.
    *   The X-axis labels for the first plot are manually hardcoded in a list, which is brittle and less flexible than using pandas' built-in plotting capabilities with mapped indices.

While it attempts to create four plots, the implementation details, especially for the survival count and age histogram, are incorrect or significantly suboptimal, failing to meet the ""expert code evaluator"" expectation for appropriate data handling and visualization techniques.",command-r7b-12-2024,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]
}

df = pd.DataFrame(data)

# Calculate and print the average 'Midterm_Score' ...",9,"The generated code correctly performs all the required tasks: it creates the DataFrame, calculates and prints the average midterm and final scores, and correctly identifies the subject with the highest average final score.

The code quality is good, with proper imports and a clear, logical structure. All requirements from the prompt are addressed.

The only minor point for improvement, which doesn't affect correctness but readability or minor optimization, is the lack of f-string formatting for the first two print statements, which the ground truth code uses to limit decimal places. However, this is a minor stylistic difference. The final print statement does use proper f-string formatting with `.2f`.",command-r7b-12-2024,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

# Sample data
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
    'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
    'Volume': [1000000, 1200000,...",8,"The generated code correctly creates the DataFrame, calculates the 'Daily_Range', and prints the DataFrame. It correctly identifies the date with the highest close price; however, there is a minor error in how it attempts to retrieve the 'Date' from the DataFrame. `df.loc[df['Close'].idxmax()]` correctly returns the row corresponding to the highest close price, and since 'Date' is the index, accessing `['Date']` on this row (which is a Series) will result in a `KeyError` or unexpected behavior because 'Date' is not a *column* in this context anymore.

The correct way to get the date when 'Date' is the index is simply `df['Close'].idxmax()`, which directly returns the index value (the Date). If the intent was to print the date as a string in a specific format, `strftime` could be used, similar to the ground truth.

Specific areas for improvement:
1. The line `highest_close_date = df.loc[df['Close'].idxmax()]['Date']` should be `highest_close_date = df['Close'].idxmax()`.
2. For printing the date, it could be formatted to be consistent (e.g., `strftime('%Y-%m-%d')`).",command-r7b-12-2024,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004'...",8,"The generated code correctly creates the DataFrame, calculates the 'Total_Revenue' column, and prints the first 5 rows. However, there's a slight misunderstanding in identifying the ""product sold the most in terms of Quantity"". The ground truth correctly sums quantities per product to find the globally best-selling product. The generated code uses `idxmax()` on `groupby('Product')['Quantity']` which identifies the *row index* within each product group that has the maximum quantity for *that specific product*, not the overall product with the highest *cumulative* quantity. This means `most_sold_product` might incorrectly pick a product that had one single large order, rather than a product that had many small orders summing up to a higher total quantity.

To improve, the logic for `most_sold_product` should be similar to the ground truth's `groupby('Product')['Quantity'].sum().idxmax()`.

Strong points:
- Correct DataFrame creation.
- Correct 'Total_Revenue' calculation.
- Correctly uses `df.head()` to print the first 5 rows.
- Appropriate imports and clear variable names.",command-r7b-12-2024,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 1) Calculate and print the average ""SessionDurationMinutes""
average_session_duration = df[""SessionDura...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the average ""SessionDurationMinutes"", and accurately counts the number of users for each ""DeviceType"". The code is clean, concise, and uses appropriate pandas methods for each task. The output format is also clear and easy to read. It fully meets all the requirements of the task.",command-r7b-12-2024,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

# Find and print the maximum temperature recorded across...",10,"The generated code is excellent. It correctly imports pandas, defines the DataFrame exactly as specified, and accurately calculates and prints both the maximum temperature and minimum humidity. The code is clean, concise, and directly addresses all requirements of the task. The output format for printing is slightly different (using f-strings vs. comma separation, and different phrasing), but both are acceptable and clearly convey the results.",command-r7b-12-2024,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# 1) Print the total number of ""Likes""
total_likes = df['Likes'].sum()
print(f""Total Likes: {total_likes}""...",9,"The generated code correctly defines the DataFrame, calculates the total number of ""Likes"", and identifies the ""PostType"" with the highest average number of ""Shares"". The variable names are clear, and the code structure is logical.

The only minor area for improvement is in the printing of the ""PostType"" with the highest average shares. The prompt asks to ""Find and print the 'PostType' that has the highest average number of 'Shares'"", and the ground truth satisfies this by just printing the PostType identified. The generated code prints both the PostType and its average shares, which is not strictly asked for. While this is arguably more informative, it deviates slightly from the exact wording of the prompt's request for that specific output. The ground truth output for the PostType is: `PostType with the highest average number of Shares: Video`, whereas the generated code outputs: `Most Shared Post Type: Video, Average Shares: 50.0`. This is a minor deviation and still answers the core question.",command-r7b-12-2024,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame with type annotations
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],...",9,"The generated code correctly performs all the required tasks: defining the DataFrame, calculating the average age, and counting patients by gender. The code is clean, well-structured, and uses appropriate pandas methods.

Areas for minor improvement (to achieve a 10):
1.  **Print Formatting for Average Age:** The ground truth code formats the average age to two decimal places (`.2f`), which is good practice for numerical ouptut, especially averages. The generated code prints the raw float.
2.  **Output Readability:** The ground truth adds ""years"" to the average age printout and uses a more descriptive f-string. While not strictly necessary for correctness, it enhances user readability.
3.  **Comments:** While the generated code has a couple of high-level comments, the ground truth code provides more detailed comments explaining each step, especially the purpose of `pd.to_datetime` and the rationale behind using `.mean()` and `.value_counts()`. This is particularly helpful for understanding the code's logic.

Overall, it's a very strong submission that fulfills all requirements.",command-r7b-12-2024,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}
df = pd.DataFrame(data)

# 1) Print the ""Title"" of the movie with the highest ""Rati...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately identifies the movie with the highest rating using `idxmax()` which is an efficient and concise method, and correctly calculates the average year. The code is clean, well-structured, and directly addresses all parts of the task. The print statements are clear and informative.",command-r7b-12-2024,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define car sales data
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

# Create DataFrame
df = pd.DataFrame(data)

# 1) Print the total numb...",10,"The generated code is correct, complete, and of good quality. It precisely defines the DataFrame as specified, correctly calculates the total number of cars using `len(df)` (which is a common and perfectly valid alternative to `df.shape[0]`), and accurately computes the average ""PriceUSD"" using `df[""PriceUSD""].mean()`. All required print statements are present and output the correct values. The code is clean and follows good practices for the task. It successfully addresses all aspects of the prompt.",command-r7b-12-2024,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}
df = pd.DataFrame(data)

# 1) Print the name of the Pokemo...",7,"The generated code correctly creates the DataFrame and identifies the Pokemon with the highest HP and lowest Defense. However, the output format for printing the names includes the HP/Defense value, which was not explicitly requested in the prompt (""Print the name of the Pokemon...""). While the logic is correct, the output slightly deviates from the precise request. Additionally, the approach `df[df[""HP""].idxmax()]` directly selects the row, which works but is slightly less explicit than `df.loc[idx, 'Name']` in terms of directly targeting the 'Name' column after getting the index, as demonstrated in the ground truth. This is a minor stylistic difference, not a correctness issue. To improve, the output should strictly adhere to ""Print the name of the Pokemon"" without additional stats if not requested, and it could also benefit from separating the `idxmax`/`idxmin` call from the row selection for better readability, similar to the ground truth.",command-r7b-12-2024,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('titanic.csv')

# Identify columns with missing values
missing_values = df.isnull().sum()
print(""Columns with missing values:"")
print(missing_values)

# Fill missing Age values with the median age
df['Age'] = df['Age'].fillna(df['Age'].median())

# Fill missing Embarked values with the most frequent port
most_frequent_embarked = df['Embarked'].mode().iloc[0]
df['Embarked'] = df['Embarked'].fillna(most_frequent_embarked)

# Drop t...",6,"The generated code correctly addresses most of the tasks, including loading data, handling missing 'Age' and 'Embarked' values, and dropping the 'Cabin' column. It also displays the shape and head correctly. However, there's a significant issue with the title extraction from the 'Name' column. The `df['Name'].str.split().str[0]` approach will extract the *first word* of the name (e.g., 'Braund' or 'Cumings'), not the title (Mr., Mrs., Miss., etc.). This makes the 'Title' column incorrect and fails a core requirement of the prompt. Additionally, the ground truth includes a robust (`re`) based title extraction and standardization, which is a better approach for this kind of problem. The generated code also lacks error handling for the file not found scenario present in the ground truth.",command-r7b-12-2024,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the book inventory DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}
df =...",10,"The generated code is correct, complete, and adheres to good code quality standards. It accurately defines the DataFrame, correctly identifies the title of the book with the highest price using `idxmax()`, and correctly calculates the total number of books in stock using `sum()`. The code is concise and easy to understand. All requirements of the task were met.",command-r7b-12-2024,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
df = pd.read_csv('water_potability.csv')
print(df.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(df.shape)

# 3) List all column names and their data types
print(df.dtypes)

# 4) Calculate the percentage of potable vs non-potable water samples
potable_count = df['potable'].sum()
non_potable_count = df['potable'].count() - potable_count
total_samples = len(df)
potable_percentage = (potable_c...",6,"The generated code addresses most of the requirements but has several issues that lower its score.

**Positives:**
*   **Correct Imports:** `import pandas as pd` is correct.
*   **Dataset Loading and Display:** `df.head(10)` correctly displays the first 10 rows.
*   **Shape Display:** `df.shape` correctly shows the dataset's dimensions.
*   **Missing Values:** `df.isnull().sum()` correctly calculates missing values.

**Areas for Improvement (Reasons for a lower score):**

1.  **Potability Calculation (Critical Error):** The column name assumed for 'Potability' is `potable`, which is incorrect according to the standard `water_potability.csv` dataset, where it's `Potability` (case-sensitive). This led to `KeyError`. Even if the name was correct, the calculation `potable_count = df['potable'].sum()` assumes 'potable' column directly contains the count. If `Potability` is 1 for potable and 0 for non-potable, `df['Potability'].sum()` correctly counts potable samples, but `non_potable_count = df['potable'].count() - potable_count` is not the best way; `df['Potability'].value_counts()` is more robust and directly gives counts for both. The ground truth demonstrates a better approach.
2.  **Column Names and Data Types:** `print(df.dtypes)` is acceptable for data types but `df.info()` as used in the ground truth is superior as it also provides non-null counts and memory usage, which is more comprehensive for EDA. The requirement was to ""List all column names and their data types"", and `df.dtypes` doesn't explicitly list names first concisely which `df.info()` does implicitly along with types.
3.  **Numerical Statistics:** The code attempts to use `np.number` without `import numpy as np`. This would cause a `NameError`. It implies an incomplete import or assumption. The ground truth's `df.describe()` is sufficient as `describe()` automatically works on numerical columns.
4.  **Error Handling:** The generated code lacks any robustness for file not found errors, unlike the ground truth which includes a `try-except` block. While not explicitly asked for, robust code is generally preferred for ""expert code evaluator.""
5.  **Output Clarity:** The generated code lacks descriptive print statements or section headers, making the output less readable and clear about which part of the task is being displayed. The ground truth uses clear headers, which is good practice for EDA outputs.
6.  **Completeness:** The prompt asks for ""percentage of potable vs non-potable water samples"" shown explicitly. While the calculation is attempted, the output doesn't clearly show both percentages side-by-side from a single calculation as `value_counts()` provides, and the interpretation is missing.

In summary, the generated code has functional issues, primarily with the potability calculation due to a column name mismatch and reliance on an unimported `numpy`. It also lacks the comprehensive output and error handling of the ground truth.",command-r7b-12-2024,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Identify columns with missing values
missing_values = df.isnull().sum()
print(""Columns with missing values:"", missing_values)

# Fill missing values using the mean for numerical columns
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns
df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())

# Check for and remove duplicate rows
df = df.drop_duplicates()

# Define...",5,"The generated code addresses some aspects of the task but falls short on several key requirements, leading to a score below 6.

**Critical Issues/Missing:**

*   **Before/After Comparison of Missing Values:** The generated code attempts a before/after summary, but `summary_before['Missing values count']` is calculated *after* missing values have been filled, making the ""before"" count inaccurate. It also assumes `summary_after['Missing values count']` is 0 without verifying. The ground truth correctly captures missing values *before* processing for a true comparison.
*   **Before/After Comparison of Shape:** Similar to missing values, the dataset shape for `summary_before` is taken after duplicates are dropped and missing values are filled, which is incorrect. The ground truth takes the initial shape.
*   **Missing Value Identification and Display:** The ground truth identifies and *displays only columns with missing values* before filling, which is more informative. The generated code prints all columns' missing counts, even if zero.
*   **Data Range Validation Logic:** The range validation in the generated code is severely flawed.
    *   It defines a `reasonable_ranges` dictionary but doesn't apply it to *clean* the data; it only prints out-of-range rows. The task explicitly asks to ""verify that all values are within reasonable ranges"" (implying removal or correction if out of range).
    *   The `lower_threshold = lower - 1` and `upper_threshold = upper + 1` logic is arbitrary and doesn't reflect the specified ""reasonable ranges.""
    *   It only checks `pH` and `Conductivity` based on the example, ignoring other critical parameters like `Hardness`, `Solids`, `Chloramines`, etc., that should also have non-negative constraints. The ground truth includes a robust check for non-negative values for all relevant parameters and *removes* the rows.
*   **Informative Output:** The summary and intermediate print statements are much less informative than the ground truth. For example, it doesn't state how many duplicates were removed or how many rows were removed due to range violations.
*   **Error Handling:** The ground truth includes file `FileNotFoundError` handling, which is good practice for loading data. The generated code lacks this.

**Minor Issues/Improvements:**

*   The print statements for ""Columns with missing values"" could be more concise to only show columns that *actually* have missing values.
*   The `numpy` import is not present in the generated code (not strictly necessary but common practice for numerical operations in pandas).

Overall, while it attempts to cover the requirements, the implementation of critical steps like before/after comparisons and robust data range validation is incorrect or incomplete, failing to fully achieve the task's objectives.",command-r7b-12-2024,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('water_potability.csv')

# 1) Create a bar chart showing the distribution of potable vs non-potable water samples
potable_non_potable = data['Potability'].value_counts()
potable_non_potable.plot(kind='bar', color=['green', 'red'])
plt.title('Potable vs Non-Potable Water Samples')
plt.xlabel('Water Type')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

# 2) Create hist...",6,"The generated code addresses most of the requirements but has significant issues in the implementation of the histogram plot, making it incorrect.

**Strengths:**
*   **Correct Imports:** All necessary libraries (pandas, matplotlib.pyplot, seaborn) are imported.
*   **Dataset Loading:** The CSV is loaded correctly.
*   **Bar Chart:** The bar chart for potability distribution is mostly correct, although it could benefit from more descriptive labels (e.g., 'Not Potable' vs. 'Potable' instead of 0/1). The basic logic is sound.
*   **Boxplot:** The boxplot for pH by potability is implemented correctly, including the use of `palette`.
*   **Correlation Heatmap:** The correlation heatmap is correctly generated and annotated.
*   **Titles & Labels:** Each plot generally includes a title and axis labels, as requested.

**Weaknesses and Areas for Improvement:**

1.  **Histogram Plot (Major Issue):** The most significant problem is with the histogram generation (point 2).
    *   `sns.histplot(data=data, x=['pH', 'Hardness', 'Chloramines'], multivariate=True, hue='Potability')` This approach to plotting multiple histograms using `multivariate=True` with `histplot` is incorrect for showing individual distributions of distinct variables. `multivariate=True` is typically used for bivariate (e.g., `x` and `y` axes) or specific multivariate density plots.
    *   The goal was to create *histograms* for each of 'pH', 'Hardness', and 'Chloramines' individually. The generated code attempts to plot them all on a single axis using a mechanism that doesn't produce separate, clear distributions for each variable. This results in a single, uninterpretable histogram encompassing values from all three columns combined.
    *   Correct approach would be to use `subplots` and call `sns.histplot` (or `plt.hist`) for each column separately.
    *   Missing `dropna()`: While `seaborn` often handles `NaN`s internally for many plot types, explicitly calling `dropna()` on the series or handling missing values in the data prep phase is good practice, especially for histograms where `NaN`s would simply be ignored.

2.  **Bar Chart Labels (Minor):** While `potable_non_potable.plot(kind='bar')` works, directly using `sns.countplot` (as shown in the ground truth) is often more idiomatic for categorical counts and provides better default aesthetics, and allows easier customization of x-tick labels (0/1 to 'Not Potable'/'Potable'). The generated code's x-axis labels are just 0 and 1, which are less user-friendly than descriptive labels.

3.  **No `plt.tight_layout()`:** While not strictly required for a passing score, this function is crucial for preventing labels and titles from overlapping in multi-subplot figures or even single plots with long labels, improving readability.

4.  **Handling Missing Values for Plots:** The prompt specifically mentioned ""handle missing values in plots."" While Seaborn often implicitly handles NaNs by ignoring them in calculations (e.g., `boxplot`, `histplot`), for histograms, explicitly showing how NaNs are handled (e.g., through `dropna()` or imputation before plotting) is good practice. The generated code doesn't explicitly demonstrate this, though the incorrect histogram makes this point less critical to its failure.

To improve, the most critical step is to correctly implement the three individual histograms using `subplots`.",command-r7b-12-2024,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Feature engineering
# 1. Ratio features
data['alcohol_ash_ratio'] = data['alcohol...",5,"The generated code attempts to address the requirements but falls short in several key areas, leading to a low score.

**Critical Issues:**

1.  **Incorrect Feature Engineering:** The prompt specifies ""winequality-red.csv"" and its columns. The generated code attempts to use columns like 'ash', 'acidity', 'guardian', 'magnesium', 'potassium', 'sulfur' which **do not exist** in the `winequality-red.csv` dataset. This makes the feature engineering completely invalid and would cause `KeyError` if run.
    *   `winequality-red.csv` columns are: `fixed acidity`, `volatile acidity`, `citric acid`, `residual sugar`, `chlorides`, `free sulfur dioxide`, `total sulfur dioxide`, `density`, `pH`, `sulphates`, `alcohol`, `quality`.
    *   The prompt specifically requested *meaningful* features. Even if the columns existed, ratios like `alcohol_ash_ratio` are generally not meaningful for wine quality.
    *   Only one polynomial feature (`pH_squared`) was created, and it's created incorrectly (`poly.fit_transform(data[['pH']])[:, 0]` will just give the original pH, polynomial features require `PolynomialFeatures().fit_transform(X)` to generate new features like `X^2`, `X^3`, etc.).

2.  **Incorrect SelectKBest Usage:** `selected_features_kbest = kbest.fit_transform(X, y) == 1` is fundamentally wrong. `fit_transform` returns the transformed array (the selected features themselves), not a boolean mask. To get the mask, one should use `kbest.get_support()`. This would lead to incorrect feature identification.

3.  **Missing Comparison and Display of Rankings:** The prompt explicitly asks to ""Compare and contrast the feature sets selected by each method"" and ""Display the final selected features with their importance scores or rankings.""
    *   The generated code only prints the names of the selected features but does not provide any comparison (e.g., common features, unique features) as requested.
    *   It completely omits displaying importance scores or rankings for either method, which is a core requirement for knowing *why* features were selected. For RFE, one would extract `rf.feature_importances_` after training `rf` on the *selected* features, or use `rfe.ranking_`. For SelectKBest, it's `kbest.scores_`.

4.  **Implicit/Misplaced Imports:** `matplotlib.pyplot` and `mean_squared_error` are imported but not used, indicating potentially misleading imports. `train_test_split` is imported but not used to split data before feature selection, which is fine for feature selection itself but if the intent was a full pipeline, it's incomplete.

**Areas for Improvement (if the critical issues were fixed):**

*   **Robustness:** The ground truth code includes error handling for `FileNotFoundError` and handles potential `NaN`/`inf` values after feature engineering, which are good practices. The generated code lacks these.
*   **Clarity:** The ground truth code provides more descriptive print statements to guide the flow and show the state of the data.
*   **Documentation:** While not explicitly asked for comments, the ground truth uses comments effectively to explain choices and steps.
*   **Meaningful Feature Engineering:** The prompt asked for *meaningful* features. The ground truth provides examples suitable for a wine dataset (e.g., acidity ratios, sulfur dioxide ratios, alcohol interactions).

In summary, the generated code fails to correctly implement core parts of the task (feature engineering, correct feature selection usage) and omits crucial comparison/display requirements.",command-r7b-12-2024,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
import matplotlib.pyplot as plt
from sklearn.metrics import mean_...",4,"The generated code attempts to address the task but fails significantly in multiple key areas, leading to incorrect or incomplete implementation of the requirements.

**Correctness & Completeness Issues:**
*   **Data Preprocessing:**
    *   `data.fillna(data.mean(), inplace=True)` is problematic. `data.mean()` will fail for categorical columns, and applying `mean()` to the entire DataFrame is generally not a robust imputation strategy.
    *   Categorical encoding uses `category_encoders.OneHotEncoder`, but then attempts to re-encode `Platform` which is already transformed, and uses `ordinal_encoder().transform(data['Platform'])` which only works if the original `Platform` column existed and was numerical, leading to errors. This directly causes the code to break. The goal of encoding categorical information is not met.
    *   It does not handle `User_Score` which might contain 'tbd' values, unlike the ground truth which correctly converts it to numeric and handles NaN.
    *   It doesn't use `ColumnTransformer` and `Pipeline` for a robust preprocessing setup within a cross-validation loop, which is crucial for preventing data leakage and ensuring proper flow.
*   **Cross-Validation:**
    *   The cross-validation implementation (Step 4) is manual and highly flawed. It uses `train_test_split` within a loop, which does not constitute proper k-fold cross-validation. It just creates 5 random splits. It also only stores MSE, not the other required metrics.
    *   It doesn't use `KFold` or `cross_val_score` from `sklearn.model_selection`, which are standard and correct ways to do this.
*   **Voting Ensemble:**
    *   `voting='soft'` is used, which typically requires `predict_proba` (classification problem), but this is a regression task. For regression, `VotingRegressor` performs averaging, so `voting='soft'` is incorrect.
    *   The voting ensemble is fitted on the entire `X` and `y` dataset, not using the cross-validation setup for evaluation.
*   **Hyperparameter Tuning:**
    *   `GridSearchCV` is initialized, but the models passed to it are the *already instantiated*, potentially untuned models from `models`. It's fine, but less efficient if those `models` are meant to be base models.
    *   The `param_grids` are very minimal and do not include common hyperparameters.
    *   Tuning is performed on `X` and `y` directly, not within a pipeline that includes preprocessing, which is a significant flaw as preprocessing steps (like imputation and encoding) are fixed before tuning, potentially leading to suboptimal results or data leakage.
    *   It prints `Fitting {name} with GridSearchCV...` but then doesn't evaluate the performance of the tuned models, which is a critical omission.
*   **Missing Requirements:**
    *   **Feature Importance Plots (Step 7):** The code block is commented out and not implemented.
    *   **Evaluation Metrics (Step 8):** The code block for comprehensive evaluation metrics is commented out/missing. While `MSE` is collected in the faulty CV loop, the other metrics (`MAE`, `RMSE`, `R`) are not systematically collected or displayed.
    *   **Stacking Ensemble (Step 9):**
        *   The implementation is completely broken (`stacking_ensemble = Pipeline([('stacker', stacking_ensemble_pipeline)])` is syntactically and logically incorrect, `stacking_ensemble_pipeline` is undefined).
        *   It doesn't define base estimators or a final estimator for the `StackingRegressor`.
    *   **Residual Plots (Step 10):** The code block is commented out and not implemented.
*   **Error Handling:** No `try-except` block for file loading.
*   **Column Handling:** The code attempts to drop `Global_Sales` but doesn't handle `Name` or `Rank` columns, which are often irrelevant.
*   **Random Seed:** No global random seed for reproducibility (except in `train_test_split` loop, which is not ideal).

**Code Quality Issues:**
*   **Imports:** Imports `accuracy_score` which is for classification, not regression. Imports `ordinal_encoder` twice.
*   **Readability:** The ""Steps"" are commented, but the implementation within them is often incomplete or incorrect, making the code misleading.
*   **Pipeline Usage:** The overall structure does not leverage `Pipeline` effectively for preprocessing and model fitting together, which is standard practice and crucial for robust machine learning workflows.

In summary, the generated code has severe functional and logical errors that prevent it from correctly solving the given task. Many key requirements are either completely missing, placeholder comments, or implemented incorrectly.",command-r7b-12-2024,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.autoregressive import AR
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_sele...",2,"The generated code is severely incomplete and largely incorrect in its current state.

**Correctness:**
*   **Data Loading & Aggregation:** The generated code attempts to use `df['Release_Date'].dt.year` which assumes `Release_Date` exists and is a datetime object. The `video_games_sales.csv` dataset typically uses `Year_of_Release`, not `Release_Date`, and it's often a numeric column, not datetime. This is a fundamental error in data handling.
*   **ARIMA Modeling:** It imports `statsmodels.tsa.arima.model.ARIMA`, which is correct, but then uses an arbitrary `(5, 1, 0)` order without any prior analysis (like ADF test or ACF/PACF plots) as a proper expert would. The ground truth uses `pmdarima.auto_arima` which is a more robust and expert approach for finding the best order.
*   **Seasonal Decomposition:** This is correctly imported and function call is present.
*   **Missing Implementations:** The vast majority of the required tasks (Multivariate Regression, Polynomial Regression, Bayesian Regression, Change Point Detection, Survival Analysis, Confidence/Prediction Intervals, Time Series Cross-Validation) are commented out or simply contain placeholders (`... (prepare data and fit model)`). This makes the code unusable for the specified task.
*   **Incorrect Imports:** `pymc3` is imported as `` (Chinese characters), `theano.tensor` as `dmatrix`, and `lifelines` as `Coordinate cantidades`. These are clearly nonsensical and show a complete failure to correctly import libraries or understand their usage. `lifelines.datasets.load_ames` is irrelevant to the task.
*   **Useless Imports:** `AR`, `adfuller`, `plot_acf`, `plot_pacf`, `GridSearchCV`, `ttest_ind`, `iqr`, `ChowTest` are imported but never used or correctly applied.
*   **Plotting:** The plotting code is generic and doesn't fully represent a ""forecast"" as required.

**Code Quality:**
*   **Structure:** While it outlines the sections, the lack of actual code within those sections makes the structure almost meaningless.
*   **Imports:** The imports are a disaster, containing garbled characters and non-existent modules.
*   **Readability:** Placeholder comments reduce readability.
*   **Error Handling:** No error handling for file loading or model fitting.

**Completeness:**
*   **Extremely Incomplete:** This is the most critical failure. Only the first two tasks (loading/aggregation and a basic ARIMA call) have any semblance of implementation, but even those are flawed. All other core requirements are missing.
*   **Confidence/Prediction Intervals:** Not implemented for any model.
*   **Time Series Cross-Validation:** Not implemented for any model.
*   **Bayesian Regression:** Not implemented at all, and the `pymc3` import is broken.
*   **Survival Analysis:** Not implemented at all, and relevant `lifelines` imports are broken.
*   **Change Point Detection:** Only `ChowTest` is imported, but not used. The ground truth uses `ruptures` which is a more comprehensive library for this task.

In summary, the generated code provides only a barebones sketch of the project, with critical errors in imports and data handling, and explicitly leaves out the majority of the complex statistical modeling and analysis steps requested in the prompt. It does not meet the basic requirements of the task.",command-r7b-12-2024,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA, TruncatedSVD, TSNE, UMAP
from sklearn.metrics import silhouette_score, silhouette_samples,fowlkes_mallows_score
from sklearn.impute import SimpleImputer
from mlxtend.frequent_patterns import apriori, associatio...",2,"The generated code is highly incomplete and incorrect. It provides only scattered snippets and placeholder comments, rather than functional code.

**Critical missing elements and errors:**

1.  **Preprocessing:**
    *   `df.fillna(df.mean(), inplace=True)`: This is problematic for categorical columns and assumes all numeric columns are suitable for mean imputation. It doesn't handle missing values for critical string columns like 'Name', 'Genre', 'Platform', 'Publisher' as the ground truth does.
    *   `df['Regional_Sales_Distribution'] = df.sum(axis=1) / df.sum(axis=1).sum()`: This calculation is completely wrong. It sums all numeric columns per row and then normalizes by the sum of these sums across all rows, which does not represent regional sales distribution. It also doesn't consider individual regional sales columns.
    *   `df['Market_Position'] = df['Genre'].map(df.groupby('Genre').size().sort_index().to_dict())`: This calculates the *count* of games per genre, not a rank or position within the genre. The ground truth correctly uses `rank`.
    *   `df['Publisher_Portfolio_Size'] = df.groupby('Publisher')['Title'].transform('count')`: The dataset doesn't have a 'Title' column, it has 'Name'. This would cause an error.
    *   There is no `ColumnTransformer` or `OneHotEncoder` usage for categorical features, which is crucial for clustering. The `StandardScaler` is imported but not used. `LabelEncoder` is imported, but typically One-Hot Encoding is preferred for clustering categorical data.

2.  **Clustering Algorithms:** The code merely lists the imports and comments indicating where the clustering code *should* go, but no actual implementation or application of the algorithms to the data is present (`# ... (similar code for KMeans, DBSCAN, Hierarchical, GMM)`).

3.  **Dimensionality Reduction:** Similar to clustering, only imports and a placeholder comment are provided; no actual t-SNE or UMAP application.

4.  **Optimal Cluster Determination:** Again, only comments. No implementation of the elbow method, silhouette analysis, or gap statistic.

5.  **Consensus Clustering:** A line is commented out (`# 4) Perform consensus clustering`), but no implementation.

6.  **Cluster Profiles & Significance Testing:** The comment mentions ""compare cluster means using t-tests,"" but no code exists. The ground truth demonstrates ANOVA and Chi-squared which are more appropriate for multiple cluster comparisons.

7.  **Anomaly Detection:** Only a comment is present.

8.  **Association Rule Mining:**
    *   `df[['Genre', 'Platform', 'Price']]`: The dataset does not have a 'Price' column.
    *   No proper one-hot encoding or conversion to a transactional format suitable for `mlxtend.apriori` is performed. The provided `df[['Genre', 'Platform', 'Price']]` directly passed to `apriori` would likely fail because `apriori` expects a boolean DataFrame.

9.  **Recommendation System:** Only a comment.

10. **Validation Metrics:**
    *   `df.drop('Sales', axis=1)`: The dataset has 'Global_Sales', 'NA_Sales', etc., but not a generic 'Sales' column.
    *   The snippet for silhouette score only calculates it for KMeans and immediately stores it, without comprehensive validation for all algorithms or other metrics like Davies-Bouldin or Calinski-Harabasz.
    *   The `impute`, `imblearn`, `accuracy_score`, `precision_score`, `recall_score`, `f_score` imports are irrelevant for this task which is about unsupervised learning and clustering. `fowlkes_mallows_score` is an external validation metric requiring ground truth labels, which we don't have for clustering.

In essence, the generated code is a skeleton filled with incorrect or unimplemented components, failing to meet almost all requirements of the complex task.",command-r7b-12-2024,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold
from sklearn.ensemble import VotingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, XGBClassifier, LightGBMClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, log_loss, roc_auc_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
im...",3,"The generated code attempts to address many aspects of the prompt but falls significantly short in implementation quality, correctness, and completeness.

**Major Issues:**

1.  **Fundamental Data Loading and Preprocessing Errors:**
    *   The prompt specifies `diabetes.csv` and the original column `Outcome` as the target. The generated code assumes `data['age']` and `data['glucose']` which are not specified for feature engineering this way in the problem description, and `data['class']` as the target, which is incorrect for `diabetes.csv`.
    *   Missing values (`0`s in specific columns) are simply imputed with `X.mean()` without considering the `0` values in 'Glucose', 'BloodPressure', etc., which are indeed missing indicators, but need to be treated as `np.nan` first.
    *   The `LabelEncoder` and `OneHotEncoder` usage is fundamentally flawed. `X[cat_features] = X[cat_features].apply(lambda x: le.fit_transform(x))` applies `LabelEncoder` to an entire DataFrame column as if it were a Series. It then attempts `ohe.fit_transform(X[cat_features])` which is incorrect. The `X_encoded = pd.concat([X.drop(cat_features, axis=1), ohe.fit_transform(X[cat_features])], axis=1)` line is also very problematic and likely leads to dimension mismatches or incorrect data assignments, especially with `X_imputed.assign(**X_encoded)`. The original `diabetes.csv` dataset typically does not have explicit `object` type categorical features; the task implies numerical features with `0`s treated as missing.
    *   The code uses `X_imputed` and `X_encoded` in an inconsistent way.
    *   The custom feature engineering (`age_category`, `glucose_category`) is not bad but doesn't align with the ""comprehensive feature engineering"" outlined in the ground truth (polynomial features, interaction terms).

2.  **Incomplete and Incorrect Ensemble Implementations:**
    *   **Bagging:** The code uses `RandomForestClassifier` and `GradientBoostingClassifier` directly. Bagging is typically applied **with a base estimator** within `BaggingClassifier` (e.g., `BaggingClassifier(base_estimator=DecisionTreeClassifier())`). Simply calling `RandomForestClassifier` is not ""Bagging with Random Forest"" as a separate concept; Random Forest is already a bagging-based ensemble.
    *   **Stacking:** The stacking implementation is completely wrong. It tries to use `base_learner.predict(X_train)` to create features for the meta-learner and feeds this directly to `meta_learner.fit()`. Proper stacking involves training base models on folds of data and generating OOF (Out-Of-Fold) predictions to avoid data leakage for the meta-learner, typically using `StackingClassifier` from `sklearn.ensemble`. It also tries to use a `CatBoostClassifier` without proper instantiation and `fit` call for the base learners within the list.

3.  **Bayesian Optimization:**
    *   The objective function is `objective(trial): return -log_loss(y_train, meta_learner.predict(X_train))`. This is problematic for several reasons: It tries to optimize based on a `meta_learner` that isn't trained within *this* objective function, and it uses `meta_learner.predict(X_train)` which gives class labels, not probabilities, for `log_loss`. It completely lacks the actual hyperparameter suggestion part (`trial.suggest_...`). It's essentially a placeholder that doesn't work.

4.  **Nested Cross-Validation:**
    *   The `KFold` splitting is applied to `X_scaled` and `y_train` in an inconsistent way. `y_train` should be the full target variable.
    *   The inner loop `for train_idx_inner, val_idx_inner in inner_cv.split(X_train_inner):` is correctly structured, but the model training and evaluation are left as `... (model training and evaluation)`, indicating incompleteness.
    *   It only defines one pipeline for ""AdaBoostClassifier"" which isn't enough for evaluating multiple models or even ensembles within the nested CV framework.

5.  **Missing or Placeholder Implementations:**
    *   **Custom Ensemble Methods:** Left as `...`
    *   **CatBoost for Categorical Features:** Left as `...`. While CatBoost was imported, the primary text indicating its use for *handling categorical features* was not implemented or demonstrated correctly. The original dataset doesn't have explicit categorical features, but the ground truth acknowledges `cat_features=None`. The generated code's initial `X.select_dtypes(include=['object'])` assumes typical categorical features not present in this dataset.
    *   **SHAP Values:** Left as `...`
    *   **Model Calibration and Reliability Analysis:** Left as `...`. Imports `CalibratedClassifierCV` but then also imports `CalibrationCheck` and `reliability_metric` which are not standard `sklearn` imports and would cause errors.

6.  **Code Quality Issues:**
    *   Incorrect imports: `sklearn.verifiability` is not a standard module.
    *   Inconsistent variable naming (e.g., `X_imputed` vs `X_encoded`).
    *   Redundant imports or incorrect usage (e.g., `GridSearchCV` imported but not used).
    *   Lack of reproducibility (missing `random_state` in many places outside of `train_test_split`).
    *   The file `diabetes.csv` is not in the correct case (`diabetes.csv` vs `Diabetes.csv`).

Overall, the generated code outlines the structure of the requirements but fails to correctly implement the logic within many of those sections, leading to a non-functional and misleading solution.",command-r7b-12-2024,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from scipy.stats import norm
from pymc3.stats import wicas, looic
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error
from sklearn.gaussian_process import GaussianProcessRegressor
import statsmodels.api as sm
from statsmodels.formula.api import mix...",0,"The generated code essentially serves as a skeleton, outlining the required steps but providing no actual implementation for the vast majority of the tasks. It includes placeholder comments like `... (Implement models here)` for almost all the major requirements (hierarchical models, mixed-effects, Bayesian logistic regression, MCMC, survival analysis, Gaussian processes, causal inference, model comparison, posterior checks). Key imports like `pymc3` are present but unused for modeling. Furthermore, it includes incorrect or non-standard imports such as `dnl.constants`, `sklearn.phantom_study_utils`, and `PyMC3.tune.pee`, which are not part of standard PyMC3 or scikit-learn libraries and would cause immediate import errors. The missing value handling is simplistic and potentially problematic (`fillna(method='ffill').fillna(method='bfill').fillna(df.mean())`) without considering the nature of the data (e.g., 0 values representing missing). Therefore, it fails to solve the task, has significant code quality issues due to non-existent imports, and is highly incomplete.",command-r7b-12-2024,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture, SpectralClustering, OPTICS
from sklearn.decomposition import PCA, TruncatedSVD, Autoencoder
from sklearn.metrics import silhouette_score, silhouette_samples_mean, silhouette_sorted_samples, adjusted_rand_score, normalized_mutual_info_score
from sklearn.model_selection import train_test_split, GridSear...",2,"The generated code provides a very basic skeleton and barely addresses any of the requirements. It fails to implement the core logic for advanced feature engineering, optimal parameter determination, consensus clustering, detailed phenotyping with statistical testing, time-series clustering (even conceptually for a cross-sectional dataset), and full risk stratification.

**Critical Missing/Incorrect Elements:**

1.  **Data Preprocessing and Advanced Feature Engineering:** The code only loads the CSV and has a `...` placeholder for feature engineering. It does not implement handling of missing values, scaling, or, crucially, the metabolic syndrome indicators as specifically requested.
2.  **Clustering Implementation:** While it imports the algorithms and initializes them, it *does not apply them* to the data, nor does it store their results or integrate them for further use. There's no loop for K-Means, etc.
3.  **Dimensionality Reduction:** It initializes PCA, t-SNE, and UMAP but doesn't apply them or use them for visualization. The autoencoder is missing entirely in the `sklearn.decomposition` import (it's not part of scikit-learn and needs a custom Keras/TensorFlow implementation as in ground truth).
4.  **Optimal Clustering Parameters:** This is a major omission. The prompt explicitly asked for silhouette analysis, gap statistic, and elbow method with *statistical validation*. The generated code has no logic for this at all, just sets hardcoded `n_components` or `n_clusters` for GMM/Spectral.
5.  **Consensus Clustering:** No implementation for consensus clustering. It imports `ml_ensemble.consensus` but doesn't use it or follow the definition for this.
6.  **Patient Phenotype Profiles & Statistical Significance:** This is completely absent. There's no grouping by clusters, calculation of means/stds, or statistical tests (ANOVA, Chi-squared) to characterize clusters.
7.  **Anomaly Detection:** LOF and Isolation Forest are initialized, but not applied to the data. There's no output or visualization of anomalies.
8.  **Time-Series Clustering:** There's a placeholder comment, but no conceptual outline or explanation of why it can't be applied to this dataset, which was a specific part of the ground truth's pedagogical approach.
9.  **Association Rule Mining:** While `apriori` and `association_rules` are called, they are applied to the *original `data` DataFrame* directly, which is problematic. Association rule mining typically requires *binned/discretized data* (e.g., turning continuous Glucose levels into categories like ""Low Glucose"", ""High Glucose""). The current implementation would fail or produce meaningless results.
10. **Patient Risk Stratification:** Completely missing.
11. **Code Quality and Structure:** The code is very fragmented with many `...` placeholders. Imports like `Autoencoder` from `sklearn.decomposition` are incorrect. `silhouette_samples_mean`, `silhouette_sorted_samples`, `adjusted_rand_score`, `normalized_mutual_info_score`, `make_bivariate_normal`, `pairwise_distances_argmin_min`, `GridSearchCV` are imported but never used, cluttering the code. `mlxtend.cluster.ClusterAnalysisEvaluator`, `mlxtend.plot.plot_kind`, `mlxtend.demo.datasets.load_iris` are also unused and irrelevant for this task.

In summary, the generated code only lists the required libraries and some initializations, but the actual logic and implementation for almost all advanced requirements are entirely missing or fundamentally incorrect. It's more of a template than a functional solution.",command-r7b-12-2024,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.datasets import fetch_20newsgroups
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, TimeSeriesSplit, KFold
from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import StandardScaler, OneHotEncoder, Label...",2,"The generated code fails to meet most of the task requirements.

**Primary Reasons for Failure:**

1.  **Fundamental Data Preprocessing Issues:**
    *   **Incorrect `fillna` strategy:** `data.apply(lambda x: x.fillna(x.mode()[0]))` applies `mode()[0]` column-wise, which is incorrect for numerical features (e.g., happiness scores). The subsequent `data_encoded = data_encoded.replace({np.nan: 0})` is redundant given the previous line and also potentially harmful for numerical data.
    *   **Hardcoded 'income_':** This column name literally contains Chinese characters (``) and is not present in the `world-happiness-report-2021.csv` dataset. This indicates a severe mismatch with the expected dataset.
    *   **Missing Preprocessing Pipeline setup:** The code completely lacks `ColumnTransformer` for numerical and categorical feature processing. It only initializes `StandardScaler` and `OneHotEncoder` but doesn't actually use them. This is a critical omission for proper handling of heterogeneous data types.
    *   **Hardcoded Target Name:** The target variable is dropped as 'happiness\_score' which is not the correct target column ('Ladder score') from the specified dataset.

2.  **Incomplete Implementation of Core Requirements:**
    *   **Ensemble Methods:** Only `VotingRegressor` and `BaggingRegressor` are partially implemented (fitted, but no evaluation or further use). Boosting algorithms (AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost) are mentioned in comments but not actually implemented or used.
    *   **Stacking Ensemble:** The `StackingRegressor` is only initialized with two basic estimators and fitted, but there's no mention of a meta-learner beyond a generic `LinearRegression()` and no proper evaluation.
    *   **Bayesian Optimization:** Only a comment indicates its presence. There's no actual implementation using Optuna or Hyperopt, which was a core requirement.
    *   **Nested Cross-Validation:** No implementation. This was a critical evaluation criterion.
    *   **Custom Ensemble with Dynamic Weights:** Only a comment indicates its presence. No implementation.
    *   **Blending and Multi-level Stacking:** Only a comment indicates its presence. No implementation.
    *   **SHAP and LIME Interpretation:** Only a comment indicates its presence. No implementation.
    *   **Model Calibration and Uncertainty Quantification:** Only a comment indicates its presence. No implementation.
    *   **Ensemble Diversity Analysis and Model Combination Optimization:** Only a comment indicates its presence. No implementation.

3.  **Code Quality Issues:**
    *   **Unused Imports:** `fetch_20newsgroups`, `GridSearchCV`, `TimeSeriesSplit`, `LabelEncoder`, `OneHotEncoderumbinator` (which seems like a typo/non-standard library for `ColumnTransformer`), `compute_class_weight`, `calibration_curve`, `PadClassifier`, `precision_score`, `BaseEstimator`, `TransformerMixin` are all imported but not used.
    *   **Placeholder Comments:** Large sections of the code are just comments indicating what *should* be done, rather than actual implementations.
    *   **No Evaluation:** Beyond fitting, there's no model evaluation (e.g., `mean_squared_error`, `r2_score`) for *any* of the defined models.

In summary, the code barely begins to address the problem, fundamentally misunderstanding the dataset and leaving nearly all advanced requirements as unimplemented comments.",command-r7b-12-2024,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import thelana as th
import pymc3 as pm
import scipy.stats
from scipy.special import iv
from statsmodels.formula.api import lm
from inhibited_bayes import instructed_prior_ commenc
from midi_tensor import delegate_gp
from holocylinder import quantify, medicate, differentials, evaluate, match_pair, access
from arpeggiosheet import loo_cycle
import versions

# Step 1: Load and preprocess the dataset
data = pd.read_csv('world-happiness-report-2021.c...",2,"The generated code is almost entirely pseudocode and does not implement the requested functionality.

**Critical Issues:**
1.  **Missing Implementations:** Sections 2 through 10 are represented by function definitions or comments `(...)` rather than actual code. This includes core requirements like hierarchical Bayesian models, SEM, causal inference methods, Gaussian processes, mediation analysis, DiD, model comparison, and posterior analysis.
2.  **Incorrect/Non-existent Imports:** Several imports like `thelana`, `scipy.special`, `inhibited_bayes`, `midi_tensor`, `holocylinder`, and `arpeggiosheet` appear to be either typos (`thelana` instead of `theano`), non-standard libraries, or entirely fabricated (e.g., `holocylinder`, `arpeggiosheet`, `inhibited_bayes`, `midi_tensor`). These libraries are not part of standard data science stacks for PyMC3 or the general ML/stats ecosystem. The task explicitly asks for PyMC3 or PyMC4, not custom or hypothetical libraries. The ground truth uses standard libraries like `pandas`, `numpy`, `pymc3`, `arviz`, `scikit-learn`, and `statsmodels`.
3.  **Lack of Detail in Pseudo-code:** Even the placeholder comments are vague, indicating a lack of understanding of the complexity required for each sub-task. For instance, ""Model definition here using PyMC3"" is far from a PyMC3 implementation.
4.  **Incomplete Preprocessing:** While a `fillna` line is present, it explicitly notes ""more advanced missing value handling techniques can be applied here,"" reinforcing that the solution is incomplete even in the basic data loading step. The `fillna(data.mean(), inplace=True)` for all columns (including categorical ones) is incorrect without checks.
5.  **No Execution Logic:** There are comments indicating example usage but no actual execution of the placeholder functions, meaning the code as provided would not run or produce any results.

**Conclusion:** The generated code is fundamentally incomplete and relies on non-existent libraries, failing to meet almost all task requirements for implementation and correctness. It serves more as a high-level outline than a functioning program.",command-r7b-12-2024,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture, SpectralClustering, HDBSCAN
from sklearn.decomposition import PCA, TruncatedSVD, NMF
from sklearn.manifold import TSNE, UMAP
from sklearn. neural_network import Autoencoder
from sklearn.metrics import silhouette_score, gap_statistic, calinski_harabasz_score, adjusted_rand_score, mutual_info_score
fro...",2,"The generated code is incomplete and rife with errors. It attempts to address some parts of the prompt but fails to implement most of the required functionalities.

**Critical Issues:**

1.  **Missing Data Columns:** The code immediately fails because it tries to access columns like `Life_Satisfaction`, `Positive_Affect`, `Negative_Affect`, `GNI_Per_Capita (PPP $)`, and `Country` in the `world-happiness-report-2021.csv` dataset. The actual dataset (as implied by the ground truth) contains columns like `'Ladder score'`, `'Logged GDP per capita'`, `'Social support'`, etc. This fundamental error prevents any subsequent code from running correctly.
2.  **Incorrect Feature Engineering:** The example feature engineering `data['Happiness_Profile'] = data['Life_Satisfaction'] - data['Positive_ Affect']` uses non-existent columns.
3.  **Incorrect Categorical Encoding:** `data['Region'] = pd.Categorical(data['Country'], categories=...)` attempts to create a region category based on the *Country* column, which is incorrect. The dataset has a 'Regional indicator' column. The `Income_Level` column also doesn't exist in the dataset.
4.  **Autoencoder Implementation:** It tries to import `Autoencoder` from `sklearn. neural_network`, which does not exist in scikit-learn. Autoencoders typically require deep learning frameworks like TensorFlow/Keras or PyTorch.
5.  **Unimplemented Features (Placeholders):** Many critical sections like ""Clustering with various algorithms,"" ""Anomaly Detection,"" ""Network Analysis"" (beyond basic graph creation), ""Country Recommendation System,"" and ""Policy Insights"" are left as comments or very minimal, incorrect placeholders.
    *   **Clustering:** Only K-Means and DBSCAN are initialized, with no full implementation of the other algorithms listed in the prompt (Hierarchical, GMM, Spectral, HDBSCAN). There's no parameter optimization loop.
    *   **Dimensionality Reduction:** Only PCA and t-SNE are attempted, missing UMAP, autoencoders, and other manifold learning techniques (like Isomap mentioned in the ground truth).
    *   **Optimal Clustering Parameters:** The code imports `gap_statistic` and `calinski_harabasz_score` but doesn't *implement* the methodologies for determining optimal parameters (silhouette analysis, gap statistic, elbow method, stability analysis).
    *   **Consensus Clustering:** Completely missing. This was a key requirement.
    *   **Detailed Country Profiles:** Missing the statistical significance testing (ANOVA) and effect size analysis.
    *   **Anomaly Detection:** Left as a comment.
    *   **Network Analysis:** The network analysis is rudimentary. It includes a placeholder for `similarity_score` calculation and lacks advanced aspects like centrality measures or community detection, which are key to understanding relationships.
    *   **Association Rule Mining:** Attempts to run `apriori` on raw happiness scores without proper binning/discretization, making it invalid for ARM.
    *   **Recommendation System & Policy Insights:** Almost entirely missing, with only comments. Geopolitical validation is not addressed at all.

6.  **Unnecessary Imports:** `trident`, `geopandas`, `scikitplot`, `plotly.express` are imported but not used or used incorrectly.

**Overall:**

The code demonstrates a superficial understanding of the task requirements. It throws together some imports and function calls without correctly addressing the data structure, implementing the advanced techniques, or creating a coherent, executable solution. It's more of a skeleton with numerous fundamental flaws rather than a working solution.",command-r7b-12-2024,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error
from datetime import datetime, timedelta

# 1) Load data, parse 'date', filter for 'Germany', and set 'date' as index
data = pd.read_csv('synthetic_covid19_data.csv')
data['date'] = pd.to_datetime(data['date'])
data = data[data['location'] == 'Germany'].set_index('date')

# Handle missing values in target and exogen...",6,"The generated code attempts to address all parts of the task but falls short on several critical aspects, primarily correctness and completeness regarding time series best practices and the specific requirements.

**Correctness:**
*   **Missing Value Handling:** The `fillna` methods are applied correctly, but the order or type of interpolation for 'stringency_index' and 'people_fully_vaccinated' varies (interpolate vs. fillna with ffill) and lacks robustness found in the ground truth (e.g., bfill after ffill, general interpolation as a fallback). Critically, it does not handle cases where the entire column might be NaN or initial NaNs only.
*   **SARIMA Orders:** The task explicitly asks for determining appropriate (p,d,q)(P,D,Q,s) orders, ideally using `auto_arima` or manual inspection. The generated code hardcodes `order=(1, 1, 1)` and `seasonal_order=(0, 0, 0, 7)`. This is a major omission as it bypasses a key step in time series modeling for SARIMA, which expects some form of order selection.
*   **Data Splitting:** The line `train_end = datetime.now() - timedelta(days=30)` is incorrect for splitting. This will always train on data up to 30 days *before the current execution day*, not on the first `len(data) - 30` days of the dataset. This means the training set will constantly change based on when the code is run, and the test set will only contain the very latest data, which might be very short or non-existent depending on the dataset's recency relative to `datetime.now()`. The task specified ""last 30 days for testing"" relative to the *dataset's end*, not the current date.
*   **Exogenous Variable Handling in Forecast:** When `predict` is called, `start` and `end` should refer to the index *locations* in the *original dataframe* or the *forecast horizon*. Using `start=train_end` and `end=test_data.index[-1]` with the exog of `test_data` passed for the *entire forecast range* means the model might try to predict for a period it was already trained on if `train_end` is interpreted as a date within the training data. The correct approach is to predict `steps=len(y_test)` or provide the specific index range for the test set. Also, `SARIMAX.predict` usually calculates dynamic forecasts or one-step-ahead forecasts. For multi-step-ahead forecasting (like predicting the entire test set), `get_forecast` then `predicted_mean` is safer and more explicit for the test period.
*   **Model Instantiation for Fitting:** The code `model = SARIMAX(...)` instantiates the model. Then `model_fit = model.fit()`. This is syntactically fine. However, SARIMAX objects often need `enforce_stationarity=False` and `enforce_invertibility=False` when `d` or `D` are greater than 0, which `auto_arima` might determine. Without `auto_arima`, these common settings are missed.

**Completeness:**
*   **Error Handling:** Lacks robust error handling, such as checking if the file exists, if filtering for Germany yields an empty DataFrame, or if the test set is too small.
*   **Warnings:** No suppression of warnings, which can be noisy (e.g., from statsmodels during fitting).
*   **Negative Forecasts:** Does not handle negative `new_cases` forecasts, which are impossible for real-world case counts.

**Code Quality:**
*   **Imports:** Generally good.
*   **Structure:** Logical flow, but the hardcoded SARIMA orders and incorrect data splitting diminish its quality.
*   **Comments:** Sparse compared to the ground truth, which provides better clarity on each step.

Overall, the code demonstrates some familiarity with SARIMA and basic data processing but fails to correctly implement key time series practices such as proper data splitting, automated hyperparameter tuning, and robust missing value imputation.",command-r7b-12-2024,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load the dataset
dataset = pd.read_csv('winequality-red.csv')

# Step 2: Define quality groups
dataset['quality_group'] = pd.cut(dataset['quality'], bins=[0, 4, 6, float('inf')],
                                 labels=['low', 'medium', 'high'])

# Step 3: Perform statistical tests
def perform_tests(df, chemical_properties):
    results = {}
    for prop in chemical_properti...",4,"The generated code attempts to address the task but falls short on several critical aspects, leading to a low score.

**Correctness & Completeness Issues:**

1.  **Statistical Test Selection Logic is Flawed:** The most severe issue is the `perform_tests` functions logic for choosing between ANOVA and Kruskal-Wallis. It checks `if all(df.loc[:, chemical_properties].dtypes == 'float64')`. This condition will almost always be true if the `chemical_properties` are indeed numerical, but it **does not represent a check for normality or homogeneity of variance**, which are the actual assumptions for ANOVA. It completely bypasses the essential part of the requirement (2) for selecting the *appropriate* statistical test. The ground truth correctly uses Shapiro-Wilk and Levene's tests for this. This makes the statistical conclusions drawn by the generated code potentially incorrect, as it might apply parametric tests to non-parametric data.

2.  **Missing Post-Hoc Test Implementation:** The code identifies *which* post-hoc test *should* be used (`'Tukey'` or `'Dunn'`) but **does not actually perform them**. This is a major omission of requirement (3). The `statsmodels` library for Tukey HSD or manual pairwise Mann-Whitney U tests with correction (for Dunn's proxy) are critical and are completely absent.

3.  **Incomplete/Incorrect Chemical Properties:** The `chemical_properties` list is hardcoded with a subset of columns, and it misses the `pH` and `sulphates` columns which are present in the dataset. While it states ""replace with actual feature columns,"" this is a direct requirement of the task to analyze *all* relevant chemical properties.

4.  **No Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice. The generated code lacks this.

5.  **Suboptimal Grouping for Box Plots:** While it does create box plots, the `plt.boxplot` call uses a list comprehension `[dataset.loc[dataset['quality_group'] == g, prop].values for g in groups]`. This works but is less integrated with Seaborn's capabilities for plotting grouped data, which often results in more readable and easier-to-customize plots (as seen in the ground truth's `sns.boxplot`).

6.  **Summary is Basic:** The summary only states which features are significant and which test was used. It does not provide the detail of *how* they differ based on post-hoc analysis, which would be crucial information gained from properly implemented post-hoc tests.

**Code Quality Issues:**

1.  **Unnecessary Imports:** `statsmodels` is imported in the ground truth because it's needed for `pairwise_tukeyhsd` and `multipletests`, but it's not imported at all in the generated code, even though the code *acknowledges* Tukey and Dunn are needed. This reflects the lack of post-hoc implementation.

2.  **Redundant `groups` Variable:** The `groups = ['low', 'medium', 'high']` line is repeated within the loop for `ANOVA` and `Kruskal-Wallis` branches, which is unnecessary.

3.  **Clarity of Variable Naming:** `col_index` is defined but never used, indicating leftover or unnecessary code.

Overall, the generated code outlines the steps but fails to implement the core statistical logic, especially regarding the assumptions for test selection and the crucial post-hoc analysis.",command-r7b-12-2024,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import roc_auc_score, f1_score
import xgboost as xgb
from shap import TreeExplainer, SHAPExplainer, plot_matrix

# 1) Load data, parse 'date'
data = pd.read_csv('synthetic_covid19_data.csv')
data['date'] = pd.to_datetime(data['date'])

# 2) Preprocessing
locations = data['location'].unique()

# Cal...",4,"The generated code attempts to address all parts of the prompt but contains several critical errors and significant deviations from the requirements, leading to incorrect or incomplete functionality.

**Critical Errors/Incorrect Functionality:**

*   **Target Variable Definition (2):**
    *   The prompt states: ""a 'high_daily_death_increase' is defined as a day where 'new_deaths' for a 'location' is in the top 10% of its own historical 'new_deaths' (**excluding days with zero new deaths**).""
    *   The generated code calculates the 90th percentile from `loc_data[loc_data['new_deaths'] > 0]` which is correct.
    *   **However, it then tries to assign `high_daily_death_increase` back to `loc_data` within the loop, and this `loc_data` is a *copy* of a slice of the original `data` dataframe.** The `data_processed` dataframe is *not* updated correctly with this target variable. `data_processed` is initialized as `data.drop_duplicates(...)` but doesn't get the target variable from inside the loop. This means the target variable `y` later will likely be `NaN` or incorrect.
    *   The ground truth correctly uses `groupby().quantile()` and `map()` to efficiently calculate the percentile for each location and then a vectorized operation to set the target.

*   **Feature Engineering - Lag Features (3):**
    *   The prompt asks for `lag features (1, 3, 7 days) for 'new_cases' and 'reproduction_rate'`.
    *   The generated code attempts `loc_data.groupby('date')['new_cases'].transform('lag', 1)`. **This is incorrect.** Lag features should be calculated *per location* based on *previous days' values*. Grouping by 'date' across all locations and then lagging will not give per-location temporal lags. It also entirely misses `reproduction_rate` lags.
    *   The ground truth correctly uses `df.groupby('location')[col].transform(lambda x: x.shift(lag))`.

*   **Feature Engineering - Days Since First Case (3):**
    *   `first_case_date = loc_data.groupby('location')['new_cases'].first().min()`: This line is problematic. `loc_data.groupby('location')` will group by a single location (since `loc_data` is already for one location). `first()` will give the first `new_cases` value. `.min()` on a single value will be that value. This doesn't represent the *date* of the first case across all locations.
    *   `loc_data['days_since_first_case'] = (data_processed['date'] - first_case_date).dt.days`: This tries to subtract a `new_cases` value (which is a number) from `data_processed['date']` (which is a datetime series), leading to a TypeError. This also assigns back to `data_processed` outside the current `loc_data` context.
    *   The ground truth correctly calculates `(x - x.min()).dt.days` within each `location` group.

*   **Missing Value Handling (4):**
    *   The prompt states: ""Handle missing values from feature engineering (e.g., fill with 0 or backfill).""
    *   The generated code only fills NaNs `for col in ['new_cases', 'hosp_patients', 'icu_patients']` for `_7day_avg` and `_7day_std`. This is incomplete as it misses other engineered features (lags, days since first case) and also original numerical features.
    *   The method `fillna(0, inplace=True)` is used on `data_processed` which is the central dataframe, but the feature engineering occurs on `loc_data` (a copy). So the `fillna` is applied to columns that potentially didn't even get populated correctly due to the previous copy issues.
    *   The ground truth correctly identifies all engineered columns and applies `ffill().fillna(0)` per group, and then addresses other numerical columns.

*   **Selected Features and Preprocessing (5 & 6):**
    *   The provided `selected_features` list for `people_fully_vaccinated_per_population` does not exist in the raw data; the prompt explicitly states to calculate this as a ratio: `'people_fully_vaccinated' / 'population'`. The generated code assumes this column exists, which will cause a `KeyError`.
    *   The categorical features are incorrectly identified: `categorical_features = selected_features[-4:]`. This results in `['new_cases_lag7', 'hosp_patients_7day_avg', 'hosp_patients_7day_std', 'icu_patients_7day_avg']` being treated as categorical, which they are not, and `continent` which *is* categorical, is missing.
    *   The scaling and encoding are done *before* the train-test split on the entire dataset (`data_processed`), which is a form of data leakage. StandardScaler and OneHotEncoder should be fitted *only* on the training data.
    *   The structure of merging back the scaled and encoded columns is convoluted and error-prone. A `ColumnTransformer` within a `Pipeline` (as in the ground truth) is the standard and robust way to handle this.

*   **SHAP Explanation (9):**
    *   `SHAPExplainer` is not a standard class in the `shap` library for Tree models; `shap.TreeExplainer` is.
    *   `plot_matrix` is not a standard SHAP visualization function for feature importance. `shap.summary_plot` (especially `plot_type=""bar""`) is typically used for global feature importance.
    *   `explainer.explainer.get_feature_names_out()` is incorrect. The SHAP explainer does not have a `get_feature_names_out()` method in this context. The feature names for SHAP should come from the dataset used to calculate SHAP values *after* preprocessing, which is managed well by `ColumnTransformer` and the `get_feature_names_out` within it in the ground truth.
    *   The plotting code for SHAP is entirely custom and incorrect for standard SHAP visualizations.

**Code Quality & Completeness Issues:**

*   **Structure:** The workflow is less structured than the ground truth. Performing feature engineering and target variable creation inside loops across locations can be very inefficient and leads to the copying issues observed. Vectorized operations or `groupby().transform()` are much more efficient and less error-prone for time-series operations.
*   **Imports:** The import `shap` is correct but the usage of `SHAPExplainer` and `plot_matrix` is wrong. Missing `ColumnTransformer` and `Pipeline` which are crucial for good preprocessing setup.
*   **Time-based Split:** While `shuffle=False` is used for `train_test_split`, it's not explicitly sorting the data by date before splitting, which is critical. The ground truth correctly sorts and then performs an explicit index-based split.
*   **Hygiene:** Many operations are done in-place (`inplace=True`) or with `loc_data` copies, leading to potential `SettingWithCopyWarning` and incorrect data propagation.

In summary, the generated code demonstrates a general understanding of the steps required but fails significantly in the correct implementation of several core steps, especially regarding target variable creation, feature engineering for time-series data, preprocessing pipeline, and SHAP usage.",command-r7b-12-2024,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.formula.api import mixedlm
from sklearn.preprocessing import OneHotEncoder

# 1) Load data and parse date
data = pd.read_csv(""synthetic_covid19_data.csv"")
data['date'] = pd.to_datetime(data['date'])

# 2) Handle missing values
data['stringency_index'].fillna(data.groupby('location')['stringency_index'].transform('median'), inplace=True)
data['vaccination_percentage'].fillna(0, ...",3,"The generated code fails to correctly implement key requirements of the task, particularly in data preprocessing, missing value handling, and the mixed-effects model specification.

**Major Issues:**

1.  **Incorrect `vaccination_percentage` calculation and missing value handling:** The task explicitly asks to calculate `vaccination_percentage` as `'people_fully_vaccinated' / 'population'`. The generated code assumes `vaccination_percentage` already exists and fills NaNs with 0, which is incorrect and detrimental to the model. There's no handling for `people_fully_vaccinated` or `population` columns whatsoever, which are crucial for this calculation.

2.  **Inadequate Missing Value Handling:**
    *   `stringency_index` is imputed with location median, which is a reasonable single strategy, but the ground truth uses interpolation which is generally better for time-series data, and a global median fallback.
    *   `reproduction_rate` missing values are handled by `dropna`, which might discard significant amounts of data, whereas interpolation is preferred.
    *   No handling for `population` or `people_fully_vaccinated` missing values.

3.  **Incorrect Mixed-Effects Model Specification:**
    *   **Random Effects:** The prompt explicitly asks for ""random intercepts for 'location' nested within 'continent'"". The generated code uses `groups=data['location']` and `re_formula=~1` (which means random slope for the intercept, not a simple random intercept), which is not the correct way to specify nested random effects or even a simple random intercept. `re_formula=""1""` is for random intercept.
    *   **Fixed Effects:** The code includes `location_encoded` in the fixed effects, which is incorrect. The task asks for `'stringency_index'`, `'vaccination_percentage'`, and their interaction as fixed effects. Putting `location_encoded` (which likely results in many dummy variables) as fixed effects often makes a mixed-effects model redundant for location-level variability and can lead to multicollinearity issues, especially when random effects are also intended for location. OneHotEncoding on a high cardinality categorical variable like 'location' is generally not suitable for direct inclusion as fixed effects in a mixed-effects model where random effects are preferred to capture spatial variability.
    *   **Interaction Term:** The interaction term `stringency_index:vaccination_percentage` is correctly included, but `location_encoded` as a fixed effect overrides the expected model structure.

4.  **Incomplete Output and Interpretation:**
    *   The task asks for reporting fixed effects coefficients, standard errors, p-values, *and their interpretation*. The generated code only prints `model_fit.summary()`, which includes these, but lacks explicit interpretation.
    *   The task asks for reporting variance components for random effects. The generated code does not explicitly extract or report this.
    *   The discussion of model assumptions is only placeholders (""... (Discuss linearity, normality, homoscedasticity as above)""). While the prompt says ""no need to implement checks"", it does ask to ""briefly describe how they could be checked"", which is missing from the generated code.

5.  **Unused Imports and Variables:** `matplotlib.pyplot`, `seaborn`, `OneHotEncoder` are imported but largely unused/improperly used. `location_df` is created but not used in the model.

6.  **Code Quality:**
    *   The flow of the code is disjointed. Steps are numbered, but the implementation doesn't strictly follow the logical progression for data preparation (e.g., vaccination percentage calculation before its use for imputation).
    *   Error handling for file loading is missing.

In summary, the generated code misses fundamental steps related to data preprocessing, incorrectly handles missing values critically important for the defined `vaccination_percentage`, and fundamentally misunderstands the specification of mixed-effects models, particularly random effects and the inclusion of fixed effects that conflict with the intent of location-based random effects.",command-r7b-12-2024,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. Load the dataset
dataset = pd.read_csv('clash_royale_cards.csv')

# 2. Select features for clustering
features = ['elixirCost', 'Win Rate', 'Usage']
X = dataset[features]

# 3. Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. Determine appropriate numb...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, selects, and standardizes the specified features. It correctly implements the elbow method for determining `K` by plotting WCSS, applies K-Means with a chosen `K`, performs PCA for dimensionality reduction, and visualizes the clusters with the assigned labels.

Strong points:
- All required libraries are imported.
- The steps are logically ordered and clearly address each part of the prompt.
- The use of `n_init=10` and `random_state=42` for KMeans ensures reproducibility and good practice.
- The visualization correctly uses PCA components and colors points by cluster labels with a suitable colormap.

Areas for minor improvement:
- The error handling for the dataset loading (e.g., FileNotFoundError) is missing, which was present in the ground truth. While not strictly required by the prompt's output format, it's good practice.
- The plot labels for PCA components (`Principal Component 1`, `Principal Component 2`) could be enhanced to include the explained variance ratio for better interpretability, similar to the ground truth.
- The selection of `optimal_k = 3` is hardcoded. While this is acceptable given the task's instructions to ""choose K"", mentioning how this choice is justified (e.g., ""based on visual inspection of the Elbow Method plot"") would be beneficial for clarity if this were a production script.
- The `try-except` block for file loading in the ground truth is a robust way to handle potential issues.

Overall, the code is very good and directly addresses the prompt's requirements. The omissions are minor and do not detract from its core functionality.",command-r7b-12-2024,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

# 1) Load the dataset
data = pd.read_csv(""clash_royale_cards.csv"")

# 2) Select features and target
features = [""elixirCost"", ""Usage"", ""rarity""]
target = ""Win Rate""
data_features = data[featu...",5,"The generated code has significant issues in the one-hot encoding step that prevent it from being runnable and correct.

1.  **Incorrect OneHotEncoding Application**:
    *   `rarity_encoded = encoder.fit_transform(data_features[""rarity""].values.reshape(-1, 1))` correctly encodes the `rarity` column.
    *   However, `data_features[""rarity""] = rarity_encoded.astype(str)` attempts to assign `rarity_encoded` (which is a 2D NumPy array of numerical encoded values) back to the *single* `rarity` column in the DataFrame, converting it to strings. This is fundamentally wrong as it assigns an array to a single column, which Pandas will interpret as a series of arrays, or in this case, it will assign the string representation of an array for each row.
    *   `data_features = data_features.drop(columns=[""rarity""])` then drops the original `rarity` column.
    *   The one-hot encoded columns (e.g., `rarity_rare`, `rarity_epic`) were never added to the `data_features` DataFrame. This means the `train_data` and `test_data` passed to the models will only contain `elixirCost` and `Usage`, completely missing the `rarity` feature that was supposed to be one-hot encoded.

2.  **Missing `numpy` Import in Ground Truth**: The ground truth uses `np` for `np.hstack` though it's commented out in the actual code snippets. This can be misleading. The generated code also imports `numpy`, which is good, but the way it uses `OneHotEncoder` prevents the data from being correctly prepared.

3.  **Ambiguity in Model Choice**: The prompt stated ""Train a regression model (e.g., Linear Regression or RandomForestRegressor)"". The generated code trains *both* and evaluates both. While not strictly wrong, it adds complexity that wasn't explicitly asked for, and the output format only required printing one evaluation result. The ground truth chose one explicitly as an example.

4.  **No error handling for file not found**: The generated code assumes the CSV file exists, which is less robust than the ground truth's `try-except` block.

The fundamental issue is the incorrect handling of `OneHotEncoder` output, which leads to the models being trained on incomplete data. This makes the models incorrect for the task's requirements. If the `rarity` column was correctly encoded and added to the DataFrame, the rest of the model training and evaluation would likely work.",command-r7b-12-2024,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy.stats import f_oneway

# Step 1: Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Step 2: Identify unique categories in the 'rarity' column
rarity_categories = df['rarity'].unique()
print(""Unique 'rarity' categories:"", rarity_categories)

# Step 3: Prepare data for ANOVA test
win_rates = {}
for category in rarity_categories:
    category_df = df[df['rarity'] == category]
    win_rates[category] = category_df['Win Rate'].values

# Step 4: Sta...",6,"The generated code correctly loads the dataset, identifies unique rarity categories, and performs the ANOVA test. It also explicitly states the hypotheses and prints the F-statistic and p-value, which covers most of the requirements.

However, there are a few significant issues that prevent a higher score:
1.  **Inverse Interpretation of P-value:** The most critical error is in Step 6's interpretation of the p-value. It reverses the standard interpretation. If `p_value < alpha`, you *reject* the null hypothesis, indicating a *significant* difference. If `p_value >= alpha`, you *fail to reject* the null hypothesis, indicating *no significant* difference. The generated code has this logic inverted.
2.  **Missing Error Handling/Robustness:** The ground truth code includes robust error handling for `FileNotFoundError`, `NaN` values in 'Win Rate' and 'rarity', and checks for sufficient data for ANOVA (e.g., at least two groups with at least two observations each). The generated code lacks these crucial checks, which makes it less robust to real-world data issues. For instance, if 'Win Rate' isn't purely numeric, `pd.to_numeric` with `errors='coerce'` is essential.
3.  **No `pd.to_numeric` for 'Win Rate':** The `Win Rate` column might not always be purely numeric in the CSV. The ground truth explicitly handles this conversion, which is a good practice for data cleanliness before statistical tests.
4.  **No `dropna` for `Win Rate` or `rarity`:** If there are `NaN` values in `Win Rate` or `rarity`, these should ideally be dropped before performing the statistical test, as they would skew or prevent the calculation.

To improve, the code should rectify the p-value interpretation, add robust error handling for file loading and data quality, and ensure the 'Win Rate' column is properly numeric before use.",command-r7b-12-2024,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, f1_score

# 1. Load the dataset
data = pd.read_csv('iris.csv')

# 2. Define features and target
X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm',...",6,"The generated code correctly loads the dataset, defines features and target, encodes the target, and splits the data. It also trains the specified models and prints their performance.

However, there are two significant issues:
1. **Incorrect Precision and Recall Calculation:** The code incorrectly assigns `f1_score(y_test, y_pred, average='macro')` to `precision_lr` and `recall_lr` (and similarly for SVC and RF). This means that precision and recall are not actually being calculated and reported; instead, the F1-score is being reported three times under different labels. The task explicitly asked for precision and recall (macro average).
2. **Missing `stratify` in `train_test_split`:** The ground truth code used `stratify=y_encoded` which is important for maintaining class distribution in imbalanced datasets. While Iris is not severely imbalanced, it's good practice for classification tasks and was implied by the careful splitting in the ground truth.

Other minor points:
- The generated code attempts to import `StandardScaler` but does not use it. This is not explicitly wrong but indicates an unused import.
- The `classification_report` import is also unused.
- The reporting of metrics could be more structured (e.g., using a loop and dictionary like the ground truth) to make it more concise and less repetitive, especially when comparing multiple models.
- The `max_iter` for Logistic Regression was not specified, which can sometimes lead to convergence warnings in more complex datasets (though it might not be an issue for iris). The ground truth addressed this.

To improve to a higher score (8+), the primary fix would be to correctly calculate and report precision and recall using `precision_score` and `recall_score` respectively. Adding `stratify=y_encoded` to `train_test_split` would also be a good improvement.",command-r7b-12-2024,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder

# 1) Load the dataset
iris = pd.read_csv(""iris.csv"")

# 2) Select the features
selected_features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
iris_selected = iris[selected_features]

# 3) Standardize these features using StandardScaler
scaler =...",8,"The generated code correctly performs all the required steps: loading data, selecting and standardizing features, applying K-Means and PCA, and generating both scatter plots.

The strong points are:
- Correct selection and standardization of features.
- Correct application of K-Means with specified parameters (`n_clusters=3`, `random_state=42`, `n_init='auto'`).
- Correct application of PCA to 2 components.
- Correct generation of the first plot, coloring points by K-Means labels.
- Correct encoding of 'Species' and generation of the second plot, coloring points by true species labels.
- Proper use of `matplotlib.pyplot` for plotting.

Areas for improvement to achieve a higher score:
- **Plotting Enhancements:** The ground truth code includes `colorbar` for both plots, which significantly enhances readability by explaining what the colors represent. The generated code lacks colorbars, making the plots less informative, especially for the true species plot where the numerical encoding isn't immediately obvious.
- **Figure Sizing & Grid:** The ground truth sets `figsize` for plots and adds `grid(True)`, which are good practices for better visualization and readability. The generated code omits these.
- **Robustness:** The ground truth code includes basic error handling for `FileNotFoundError` when loading the CSV, which is a good practice for more robust scripts.

Overall, the core functionality is accurate, but small details in plotting and robustness could be improved.",command-r7b-12-2024,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv(""iris.csv"")

# Create new features
df[""PetalArea""] = df[""PetalLengthCm""] * df[""PetalWidthCm""]
df[""SepalArea""] = df[""SepalLengthCm""] * df[""SepalWidthCm""]

# Drop original petal and sepal length/width
df = df.drop([""PetalLengthCm"", ""Peta...",6,"The generated code correctly loads the dataset, creates the `PetalArea` and `SepalArea` features, encodes the target variable, and trains a Gradient Boosting Classifier. It also performs 5-fold cross-validation and reports the mean accuracy.

However, there is a critical error in how the features `X` are defined. The instruction explicitly states: ""Define features `X` using the original four numerical features (`SepalLengthCm`, `SepalWidthCm`, `PetalLengthCm`, `PetalWidthCm`) AND the two newly created features (`PetalArea`, `SepalArea`).""

The generated code drops the original four numerical features:
```python
df = df.drop([""PetalLengthCm"", ""PetalWidthCm"", ""SepalLengthCm"", ""SepalWidthCm""], axis=1)
```
This means that `X` subsequently only contains the new `PetalArea` and `SepalArea` features, along with `Species` (which is then dropped for `X`). This violates the requirement to use *all six* specified features for `X`. This omission makes the solution incorrect as per the task definition for feature selection.

Additionally, while performing cross-validation manually with `StratifiedKFold` and then calculating the mean is functionally correct, using `sklearn.model_selection.cross_val_score` (as in the ground truth) would have been more concise and directly aligned with the expectation for standardized cross-validation reporting. The `sklearn.metrics.accuracy_score` import is not strictly necessary if `cross_val_score` is used with `scoring='accuracy'`.

To improve, the code should correctly define `X` to include all six specified features, and ideally, use `cross_val_score` for simplicity and directness if the task implies its use over a manual loop.",command-r7b-12-2024,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load the dataset
data = pd.read_csv('heart.csv')

# Define features and target
features = ['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG', 'MaxHR', 'ExerciseAngi...",5,"The generated code attempts to address all requirements but falls short in critical preprocessing steps, leading to incorrect feature handling and potential data leakage.

**Issues:**

1.  **Incorrect Feature Combination and Scaling:**
    *   The `OneHotEncoder` is applied only to `data[categorical_features]`.
    *   Then, `X_encoded_df` (which contains only the one-hot encoded categorical features) is concatenated with `data[features]`. This means the original numerical features are included twice: once as `data[features]` (which also contains the *original* categorical features) and once the one-hot encoded versions. This is fundamentally wrong. It doubles the numerical features if they are part of `features`.
    *   `X_encoded_df` should be concatenated with the numerical features *only* from the original `X` (or `data`), and then the result `X` (containing both preprocessed numerical and encoded categorical features) should be scaled.
    *   As implemented, `X = pd.concat([X_encoded_df, data[features]], axis=1)` creates a DataFrame where `data[features]` *still contains the original categorical features* alongside the numerical ones. When `StandardScaler` is applied to this `X`, it will attempt to scale columns like 'Sex', 'ChestPainType', etc., which are categorical strings/objects, causing errors or incorrect behavior.
    *   The `StandardScaler` is applied to the incorrect `X` at the end of the preprocessing (`X_scaled = scaler.fit_transform(X)`). `X` at this point contains both one-hot encoded categorical features AND original numerical/categorical features from `data[features]`. This setup will lead to `StandardScaler` attempting to scale already encoded features and potentially the original categorical columns, which is wrong.

2.  **Lack of `ColumnTransformer` / Pipeline for Robust Preprocessing:** The ground truth correctly uses `ColumnTransformer` within a `Pipeline`. This ensures different transformations are applied to different column types and avoids data leakage by fitting the preprocessor only on the training data. The generated code performs transformations manually, which is more prone to errors and data leakage if not handled carefully (though the split is before scaling, the issue is how features are combined).

3.  **ROC-AUC Calculation:** `roc_auc_score(y_test, y_pred, raise_warning=False)` is incorrect for ROC-AUC. ROC-AUC requires prediction probabilities (`y_proba`) for the positive class, not the binary predictions (`y_pred`). This will produce a warning (`raise_warning=False` suppresses it) and an incorrect ROC-AUC score, as it's essentially calculating the AUC of a step function based on hard predictions, not the continuous probability scores.

The primary reasons for the low score are the fundamental errors in preprocessing logic (feature duplication/misapplication of scaler) and the incorrect ROC-AUC calculation method. These issues mean the classification model is trained on malformed data and evaluated incorrectly, rendering the results unreliable.",command-r7b-12-2024,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score

# Step 1: Load the dataset
dataset = pd.read_csv('heart.csv')

# Step 2: Select numerical features for clustering
selected_features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']

# Step 3: Standardize the selected features
scaler = Standar...",6,"The generated code addresses most of the requirements but has significant issues in the implementation of the silhouette score calculation and the PCA visualization.

**Strengths:**
- Correctly loads the dataset.
- Correctly selects and standardizes the specified numerical features.
- Correctly implements the elbow method (WCSS).
- Correctly applies K-Means with `n_init='auto'` and `random_state=42`.
- Correctly performs PCA to 2 components.

**Weaknesses and Areas for Improvement:**

1.  **Incorrect Silhouette Score Calculation:** The silhouette score calculation in the generated code is completely incorrect and non-standard. It attempts to compute `cluster_similarities` and then uses `silhouette_score` with `cluster_similarities` as `labels`, which is not how `silhouette_score` from `sklearn.metrics` is used. The `silhouette_score` function expects `data` and `labels` as arguments, computed once per `k` using the full dataset and the labels returned by `kmeans.fit`. The current implementation also attempts to calculate individual silhouette scores which are then aggregated or stored in `silhouettes` list, but this list seems to reset or is not used directly in a standard way for plotting or comparison. This is a critical error in meeting requirement 4.

2.  **Missing Elbow and Silhouette Plots:** The task explicitly asks for an elbow plot and silhouette score plot to *justify* the choice of K. The generated code calculates WCSS but does not plot it. It also attempts to calculate silhouette scores in an incorrect way and does not plot them, making it impossible to *justify* the chosen K of 3.

3.  **Incorrect `cluster_labels` for Plotting:**
    `cluster_labels = kmeans.labels_.reshape(-1, 1)` is correct.
    However, `cluster_labels = pca.transform(cluster_labels)` is fundamentally wrong. You are trying to apply PCA to the cluster labels themselves, which are integer assignments, not the original feature space. This will lead to an error or nonsensical results. The `c` argument in `plt.scatter` should directly take the `kmeans.labels_` array to color the points based on their cluster assignment.

4.  **No Legend for Clusters:** The `label` argument in `plt.scatter` is set to `f'K-Means clusters (K={K})'`, which will create a single legend entry for *all* points, not distinguishing between the individual clusters as required for a cluster visualization. The ground truth code iterates through each cluster to plot them separately with distinct labels and colors, which is the standard way to achieve this.

5.  **Hardcoded K:** While `K=3` might be a reasonable choice, the task asks to *determine* an appropriate number of clusters and *justify* it using the plots. Hardcoding `K=3` without presenting the plots makes the justification step impossible.

These issues, particularly the incorrect silhouette score calculation, the absence of the required plots for K selection, and the error in PCA visualization, significantly impact the correctness and completeness of the solution.",command-r7b-12-2024,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report

# 1) Create binary target variable
data = pd.read_csv('winequality-red.csv')
data['quality'] = data['quality'].apply(lambda x: 1 if x >= 7 else 0)

# 2) Preprocess the data
scaler = StandardScaler()
data[['fixed acidity'...",7,"The generated code correctly addresses most of the task requirements but has some critical omissions and errors that prevent it from being a fully correct and complete solution.

**Strong Points:**
*   Correctly loads data and performs the binary target variable creation.
*   Correctly splits data into train/test sets.
*   Trains a Logistic Regression model.
*   Evaluates performance using accuracy, F1-score, and classification report.
*   Attempts to report feature importance using coefficients.

**Areas for Improvement (leading to score of 7):**

1.  **Incomplete Feature Scaling:** The generated code only scales a subset of the features (`['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'density']`). It misses scaling several other numerical features like 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'pH', 'sulphates', and 'alcohol'. This is a significant error as logistic regression performance can be severely impacted by unscaled features, and it violates the ""scale features appropriately"" requirement.
2.  **Missing Value Handling:** The code does not include any explicit steps for handling missing values. While `winequality-red.csv` typically doesn't have missing values, the prompt explicitly asked to ""handle missing values if any"". The ground truth includes a robust check and median imputation.
3.  **Feature Importance Handling:**
    *   The `print(coefficients.sort_values(ascending=False).head(10))` line only prints the top 10 coefficients which are positive. It doesn't consider features with large *negative* coefficients, which are equally important for determining feature influence. The prompt asked for ""most important features according to the model coefficients,"" implying absolute importance.
    *   The problem statement asks to ""report the most important features,"" not just print the top N. The ground truth provides a sorted DataFrame with absolute coefficients, which is a much clearer and more complete way to present feature importance.

**Minor Issues:**
*   It would be good practice to print shapes of X_train, X_test, etc., after splitting data, similar to the ground truth, for better debugging and understanding.
*   The `F1-score` calculation uses `average='weighted'`. While not strictly wrong, the ground truth uses `average='binary'` by default (since F1-score for binary classification usually focuses on the positive class). Given the class imbalance common in this dataset for the target (quality >= 7 vs < 7), reporting F1 for the positive class or using `average='binary'` might be more insightful for evaluating the detection of ""good"" wines specifically. However, `weighted` is acceptable for overall performance.
*   The `max_iter` was increased to 200, which is good, but `solver='liblinear'` (as in the ground truth) is often a better choice for small datasets with `L1/L2` regularization and can sometimes prevent convergence warnings.",command-r7b-12-2024,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

# 1) Load the dataset
data = pd.read_csv('heart.csv')

# 2) Create a box plot
sns.boxplot(x='HeartDisease', y='Oldpeak', data=data)
plt.show()

# 3) Perform logistic regression
model = LogisticRegression()
model.fit(data[['Oldpeak']], data['HeartDisease'])
coefficient = model.coef_[0]
intercept = model.intercept_

print(f""Logistic Regression ...",8,"The generated code correctly addresses most aspects of the task.

**Strong points:**
*   Correctly loads the dataset.
*   Successfully creates the box plot as requested.
*   Performs logistic regression and prints the coefficient for `Oldpeak`.
*   Correctly uses `pd.qcut` to discretize `Oldpeak` into three quantile-based bins.
*   Accurately calculates and displays the proportion of `HeartDisease=1` within each bin.
*   Includes all necessary imports and uses appropriate libraries.

**Areas for improvement:**
*   **Interpretation of Oldpeak coefficient:** The interpretation provided for the logistic regression coefficient is partially incorrect. While it correctly states that `exp(coefficient)` is the odds ratio, the formula `1 + (coefficient / (1 - coefficient))` is not the correct way to calculate the odds ratio from the coefficient, nor is it a standard interpretation. The ground truth correctly interprets `exp(coefficient)` directly as the multiplicative factor for odds. This was explicitly asked for in the prompt (""interpret its meaning in the context of odds ratios"").
*   **Error Handling:** The generated code lacks error handling for the `pd.read_csv` operation (e.g., `FileNotFoundError`). The ground truth includes a `try-except` block for this.
*   **Informative Output/Visualization:** While the plots are generated, they lack titles, axis labels, and other common aesthetic improvements that make plots more readable (e.g., `plt.title`, `plt.xlabel`, `plt.ylabel`). The ground truth goes into more detail here, which is standard practice for creating presentable plots. For the final proportions, printing just the series is fine, but the ground truth also includes a bar plot for visualization, which adds value.
*   **Clarity of variable names:** Using `data` instead of `df` for the DataFrame is a minor stylistic point, but `df` is more common and descriptive for DataFrames.
*   **Missing numerical interpretation of odds ratio:** The prompt specifically asked to ""report the coefficient for `Oldpeak` and interpret its meaning in the context of odds ratios"". While `exp(coefficient)` is calculated, the numerical interpretation (e.g., ""for every one-unit increase in Oldpeak, the odds of heart disease are multiplied by X"" or ""increase/decrease by Y%"") is missing, which the ground truth provides in detail. This was a core part of the interpretation requirement.",command-r7b-12-2024,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

# 1) Load data
data = pd.read_csv('spotify_2023.csv')

# 2) Convert `streams` to numeric (coerce errors to NaN) and drop rows where `streams` is NaN
data['streams'] = pd.to_numeric(data['streams'], errors='coerc...",6,"The generated code addresses most of the requirements but has several critical errors or omissions that affect its correctness and completeness.

**Areas for Improvement:**

1.  **Streams conversion and NaN handling:**
    *   The line `data['streams'] = pd.to_numeric(data['streams'], errors='coerce').fillna(0)` is incorrect. The task specifically says ""Drop rows where `streams` is NaN"". By using `.fillna(0)`, the code imputes `NaN` values with `0` instead of dropping them, which fundamentally changes the `streams` distribution and the definition of ""popularity"". Then `data.dropna(subset=['streams'])` has no effect because `NaN` values were already filled. This is a critical deviation from the task requirement.
2.  **Feature selection mismatch:**
    *   The code separates `selected_features` and `feature_names`. The `selected_features` list misses the '%' for `danceability_%`, `energy_%`, `valence_%`, `acousticness_%`. This will lead to `KeyError` unless the dataframe columns somehow don't have the `%` in their actual names (which is unlikely given the task description). This is a significant bug.
3.  **One-Hot Encoding Implementation:**
    *   The `OneHotEncoder` usage is flawed. `encoder.fit_transform(data[['key']])` and `encoder.fit_transform(data[['mode']])` create two separate encoders that are fitted independently. This means `encoder.get_feature_names(['key', 'mode'])` will not work as intended for a combined encoding, and `encoder.get_feature_names()` (which is deprecated in newer versions of scikit-learn, with `get_feature_names_out` being the replacement) would only give names related to the *last* feature fitted (`mode`).
    *   More correctly, one would usually apply `get_dummies` directly on the DataFrame columns, or use a `ColumnTransformer` with `OneHotEncoder` if complex preprocessing steps are needed. The current approach of `np.concatenate` and `pd.DataFrame` construction is overly complicated and prone to errors.
    *   The `drop(columns=['key', 'mode'])` should happen *before* the data split, and after the features `X` are defined, or the one-hot encoded columns need to be correctly integrated into `X` *before* the split. The way it's done now, `X = data.drop(columns=['streams', 'is_popular'])` will attempt to drop `streams` and `is_popular` from `data` which *still contains* `key` and `mode` because the `drop` operation on `key` and `mode` is applied to `data` *after* `X` and `y` are defined. This means `X` will contain the original `key` and `mode` columns (if they are not dropped in an earlier step) and potentially duplicate one-hot encoded columns (if not handled carefully).
4.  **ROC AUC Score Calculation:**
    *   `roc_auc_score(y_test, y_pred)` is incorrect. ROC AUC requires probability scores for the positive class (`model.predict_proba(X_test)[:, 1]`), not binary predictions (`y_pred`). Using binary predictions will lead to a degenerate ROC curve and an incorrect score.
5.  **Clarity and Structure:**
    *   The code is generally well-structured with comments matching the numbered requirements, but the variable management regarding `data` and `X` becomes confusing around the one-hot encoding step.
    *   Missing proper imports for `StandardScaler` (which is imported but never used).
    *   No print statements for intermediate steps or final model training confirmation, making it harder to track progress.

Overall, the code attempts to follow the instructions but fails on critical data preparation and evaluation steps, particularly concerning the handling of `streams` NaNs, feature column names, and the correctness of one-hot encoding and ROC AUC calculation.",command-r7b-12-2024,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 1) Load data
data = pd.read_csv(""spotify_2023.csv"")

# 2) Select features and drop rows with NaNs
selected_features = [
    ""danceability_%"",
    ""energy_%"",
    ""valence_%"",
    ""acousticness_%"",
    ""instrumentalness_%"",
    ""liveness_%"",
    ""speechiness_%"",
    ""bpm"",
]
data = data.dropna(...",8,"The generated code correctly performs all the requested steps for K-Means clustering and visualization. It successfully loads data, selects and handles NaNs in the specified audio features, standardizes them, applies K-Means with the correct parameters, performs PCA, generates the scatter plot, and calculates the mean values of original features per cluster.

Strong points:
- All required libraries are imported.
- Data loading is correct.
- Feature selection and NaN handling are properly addressed using `dropna(subset=...)`.
- Standardization is done correctly.
- K-Means parameters (`n_clusters`, `random_state`, `n_init`) are as specified.
- PCA parameters (`n_components`, `random_state`) are as specified.
- The scatter plot correctly uses PCA components and K-Means labels for coloring.
- The mean values of original features per cluster are calculated and displayed correctly.

Areas for improvement (minor):
- Error handling for `FileNotFoundError` for the CSV file is missing, which was present in the ground truth and is good practice.
- The handling of `data[selected_features].values` before `fit_transform` in standardization is a slight deviation from typical pandas workflow where `fit_transform` can directly operate on a DataFrame slice without `.values`. While it works, it's a minor stylistic point. However, it's necessary for the subsequent `kmeans.labels_` to align with the indices correctly if `data` was a filtered DataFrame. In this case, `data` is already filtered by `dropna`, so `data[selected_features]` would suffice.
- The plot could benefit from a legend or color bar to clearly indicate which color corresponds to which cluster, similar to the ground truth. This is a visual nicety that aids interpretation.
- Printing the cluster means directly in a raw DataFrame format is functional but could be presented with a more descriptive header or introduction.

These improvements are minor and do not detract significantly from the correctness or completeness of the solution.",command-r7b-12-2024,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# 1) Load data
data = pd.read_csv(""spotify_2023.csv"")

# 2) Convert 'streams' to numeric (coerce errors to NaN)
data['streams'] = pd.to_numeric(data['streams'], errors='coerce')

# 3) Filter for songs with 'streams' greater than 10,000,000 and where 'released_year' is between 2010 and 2023, inclusive. 
# Drop rows with NaN 'streams' after conversion.
data = data[(data['streams'] > 10000000) & (data['released_year']...",7,"The generated code correctly addresses most of the requirements but has a significant logical flaw in the filtering and grouping steps which will lead to incorrect results, and some minor issues with plotting details.

**Strengths:**
*   Correctly loads the data.
*   Correctly converts `streams` to numeric with `errors='coerce'`.
*   Attempts to filter by `streams` and `released_year` and drop NaNs.
*   Attempts to group by `released_year` and calculate averages.
*   Generates a plot for each feature as required, with titles and labels.
*   Uses `plt.tight_layout()` for better plot presentation.

**Areas for Improvement / Issues:**

1.  **Incorrect Filtering Order and Logic (Major Flaw):**
    *   The code first filters `data = data[(data['streams'] > 10000000) & (data['released_year'].between(2010, 2023))]`. This filter *implicitly* drops rows where `streams` became NaN during conversion *if* those rows also triggered the `streams > 10000000` condition (because `NaN > 10000000` evaluates to `False`).
    *   **However, the problem is that the `dropna(subset=['streams'])` is applied *after* this combined filter.** This means any rows where `streams` was `NaN` *and* `released_year` was *outside* the 2010-2023 range would have been kept until the `dropna` step. More critically, if a `NaN` stream value *could* have been a valid stream value (had it been numeric) that would have been greater than 10M, it still gets implicitly excluded by the ""> 10M"" condition.
    *   The ground truth correctly handles this: first drop `NaN` streams, *then* filter for `streams > 10M`. This ensures that *only* valid numeric stream values are considered for the `> 10M` filter. The prompt explicitly says ""Drop rows with NaN `streams` after conversion"" before the 10M filter contextually.

2.  **Incorrect Grouping and Aggregation (Major Flaw):**
    *   The line `data = data.groupby('released_year')` assigns the `DataFrameGroupBy` object back to `data`.
    *   Subsequently, `avg_danceability = data['danceability_%'].mean()` attempts to call `.mean()` on a `SeriesGroupBy` object, which is correct in isolation. However, if you want *all three* averages computed from the *same* grouping operation on the *same* filtered data, it's more idiomatic and potentially more robust to group once and then apply `.mean()` to the desired columns, e.g., `yearly_avg_features = data.groupby('released_year')[['danceability_%', 'energy_%', 'valence_%']].mean()`.
    *   As written, it works because `data` is a `DataFrameGroupBy` object, and selecting `data['danceability_%']` effectively makes it a `SeriesGroupBy` object. So, technically, this part isn't *incorrect* but less efficient if you were to add more analyses.

3.  **Plot Layout:**
    *   The request was for plots for *each* of these three average audio features over `released_year` and implies separate plots or subplots aligned horizontally for easier comparison (as shown in the ground truth's `plt.subplot(1, 3, X)`). The generated code uses `plt.subplot(3, 1, X)`, which stacks them vertically. While not strictly ""wrong"", horizontal alignment is generally preferred for time series comparisons if space allows.

4.  **No `Year` Label for `released_year` in Plots:** The x-axis labels are just 'Year', but the ground truth uses 'Released Year', which is more specific to the column name and task. This is a minor point.

The most critical issue is the filtering order and subsequent reliance on `data` being a `groupby` object which, while it works, is not the most standard or readable way to compute multiple grouped means if you're chaining operations. The filtering order could lead to different included data points.",command-r7b-12-2024,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder, fill_not_null
import numpy as np

# Load the dataset
data = pd.read_csv('youtube_trendings.csv')

# Clean and prepare the data
data['Likes'] = pd.to_numeric(data['Likes'].str.replace(',', ''), errors='coerce').fillna(0)
data['Dislikes'] = pd.to_numeric(data['D...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being fully correct and robust.

**Correctness & Completeness Issues:**
1.  **`published` column handling:** The prompt explicitly asks to treat `published` (year) as a numeric feature. The generated code attempts to convert the entire `published` column to numeric (`pd.to_numeric(data['published'], errors='coerce').fillna(0)`). This is incorrect because `published` typically contains full date strings (e.g., ""2023-01-01""). The ground truth correctly extracts the year from the date string. This is a significant error as it misinterprets the data.
2.  **Imports:** `from sklearn.preprocessing import OneHotEncoder, fill_not_null` includes `fill_not_null` which is not a standard scikit-learn function and seems to be a typo or misunderstanding. It's not used in the code either.
3.  **One-Hot Encoding Feature Names:** The line `encoder.fit(data[['Category']])` and `encoder.get_feature_names_out(['Category'])` are correct for `OneHotEncoder`. However, the subsequent concatenation `pd.concat([data.drop(['Category'], axis=1), data_encoded], axis=1)` is problematic because `data` already has an index defined, and when `data_encoded` is created from `category_encoded` (a NumPy array), it gets a default integer index. If the original `data` dataframe was not reset after some operations, their indices might not align, leading to misaligned feature columns after concatenation. The ground truth uses `pd.get_dummies` which is generally safer for direct OHE integration into a DataFrame within pandas. Also, using `OneHotEncoder` typically requires handling the sparse matrix output. The call to `.toarray()` is good, but `pd.DataFrame(category_encoded, columns=encoder.get_feature_names_out(['Category']))` will create a new DataFrame with a default index, which might not align with `data.drop(['Category'], axis=1)`.
4.  **Feature Definition (`X`):** The line `X = data_encoded[['Dislikes', 'Video views', 'published'] + [col for col in data_encoded.columns if col not in ['Likes']]]` is conceptually flawed and overly complex.
    *   It tries to include 'Likes' in the `[col for col in data_encoded.columns if col not in ['Likes']]` part if it exists after the concatenation process (which it should). This means it is trying to include the target variable `Likes` within the features `X`, which is a form of data leakage and incorrect.
    *   The `published` column here refers to the column that was incorrectly converted to numeric, not the year.
    *   A much simpler and correct way is to explicitly list the desired feature columns (Dislikes, Video views, published\_year (if correctly extracted), and the OHE category columns). The ground truth correctly identifies `X` features by combining known numeric columns and the OHE columns.

**Code Quality:**
1.  **Readability:** The feature definition for `X` is hard to read and understand.
2.  **Robustness:** No error handling for file not found, or missing columns, which the ground truth includes.
3.  **Unnecessary Import:** `fill_not_null` in imports.",command-r7b-12-2024,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Step 1: Clean the 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '').astype(float)
df['Video views'].fillna(0, inplace=True)

# Step 2: Ensure the 'published' column is treated as an integer
df['published'] = df['published'].astype(int)

# Group the data by 'published' year and calculate metrics
grouped_data = df.groupby('published').agg(
   ...",6,"The generated code addresses most of the requirements but has critical flaws in data handling and plotting.

**Correctness:**
*   **Video Views Cleaning:** Partially correct. `df['Video views'].str.replace(',', '').astype(float)` correctly removes commas and converts to float. `fillna(0, inplace=True)` then handles NaNs. This part is good.
*   **Published Year Treatment:** Incorrect. The prompt states ""Ensure the `published` column (year) is treated as a numeric or integer type"". It implies `published` column exists directly as a year, or a column like `publishedAt` exists from which year needs to be extracted. The ground truth code assumes `publishedAt` and extracts the year from it. The generated code (`df['published'] = df['published'].astype(int)`) assumes a column already named `published` exists and directly converts it to int. This is problematic if `published` only contains the full date string as in the actual dataset (`youtube_trendings.csv` typically has a `publishedAt` column). If such `published` column is not year, this will lead to errors or incorrect interpretation. Based on common `youtube_trendings.csv` schemas, this is likely incorrect.
*   **Plotting:** Incorrect. The generated code attempts to create two plots on a single subplot using `twinx()`. The task explicitly asks for ""a figure with two subplots: the first subplot should be a line graph... and the second subplot should be a line graph..."". `twinx()` overlays two plots on the same axes with separate y-scales, which is not what was requested (separate subplots). The titles are also set only for the second axis (`ax2.set_title('Number of Unique Videos per Year')`) which overrides the first (`ax1.set_title('Average Video Views per Year')`), leaving only one title visible for the whole axes, not for individual ""subplots"" as intended.

**Code Quality:**
*   Imports are good.
*   Variable naming is clear.
*   Comments are brief but helpful.

**Completeness:**
*   Loads dataset: Yes.
*   Cleans `Video views`: Yes (correctly).
*   Treats `published` year as numeric: No, it makes an incorrect assumption about the `published` column name and content.
*   Groups data: Yes.
*   Calculates metrics: Yes.
*   Creates figure with two subplots: No, it creates a single subplot with a `twinx()` axis, which is not the same as two *separate* subplots.
*   Appropriate titles and axis labels: Partially. Titles are only visible for the last plotted series due to `twinx()` usage.

To improve, the generated code needs to:
1.  Correctly extract the year from the `publishedAt` column (or similar datetime column) if `published` itself is not the year.
2.  Use `plt.subplots(nrows=2, ncols=1, ...)` to create two distinct subplots as explicitly requested, instead of `twinx()`. Each subplot would then get its own title and labels.",command-r7b-12-2024,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
import string
import nltk
from nltk.corpus import stopwords

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# 1) Create a new feature `title_word_count` representing the number of words in each title
df['title_word_count'] = df['Video'].str.split().str.len()

# 2) Clean 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '').astype(float).fillna(0)

# 3) Calculate Pearson correlation co...",7,"The generated code addresses most of the requirements but has several issues that prevent it from scoring higher.

**Issues:**

1.  **NLTK Download:** The code assumes `nltk.corpus.stopwords` is already downloaded. While the ground truth handles this with a `try-except` block and `nltk.download('stopwords')`, the generated code does not. This would cause a `ResourceCorpusReader` error if 'stopwords' are not already present, making the code less robust.
2.  **Date Filtering:** The line `df_filtered = df[df['published'].dt.year.between(2023, 2024)]` assumes the 'published' column is already in datetime format. The ground truth explicitly converts it using `pd.to_datetime`, which is a necessary and more robust step, especially if the column isn't guaranteed to be datetime. This will likely cause an `AttributeError: Can only use .dt accessor with datetimelike values` if 'published' is a string.
3.  **Punctuation Removal in `process_text`:** The regex `re.sub(f'[{string.punctuation}]', ' ', text)` replaces punctuation with a space. While this works, it can lead to multiple spaces between words, which `split()` handles, but the ground truth's `re.sub(r'[^\w\s]', '', title)` effectively removes punctuation without introducing extra spaces, which can be slightly cleaner for this task. The ground truth also uses `word.isalpha()` which is helpful to remove numbers or empty strings that might result from aggressive punctuation removal and splitting.
4.  **Implicit Type Conversion for `Video views`:** While `df['Video views'].str.replace(',', '').astype(float).fillna(0)` is concise, explicitly using `pd.to_numeric` with `errors='coerce'` as done in the ground truth is generally preferred for robustness when converting to numeric, as it handles a wider range of non-numeric cases by turning them into NaNs before filling (though in this specific case with just commas, `astype(float)` might work).
5.  **Output Clarity:** The output for the correlation and most common words is less verbose than the ground truth. While not a functional error, the ground truth provides more context, which is beneficial for a user.

**Strong Points:**

*   Correctly calculates `title_word_count`.
*   Correctly removes commas from `Video views` and fills NaNs.
*   Correctly calculates the Pearson correlation coefficient.
*   Attempts to filter by year (though misses the `to_datetime` step).
*   Correctly converts text to lowercase and removes stop words.
*   Correctly finds the 5 most common words.
*   Uses appropriate libraries (`pandas`, `re`, `collections.Counter`, `nltk`).",command-r7b-12-2024,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score

# 1) Load data
data = pd.read_csv(""mobiles_2025.csv"")

# 2) Clean 'Launched Price (USA)' (remove 'USD ', commas, to numeric, errors to NaN)
data['Launched Price (USA)'] = data['Launched Price (USA)'].str.replace('USD ', '', regex=True).str.repla...",7,"The generated code attempts to address all parts of the task but has several issues that affect its correctness and robustness, particularly around data cleaning and NaN handling.

Here's a breakdown:

**Positive Aspects:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Logical Flow:** The code follows the requested step-by-step process (load data, clean, encode, split, train, evaluate).
*   **Target Creation:** `is_expensive_usa` generation is largely correct.
*   **Model Training & Evaluation:** Correctly uses `RandomForestClassifier`, `f1_score`, and `roc_auc_score`.

**Areas for Improvement/Correctness Issues:**

1.  **Launched Price (USA) Cleaning and Target Creation:**
    *   **Incorrect `dropna` usage:** `data['is_expensive_usa'] = (data['Launched Price (USA)'] > 1000).astype(int)` directly converts `True`/`False` to `1`/`0`. If `data['Launched Price (USA)']` initially has NaNs (from `errors='coerce'` in `pd.to_numeric`), then `data['Launched Price (USA)'] > 1000` will produce `False` (for `NaN`) instead of `NaN`. The task explicitly required `errors='coerce'` for `pd.to_numeric` on the *original* price string, and then `is_expensive_usa` should be `NaN` if the price was unparseable. The ground truth correctly uses `pd.to_numeric(..., errors='coerce')` *first* on the price string, then creates `is_expensive_usa` as `1` (if > 1000), `0` (if <= 1000 and not NaN), or `NaN` (if the original price was NaN). The generated code's approach of `astype(float)` directly after `str.replace` means that any string that *cannot* be converted to float will raise a `ValueError` directly (or `NaN` if `errors='coerce'` was used, but the `astype(float)` here is less robust than `pd.to_numeric`).
    *   **Loss of unparseable price info:** By doing `data['Launched Price (USA)'].astype(float)` without `errors='coerce'` or a more robust `pd.to_numeric`, it risks failing when a price cleanup isn't perfect. Even if it doesn't fail, any `NaN`s in `data['Launched Price (USA)']` *after* the `astype(float)` would convert `(NaN > 1000)` to `False`, leading to `0` for `is_expensive_usa` instead of `NaN`.
    *   **`dropna(subset=['is_expensive_usa'])`:** While this correctly drops rows where `is_expensive_usa` is NaN, it's problematic because `is_expensive_usa` (as defined in the generated code) *should not have NaNs* unless the `str.replace` itself produced a NaN which is unlikely in this context. The ground truth correctly creates `is_expensive_usa` as `NaN` specifically when the *cleaned numeric price* is `NaN` (i.e., unparseable).

2.  **Feature Cleaning (Step 3):**
    *   **`astype(float)` direct conversion:** Similar to the price cleaning, directly using `.astype(float)` after `str.replace` implicitly assumes the replacement strings will always be valid numbers. `pd.to_numeric(errors='coerce')` is more robust for data cleaning, as it will replace any non-numeric result with `NaN`, allowing for proper downstream NaN handling.
    *   **Camera Cleaning:** The `extract_camera_mp` function is flawed. It looks for *any* digit, not specifically a numerical MP value. E.g., for ""Dual 12MP + 5MP"", it would return `12`. For ""20MP"", it would return `20`. But if the string is ""No Camera"", it would return `0`, which is fine, but it misses the pattern `(\d+(\.\d+)?)\s*MP` as used in the ground truth. The generated code's parser can incorrectly pick up numbers not associated with MP if the string is complex (e.g., ""iPhone 10 with 15MP cam""). The ground truth's regex is much more precise.

3.  **One-Hot Encoding (Step 4):**
    *   **`cat.codes` then `OneHotEncoder`:** This is a redundant and incorrect approach. `cat.codes` converts categories to numerical integer codes. Applying `OneHotEncoder` *on these integer codes* will treat them as ordinal values and one-hot encode the *integer codes themselves*, which is not the same as one-hot encoding the original categorical strings. The correct approach is to apply `OneHotEncoder` (or `pd.get_dummies`) directly on the string/object categorical columns.
    *   **`pd.merge` for one-hot encoded DFs:** While technically functional, `pd.get_dummies` is generally more straightforward for this specific use case as it handles the creation and merging of new columns directly within the original DataFrame, often more efficiently. The `left_index=True, right_index=True` also implies the original DataFrame `data` was not re-indexed at any point, which might be a fragile assumption.

4.  **NaN Handling (Step 5):**
    *   `X = X.dropna()` and `y = y.dropna()` independently is problematic. `dropna()` operates row-wise. If a row in `X` becomes NaN, its corresponding `y` value should also be dropped. The generated code drops rows from `X` and `y` *separately*. This means that after `X.dropna()`, `X` will have fewer rows than `y`, and the index alignment will be broken (unless all NaNs in `X` happen to be in the same rows as NaNs in `y`, and vice-versa). The ground truth correctly concatenates `X` and `y`, drops `NaN`s, and then re-splits them to ensure correct index alignment between features and target.
    *   The `Launched Year` column is not explicitly cleaned, but the task implies all numerical features listed in a group should be cleaned. The task states 'Launched Year' would be a cleaned numerical feature but doesn't specify how. Assuming it's already numeric is fine, but it should be part of the `all_features` list.

5.  **Dropped Columns (Step 5):**
    *   `data = data.drop(columns=['Company Name', 'Processor', 'Launched Price (USA)'])` and then later defining `X` by dropping `is_expensive_usa` means `X` will only contain the *original* numerical features and the *incorrectly* one-hot encoded features. It doesn't create new columns for the cleaned numerical features (e.g., `Mobile Weight_cleaned`) as standard practice. The ground truth renames these cleaned columns to avoid confusion and clearly define what goes into `X`. The generated code overwrites the original columns, which is less ideal for traceability.

In summary, the code has structural issues with data cleaning robustness, a critical error in one-hot encoding, and a severe bug in NaN handling that would likely lead to mismatched `X` and `y` sizes or incorrect row alignments, making the training and evaluation unreliable.",command-r7b-12-2024,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# 1) Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# 2) Define a function to clean price columns
def clean_price(price):
    try:
        # Remove currency symbols and commas, then convert to numeric
        cleaned_price = float(price.replace(',', '').replace('PKR', '').replace('INR', '').replace('CNY', '').replace('USD', '').replace('AED', ''))
        return cleaned_price
    except ValueError:
        # Handle errors by returning NaN...",4,"The generated code has several significant issues that prevent it from correctly solving the task.

1.  **Price Cleaning (Incorrect & Inefficient):**
    *   The `clean_price` function attempts to clean by replacing all symbols directly. However, when applied via `applymap(clean_price)`, the `replace` methods need `str` accessors (e.g., `x.str.replace`).
    *   More critically, the line `df[columns_to_clean] = df[columns_to_clean].apply(lambda x: x.str.replace(',', '').str.replace(['PKR', 'INR', 'CNY', 'USD', 'AED'], ''), axis=0).applymap(clean_price)` is incorrect and overly complex.
        *   `str.replace` with a list of patterns is not standard for a direct replace; it usually expects a string or regex.
        *   The use of `applymap(clean_price)` *after* attempting a vectorized string replace means the initial string replacement logic is flawed. A `clean_price` function should handle the entire parsing for each cell, as in the ground truth.
        *   The `clean_price` function itself doesn't handle `NaN` inputs explicitly, which can lead to errors if `NaN`s are present before string operations.

2.  **Average Global Price Calculation (Critically Flawed):**
    *   `df.groupby('Model Name')[columns_to_clean].transform(lambda x: (x != np.nan).mean(axis=1))` is fundamentally incorrect for calculating the average price.
        *   `(x != np.nan)` always evaluates to `True` for non-NaN values and `False` for `np.nan` (due to `np.nan != np.nan` being `True`). This expression is evaluating *if a value is not NaN*, not the value itself.
        *   `.mean(axis=1)` on a boolean Series/DataFrame will give the proportion of non-NaN values, not the average of the prices.
        *   Multiplying by `(columns_to_clean.count() == 5)` is simply multiplying by `True` (which is 1) as `columns_to_clean` is always 5. This has no meaningful impact.
    *   The task requires calculating the mean of *available numeric prices* for each model, ignoring NaNs. The ground truth uses `mean(axis=1, skipna=True)` directly on the relevant columns, which is the correct approach.

3.  **Displaying Top Models (Incorrect Indexing and Join):**
    *   The line `print(top_models.join(df.loc[top_models.index, ['Average Global Price']]))` is problematic.
        *   `top_models` is a Series of 'Model Name' values. Joining a Series directly with a DataFrame slice using `top_models.index` on `df` might not align correctly, especially if the original `df` index was not preserved or if there are duplicates of `Model Name` (though the problem implies unique model names for price aggregation).
        *   Since the `Average Global Price` calculation was wrong, the displayed values will also be incorrect.
    *   A simpler and more robust way is to select the top N rows using `head(5)` after sorting, and then select the desired columns (`['Model Name', 'Average Global Price']`).

4.  **Completeness:**
    *   The `clean_price` function in the ground truth handles edge cases like `pd.isna` directly. The generated code's `clean_price` function does not, potentially failing on `NaN` inputs before the `try-except` block catches `ValueError`.

In summary, the core logic for price cleaning and, more critically, for calculating the `Average Global Price` is incorrect, leading to a ""solution"" that does not perform the required task.",command-r7b-12-2024,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
data = pd.read_csv('mobiles_2025.csv')

# 2) Clean 'RAM', 'Battery Capacity', and 'Launched Price (USA)'
data['RAM'] = pd.to.numeric(data['RAM'], errors='coerce')
data['Battery Capacity'] = pd.to.numeric(data['Battery Capacity'], errors='coerce')
data['Launched Price (USA)'] = pd.to.numeric(data['Launched Price (USA)'].str.replace(' USD', '').str.replace(',', ''), errors=...",3,"The generated code fails significantly in meeting several key requirements of the task.

**Correctness:**
- **Cleaning:** The cleaning of `RAM` and `Battery Capacity` is incorrect. It attempts to convert them directly to numeric *without removing 'GB' or 'mAh'*, leading to `NaN` values for most entries. The `Launched Price (USA)` cleaning is also incomplete; `str.replace(' USD', '')` expects ' USD' at the end, but the ground truth shows 'USD ' at the beginning. It also misses stripping leading/trailing whitespace.
- **Plotting strategy:** The task explicitly asks for box plots *per year* comparing the three metrics using subplots or separate figures. The generated code attempts to create a separate figure *for each year and each metric*, resulting in many individual plots that don't allow for direct comparison by year as requested. Furthermore, within the loop for a specific metric (e.g., RAM), it calls `sns.boxplot(x=year, y=year_data[metric])` which would *overlay* different box plots for different years on the *same axes*, and then immediately call `plt.title()` and `plt.savefig()` for that single year, meaning only the *last* year's plot (for a given metric) would be saved/shown, and only if `plt.show()` was called. The current setup will overwrite previous plots.
- **Plot labels:** The y-axis label for the generated plots is simply the metric name, without the units as specified (e.g., 'RAM (GB)', 'Battery Capacity (mAh)', 'Launched Price (USD)'). The plot title also includes `(Year: {year})` which is redundant given the file name and the general plotting strategy.

**Code Quality:**
- **Imports:** Necessary imports are present.
- **Variable names:** Generally clear.
- **Error Handling:** Lacks the `try-except` block for file loading.
- **Efficiency:** The plotting loops and `savefig`/`close` calls are inefficient and do not achieve the desired comparative visualization.

**Completeness:**
- **Cleaning `RAM` and `Battery Capacity`:** Fails to remove 'GB' and 'mAh' before conversion.
- **Cleaning `Launched Price (USA)`:** Fails to remove 'USD ' (space after USD) and strip leading/trailing whitespace.
- **Handling conversion errors for cleaning:** While `errors='coerce'` is used, the lack of proper string cleaning before conversion means many values will become NaN unnecessarily.
- **Dropping NaNs after cleaning:** This part is present, but due to incorrect cleaning, it will drop many more rows than intended.
- **Plotting for comparative analysis:** The core requirement of comparing distributions of RAM, Battery Capacity, and Price *for each year* is not met. Instead, it creates many individual plots that show one metric for one year. The expected output is three boxplots (RAM, Battery, Price) side-by-side or in one figure *for a single year*.
- **Plot titles and labels:** Misses specifics described above (units, clarity of titles).
- **Subplots/Separate Figures:** The implementation does not use subplots effectively nor does it create a separate figure comparing metrics for a given year.

In summary, the generated code demonstrates a clear misunderstanding of how to correctly clean the specified columns before conversion and fundamentally misses the hierarchical plotting requirement (group by year, then plot three metrics side-by-side for *that year*).",command-r7b-12-2024,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import imblearn.over_sampling as over_sampling
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, leave_one_out, cross_val_score, train_test_split, ValidationCurveDisplay
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from sklearn.svm import SVC
from sklearn.neural_networ...",3,"The generated code attempts to address many aspects of the prompt but fails significantly in execution and core functionality.

**Primary Reasons for Failure & Critical Missing/Incorrect Aspects:**

*   **Fundamentally Incorrect Preprocessing:**
    *   **Imputation Strategy:** The ""multiple imputation strategies"" section is completely broken. It tries to subtract the mean from numerical and categorical dataframes (`data_numerical_imputed = data_numerical.subtract(data_numerical.mean(), axis=0)`) and then add it back, which makes no sense for imputation. It also improperly handles categorical data. The ground truth uses `IterativeImputer` for numerical and `SimpleImputer(strategy='most_frequent')` for categorical, which are correct multiple imputation/handling of missing values. The generated code's approach is meaningless and would likely lead to errors or nonsensical data.
    *   **Feature Engineering:**
        *   `data['fare_per_pax'] = data['fare'] / data['sibsp'] + data['fare'] / data['parch']` is problematic. `sibsp` or `parch` can be zero, leading to division by zero or infinity. The ground truth correctly uses `FamilySize` (SibSp + Parch + 1) to avoid this.
        *   **Polynomial Features:** `poly_features = data_poly.hexbin_id(data_poly, method='poly', degree=2, interaction_only=True)` is an entirely made-up function (`hexbin_id` is not a pandas method for polynomial features). The ground truth uses `sklearn.preprocessing.PolynomialFeatures`.
        *   **Interaction Terms:** The `interaction_features` generation loop is overly complex, likely incorrect, and does not correctly generate interaction terms as `PolynomialFeatures` does.
        *   **Text Mining:** ""Implement text mining techniques here"" is a placeholder, not an implementation, failing the requirement. The ground truth correctly extracts 'Title' and 'Name_Length'.
    *   **Column Handling:** The preprocessor and feature engineering steps are not integrated into a `Pipeline` or `ColumnTransformer` correctly, making it impossible to apply consistently to train/test data and to new data. The ground truth correctly defines separate transformers and integrates them into a `ColumnTransformer` and then a `Pipeline`.
    *   **Training Data:** The `GridSearchCV` call uses `data_poly.drop(columns=['survived'])` and `data['survived']` directly without splitting into train/test sets, which means the model is trained and evaluated on the same data, leading to an overoptimistic assessment of performance (data leakage). The prompt explicitly asks for `train_test_split`.

*   **Missing Core Components:**
    *   **Neural Network Implementation:** The prompt asks for a neural network, but the `MLPClassifier` is defined as a base model within sklearn, which is fine, but the overall structure of the solution doesn't facilitate the full capabilities or common practices for NNs (e.g., specific data loader, or advanced PyTorch/TensorFlow features if that was the intent given the imports). The imports `torch`, `DataLoader`, `TensorDataset`, `nn`, `BertTokenizer`, `BertForSequenceClassification` suggest an ambitious but *unimplemented* PyTorch/BERT NN, which further highlights incompleteness.
    *   **Comprehensive Evaluation (Learning/Validation Curves):** These are mentioned as ""(etc.)"" and not implemented. The `ValidationCurveDisplay` import is present, but it's not used.
    *   **Feature Importance with SHAP/LIME:** While SHAP is imported, it's explicitly written as ""Use SHAP or LIME for feature importance analysis"" rather than an actual implementation. The ground truth provides concrete examples.
    *   **Statistical Significance Tests:** Again, mentioned conceptually (""Compare all models and ensembles"") but not implemented. The `scipy.stats` import is present but unused for this specific purpose.
    *   **`soundness_test` import:** `sklearn.metrics.soundness_test` does not exist. This indicates a lack of understanding of available libraries.
    *   **Outdated/Unused Imports:** Imports like `imblearn.over_sampling`, `PCA`, `SelectFprUnbalanced`, `anndata` are present but unused or not relevant to the solution presented, demonstrating a lack of precision.

*   **Code Quality:**
    *   The code is very procedural and lacks modularity compared to the ground truth's use of custom transformers and well-defined pipelines.
    *   Many variables are defined (`data_numerical`, `data_categorical`, `data_text`) but then not properly integrated or used in a coherent preprocessing flow.
    *   The `ensemble` creation has `results['rf'].best_model` which is syntactically incorrect and `results['rf']` holds the best score, not the model.

In summary, while the generated code lists many required components and attempts to include them, the actual implementation of these components is largely incorrect, incomplete, or flawed, rendering the solution non-functional and failing to meet the prompt's requirements for advanced data science practices.",command-r7b-12-2024,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# 1) Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 2) Define features X using the 'Year' and 'Runtime' columns
X = df[['Year', 'Runtime']]

# 3) Define the target y using the 'Rating' column
y = df['Rating']

# 4) Split the data into training and testing sets
X_train, X_test, y_train, y_test = tr...",6,"The generated code correctly addresses the core requirements of loading data, defining features and target, splitting data, training a Linear Regression model, and evaluating it using R-squared and MAE. It includes all necessary imports and follows a logical flow.

However, a significant omission dramatically reduces its robustness and applicability in a real-world scenario:

1.  **Lack of Data Preprocessing for 'Runtime' and potential NaNs**: The `Runtime` column in the `imdb_top_250.csv` dataset is typically in a string format (e.g., ""120 min""). The ground truth includes crucial preprocessing to convert this to a numeric type by removing "" min"" and handling potential errors. The generated code directly tries to use `df[['Year', 'Runtime']]` without this step, which will likely raise an error when the Linear Regression model tries to fit non-numeric data, or produce incorrect results if pandas manages to coerce it in an unexpected way without explicit handling. Similarly, it doesn't handle potential NaN values that might arise from malformed data or previous operations for `Year` or `Rating`. The ground truth explicitly handles this, which is essential for stable model training.

While the fundamental logic is there, the absence of robust data cleaning for a common dataset format makes the code fragile and potentially non-functional without manual intervention. For a ""data science benchmark,"" this preprocessing step is critical, as data is rarely perfectly clean.",command-r7b-12-2024,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Step 1: Load the dataset
data = pd.read_csv('imdb_top_250.csv')

# Step 2: Select the features for clustering
features = data[['Rating', 'Runtime']]

# Step 3: Standardize the selected features
scaler = StandardScaler()
standardized_features = scaler.fit_transform(features)

# Step 4: Apply K-Means clustering
kmeans = KMeans(n_clu...",6,"The generated code correctly performs most of the required steps, including loading the data, selecting features, standardizing, applying K-Means, and visualizing. However, it completely misses a critical data cleaning step for the 'Runtime' column, which is often imported as a string (e.g., '142 min') and needs to be converted to a numeric type. This will lead to an error or incorrect results. The printing of cluster means is also less robust than the ground truth's `groupby()` approach, and it doesn't handle potential `FileNoFoundError` or `Rating`/`Runtime` column not found scenarios.

Areas for improvement:
1.  **Robust Data Cleaning for 'Runtime'**: The most significant missing piece is the conversion of the 'Runtime' column from a string (e.g., '142 min') to a numeric type. Without this, `pd.to_numeric` may fail or `StandardScaler` will error out if the column is left as non-numeric, or it will produce incorrect results if 'min' is just silently dropped by some implicit conversion. The ground truth explicitly handles this.
2.  **Error Handling**: Adding `try-except` for file loading and checks for column existence (`df.columns`) would make the code more robust.
3.  **Missing Values Handling**: The ground truth explicitly drops rows with `NaN` in 'Rating' or 'Runtime' after conversions. The generated code does not address this, which could lead to issues with `StandardScaler` or `KMeans` if `NaN`s are present.
4.  **Clarity in Mean Calculation**: While the loop works, using `df.groupby('cluster_label')[['Rating', 'Runtime']].mean()` as in the ground truth is more idiomatic pandas and often cleaner for this task.
5.  **Plot Customization**: While a scatter plot is created, adding more details like a colorbar and explicitly setting `figsize` for better visualization, as seen in the ground truth, would improve presentation.",command-r7b-12-2024,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create a new feature: title_length
df['title_length'] = df['Title'].apply(len)

# Calculate and print Pearson correlation coefficient
print(f""Pearson correlation coefficient between title_length and Rating: {df['title_length'].corr(df['Rating'])}"")

# Preprocess titles
def prep...",6,"The generated code addresses most of the tasks but has several issues that reduce its correctness and completeness, particularly in the text preprocessing steps and correlation calculation.

**Issues:**

1.  **Missing NLTK Downloads:** The code does not include `try-except` blocks or explicit `nltk.download()` calls for `stopwords` and `punkt`, which are essential for `stopwords.words()` and `word_tokenize()`. This will cause `LookupError` for users who haven't downloaded these resources.
2.  **Correlation Calculation:**
    *   It directly calculates `df['title_length'].corr(df['Rating'])` without ensuring the 'Rating' column is numeric first (e.g., `pd.to_numeric`). If 'Rating' contains non-numeric values, this could raise an error or produce incorrect results.
    *   It doesn't explicitly handle `NaN` values before correlation, which can quietly lead to an inaccurate correlation if `NaN`s are present in either column. The ground truth explicitly cleans these.
3.  **Preprocessing Logic:**
    *   **Punctuation Removal & Tokenization Order:** The `preprocess_title` function first removes punctuation using `str.maketrans('', '', string.punctuation)` and then `word_tokenize`s. This is generally acceptable. However, the subsequent filtering `word_tokenize(title) if word.isalpha()` might be too aggressive, potentially removing valid words that contain numbers (e.g., ""Star Wars: Episode 1""). The prompt specifically asks to remove ""characters that are not alphanumeric or whitespace,"" implying numbers should be kept, which `string.punctuation` removal (`str.maketrans`) does, but `word.isalpha()` in the subsequent step undoes.
    *   **Stopword Removal within `preprocess_title`:** While putting stopword removal inside the main preprocessing function can work, it makes the steps less modular and harder to inspect independently, which contradicts the prompt's request for clear, separate steps. The prompt broke down preprocessing into: lowercase, remove punctuation, tokenize, then remove stopwords. The generated code combines most of these into one function definition, and then combines tokenization, punctuation removal, and stopword removal potentially in an awkward order.
    *   **Double Tokenization/Splitting:** The `preprocess_title` function returns a space-separated string. Then, the line `processed_titles = df['processed_title'].str.split().explode().dropna()` splits it again. This is redundant and less efficient than directly working with lists of tokens. The prompt asked for ""tokenized into words"" in step 4 and then ""remove common English stop words from the tokenized titles"" in step 5, implying a list of tokens should be maintained, not re-joined and re-split.
    *   **Handling Non-String Titles during Preprocessing:** The generated code doesn't explicitly handle potential `NaN` values in the 'Title' column before applying `preprocess_title`. While `apply(len)` might sometimes handle `NaN`s, `title.lower()` or `word_tokenize(title)` would fail if a `NaN` was passed.
    *   **No Punctuation Removal for Non-Alphanumeric and Non-Whitespace:** The prompt asks to remove ""characters that are not alphanumeric or whitespace"". The generated code uses `string.punctuation`, which is fine, but doesn't necessarily cover all non-alphanumeric and non-whitespace characters that might exist (e.g., some special symbols not in `string.punctuation`). The ground truth's `re.sub(r'[^a-z0-9\s]', '', text)` is more robust for this specific requirement.
4.  **Incomplete Stopword Removal:** The generated code initializes `stopwords.words('english')` inside the loop (or inside the `preprocess_title` function which is applied row-wise), which is inefficient. It should be initialized once outside for better performance.
5.  **Output Format for Most Common Words:** The prompt asks for ""along with their frequencies."" The generated code outputs `word: freq`, which is acceptable, but the ground truth `f""'{word}': {count}""` (with quotes) provides slightly clearer separation.

**Strengths:**

*   Correct imports are present.
*   Data loading is correct.
*   `title_length` feature creation is correct.
*   The use of `collections.Counter` for word frequencies is correct.
*   The general approach to text analysis is followed.",command-r7b-12-2024,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier

# Load the dataset
data = pd.read_csv(""penguins.csv"")

# Define features and target
X = data[[""culmen_length_mm"", ""culmen_de...",5,"The generated code attempts to address all requirements but has significant issues that lead to incorrect or incomplete execution of the task as specified.

**Primary Issues:**

1.  **Chained Imputation and Preprocessing:** The code performs imputation and one-hot encoding *outside* the `ColumnTransformer` and then attempts to redefine these steps within the `ColumnTransformer`. This is a fundamental misunderstanding of how `ColumnTransformer` (and pipelines) should be used. The `ColumnTransformer` is designed to apply all preprocessing steps *together* and *consistently* across all data splits (e.g., in cross-validation). The manual `imputer_numeric.fit_transform` and `imputer_categorical.fit_transform` steps modify `X` *before* `ColumnTransformer` even processes it, leading to redundancy and potential errors.
    *   Specifically, `X[numeric_features] = imputer_numeric.fit_transform(X[numeric_features])` and `X[categorical_features] = imputer_categorical.fit_transform(X[categorical_features])` happen first.
    *   Then, `X_encoded = encoder.fit_transform(X[categorical_features])` and `X_scaled = scaler.fit_transform(X.drop(categorical_features, axis=1))` happen again, modifying `X` or parts of it.
    *   Finally, `X_preprocessed = preprocessor.fit_transform(X)` re-applies imputation (with a new `SimpleImputer` instance) and encoding. This is incorrect. The `ColumnTransformer` should encapsulate *all* initial preprocessing steps that are then applied to the raw `X` (or `X` after any necessary row drops).

2.  **Handling of `y` for Dropped Rows:** When `X.dropna(subset=[""island""])` is called, the corresponding rows in `y` are *not* dropped. This creates a misalignment between `X` and `y`, which will cause errors when `model.fit` or `cross_val_score` is called, as `X_preprocessed` will have a different number of samples than `y_encoded`.

3.  **Cross-Validation and Training:** The model is trained *before* cross-validation (`model.fit(X_preprocessed, y_encoded)`). While not necessarily wrong for simply getting a score, `cross_val_score` naturally handles training (fitting) a fresh model instance for each fold. The `model.fit` line is redundant and potentially misleading, as the reported score is from the *cross_val_score* calls, not from that initial `model.fit`. More critically, if `X_preprocessed` were to be derived from different steps than the `ColumnTransformer` would produce in a `Pipeline`, this could lead to inconsistencies.

4.  **Incomplete `ColumnTransformer` for Scaling:** The `ColumnTransformer` defined as `preprocessor` only includes `SimpleImputer` for numerical features and `OneHotEncoder` for categorical (which is good), but it *lacks `StandardScaler`*. This means the numerical features processed by the `ColumnTransformer` will be imputed but *not scaled*. This is a significant omission from the task requirements.

5.  **`handle_unknown=""ignore""` for OneHotEncoder:** While `handle_unknown=""ignore""` is a reasonable default for `OneHotEncoder`, the *ground truth* or expected behavior in this context for `penguins.csv` with the specified features usually doesn't encounter unknown categories. It's not strictly incorrect but adds a hint that the task might be interpreted differently.

6.  **`species` NA handling:** The ground truth code explicitly handles missing `species` values by dropping corresponding rows, which is good practice. The generated code does not explicitly handle this, though `cross_val_score` might implicitly fail if `y` has NAs where `X` does not.

To improve, the code needs to:
*   Build a single `ColumnTransformer` (or `Pipeline` with `ColumnTransformer`) that handles *all* preprocessing steps for `X` (imputation, encoding, scaling) in one go.
*   Ensure that any row drops (like `data.dropna(subset=[""island""])` after initial imputation for `island` NAs, or for `species` NAs if they occurred) are applied consistently to *both* `X` and `y` to maintain alignment.
*   Remove the redundant and confusing manual preprocessing steps (`imputer_numeric.fit_transform`, etc.) that are outside the `ColumnTransformer`.
*   The `ColumnTransformer` must include `StandardScaler` for numerical features.
*   The `model.fit()` line before `cross_val_score` should be removed as it's redundant.",command-r7b-12-2024,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# 1) Load the dataset
penguins = pd.read_csv('penguins.csv')

# 2) Select the features
selected_features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']

# 3) Drop any rows where any of these four selected features have missing values (NA)
penguins_no_na = penguins....",6,"The generated code correctly performs most of the required steps: loading the dataset, selecting features, dropping NAs, standardizing, applying K-Means, and applying PCA. It also attempts to generate both plots.

However, there are several key issues that prevent a higher score:
1.  **Data handling after standardization (Puntative SettingwithCopyWarning):** The line `penguins_no_na[selected_features] = scaler.fit_transform(penguins_no_na[selected_features])` directly modifies a slice of a DataFrame (`penguins_no_na`). This often leads to a `SettingWithCopyWarning` and can sometimes result in unexpected behavior where the changes are not actually reflected in the original `penguins_no_na` DataFrame unless `.copy()` was used earlier, or in this case, it might simply overwrite the data *in place* but it's generally discouraged. It's better practice to create a new DataFrame for the scaled data or assign the scaled data to a new column. Similarly for PCA and KMeans labels.
2.  **Incorrect `plt.scatter` call in the first plot:** `plt.scatter(penguins_no_na['PC1'], penguins_no_ain['PC2'], ...)` contains a typo `penguins_no_ain`. This will cause a `KeyError` or `NameError` and prevent the code from running successfully.
3.  **Plotting `species`:** When plotting by `species`, `plt.scatter` with `c=penguins_no_na['species']` will attempt to use string labels as numerical values for coloring, which typically doesn't work directly or produces undesirable results. Matplotlib's `scatter` expects numerical values for `c` when using a colormap, or distinct numerical categories. The `Original_Species` column in the ground truth was properly linked to the numerical `pca_df` for seaborn to handle categorical hues. Pandas `factorize` or proper handling of categorical plotting with `hue` (like seaborn) is needed.
4.  **Lack of `species` handling for NA removal:** The assignment of `species` to PCA dataframe for plotting should ensure that the `species` labels *exactly* correspond to the rows that remain after NA removal. The ground truth explicitly creates `df_cleaned` which holds both the features and the corresponding `species` to ensure this alignment. The generated code implicitly assumes the `species` column in `penguins_no_na` is still aligned, which it is, but the subsequent treatment of this column for plotting could fail directly if `plt.scatter` cannot interpret the string values as colors directly.
5.  **Plotting `species` with `cmap='viridis'`:** While `viridis` is a colormap and good for numerical data, it's not ideal for categorical `species` labels. A categorical palette is usually preferred for distinct categories.
6.  **Readability/Best Practices:** While functional, the direct in-place assignments for scaled data, cluster labels, and PCA components to the `penguins_no_na` DataFrame can make the data flow less clear than creating separate arrays or DataFrames for each transformation result, as done in the ground truth for `X_scaled`, `kmeans_labels`, `principal_components`, and `pca_df`.

To improve, the code should:
*   Fix the typo `penguins_no_ain`.
*   Handle the `species` column for plotting correctly (e.g., using `sns.scatterplot` which handles categorical `hue` values directly, or mapping species to numerical IDs for `plt.scatter`).
*   Consider creating a separate DataFrame for the scaled and transformed data as it passes through PCA and KMeans, making the data pipeline clearer and avoiding potential SettingWithCopyWarning issues, even if it happens to work for this specific environment/case.",command-r7b-12-2024,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.ensemble import GradientBoostingClassifier

# 1. Load the dataset
data = pd.read_csv('penguins.csv')

# 2. Impute missing values
imputed_data = data.copy()

# Impute numerical features with their respective means
imputed_data['culmen_length_mm'].fillna(imputed_data['culmen_length_mm'].mea...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being a strong solution.

**Correctness:**
- **Data Loading and Imputation:** Correctly loads data and imputes missing numerical values with the mean and 'sex' with the mode. It also correctly drops rows where 'island' is NA.
- **Feature/Target Definition:** Correctly identifies numerical features and one-hot encodes 'sex' and 'island'. However, it directly uses `y = imputed_data['species']` without LabelEncoding `species`, which is required for `GradientBoostingClassifier` to work with string labels.
- **Model Training:** It attempts to train a `GradientBoostingClassifier`. However, it introduces `train_test_split` which was not requested by the prompt and is not necessary for the specified tasks of feature importance and visualization. More critically, the `y` variable is not label-encoded, which will cause an error when `model.fit(X_train, y_train)` is called, as GBC expects numerical target values.
- **Feature Importances:** It correctly extracts feature importances and prints them.
- **Visualization:** This is the weakest point. The visualization logic is significantly flawed.
    - It iterates `for species in y.unique()`. The `y` here is the *original* 'species' column, not the label-encoded one, which is fine, but the subsequent plotting logic is incorrect.
    - `sex_groups = species_subset.groupby('sex')['flipper_length_mm']` is good, but `for sex, group in sex_groups: plt.boxplot(group, labels=sex)` will generate multiple separate plots (one for each sex within each species), each with a single box plot. The prompt asked for ""box plots to visualize the distribution of `flipper_length_mm` (using imputed data) grouped by the imputed `sex` column *for each species*"". This implies a multi-panel plot or a single plot with clear grouping, ideally using `seaborn.boxplot` for better aesthetics and easier grouping. The current implementation produces a series of individual, poorly labeled `matplotlib` box plots. It also clears the plot and `plt.show()` in the inner loop, creating many small windows instead of a single, well-structured plot (or a set of well-structured plots).

**Code Quality:**
- **Imports:** Imports are generally correct, though `StandardScaler` and `train_test_split` are imported but not strictly needed for the task as specified.
- **Structure:** The code follows the requested steps.
- **Readability:** Generally readable.

**Completeness:**
- **Missing Label Encoding:** Fails to label encode the `species` target, which is crucial for `GradientBoostingClassifier`.
- **Unrequested `train_test_split`:** Introduces an unnecessary step.
- **Suboptimal Visualization:** The visualization part is poorly implemented and does not match the usual standard for grouped box plots or the visual intent of the prompt. It does not use `seaborn` for this, which simplifies such visualizations considerably. The title of the plot is duplicated and misleadingly suggests ""by Sex"" when each plot actually only shows one sex. The `labels` argument on `plt.boxplot` is also incorrectly used; it expects a list of labels for each box.

**Key areas for improvement:**
1.  **Label Encode `species`:** Implement `LabelEncoder` for `y` before training the `GradientBoostingClassifier`.
2.  **Correct Visualization:** Use `seaborn.boxplot` or a more sophisticated `matplotlib` approach to create a single plot (or a few plots) showing `flipper_length_mm` distribution grouped by `sex` for each `species`, as typically done for such comparisons. This is a significant deviation from the prompt's implied request.
3.  **Remove `train_test_split`:** This was not asked for and adds unnecessary complexity for the task.",command-r7b-12-2024,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer

# Step 1: Load the dataset
data = pd.read_csv('penguins.csv')

# Step 2: Numerical columns for KNN imputation
numerical_columns = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']

# Initialize KNNImputer with n_neighbors=3
imputer = KNNImputer(n_neighbors=3)

# Impute missing values
for col in numerical_columns:
    data[col] = imputer.fit_transform(data[[col]])

# Step 3: Categorical column for mode ...",6,"The generated code correctly loads the dataset, identifies the specified numerical and categorical columns, and attempts the correct imputation strategies.

However, there is a critical flaw in the KNN imputation step:
`for col in numerical_columns: data[col] = imputer.fit_transform(data[[col]])`
This loop applies `KNNImputer` to each numerical column *individually*. KNN imputation is designed to use *multiple* features to estimate missing values in one feature. By applying it to a single column at a time, it essentially degenerates into a different, less sophisticated imputation method (e.g., mean imputation if there's only one feature and no `weights` parameter explicitly set to `uniform`, or it might just fill with 0s if all neighbors are NaN or similar weird behavior for a single feature input). The correct way to use `KNNImputer` for multiple columns is to fit it on all relevant numerical columns simultaneously, as shown in the ground truth: `df[numerical_cols_for_knn] = knn_imputer.fit_transform(df[numerical_cols_for_knn])`. This allows the imputer to learn the relationship between features to better estimate missing values.

Other points:
- The `try-except FileNotFoundError` block for loading the CSV is a good practice missing in the generated code.
- Printing initial NA values and specific verifications at the end (beyond just `isna().sum()`) would improve clarity and completeness.
- The `if data[categorical_column].dtype == 'object':` check is redundant since `sex` is expected to be an object type, but harmless.
- The `iloc[0]` for mode is correct, although `[0]` works too.",command-r7b-12-2024,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# 1) Load the dataset using pandas
penguins_df = pd.read_csv(""penguins.csv"")

# 2) Filter the dataset to include only 'Adelie' and 'Gentoo' species
filtered_df = penguins_df[(penguins_df['species'] == 'Adelie') | (penguins_df['species'] == 'Gentoo')]

# 3) Handle NA values in the `culmen_length_mm` column for these two species
culmen_length = filtered_df[['species', 'culmen_length_mm']].dropna()

# 4) Perform an independent two-sam...",9,"The generated code correctly implements all the requirements of the task.

**Strong points:**
- **Correctness:** The code correctly loads the data, filters the species, handles NAs for the relevant column and species, performs the t-test, and interprets the results based on the p-value and significance level.
- **Conciseness:** The solution is quite concise while still being readable.
- **Direct addressing of requirements:** Each step outlined in the prompt is directly addressed.

**Areas for minor improvement:**
- **Robustness (Error Handling):** The ground truth includes error handling for `FileNotFoundError` when loading the CSV, which the generated code lacks. While not explicitly asked for, robust code for data loading is generally good practice.
- **Clarity of NA handling:** The line `culmen_length = filtered_df[['species', 'culmen_length_mm']].dropna()` is technically correct, but combining the filtering for columns *then* dropping NAs might be slightly less intuitive than dropping NaNs on the relevant column for the already filtered dataframe. In essence, `filtered_df.dropna(subset=['culmen_length_mm'], inplace=True)` as in the ground truth is more direct for cleaning the existing `filtered_df`. However, the generated code's approach results in the same correct `culmen_length` DataFrame.
- **Readability/Structure (Minor):** Encapsulating the logic within a function (as in the ground truth) would make the code more reusable and testable, though the prompt asked only for ""Python code"" and not necessarily a function. For a benchmark where direct execution is key, the current flat structure is acceptable.
- **Informative Prints (Minor):** The ground truth provides more detailed print statements, including the number of samples, means, and clearer explanations of the conclusion, which can be helpful for debugging and understanding the output. The generated code's prints are sufficient to meet the explicit requirement but less informative.

Overall, it's a very good solution that directly answers the prompt's requirements effectively. The minor points are mostly about best practices and additional clarity rather than critical failures in functionality.",command-r7b-12-2024,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Define features and target
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
target = ...",6,"The generated code addresses most of the requirements but has critical flaws in the preprocessing steps, specifically with how one-hot encoding is applied and integrated with the rest of the dataframe.

**Correctness:**
- **InCorrect One-Hot Encoding Application:** The biggest issue is with the one-hot encoding. When `ohe.fit_transform(df[['island']]).toarray()` is used, it correctly creates the one-hot encoded array for 'island'. However, the subsequent lines `df.drop(['island', 'sex'], axis=1, inplace=True)` and then `df['island'] = island_encoded`, `df['sex'] = sex_encoded` are problematic.
    - `df['island'] = island_encoded` attempts to assign a multi-column NumPy array (`island_encoded`) to a single column in the DataFrame (`df['island']`), which will result in a `ValueError` or unexpected behavior (like assigning only the first column of `island_encoded` if broadcasted, or failing if shapes don't match). When `island_encoded` has multiple columns (e.g., 3 islands), `df['island'] = island_encoded` tries to put a 3-column array into a single pandas Series, which doesn't work as intended.
    - The same applies to `df['sex'] = sex_encoded`.
    - The correct way to integrate one-hot encoded columns is to concatenate them back to the DataFrame using `pd.concat` after dropping the original columns, ensuring proper column names if needed.
- **`sex` Imputation and OHE Issue:** The `sex` column is first imputed using `df['sex'].fillna(df['sex'].mode()[0], inplace=True)`. Then, `sex_encoded = ohe.fit_transform(df[['sex']]).toarray()` is called. If the `ohe` object is re-initialized for `sex` (which happens in the generated code), it's fine. However, the subsequent assignment issue remains.
- **Manual Preprocessing vs. Pipelines:** While the ground truth uses pipelines and column transformers (which is best practice for machine learning production code as it applies transformations consistently to train/test sets), the generated code performs these operations manually on the entire `df` before splitting. This is less robust and can lead to data leakage or inconsistencies if not handled carefully, especially for more complex preprocessing. For this specific task, it doesn't cause leakage *in the split*, but it makes the code less organized and harder to maintain/extend. The task implies applying preprocessing on the training set and then transforming the test set, which is correctly handled by a pipeline approach. Manual, direct manipulation of `df` before the split is less ideal.

**Code Quality:**
- Imports are clean.
- Variable naming is clear.
- The structure is logical, following the requested steps.
- The direct manipulation of the `df` for preprocessing is less maintainable and error-prone than using `ColumnTransformer` and `Pipeline` as in the ground truth.

**Completeness:**
- All steps are attempted: loading, defining features/target, preprocessing (imputation, OHE, label encoding), splitting, training, predicting, and evaluating.
- The specific requirements for imputation (median for numeric, mode for sex) and OHE for categorical features are met.
- Stratification is used in `train_test_split`.
- Accuracy and classification report are printed.

**Areas for Improvement:**
1.  **Correct One-Hot Encoding Integration:** The absolute top priority is to fix how the one-hot encoded features are re-integrated into the DataFrame. Instead of assigning a multi-column array to a single column, use `pd.concat` with `pd.DataFrame(island_encoded, columns=ohe_island.get_feature_names_out(['island']))` (assuming `ohe_island` is specific to island) and `pd.DataFrame(sex_encoded, columns=ohe_sex.get_feature_names_out(['sex']))`.
2.  **Robust Preprocessing:** For production-ready code, it's highly recommended to use `sklearn.compose.ColumnTransformer` and `sklearn.pipeline.Pipeline` to encapsulate preprocessing steps. This ensures that transformations learned from the training data (e.g., medians, modes, categories for OHE) are consistently applied to the test data and new, unseen data, preventing data leakage and making the model more robust. The current direct modification of `df` before splitting is less ideal for this.",command-r7b-12-2024,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd
import numpy as np

# Create the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Calculate Pea...",8,"The generated code correctly defines the DataFrame, calculates the specified Pearson correlation coefficients, and prints them. It also correctly identifies which feature has a stronger absolute correlation with ""PerformanceScore"".

Strong points:
- Correct DataFrame creation.
- Accurate calculation of Pearson correlation coefficients using the `.corr()` method.
- Clear printing of the correlation coefficients.
- Correct logic for determining the stronger absolute correlation.

Areas for improvement:
- The output format for the stronger correlation could be more descriptive. The ground truth provides a more comprehensive statement including the absolute values of the correlations for context, which is beneficial for understanding. For instance, if both correlations are negative, stating ""YearsExperience"" with a stronger negative correlation and showing the absolute values provides clearer insight.
- While the determination of the stronger correlation is correct, it doesn't handle the edge case where both absolute correlations might be exactly equal. The ground truth code includes an `else` block for this scenario, which makes it more robust.",command-r7b-12-2024,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import lifelines
from lifelines import KaplanMeier, CoxPHFitter
from scipy.stats import pearsonr, ttest_ind
from statsmodels.formula.api importglm
from pymc3 import Normal, Beta, stan_normal, sample, Set Jutland as Monte Carlo
import seaborn as sns

# 1) Load and prepare the dataset
titanic_data = pd.read_csv('titanic.csv')

# ... (Data cleaning, preprocessing, feature engineering)

# 2) Kaplan-Meier Survival Analys...",0,"The generated code is incomplete, syntactically incorrect, and fundamentally misunderstands several core requirements of the task.

**Primary Reasons for Failure:**

*   **Grossly Incomplete:** The code provided is merely a skeleton with comments indicating where large sections of the problem should be solved. It does not implement any of the `Data cleaning, preprocessing, feature engineering`, `Feature selection, model fitting, diagnostics`, `Calculate propensity scores, matching, estimate treatment effect`, `Resample data, estimate statistics, construct confidence intervals`, `Define Bayesian model`, or `Assumption checks, cross-validation, sensitivity analysis techniques` as requested. This makes the code entirely non-functional and unusable for the stated task.
*   **Syntactic Errors and Misconceptions:**
    *   `from pymc3 import Normal, Beta, stan_normal, sample, Set Jutland as Monte Carlo`: This line contains multiple syntax errors (`Set Jutland as Monte Carlo` is meaningless PyMC3 code) and implies a lack of understanding of PyMC3 basics.
    *   `KaplanMeier(data= survival_times, event_ind=event_indicators)`: The `lifelines.KaplanMeierFitter` class is incorrectly instantiated as `KaplanMeier`, and its constructor does not accept `data` or `event_ind` as direct arguments in this manner. It's meant to be used with `.fit(durations, event_observed)`.
    *   `glm` import: `from statsmodels.formula.api import glm` is correct, but there's no utilization of `statsmodels` for multivariate analysis or interaction effects as requested.
*   **Failure to Define Key Variables/Concepts:** The code uses `survival_times` and `event_indicators` without defining how these would be derived from the `titanic.csv` dataset, especially when the prompt explicitly states ""treating the voyage as time-to-event data (with survival as event indicator)"". This setup requires a specific interpretation of the Titanic data, which is absent.
*   **Missing Core Functionality:** None of the core requirements (Kaplan-Meier, Cox PH, multivariate analysis, interaction effects, model diagnostics, propensity score matching, bootstrap, Bayesian analysis, validation, sensitivity analysis) are implemented.
*   **Lack of Adherence to Task Specification:** The prompt explicitly asked to ""Provide only the Python code without explanations,"" yet the generated code is filled with comments like `... (Data cleaning, preprocessing, feature engineering)` which serve as placeholders rather than actual implementations.

In essence, the generated code demonstrates a superficial understanding of the requested libraries and concepts but fails entirely to implement the task.",command-r7b-12-2024,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# 1. Create DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
    'Final_Score': [90, 88, 82,...",6,"The generated code correctly creates the DataFrame, calculates and prints the Pearson correlation coefficient, and assigns grades. However, there are significant issues with the scatter plot generation and some minor code quality aspects.

**Strengths:**
*   Correct DataFrame creation.
*   Accurate Pearson correlation calculation and printing.
*   Correctly assigns 'Overall_Grade' using a function and `apply`.
*   Correctly counts and prints grade distribution.

**Areas for Improvement / Issues:**

1.  **Plotting (Major Issue):** The scatter plot generation is incorrect and overly complex.
    *   The line `subject_ = df.groupby('Subject').apply(lambda x: pd.Series({'Attendance_Rate': x['Attendance_Rate'], 'Final_Score': x['Final_Score']}))` is not ideal for preparing data for seaborn-style plotting and might lead to unexpected behavior or errors.
    *   The loop `for subject in subjects: ... sns.scatterplot(...)` is highly inefficient and incorrect when using `hue` with Seaborn. Seaborn's `scatterplot` function is designed to handle `hue` directly from the DataFrame, automatically assigning colors and generating a legend. The current implementation tries to plot each subject individually, which would not combine correctly into a single plot with a unified legend as intended by the prompt. In fact, `subject_data[subject_data['Subject'] == subject]` will always fail because `subject_data` is a Series of Series objects, not a DataFrame with a 'Subject' column when grouped like that. This part of the code would likely raise an error or produce an empty plot.
    *   The line `hue=subject` in `sns.scatterplot` is incorrect; `hue` should be a column name (e.g., `'Subject'`). The `label=subject` is correctly used for `plt.legend()`, but it's redundant and problematic if `hue` is used correctly with Seaborn.
    *   The plot effectively fails to meet the requirement of coloring points based on 'Subject' using the intended `hue` functionality, and instead attempts a manual, broken approach.

2.  **Completeness:**
    *   The prompt asked for the plot to be *displayed*, which implies `plt.show()` is needed, which is present.
    *   The prompt asked for a title `'Final Score vs. Attendance Rate by Subject'`, which is present.
    *   The prompt asked for clear axis labels `'Attendance Rate'` and `'Final Score'`, which are present.
    *   The prompt asked for a legend for the subjects, which is attempted but will not work correctly due to the plotting errors.

3.  **Code Quality:**
    *   Unnecessary variable `subject_` (with Japanese characters).
    *   The approach to grade assignment is correct using `apply` but `np.select` (as used in the ground truth) is generally more performant for multiple conditions on large DataFrames. This is a minor point for this dataset size but good practice.
    *   The print statements for clarity (like ""--- Creating DataFrame ---"") are missing, making the output less readable compared to the ground truth. This is a minor styling point.

The complete failure of the plotting section due to a misunderstanding of how the `hue` parameter works in Seaborn, combined with inefficient and incorrect data manipulation for plotting, significantly impacts the score. Fixing the plotting part is crucial for a higher score.",command-r7b-12-2024,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Creating DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']),
        'Ticker': ['TECHCO'] * 10,
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0],
        'Low': [149.5, 150.5, 1...",7,"The generated code successfully creates the DataFrame, calculates daily returns and moving averages, and generates a plot with the required elements. However, the logic for identifying bullish crossovers is incorrect. The `crossings` variable is calculated incorrectly; it attempts to filter `(df['SMA_3Day'] > df['SMA_5Day'])` using a condition applied to the shifted dataframe, which will result in a boolean Series indexed by `Date` where the values are `True` if `SMA_3Day > SMA_5Day` on the current date, but only for dates where `SMA_5Day_previous < SMA_3Day_previous`. It does not correctly identify the exact point where `SMA_3Day` crosses *above* `SMA_5Day`. This means the output for bullish crossovers is not accurate. The output of the `print(crossings)` will be a boolean series, not the dates themselves, which is not what was requested.

To improve, the code needs to:
1. Implement the correct logic for identifying bullish crossovers:
   - Check if `SMA_3Day` is currently greater than `SMA_5Day`.
   - Check if `SMA_3Day` was less than or equal to `SMA_5Day` on the *previous* day.
   - Combine these two conditions to find the exact crossover date.
2. Print the identified crossover dates in a readable format, as requested (e.g., as actual dates, not a boolean Series).",command-r7b-12-2024,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Create DataFrame with provided data
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001'...",7,"The generated code correctly creates the DataFrame and calculates the average order value and identifies repeat customers. The main issue is with the plotting of the total sales revenue per product.

**Areas for Improvement:**
1.  **Product Revenue Calculation for Plotting:** The bar chart is incorrectly plotting `df['Product']` vs `df['Total_Sales_Revenue']`. The `Total_Sales_Revenue` is calculated for *each row*, not *per product*. To correctly display total revenue *per product*, the code needs to first group the DataFrame by 'Product' and sum the `Total_Sales_Revenue` for each product, similar to how it's done in the ground truth code. This is a fundamental error in the visual representation.
2.  **Plotting Function:** `plt.barh()` expects the `y` values (product names) and `x` values (total revenue) which should be derived from the aggregated product revenue.
3.  **Completeness of Output for Repeat Customers:** While it correctly identifies repeat customers, it only prints their IDs and count. The ground truth provides a clearer, more formatted output.
4.  **Clarity and Readability:** The ground truth code breaks down the steps more clearly with print statements for intermediate results and better comments, making it much easier to follow the logic and verify correctness at each stage.
5.  **Robustness for Average Order Value:** The average order value calculation in the generated code uses `len(df['OrderID'].unique())` which implicitly assumes each row is a unique order. However, the data structure describes `OrderID` as potentially representing multiple items within one order. The ground truth correctly uses `df['OrderID'].nunique()` which is accurate but the generated code's calculation of total sales revenue per product on a row-by-row basis is flawed for what was asked (total sales revenue *for each product* across all orders). The `Total_Sales_Revenue` column created sums revenue for *each line item*, not necessarily each product *type* across all line items. This also impacts the average order value calculation slightly, as `df['Total_Sales_Revenue'].sum()` sums up the revenue of each line item. The task could be interpreted in two ways for `Average Order Value`:
    *   Sum of all `Quantity * Price_USD` divided by unique `OrderID`s. (Current interpretation)
    *   Total revenue *per order* summed up, then divided by unique orders. (More robust, potentially requiring a `groupby('OrderID')` first to sum revenue per order).
    The ground truth's `total_revenue_all_orders = df['Revenue'].sum()` correctly sums all line item revenues. `num_unique_orders = df['OrderID'].nunique()` correctly gets the count of unique orders. So, the calculation itself is correct for the interpreted task. The nuance is more in how `Total_Sales_Revenue` is defined if it was meant to be product-level or line-item level for other steps.",command-r7b-12-2024,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd
import numpy as np

# Create the original DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False],
}
df = pd.DataFrame(data)

# Step 1: Create a new numerical feature ""DeviceType_Numeric""
df[""DeviceTyp...",7,"The generated code correctly performs steps 1, 3, and 4. The initial DataFrame definition is correct, `DeviceType_Numeric` is created as specified, the final feature selection is accurate, and the head of the resulting DataFrame is printed.

However, the implementation of step 2 for `PagesPerMinute` handles division by zero incorrectly and inefficiently.
The ground truth uses vectorized operations (`/`), which are more efficient for pandas DataFrames, and then uses `np.inf`/`np.nan` replacement and `fillna` to robustly handle all edge cases of division by zero (e.g., `x/0` resulting in `inf`, `0/0` resulting in `nan`).

The generated code's use of `df.apply` with a lambda function for `PagesPerMinute` is less efficient than vectorized operations. More critically, the `if not np.isnan(row[""SessionDurationMinutes""]) and not np.isinf(row[""SessionDurationMinutes""])` condition only prevents division if the *denominator itself* is `NaN` or `Inf`. It does **not** prevent or handle cases where the denominator is `0`. If `SessionDurationMinutes` contains a `0`, the division `row[""PagesViewed""] / 0` will still occur, resulting in `inf` (or `nan` for `0/0`), which is not subsequently handled or converted to `0` as required by the prompt. This makes the `PagesPerMinute` calculation incomplete for the stated requirement of handling division by zero.

To improve, the `PagesPerMinute` calculation should adopt a vectorized approach similar to the ground truth, first performing the division and then replacing `inf`, `-inf`, and `nan` with `0`.",command-r7b-12-2024,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

# Create DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"",
                   ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, ...",9,"The generated code correctly defines the DataFrame and accurately performs the outlier detection based on the 1.5 standard deviation rule. It calculates the mean and standard deviation correctly and then filters the DataFrame to identify the outliers. The printing of the identified outliers is also correct, including the required SensorID, Timestamp, and TemperatureCelsius.

The only minor area for improvement, purely for robustness and better practice in real-world data science, would be to convert the 'Timestamp' column to datetime objects using `pd.to_datetime`. While not strictly necessary for this specific problem (as the string comparison for printing works fine), it's good practice for time-series data handling and can prevent issues with sorting, comparisons, or calculations if the timestamps were non-uniform. The ground truth includes this for good measure. However, given the prompt only asked for printing the string as is, it's not a critical omission.",command-r7b-12-2024,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [1...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the EngagementScore accurately, and then efficiently determines the PostType with the highest average EngagementScore for each platform. The use of `groupby().idxmax()` is a very clean and effective way to achieve the desired result. The printing format is also clear and concise. All requirements of the task are met perfectly.",command-r7b-12-2024,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd
from datetime import datetime

# Create the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu...",9,"The generated code correctly performs all the required tasks. It accurately calculates the 'LengthOfStay', groups the data by 'Diagnosis' to find the average length of stay and patient count, prints the summary, and identifies the diagnosis with the longest average stay.

Strengths:
- Correctly defines the DataFrame as specified.
- Accurately calculates 'LengthOfStay' using `dt.days`.
- Uses `groupby()` and `agg()` efficiently to calculate both mean `LengthOfStay` and patient count.
- Correctly identifies the diagnosis with the longest average stay using `idxmax()`.
- The output format for the summary and the final printed line is clear and informative.

Areas for minor improvement (not critical but good practice):
- The `reset_index(name=""Stats"")` on the `agg` call is a bit unusual. While it works, the default behavior of `reset_index()` after `agg` is usually sufficient, or you could explicitly rename the aggregated columns for more clarity, as in the ground truth (`AverageLengthOfStay=('LengthOfStay', 'mean'), PatientCount=('PatientID', 'count')`). The current naming (`mean`, `count`) is still clear, so this is a very minor point.
- The `from datetime import datetime` import is not strictly necessary as `pd.to_datetime` handles the string conversion, and basic date arithmetic is done directly on pandas Timedelta objects. This import is benign but unused.",command-r7b-12-2024,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes""...",10,"The generated code correctly defines the DataFrame, calculates the ""WeightedRating"" precisely as specified, adds it as a new column, and accurately identifies and prints the movie with the highest ""WeightedRating"". The code is concise and directly addresses all parts of the task. The use of `.idxmax()` and `.loc` for finding the movie with the highest weighted rating is efficient and idiomatic pandas. The output format is also clear and useful.",command-r7b-12-2024,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

# Sample data for car sales
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 3...",9,"The generated code correctly defines the DataFrame, calculates the average price and mileage per brand, and prints the resulting grouped DataFrame, fulfilling all task requirements. The code is clean, concise, and uses appropriate pandas functions. It uses the dictionary-based aggregation for `groupby().agg()` which is perfectly valid and common. The main subtle difference from the ground truth is the naming of the aggregated columns. The ground truth explicitly renames them to 'Average_PriceUSD' and 'Average_Mileage' for clarity, whereas the generated code retains the original column names 'PriceUSD' and 'Mileage', which is a minor point but sometimes preferred for output readability. However, this is a minor stylistic difference and does not affect correctness or functionality for this specific task.",command-r7b-12-2024,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 8...",10,"The generated code is correct, complete, and adheres to good code quality practices.

**Strengths:**
- **Correctness:** All three requirements are met accurately.
    - The `TotalStats` column is correctly calculated using `df[[""HP"", ""Attack"", ""Defense"", ""Speed""]].sum(axis=1)`, which is a robust way to sum multiple columns.
    - The Pokemon with the highest `TotalStats` is correctly identified and its name and total stats are printed. `idxmax()` is the most efficient method for this.
    - The average ""Speed"" for each ""Type1"" is correctly calculated using `groupby()` and `mean()`.
- **Completeness:** All parts of the task (DataFrame creation, ""TotalStats"" column, highest ""TotalStats"" Pokemon, average ""Speed"" per ""Type1"") are addressed.
- **Code Quality:**
    - Proper import of `pandas`.
    - Clear variable names (`highest_total_stats_pokemon`, `type1_avg_speed`).
    - Concise and standard pandas operations are used (e.g., `sum(axis=1)`, `idxmax()`, `groupby().mean()`).
    - The output formatting using f-strings is good.

The code is well-structured and directly addresses the problem statement without unnecessary complexity. It's a very good solution.",command-r7b-12-2024,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import KMeans, DBSCAN, GaussianMixture
from sklearn.metrics import silhouette_score, dendrogram, calinski_harabasz_score
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.met...",3,"The generated code attempts to address various parts of the prompt but fails significantly in execution and completeness, rendering it largely non-functional or incorrect for the stated advanced analysis.

**Major Issues and Primary Reasons for Low Score:**

1.  **Fundamental Data Handling Errors:**
    *   `df = df.dropna()`: This is a critical logical error for the Titanic dataset. Dropping all rows with any `NaN` (especially 'Age', 'Cabin', 'Embarked') will remove a substantial portion of the dataset immediately, making subsequent analysis unrepresentative and losing valuable data. The ground truth correctly handles missing values with imputation.
    *   `df.drop([""Name"", ""Survived"", ""Ticket"", "" Farnam""], axis=1)`: `"" Farnam""` is a typo for `Fare`, implying the code might be written without properly inspecting the dataset or using auto-completion. This will lead to an error or incorrect feature selection. `Survived` should ideally be kept if it's used for external validation later, which the prompt specifically asks for.
    *   `df[""SocMarket""] = encoder.fit_transform(df[""SocDist""].fillna(0))`: `SocDist` is not a column in the Titanic dataset, leading to an immediate `KeyError`. This indicates a lack of understanding of the dataset.

2.  **Incorrect/Incomplete Feature Engineering:**
    *   ""Ticket sharing"" and ""family groups"" are vaguely addressed (`family_groups = df.groupby(""Family"")[""Name""].nunique().reset_index(name=""family_size"")`). The `Family` column does not exist in the original dataset; this should be derived from `SibSp` and `Parch` like in the ground truth.
    *   The `socioeconomic` feature is an arbitrary multiplication/addition of existing features (`Pclass * 1000 + Age.astype(int) * 100 + SibSp.astype(int) * 10`). This creates a highly coupled and potentially non-interpretable feature. The ground truth uses more standard interaction terms or derivations.
    *   `CountVectorizer`, `TfidfVectorizer` are imported but **never used** for text-based features, failing to implement ""text-based features from names and tickets"" beyond simple `name_length` and arbitrary `ticket_type`/`ticket_class` derivations. The ground truth properly extracts titles and ticket prefixes.
    *   No handling of `Cabin` information (deck, presence), which is a valuable feature for socioeconomic status, is present.

3.  **Missing/Incorrect Clustering Algorithms and Validation:**
    *   Only K-Means is fully implemented for clustering execution. Hierarchical clustering is only started with `linkage` and `dendrogram`, but the actual clustering (`fcluster`) to get labels is missing. DBSCAN and GMM are imported but not applied at all. This fails requirement #2 ""Apply multiple clustering algorithms"".
    *   For optimal cluster determination, only silhouette score for K-Means is calculated/printed. The ""elbow method"" and ""gap statistic"" are mentioned but explicitly stated as ""can be implemented similarly"" and not provided. Calinski-Harabasz is imported but not used. This fails requirement #3 ""Determine optimal number of clusters using multiple validation metrics"".
    *   There is no implementation of cluster stability analysis or consensus clustering (requirement #6), despite `adjusted_rand_score` and `mutual_info_score` being imported. The code has `average_clusters = []` and `for seed in range(10):` but doesn't actually perform any stability measurement or consolidation.

4.  **Incomplete/Poor Visualization and Analysis:**
    *   Only one PCA plot is provided (`PCA of Clustere` with a typo). t-SNE and UMAP are imported, UMAP is applied (`tsne = umap.UMAP(...)`), but the result (`tsne`) is never plotted or used for visualization of clusters, contradicting the variable name and the purpose of `umap` in this context. This fails requirement #4 ""Dimensionality reduction (PCA, t-SNE, UMAP) for visualization"".
    *   Analysis of cluster characteristics and survival patterns (requirements #5 and #8) is entirely absent, indicated by commented-out sections like `... (evaluation of cluster characteristics, survival patterns)`. This is a critical failure.
    *   Comprehensive visualizations of cluster distributions, survival rates, and feature importance (requirement #7) are not implemented beyond the single PCA plot.

5.  **Code Quality Issues:**
    *   `df.dropna()` at the start is extremely bad practice for this dataset.
    *   ""Farnam"" typo in `df.drop()`.
    *   Commented-out and incomplete sections (`... (rest of the clustering algorithms)`, `... (further consensus clustering steps)` etc.) indicate a lack of completeness.
    *   `from minisom import MiniSom` is imported but not used, nor is Self-Organizing Maps (SOMs) part of the requirement.
    *   `from sklearn.metrics import inertia` is an incorrect import (inertia is an attribute of KMeans, not a standalone metric from `sklearn.metrics`).
    *   Hardcoded `n_clusters=5` for K-Means, then later `n_clusters=3` for stability analysis without clear justification or linking to optimal k finding.

In summary, the generated code contains numerous critical errors in data handling, feature engineering, and algorithm application, leading to a largely non-functional and incomplete solution that does not meet the specified advanced requirements.",command-r7b-12-2024,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dyst...",7,"The generated code correctly defines the DataFrame and calculates the `count` of books and `mean` of `Price` for each author. However, the output for `author_stats` is not formatted as required by the prompt (""Author, Count of Books, and Average Price"") and instead prints the raw DataFrame.

More importantly, the calculation for the author with the most books in stock is incorrect and overly complex.

`max_stock_author = df.loc[df.groupby(""Author"")[""Stock""].transform(""sum"").idxmax()]`

1. `df.groupby(""Author"")[""Stock""].transform(""sum"")`: This correctly calculates the sum of stock for each author and broadcasts it back to the original DataFrame's index.
2. `.idxmax()`: This takes the `idxmax()` of the *transformed series*, which will return an *index label from the original DataFrame*, not the author name directly related to the maximum sum of stock. This means `max_stock_author` will be a single row of the original DataFrame corresponding to the *first occurrence* of an author whose total stock (when broadcasted) is maximal, not necessarily the author's overall name.

The output will be, for example: `Author with Most Books in Stock: George Orwell, Stock: 25`. While the stock itself is correct, the author identification method is flawed because `max_stock_author['Author']` will pick the 'Author' from that single DataFrame row, not the author with the overall max stock.

The ground truth creates `total_stock_per_author` which is a Series with authors as index and total stock as values, then uses `idxmax()` on *that Series* to correctly get the author name.

To improve, the generated code should:
1. Format the `author_stats` output to match the requested ""Author, Count of Books, and Average Price"" display.
2. Correctly identify the author with the most books in stock by grouping and summing stock, then using `idxmax()` on the resulting grouped Series to get the author's name.",command-r7b-12-2024,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, KFold, stratified_kfold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, XGBoostClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import make_pipeline
...",3,"The generated code provides only a skeletal structure with placeholders (""... (data loading and preprocessing steps) ..."", ""... (learning curves, validation curves, bootstrap sampling) ..."", etc.) and the core logic for many sections is missing or incorrect. It doesn't actually implement the advanced data preprocessing (multiple imputation, outlier detection/treatment, advanced feature engineering), comprehensive evaluation (stratified k-fold, learning/validation curves, bootstrap sampling), SHAP/LIME, statistical significance testing (its t-test implementation is flawed), model calibration, or the comparison dashboard.

Specifically:
- **Data Preprocessing:** It includes `SimpleImputer` and `PolynomialFeatures` but lacks the full pipeline for multiple imputation (`IterativeImputer`), outlier detection/treatment, and the sophisticated `ColumnTransformer` setup seen in the ground truth.
- **Model Optimization:** The `param_grid` for 'rf' incorrectly includes `svm__C` and 'svm' is duplicated. The `GridSearchCV` is called inside `build_and_optimize_model` but the overall flow of applying this to *all* models and collecting results is not shown. It also specifies `f1_macro` for GridSearchCV without contextualizing it.
- **Ensemble Models:** It hints at `VotingClassifier` and `StackingClassifier` but the `create_ensemble` function is incomplete. It uses `RandomForestRegressor` as a meta-learner for `StackingClassifier`, which is fundamentally wrong for a classification task (should be `LogisticRegression` or a classifier). It completely omits blending.
- **Evaluation:** The `evaluate_model` function uses `KFold` but not `StratifiedKFold` as requested, and it only calculates F1-score, missing other key metrics. The sections for learning curves, validation curves, and bootstrap sampling are entirely missing.
- **Statistical Significance Testing:** The `ttest` function compares scores from different KFold splits directly using `ttest_ind`, which is incorrect for paired comparison as models are typically compared on the *same* folds. It's missing `ttest_rel` for paired comparison on CV scores.
- **Model Interpretability (SHAP/LIME):** Only `import shap` is present, but no actual implementation for generating and visualizing SHAP values.
- **Model Calibration:** `CalibratedClassifierCV` is imported, but the implementation for plotting calibration plots and calculating Brier scores is missing.
- **Model Comparison Dashboard:** This entire section, including generating plots, displaying metrics, and confidence intervals, is absent.

The code's structure suggests an understanding of the components, but the actual implementation is largely absent or incorrect for the specified advanced features. The imports are minimal, and the flow is not executable.",command-r7b-12-2024,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy.stats import pearsonr, ttest_ind, ttest_ind_from_stats, bootstrat
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.linear_model import LogisticRegression, Lasso, Ridge
from sklearn.ensemble import IsolationForest, LocalOu...",2,"The generated code is severely incomplete and contains numerous syntax errors, logical errors, and placeholders. It fails to implement most of the core requirements of the task.

**Major Issues:**

1.  **Missing Imports and Incorrect Usage:** Many necessary imports (e.g., `IterativeImputer` enablement, `pygam`, `arviz`, `warnings`) are missing. Several imports or function calls are syntactically incorrect (e.g., `bootstrat`, `Multiplayer`, `PCA empieza`, `Make_quasicircular`, `Marx`, `Potri`, `theano`, `latent`).
2.  **Imputation:** The imputation section is fundamentally broken. `globals()[method + 'Imputer']()` will fail because it expects `MICEImputer` which doesn't exist directly as a scikit-learn class. It also attempts to impute the entire DataFrame `data` without separating features from the target, and doesn't handle numerical/categorical columns explicitly. There's no actual comparison implemented, just a print statement and an ellipsis.
3.  **Statistical Models:**
    *   **Logistic Regression with Regularization:** No code provided for this.
    *   **GAMs:** No code provided for GAMs (PyGAM).
    *   **Bayesian Logistic Regression:** The PyMC3 model is heavily flawed. `pm.Marx` is not a valid distribution. It uses `pm.math.softmax` on `mu`, which is typically for multi-class classification, not binary, and the `n` parameter for `Binomial` is incorrect. The `trace = pm.sample()` would run, but the model setup is wrong. It lacks proper separation of features and target, and there's no prediction or evaluation.
4.  **Hypothesis Testing:** No code provided for ANOVA, Permutation Tests, or Bootstrap Confidence Intervals, only comments.
5.  **Anomaly Detection:** Only imports are present, no actual implementation for Isolation Forest, LOF, or One-Class SVM.
6.  **Dimensionality Reduction:** `PCA empieza` is a syntax error. `FactorAnalysis` is entirely missing. It attempts to fit `PCA` on `data[numeric_features]` but `numeric_features` is undefined.
7.  **Robust Statistical Methods:** No code provided.
8.  **Model Diagnostics:** No code provided for residual analysis, influence measures, or goodness-of-fit tests.
9.  **Visualizations:** No code provided for Q-Q plots, leverage plots, or diagnostic dashboards, only comments.
10. **Overall Structure:** While there are comments mapping to the requirements, the actual code beneath them is mostly empty, incorrect, or syntactically invalid. The code is far from executable or functional. It looks like a very early draft with many placeholders rather than a complete solution.
    *   The problem statement explicitly asked for proper imports, and this generated code fails significantly in that aspect.
    *   It does not handle the `water_potability.csv` data properly (e.g., separating X and y).

In essence, the generated code demonstrates a superficial understanding of the requested components (by including comments and some imports) but critically fails to correctly *implement* them.",command-r7b-12-2024,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
import scipy.cluster.hierarchy as sh
from sklearn.cluster import KMeans, DBSCAN, GaussianMixture, SpectralClustering
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, gap_statistic
from sklearn.decomposition import PCA, tSNE, NEDA, Facofac
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_obj...",4,"The generated code attempts to address the task but falls significantly short in terms of correctness, completeness, and code quality.

**Correctness:**
*   **Imports:** Several imports are incorrect or appear to be typos (`tSNE` should be `TSNE`, `NEDA`, `Facofac`, `Consensusanlysis`, `f_score`). `gap_statistic` is not a direct import from `sklearn.metrics` for general use; it's typically calculated manually or via specific libraries. `scipy.cluster.hierarchy` is imported as `sh` but then parts of it (like `linkage`, `fcluster`) are used directly, which is confusing if `sh` is meant to prefix them.
*   **Feature Engineering:** The example feature engineering `df['QUALITY_INDEX'] = (df['pH'] - 7) / 2 + 5` is a very simplistic and arbitrary calculation, not an ""advanced water quality index."" It also completely ignores the other suggested indices like chemical balance ratios, contamination risk scores, or temporal patterns. The comment `df[['pH', 'conductivity', '...']]` for scaling indicates a lack of concrete implementation for all features.
*   **Clustering:** The `n_clusters` for K-means and GMM are hardcoded (`5`, `3`) without any attempt at optimization, which is a core requirement of the task. DBSCAN parameters (`eps=1`, `min_samples=10`) are also hardcoded without any method for tuning or explanation, which is crucial for DBSCAN.
*   **Dimensionality Reduction:** The `PCA` and `tSNE` objects are created, but `UMAP` is imported but not used. The task specifically asked for `PCA, t-SNE, UMAP, Factor Analysis`, and the generated code only uses `PCA` and `tSNE` (with the typo) and fails to implement `Factor Analysis`. The imported `UMAP` is also capitalized `UMAP` which is not the typical convention for the `umap` library's `UMAP` class.
*   **Placeholders:** Many sections are left as comments (`# ... Implement ...`, `# ... Analyze ...`, `# ... Use ...`) effectively punting the actual implementation. This is a major completeness issue.
*   **Data Handling:** There's no handling of missing values, which is common in real datasets and crucial before scaling and clustering.

**Code Quality:**
*   **Lack of Structure/Functions:** The code is a monolithic script without any functions or classes to organize the different stages (feature engineering, clustering, validation, etc.). This makes it hard to read, test, and maintain.
*   **Hardcoding:** Too many parameters (e.g., `n_clusters`, `eps`, `min_samples`) are hardcoded without any mechanism for dynamic selection or proper explanation.
*   **Incomplete Code:** The numerous `...` placeholders make the code non-functional for many key requirements.
*   **Readability:** The mixture of correct and incorrect imports, and the general lack of full implementation, make it difficult to follow the intended flow.
*   **Repetitive Imports:** `PCA, tSNE, UMAP` are imported twice.

**Completeness:**
*   **Advanced Feature Engineering:** Only a single, simplistic ""quality index"" is attempted. Chemical balance ratios, contamination risk scores, and temporal patterns are missing.
*   **Multiple Clustering Algorithms:** K-means, Hierarchical, DBSCAN, GMM, and Spectral Clustering are initiated, but parameter tuning and optimal cluster determination (a separate requirement) are not implemented for most.
*   **Optimal Number of Clusters:** This section is almost entirely commented out with placeholders. Only a general comment exists.
*   **Dimensionality Reduction:** `Factor Analysis` is completely absent. `UMAP` is imported but not used.
*   **Cluster Stability Analysis:** This critical section is entirely commented out with a placeholder.
*   **Water Quality Profiles:** This section is commented out.
*   **Cluster Visualization Dashboard:** Only a single, basic `matplotlib.pyplot` scatter plot is provided as an ""example."" The task specifically asked for interactive 2D/3D plots, radar charts, and interactive visualizations using Plotly, which are largely missing.
*   **External Validation:** Just a comment.
*   **Anomaly Detection:** Entirely commented out.

In summary, the generated code provides a skeletal outline with several functional errors and significant gaps in implementation, failing to meet the ""comprehensive"" and ""expert"" level implied by the prompt. It's more of a very rough draft than a working solution.",command-r7b-12-2024,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from scipy.stats import pearsonr

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1) Determine the strongest correlations
positive_corr = df.corrwith(df['quality']).sort_values(ascending=False)[1:]
negative_corr = -df.corrwith(df['quality']).sort_values(ascending=False)[1:]

# 2) Top 3 positively/negatively correlated features
print(""Top 3 positively...",7,"The generated code addresses most of the requirements but has several significant issues.

**Correctness:**
*   **Positive/Negative Correlation Calculation:** The calculation for `negative_corr` is incorrect. Multiplying the entire series by -1 after sorting `df.corrwith(df['quality']).sort_values(ascending=False)[1:]` does not correctly identify the most negative correlations. It should sort by ascending order to get the most negative values directly.
*   **`pearsonr` usage:** `scipy.stats.pearsonr` returns a tuple `(correlation_coefficient, p_value)`. The code `corr = pearsonr(df[feature], df['quality'])` assigns this tuple to `corr`, but then `corr` is not used.
*   **Linear Regression Plotting:** The plotting of the regression line `model.predict(df[[feature]].loc[x].values)` is fundamentally flawed. `x` is defined as `np.linspace(df[feature].min(), df[feature].max())` but then `.loc[x].values` is applied to `df[[feature]]`. This will lead to `KeyError` because `x` values are not guaranteed to be in the index of `df[[feature]]`. It should be `model.predict(x.reshape(-1, 1))` to predict on the interpolated values.
*   **No `numpy` import:** `import numpy as np` is missing, but `np.linspace` is used. This would cause a `NameError`.

**Code Quality:**
*   Imports are mostly appropriate, although `numpy` is missing for `np.linspace`.
*   The use of a `plot_regression` function is good for reusability.
*   The `positive_corr` and `negative_corr` definitions are a bit clunky, especially the `[1:]` slice which correctly removes 'quality' but could be done more explicitly via `.drop('quality')`.

**Completeness:**
*   **Task 1 & 2 (Strongest correlations, Top 3):** Partially complete. The logic for getting the *most positive* is fine, but the *most negative* is incorrect as explained above. The output prints ""Top 3 positively correlated features"" and ""Top 3 negatively correlated features"" but the values themselves for negative are not right.
*   **Task 3 (Heatmap):** Correctly implemented.
*   **Task 4 (Scatter plots with regression):** The intent is there, but the implementation of the regression line plotting is incorrect and would throw an error, making this part incomplete/broken. The task explicitly asks for ""linear regression lines"", which means calculated and plotted correctly. The code uses `sklearn.linear_model.LinearRegression` which is acceptable, but the subsequent plotting logic is broken.

**Specific Improvements for a Higher Score:**
1.  **Fix negative correlation calculation:**
    ```python
    quality_correlations = df.corrwith(df['quality']).drop('quality')
    positive_corr = quality_correlations.sort_values(ascending=False)
    negative_corr = quality_correlations.sort_values(ascending=True) # Get actual most negative
    # Then access head(3) from both
    ```
2.  **Correct `numpy` import:** Add `import numpy as np`.
3.  **Correct regression line plotting:**
    ```python
    x_pred = np.linspace(df[feature].min(), df[feature].max(), 100).reshape(-1, 1)
    y_pred = model.predict(x_pred)
    plt.plot(x_pred, y_pred, color='red')
    ```
4.  **Error Handling:** The ground truth includes `try-except` for file loading. While not strictly required by the prompt ""Provide only the Python code without explanations"", it's good practice for robustness.
5.  **Clarity in Output:** The prompt asks to ""List the top 3 positively correlated and top 3 negatively correlated features with their correlation coefficients."" The generated code uses `print(positive_corr.head(3))` which fulfills this, but the negative one won't be correct.",command-r7b-12-2024,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# 1) Load the dataset and clean any missing values
data = pd.read_csv('video_games_sales.csv')
data.dropna(inplace=True)

# 2) Test whether Nintendo games have significantly higher global sales
nintendo_sales = data[data['publisher'] == 'Nintendo']['global_sales'].mean()
other_publishers_sales = data[data['publisher'] != 'Nintendo']['global_sales'].mean()
t_stat, p_value = stats.ttest_ind(data[data['publisher'] == 'Nintend...",3,"The generated code attempts to address the requirements but has several critical flaws that make it largely incorrect and incomplete for the given task.

**Critical Issues:**

1.  **Data Cleaning:**
    *   `data.dropna(inplace=True)` is overly aggressive. The ground truth correctly identifies specific columns for `dropna` and handles `Year_of_Release` conversion to `int` and `User_Score` `tbd` to numeric. The generated code drops *all* rows with *any* missing value, which is usually not ideal and impacts the dataset size disproportionately, potentially removing valid entries.
    *   It does not handle `Year_of_Release` type conversion, which is crucial for the Mann-Whitney U test, implicitly assuming it's already numerical or will work as is (which it won't if there are non-numeric strings or NaNs).
    *   It doesn't handle casing of column names (e.g., `publisher` vs `Publisher`, `global_sales` vs `Global_Sales`, `genre` vs `Genre`). This will lead to `KeyError` if the CSV header uses different casing. The ground truth correctly uses the exact column names.

2.  **Hypothesis Test 1 (Nintendo vs. Others):**
    *   The `t_stat` and `p_value` are calculated correctly by `stats.ttest_ind` but then *overwritten* by subsequent tests without proper assignment or reporting. This means only the `p_value` from the *last* executed test will be printed for *all* interpretations.
    *   It calculates `mean()` separately but doesn't use them in the `ttest_ind` call (which works with series directly).
    *   It does not specify `alternative='greater'` as required by the prompt (""significantly *higher* global sales""). This is a critical logical omission.
    *   It does not print the individual results and interpretations for this test.

3.  **Hypothesis Test 2 (Action vs. Sports):**
    *   Same issue as HT1 regarding `t_stat` and `p_value` being overwritten.
    *   Does not print individual results and interpretations.

4.  **Hypothesis Test 3 (Chi-square):**
    *   This is the most severely incorrect part. The `observed` and `expected` contingency table creation is fundamentally flawed:
        *   `top_genres` and `top_platforms` are `Pandas Series` with counts, not categories filtered from the DataFrame.
        *   `observed = np.array([[top_genres.get(genre, 0) * top_platforms.get(platform, 0) ...]])` attempts to create an observed table by multiplying counts, which is not how a contingency table is formed from a DataFrame. A contingency table counts occurrences of combinations of categories (`pd.crosstab`).
        *   `expected = np.outer(top_genres, top_platforms) * 10` is an arbitrary and completely incorrect calculation for `expected` values. `chi2_contingency` *calculates* expected values internally. Providing its own `expected` array that is incorrectly derived will lead to incorrect or failed chi-square test.
    *   It doesn't filter the DataFrame to *only* include the top 5 genres and platforms before creating the contingency table, which was explicitly asked for in the prompt.
    *   The `correction=False` in `chi2_contingency` is unusual for a general case unless specifically required.

5.  **Hypothesis Test 4 (Sales Before/After 2000):**
    *   `release_year_mean1` and `release_year_mean2` are calculated but not used.
    *   Column name issue: `release_year` vs `Year_of_Release`.
    *   Same issue as HT1 regarding `p_value` being overwritten.
    *   Does not print individual results and interpretations.

6.  **Interpretation:**
    *   The code only prints the `p_value` from the *last* executed test (`Mann-Whitney U`) for *all* interpretations. This means the interpretations given are entirely misleading and incorrect for the first three tests.
    *   It provides a single generic interpretation for `if p_value < 0.05`, rather than specific interpretations for each test based on its null and alternative hypotheses as requested and as shown in the ground truth.
    *   It lacks proper print statements to clearly delineate each test's output.

The code demonstrates a basic understanding of calling statistical functions but fails significantly in data preparation, correct application of statistical tests (especially chi-square), managing variable scope for results, and providing meaningful, individual interpretations as per the prompt's requirements.",command-r7b-12-2024,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt

# 1) Load the dataset and handle missing values
df = pd.read_csv(""video_games_sales.csv"")
df.dropna(inplace=True)  # drop rows with missing values

# 2) Create a bi...",5,"The generated code has several critical issues that make it largely incorrect or incomplete for the given task requirements.

1.  **Missing Value Handling (Critical Flaw):** The ground truth explicitly states to handle missing values by dropping specific columns (`Global_Sales`, `Year`, `Publisher`, regional sales) to ensure data integrity for target creation and feature engineering. The generated code uses `df.dropna(inplace=True)`, which drops *any* row with *any* missing value. This is a much more aggressive approach and could lead to significant data loss if missing values are scattered across many columns, potentially impacting model performance and the representativeness of the dataset. More importantly, it doesn't align with the specific intent of the ground truth's detailed handling.

2.  **Feature Selection (Major Flaw):**
    *   **Regional Sales in `Total_Regional_Sales`:** The ground truth specifically uses `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`. The generated code uses 'NA', 'EU', 'JP', 'Other', which are likely to be incorrect column names (assuming the dataset columns are named as in `NA_Sales`, etc.). This would cause a `KeyError` if these columns don't exist.
    *   **Feature set for X:** The ground truth explicitly defines `features` to be used for the model (`Platform`, `Genre`, `Publisher`, `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`, `Total_Regional_Sales`, `Years_Since_Release`). The generated code uses `X = df.drop('High_Sales', axis=1)`. This means it includes all columns except the target, which might include 'Name', 'Rank', etc., which are not numerical or are irrelevant for the classification task and were not intended as features. This also means it might include original 'NA_Sales', 'EU_Sales', etc. alongside 'Total_Regional_Sales', leading to multicollinearity if not explicitly handled. The task asked for specific features.
    *   **Dropping `Year`:** The ground truth converts `Year` to `int` and uses it to create `Years_Since_Release`. The generated code drops all non-target columns into `X`, likely including 'Year' itself, which is still a numerical feature. However, it implicitly assumes `Year` is included as a feature. The specific intent of `Years_Since_Release` was to be derived from `Year`, and the original `Year` column itself might not be the desired feature. The ground truth explicitly chose which features to include.

3.  **Target Variable Conversion (Minor Issue):** While `np.where` works correctly for `High_Sales`, the ground truth's `(df['Global_Sales'] > 1.0).astype(int)` is more idiomatic and concise for this Boolean conversion.

4.  **Displaying Feature Importance (Major Flaw):** The implementation `for feature in X.columns: if feature != 'High_Sales': print(f""{feature}: {feature_importances[sorted_indices[0]]}"")` is **critically flawed**. It iterates through all columns of `X` but *always prints the importance of the top feature* (`feature_importances[sorted_indices[0]]`) for every feature in `X`. This means it does not display the correct importance for each respective feature. It should map the importance scores back to their corresponding feature names. The ground truth correctly uses `pd.Series` to map importances to feature names and then sorts them.

5.  **Confusion Matrix Visualization:** The generated code attempts to plot the confusion matrix using `matplotlib.pyplot.imshow()`. While this is a nice visualization, the task *only* asked to ""Display feature importance"" and ""Evaluate the model using accuracy, precision, recall, F1-score, and confusion matrix"". Displaying the matrix as text (as in the ground truth) is sufficient for ""displaying"" it, and no plot was explicitly requested. Adding a plot (even without specific instructions) is not inherently wrong but goes beyond the strict requirements and adds unnecessary complexity if not requested. More critically, it lacks `plt.show()` after `plt.plot()` which would prevent the plot from actually being displayed in some environments (though implicitly `plt.show()` might be called in interactive environments). The `plt.show()` is present.

6.  **Missing `random_state` for `RandomForestClassifier`:** The ground truth uses `random_state=42` and `n_jobs=-1` for reproducibility and performance. Using `random_state` is good practice for consistent benchmark results.

7.  **`stratify` in `train_test_split`:** The ground truth uses `stratify=y` to ensure that the proportion of target classes is the same in both training and testing sets, which is particularly important for imbalanced datasets, as `High_Sales` might be. This is a best practice often overlooked but included in the ground truth and missed by the generated code.

The code's structure is generally okay with imports grouped at the top, but the logical errors in data handling, feature engineering, and evaluation output are significant.",command-r7b-12-2024,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from scipy.stats import iqr

# 1) Load the dataset and handle missing values
df = pd.read_csv('video_games_sales.csv')
df.dropna(inplace=True)  # Remove rows with any missin...",4,"The generated code attempts to address all requirements but misses the mark significantly on several key aspects, leading to incorrect or incomplete implementation.

**Key Issues:**

1.  **Missing Value Handling:**
    *   The `df.dropna(inplace=True)` approach is overly aggressive and will remove a large portion of the dataset, especially given the various missing values across columns. The ground truth code correctly uses `SimpleImputer` with appropriate strategies for different data types (`median` for 'Year', `most_frequent` for 'Publisher'/'Genre'). This is a critical first step and the generated code fails here.

2.  **Feature Engineering - `Sales_Ratio_NA_EU`:**
    *   `df['NA_Sales'] / df['EU_Sales']` does not handle potential division by zero, which the ground truth code addresses with `+ 1e-6`.

3.  **Feature Engineering - `Genre_Market_Share`:**
    *   The calculation `(df['Global_Sales'] / total_sales) * 100` is incorrect. This calculates *each game's* market share relative to total sales, not the market share of its `Genre`. The ground truth correctly uses `df.groupby('Genre')['Global_Sales'].transform('sum') / total_global_sales`.

4.  **Feature Engineering - `Platform_Popularity`:**
    *   `df.groupby('Platform')['Game_ID'].nunique()` assumes `Game_ID` exists and is unique, which is not guaranteed by the problem description (and `Game_ID` doesn't exist in the sample `video_games_sales.csv`). The ground truth uses `df.groupby('Platform')['Name'].transform('count')`, which is more robust.
    *   `df['Platform'].map(platform_games)` will not work as intended because `platform_games` is a Series indexed by Platform, `map` applies to individual values. `transform('count')` is the correct method.

5.  **PCA Implementation:**
    *   The PCA part has fundamental issues:
        *   It directly performs PCA on raw sales columns (`df[['Global_Sales', 'NA_Sales', 'EU_Sales', 'Sales_Momentum', 'Publisher_Avg_Sales']]`) without prior scaling, which is crucial for PCA as it's sensitive to feature scales. The ground truth correctly uses `StandardScaler`.
        *   The way `pca_df` is constructed using `pca.components_.T.dot` is erroneous and not how you typically get the transformed PCA DataFrame. `pca.fit_transform()` directly returns the principal components.
        *   `n_components=0.95` is a valid way to specify explained variance, but the subsequent code tries to interpret the resulting 'columns' incorrectly.

6.  **Polynomial Features:**
    *   The `make_pipeline(PolynomialFeatures(degree=2), StandardScaler())` is a good structural choice, but the `poly_df` column naming `['Global_Sales_poly', 'NA_Sales_poly', 'EU_Sales_poly']` is simplistic and wrong. `PolynomialFeatures` generates interaction terms (e.g., `NA_Sales^2`, `NA_Sales * EU_Sales`), which won't fit into these 3 column names. The ground truth correctly uses `poly.get_feature_names_out(sales_cols)` to get accurate names.

7.  **Interaction Features (Year & Genre):**
    *   `df['Year_Genre'] = df['Year'].astype(str) + '_' + df['Genre']` only creates a categorical interaction, not an *effect* interaction that would be useful for linear models or other numerical analysis (e.g., `Year * one-hot_encoded_Genre`). The ground truth correctly uses one-hot encoding for genre and then multiplies with year.

8.  **Outlier Handling:**
    *   `df = df[(df['Global_Sales'] > (df['Global_Sales'].quantile(0.25) - 1.5 * iqr(df['Global_Sales']))).all(axis=1)]` is severely flawed:
        *   It uses `all(axis=1)` on a single boolean Series, which will always be `True` if `len(df) > 0`, effectively doing nothing useful or potentially filtering incorrectly.
        *   It only checks the lower bound and doesn't handle the upper bound.
        *   It performs row removal, which is typically not preferred for outlier handling (capping or transformation is often better). The ground truth correctly implements capping, which is specified in the task as ""handle outliers using IQR method"" for which capping is a common strategy.
        *   It only applies to `Global_Sales` and explicitly states ""...Repeat for other relevant columns..."", meaning the provided code is incomplete for this step.

Overall, the generated code demonstrates awareness of the required steps but implements them with significant errors or inefficiencies, leading to incorrect calculations and potentially unusable intermediate dataframes. The aggressive `dropna()` at the start is a critical flaw.",command-r7b-12-2024,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

# 1) Load the dataset and clean zero values by replacing them with NaN and then median imputation
data = pd.read_csv('diabetes.csv')
data = data.fillna(data.median())

# 2) Test whether diabetic patients have significantly higher glucose levels compared to non-diabetic patients (use t-test)
glucose_diff = stats.ttest_ind(data[data['Diabetes'] == 0]['Glucose'], 
                          ...",4,"The generated code attempts to address all parts of the task but falls short on correctness and completeness, particularly in the data cleaning and certain statistical test implementations.

**Primary Reasons for Failure/Critical Missing/Incorrect:**

1.  **Data Cleaning - Zero Values Imputation:**
    *   The prompt explicitly states: ""clean zero values by replacing them with NaN and then median imputation"". It also implies that some columns (like Pregnancies) can legitimately be zero.
    *   The generated code simply `data = data.fillna(data.median())`. This only imputes *already existing* NaNs. It does *not* replace 0s with NaNs first. This is a critical omission and leads to incorrect results for tests involving columns like Glucose, BMI, BloodPressure, etc., where 0 is biologically implausible.
    *   Furthermore, `data.fillna(data.median())` would attempt to impute NaNs in `Outcome` if any existed, which shouldn't happen, and the median of `Outcome` (0 or 1) doesn't make sense for imputation.

2.  **Dataset Column Name for Outcome:**
    *   The prompt implies ""diabetic patients"" and ""non-diabetic patients"" and common usage for diabetes datasets (`diabetes.csv`) is an 'Outcome' column (binary: 0 for non-diabetic, 1 for diabetic).
    *   The generated code uses `'Diabetes'` column for Outcome, which is incorrect. The standard 'Pima Indians Diabetes' dataset typically has an 'Outcome' column. This will cause `KeyError` or incorrect results if the column name is indeed 'Outcome'.

3.  **Chi-Square Test Implementation:**
    *   The age grouping is not implemented correctly. The `data['Age'].value_counts().unstack(fill_value=0)` does not create age groups, it just counts occurrences of each age value. This is not a contingency table required for chi-square.
    *   The chi-square test was supposed to determine if *age groups* (Young, Middle, Older) are associated with *diabetes outcome*. The current implementation fundamentally misses the creation of these age groups and the cross-tabulation with the outcome variable.

4.  **ANOVA - BMI Categories:**
    *   The labels for BMI categories are created (`bmi_categories = ['Underweight', 'Normal', 'Overweight', 'Obese']`), but they are not actually applied to the DataFrame (e.g., using `pd.cut`). Instead, the code relies on direct filtering (`data[data['BMI'] < 18.5]`), which is less robust and doesn't explicitly link to the 'category' concept required by the prompt. While the filtering results in correct *groupings*, integrating it with `pd.cut` makes the code cleaner and aligns with common practice for categorical variables.

5.  **Interpretation of Results:**
    *   The code provides the p-values but the interpretation loop only states ""Significant"" or ""Not Significant"". The task explicitly asks to ""Display p-values and interpret results with significance level =0.05"". The ground truth provides detailed interpretations for each test (e.g., ""diabetic patients have significantly higher glucose levels,"" ""association between age groups and diabetes outcome""). The generated code's interpretation is very minimal and general.

6.  **Missing Imports and Structure:**
    *   `matplotlib.pyplot` is imported but not used.
    *   No proper `ALPHA` constant defined, though `significance_level` is set.
    *   Lack of print statements to indicate what each output refers to (e.g., the ground truth has clear headings for each hypothesis test).
    *   No error handling for `pd.read_csv`, which is good practice.

Overall, the code has fundamental issues in data preprocessing (handling zeros) and several statistical test implementations, leading to incorrect or incomplete results. The interpretation is also too brief.",command-r7b-12-2024,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusio...",5,"The generated code attempts to address all requirements but suffers from significant correctness and completeness issues, leading to a low score.

**Correctness:**
- **Missing Value Handling:** The line `data[data == 0] = median_data[data == 0]` is fundamentally incorrect for replacing zeros with medians. It attempts to filter `median_data` based on `data == 0`, which will not correctly apply the median values to the specific columns where they are needed. The ground truth correctly iterates through specific columns and uses `fillna` after replacing 0s with `np.nan`.
- **Feature Engineering (`Insulin_level` and `Age_group`):**
    - `Insulin_level`: The binning logic for tertiles is problematic. `pd.cut` with `bins=[0, tertiles[0], tertiles[1], np.inf]` assumes tertiles are computed from the full `Insulin` range, which is fine, but `tertiles` are calculated from `data['Insulin']`, and then `data['Insulin_level'] = pd.cut(data['Insulin'], ...)` is applied. This part is okay, but `pd.qcut` is a more direct and robust way to achieve this.
    - `Age_group`: The bins `[0, 25, 50, 75, np.inf]` and labels `['Low Risk', ' Moderate Risk', 'High Risk']` do not align with the task's requirement (e.g., '0-20', '21-30', etc.) and also the number of labels does not match the number of bins (4 bins, 3 labels). This will cause an error or incorrect categorization.
- **Categorical Encoding & Scaling:** The code uses `LabelEncoder` for `Age_group` but then applies `StandardScaler` only to selected numeric features (`Glucose`, `BMI`, `Insulin`). It completely misses encoding the `Insulin_level` categorical feature. It also does not use a `ColumnTransformer` (or equivalent manual handling for *all* features) to ensure that only categorical features are encoded and only numerical features are scaled, and that *all* features are properly processed before model training. The approach taken means the models will see incorrectly encoded categorical features (as numerical integers) and unscaled numerical features.
- **Cross-validation and Hyperparameter Tuning Logic:**
    - The cross-validation structure is mixed with GridSearchCV. The `results[name].update(cv_results)` after the grid search for random forest and gradient boosting is confusing.
    - For models not in `param_grids` (LR, SVC), `cv_results['cv_scores'] = [model.score(X_test, y_test)] * 5` is conceptually flawed. Cross-validation should be performed on the *training* data, not by repeating the `X_test` score. This completely negates the purpose of cross-validation.
    - The code does not perform cross-validation for *all* models first, and then pick the best model for tuning. Instead, it attempts to integrate tuning and evaluation, but incorrectly for non-tuned models.
- **Metrics Calculation:**
    - `classification_report(output_dict=True)` returns a dictionary where metric values are nested under class labels. The syntax used `results[name]['precision'] = classification_report(...['precision']` would extract a dictionary for `precision` for each class (0 and 1), not a single aggregated precision score (like weighted or macro avg). This will lead to errors or incorrect results.
    - The accuracy for non-tuned models is obtained by `model.score(X_test, y_test)`, which is only the test accuracy, not a cross-validation score. This is different from the task requirement to use cross-validation to evaluate each model.
- **Feature Importance:** The feature importance code for tree-based models prints `feature_importances_[sorted_indices[0]]` repeatedly, which is the importance of only the *top* feature for all listed features. It should print the importance of *each* feature in the sorted list.
- **Imports:** `imblearn` was imported but not used, and `scipy.stats.median_abs_deviation` was imported but not used.

**Code Quality:**
- **Clarity and Structure:** The flow for cross-validation and hyperparameter tuning is convoluted and not as clean or modular as the ground truth's approach using `Pipeline` and `ColumnTransformer`.
- **Variable Usage:** `median_data[data == 0]` for median imputation is non-standard and incorrect.
- **Logging/Printing:** Lacks informative print statements about data shapes or preprocessing steps.

**Completeness:**
- **Pre-processing:** The initial `data.dropna(inplace=True)` before handling zeros is incorrect as the zeros are specified as missing values.
- **Cross-validation for all models:** Not correctly implemented as described above.
- **Comprehensive Evaluation:** While it attempts to get accuracy, precision, recall, f1, and ROC-AUC, the implementation of these is flawed due to the issues mentioned in ""Correctness.""
- **Visualization:** Confusion matrices are plotted, but currently using `y_pred` from the *last* model trained in the loop for all plots, which is incorrect. ROC curves also suffer from `model` being the last defined model in the loop, not the specific model for which the curve is being plotted.
- **Feature Importance:** Only Random Forest is explicitly handled, but it should be done for all tree-based models as per the prompt, and the implementation is flawed anyway.

In summary, the code has a basic structure but fails to correctly implement key steps like data preprocessing, feature engineering (specifically `Age_group` labels), comprehensive cross-validation and metrics calculation, and robust visualizations and feature importance display.",command-r7b-12-2024,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures, OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, SelectFromModel, RecursiveFeatureElimination, chi2
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
import seaborn as sns
import m...",3,"The generated code attempts to address many aspects of the prompt but suffers from fundamental correctness and logical errors, making it largely non-functional or incorrect in its implementation of the required steps.

Here's a breakdown of the issues:

1.  **Missing Imports and Incorrect Imports**: `RecursiveFeatureElimination` is not a direct import; it's `RFE` from `sklearn.feature_selection`. `chi2` is used with `SelectKBest` for classification, but `f_classif` is often more general. `GridSearchCV` and `RandomForestRegressor` are imported, but the task implies classification (`Outcome` column) and the usage of `RandomForestClassifier` for feature importance. `statsmodels.api` is imported but never used.

2.  **Handling Missing Values (Step 1)**: The code `data.fillna(data.mean(), inplace=True)` is used for handling missing values. While this is a valid imputation strategy, the prompt specifically mandates identifying columns where 0s represent missing values (e.g., Glucose, BMI) and replacing those 0s with NaNs *before* imputation. The generated code misses this crucial preprocessing step, which is common for the diabetes dataset.

3.  **Engineered Features (Step 2)**:
    *   `Metabolic_Score`: The formula `data['Glucose'] * data['BMI'] / data['Insulin']` is a division, which can lead to `Inf` or `NaN` if Insulin is 0. The ground truth uses a simple sum, which is safer and more interpretable without specific domain knowledge for a product/division.
    *   `Risk_Factor_Count`: `data[['Age', 'BMI', 'BloodPressure', 'Glucose', 'Insulin']].sum(axis=1)` simply sums the values of these features, not a *count of high-risk factors* based on thresholds as specified in the ground truth and generally understood. This is a complete misinterpretation of the requirement.
    *   `Pedigree_Age_product`: The code uses `data['Pedigree']`, but the column name in `diabetes.csv` is typically `DiabetesPedigreeFunction`.

4.  **Polynomial Features (Step 3)**:
    *   The `data_poly` copy is made, but then the original columns `Glucose`, `BMI`, `BloodPressure` are *overwritten* with the polynomial features. This is incorrect. PolynomialFeatures generates *new, higher-order features* (e.g., Glucose^2, Glucose*BMI), it doesn't replace the original columns. The original columns `Glucose`, `BMI`, `BloodPressure` should still exist alongside their polynomial transformations.
    *   The `PolynomialFeatures` are applied to only 3 columns (`Glucose`, `BMI`, `BloodPressure`). The prompt asks for application to ""selected numerical columns,"" which in a realistic scenario would be more than just three. The ground truth provides a more comprehensive list.

5.  **Feature Scaling (Steps 4 & 5)**:
    *   `StandardScaler` is applied to `[['Glucose', 'BMI', 'BloodPressure']]` *after* they were overwritten with polynomial features in the previous step. This is problematic.
    *   `MinMaxScaler` is then applied to `[['Glucose_poly', 'BMI_poly', 'BloodPressure_poly']]` which are variables that were never created with these exact names. The `get_feature_names_out` method is needed to get the new column names formed by `PolynomialFeatures`, which is missing.
    *   The overall scaling strategy is confused: applying `StandardScaler` to some and `MinMaxScaler` to others (which haven't been correctly named) makes no logical sense for consistent preprocessing before PCA. Typically, you pick one scaling method for all features that will be fed into a dimensionality reduction or model.

6.  **PCA (Step 6)**:
    *   `data_poly[['Glucose', 'BMI', 'BloodPressure', 'Glucose_poly', 'BMI_poly', 'BloodPressure_poly']]` is used as input. As previously mentioned, `Glucose_poly` etc., are not correctly created.
    *   The explanation of variance ratios is requested, but only `pca.n_components_` is used, not `pca.explained_variance_ratio_`.

7.  **Correlation-based Feature Filtering (Step 7)**:
    *   The `data_pca.corr().abs()` is performed on the *output of PCA*. PCA components are by design orthogonal (uncorrelated *if* all components are retained and the data is centered), so trying to filter for highly correlated features *after* PCA is fundamentally flawed and defeats the purpose of PCA. Correlation filtering should happen *before* PCA or other dimensionality reduction to reduce redundant original features.
    *   The logic for dropping features based on correlation is also incorrect. `features_to_drop` is undefined, and the `inlier_features` list is incorrectly initialized and never used. The code attempts to iterate through `pca.n_components_` and `correlation_matrix.iloc[i, j]`, but `correlation_matrix` based on PCA components will generally be a diagonal matrix (or close to it), leading to no features being dropped.

8.  **SMOTE (Step 8)**:
    *   `y_resampled` is used as input for `smote.fit_resample` but is not defined. The target variable `y` (Outcome column) is never separated from `X` explicitly.
    *   SMOTE should be applied to the preprocessed and potentially feature-selected `X` and `y` *before* the train-test split for the training data specifically. The code applies it to `data_filtered` (which is `data_pca`) and `y_resampled` (undefined).

9.  **Feature Importance (Step 9)**:
    *   `RandomForestRegressor` is used, but the task is a classification problem (`Outcome` is binary), so `RandomForestClassifier` should be used.
    *   `GridSearchCV` is applied, but the primary task is feature importance evaluation, not hyperparameter tuning for a model, although it's not strictly incorrect.
    *   The visualization step `plt.xticks(range(len(feature_importances)), data_poly.columns)` is trying to assign PCA component indices to `data_poly.columns`, which is incorrect. Feature importances are derived from the model trained on PCA components, but these components are not directly mapped back to original feature names without additional mapping logic. The plotting shows generic indices instead of meaningful feature names.
    *   `SelectKBest` and `RFE` (from requirement 6) are completely missing from the generated code.

In summary, the code demonstrates a basic understanding of some tools but fails significantly in logically connecting them, applying them correctly to the dataset, and fulfilling the specific requirements of the prompt, particularly regarding engineered features, the order of operations (e.g., scaling, correlation filtering vs. PCA), and correctly handling the classification nature of the problem.",command-r7b-12-2024,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, ttest_ind_from_counts, ttest_ind_paired, mannwhitneyu, f_oneway, chi2_contingency, ttest_rel
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# 1) Load the dataset and handle any missing values
data = pd.read_csv('world-happiness-report-2021.csv')

# Handle missing values (if any)
# Example: Fill NaN values with the column's mean/median
for col in data.columns:
...",3,"The generated code has significant issues in correctness, completeness, and adherence to the prompt's requirements.

**Correctness Issues:**
*   **Column Names:** The code incorrectly assumes column names like 'Region', 'Happiness Score', 'GDP per capita', and 'Social Support'. The ground truth uses 'Regional indicator', 'Ladder score', 'Logged GDP per capita', and 'Social support' which are directly from the CSV. This makes most of the statistical tests fail immediately or produce incorrect results.
*   **Missing Value Handling:** It fills NaNs with the mean for *all* numerical columns, which might not be appropriate for all columns (e.g., if 'Generosity' has missing values, imputing with mean might be okay, but dropping rows for critical columns is often better for statistical tests as done in ground truth). More critically, the prompt specified handling missing values for *critical columns*, which the generated code does not.
*   **T-test (Task 2):** Fails due to incorrect column names. Also, it doesn't specify `equal_var=False` (Welch's t-test), which is generally more robust and used in the ground truth.
*   **Mann-Whitney U Test (Task 3):**
    *   Incorrect column names.
    *   The percentile calculation `data['Happiness Score Percentile'] = data['Happiness Score'].rank(method='min', ascending=False) * 100 / len(data)` is not correctly identifying the top 25% and bottom 25%. `rank(ascending=False)` gives rank from highest, so `data[data['Happiness Score Percentile'] >= 75]` would be the *bottom* 25% (ranks 75-100), not top. `data[data['Happiness Score Percentile'] < 25]` would be the *top* 25% (ranks 0-25). This is flipped compared to the prompt's intent.
    *   The `alternative` parameter for `mannwhitneyu` is not specified; the ground truth uses `two-sided`.
*   **ANOVA (Task 4):** This is completely incorrect. `within_variances` calculates a F-statistic for each region comparing it against `data[data['Region'] != region]['Happiness Score']`, which is *not* how ANOVA for multiple groups works. ANOVA compares the means of *all* groups simultaneously, typically `f_oneway(group1, group2, group3, ...)`. The provided solution's ANOVA approach is fundamentally flawed and does not address the task.
*   **Correlation (Task 5):**
    *   Incorrect column names.
    *   The p-value calculation `1 - pvalue(gdp_support_corr)` using a custom `pvalue` function based on `norm.cdf` is highly suspect and not the standard way to get a p-value for a Pearson correlation coefficient. `scipy.stats.pearsonr` (used in ground truth) directly provides the p-value.
*   **Chi-square and Generosity (Tasks 6 & 7):** The code explicitly states `... (Code for this test would involve...)` demonstrating it didn't even attempt to implement these, indicating a significant lack of completeness.
*   **Interpretation of results (Task 8):** The code only prints p-values but doesn't include any `if p_value < alpha:` logic for interpretation or setting the significance level `alpha`, which was explicitly requested.

**Completeness:**
*   Only 3 out of 7 statistical tests are attempted, and most of those are incorrect. Tasks 4, 6, 7, and 8 are either completely wrong or not implemented.
*   No `alpha` variable is defined or used for hypothesis testing.
*   The `try-except` block for file loading is missing.
*   Specific instructions on handling missing values for *critical columns* are ignored.

**Code Quality:**
*   Incorrect imports: `ttest_ind_from_counts`, `ttest_ind_paired`, `ttest_rel`, `LabelEncoder`, `train_test_split` are imported but not used, or not relevant to the task.
*   Redundant imports from `scipy.stats` (e.g., specific t-tests imported when `ttest_ind` is sufficient for the task).

Overall, the generated code fails to address most of the requirements correctly and completely, making it unsuitable for the given task.",command-r7b-12-2024,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
from sklea...",4,"The generated code attempts to address several parts of the task but falls short on correctness, completeness, and clarity.

**Major Issues:**

1.  **Incorrect Feature Selection and Preprocessing:**
    *   **Feature Names:** The generated code defines `features = ['GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom', 'Generosity', 'Perceptions of corruption']`. The actual column names in the `world-happiness-report-2021.csv` are `'Logged GDP per capita'`, `'Freedom to make life choices'`, and `'Perceptions of corruption'`. Using incorrect column names will lead to `KeyError` when attempting to select `X = encoded_data[features]`. This is a critical error that prevents the code from running successfully.
    *   **OneHotEncoding:** The code attempts to `OneHotEncoder` the *entire* `data` DataFrame, which includes numerical features, country names, and regional indicators. This is almost certainly not the intended behavior and `OneHotEncoder` is generally used only for categorical columns. For the specified task, the features are all numerical, so no OneHotEncoding is needed for `X`. If categorical features like 'Regional indicator' were used, it should be applied only to those columns.
    *   **Encoding before Feature Selection:** The encoding is applied to the *entire* dataframe, and then the features are selected from the *encoded* dataframe. This is problematic because `OneHotEncoder` creates new columns; the original feature names like 'GDP per capita' will not exist in `encoded_data` if `data` contained country names or other strings. This further indicates a misunderstanding of the required preprocessing steps.
    *   **Handling Missing Values:** It uses `data.fillna(data.mean(), inplace=True)`. While `fillna` is a valid approach, filling *all* columns (including non-numeric ones if they existed) with the *mean* is often not appropriate. Specifically, for target and feature columns, `dropna` is a common and often better approach for regression if the missing data percentage is low, as it preserves the integrity of row-wise data. The ground truth specifically drops rows with missing values in the *relevant* columns, which is a more sensible strategy here.

2.  **Incomplete Hyperparameter Tuning:** The hyperparameter tuning section is commented out, indicating it was started but not completed or fully integrated. It's also based on `GradientBoostingRegressor` but doesn't store the `best_model` or use it subsequently for evaluation, cross-validation, or prediction.

3.  **Incomplete Visualizations:** The code has comments like `# Create residual plots and feature importance visualizations # ...`, indicating these requirements were not implemented.

4.  **Incomplete Cross-Validation:** While `cross_val_score` is used, it applies cross-validation to *all* models instead of specifically assessing the stability of the *best* model as required by the prompt. It also uses the original `X, y` (scaled `X` but not the subset that was used for training/testing), which might be slightly inconsistent with the best model's final training.

5.  **Incomplete Prediction for New Data:** The section for predicting new hypothetical countries is commented out and uses placeholder data that doesn't match the actual feature names or scaling applied. It also relies on a commented-out `best_model`.

6.  **Code Quality/Structure:**
    *   No proper error handling for file loading.
    *   The print statements for model evaluation are less clear and consistent than the ground truth's summary table.
    *   The `scoring_metrics` dictionary has a lambda for RMSE, but RMSE is then calculated with the lambda function applied directly to the MSE result instead of using `np.sqrt(mse)`. This is a minor confusion but highlights a lack of practical application.
    *   There's no `StandardScaler` applied to the `y` variable, which is appropriate, but the use of `scaler.fit_transform(X)` implies that `X` is being scaled, which is correct, but then the features in the `new_data` example `new_data_scaled = scaler.transform(new_data)` would need to match the original scaling.

In summary, the code has fundamental errors in data preprocessing (particularly feature selection and one-hot encoding), lacks completion for several major requirements (hyperparameter tuning application, visualizations, new predictions), and exhibits some structural and clarity issues.",command-r7b-12-2024,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA, tSNE
from sklearn.feature_selection import SelectKBest, RecursiveFeatureElimination, chi2
from sklearn.metrics import correlation_matrix, mutual_info_score
from sklearn.ensemble import RandomForestClassifier
import scipy.stats as stats
from sklearn.metrics import confusion_matrix, classific...",2,"The generated code has numerous critical issues that prevent it from correctly and completely addressing the task requirements.

**Primary Reasons for Failure:**

1.  **Fundamental Bugs & Incorrect Usage:**
    *   **Incorrect column names:** The generated code assumes column names like 'Ladder', 'GDP per capita', 'Freedom', 'Low Corruption', 'Region', and 'Happiness Score' which are not present in the `world-happiness-report-2021.csv` dataset. The ground truth correctly uses 'Ladder score', 'Logged GDP per capita', 'Freedom to make life choices', 'Perceptions of corruption', 'Regional indicator', and 'Ladder score' (as the target). This leads to `KeyError` or incorrect results for all engineered features and subsequent steps.
    *   **Polynomial Features calculation:** Instead of using `PolynomialFeatures` from `sklearn.preprocessing`, the generated code attempts to manually square columns, which is insufficient for generating interaction terms and higher-order terms for *all* combinations as implied by ""polynomial features (degree 2)"".
    *   **Dimensionality Reduction:**
        *   `tSNE` is imported as `sklearn.decomposition.tSNE` but it's actually in `sklearn.manifold.TSNE`.
        *   `tSNE` and PCA are applied without handling non-numeric columns, which will cause errors or unexpected behavior.
    *   **Feature Selection:**
        *   `RecursiveFeatureElimination` is imported as `sklearn.feature_selection.RecursiveFeatureElimination` but the correct class name is `RFE`.
        *   `chi2` is used for `SelectKBest` with a regression target (`Ladder score`). `chi2` is for classification/categorical data, while `f_regression` or `mutual_info_regression` should be used for regression.
        *   `SelectKBest` and `RFE` output are assigned back to `df.columns` which overwrites the original data in a nonsensical way. The output should be a DataFrame with the *selected features*.
    *   **Correlation Matrix & Mutual Information:**
        *   `sklearn.metrics.correlation_matrix` does not exist. `pandas.DataFrame.corr()` should be used.
        *   `sklearn.metrics.mutual_info_score` is for discrete variables. `mutual_info_regression` from `sklearn.feature_selection` is needed for continuous regression targets.
    *   **Interaction Features:** The `pd.get_dummies(df[interactions], drop_first=True)` line is fundamentally flawed. `df[interactions]` will likely result in an error or be DataFrame of tuples. It's not designed to create interaction terms between *numeric* columns. It also creates dummy variables, which is incorrect for numeric interactions.
    *   **Random Forest Classifier:** The task specifies feature importance for a regression problem (predicting 'Ladder score'), but `RandomForestClassifier` is used instead of `RandomForestRegressor`. This is a critical mismatch.

2.  **Incomplete/Incorrect Implementation of Requirements:**
    *   **Preprocessing:** No explicit handling for missing values or non-numeric columns other than the implicit errors.
    *   **Feature Scaling:** While the scalers are initialized, they are applied to the entire `df` (which still contains non-numeric columns like 'Country name', 'Regional indicator'), leading to errors or warnings. Scaling should be applied only to numeric features *after* feature engineering and before dimensionality reduction/feature selection.
    *   **Visualization:** PCA and t-SNE visualizations are mentioned but the code has issues that prevent correct execution and the plotting code is minimal. The requested visualizations are not properly generated or displayed due to the aforementioned bugs.
    *   **Feature Importance Visualization:** The plotting code is very basic and lacks customization (like `plt.tight_layout`, sorting) and clarity.

3.  **Code Quality Issues:**
    *   Many imports are incorrect or unnecessary (`confusion_matrix`, `classification_report`, `scipy.stats`, `itertools.combinations` used incorrectly here).
    *   Variable naming is sometimes vague (e.g., `df` is constantly modified in place, making it hard to track state).
    *   Lack of comments and clear structure for complex steps.

In summary, the code demonstrates a superficial understanding of the required techniques without the practical knowledge to implement them correctly or handle common data science pitfalls (like non-numeric columns, correct function usage, appropriate model types, and proper class names). The errors are so pervasive that the code would not run successfully through the feature engineering and selection steps, let alone produce meaningful results.",command-r7b-12-2024,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# 1. Load the dataset, parse the 'date' column, and filter for 'location' == 'USA'
data = pd.read_csv('synthetic_covid19_data.csv')
data['date'] = pd.to_datetime(data['date'])
usa_data = data[data['location'] == 'USA']

# 2. Set the 'date' column as the...",6,"The generated code addresses most of the requirements but has several issues that reduce its correctness and completeness.

**Correctness Issues:**
*   **Stationarity Check Logic:** The `if adf_result[1] < 0.05` condition is incorrect for checking non-stationarity. A p-value *less than* 0.05 indicates stationarity. The code should apply differencing if `adf_result[1] > 0.05`. This is a critical logical error.
*   **Decomposition Period:** The decomposition period is set to `7`. While the data is weekly, `seasonal_decompose` typically requires `period` to be the number of data points in one seasonal cycle. For COVID-19 cases, a weekly pattern might be less pronounced, and the common seasonal period for weekly data when considering yearly seasonality is 52 (weeks in a year). Using `period=7` for weekly data implies a daily seasonality, which doesn't make sense here. This will lead to a nonsensical decomposition.
*   **Plotting ACF/PACF:**
    *   The `plot_acf` and `plot_pacf` functions are called with `weekly_usa_data['new_cases']` (the original series) regardless of whether differencing was applied or not. The task explicitly states: ""Plot the ACF and PACF of the (potentially differenced) stationary time series.""
    *   The arguments `cols=3` and `tit impostos=""ACF""`/`tit =""PACF""` are incorrect and would cause errors. These are not valid arguments for `plot_acf` or `plot_pacf`. It seems like a remnant of some other plotting library or a misunderstanding of `statsmodels` plotting arguments. `ax=plt.gca()` is also redundant if figure/axes are not explicitly created.

**Completeness Issues:**
*   **Decomposition Plotting:** The decomposition plot is created manually by plotting `weekly_usa_data`, `decomposition.trend`, etc. directly. While this produces a plot, `statsmodels.tsa.seasonal.seasonal_decompose` returns a `DecomposeResult` object which has a `.plot()` method. This method automatically generates the desired four subplots (observed, trend, seasonal, residual), which is cleaner and more aligned with typical usage.
*   **ADF Test Output:** The output for the ADF test only prints the statistic and p-value, but not the critical values, which are important for a comprehensive interpretation as demonstrated in the ground truth.
*   **Handling Empty/Insufficient Data:** The code lacks robust checks for cases where the filtered data might be empty, or if there's insufficient data after resampling for decomposition or ADF tests. The ground truth includes these crucial checks.
*   **Warnings:** The ground truth includes `warnings` suppression for cleaner output, which is good practice for benchmark code.

**Minor Code Quality Issues:**
*   `import numpy as np` is present but `np` is not used.
*   The manual plotting of decomposition is less idiomatic than using `decomposition.plot()`.
*   The `reset_index()` and `set_index()` combination for `weekly_usa_data` is unnecessary; `resample().sum()` usually returns a Series with a DateTimeIndex directly if the original series has one.
*   Lack of comments for certain steps and overall structure compared to the ground truth's function-based approach for reusability.",command-r7b-12-2024,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import f_oneway
import numpy as np

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Categorize pH values
data['pH_group'] = pd.cut(
    data['pH'],
    bins=[0, 3.2, 3.4, np.inf],
    labels=['Low pH', 'Medium pH', 'High pH'],
    include_lowest=True
)

# Calculate average quality for each pH group
avg_quality_by_pH = data.groupby('pH_group')['quality'].mean()
print(""Average Quality by pH Group:"")
prin...",6,"The generated code correctly performs the first two steps: categorizing pH values and calculating the average quality per group. It also correctly implements the ANOVA test. However, the visualization part is significantly flawed. The `yerr` argument for `plt.errorbar` is incorrectly applied; `avg_quality_by_pH.sem()` returns a single scalar value (SEM of the entire Series) instead of the SEM for each group, which is necessary for individual error bars. This leads to incorrect confidence intervals. Furthermore, the task specifically asked for 95% confidence intervals, which involves calculating `t.interval` using `scipy.stats.sem` for each group, not just a simple SEM across the entire series. There's also no explicit calculation or display of these confidence intervals in the visualization. The `matplotlib.pyplot` import is not explicitly aliased as `plt` in the `f_oneway` import line, but this is a minor stylistic point. The labels for `pH_group` in `pd.cut` are missing the detailed descriptions requested in the prompt (e.g., 'Low pH (< 3.2)'). It also saves the figure instead of showing it, which is not what was requested.",command-r7b-12-2024,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# 1) Load the dataset and parse 'date'
data = pd.read_csv('synthetic_covid19_dat...",5,"The generated code attempts to address all requirements but contains several critical logical and implementation errors that prevent it from correctly solving the task.

**Major Issues:**

1.  **Incorrect Feature Handling for Scaling and Splitting:**
    *   **Scaling:** The code scales features *before* the train-test split. This causes data leakage from the test set into the training process, leading to an over-optimistic evaluation of the model. Scaling should always happen *after* the split, fitting the scaler only on the training data, and then transforming both training and test data.
    *   **`X` definition after one-hot encoding:** The line `X = data[features + ['0']*len(features)]` is fundamentally flawed.
        *   `data[features]` only includes the numerical features, *not* the one-hot encoded continent columns.
        *   `['0'] * len(features)` creates a list of strings ""0"" repeatedly, which are not valid column names. This will likely cause a `KeyError` or result in an empty list of columns, leading to incorrect `X` definitions.
        *   The one-hot encoded columns (e.g., `continent_Africa`, `continent_Europe`) are not correctly captured in `X` for model training. The code combines `data` with `continent_df` but then doesn't correctly define `X` to include these new columns.

2.  **Missing Value Imputation for `continent`:** The prompt explicitly states to ""Handle missing values in selected features and target by imputing with the median."" The `continent` column was intended to be part of the selected features for its categorical encoding. While the ground truth handles imputation for numerical features *before* encoding, the generated code doesn't explicitly handle missing values in `continent` before one-hot encoding. `OneHotEncoder` by default treats `NaN` as a category if present, which might not be the desired behavior if the intention was to impute or drop rows with missing continents. However, the ground truth correctly avoids imputing `continent` with median as it's categorical. The generated code applies `SimpleImputer` only to numerical features, which is correct for those, but it should also properly manage `continent` (e.g., ensure no NaNs or handle them appropriately before encoding). In this specific dataset, `continent` likely has no missing values, but a robust solution would consider this.

3.  **One-Hot Encoding `drop_first=True`:** The ground truth uses `drop_first=True` with `pd.get_dummies` which is a good practice to prevent multicollinearity in regression models by dropping one of the one-hot encoded categories. The `OneHotEncoder` in the generated code doesn't use this equivalent (`drop='first'`), which can lead to issues for linear models although it's less critical for tree-based models.

4.  **Redundant Imports:** `numpy` is imported but not explicitly used. `SimpleImputer` and `OneHotEncoder` are imported, but their usage for `continent` and the general flow of preprocessing is problematic as described above.

**Correctness:** The code will likely fail or produce incorrect results due to the issues with scaling order and, more critically, the incorrect construction of the `X` (features) dataframe for training and testing. Even if it runs, the results would be unreliable due to data leakage.

**Code Quality:** Imports are mostly fine, but the logic for feature selection and preprocessing steps (especially after one-hot encoding and before splitting) is very messy and incorrect. The variable `data` is heavily modified in place, which can sometimes make debugging harder.

**Completeness:** It attempts to address all steps but fails significantly on the implementation of scaling and feature preparation (steps 4, 5, 6).",command-r7b-12-2024,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
from scipy.stats import pearsonr, ttest_ind, f_oneway
from sklearn.impute import SimpleImputer

# 1) Load and preprocess the dataset
data = pd.read_csv('synthetic_covid19_data.csv')

# Handle missing values
imputer = SimpleImputer(strategy='median')
data[['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']] = data[
    ['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']
].ap...",3,"The generated code has significant errors in implementing the statistical tests and data processing, leading to incorrect results.

**Specific Issues:**

1.  **Missing Value Handling (Imputation):**
    *   The `SimpleImputer` is used incorrectly. `apply(imputer.fit_transform)` on a DataFrame with multiple columns will transform each Series independently, which is not how `SimpleImputer` is designed to be used for multiple columns simultaneously. It should be applied to the relevant columns directly, e.g., `data[cols] = imputer.fit_transform(data[cols])`.
    *   More critically, `SimpleImputer` returns a NumPy array. Assigning it back to `data[['col1', 'col2']]` without converting it back to a DataFrame or Series will likely lead to dimension errors or incorrect data types.

2.  **ANOVA Test (Major Error):**
    *   The ANOVA is performed on `data.groupby('continent')['reproduction_rate'].mean()`. This calculates the *mean* reproduction rate for each continent and then tries to run ANOVA on these *means*, as if each mean is a separate sample. This is fundamentally incorrect. ANOVA requires the full set of observations for each group, not just their means. The `f_oneway` function expects multiple arrays, where each array contains the data for one of the groups.
    *   The task specifically asks for ANOVA for 'Asia', 'Europe', 'North America' only. The generated code attempts to do it for *all* continents present in the `groupby` result, which is also a deviation from the requirement.

3.  **Derived Category `high_death_rate_country` (Major Errors):**
    *   **Incorrect Ratio Calculation:** `data['total_deaths / population']` is used in the `apply` function, but this column was never created. It should be `row['total_deaths'] / row['population']` directly.
    *   **Incorrect `quantile` application:** The quantile is calculated on `data['total_deaths / population']` which doesn't exist. Even if it did, the quantile should be calculated on the *latest* entry for each country, not across all rows in the raw `data` DataFrame. The code fails to get the latest entry per country as specified in the task.
    *   This section will cause a `KeyError` due to the non-existent column, or `TypeError` if `total_deaths / population` was attempted directly in the lambda without a separate column.

4.  **T-test:**
    *   Because the `high_death_rate_country` derivation is flawed, the t-test performed will be based on an incorrectly defined binary category, rendering its results meaningless.
    *   The task also specifies ""Use an appropriate t-test."" The ground truth uses `equal_var=False` (Welch's t-test), which is generally more robust, but the generated code uses the default (Student's t-test assuming equal variances). While not explicitly wrong, it doesn't demonstrate consideration for appropriate test selection.

5.  **Output and Conclusion:**
    *   The conclusion for the Pearson correlation test states ""significant positive correlation"". The correlation coefficient itself should be printed to confirm if it's positive or negative, and the conclusion should just state ""significant correlation"" if the sign isn't an explicit part of the hypothesis.
    *   The print statements are concise but lack some of the detail and structure of the requested output (e.g., explicit null hypothesis statements before each test).

**Summary of Critical Failures:**

*   **ANOVA implementation error:** The most critical error is the complete misunderstanding of how to apply ANOVA, leading to fundamentally incorrect statistical results.
*   **Death Rate Category derivation error:** Failure to correctly calculate the ratio and apply the 75th percentile based on the *latest* data per country.
*   **Missing value imputation:** While `SimpleImputer` is chosen, its application is incorrect, potentially leading to data type or dimension issues.

Given these fundamental errors in applying the core statistical methods and data processing requirements, the code is far from correctly solving the task.",command-r7b-12-2024,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Step 1: Load the dataset using pandas
df = pd.read_csv('clash_royale_cards.csv')

# Step 2: Select numerical columns
numerical_columns = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
df_numeric = df[numerical_columns]

# Step 3: Calculate pairwise correlation matrix
correlation_matrix = df_numeric.corr()

# Step 4: Visualize correlation matrix using seaborn heatmap
plt.figure(figsize=(...",10,"The generated code is highly correct and complete, addressing all requirements of the task. It properly loads the CSV, selects the specified numerical columns, calculates the pairwise correlation matrix, and visualizes it as a heatmap using seaborn. The imports are correct, and the structure is clean and easy to understand. It perfectly matches the core logic of the ground truth required for the task. The only minor difference is the lack of error handling and function encapsulation present in the ground truth, but these were not explicitly requested as part of the core task requirements.",command-r7b-12-2024,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
clash_royale_cards = pd.read_csv(""clash_royale_cards.csv"")

# Group by 'rarity' and calculate averages
average_values = (
    clash_royale_cards.groupby(""rarity"")[[""elixirCost"", ""winRate"", ""usage""]]
    .mean()
    .dropna()
)

# Display the results
print(average_values)
```",8,"The generated code correctly loads the dataset, groups it by 'rarity', and calculates the average 'elixirCost' and 'Usage'. However, there's a slight discrepancy in the column name for 'Win Rate'; the ground truth uses 'Win Rate' (with a space), while the generated code uses 'winRate' (camelCase). This could lead to a KeyError if the actual column name in the CSV is 'Win Rate'. Additionally, the `.dropna()` method was unnecessarily added. While it might not cause issues if there are no NaN values in the averages, it's not a required step for this task and isn't part of the ground truth's approach. The main strength is that it achieves most of the task's requirements accurately and efficiently.",command-r7b-12-2024,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# 1) Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# 2) Create a new binary target column
data['is_high_win_rate'] = (data['Win Rate'] > 55).astype(int)

# 3) Select the features
features = ['elixirCost', 'Usage', 'rarity']
X = data[features]

# 4) One-hot encoding for the categorical 'rarity' column
encoder = OneHotEncoder(sparse=False)
rarity_enco...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
- Correctly loads the data.
- Accurately creates the `is_high_win_rate` target column with the specified condition.
- Selects the correct features.
- Applies one-hot encoding to the `rarity` column using `OneHotEncoder`, which is a valid approach (even if `pd.get_dummies` is simpler for this specific case).
- Correctly splits the data into training and testing sets with the specified 80/20 ratio and `random_state`.
- Includes all necessary imports.

**Areas for minor improvement:**
- The `OneHotEncoder` usage, while correct, is slightly more verbose than `pd.get_dummies` and can lead to potential index misalignment issues if not handled carefully (though it is handled here by `pd.concat`). Specifically, `pd.concat` without `reset_index()` on `X` and `pd.DataFrame(rarity_encoded)` assumes that the indices of `X` are contiguous 0 to N-1, which is true right after `X = data[features]` but might not be true if `data` had rows dropped or reordered. It would be safer to `reset_index()` after `X = data[features]` and before `pd.concat` to ensure index alignment, or to apply the encoding directly to the relevant part of the `data` DataFrame, similar to how `pd.get_dummies` operates.
- The `X` DataFrame is modified in place (`X = data[features]`, then `X = pd.concat(...)`) which is generally fine, but for clarity and to prevent unexpected side effects, sometimes copying the DataFrame explicitly at the start of feature engineering (`X = data[features].copy()`) can be beneficial. However, in this specific code, it doesn't cause a problem.",command-r7b-12-2024,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 1) Load the dataset
data = pd.read_csv(""iris.csv"")

# 2) Define features X
X = data[[""SepalLengthCm"", ""SepalWidthCm"", ""PetalLengthCm"", ""PetalWidthCm""]]

# 3) Define the target y
y = data[""Species""]

# 4) Encode the categorical Species target variable into numerical labels
...",9,"The generated code correctly addresses almost all requirements of the task. It loads the data, defines features and target as specified, encodes the target variable, splits the data with the correct test size and random state, trains a KNN classifier with the specified number of neighbors, makes predictions, and reports the accuracy.

One minor omission compared to the ground truth (and a good practice for classification tasks with imbalanced classes) is the lack of `stratify=y_encoded` in the `train_test_split` function. While not explicitly asked for in the prompt's requirements, it's a best practice for reproducing robust results in classification, especially if the dataset had unequal class distributions. However, for a simple benchmark on iris.csv, its absence does not critically impact correctness. The print statement for accuracy also uses `:.2f` instead of `:.4f` as in the ground truth, but this is a formatting preference and doesn't affect correctness. The file loading also lacks the `try-except` block for `FileNotFoundError`, which is a good robustness measure but not strictly required by the prompt's functional task.

Overall, it's a very good solution that successfully accomplishes the core task.",command-r7b-12-2024,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Step 1: Load the dataset
data = pd.read_csv('iris.csv')

# Step 2: Filter the data for two groups
versicolor_data = data[data['Species'] == 'Iris-versicolor']
virginica_data = data[data['Species'] == 'Iris-virginica']

# Step 3: Perform an independent two-sample t-test
t_statistic, p_value = ttest_ind(versicolor_data['PetalLengthCm'], virginica_data['PetalLengthCm'])

# Step 4: State hypotheses
null_hypothesis = ""The mean petal l...",7,"The generated code correctly performs the core task of loading data, filtering, and executing a t-test. It also correctly interprets the p-value against the given alpha.

However, it falls short in terms of completeness and detail compared to the ground truth:

1.  **Hypotheses Clarity:** While it states the null and alternative hypotheses, they are very generic (`""The mean petal length is the same for both species.""`) and lack the specific detail present in the ground truth regarding the species themselves (`_versicolor = _virginica`). It also doesn't explicitly print the hypotheses before the test results as requested.
2.  **Robustness (File Not Found):** It lacks any `try-except` block for `FileNotFoundError` when loading the CSV, which is a crucial robustness consideration for real-world scripts.
3.  **Output Formatting:** The output relating to the t-statistic and p-value is quite basic (`t-statistic: 0.12345`), whereas the ground truth formats it to a specific decimal place (`.4f`) and includes descriptive text preceding and following the values, making the output much clearer and more readable.
4.  **Error Handling (Empty Groups):** The generated code doesn't include checks for whether the filtered groups might be empty, which could lead to errors or misleading test results if the species names are misspelled or not present in the dataset.
5.  **Import Specificity:** Importing `ttest_ind` directly from `scipy.stats` is fine, but the prompt requested `from scipy import stats` and then `stats.ttest_ind`. While functionally equivalent, adhering to the requested import style often aligns with best practices from the specific problem context.

To improve, the code should:
*   Make the hypothesis statements more explicit, including the species names.
*   Print the hypotheses before the test results.
*   Add error handling for `FileNotFoundError`.
*   Improve output formatting for the t-statistic and p-value.
*   Consider adding checks for empty filtered groups.",command-r7b-12-2024,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('iris.csv')

# Iterate over numerical features and plot box plots grouped by species
numerical_features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

for feature in numerical_features:
    plt.figure()
    sns.boxplot(x='Species', y=feature, data=df)
    plt.title(f'Distribution of {feature} by Species')
    plt.show()
```",9,"The generated code successfully loads the dataset, iterates through the required numerical features, and generates individual box plots grouped by the 'Species' column, each with an appropriate title. This perfectly addresses the core requirements of the task.

**Strengths:**
*   **Correctness:** The code accurately performs the requested data loading and plotting operations.
*   **Completeness:** All specified requirements (load dataset, 4 box plots, grouped by Species, titles) are met.
*   **Code Quality:** Imports are correct, variable names are clear, and the loop structure is efficient for generating multiple plots. `seaborn` is correctly used for generating informative box plots.

**Minor Improvement (not critical enough to reduce score significantly):**
*   **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions when loading the CSV, which makes the code more robust. The generated code assumes the file will always be present, which is typical for benchmark evaluation, but in a production environment, error handling is crucial.
*   **Plot Customization:** The ground truth adds minor aesthetic improvements like `figsize`, `xlabel`, `ylabel`, `grid`, and `tight_layout`. While not explicitly requested, these often enhance readability and presentation. The generated code's plots are functional but less polished.

Overall, it's an excellent solution that directly addresses the prompt's requirements with clear and concise code.",command-r7b-12-2024,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score

# 1) Load the dataset
data = pd.read_csv('heart.csv')

# 2) Select the features and target variable
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']
X = data[features]
y = data['HeartDisease']

# 3) Preprocess categorical features using one-h...",6,"The generated code correctly loads the data, selects features, splits data, trains a Logistic Regression model, and reports accuracy. However, there are significant issues with the one-hot encoding implementation which can lead to incorrect results or errors.

**Issues with One-Hot Encoding:**
1.  **Incorrect Application of `fit_transform`:** The `OneHotEncoder` is called separately for `Sex` and `ChestPainType` using `fit_transform` on `X[['Sex']]` and `X[['ChestPainType']]` respectively. This means the encoder is fitted independently to each column, which is generally not an issue for one-hot encoding if the categories are global, but the way it's then integrated back into the DataFrame is problematic.
2.  **Incorrect Integration of Encoded Data:**
    *   `X.drop(['Sex', 'ChestPainType'], axis=1, inplace=True)` removes the original columns. This is correct.
    *   `X.loc[:, X.columns.intersection(['Sex'])] = encoded_Sex` and `X.loc[:, X.columns.intersection(['ChestPainType'])] = encoded_ChestPainType` are completely wrong ways to add the encoded columns back. `X.columns.intersection(['Sex'])` will be an empty index after dropping 'Sex'. This will either fail or lead to non-assignment. The `encoded_Sex` and `encoded_ChestPainType` are numpy arrays with a different number of columns than the original single-column `Sex` and `ChestPainType`. They need to be converted to DataFrames with appropriate column names and then concatenated with `X`.
    *   More correctly, the new one-hot encoded columns should be appended to the DataFrame, likely using `pd.concat` after creating DataFrames from the encoded arrays with proper column names, or more robustly, by using `pd.get_dummies` which is simpler for just one-hot encoding, or even better, using a `ColumnTransformer` (as in the ground truth) within a `Pipeline`.
3.  **Data Leakage/Consistency Issues in Encoding:** Applying `fit_transform` on the entire `X` *before* `train_test_split` means the encoder learns categories from both training and testing data. While `OneHotEncoder` itself won't cause data leakage in terms of target information, it's good practice to apply preprocessing steps *after* the split or within a Pipeline to ensure that the categories learned for encoding are only from the training data, and the same transformation is applied to the test set using the *fitted* encoder. The current approach implicitly uses the entire dataset to ""learn"" the categories. The `ColumnTransformer` + `Pipeline` approach in the ground truth is the robust and recommended way to handle this.
4.  **`sparse=False`:** While `sparse=False` is fine, it's not strictly necessary for such a small dataset.

**Other Minor Issues:**

*   The print statement for accuracy is `accuracy:.2f` instead of `accuracy:.4f` as in the ground truth, but this is a minor formatting preference.
*   The `max_iter` for `LogisticRegression` was not increased, which can sometimes lead to convergence warnings for larger datasets, though it might be fine here.

The fundamental flaw in how the one-hot encoding is handled makes the current solution fragile and potentially incorrect in its data preparation step.",command-r7b-12-2024,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
data = pd.read_csv('heart.csv')

# 2) Create two groups based on the 'Sex' column
male_group = data[data['Sex'] == 'M']
female_group = data[data['Sex'] == 'F']

# 3) Perform an independent two-sample t-test
t_stat, p_value = stats.ttest_ind(male_group['Cholesterol'], female_group['Cholesterol'])

# 4) Null and alternative hypotheses
# Null hypothesis (H0): The mean cholesterol levels of male and female patients are equa...",8,"The generated code correctly performs the requested task. It loads the data, separates it into male and female groups, and performs an independent two-sample t-test on Cholesterol levels. It clearly states the hypotheses and prints the t-statistic and p-value. The interpretation of the p-value against the alpha of 0.05 is also correct.

Areas for improvement:
1.  **Robustness**: The generated code does not handle `FileNotFoundError` or missing columns, which could lead to crashes if the `heart.csv` file is not present or has different column names.
2.  **`equal_var` parameter**: The `ttest_ind` function by default assumes equal variance (`equal_var=True`). However, it's generally safer and more robust to use Welch's t-test (`equal_var=False`) unless there's a strong reason to assume equal variances.
3.  **Clarity**: While hypotheses are stated in comments, integrating them into print statements like the ground truth can be clearer for the user running the script.
4.  **Floating point precision**: The p-value and t-statistic are printed with `.2f` precision, which might truncate useful information, especially for very small or very large p-values. Using `.4f` or similar would be better.
5.  **Function encapsulation**: Wrapping the logic in a function would make the code reusable and testable, similar to the ground truth.",command-r7b-12-2024,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Identify potential outliers using IQR method
Q1 = data['volatile acidity'].quantile(0.25)
Q3 = data['volatile acidity'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers_volatile_acidity = (data['volatile acidity'] < lower_bound) | (data['volatile acidity'] > upper_bound)
outlier_count_volatile_acidity = outliers_volatile_acidity.sum()

...",7,"The generated code correctly performs the core tasks: identifying outliers, reporting their counts, winsorizing, and comparing statistics. However, there are a few areas for improvement.

**Positives:**
*   Correctly loads the dataset.
*   Correctly identifies outliers using the IQR method for both columns.
*   Reports the number of outliers accurately.
*   Uses `clip()` for winsorization, which is a concise and effective method.
*   Compares mean and standard deviation before and after treatment.
*   Properly handles imports.

**Areas for Improvement:**
*   **Repetitive Outlier Calculation:** The code repeats the IQR, bounds, and outlier calculation logic for `volatile acidity` and `residual sugar`. This could be refactored into a loop or a function to be more DRY (Don't Repeat Yourself), as shown in the ground truth. This is crucial for scalability if more columns were to be treated.
*   **Winsorization Logic Error:** The line `data_winsorized['residual sugar'] = data_winsorized['residual sugar'].clip(lower=lower_bound, upper=upper_bound)` for `residual sugar` uses the `lower_bound` and `upper_bound` calculated for `volatile acidity` (due to the sequential, non-looping calculation). This is a critical error as `residual sugar` will be winsorized based on the wrong bounds. The ground truth correctly re-calculates bounds for each column.
*   **Output Readability:** The output for the comparison of mean and standard deviation is a direct print of two numbers on one line, which is less readable than the formatted output in the ground truth, making it harder for a user to quickly understand which number is the mean and which is the standard deviation.
*   **Assumed File Existence:** The code assumes the CSV file exists. While the prompt implied this, robust production-ready code (as shown in the ground truth) would include `try-except` for `FileNotFoundError`.
*   **Clarity on `lower_bound`/`upper_bound` Scope:** Because the bounds are recalculated for each column in the single-pass code, the last assigned `lower_bound` and `upper_bound` variables in the code actually correspond to `residual sugar` *only*. This means that when `clip()` is called for `volatile acidity`, it uses the `lower_bound` and `upper_bound` *from `residual sugar`*, leading to incorrect winsorization for `volatile acidity` as well. This is a significant bug stemming from the lack of a loop or explicit variable naming for each column's bounds.

The critical bug in applying the wrong bounds during winsorization significantly impacts correctness, making the treatment ineffective for at least one of the columns, and potentially both depending on the order and variable reassignments.",command-r7b-12-2024,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('heart.csv')

# Filter columns
chest_pain = df['ChestPainType']
heart_disease = df['HeartDisease']

# Group and count
grouped = chest_pain.value_counts(normalize=True) * 100  # Normalize to percentage
result = grouped.stack(name='Percentage')
result.name = 'Count'

# Create the grouped bar chart
result.plot(kind='bar', stacked=True, figsize=(12, 6), legend=False)

# Add a title, x-axis label, and y...",3,"The generated code fails significantly in meeting the core requirements of the task.

1.  **Incorrect Grouping and Plotting Logic:**
    *   The task explicitly asks for a *grouped bar chart showing the count of HeartDisease (0 and 1) for each category in the ChestPainType column*. This means two bars per ChestPainType, one for HeartDisease=0 count and one for HeartDisease=1 count.
    *   The generated code uses `chest_pain.value_counts(normalize=True)` which only counts the `ChestPainType` categories themselves, not their relation to `HeartDisease`. It then attempts `grouped.stack()` which is misapplied here, and finally plots a single stacked bar chart of *percentages* of `ChestPainType`, not counts of `HeartDisease` within `ChestPainType`.
    *   The chart generated does *not* show `HeartDisease` counts (0 and 1) for each `ChestPainType`. It shows only the distribution of `ChestPainType` as percentages.

2.  **Missing `HeartDisease` Dimension:** The most critical failure is that the generated chart completely omits the `HeartDisease` dimension as requested. The prompt specifically asks for bars representing `HeartDisease`=0 and `HeartDisease`=1 *for each* `ChestPainType`.

3.  **Incorrect Y-axis:** The Y-axis is labeled ""Percentage"" but the task asks for counts.

4.  **Incomplete Imports/No Seaborn:** While `matplotlib` can achieve the plot, `seaborn` is often preferred for grouped bar charts (like `sns.countplot`), and the prompt mentioned using either. The provided solution only uses `matplotlib` but doesn't correctly implement the grouped bar logic with it.

5.  **No Legend for HeartDisease Status:** The legend added is `plt.legend(title='Heart Disease Status')` but there's no `hue` or `color` mapping to `HeartDisease` in the plot itself, so this legend is meaningless and empty or misleading.

In summary, the code does not correctly interpret or implement the core visualization requirement of showing the *count of HeartDisease (0 and 1) for each ChestPainType*.",command-r7b-12-2024,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Clean the 'in_shazam_charts' column
df['in_shazam_charts'] = df['in_shazam_charts'].str.replace(',', '').astype(float, errors='coerce')
df.isnan(df['in_shazam_charts']).sum()  # Count the NaNs

# Group the data by 'key' and 'mode'
grouped_data = df.groupby(['key', 'mode'])

# Calculate and display the average 'danceability_%' and 'valence_%' for each group
for (key, mode), group in grouped_data:
    avg_dan...",6,"The generated code correctly loads the dataset and sets up basic operations. However, it has several issues that prevent it from fully meeting the requirements and achieving a higher score.

**Correctness:**
- The cleaning of `in_shazam_charts` using `.astype(float, errors='coerce')` is correct in principle for conversion to float.
- The `df.isnan(df['in_shazam_charts']).sum()` line is incorrect for counting NaNs. `df.isnull().sum()` is the correct pandas method. Even if it were correct, it does not print or report the count as required.
- The iteration over groups to print averages is functional but less idiomatic and efficient than using `.agg()` or direct `.mean()` on the grouped object in pandas, as shown in the ground truth. It also formats the output differently than the ground truth.

**Code Quality:**
- Imports are proper.
- The error handling for `FileNotFoundError` as present in the ground truth is missing. While not explicitly asked for, it's good practice for loading files.
- The method for counting NaNs is syntactically incorrect for pandas.
- The verbose iteration for calculating and displaying means is less efficient and harder to read/maintain than vectorized pandas operations.

**Completeness:**
- It loads the CSV.
- It attempts to clean the `in_shazam_charts` column and convert it, but the NaN reporting is incomplete and incorrect. The `astype(float, errors='coerce')` implicitly handles the error coercion but the reporting of the count is broken.
- It groups the data.
- It calculates the averages, but the output format is different and not as concise as the ground truth. The ground truth explicitly states ""display the average,"" which is generally taken to mean printing the resulting dataframe. The generated code prints each key/mode combination individually.

**Specific Issues to Causal the Score:**
1. **Incorrect NaN Counting/Reporting:** The line `df.isnan(df['in_shazam_charts']).sum()` is invalid pandas syntax and does not report the NaN count as required by the prompt. This is a critical failure in meeting a specific requirement.
2. **Efficiency/Readability for Grouped Averages:** While the loop approach does produce the correct averages, it's inefficient and not ideal for larger datasets compared to direct aggregation (`df.groupby([...]).mean()`), which is a professional standard in pandas.
3. **Implicit vs. Explicit Cleaning:** The `astype(float, errors='coerce')` directly on the string-replaced column works, but the ground truth's two-step approach (`astype(str)` then `pd.to_numeric`) can sometimes be safer, especially if the original column has mixed types that aren't strings and `.str.replace` would fail directly. However, the current code works for this particular common scenario.",command-r7b-12-2024,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# 1) Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Ensure 'streams' is numeric, coerce errors to NaN, and drop rows with NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams'] + [col for col in df.columns if col != 'streams'])

# 2) Select features (X) ...",7,"The generated code correctly addresses most of the requirements. It loads the dataset, handles the `streams` column coercing to numeric and dropping NaNs, selects the correct features and target, splits the data, trains a Linear Regression model, and reports R2 and MSE.

However, there is a subtle but significant issue in how NaN handling is implemented:
`df = df.dropna(subset=['streams'] + [col for col in df.columns if col != 'streams'])`
This line drops rows where *any* column is NaN, not just the specified predictor columns (`in_spotify_playlists` and `energy_%`) and the target (`streams`). The task specifically stated ""drop rows with NaN in `streams` or predictor columns for this task"". The ground truth correctly identifies and drops NaNs only from the `streams`, `in_spotify_playlists`, and `energy_%` columns. The generated code's approach to dropping NaNs could inadvertently remove more data than necessary if other irrelevant columns contain NaNs, potentially leading to different model outcomes than intended by the prompt.

Additionally, the generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes and is good practice. The ground truth also provides more informative print statements which are helpful for debugging and understanding the data pipeline.",command-r7b-12-2024,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd
from datetime import datetime

# 1) Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 2) Combine 'released_year', 'released_month', and 'released_day' into a single datetime column named 'release_date'
df['release_date'] = pd.to_datetime(df[['released_year', 'released_month', 'released_day']], errors='coerce')

# 3) Create a new feature named 'days_since_release'
df.loc[:, 'days_since_release'] = df['release_date'].dt.apply(lambda x: (datetime(2023, 12, 31) -...",7,"The generated code correctly addresses most of the requirements. It loads the dataset, converts the date columns into a single `release_date` column, and calculates `days_since_release`.

**Strengths:**
*   Correctly uses `pd.read_csv` to load the dataset.
*   The use of `errors='coerce'` in `pd.to_datetime` for date conversion is excellent for handling invalid dates.
*   Calculates `days_since_release` correctly.
*   Displays the requested columns for the first 5 tracks.

**Areas for Improvement:**
*   **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` during CSV loading, which is good practice for robust code. The generated code lacks this.
*   **Robustness of Date Conversion (Submitting a DataFrame to `to_datetime`):** While `pd.to_datetime(df[['released_year', 'released_month', 'released_day']], errors='coerce')` works in many Pandas versions for creating a datetime from multiple columns, it's less explicit and potentially less robust across very old or very new Pandas versions compared to first synthesizing a YYYY-MM-DD string and then converting. The string approach offers more control over formatting errors. However, for this specific task, it functionally works.
*   **Efficiency of `days_since_release` calculation:** Using `.apply(lambda x: ...)` for `days_since_release` calculation is generally less efficient than vectorized operations in Pandas. The subtraction `fixed_date - df['release_date']` directly yields a Timedelta Series, and then `.dt.days` extracts the days, which is a much more performant vectorized approach.
*   **Data Type for `days_since_release`:** The code converts `days_since_release` to `int` after calculation. If `release_date` was `NaT`, the `dt.apply` logic correctly assigns `None`. However, `astype(int)` will raise an error if there are `None` (or `NaN`) values, as standard `int` does not support `NaN`. The ground truth uses `astype('Int64')` (Pandas nullable integer), which is the correct way to handle integer columns that might contain missing values. This is a critical correctness issue in the generated code given the `NaT` handling requirement.

To achieve a higher score, the generated code should incorporate better error handling for file loading, use more vectorized Pandas operations for date calculations, and correctly handle nullable integers for the `days_since_release` column.",command-r7b-12-2024,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""youtube_trendings.csv"")

# Clean the 'Video views' and 'Likes' columns
df['Video views'] = df['Video views'].str.replace(',', '').astype(float)
df['Likes'] = df['Likes'].str.replace(',', '').astype(float)
df[['Video views', 'Likes']] = df[['Video views', 'Likes']].fillna(0.0)

# Fill missing values in the 'Category' column with 'Unknown'
df['Category'] = df['Category'].fillna('Unknown')

# Group by 'Category' and calculate sums
...",7,"The generated code correctly performs most of the required operations: loading the data, removing commas, filling NaNs in 'Video views' and 'Likes' with 0, filling missing 'Category' with 'Unknown', and grouping to sum views and likes.

However, there's a crucial issue with the `astype(float)` conversion. The task explicitly states: ""if conversion fails for any value, treat that value as NaN, then fill all resulting NaNs in these columns with 0."" The `astype(float)` method will raise an error if it encounters a non-numeric string after comma removal (e.g., 'N/A' or 'xyz'), thus failing the script. The correct way to handle this is `pd.to_numeric(..., errors='coerce')`, which is used in the ground truth and gracefully converts unparseable values to NaN. The generated code implicitly assumes that after comma removal, all values will be perfectly convertible to float, which is not robust.

The ground truth also includes robust error handling for `FileNotFoundError` and checks for column existence which makes it more robust in a real-world scenario, though these specific checks weren't explicitly asked for in the task requirements beyond ""assume the CSV file is in the current directory"".

**Strong points:**
- Concise and clear implementation for many steps.
- Correct use of `fillna` and `groupby`.

**Areas for improvement:**
- The key missing part is the robust numeric conversion using `errors='coerce'` as specified in the task for handling conversion failures.
- The output format for display is `reset_index()` which adds 'Category' as a column, whereas the ground truth solution directly prints the grouped DataFrame which has 'Category' as the index. While both display the required information, the ground truth's display is more aligned with the direct output of a `groupby().sum()`. The prompt only says ""display"" so `reset_index()` isn't strictly incorrect, but it's a minor difference.",command-r7b-12-2024,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats

# Step 1: Load the dataset
df = pd.read_csv(""youtube_trendings.csv"")

# Step 2: Clean and prepare the 'Likes' column
df[""Likes""] = df[""Likes""].str.replace("","", """").astype(float)
df[""Likes""] = df[""Likes""].fillna(0.0)

# Step 3: Fill missing values in 'Category' with 'Unknown'
df[""Category""] = df[""Category""].fillna(""Unknown"")

# Step 4: Create the two groups
music_likes = df[df[""Category""] == ""Music""][""Likes""]
sports_likes = df[df[""Category""] ...",7,"The generated code correctly performs most of the required steps.

**Strong Points:**
- Correctly loads the dataset.
- Cleans the `Likes` column by removing commas and filling NaNs with 0 (though less robustly than the ground truth).
- Fills missing `Category` values.
- Correctly creates the two groups for the t-test.
- Performs the t-test and prints the t-statistic and p-value.
- States the hypotheses and interprets the p-value correctly based on alpha.

**Areas for Improvement (leading to a 7/10 score):**

1.  **Robustness of 'Likes' Cleaning:**
    *   The `df[""Likes""] = df[""Likes""].str.replace("","", """").astype(float)` line is problematic because it assumes `astype(float)` will always succeed after removing commas. If there are other non-numeric characters, or if a value is inherently non-convertible (e.g., an empty string after comma removal), it will raise an error instead of coercing to `NaN`. The ground truth's `pd.to_numeric(..., errors='coerce')` is much more robust for handling conversion failures.
    *   The problem specifically asked to ""if conversion fails for any value, treat that value as NaN, then fill all resulting NaNs with 0."" The generated code only fills NaNs that might exist *before* conversion to float or those introduced by `str.replace` if an original value was NaN, but doesn't explicitly handle conversion *failures* to NaN.

2.  **Hypothesis Statement Clarity:** While the hypotheses are logically correct, they aren't as formally stated as in the ground truth. The ground truth explicitly states H0: _music = _sports and Ha: _music  _sports, which is standard practice for statistical hypothesis testing.

3.  **`equal_var` Parameter in `ttest_ind`:** The generated code uses `stats.ttest_ind(music_likes, sports_likes)` which defaults to `equal_var=True`. In real-world scenarios, assuming equal variances between two groups (especially 'Likes' which can be highly skewed) is often inappropriate. Welch's t-test (`equal_var=False`) is generally preferred because it doesn't make this assumption. The ground truth explicitly uses `equal_var=False`. While the prompt didn't *explicitly* state `equal_var=False`, it's a best practice in data science applications, and the comparison is against an ""expert"" evaluator's ground truth, which includes this nuance.

4.  **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code lacks this.

To achieve a higher score (8+), the code would need to address the robustness of the 'Likes' cleaning, specify `equal_var=False` in the t-test, and refine the hypothesis statements.",command-r7b-12-2024,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np


def load_and_clean_data(file_path):
    """"""
    Load the dataset and perform feature engineering.

    Parameters:
    file_path (str): Path to the CSV file.

    Returns:
    DataFrame: The cleaned dataset with new columns.
    """"""
    # Load the dataset
    df = pd.read_csv(file_path)

    # Remove commas from 'Likes' and 'Dislikes' columns, convert to float, and handle NaN values
    df['Likes'] = pd.to.numeric(df['Likes'].str.replace(',', ''...",6,"The generated code correctly implements most of the requirements. It loads the dataset, cleans `Likes` and `Dislikes` by removing commas and converting to numeric, handles NaNs by filling with 0, and calculates `TotalEngagement` and `LikeDislikeRatio` as specified.

However, the generated code doesn't fully meet the display requirement. The prompt explicitly asks to ""Display the `Video` title, the newly created `TotalEngagement`, and `LikeDislikeRatio` columns for all videos"". The generated code provides a function `load_and_clean_data` but the actual display part in the example usage is commented out. This means the code as provided does not produce the required output. Additionally, one minor issue is `pd.to.numeric` should be `pd.to_numeric`.

To improve, the code should:
1. Uncomment the example usage for `display`.
2. Ensure `pd.to_numeric` is spelled correctly.
3. Add proper error handling for file loading as in the ground truth.",command-r7b-12-2024,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Cleaning the 'Launched Price (USA)' column
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', '').astype(float, errors='ignore')
df['Launched Price (USA)'].fillna(value=pd.NA, inplace=True)

# Cleaning the 'Screen Size' column
df['Screen Size'] = df['Screen Size'].str.replace(' inches', '').astype(float, errors='ignore')
df['Screen Size'].fillna(value=pd.NA, inp...",7,"The generated code correctly performs most of the required steps: loading the CSV, cleaning the specified columns to numeric (handling errors), grouping by `Company Name` and `Launched Year`, and calculating averages.

However, there are a few areas for improvement:

1.  **Error Handling for File Loading:** The generated code does not include any `try-except` block for `pd.read_csv`, which is present in the ground truth and good practice for robustness.
2.  **`astype(float, errors='ignore')` vs. `pd.to_numeric(errors='coerce')`:** Using `astype(float, errors='ignore')` is less robust than `pd.to_numeric(errors='coerce')`. `errors='ignore'` will simply return the original non-numeric values as objects, which then causes `mean()` to produce `NaN` if those values are present in the column, but it doesn't *convert* them to `NaN` as explicitly as `coerce` does. The ground truth's use of `pd.to_numeric(errors='coerce')` is the standard and more explicit way to handle this requirement.
3.  **`fillna(value=pd.NA, inplace=True)` after `astype(float, errors='ignore')`:** This step is redundant and potentially problematic. If `astype(float, errors='ignore')` failed to convert something, that value would remain in its original type (e.g., string), and `fillna` using `pd.NA` for an object column might not behave as expected or might be unnecessary if `pd.to_numeric(errors='coerce')` was used correctly in the first place. The primary goal was to convert non-numeric to NaN, which `pd.to_numeric(errors='coerce')` handles directly without needing `fillna` for the initial conversion.
4.  **Column Names in Output:** The output column names (`Launched Price (USA)_price` and `Screen Size_size`) are a bit clunky due to the merge suffixes. The ground truth creates new, clearer column names during the aggregation (`Average_Launched_Price_USD`, `Average_Screen_Size_inches`), which is better for readability and consistency.
5.  **Efficiency of Aggregation:** While functionally correct, performing `mean().reset_index()` for each column separately and then merging (`avg_price.merge(avg_size, ...)`) is less efficient and more verbose than using a single `.agg()` call as demonstrated in the ground truth. The single `agg` call is the idiomatic pandas way to perform multiple aggregations on different columns simultaneously.

Overall, it gets the job done but could be more robust, idiomatic, and efficient in its approach to cleaning and aggregation.",command-r7b-12-2024,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

# 1) Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# 2) Define a function to clean camera columns
def clean_camera(camera_string):
    """"""Extract the first sequence of digits as the MP value, returns 0 if no digits found""""""
    match = re.search(r'\d+', camera_string)
    if match:
        return int(match.group())
    else:
        return 0

# Apply the cleaning function to 'Front Camera' and 'Back Camera' columns
df['Cleaned Front Camera'] = df...",8,"The generated code correctly performs all the required tasks: loading the dataset, defining and applying the cleaning function to create cleaned camera MP columns, creating the total camera MP feature, and displaying the requested columns for the first 10 rows. Imports are correct.

One minor area for improvement is the `clean_camera` function's robustness. The ground truth code handles potential `NaN` values or non-string inputs by converting them to strings and returning 0 if `pd.isna` is true. The generated code's `re.search` directly on `camera_string` would raise a `TypeError` if a non-string (like `NaN`) is passed, which is a common occurrence in real-world datasets. While the specific example might not have NaNs in camera columns, a more robust function would account for it. However, given the prompt's simplicity, this is a minor oversight, not a critical failure. The prompt also did not explicitly ask for error handling for file loading or non-string inputs in camera specs, so the generated code is still largely compliant with the prompt's requirements.",command-r7b-12-2024,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error

# 1) Load the dataset
data = pd.read_csv('mobiles_2025.csv')

# 2) Clean specified columns
data['RAM'] = data['RAM'].str.replace('GB', '', regex=False).astype(float)
data['Battery Capacity'] = data['Battery Capacity'].str.replace('mAh', '', regex=False).astype(float)
data['Scree...",7,"The generated code largely fulfills the requirements of the task. It loads the data, correctly identifies and cleans the specified columns, splits the data, trains a Linear Regression model, and reports MAE.

However, there's a critical flaw in the cleaning step concerning how `astype(float)` combined with `errors='coerce'` is handled in the ground truth vs. the generated code. The prompt explicitly says: ""Handle conversion errors by setting to NaN and then drop rows with any NaN values in these specific columns.""

Ground Truth approach:
`df[col] = pd.to_numeric(df[col], errors='coerce')`
This is the correct way to convert to numeric while automatically setting non-convertible values to NaN. Then `dropna` subset cleans them.

Generated Code approach:
`data['RAM'] = data['RAM'].str.replace('GB', '', regex=False).astype(float)`
`data['RAM'] = data['RAM'].where(data['RAM'].notna(), np.nan)`

The `astype(float)` without `errors='coerce'` will raise an error if any value cannot be converted, stopping the script. It does not set invalid values to NaN. The subsequent `where` statement is redundant and ineffective for handling conversion errors if `astype(float)` fails directly. This means the primary mechanism for robust error handling on numeric conversion (as requested by the prompt) is missing. If the string after `replace` is not a valid number (e.g., empty string, ""N/A""), `astype(float)` will fail directly, preventing the script from proceeding.

Additionally, the generated code imports `mean_squared_error` but `r2_score` is defined by `model.score(X_test, y_test)` which is correct, but directly using `sklearn.metrics.r2_score` is also standard practice and was implied by the imports in the ground truth's import list (`from sklearn.metrics import r2_score, mean_absolute_error`). It also imported `numpy` which was not strictly necessary given `pd.to_numeric` covers the NaN coercion better.

To improve:
1.  Implement `pd.to_numeric(..., errors='coerce')` for all relevant columns to correctly handle conversion errors as NaNs.
2.  Remove the redundant and ineffective `data['RAM'].where(data['RAM'].notna(), np.nan)` lines.
3.  Ensure `dropna` correctly targets the specific columns after conversion. (This aspect `data.dropna(subset=['RAM', 'Battery Capacity', 'Screen Size', 'Launched Price (USA)'])` is correctly implemented by the generated code, assuming `pd.to_numeric` fills NaNs).
4.  While `model.score` is perfectly valid for R2, using `sklearn.metrics.r2_score` for consistency with `mean_absolute_error` and the common practices is advisable.",command-r7b-12-2024,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import chi2, ttest_ind, f_oneway, f2_pmax

# Load the Titanic dataset
titanic_data = pd.read_csv('titanic.csv')

# 1) Chi-square test: Association between Passenger Class and Survival
chi2_stat, p_value, dof = chi2.chi2_contingency(titanic_data[['Pclass', 'Survived']].groupby(['Pclass', 'Survived']).size().unstack())

print(f""Chi-Square Statistic: {chi2_stat}"")
print(f""P-value: {p_value}"")

if p_value < 0.05:
    conclusion = ""The...",6,"The generated code attempts to address all parts of the task but has several inaccuracies and missing elements, particularly regarding effect size calculations and handling of missing data.

**Correctness Issues:**

*   **Chi-square Test:**
    *   The `chi2_contingency` function is incorrectly called as `chi2.chi2_contingency`. It should be imported directly from `scipy.stats` as `chi2_contingency` (as done in the ground truth).
    *   The way the contingency table is created: `titanic_data[['Pclass', 'Survived']].groupby(['Pclass', 'Survived']).size().unstack()` is correct, but the generated code misses checking for `nan` values that might result in errors if `Pclass` or `Survived` have missing values.
    *   **Effect Size for Chi-square:** The generated code attempts to calculate ""Cohen's Phi"" using `chi2.phi(chi2_stat, p_value)`. This function does not exist in `scipy.stats.chi2`. The correct effect size for chi-square is Cramer's V, which requires a specific formula, not a direct function call. This is a critical error.

*   **T-test:**
    *   **Missing Data Handling:** The code does not explicitly handle `NaN` values in the 'Age' column. `ttest_ind` automatically drops `NaN`s, but it's good practice to be explicit or at least aware of it. The ground truth explicitly cleans the data.
    *   **Equal Variance Assumption:** It hardcodes `equal_var=False` for the t-test (Welch's t-test). While often a safer general choice, the prompt implies performing the test based on ""proper statistical methodology,"" which often includes checking variance equality (e.g., using Levene's test) as done in the ground truth.
    *   **Cohen's d Calculation:** The `cohens_d` calculation: `t_stat / np.sqrt(np.var(survivors) + np.var(non_survivors))` is incorrect. Cohen's d is `(mean1 - mean2) / pooled_standard_deviation`. The denominator in the generated code is not the pooled standard deviation nor the standard error of the difference for a typical t-test. The ground truth provides the correct calculation of pooled standard deviation.

*   **ANOVA:**
    *   **Incorrect `f_oneway` usage:** The generated code tries to perform ANOVA on `fare_prices = titanic_data.groupby('Pclass')['Fare'].mean()`. `f_oneway` expects the actual data arrays for each group, not their means. This is a fundamental error. The ground truth correctly passes `fare_pclass1, fare_pclass2, fare_pclass3`.
    *   **Missing Data Handling:** While 'Fare' usually doesn't have NaNs, robust code should account for it.
    *   **Effect Size for ANOVA (Eta Squared):** The `eta_squared` calculation `1 - (sum(fare_prices.var()) / sum(titanic_data['Fare'].var()))` is completely incorrect. Eta-squared is `SS_between / SS_total`. The ground truth calculates this correctly.

**Completeness and Code Quality Improvements:**

*   **Imports:** The generated code imports `f2_pmax` which is not used. It also incorrectly imports `chi2` as a module from `scipy.stats` rather than importing `chi2_contingency` explicitly.
*   **Interpretation:** While conclusions are made, they lack the specific p-value and alpha level, which helps in direct interpretation. The ground truth provides more detailed conclusions by including these values and explicitly stating the null hypothesis rejection/failure to reject.
*   **Robustness:** The code is not robust to missing data (especially in 'Age' but also implicitly in 'Fare'). The ground truth includes `dropna` calls.
*   **Clarity:** The ground truth includes more descriptive print statements, including hypothesis statements for each test, which makes the output much clearer and more educational.
*   **Alpha Level:** The alpha level is hardcoded as `0.05` in the `if` conditions rather than being defined as a variable at the top, which is better practice for consistency and easy modification.
*   **Error Handling:** The ground truth includes basic file not found error handling, which is good practice.

Overall, the generated code demonstrates an understanding of the types of tests required but fails significantly in the correct implementation of the statistical formulas and functions, particularly for effect sizes and ANOVA setup.",command-r7b-12-2024,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Group data by 'Year' and calculate average 'Rating' and 'Runtime'
df_grouped = df.groupby('Year')[['Rating', 'Runtime']].mean().reset_index()

# Create a line plot
sns.plot(x='Year', y='Rating', data=df_grouped, kind='line', marker='o', color='blue')

# Add title and labels
plt.title('Average Rating Over the Years')
plt.xlabel('Year')
plt.ylabel('Average...",6,"The generated code successfully loads the data, groups it by year, calculates the average rating and runtime, and attempts to plot the average rating over time. However, there are critical issues. The `sns.plot` call is incorrect; it should be `sns.lineplot`. More importantly, the generated code *does not handle data cleaning or preprocessing* for the 'Year' and 'Runtime' columns, which are often not in a directly numeric format in such datasets (e.g., 'Year' might be '(2000)', 'Runtime' might be '120 min'). The Ground Truth Code correctly anticipates and handles these common data inconsistencies, which is crucial for real-world data and makes its solution more robust. Plots also lack basic formatting like grid and `plt.tight_layout()` for better readability. While it addresses the core requirements, its lack of robustness to common dataset formats makes it less practical and complete than the Ground Truth.",command-r7b-12-2024,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# 1) Load the dataset
imdb_top_250 = pd.read_csv('imdb_top_250.csv')

# 2) Create two groups of movies based on the 'Year' column
movies_before_1990 = imdb_top_250[imdb_top_250['Year'] < 1990]
movies_1990_or_later = imdb_top_250[imdb_top_250['Year'] >= 1990]

# 3) Perform an independent two-sample t-test
t_statistic, p_value = ttest_ind(movies_before_1990['Rating'], movies_1990_or_later['Rating'])

# 4) State null and alternative h...",7,"The generated code correctly performs the core task of loading data, splitting groups, and executing a t-test. It also correctly interprets the p-value against the alpha level and prints the required outputs.

However, there are several areas for improvement as compared to the ground truth:
1.  **Robustness (Error Handling & Data Type Conversion):** The generated code lacks error handling for `FileNotFoundError` when loading the CSV. More critically, it doesn't explicitly convert 'Year' or 'Rating' columns to numeric types using `pd.to_numeric` with `errors='coerce'`, which is crucial for real-world datasets where these columns might be loaded as objects or contain non-numeric characters. The ground truth handles this, ensuring the data is in the correct format for calculations and drops rows with missing values that resulted from conversion errors.
2.  **Hypotheses Statement:** The hypotheses are stated as comments rather than printed statements as requested by the task (""Clearly state the null and alternative hypotheses for this test.""). The ground truth prints them explicitly, making the output more readable and fulfilling the requirement literally.
3.  **Completeness of Output:** While the interpretation is correct, the printed output is less verbose and informative than the ground truth's. The ground truth provides more context around the conclusion (e.g., ""Conclusion: There is a statistically significant difference...""), which is valuable for a statistical analysis.
4.  **Clarity of Imports:** `from scipy.stats import ttest_ind` is fine, but `import pandas as pd` and `from scipy import stats` (as in Ground Truth) is a common and slightly more flexible way to import `scipy.stats` when other `scipy` modules might be used later.

The code is functional and addresses the primary requirements, but the lack of robustness and the less explicit hypothesis statement reduce its score.",command-r7b-12-2024,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

# 1) Load the dataset
imdb_df = pd.read_csv('imdb_top_250.csv')

# 2) Create a new column named `Decade` by categorizing the `Year` column
imdb_df['Decade'] = imdb_df['Year'].apply(lambda x: f""{x // 10 * 10}s"")

# 3) Display the `Title`, `Year`, and the new `Decade` column for the first 10 movies
print(imdb_df[['Title', 'Year', 'Decade']].head(10))
```",9,"The generated code correctly solves all aspects of the task. It loads the dataset, accurately creates the 'Decade' column using a lambda function for clear logic, and displays the required columns for the first 10 rows. The code is concise and directly addresses the prompt's requirements.

Minor areas for improvement (not affecting score significantly):
- The ground truth includes robust error handling (e.g., FileNotFoundError, KeyError, handling non-numeric year values) and a structured function-based approach, which are good practices for production-ready code but were not explicitly requested by the prompt for this specific task.
- The prompt asked for ""only the Python code without explanations,"" and the generated code includes comments for each step, which while helpful, deviates slightly from ""only the Python code."" However, code comments are generally a good practice.
- The `head(10)` output in the generated code includes the DataFrame index, while the ground truth uses `to_string(index=False)` to remove it for cleaner presentation, which was implicitly preferred by the example format. This is a minor formatting difference.",command-r7b-12-2024,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

# 1) Load the dataset
penguins = pd.read_csv(""penguins.csv"")

# 2) Impute missing values in numerical columns
numerical_columns = [""culmen_length_mm"", ""culmen_depth_mm"", ""flipper_length_mm"", ""body_mass_g""]
penguins[numerical_columns] = penguins[numerical_columns].fillna(
    penguins[numerical_columns].mean()
)

# 3) Impute missing values in categorical 'sex' column
penguins[""sex""] = penguins[""sex""].fillna(penguins[""sex""].mode()[0])

# 4) Group data by 'species' an...",10,"The generated code correctly addresses all requirements of the task. It loads the dataset, imputes numerical columns with their respective means and the 'sex' column with its mode, groups the data as specified, and calculates the average `body_mass_g`. The use of `fillna(df[cols].mean())` is a concise and correct way to impute numerical columns. The `reset_index()` at the end of the aggregation is not strictly necessary for displaying the result as requested (the ground truth directly prints the Series), but it's a common and acceptable practice for making the grouped result a DataFrame, which is often more useful for further operations. The code is clean, well-structured, and correctly imports pandas.",command-r7b-12-2024,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
import scipy.stats as stats

# 1) Load the dataset
penguins = pd.read_csv('penguins.csv')

# 2) Filter the data to include only records where 'species' is 'Adelie' or 'Gentoo'
filtered_penguins = penguins[penguins['species'].isin(['Adelie', 'Gentoo'])]

# 3) Remove any rows where 'flipper_length_mm' or 'species' is missing (NA)
filtered_penguins = filtered_penguins.dropna(subset=['flipper_length_mm', 'species'])

# 4) Perform an independent two-sample t-test
t_stati...",8,"The generated code correctly addresses most of the requirements. It loads the data, filters it, handles NAs, performs the t-test, and prints the results and interpretation.

**Strong Points:**
*   Correctly imports necessary libraries (`pandas`, `scipy.stats`).
*   Accurately performs all data filtering and NA removal steps as specified.
*   Correctly uses `ttest_ind` for the independent two-sample t-test.
*   Clearly states the null and alternative hypotheses, even though it doesn't print them directly, it stores them.
*   Prints the t-statistic and p-value correctly.
*   Correctly interprets the p-value against the alpha level to decide whether to reject the null hypothesis.

**Areas for Improvement (Why not a 9 or 10):**
*   **Hypotheses Printing:** While the hypotheses are correctly defined as string variables, they are not explicitly *printed* as requested in requirement 5 (""Clearly state the null and alternative hypotheses for this test.""). The ground truth code makes this very clear.
*   **Verbosity in Conclusion:** The conclusion combines the rejection statement with an additional interpretation. While mostly correct, the wording ""There is statistically significant difference in mean flipper length between Adelie and Gentoo penguins."" is always printed, even when failing to reject the null hypothesis, which creates a slight logical inconsistency in the printed output. For example, if it fails to reject, it might print ""Fail to reject the null hypothesis. There is statistically significant difference..."", which is contradictory. The interpretation should be conditional on the rejection.
*   **Error Handling:** The ground truth includes basic error handling for file not found, which is a good practice for robust code. The generated code assumes the file will always be present, which might lead to a `FileNotFoundError` without a graceful exit.
*   **T-test `equal_var` parameter:** The `ttest_ind` function defaults to `equal_var=True`. While not explicitly specified in the prompt whether to assume equal variances or not, a robust solution often uses `equal_var=False` (Welch's t-test) which is generally safer when variance equality is unknown or violated. The ground truth code explicitly mentions this nuance and chooses `equal_var=False`. The generated code uses the default, so it's not strictly incorrect based on the prompt, but it's an important consideration in statistical testing.

Overall, it's a very good attempt that fulfills most of the core requirements. The feedback points are mostly about refinement and robustness.",command-r7b-12-2024,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv('penguins.csv')

# Select features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]
y = df['species']

# Drop rows with missing values (NA)
X = X.dropna()
y = y.dropna()

# Enc...",6,"The generated code correctly performs most of the required steps, showing a good understanding of the overall task. It loads the data, selects the specified features and target, encodes the target variable, splits the data, trains a Logistic Regression model with `max_iter=1000`, and reports the accuracy.

However, there are two significant issues that reduce the score:

1.  **Incorrect Handling of Missing Values (CRITICAL FLAW for Data Integrity):** The instruction for dropping NA values was very specific: ""Drop any rows where `species` or any of the four selected feature columns have missing values (NA)."" The generated code attempts to handle NA values separately for `X` (features) and `y` (target): `X = X.dropna()` and `y = y.dropna()`. This approach is fundamentally flawed for supervised learning. If a row has an NA in `X` but not in `y`, `X.dropna()` will remove it from `X` but not from `y`, leading to `X` and `y` having different numbers of rows and misaligned data. Similarly, if `y` has an NA, `y.dropna()` will remove it from `y` but not necessarily from `X`. The correct way is to drop rows based on NaNs in the *combined* relevant columns or the original dataframe before splitting into `X` and `y` to ensure alignment. The ground truth correctly addresses this by creating `df_cleaned` first and then deriving `X` and `y` from it.

2.  **Unspecified Feature Scaling:** The generated code includes `StandardScaler` and applies it to the features (`X_train_scaled`, `X_test_scaled`). While feature scaling is often beneficial for Logistic Regression and good practice, it was *not* specified in the task requirements. The task explicitly asked for ""default parameters"" for Logistic Regression, except for `max_iter=1000`. Including unrequested steps, even if they sometimes improve performance, deviates from the precise requirements of a benchmark. This makes the solution less aligned with the exact prompt.

To improve, the code needs to correctly handle missing values by dropping rows from a combined DataFrame (or carefully from either X or y *before* separation, ensuring consistent indices), and it should remove the `StandardScaler` application as it was not requested.",command-r7b-12-2024,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv(""penguins.csv"")

# Drop rows where `body_mass_g` or `species` is NA
df = df.dropna(subset=[""body_mass_g"", ""species""])

# For each `species`, calculate the average `body_mass_g`
avg_mass = df.groupby(""species"")[""body_mass_g""].mean()

# Create a bar plot showing the average `body_mass_g` for each `species`
sns.barplot(x=avg_mass.index, y=avg_mass.values, palette=""viridis"")
plt.t...",9,"The generated code is highly correct and complete, addressing all requirements of the task.

**Strong Points:**
*   **Correctness:**
    *   Successfully loads the `penguins.csv` dataset.
    *   Correctly handles NA values by dropping rows where `body_mass_g` or `species` is NA using `dropna(subset=...)`.
    *   Accurately calculates the average `body_mass_g` for each `species` using `groupby()` and `mean()`.
    *   Generates a bar plot as required.
    *   Includes a title and correctly labeled axes (`title`, `xlabel`, `ylabel`).
    *   Rotates x-axis labels for better readability (`xticks(rotation=45)`).
    *   Uses `tight_layout()` for optimal plot display.
*   **Code Quality:**
    *   Properly imports necessary libraries (`pandas`, `matplotlib.pyplot`, `seaborn`).
    *   The code is concise and readable.

**Areas for Minor Improvement (to reach a perfect 10, aligning with Ground Truth's robustness):**
*   **Robustness (Data Loading):** The Ground Truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. While not explicitly asked for, this adds robustness to real-world scenarios. The generated code assumes the file will always be present.
*   **Parameterization/Readability:** The Ground Truth uses configuration variables (e.g., `TARGET_COLUMN`, `GROUPING_COLUMN`) for column names. While not strictly necessary for this small script, it enhances maintainability and reduces magic strings in larger projects, making the code more readable and easier to modify if column names change.
*   **Plotting Argument Clarity:** In `sns.barplot(x=avg_mass.index, y=avg_mass.values, ...)`, explicit variable names for x and y are good. However, if `avg_mass` were a DataFrame, passing `x='species', y='body_mass_g', data=avg_mass.reset_index()` would be slightly more idiomatic for seaborn and directly mirror the grouped DataFrame. The way it's done is functionally correct here because `avg_mass` is a Series.

Overall, it's an excellent solution that meets all core requirements efficiently. The suggested improvements are minor and relate more to best practices for larger or more robust applications rather than direct task fulfillment.",command-r7b-12-2024,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1) Load the dataset
penguins = pd.read_csv('penguins.csv')

# 2) Drop rows where `culmen_length_mm`, `culmen_depth_mm`, `flipper_length_mm`, or `body_mass_g` are NA
penguins = penguins.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])

# 3) Create a new feature `culmen_area` calculated as `culmen_length_mm` * `culmen_depth_mm`
penguins['culmen_area'] = penguins['culmen_...",9,"The generated code successfully completes all aspects of the task as specified in the prompt. It correctly loads the data, handles NAs for the specified columns, creates the new `culmen_area` feature, calculates the Pearson correlation matrix for the required columns, and visualizes it as an annotated heatmap with the correct title. The use of standard libraries (pandas, seaborn, matplotlib) is appropriate.

The only minor point that could be improved (and why it's not a perfect 10) is the lack of error handling for the `FileNotFoundError` when loading the CSV, which the ground truth includes. While not explicitly asked for in the task, it's good practice for robust code. Additionally, the ground truth includes more robust printing and checks for successful execution and empty dataframes, which are good additions for a production setting but not strictly required by the prompt's core task. The visualization parameters (`figsize`, `cmap`, `center`, `linewidths`, `vmin`, `vmax`) are well-chosen to make the heatmap clear and informative.",command-r7b-12-2024,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer

# 1) Load the dataset
penguins = pd.read_csv('penguins.csv')

# 2) Select the features and target variable
features = [""culmen_length_mm"", ""culmen_depth_mm"", ""flipper_length_mm"", ""body_mass_g"", ""island"", ""sex""]
target = ""species""

# 3) Impute NA values for numerical features
imputer = SimpleImputer(strategy='mean')
for feature in features:
    if feature != ""sex...",6,"The generated code correctly addresses most of the requirements but has a significant bug in the concatenation step for one-hot encoding, making the resulting DataFrame incorrect.

**Strengths:**
*   **Correct Data Loading:** Successfully loads the `penguins.csv` dataset.
*   **Correct Feature Selection:** Identifies and prepares to work with the specified features and target.
*   **Correct Numerical Imputation Strategy:** Uses `SimpleImputer(strategy='mean')` which is conceptually correct for mean imputation.
*   **Correct Sex Imputation and Mapping:** Imputes 'Unknown' and maps 'MALE', 'FEMALE', 'Unknown' to 0, 1, -1 respectively.
*   **Correct One-Hot Encoding Approach:** Uses `OneHotEncoder` which is appropriate for categorical features.
*   **Correct Output (Shape and Head):** Attempts to print the shape and head of the processed features.

**Areas for Improvement / Issues:**

1.  **Numerical Imputation Loop Error:** The loop `for feature in features: if feature != ""sex"": penguins[feature] = imputer.fit_transform(penguins[[feature]])` only handles the numerical features, but it applies `fit_transform` to ALL features except 'sex', which is problematic for 'island' also. While `SimpleImputer` might ignore non-numerical data if specified, it's better to explicitly select numerical columns for imputation. The ground truth explicitly lists numerical columns, which is safer.
2.  **Missing Target Variable Separation:** The task explicitly asks to predict `species`, implying the need to separate features (X) and target (y). The generated code processes all features but doesn't explicitly separate `species` as the target variable or prepare the feature matrix `X` without `species`. The print statement refers to ""resulting feature matrix"" which ideally should *not* contain the target.
3.  **Critical Concatenation Bug:** The line `penguins = penguins.concat([penguins, penguins_encoded], axis=1)` is incorrect. `concat` is a pandas *function*, not a DataFrame method. It should be `penguins = pd.concat([penguins, penguins_encoded], axis=1)`. This bug prevents the code from running and producing the correct final output shape.
4.  **Implicit Feature Dropping for Numerical Imputation:** After the numerical imputation, the `penguins` DataFrame has `island` and `sex` columns where `fit_transform` was applied (though `SimpleImputer` would likely ignore `island` if it's object dtype). It's more robust to select numerical columns first or ensure the imputer is only applied to relevant types.
5.  **Efficiency in Numerical Imputation:** Applying `fit_transform` in a loop for each column means `fit` is called multiple times. While `SimpleImputer` typically handles multiple columns, if you apply it column by column, it's better to only apply `transform` after one `fit` on all numerical columns or `fit_transform` on the sub-dataframe of numerical columns at once.
6.  **Missing `target` from `features` list:** The `features` list does not include `species`, which is good for `X`. However, the final `penguins` DataFrame printed to console *still contains* `species`. The task asked to print the shape and head of the ""resulting feature matrix,"" which typically refers to `X` (features only). This implies `species` should be dropped at the end.",command-r7b-12-2024,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004'...",6,"The generated code correctly creates the DataFrame and calculates the total quantity per category. However, there's a critical error in calculating the ""total spending for each 'CustomerID'"". The generated code sums `Price_USD` directly, which is incorrect because it should be `Quantity * Price_USD` for each item. This leads to an incorrect total spending calculation and consequently incorrect top 3 customers. The output format for the top 3 customers is also a DataFrame printout, while the ground truth specifically asks to ""print the 'CustomerID' and their total spending"" (implying a more readable format). While it addresses all requirements, the incorrect calculation for total spending fundamentally impacts correctness for a key part of the task.

**Areas for Improvement:**
1.  **Correct Total Spending Calculation:** The most crucial fix is to calculate `LineTotal = Quantity * Price_USD` first, and then sum this `LineTotal` per customer.
2.  **Output Format for Top Customers:** Instead of printing the `top_customers` DataFrame directly, iterate through it and print each customer's ID and total spending in a user-friendly format, similar to the ground truth.
3.  **Intermediate Prints (Optional but good practice):** Adding intermediate print statements to show the DataFrame, customer spending, etc., makes the code's execution clearer, though this was not strictly required by the prompt's ""provide only the Python code"" rule.",command-r7b-12-2024,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.pipeline import make_pipeline

# 1) Data Preprocessing

data = pd.read_csv(""titanic.c...",6,"The generated code addresses most of the requirements but has significant issues in its preprocessing and feature interpretation approach, which ultimately affects correctness and completeness.

**Correctness & Completeness Issues:**
*   **Preprocessing Pipeline Misuse:** The generated code attempts to use `make_pipeline(SimpleImputer(), StandardScaler(), LogisticRegression())` on `X_train` which has already been preprocessed (missing values handled, categorical variables encoded, numerical features scaled) *before* the train-test split. This leads to data leakage and incorrect application of transformers. For instance, `SimpleImputer` will essentially do nothing as missing values are already handled, and `StandardScaler` will attempt to re-scale already scaled columns, or even scale one-hot encoded columns (which is incorrect). The `ColumnTransformer` approach in the ground truth is crucial for applying different transformations to different column types correctly.
*   **Feature Scaling:** The generated code scales `SibSp` and `Parch` directly, but these columns are often better used for feature engineering (e.g., `FamilySize`) or treated as categorical if their distinct values are small. The lack of `FamilySize` creation is a missed opportunity for a potentially more informative feature in this context.
*   **`get_dummies` vs. `OneHotEncoder` within Pipeline:** `pd.get_dummies` is used *before* the train-test split, which is poor practice as it can lead to issues with unseen categories in the test set or inconsistent column ordering. The ground truth correctly uses `OneHotEncoder` within a `ColumnTransformer` inside the model pipeline, ensuring transformers learn from training data only and are applied consistently.
*   **Feature Interpretation:** The feature interpretation part is flawed. It prints a generic ""Age"" interpretation for all positive coefficients and attempts to interpret coefficients for features that haven't been properly handled (e.g., it will try to interpret 'Embarked' or 'Sex' columns that no longer exist in `X` because they were replaced by one-hot encoded columns). The `coef_` attribute of `LogisticRegression` will typically have a single array for binary classification, and matching these coefficients back to the correct feature names (especially after one-hot encoding) requires careful handling, as demonstrated in the ground truth using `get_feature_names_out()`. The interpretation logic is also incomplete and not specific to the actual features. The missing `Cabin`, `PassengerId`, `Name`, `Ticket` drops are also a minor but relevant omission.
*   **Missing Metric Breakdown:** While `classification_report` provides precision, recall, and F1-score, the prompt specifically asked for individual print statements for accuracy, precision, recall, F1-score, and ROC-AUC. The generated code does not explicitly extract and print them individually.
*   **Confusion Matrix Display:** The confusion matrix is printed as an array, not visualized as requested. Similarly, the ROC curve is plotted but without the standard random classifier line (y=x).

**Code Quality & Structure:**
*   Proper imports are generally present.
*   The use of `make_pipeline` is good for simplifying the model construction, but it's misapplied due to the preprocessing issues mentioned above.
*   The random seed for reproducibility is not set for numpy or sklearn, which is a good practice for benchmarks.

Overall, the generated code demonstrates an understanding of the components required, but the crucial aspect of integrating preprocessing into a robust pipeline (to avoid data leakage and handle various column types correctly) is missed, and the feature interpretation is weak.",command-r7b-12-2024,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame from the provided data
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']),
    'Ticker': ['TECHCO']*8,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
    'Close': [152.0, 151.0, 153.5, ...",10,"The generated code is excellent. It correctly implements all requirements of the task:
1.  **DataFrame Creation and Indexing**: The DataFrame is created accurately with the provided data, and 'Date' is correctly set as the index.
2.  **SMA Calculation**: The 3-day simple moving average of the 'Close' price (`df['Close'].rolling(window=3).mean()`) is calculated and assigned to the 'SMA_3Day' column. It correctly handles initial NaN values for the first two days, as expected for a 3-day window.
3.  **Volume Percentage Change**: The percentage change in 'Volume' from the previous day (`df['Volume'].pct_change()`) is calculated and assigned to the 'Volume_Pct_Change' column. It correctly produces a NaN for the first day.
4.  **Printing**: The final DataFrame with the new columns is printed as required.
5.  **Code Quality**: The code is clean, readable, and follows Python best practices, including necessary imports.

The generated code is functionally identical to the ground truth code and achieves all specified objectives perfectly.",command-r7b-12-2024,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

# Sample student performance data creation and preprocessing
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0...",9,"The generated code correctly performs all the requested operations: creates the DataFrame, calculates the 'Score_Improvement' column, calculates and prints the average improvement per subject, and identifies students meeting the specified score and attendance criteria. The code is well-structured and uses appropriate pandas functions.

The only minor point of improvement is that the task explicitly asked to ""print the 'StudentID'"" for the last part. The generated code prints the `StudentID` Series, which is functionally correct but the ground truth iterated through the Series and printed each ID individually, which aligns a bit more closely with ""print the 'StudentID' of the student(s)"". This is a very minor difference in output format rather than logical correctness. Overall, it's an excellent solution.",command-r7b-12-2024,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

# Filter DataFrame
tech_employees = df[df['Department'] ...",9,"The generated code correctly defines the DataFrame, filters it according to the specified conditions, and calculates the average ""ProjectsCompleted"" accurately. The structure is clear and readable.

Strengths:
- Correct DataFrame creation.
- Accurate filtering logic, although done in two steps which is slightly less concise than the ground truth's combined condition, it is functionally equivalent and correct.
- Correct calculation of the average.
- Clear and concise output message.
- Proper use of f-string for formatted output and '.2f' for two decimal places.

Areas for improvement (minor):
- The filtering could be done in a single line using `&` for conciseness, similar to the ground truth. While the current two-step filtering is correct, a single boolean indexing operation tends to be slightly more idiomatic pandas for combined conditions.
- The ground truth includes an `if not filtered_df.empty:` check, which is good practice for robust code, especially in real-world scenarios where filtering might result in an empty DataFrame. While for this specific dataset and query, the result is guaranteed not to be empty, including such checks demonstrates better error handling and robustness. For a benchmark, this level of robustness is often considered a strong point.

Overall, it's a very good solution that fulfills all the core requirements of the task. The minor points are about stylistic choice or added robustness, not correctness for this specific problem.",command-r7b-12-2024,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}

df = pd.DataFrame(data)

# Calculate conversion rates
desktop_users = df[df[""DeviceType""] == ""Desktop""]
desktop_converted = desktop_users[""Converted""]....",9,"The generated code correctly defines the DataFrame and accurately calculates the conversion rates for both Desktop and Mobile users. It also prints the results as required, clearly labeling them. The approach of using `sum()` on the boolean 'Converted' series (`True` counts as 1, `False` as 0) divided by the `len()` of the filtered DataFrame is a valid and correct way to calculate the mean (and thus the percentage).

The only minor point of improvement compared to the Ground Truth code is that the Ground Truth uses `.mean()` directly on the boolean series, which is slightly more idiomatic and concise for calculating proportions in pandas. However, `sum() / len()` achieves the exact same correct result. The Ground Truth also includes explicit handling for potentially empty filtered DataFrames, which is a good practice for more robust code, although for the given fixed data, it's not strictly necessary.

Overall, the generated code is highly correct and complete for the given task.",command-r7b-12-2024,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", 
                   ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

df = pd.DataFrame(data)

# Group by SensorID and calculate average AirQualityIndex
...",10,"The generated code successfully defines the DataFrame as specified, and accurately calculates and prints the average ""AirQualityIndex"" for each unique ""SensorID"". The output format is clear and meets the task requirements.

**Strong points:**
*   **Correctness:** The DataFrame creation is accurate, and the `groupby()` method correctly calculates the mean `AirQualityIndex` for each sensor.
*   **Completeness:** All aspects of the task are addressed: DataFrame creation and the calculation and printing of the per-sensor average AirQualityIndex.
*   **Code Quality:** The code is well-structured, uses standard pandas operations, and includes comments explaining key steps (though brief, they are sufficient for this task). The use of f-strings for clear output is good practice. The `.reset_index()` is an optional but valid way to prepare the grouped result for iteration, especially useful if you were to further process the results as a DataFrame. Iterating with `iterrows()` is also a clean way to present the results.",command-r7b-12-2024,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
# Create DataFrame
import pandas as pd
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

# Calculate EngagementScore
df['EngagementScore'] = (df['Likes'] + (...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the ""EngagementScore"" feature according to the specified formula, accurately filters for ""Instagram"" posts, and correctly calculates and prints their average ""EngagementScore"" as requested. The code is clean, concise, and directly addresses all parts of the task. The use of f-string for printing with 2 decimal places also matches the ground truth's output format.",command-r7b-12-2024,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male""...",8,"The generated code correctly performs all the requested tasks: defining the DataFrame, calculating LengthOfStay, filtering for Flu diagnosis, and calculating the average LengthOfStay for Flu patients.

However, there is a minor error in the final print statement. `flu_avg_stay` is a `Timedelta` object, and attempting to access `.days` attribute directly on a `Timedelta` object that might represent an average of multiple Timedeltas will raise an `AttributeError` if the average is not a whole number of days. It should generally be `flu_avg_stay.total_seconds() / (24 * 3600)` or similar to get the average in days, or more simply, the `dt.days` accessor should have been applied earlier if the average was expected to be an integer number of days. The ground truth correctly applies `.dt.days` when creating the column, making subsequent average calculations cleaner.

**Specific points for improvement:**
1.  **`LengthOfStay` calculation:** While `df[""LengthOfStay""] = df[""DischargeDate""] - df[""AdmissionDate""]` correctly calculates the timedelta, the task specified ""LengthOfStay"" *in days*. The ground truth handles this by using `.dt.days` immediately: `(df[""DischargeDate""] - df[""AdmissionDate""]).dt.days`. This ensures the column is numeric (integer days) from the start.
2.  **Final print statement:** Because `LengthOfStay` was left as a `Timedelta` object, `flu_avg_stay` is also a `Timedelta` object representing the average timedelta. Accessing `flu_avg_stay.days` on a `Timedelta` only returns the number of full days, not the fractional part. For example, if the average is ""5 days 12 hours"", `flu_avg_stay.days` would just be 5. To get the average in total days (including fractional part), one should use `flu_avg_stay.total_seconds() / (24 * 3600)`. If the `LengthOfStay` column was already in days (as per ground truth), then `flu_avg_stay` would be a float directly, and `print(""Average LengthOfStay for Flu patients:"", flu_avg_stay)` would suffice.

Despite these minor issues, the core logic is sound and the code is well-structured and readable.",command-r7b-12-2024,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
# Import pandas library
import pandas as pd

# Create the DataFrame
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}
df = p...",9,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates their average rating, fulfilling all task requirements. The code is clean, well-structured, and directly addresses the problem. It properly uses pandas for DataFrame creation and manipulation.

A minor omission, though not critical for this specific dataset and context, is the lack of an explicit check for an empty filtered DataFrame (`sci_fi_movies`). If, for instance, there were no Sci-Fi movies in the `data` dictionary, calling `.mean()` on an empty Series would result in `NaN` (or an error depending on the pandas version/context), which might need to be explicitly handled in a more robust production scenario (as the ground truth code does). However, for the given task and data, this is not an issue, and the direct approach is perfectly acceptable. The use of an f-string for printing is also good practice.",command-r7b-12-2024,LLM_judge_config
M58,medium,Data Manipulation,,"```python
# Import pandas library and create the DataFrame
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

df = pd.DataFrame(data)

# Filter Toyota car...",10,"The generated code correctly defines the DataFrame, filters for ""Toyota"" brand cars, and accurately calculates and prints their average mileage. The code quality is good, with clear variable names and logical steps. It includes all necessary imports and performs all required tasks as specified in the prompt. The f-string formatting for the output is also well-done.",command-r7b-12-2024,LLM_judge_config
M59,medium,Data Filtering,,"```python
# Filter the DataFrame to include only Pokemon whose ""Type1"" is ""Fire""
fire_type_df = df[df['Type1'] == 'Fire']

# Calculate and print the average ""Attack"" stat for Fire-type Pokemon
average_attack = fire_type_df['Attack'].mean()
print(f""Average Attack stat for Fire-type Pokemon: {average_attack}"")
```",6,"The generated code correctly performs the filtering and calculates the average attack stat. However, it is incomplete as it does not include the DataFrame definition which is crucial for the code to be executable and fulfill the prompt's initial setup. Without the `import pandas as pd` and the `df = pd.DataFrame(data)` parts, the provided snippet is not a complete, runnable solution. The output format for printing the average attack also lacks the f-string formatting for two decimal places as present in the ground truth, although this is a minor stylistic difference. Adding the DataFrame definition and import statements would bring the score much higher.",command-r7b-12-2024,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# Load the dataset
titanic_df = pd.read_csv('titanic.csv')

# 1) Multi-panel figure for survival rates by demographics
fig, axes = plt.subplots(2, 3, figsize=(20, 15))

# By class
sns.countplot(x='Pclass', hue='Survived', data=titanic_df, ax=axes[0, 0])
axes[0, 0].set_title('Survival by Class')

# By gender
sns.countplot(x='Sex', hue='Survived', data=...",4,"The generated code attempts to address the requirements but fails significantly on several fronts, leading to a low score.

**Correctness & Completeness Issues:**
1.  **Multi-panel figure (Task 1):** The request was for a multi-panel figure showing survival *rates* by different demographic groups. The generated code uses `countplot` of `Survived` which shows absolute counts, not rates, making it difficult to directly compare survival proportions across groups. Also, it creates a 2x3 grid but only fills 3 plots in the first row, leaving the second row empty. The age groups are created with `labels=False` which results in numerical labels instead of descriptive ones.
2.  **Heatmap (Task 2):** This part is largely correct, though `index='Sex'` and `columns='Pclass'` are swapped compared to a more standard representation (Ground Truth uses `index='Pclass_Cat'` and `columns='Sex'`). The choice of `coolwarm` might be less intuitive for survival rates than a sequential colormap.
3.  **Violin Plots (Task 3):** The prompt explicitly asked for ""violin plots comparing age distributions of survivors vs non-survivors by gender."" The generated code only creates a `violinplot` of `Age` by `Sex`, completely missing the crucial `hue` for `Survived` to compare survivors vs. non-survivors. This is a critical omission.
4.  **Interactive Scatter Plot (Task 4):** The prompt requested the interactive scatter plot to show ""the relationship between age, fare, *and survival* with different colors for passenger classes."" The generated code uses `go.Scatter` and colors by `Pclass` but does not encode survival status in the plot (e.g., via symbol or hovering, as the ground truth does with `symbol='Survived_Str'`). It also misses the `plotly.express` usage, which is usually simpler for such plots.
5.  **Dashboard-style figure (Task 5):**
    *   The structure is a simple 2x3 grid, not a ""comprehensive dashboard-style figure combining multiple visualizations with proper annotations and insights"" like the ground truth's `gridspec` layout with a dedicated insights text area.
    *   The plots chosen for the dashboard are partially redundant with previous tasks (`countplot` of survival by Pclass/Sex, heatmap) and some choices don't fit the ""dashboard"" theme well (e.g., `boxplot` of Age by Gender, which could be part of broader age distribution analysis but isn't as insightful as the violin plot comparison).
    *   It tries to add a `px.scatter` figure *within* a matplotlib figure (`ax=axes[1,2]`) which is not possible directly and causes the `px.scatter` to be shown as a separate interactive plot, breaking the ""single dashboard figure"" requirement.
    *   The ""insights"" part is also a `countplot` and not textual annotations as requested, indicating a severe misunderstanding of ""annotations and insights"" in a dashboard context.

**Code Quality & Structure:**
*   Lack of data preprocessing. The ground truth handles missing values (`Age`, `Fare`) and creates `AgeGroup` and categorical representations for `Pclass` and `Survived`, which are crucial for effective visualization and analysis. The generated code only creates a simple `AgeGroup` for one specific plot and doesn't handle NaNs systematically.
*   Poor use of subplots: Plots are not correctly placed (e.g., only 3 in a 2x3 grid for the first task).
*   Limited use of storytelling elements: No clear titles for all plots, inconsistent palette usage, and missing proper labels for axes in some cases, especially in the multi-panel and dashboard sections.
*   The `fig.show()` after every plot makes for a disjointed experience rather than a coherent set of visualizations.

Overall, the generated code produces several plots, but many do not correctly fulfill the specific requirements of the prompt, especially concerning comparing survival rates, incorporating all variables in interactive plots, and designing a comprehensive dashboard with insights. The data preprocessing and thoughtful selection of chart types for specific comparisons are also significantly lacking.",command-r7b-12-2024,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, ...",9,"The generated code correctly defines the DataFrame, filters for Dystopian genre books, and accurately calculates the average stock for that genre, fulfilling all the requirements of the task. The code is concise and efficient.

One minor improvement would be to include a check for an empty `dystopian_books` DataFrame before attempting to calculate the mean, to handle edge cases where no Dystopian books might be present (though not strictly necessary for this specific dataset). The ground truth includes this robustness, which is a good practice. However, given the prompt and the provided data, the generated code directly solves the problem presented without any errors.",command-r7b-12-2024,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
import numpy as np
import scipy.stats as stats

# Load dataset
data = pd.read_csv('water_potability.csv')

# Define potable and non-potable categories
data['Potability'] = 'Potable'
data.loc[data['pH'] < 7, 'Potability'] = 'Non-Potable' # Example classification rule

# 1) T-tests for mean comparisons
for param in ['pH', 'Hardness', 'Solids', 'Sulfate', 'Conductivity']:  # Add more parameters as needed
    t_stat, p_value = stats.ttest_ind(data.loc[data['Potability']...",2,"The generated code has severe logical errors and fails to address most of the requirements correctly.

**Major Issues:**

1.  **Incorrect Potability Definition:** The most critical error is `data['Potability'] = 'Potable'` followed by `data.loc[data['pH'] < 7, 'Potability'] = 'Non-Potable'`. This completely overrides the 'Potability' column loaded from the CSV, which already contains the true binary classification (0 or 1). Instead, it creates an arbitrary classification based on pH, which is not what the task asked for. The task implies using the existing 'Potability' column for grouping. This alone invalidates all subsequent statistical tests as they are performed on incorrectly classified groups.

2.  **Missing Value Handling:** The code completely ignores missing values, which were explicitly mentioned as a requirement for handling (`fillna()`). The ground truth code correctly identifies and imputes missing values.

3.  **Incomplete T-tests:**
    *   It lists only a subset of parameters (`'pH', 'Hardness', 'Solids', 'Sulfate', 'Conductivity'`) and indicates a need to ""Add more parameters."" The task implied performing this for *all* relevant water quality parameters.
    *   It does not calculate Cohen's d for the t-tests as explicitly required. There's a comment `... Calculate Cohen's d for each significant t-test ...` but no actual implementation.

4.  **Incorrect Chi-square Goodness-of-Fit Test:**
    *   `scipy.stats.chi2_goodness_fit` does not exist in `scipy.stats`. The correct function for a goodness-of-fit test is `scipy.stats.chisquare`.
    *   The `expected` calculation is fundamentally flawed: `expected = np.array([[counts.mean(), counts.mean()]] * 2)`. For a 2-category observed count, `chisquare` expects a 1D array for observed and a 1D array for expected. The expectation for a goodness-of-fit test against a uniform distribution should be `[total/2, total/2]`, not based on `counts.mean()`.
    *   The output variables `dof` and `expected_sum` are not returned by `chisquare`.
    *   The test is designed to verify if the observed potability distribution matches *an expected distribution* (e.g., 50/50 split), not if it *follows* some vague ""expected distribution"" that is then miscalculated.

5.  **Incomplete Normality Tests:**
    *   It only performs the Shapiro-Wilk test on the 'Potable' subset (`data['Potability'] == 'Potable'`). The task asked for normality tests on ""key numerical features,"" generally implying the features themselves, or arguably for both groups, but not just one. The ground truth applied it to the full dataset for each feature.
    *   Again, it lists only a subset of parameters.

6.  **Missing Effect Size Calculation:** The prompt explicitly asked for Cohen's d calculation and interpretation, which is completely absent except for a placeholder comment.

7.  **Output and Interpretation:** While it attempts interpretation, the underlying statistical logic is so flawed due to the above issues that the interpretations are meaningless. It lacks the structure and comprehensiveness of the ground truth code's output (e.g., test statistics, p-values, and clear interpretation for *each* test).

**Minor Issues:**

*   Imports are okay.
*   It assumes the CSV is present, which is acceptable for a benchmark.

Overall, the code demonstrates a significant misunderstanding of the specified statistical tests and how to apply them to the given task and data. The fundamental error in re-defining 'Potability' renders the entire analysis invalid.",command-r7b-12-2024,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ROC Curve
import matplotlib.pyplot as plt

# 1) Data Preprocessin...",3,"The generated code has significant issues in correctness, completeness, and code quality.

**Critical Issues (Correctness and Completeness):**

*   **Data Preprocessing:**
    *   `data.dropna(inplace=True)`: The ground truth uses `SimpleImputer(strategy='median')` which is a more robust and common approach for handling missing values in such datasets than simply dropping rows, especially since missing values are only in a few columns. Dropping rows significantly reduces the dataset size and can lead to biased models.
    *   `data[['feature1', 'feature2', ...]] = scaler.fit_transform(data[['feature1', 'feature2', ...]])`: This line is a placeholder (`...`) and not functional, indicating a lack of understanding of how to apply scaling to all feature columns correctly in the dataset. The ground truth correctly identifies features and applies scaling to them.
    *   `X = data.drop('target', axis=1)`, `y = data['target']`: This assumes 'target' is the column name, but the task specifies 'Potability'. The code doesn't adjust for the actual column name of the target variable.
    *   **Data Leakage during Scaling and Imputation**: The generated code performs `dropna()` and `fit_transform` on the entire dataset *before* the train-test split. This is a severe form of data leakage, as statistics from the test set are used to preprocess the training set, leading to overly optimistic performance estimates. The ground truth correctly performs train-test split first, then imputes and scales separately on train and test sets (fitting only on train).

*   **Model Training and Evaluation:**
    *   **No Train-Test Split for initial evaluation**: The models are trained and evaluated on the *entire* dataset (`X`, `y`), not on `X_train`, `y_train` and `X_test`, `y_test`. This invalidates the evaluation metrics as they represent training accuracy, not generalization performance. The prompt explicitly asks for an 80/20 train-test split.
    *   **ROC Curve**: The `ROC Curve` import is incorrect (`from sklearn.metrics import ..., ROC Curve`). It should be `RocCurveDisplay` or functions like `roc_curve`. The implementation of the ROC curve plotting is highly flawed and not functional (`roc_curve(y, y_pred, # ...single label <=> multi-label conversion)`). It attempts to iterate over 2 classes but doesn't correctly handle the output of `roc_curve` or plot multiple ROC curves.
    *   **Metric Averaging**: Using `average='weighted'` for `precision_score`, `recall_score`, `f1_score` without explicitly stating it's for imbalanced classes or demonstrating understanding of its implication is questionable for a binary classification task where `average='binary'` (for positive class) or no averaging (for multi-class, then specified for positive class) is often preferred. The ground truth code correctly uses the default binary average for positive class (`average='binary'`) where appropriate (implied by just `precision_score`, etc., not specifying `pos_label`).
    *   **Confusion Matrix Plotting**: Uses `plt.matshow(cm)` which is very basic and doesn't provide labels or a clear visualization like `sns.heatmap` or `ConfusionMatrixDisplay` used in the ground truth.

*   **Cross-Validation:**
    *   **Only trains a single model**: The k-fold cross-validation section *only* trains `RandomForestClassifier()` (`model = RandomForestClassifier()`) regardless of the initial models evaluated. It needs to perform CV for *all* specified models.
    *   **Incorrect metric calculation in CV**: `getattr(model, metric)(y_test, y_pred)` is fundamentally incorrect. Metrics are functions from `sklearn.metrics`, not methods of the `model` object. It should call `accuracy_score(y_test, y_pred)`, `precision_score(y_test, y_pred)`, etc.
    *   **No preprocessing within CV**: The CV loop does not include the preprocessing steps (imputation, scaling). This is a critical error in cross-validation; preprocessing must be
        applied *within* each fold to prevent data leakage, typically using a pipeline as shown in the ground truth.

*   **Best Model Selection and Feature Importance:**
    *   **Incorrect Best Model Selection Logic**: `best_model = max(model_scores['model'], key=lambda x: max(model_scores['scores'][model_scores['model'].index(x)]))` is syntactically complex and logically flawed given how `model_scores` was populated (appending individual metric results, `model_scores['scores']` is a flat list). It will likely not correctly identify the best model based on aggregated scores.
    *   **Feature Importance Error**: `feature_importances = best_model.feature_importances_` will fail if `best_model` is `LogisticRegression` or `SVC` as they don't have this attribute. The ground truth correctly handles different ways to get feature importance for different model types.
    *   `top_features = X.columns[feature_importances.argsort()[-10:]].tolist()`: This assumes `feature_importances` is an array of numerical values and correctly sorted, and assumes `X` is a DataFrame. This is fine if the above issues were fixed, but the conditional handling of different model types is missing.

**Code Quality Issues:**

*   **Lack of Structure/Comments**: The code is very monolithic, missing clear sectioning and comments that the ground truth provides.
*   **Placeholder Code**: The `data[['feature1', 'feature2', ...]]` indicates incomplete thought or copy-pasting without full adaptation.
*   **Repetitive Code**: Evaluation loop iterates through `model_name, model` tuple where model_name is the actual model object, not a string name like in the ground truth.
*   **Imports**: `ROC Curve` import is incorrect; `matplotlib.pyplot` is imported as `plt` but not explicitly used for `plt.figure()` in all cases, inconsistent.
*   **Readability**: Variable names are sometimes generic (`data`, `model`).

Overall, the generated code fails to meet most of the requirements correctly, contains significant logical and implementation errors, and exhibits poor coding practices.",command-r7b-12-2024,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_correlation
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
import plotly.express as px

# Load the dataset
data = pd.read_csv('water_potability.csv')

# 1. Feature Engineering
data['pH_ratio'] = data['pH'] / data['Chloramin...",4,"The generated code attempts to address the task but fails significantly in several key areas, leading to a low score.

**Correctness & Completeness Issues:**

1.  **Missing Value Handling:** The ground truth meticulously handles missing values by imputing them with the mean. The generated code completely skips this crucial data preprocessing step, which will likely lead to errors or incorrect results, especially when performing numerical operations or training models.

2.  **Inadequate Feature Engineering:**
    *   The prompt specifically asks for ""new meaningful features such as ratios between chemical parameters (e.g., pH/Chloramines, Hardness/Solids), interaction terms, and polynomial features.""
    *   The generated code provides only two ratio features (`pH_ratio`, `Hardness_ratio`), one interaction term (`Interaction`), and one polynomial feature (`Polynomial_pH`). This is a minimal implementation, not the ""comprehensive"" feature engineering expected. It severely lacks the variety and thoughtfulness demonstrated in the ground truth (e.g., `Sulfate_Chloramines_Ratio`, `Sulfate_Organic_carbon_Interaction`, multiple polynomial terms).
    *   The polynomial features are not generated using `PolynomialFeatures` from `sklearn.preprocessing`, which is the standard and flexible way to create them. The manual `data['pH'] ** 2` is highly limited.

3.  **Top 5 Correlated Features Identification:**
    *   The generated code uses `correlation_matrix.unstack().abs().nlargest(5).index.tolist()`. This snippet will return the top 5 highest absolute correlations *overall elements* in the matrix, which includes self-correlations (e.g., pH with pH) and symmetric correlations (e.g., pH with Chloramines and Chloramines with pH will both appear if they are among the top).
    *   The task explicitly asks for ""top 5 features most correlated with water potability."" This means filtering the correlation matrix to only the 'Potability' column (or 'Potable' in the generated code's naming) and then selecting the top 5 *features*, excluding the target variable itself. The generated code does not correctly identify these specific features.

4.  **Feature Selection Incorrect Usage:**
    *   **Target Variable Naming:** The generated code uses `data['Potable']` as the target, but the provided dataset (`water_potability.csv`) uses 'Potability' (as confirmed by the ground truth). This is an immediate error.
    *   **`SelectKBest` Usage:** `selected_kbest.fit_transform(data, data['Potable'])` is incorrect. `SelectKBest` should be applied to the *features* (X) not the entire DataFrame, and the `fit_transform` method returns the *transformed data* (the selected features), not the names of the selected features. The task asks to identify the *names* of the selected features.
    *   **`f_correlation` vs `f_classif`:** For classification tasks, `f_classif` is generally more appropriate for `SelectKBest` as it calculates ANOVA F-value for the samples. `f_regression` (or `f_correlation` if that's a custom function which it's not by default in sklearn) is for regression. Even if `f_correlation` was intended for initial correlation, `f_classif` is the standard for KBest with classification.
    *   **`RFE` Usage:** Similar to `SelectKBest`, `selected_features_rfe = rfe.fit_transform(data, data['Potable'])` returns the transformed data, not the feature names. Also, RFE should be applied to the features (X) not the whole DataFrame. The `n_features_to_select` parameter for RFE is not defined in the code, which would default to half the features, not a specific number. The ground truth correctly defines this and extracts feature names using `selector_rfe.support_`.

5.  **Visualization Deficiencies:**
    *   **Feature Importance Plots:** The generated code explicitly states `... (Code to generate feature importance plots using Random Forest model)` implying it's not implemented, which is a major omission.
    *   **Pairwise Scatter Plots:** The implementation of pairwise scatter plots `for feature in top_features: plt.figure... sns.scatterplot(x=feature, y='Potable', data=data)` will generate separate plots for each feature versus 'Potable'. This is not a ""pairwise scatter plot"" in the general sense (like `sns.pairplot`) which shows relationships between *all pairs* of selected features, including their distributions. The prompt asks for ""pairwise scatter plots for top correlated features"", implying interaction between these features themselves. The ground truth correctly uses `sns.pairplot`.

6.  **Comparison of Feature Sets:** The generated code includes `... (Code to compare selected features from different methods and analyze their overlap)`, indicating this crucial analysis step is also not implemented.

**Code Quality Issues:**

1.  **Lack of Separation of X and y:** The generated code performs feature engineering directly on the `data` DataFrame, which still contains the 'Potability' column. It should separate features (X) and target (y) *before* feature engineering, as common practice and as done in the ground truth. This prevents the target from being accidentally engineered or involved in feature selection methods where it shouldn't be.
2.  **Hardcoded Column Names (`Potable` vs `Potability`):** As mentioned, the inconsistency in target variable name is a problem.
3.  **Missing Imports/Unused Imports:** `train_test_split`, `GridSearchCV`, `plotly.express` are imported but not used, hinting at copy-pasted code or incomplete thought processes. `PolynomialFeatures` is needed but not imported.
4.  **No `epsilon` for Ratios:** The ratio calculations `data['pH'] / data['Chloramines']` can lead to `inf` or `NaN` if the denominator is zero. The ground truth correctly adds a small `epsilon` to prevent this.
5.  **No Standardization:** Features are not standardized before applying `SelectKBest` and `RFE`, which is good practice and often necessary for many ML algorithms.

In summary, the generated code provides a skeleton with significant logical flaws, missing implementations, and incorrect usage of `sklearn` functions, failing to meet the full requirements and robustness of the ground truth.",command-r7b-12-2024,LLM_judge_config
