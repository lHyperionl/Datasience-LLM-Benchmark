task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""winequality-red.csv"")

# 1. List column names and data types
print(""Column Names and Data Types:"")
print(df.dtypes)
print(""\n"")

# 2. Descriptive statistics for numerical columns
numerical_cols = df.select_dtypes(include=['number']).columns
print(""Descriptive Statistics for Numerical Columns:"")
print(df[numerical_cols].describe())
print(""\n"")

# Calculate and report median separately, as describe() only provides mean
print(""Medi...",7,"The generated code correctly loads the dataset, identifies data types reasonably well using `df.dtypes`, and calculates descriptive statistics for numerical columns using `describe()`. It also correctly reports the total number of rows and columns.

However, there are a few areas for improvement to achieve a higher score:
1.  **Data Type Identification Detail**: While `df.dtypes` is useful, the ground truth explicitly iterates through columns to identify if they are ""Numerical"" or ""Categorical/Ordinal"" based on specific pandas API types (`is_numeric_dtype`, `is_object_dtype`, `is_categorical_dtype`). The generated code's `df.dtypes` output is more basic.
2.  **Median Reporting**: The `describe()` method *does* include the median (50th percentile). The generated code unnecessarily calculates it separately after `describe()` which is redundant.
3.  **Categorical/Ordinal Column Identification**: This is the weakest point. The task specifically asks to ""Identify any columns that appear to be categorical or ordinal."" The generated code explicitly states ""None identified using a basic heuristic"" and acknowledges ""In this dataset, there are no truly categorical columns without additional analysis."" The ground truth provides a more nuanced approach, correctly identifying that while all columns are numerically represented, `quality` is often treated as an ordinal variable. The generated code falls short of attempting this identification or providing a more insightful observation beyond ""None identified.""
4.  **Error Handling**: The ground truth includes robust error handling for `FileNotFoundError`, which is good practice for loading datasets. The generated code lacks this.",gemini-2.0-flash,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('video_games_sales.csv')
except FileNotFoundError:
    print(""Error: video_games_sales.csv not found. Place it in the current directory."")
    exit()

# Display the first 10 rows
print(""First 10 rows:"")
print(df.head(10))
print(""\n"")

# Show the shape of the dataset
print(""Shape of the dataset:"")
print(df.shape)
print(""\n"")

# List all column names and their data types
print(""Column names and data types:"")
print(df.dtype...",7,"The generated code correctly performs most of the requested EDA tasks. It loads the dataset, displays the first 10 rows, shows the shape, lists column names and datatypes, displays unique values, calculates summary statistics for sales columns, and identifies missing values.

Areas for improvement:
- The display of column names and data types uses `df.dtypes` which is acceptable but `df.info()` from the ground truth provides a more comprehensive overview including non-null counts and memory usage, which is often preferred for EDA.
- When displaying top 10 games, it only shows 'Name' and 'Global_Sales'. The ground truth includes 'Platform', 'Genre', 'Publisher' which offers more context and is generally more useful when identifying ""top games"".
- The ground truth includes more robust error handling and checks (e.g., checking if columns exist before trying to access them for sorting or unique values, and better messaging for missing files). While the generated code has basic file error handling, it lacks checks for column existence.
- The output formatting in the ground truth is much clearer with headers and separators using print statements, making the output easier to read and distinguish different sections of the analysis. The generated code's output is functional but less organized.",gemini-2.0-flash,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values in the Year column
median_year = df['Year'].median()
df['Year'] = df['Year'].fillna(median_year)

# Remove games with missing Publisher information
df = df.dropna(subset=['Publisher'])

# Create 'Years_Since_Release' column
current_year = 2023
df['Years_Since_Release'] = current_year - df['Year']

# Remove duplicate games
df = df.drop_duplicates(subset=['Name', 'Platform'])

# Fil...",7,"The generated code addresses most of the requirements correctly and achieves the main goal of cleaning the dataset. However, there are a few areas for improvement that prevent it from getting a higher score, primarily related to robustness and completeness.

**Strengths:**
- Correctly loads the dataset.
- Handles missing 'Year' values with the median.
- Correctly removes rows with missing 'Publisher'.
- Correctly creates the 'Years_Since_Release' column.
- Correctly removes duplicates based on 'Name' and 'Platform'.
- Correctly filters out low-selling games.
- Displays the shape and head of the cleaned dataset.

**Areas for Improvement:**
- **Data Type Handling (Year column):** The ground truth explicitly converts the 'Year' column to numeric (`pd.to_numeric(errors='coerce')`) first, which is crucial if the year column might contain non-numeric strings. After filling NaNs, it also converts it to `int`. The generated code assumes 'Year' is already numeric or can be implicitly converted, which might lead to errors if the column contains non-numeric entries.
- **Robustness of 'Years_Since_Release':** The ground truth ensures `Years_Since_Release` values are non-negative. If a `Year` value is in the future (e.g., 2024 and `CURRENT_YEAR` is 2023), the generated code would produce negative values, which is unrealistic for ""years since release."" This is a minor but important detail for data quality.
- **Robustness of 'Global_Sales':** Similar to 'Year', the ground truth explicitly converts 'Global_Sales' to numeric before filtering, handling potential errors. The generated code assumes 'Global_Sales' is numeric, which could cause issues if it's not (e.g., if it's stored as strings with commas or other non-numeric characters).
- **Missing Information/Clarity:** The print statements in the ground truth for each step (`print(f""Shape after ..."")`) provide excellent debugging and progress tracking, which is missing in the generated code. While not strictly a ""cleaning"" step, it's good practice for debugging and understanding the data transformation.
- **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions, making it more robust against common issues, which is absent in the generated code.",gemini-2.0-flash,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 2. Bar chart: Top 10 gaming platforms by total global sales
platform_sales = df.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False).head(10)
plt.figure(figsize=(12, 6))
platform_sales.plot(kind='bar')
plt.title('Top 10 Gaming Platforms by Total Global Sales')
plt.xlabel('Platform')
plt.ylabel('Total Global Sales (Millions)')
plt.xticks(rotation=45)
...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being a strong solution.

**Positive points:**
*   It attempts to create all the requested visualization types (bar, pie, histogram, scatter, line).
*   Titles and labels are generally present for most plots.
*   Uses `matplotlib.pyplot` and `pandas` as requested.

**Areas for improvement/issues:**
1.  **Robustness (Data Loading & Preprocessing):** The ground truth includes a `try-except` block for file loading, which is good practice. More importantly, it handles common data issues like converting 'Year_of_Release' and sales columns to numeric types and dropping NaNs. The generated code assumes the data is perfectly clean and numeric, which is often not the case with real-world datasets, especially 'Year' which can be inconsistent in CSVs.
2.  **Pie Chart Readability:** The generated code plots all genres in the pie chart. For datasets with many unique genres (as is likely the case here), this makes the pie chart unreadable due to too many small slices and overlapping labels. The ground truth addresses this by aggregating less frequent genres into an ""Other"" category, which is a standard best practice for pie charts.
3.  **Line Plot Year Column:** The generated code uses `df.groupby('Year')`. The problem description states the dataset is 'video_games_sales.csv', and typical datasets of this kind usually have a 'Year_of_Release' column, not just 'Year'. The ground truth correctly identifies and uses 'Year_of_Release'. This indicates an assumption without checking the actual dataset column names.
4.  **Sales Units in Plots:** Labels like ""Global Sales (Millions)"" are more descriptive and helpful for interpretation. The generated code omits this for some plots, like the histogram.
5.  **Aesthetics and Clarity:** While the ground truth's use of `df.dropna` for sale columns might be slightly aggressive, it highlights careful thought about preparing the data for plotting. The generated code also lacks some common matplotlib enhancements seen in the ground truth, such as `plt.grid()`, specific colors (`color='skyblue'`, `color='lightcoral'`, etc.), or adjusting marker size (`s=10` for scatter plot). The `xticks` on the line plot are an attempt at improvement but might still lead to crowded labels depending on the year range.

The lack of data cleaning/preprocessing for `Year` and sales columns could lead to errors or unexpected behavior, especially for the line plot and sales calculations, if the raw data is not perfectly clean.",gemini-2.0-flash,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd

# 1. Load the dataset and display the first 10 rows
df = pd.read_csv('diabetes.csv')
print(""First 10 rows:\n"", df.head(10))

# 2. Show the shape of the dataset (rows and columns)
print(""\nShape of the dataset:"", df.shape)

# 3. List all column names and their data types
print(""\nColumn data types:\n"", df.dtypes)

# 4. Calculate the percentage of diabetic vs non-diabetic patients (Outcome column)
outcome_counts = df['Outcome'].value_counts(normalize=True) * 100
prin...",6,"The generated code correctly performs most of the requested EDA steps.

**Strengths:**
*   Successfully loads the CSV and displays the first 10 rows.
*   Correctly identifies and displays the shape of the dataset.
*   Accurately calculates and presents the percentage of diabetic vs non-diabetic patients.
*   Correctly identifies and displays missing values for each column.
*   Provides basic summary statistics for numerical columns.

**Areas for Improvement:**
*   **Column Names and Data Types (Requirement 3):** `df.dtypes` only shows data types. `df.info()` is preferred as it also lists column names and non-null counts, providing a more comprehensive overview as requested.
*   **Identification of Zero Values for Missing Data (Requirement 7):** The generated code simply counts zeros across *all* columns. The ground truth specifically lists columns (e.g., 'Glucose', 'BloodPressure', etc.) where zeros might represent missing data, acknowledging that columns like 'Pregnancies' or 'Outcome' can legitimately be zero. This distinction is crucial for accurate EDA.
*   **Robustness:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is a good practice for real-world scenarios.
*   **Clarity of Output:** While functional, the output formatting could be improved for readability, similar to the ground truth which uses clear headers and separators for each section.
*   **Unnecessary Output:** The `(df == 0).sum()` for zero values counts zeros in columns like 'Pregnancies' and 'Outcome' which are legitimately zero and not indicators of missing data an expert would be looking for. The GT carefully filters this.",gemini-2.0-flash,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv(""diabetes.csv"")

# Identify columns with zero values that represent missing data
cols_to_replace = [""Glucose"", ""BloodPressure"", ""SkinThickness"", ""Insulin"", ""BMI""]

# Replace zero values with NaN
df[cols_to_replace] = df[cols_to_replace].replace(0, np.nan)

# Handle missing values by filling them with the median
for col in cols_to_replace:
    df[col] = df[col].fi...",7,"The generated code addresses most requirements but has a few inaccuracies and areas for improvement.

**Positive Points:**
*   Correctly loads the dataset.
*   Correctly identifies and replaces zeros with NaN for specified columns.
*   Correctly fills NaN values with the median for the specified columns.
*   Successfully removes duplicate rows.
*   Scales numerical features using `StandardScaler`.
*   Displays the shape and head of the cleaned dataset.

**Areas for Improvement/Corrections:**
*   **BMI Category Bins:** The `BMI_Category` bins are slightly incorrect in the generated code (`25` vs `24.9`, `30` vs `29.9`). The ground truth uses `[0, 18.5, 24.9, 29.9, np.inf]` which correctly aligns with the standard BMI categories. The `right=False` in the generated code also means the upper bound is exclusive, whereas for BMI categories, the upper bound is usually inclusive for the lower category (e.g., Normal is up to 24.9 inclusive). The ground truth's `right=True` combined with the correct bins is more accurate.
*   **Robustness to Missing Columns:** The generated code directly indexes `df[cols_to_replace]` and iterates through `cols_to_replace` assuming all columns exist. The ground truth code includes checks (`if col in df.columns:`) before attempting operations on columns, which makes it more robust to variations in the input dataset.
*   **Scaling All Numerical Features:** The generated code explicitly lists all numerical columns for scaling. While this works for *this specific dataset*, the ground truth uses `df.select_dtypes(include=np.number).columns.tolist()` and then removes 'Outcome', which is a more general and robust approach for scaling all numerical features *except* the target variable, regardless of the dataset's exact column names (as long as 'Outcome' is the known target).
*   **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code lacks this.
*   **Clarity on Original vs. Cleaned Shape:** The ground truth prints both the original and cleaned dataset shapes, providing better context for the cleaning process. The generated code only shows the cleaned shape.
*   **Explicit Imports:** While standard, the ground truth explicitly imports `numpy as np`, `pandas as pd`, etc., as per good practice, though the generated code uses `np` and `pd` correctly, implicitly implying the common aliases.

Overall, the generated code performs the core tasks, but could be made more robust, accurate (especially with BMI categories), and provide clearer output by incorporating some of the practices seen in the ground truth.",gemini-2.0-flash,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('diabetes.csv')

# 1. Correlation Heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Diabetes Data')
plt.show()

# 2. Histograms for Numerical Features
data.hist(bins=20, figsize=(15, 10))
plt.suptitle('Histograms of Numerical Features', x=0.5, y=0.92, fontsize=16)
plt.show()


# 3. B...",8,"The generated code successfully creates most of the requested visualizations.

Strong points:
- All required types of plots (heatmap, histograms, box plots, pie chart, scatter plots, pair plot) are generated.
- Proper imports are included.
- Basic titles are present for all plots.
- The use of `hue='Outcome'` for scatter and pair plots correctly incorporates the requested coloring.

Areas for improvement to achieve a higher score:
- **Loading the dataset:** The generated code directly loads the CSV without any error handling (`try-except FileNotFoundError`). The ground truth includes robust loading with print statements for initial data exploration.
- **Histograms:** While `data.hist()` is a quick way to generate histograms, it doesn't allow for custom styling (like KDE) or individual control over titles/labels per subplot as easily as looping through columns with `sns.histplot`. The ground truth provides more granular control and better presentation for histograms.
- **Box Plots:** The `subplot` arrangement for box plots is `(3, 3, i + 1)`, which might not fit all features perfectly if there are more than 9 numerical features. The ground truth dynamically calculates `n_rows` and `n_cols` for better adaptability. Additionally, the titles and labels in the generated code are slightly less descriptive than the ground truth (e.g., ""Feature vs Outcome"" vs. ""Feature by Diabetes Outcome""). The overall suptitle for box plots is placed before `plt.tight_layout()`, causing it to overlap in some cases.
- **Pie Chart:** The labels for the pie chart (`['No Diabetes', 'Diabetes']`) are slightly less explicit than the ground truth's `['Non-Diabetic (0)', 'Diabetic (1)']`. The ground truth also adds `shadow=True` and `explode` for better visual appeal.
- **Legends:** The scatter plots and pair plot could benefit from explicit legend labels like in the ground truth (e.g., 'Non-Diabetic', 'Diabetic') instead of just 0 and 1.
- **General Styling:** The ground truth code sets a `sns.set_style(""whitegrid"")` at the beginning, which provides a consistent aesthetic. The generated code lacks such general styling.

Overall, the code is very functional and meets most requirements, but lacks some of the polish, robustness, and detailed customization found in the ground truth.",gemini-2.0-flash,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

# 1. Load the dataset and display the first 10 rows
df = pd.read_csv(""world-happiness-report-2021.csv"")
print(""First 10 rows:\n"", df.head(10))

# 2. Show the shape of the dataset (rows and columns)
print(""\nShape of the dataset:"", df.shape)

# 3. List all column names and their data types
print(""\nColumn names and data types:\n"", df.dtypes)

# 4. Display the unique regional indicators and count of countries per region
print(""\nUnique regional indicators:\n"", df['Re...",8,"The generated code correctly addresses all requirements of the task. It properly loads the dataset, displays the first 10 rows, shows the shape, lists column names and data types, provides unique regional indicators and counts, identifies the top 10 happiest countries, calculates summary statistics for specified factors, and identifies missing values.

Strengths:
- All task requirements are met.
- The use of `df.head(10)`, `df.shape`, `df.dtypes`, `df['Regional indicator'].unique()`, `df['Regional indicator'].value_counts()`, `df.sort_values().head(10)`, `df[happiness_factors].describe()`, and `df.isnull().sum()` demonstrates good knowledge of pandas functionalities for EDA.
- The code is concise and readable.

Areas for Improvement:
- **Error Handling**: The generated code does not include any error handling for `FileNotFoundError` when reading the CSV, which is present in the ground truth and is a good practice for robust code. If the file is not found, the script would crash.
- **Output Clarity**: While the requested information is printed, the ground truth code uses separators (`=""*70""`) and more explicit labels (e.g., ""1. First 10 rows of the dataset:"") to improve the readability and demarcation of different output sections. The generated code's print statements are a bit more terse.
- **Robustness for Column Existence**: The generated code assumes all specified columns (`'Regional indicator'`, `'Ladder score'`, `'Country name'`, and `key_happiness_factors`) always exist. The ground truth includes checks (`if 'Regional indicator' in df.columns:` etc.) which make it more robust if the dataset structure changes or is unexpectedly different.
- **`df.info()` vs `df.dtypes`**: For listing column names and data types, `df.info()` (used in ground truth) provides more comprehensive information, including non-null counts and memory usage, which is often more useful in EDA than just `df.dtypes`.

Overall, a very good attempt that fulfills the core requirements, but could be enhanced with better error handling, output formatting, and robustness for real-world scenarios.",gemini-2.0-flash,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Load the dataset
df = pd.read_csv(""world-happiness-report-2021.csv"")


# Handle missing values


def handle_missing_values(df):
    for col in df.columns:
        if df[col].isnull().any():
            if pd.api.types.is_numeric_dtype(df[col]):
                df[col].fillna(df[col].mean(), inplace=True)
            else:
                df[col].fillna(df[col].mode()[0], inplace=Tru...",7,"The generated code correctly addresses most of the requirements but has some minor issues and areas for improvement.

**Strengths:**
- **Correct Imports:** All necessary libraries are imported.
- **Missing Value Handling:** The `handle_missing_values` function correctly identifies and fills missing numerical values with the mean and categorical with the mode.
- **Outlier Removal:** The `remove_outliers_iqr` function is correctly implemented for the 'Ladder score'.
- **Standardization:** `StandardScaler` is used.
- **Encoding:** `LabelEncoder` is used for 'Regional indicator'.
- **Display:** The final shape and head are printed.

**Areas for Improvement/Minor Issues:**
- **Loading Dataset Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` which is good practice for robustness. The generated code lacks this.
- **Happiness_Category Definition:** The bins for `pd.cut` are slightly off compared to the request: `(High: >6.5, Medium: 4-6.5, Low: <4)`. The generated code uses `bins=[0, 4, 6.5, 10]` which correctly categorizes 'Low' as (0, 4], 'Medium' as (4, 6.5], and 'High' as (6.5, 10]. However, the problem statement defined 'Medium' as `4-6.5` (inclusive of 4 and 6.5) and 'Low' as `<4`. The `np.select` in the ground truth handles these edge cases more precisely, ensuring strict adherence to the definition.
- **GDP_Category Labels:** The request asks for 'GDP_Category' based on quartiles. The generated code uses `labels=False` for `pd.qcut`, which returns integer indicators for bins (0, 1, 2, 3), not explicit labels like 'Q1', 'Q2', 'Q3', 'Q4' as in the ground truth, which is more descriptive.
- **Well_being_Score Calculation:** The request asks for the ""average of Social support, Healthy life expectancy, and Freedom"". The generated code correctly averages these three. The ground truth explicitly uses `mean(axis=1)`, which is a clear and direct way to express the average. Both are functionally equivalent for three columns.
- **Standardization - Columns to Exclude:** The generated code explicitly removes ""Ladder score"" and ""GDP_Category"" from `numerical_cols` before scaling.
    - ""Ladder score"" *should* be scaled/standardized as it was a numerical feature, unless there's a specific reason not to. The problem statement says ""Standardize numerical features using StandardScaler"", ladder score is a numerical feature. The ground truth scales all numerical features present at that stage, including 'Ladder score'.
    - ""GDP_Category"" after `pd.qcut(..., labels=False)` becomes numerical (0, 1, 2, 3) but is essentially categorical. Standardizing this numerical representation of categories might not be ideal; it's better left as is or encoded differently if it were to be treated as a numerical feature. The ground truth gives it explicit labels, making it effectively a categorical feature.
    - The ground truth correctly identifies remaining numerical columns for scaling, including `Well_being_Score` which was added.
- **Function Usage:** While breaking down into functions like `handle_missing_values` and `remove_outliers_iqr` is good for modularity, for a single script run, directly implementing them might be slightly more concise, as done in the ground truth. This is a style preference, not a correctness issue, but the ground truth's directness might be slightly more ""expert"" for this specific task given its sequential nature.

Overall, the generated code performs most of the required transformations, but the details in `Happiness_Category` and especially the column selection for `StandardScaler` diverge from the optimal solution and the problem statement's intent for 'Ladder score'.",gemini-2.0-flash,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go

# 1. Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# 2. World Map Visualization of Happiness Scores
fig_map = px.choropleth(df,
                     locations=""Country name"",
                     locationmode='country names',
                     color=""Ladder score"",
                     hover_name=""Country name"",
            ...",7,"The generated code addresses most of the requirements and produces the requested visualizations. However, there are several areas for improvement:

**Points for Improvement:**

1.  **Column Renaming:** The ground truth renames columns for better readability and consistency (`Ladder score` to `Happiness Score`, `Logged GDP per capita` to `GDP per Capita`, etc.). The generated code uses the original column names, which can be less intuitive when plotting.
2.  **Dataset Loading Robustness:** The ground truth includes a `try-except` block for `FileNotFoundError`, making the code more robust. The generated code assumes the file exists.
3.  **Visualization Aesthetics & Formatting:**
    *   **Scatter Plots:** The generated scatter plots lack `hue` for regional indicators, which was implicitly expected from the task description (or inferred from the ground truth's common aesthetic choices for similar datasets). Also, no legends are provided.
    *   **Heatmap:** The heatmap's correlation columns are removed explicitly (`df.drop(columns=['Ladder score', 'upperwhisker', ...])`). The ground truth selects specific numerical columns, which is a safer and more precise approach, as dropping columns might inadvertently remove relevant features or fail if column names change. The generated heatmap also lacks rotated x-axis labels which improves readability.
    *   **Box Plots:** The x-axis labels are rotated, but the overall figure size might be improved for readability if more than 14 different regions are present.
    *   **Titles and Labels:** While titles and axis labels are present, the ground truth often provides slightly more descriptive titles (e.g., including the year ""2021"").
4.  **Radar Chart Normalization:** The generated radar chart uses `plotly.graph_objects.go.Scatterpolar` directly, but it doesn't normalize the feature values (`range=[df[factors].min().min(), df[factors].max().max()]`). This can lead to misleading interpretations if the features have vastly different scales. The ground truth uses `MinMaxScaler` for normalization, which is crucial for a comparative radar chart. This is a significant drawback for the radar chart's effectiveness.
5.  **Imports:** `numpy` and `math` are not imported but might be useful for more complex operations or if the code were to be expanded. While not explicitly used in the generated code's current form, it's good practice for data science scripts.
6.  **Default Plot Styles:** The ground truth sets default `sns.set_style` and `plt.rcParams` for consistent aesthetics, which the generated code omits.

The code is functional and produces most required plots, but the lack of normalization in the radar chart and some aesthetic/robustness issues prevent a higher score.",gemini-2.0-flash,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd

# 1. Load the dataset and parse the 'date' column
df = pd.read_csv(""synthetic_covid19_data.csv"", parse_dates=['date'])

# 2. Display the first 10 rows
print(""First 10 rows:\n"", df.head(10))

# 3. Show the shape of the dataset
print(""\nShape of the dataset:"", df.shape)

# 4. List column names and their data types
print(""\nColumn names and data types:\n"", df.dtypes)

# 5. Calculate and display the total sum of 'total_cases' and 'total_deaths' grouped by 'continent'
c...",7,"The generated code addresses most of the requirements correctly, showcasing good Python and Pandas usage for basic EDA.

Here's a breakdown:

**Strengths:**
*   **Correctness:**
    *   Loads data and parses dates correctly.
    *   `df.head(10)`, `df.shape`, `df.dtypes`, `df.isnull().sum()` are all correct implementations for their respective tasks.
    *   Grouping and summing for continental summary is correct.
    *   The `idxmax()` approach for finding the latest date per location is a valid and efficient way to achieve that step.
*   **Completeness:** Addresses all 8 points of the task.
*   **Code Quality:** Imports are proper. The code is structured logically with comments for each task.

**Areas for Improvement to achieve a higher score:**

*   **Error Handling (Crucial for robust code):** The ground truth includes `try-except` blocks for file loading, which is a best practice. The generated code lacks this, making it prone to errors if the file is not found.
*   **Comprehensive `df.info()` vs. `df.dtypes`:** While `df.dtypes` fulfills the literal requirement of listing column names and data types, `df.info()` (used in the ground truth) provides much more comprehensive information, including non-null counts and memory usage, which is standard practice in initial EDA. This is a minor point but makes the `df.info()` approach more ""expert"" like.
*   **Summary Statistics (Minor Enhancement):** The generated code uses `.describe()` which is great for a quick overview, but then manually adds median. The ground truth explicitly used `df.agg(['mean', 'median', 'min', 'max', 'std'])` which directly yields all required statistics in one go, including the mean, min, max, and std for both columns simultaneously in a clean table, fulfilling the requirement more directly than printing two `.describe()` outputs and then two individual medians.
*   **Handling Missing/Non-numeric Data:** The ground truth includes more robust handling of potential NaNs or non-numeric values, especially when calculating sums or statistics (e.g., `dropna(subset=['continent'])`, `pd.to_numeric(errors='coerce')`). While the provided synthetic data might be clean, robust code anticipates real-world data issues. This is particularly relevant for `total_cases` when identifying top locations and for `new_cases`/`new_deaths` summary statistics.
*   **Clarity in Output:** The ground truth adds more descriptive print statements (e.g., ""--- First 10 rows of the dataset ---"") and uses newlines (`\n`) for better readability of the output, which is a good practice for reports.",gemini-2.0-flash,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('winequality-red.csv', sep=';')

# 1. Calculate and report the average 'quality'
average_quality = df['quality'].mean()
print(f""Average Quality: {average_quality:.2f}"")

# 2. Group by 'quality' and calculate average 'alcohol'
quality_alcohol = df.groupby('quality')['alcohol'].mean()
print(""\nAverage Alcohol content by Quality:"")
print(quality_alcohol)

# 3. Create a bar chart
...",7,"The generated code correctly performs two out of three main tasks: calculating the mean quality and grouping by quality to get average alcohol content. The visualization is also implemented correctly, although it's missing some of the refinements present in the ground truth code (like explicit data labels on bars).

However, there's a critical error in loading the CSV file: `pd.read_csv('winequality-red.csv', sep=';')`. The `winequality-red.csv` file is comma-separated, not semicolon-separated. This will lead to an error where the entire row is read as a single column and subsequent operations will fail or produce incorrect results. This renders the executed code incorrect.

Other minor improvements could include:
- A `try-except` block for file loading for robustness.
- Adding data labels to the bar chart for easier readability of exact values.
- Setting `xticks` automatically or based on the grouped data's index rather than hardcoding `range(3,9)`, although this might be acceptable for this specific dataset.
- More descriptive print statements for clarity, similar to the ground truth.

Fixing the `sep=';'` issue is crucial for the code to run correctly. Had that been correct, the score would likely be higher (8 or 9).",gemini-2.0-flash,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# 1. Parse 'date' column as datetime objects
df['date'] = pd.to_datetime(df['date'])

# 2. Handle missing values
df['hosp_patients'] = df['hosp_patients'].fillna(0)
df['icu_patients'] = df['icu_patients'].fillna(0)

numerical_cols = ['positive_rate', 'reproduction_rate']
for col in numerical_cols:
    df[col] = df[col].fillna(df[col].mean())

# 3. Create 'mortality_rate' column
df...",8,"The generated code correctly addresses most of the requirements.

**Strong Points:**
- Correctly loads the dataset and parses the 'date' column.
- Handles missing values for `hosp_patients` and `icu_patients` by filling with 0.
- Handles missing values for `positive_rate` and `reproduction_rate` by filling with the mean.
- Correctly calculates `mortality_rate` and handles division by zero by replacing `inf` and `NaN` with 0.
- Correctly calculates `vaccination_rate_per_capita` and handles division by zero.
- Filters the dataset for the year 2021.
- Displays the shape and head of the cleaned dataset.
- Uses proper imports (`pandas` and `numpy`).

**Areas for Improvement:**
- **Robustness in missing value handling for numerical columns:** The prompt explicitly stated ""For other numerical columns like 'positive_rate', 'reproduction_rate', fill missing values with the mean of their respective column."" The generated code does this. However, the ground truth explicitly adds `pd.to_numeric(errors='coerce')` for 'positive_rate', 'reproduction_rate', 'total_deaths', 'total_cases', 'people_fully_vaccinated', and 'population' before performing calculations or mean imputation. While the generated code might work if these columns are already numeric or can be implicitly coerced, explicitly converting them makes the code more robust against non-numeric data that might appear in a real-world dataset. The generated code assumes these critical columns are already numeric, which might not always be the case for ""synthetic"" data that could contain strings or other non-numeric entries.
- **Error Handling for File Not Found:** The generated code lacks a `try-except` block for `FileNotFoundError` when loading the CSV, which was present in the ground truth. While not explicitly asked for in the prompt, it significantly improves robustness for a data processing script.
- **Clarity in Output:** The prompt asked to ""Display the shape of the cleaned dataset and the first 5 rows with the new columns."" The ground truth includes descriptive print statements like ""Shape of the cleaned dataset (records from 2021):"" and ""First 5 rows of the cleaned dataset (records from 2021) with new columns:"". The generated code merely prints `df.shape` and `df.head()`, which is functional but less user-friendly.",gemini-2.0-flash,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Parse the 'date' column
df['date'] = pd.to_datetime(df['date'])

# Time series line plot of 'new_cases' for 'India'
india_data = df[df['location'] == 'India']
plt.figure(figsize=(12, 6))
plt.plot(india_data['date'], india_data['new_cases'])
plt.title('New COVID-19 Cases in India Over Time')
plt.xlabel('Date')
plt.ylabel('New Cases')
plt.xticks(...",6,"The generated code addresses most of the task requirements but has several areas for improvement in terms of correctness, completeness, and adherence to best practices for data visualization.

**Strengths:**
*   Successfully loads the data and parses the date column.
*   Creates all five requested plot types (time series, bar, scatter, histogram, box plot).
*   Uses `matplotlib.pyplot` and `seaborn` as requested.
*   Includes basic titles and labels for all charts.

**Areas for Improvement (leading to a score of 6):**

1.  **Time Series Plot (India 'new_cases'):**
    *   **Minor:** Uses `plt.plot` directly instead of `sns.lineplot`. While `plt.plot` works, `sns.lineplot` often provides better default aesthetics and more robust handling of time series data, especially with different aggregations or confidence intervals, which is common with Seaborn. The plot also lacks a grid which would improve readability.
2.  **Bar Chart ('total_deaths' by 'continent'):**
    *   **Major Correctness Issue:** The code calculates `df.groupby('continent')['total_deaths'].sum()`. This is incorrect for ""total deaths"". `total_deaths` is a cumulative metric. Summing all `total_deaths` entries for all dates within a continent will grossly overstate the actual total deaths, as it sums the same deaths multiple times across different dates. The ground truth correctly addresses this by taking the *latest* `total_deaths` for each *location* and then summing those by *continent*. This is a critical logical error in data aggregation for cumulative metrics.
    *   **Minor:** Uses `df.plot(kind='bar')` which is functional but `sns.barplot` would allow for more immediate customization like color palettes and ordering.
3.  **Scatter Plot ('gdp_per_capita' vs 'total_vaccinations'):**
    *   **Correctness/Completeness:** While it correctly attempts to use the latest data per location (`df.groupby('location').last().reset_index()`), it doesn't handle `NaN` values for `gdp_per_capita` or `total_vaccinations` explicitly. This can lead to points not being plotted or errors if the column is entirely `NaN`.
    *   **Missing Features:** It does not use `hue` or `size` for additional dimensions as the task implies (though the task description for scatter plot wasn't explicit about these, the ground truth showcases how it's done effectively). The ground truth's use of `sns.scatterplot` with `hue='continent'` and `size='population'` significantly enhances the plot's informational value. It also doesn't apply log scales to axes, which are often beneficial for these types of financial/cumulative metrics.
4.  **Histogram ('reproduction_rate'):**
    *   **Minor:** Uses `plt.hist` directly instead of `sns.histplot`. `sns.histplot` often includes a KDE by default, which can be helpful for understanding the distribution, and offers more customizable options like `stat='density'` for normalization.
5.  **Overall Code Quality:**
    *   **Missing Error Handling:** The generated code lacks any `try-except` blocks for file loading, which is crucial for robust scripts.
    *   **Aesthetics:** The plots are functional but lack the enhanced aesthetics provided by `seaborn.set_theme(style=""whitegrid"")` from the ground truth. They also miss adding grids to plots, which often aids readability.
    *   **Labels/Titles:** While present, they are less descriptive than the ground truth's, e.g., ""New COVID-19 Cases in India Over Time"" vs ""Daily New COVID-19 Cases in India Over Time"".
    *   **Efficiency for Scatter Plot:** `df.groupby('location').last().reset_index()` is correct for finding the latest entry, but could be combined with `sort_values` and `drop_duplicates` for more explicit control, though for this dataset, `last()` in group by will work. Explicitly dropping `NaN`s *before* plotting is better practice.

To improve, the code needs to address the cumulative sum issue for total deaths, incorporate more `seaborn` features (like `hue`/`size` and log scales for scatter, KDE for histogram), enhance plot aesthetics (grids, seaborn themes), and add robust error handling.",gemini-2.0-flash,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('clash_royale_cards.csv')
except FileNotFoundError:
    print(""Error: clash_royale_cards.csv not found in the current directory."")
    exit()


# 1. List column names and data types
print(""Column Names and Data Types:"")
print(df.dtypes)

# 2. Descriptive statistics for numerical columns
numerical_cols = ['elixirCost', 'Win Rate', 'Usage']

print(""\nDescriptive Statistics for Numerical Columns:"")
for col in numerical_cols...",9,"The generated code correctly addresses all parts of the task: loading the CSV, identifying column types, calculating descriptive statistics for the specified columns, and reporting the total number of rows. It includes proper imports and handles the `FileNotFoundError`.

**Strong Points:**
*   **Correctness:** All calculations and reports are accurate.
*   **Completeness:** All aspects of the prompt are covered.
*   **Error Handling:** Includes basic error handling for `FileNotFoundError`.
*   **Readability:** The code is well-structured and easy to understand with clear print statements.

**Minor Areas for Improvement (for an even higher score):**
*   **Descriptive Statistics Output:** While correct, the output for descriptive statistics is printed column by column. The ground truth's use of `.agg()` provides a more consolidated and typically preferred tabular output from pandas, which is easier to read when comparing multiple statistics across columns.
*   **Column Names Listing:** The prompt asked to ""List all column names and identify the data type of each column."" The generated code directly prints `df.dtypes`, which shows both, but doesn't explicitly *list* the column names separately in the way the ground truth did (`for col in df.columns: print(f""- {col}"")`). This is a minor stylistic difference, as `df.dtypes` does implicitly show column names.
*   **Robustness of File Path:** The ground truth includes a `file_path` variable, which makes it slightly more maintainable if the filename changes.
*   **More Specific Error Handling:** The ground truth includes a more general `except Exception as e` for broader error trapping during file loading, which is good practice.

Overall, it's a very good solution that fully implements the requirements. The suggestions are mostly about refining the output format and minor stylistic choices for maximizing clarity and robustness.",gemini-2.0-flash,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""clash_royale_cards.csv"")

# Filter for elixirCost <= 3
df_filtered = df[df['elixirCost'] <= 3]

# Select Card and Win Rate columns
df_selected = df_filtered[['Card', 'Win Rate']]

# Sort by Win Rate in descending order
df_sorted = df_selected.sort_values(by='Win Rate', ascending=False)

# Display the result
print(df_sorted)
```",9,"The generated code correctly implements all the requirements of the task: loading the CSV, filtering by `elixirCost`, selecting the specified columns, and sorting by `Win Rate` in descending order. It's concise and readable.

The only minor area for improvement, preventing a perfect score, is the lack of robust error handling (e.g., `FileNotFoundError` or checking for column existence) which is present in the ground truth. While not explicitly asked for in the prompt's short description, it's good practice for production-ready code. However, given the prompt's focus on the core data manipulation steps, the generated code performs excellently.",gemini-2.0-flash,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
try:
    df = pd.read_csv(""clash_royale_cards.csv"")
except FileNotFoundError:
    print(""Error: clash_royale_cards.csv not found. Please ensure the file is in the current directory."")
    exit()

# Generate histogram of elixirCost
plt.figure(figsize=(10, 6))
plt.hist(df['elixirCost'], bins=range(0, 11), edgecolor='black')  # Bins from 0 to 10
plt.title('Distribution of Elixir Cost in Clash Royale Cards')
plt.xlabel...",8,"The generated code successfully loads the dataset and generates a histogram for `elixirCost` with a title and labeled axes, fulfilling all core requirements. The error handling for `FileNotFoundError` is also good.

Areas for improvement:
1.  **Binning for `elixirCost`**: While `range(0, 11)` for bins is a reasonable attempt, typical elixir costs are 1 through 10. Using `range(1, 12)` would ensure specific bins for each integer cost from 1 to 11 (if 11 exists in data), similar to the GT. Or, more precisely, if the data only has costs from 1 to 10, then `range(1, 12)` might be optimal to show each integer cost clearly and as discrete categories, rather than a continuous distribution.
2.  **Using Seaborn**: The prompt mentioned using Matplotlib *or* Seaborn. While Matplotlib is used, Seaborn offers more visually appealing plots with less code (e.g., `sns.histplot`). The GT uses Seaborn, which is a good practice for data visualization.
3.  **Missing a small detail in error handling**: The `exit()` call is good, but a more informative message about what columns *are* available if 'elixirCost' were missing (not applicable here as it works, but a robustness point in a real-world scenario) or a general `Exception` catch would make it more robust. (The GT includes this for completeness, though it's not strictly required by the prompt's simplicity).

Overall, it's a very good attempt that meets the primary objectives.",gemini-2.0-flash,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# Load the iris.csv dataset
iris_data = pd.read_csv(""iris.csv"")

# 1. List all column names and identify the data type of each column
print(""Column Names and Data Types:"")
print(iris_data.dtypes)
print(""\n"")

# 2. Calculate and report basic descriptive statistics for numerical columns
numerical_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
print(""Descriptive Statistics for Numerical Columns:"")
print(iris_data[numerical_cols].describe())
...",8,"The generated code correctly performs all the required tasks: loading the dataset, listing column data types, calculating descriptive statistics for the specified numerical columns, and reporting unique species counts.

**Strong Points:**
*   Correctly loads the CSV using `pd.read_csv()`.
*   Successfully uses `df.dtypes` to show column names and their data types.
*   Effectively uses `df[numerical_cols].describe()` to get mean, std, min, and max.
*   Correctly identifies the median separately using `df[numerical_cols].median()`.
*   Accurately uses `df['Species'].value_counts()` and `df['Species'].nunique()` to report species information.
*   Includes proper import for pandas.

**Areas for Improvement (Why not a 10):**
*   **Column Names Listing:** The task asked to ""List all column names AND identify the data type of each column."" While `df.dtypes` shows both, explicitly printing `df.columns.tolist()` first, as done in the ground truth, would have been slightly more explicit for ""listing column names.""
*   **Descriptive Statistics Presentation:** The `describe()` method already includes mean, std, min, and max. Calculating median separately and printing it afterwards is slightly less consolidated than an approach that presents all required statistics together (e.g., using `agg` which allows custom selection, or formatting the output of `describe` to include the median if needed, or simply accepting `describe` as comprehensive enough). The prompt specifically asked for ""mean, median, standard deviation, min, max,"" and `describe()` alone misses the median. The separate printing of median addresses this but could be more integrated.
*   **Robustness:** The code lacks error handling for `FileNotFoundError` or other exceptions during file loading, which is a good practice for real-world applications (addressed by the `try-except` block in the ground truth).
*   **Clarity of Output:** The output formatting could be improved with more descriptive headers or separators, similar to the ground truth, to clearly distinguish between different sections of the report. The ground truth uses `---` and print statements to label each section clearly.",gemini-2.0-flash,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

# Load the iris dataset
iris_data = pd.read_csv('iris.csv')

# Filter the dataset for 'Iris-setosa'
setosa_data = iris_data[iris_data['Species'] == 'Iris-setosa']

# Select and display 'SepalLengthCm' and 'SepalWidthCm'
result = setosa_data[['SepalLengthCm', 'SepalWidthCm']]

print(result)
```",8,"The generated code correctly performs all requested operations: loading the CSV, filtering by 'Species' == 'Iris-setosa', and selecting/displaying the specified columns. It uses proper imports and good structure.

However, it lacks robust error handling, specifically for `FileNotFoundError` (if 'iris.csv' doesn't exist) or `KeyError` (if expected columns are missing), which are present in the ground truth. While the prompt implied a straightforward execution without necessarily requiring extensive error handling as criteria, the ground truth demonstrates best practices for production-ready code. For a simple benchmark where the input is guaranteed, this might be acceptable. If the task implicitly requires robust handling often seen in larger data science contexts, its score would be lower. Given the explicit requirements of loading, filtering, and selecting, it fully meets those.",gemini-2.0-flash,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the iris dataset
iris_data = pd.read_csv(""iris.csv"")

# Create a scatter plot with color-coded species
plt.figure(figsize=(10, 6))

species = iris_data['Species'].unique()
colors = ['red', 'green', 'blue'] # can be expanded or modified for more species

for i, sp in enumerate(species):
    subset = iris_data[iris_data['Species'] == sp]
    plt.scatter(subset['SepalLengthCm'], subset['SepalWidthCm'], 
                color=colo...",9,"The generated code correctly solves the task by loading the CSV, creating a scatter plot with `SepalLengthCm` vs `SepalWidthCm`, coloring points by `Species`, and including a title and axis labels. All required imports are present.

Strengths:
- **Correctness:** The plot is generated as requested, and the coloring by species works.
- **Completeness:** All explicit requirements from the prompt (load CSV, scatter plot, SepalLengthCm vs SepalWidthCm, color by Species, title, x-label, y-label, imports) are met.
- **Readability:** The code is concise and easy to understand.
- **Dynamic Species Coloring:** It correctly iterates through unique species to color points, which is a robust approach.

Areas for minor improvement (not critical enough to deduct significant points):
- **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` and `KeyError`, which is good practice for more robust applications. The generated code lacks explicit error handling.
- **Hardcoded Colors:** While functional for the three Iris species, `colors = ['red', 'green', 'blue']` is hardcoded. If the dataset had more species, this would lead to an `IndexError`. A more robust solution might use a colormap or a color palette generator (e.g., from `matplotlib.cm` or `seaborn`).
- **Axis Labels:** The x-label ""Sepal Length (cm)"" and y-label ""Sepal Width (cm)"" are slightly different from the column names ""SepalLengthCm"" and ""SepalWidthCm"" used in the ground truth, which were just derived from the column names. While descriptive, using the exact column names as labels is often a direct way to meet such requirements unless formatting is explicitly asked for. This is a minor stylistic difference.",gemini-2.0-flash,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# 1. List column names and data types
print(""Column Names and Data Types:"")
print(df.dtypes)

# 2. Descriptive statistics for numerical columns
numerical_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']
print(""\nDescriptive Statistics for Numerical Columns:"")
print(df[numerical_cols].describe())

# Calculate and report median and standard deviation
print(""\nMedian for Numerical Columns:"")
print(df[numerical_cols]...",7,"The generated code correctly loads the dataset, lists column names and data types, and calculates descriptive statistics for the specified numerical columns. It also correctly reports the value counts for the `HeartDisease` column.

Areas for improvement:
1.  **Descriptive Statistics Presentation**: The descriptive statistics are partially correct. The prompt asked for specific statistics (mean, median, standard deviation, min, max) in a concise manner. The generated code prints `describe()` which includes 25%, 50% (median), and 75% percentiles, and then prints median and standard deviation separately. The ground truth correctly extracts and presents only the requested statistics from `describe()` and renames '50%' to 'median' for clarity, which is a more complete and cleaner way to present the requested data.
2.  **Error Handling**: The generated code lacks error handling for `FileNotFoundError` or other potential issues during CSV loading, which is present in the ground truth and is good practice for robust code.
3.  **Clarity in Output**: While the output is technically correct for most parts, the ground truth adds more descriptive print statements (e.g., ""Successfully loaded..."", ""Interpretation: 0 = No Heart Disease, 1 = Heart Disease"") which improves the user experience and clarity of the output.
4.  **Column Existence Check**: The ground truth includes checks to ensure the specified numerical columns and the `HeartDisease` column actually exist in the DataFrame before attempting operations on them, which makes the code more robust against variations in input data.",gemini-2.0-flash,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""heart.csv"")

# Filter the dataset
filtered_df = df[(df['Age'] > 50) & (df['HeartDisease'] == 1)]

# Select and display the desired columns
selected_columns = filtered_df[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]

print(selected_columns)
```",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, filters the data based on the specified age and heart disease conditions, and then selects and displays only the required columns. The code is concise and directly solves the problem.

Strong points:
- **Correctness**: Achieves all filtering and selection tasks accurately.
- **Conciseness**: The solution is very direct and to the point.
- **Completeness**: All filtering and display requirements are met.

Areas for minor improvement (not critical for the task but good practice):
- **Error Handling**: The ground truth includes `try-except` blocks for `FileNotFoundError` and `KeyError`, which are good practices for robust code, especially when dealing with external files and potential column name issues. The generated code lacks this.
- **Function Encapsulation**: While not explicitly asked for, wrapping the logic in a function (as in the ground truth) with a `file_path` argument makes the code more reusable and testable.
- **Clarity of Output**: The ground truth adds a `print` statement before displaying the DataFrame, providing context (`""Filtered Data...""`).

Overall, for a direct solution to the given programming task, the generated code is excellent. The minor points are about robustness and code structure typically found in more production-ready code, not about directly solving the immediate problem.",gemini-2.0-flash,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

# Load the dataset
try:
    wine_data = pd.read_csv('winequality-red.csv', sep=';')
except FileNotFoundError:
    print(""Error: winequality-red.csv not found in the current directory."")
    exit()

# Filter wines based on alcohol and pH
filtered_wines = wine_data[(wine_data['alcohol'] > 12) & (wine_data['pH'] < 3.3)]

# Find the wine with the highest quality
if not filtered_wines.empty:
    best_wine = filtered_wines.loc[filtered_wines['quality'].idxmax()]

    # D...",9,"The generated code correctly implements all the requested functionalities: filtering with compound conditions, finding the maximum quality in the subset, and displaying the features of the selected wine. It includes `pandas` import and handles the `FileNotFoundError`.

Areas for minor improvement:
1.  **Read CSV Separator:** The ground truth code assumes the default comma separator for CSV, which is typical for `winequality-red.csv`. The generated code uses `sep=';'`. While sometimes datasets use semicolons, for `winequality-red.csv`, it's usually comma-separated. Using the correct separator or assuming a comma would align better with general usage for this specific dataset. If the dataset truly uses semicolons, then this is correct; otherwise, it would lead to an error or misinterpretation of columns. Assuming typical `winequality-red.csv` format, this is a slight deviation.
2.  **Output Format:** The `print(best_wine)` output format is a pandas Series default, which is generally acceptable. The ground truth uses `to_string()` for potentially better presentation, especially if the Series has many features and `print()` might truncate. This is a minor stylistic difference.
3.  **Function Encapsulation:** The ground truth encapsulates the logic within a function, which is good practice for reusability and modularity. The generated code is a script, which is acceptable for a small task but less robust.
4.  **Error Handling (General):** The ground truth includes more comprehensive error handling (e.g., `KeyError` for missing columns, general `Exception` catch). The generated code only handles `FileNotFoundError`.

Overall, the core logic is sound and achieves the task's requirements. The points for improvement are mostly around robustness, best practices, and slight output formatting or dataset specifics.",gemini-2.0-flash,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv(""heart.csv"")

# Histogram for Age distribution
plt.figure(figsize=(8, 6))
plt.hist(df['Age'], bins=20, color='skyblue', edgecolor='black')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Bar chart for Sex distribution
plt.figure(figsize=(6, 4))
sex_counts = df['Sex'].value_counts()
plt.bar(sex_counts.index, sex_counts.values, color=['sa...",7,"The generated code correctly loads the dataset and generates both the histogram for `Age` and the bar chart for `Sex` distribution with titles and labeled axes. This fulfills the core requirements of the task.

However, there are a few areas for improvement:
1.  **Readability/Labels for Sex:** The `Sex` column in `heart.csv` typically uses 0 and 1 to represent Female and Male, respectively. The generated bar chart will show 0 and 1 on the x-axis, which is not as descriptive as 'Female' and 'Male'. The ground truth addresses this by mapping these numerical values to more meaningful labels. This improves the understanding of the plot significantly.
2.  **Using `seaborn` fully:** While `matplotlib` is used for both plots, the prompt specified ""Use matplotlib or seaborn for plotting."" The ground truth leverages `seaborn`'s `histplot` and `countplot`, which often produce more aesthetically pleasing and informative plots (e.g., `histplot` with KDE line, `countplot` handling categories directly).
3.  **Error Handling (minor omission):** The ground truth includes robust error handling for `FileNotFoundError` and `KeyError`, which is good practice for real-world scripts. The generated code lacks this, though it wasn't explicitly requested in the task description for the *generated* code.
4.  **Plot Refinements:** The ground truth includes `figsize` settings, `edgecolor` for histogram bars, `tight_layout()`, and `grid` for better visual presentation, which are good practices. The generated code has reasonable `figsize` but could benefit from other visual polish.",gemini-2.0-flash,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 1. List column names and data types
print(""Column Names and Data Types:"")
print(df.dtypes)

# 2. Convert 'streams' to numeric, handling errors
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# Report NaNs in 'streams'
print(""\nNumber of NaNs in 'streams' column:"", df['streams'].isnull().sum())

# 3. Calculate descriptive statistics for numerical columns
numerical_cols = ['streams', 'bpm', 'da...",7,"The generated code correctly performs most of the required tasks: loading the CSV, converting the 'streams' column to numeric with error handling, reporting NaNs, calculating descriptive statistics for the specified columns, and counting unique artists.

However, there are several areas for improvement to reach a higher score:

1.  **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. The generated code lacks this, which is crucial for robust data analysis scripts.
2.  **Reporting NaNs in `streams`:** While the generated code reports the *count* of NaNs, the ground truth code also prints a sample of rows containing those NaNs, which is more informative for debugging or understanding the data quality issue. It also explicitly shows the dtype before and after conversion.
3.  **Descriptive Statistics Specificity:** The ground truth code specifically extracts `mean`, `median` (represented by '50%'), `std`, `min`, `max` from the `.describe()` output, aligning more precisely with ""basic descriptive statistics"" often requested. The generated code prints the full `.describe()` output, which includes additional metrics like 'count' and '25%', '75%'. While not strictly wrong, it's less precise to the prompt's request for ""mean, median, std, min, max"".
4.  **Implicit Column Existence Checks:** The generated code assumes all specified columns (`bpm`, `danceability_%`, etc., and `artist(s)_name`) exist. The ground truth includes checks (`if col in df.columns`), making it more robust against variations in column names or missing columns in the dataset.
5.  **Output Clarity/Formatting:** The ground truth provides more descriptive print statements and uses more clear separation between tasks (e.g., using `---` dividers), which improves readability. For instance, `df.info()` provides a more comprehensive summary than just `df.dtypes` for point 1.",gemini-2.0-flash,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

def load_and_process_spotify_data(file_path=""spotify_2023.csv""):
    """"""
    Loads, filters, and processes Spotify data from a CSV file.

    Args:
        file_path (str): Path to the CSV file.  Defaults to ""spotify_2023.csv"".

    Returns:
        pandas.DataFrame: DataFrame containing the top tracks released in 2023,
                         sorted by streams in descending order. Returns an empty
                         DataFrame if the file is not found or no ...",9,"The generated code successfully completes all the requirements of the task. It loads the CSV, filters by `released_year` = 2023, selects the specified columns, handles the `streams` column as numeric with error coercion, and sorts the results in descending order.

**Strong points:**
- Encapsulates the logic within a function, `load_and_process_spotify_data`, which makes the code more modular and reusable.
- Includes `try-except` block for `FileNotFoundError`, improving robustness.
- Explicitly uses `.copy()` after filtering (`df[df['released_year'] == 2023].copy()`) to prevent `SettingWithCopyWarning`, which is good practice.
- Clear variable names.
- Uses an `if __name__ == '__main__':` block for executing the function, typical for Python scripts.

**Areas for minor improvement:**
- The ground truth code included `dropna(subset=['streams'])` before sorting to handle NaN values resulting from coercion, which ensures that only valid numeric streams are considered for sorting and display. The generated code sorts without dropping NaNs, meaning rows with non-numeric `streams` will appear at the end after sorting. While not explicitly stated as a requirement to drop NaNs, it's generally good practice when sorting by a numeric column to consider how NaNs should be handled for a ""top N"" list. This is a subtle difference and doesn't explicitly violate the prompt, but it's an edge case consideration.
- The prompt asks to ""display the `track_name`, `artist(s)_name`, and `streams` columns"", which the `print(top_2023_tracks)` accomplishes. The ground truth also adds a descriptive print statement, ""Songs released in 2023, sorted by streams (descending):"", which slightly enhances user understanding, but this is a minor stylistic point.",gemini-2.0-flash,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 1. Histogram of 'bpm'
plt.figure(figsize=(8, 6))
plt.hist(df['bpm'], bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Beats Per Minute (BPM)')
plt.xlabel('BPM')
plt.ylabel('Frequency')
plt.show()

# 2. Scatter plot of 'danceability_%' vs 'energy_%'
plt.figure(figsize=(8, 6))
plt.scatter(df['danceability_%'], df['energy_%'], color='coral', alpha=0.7)
plt...",9,"The generated code successfully loads the dataset, generates an appropriate histogram for `bpm`, and creates a scatter plot for `danceability_%` vs `energy_%`. Both plots have titles and appropriately labeled axes, fulfilling all key requirements of the task. The imports are correct, and the code structure is clear and concise.

Areas for minor improvement (not critical for the assigned score but good practice):
- The `plt.show()` calls are placed immediately after each plot definition. While this works, it's generally better practice to define all plots and then call `plt.show()` once at the very end of the script to display all figures simultaneously. This is more efficient if you're generating many plots within a single script.
- Error handling for file not found or missing columns could be added for robustness, as shown in the ground truth, but this specific task did not explicitly ask for it.",gemini-2.0-flash,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: youtube_trendings.csv not found in the current directory."")
    exit()

# 1. List column names and data types
print(""Column Names and Data Types:"")
print(df.dtypes)
print(""\n"")

# 2. Clean 'Video views', 'Likes', and 'Dislikes' columns
cols_to_clean = ['Video views', 'Likes', 'Dislikes']
for col in cols_to_clean:
    df[col] = df[col]...",9,"The generated code correctly addresses all requirements of the task: it loads the data, lists column names and types, cleans the specified numeric columns by removing commas, converting to numeric, and filling NaNs with 0. It then calculates and prints descriptive statistics for these columns, fills missing 'Category' values with 'Unknown', and reports the unique category count. The structure is logical, and imports are correct.

Minor areas for improvement:
- The `df.dtypes` output for column names and data types is less informative than `df.info()`. `df.info()` also provides non-null counts, which is useful for data quality assessment.
- There's no check if 'Category' column exists before trying to fill NaNs or count unique values, although for this specific task, it's implied the column will exist. The ground truth includes checks for columns to make the code more robust.
- The descriptive statistics output could be formatted more cleanly (e.g., using f-strings for alignment or rounding).
- The `numpy` import is not used.",gemini-2.0-flash,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('youtube_trendings.csv', encoding='utf-8')
except FileNotFoundError:
    print(""Error: youtube_trendings.csv not found."")
    exit()

# Clean and convert 'Video views' column
def clean_views(views):
    try:
        return float(views.replace(',', ''))
    except:
        return np.nan

df['Video views'] = df['Video views'].apply(clean_views)
df['Video views'] = df['Video views'].fillna(0)

# Filter th...",9,"The generated code correctly addresses all parts of the task. It properly loads the CSV, cleans the `Video views` column (handling commas, converting to numeric, and filling NaNs with 0), filters the data as required, selects the correct columns, and sorts the result. The use of a custom function for cleaning is a valid approach, though `pd.to_numeric` with `errors='coerce'` is typically more idiomatic and potentially more performant for large datasets. The addition of `encoding='utf-8'` to `read_csv` is a good practice for compatibility, and `np.nan` import and usage are appropriate. The `copy()` to avoid `SettingWithCopyWarning` is also a good detail for robustness.

Areas for minor improvement:
1. While the custom `clean_views` function works, `pd.to_numeric(df['Video views'].astype(str).str.replace(',', '', regex=False), errors='coerce')` is generally the more idiomatic and often more efficient pandas way to perform the conversion on an entire Series.
2. The `try-except` block for `FileNotFoundError` could include the filename in the print statement for clearer user feedback, as seen in the ground truth.",gemini-2.0-flash,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Likes' and 'Video views' columns
for col in ['Likes', 'Video views']:
    df[col] = df[col].astype(str).str.replace(',', '', regex=True)
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

# Scatter plot of Video views vs. Likes
plt.figure(figsize=(10, 6))
plt.scatter(df['Video views'], df['Likes'], alpha=0.5)
plt.title('Video Views vs. Likes')
plt.xl...",9,"The generated code correctly performs all the requested tasks: loading the CSV, cleaning the specified columns (removing commas, converting to numeric with error handling, and filling NaNs with 0), and generating the scatter plot with the correct title and axis labels. The code is concise and efficient.

One minor improvement for completeness could be adding error handling for the `pd.read_csv()` call, similar to the ground truth, to gracefully handle cases where the file might not exist. This is a common best practice for file operations. However, for a direct comparison against the core task requirements, the current solution is excellent. The use of `regex=True` in `str.replace` is also unnecessary for a literal comma and could be `False` for marginal performance gains, but it doesn't affect correctness here.",gemini-2.0-flash,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""mobiles_2025.csv"")

# Clean 'Mobile Weight' column
df['Mobile Weight'] = pd.to_numeric(df['Mobile Weight'].str.replace('g', ''), errors='coerce')

# Clean 'RAM' column
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')

# Clean 'Battery Capacity' column
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')

# Clean 'Launched Price (USA)' column
df['Launche...",9,"The generated code correctly addresses almost all requirements of the task. It loads the CSV, cleans the specified columns (`Mobile Weight`, `RAM`, `Battery Capacity`, `Launched Price (USA)`) by removing units and handling conversion errors to `NaN`. It then correctly prints the column names and data types, calculates and prints descriptive statistics for the cleaned numeric columns, and reports the count of unique values in 'Company Name'.

One minor area for improvement is in the cleaning logic for 'Mobile Weight', 'RAM', and 'Battery Capacity'. The `str.replace` methods are called without `.str.strip()`. While this might not cause issues with the specific dataset if there are no leading/trailing spaces before the unit, it's a good practice to include `str.strip()` to handle such cases robustly, as done in the ground truth. For `Launched Price (USA)`, it correctly handles two replacements and then conversion.

Also, the descriptive statistics output is slightly verbose as it prints the full `.describe()` output for each column, rather than just mean, median, std, min, max as specified by the ground truth's specific selection (`['mean', '50%', 'std', 'min', 'max']`). However, the required statistics are present within the output.

Overall, it's a very good solution that successfully achieves the task.",gemini-2.0-flash,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""mobiles_2025.csv"")

# Clean 'Launched Price (India)'
df['Launched Price (India)'] = df['Launched Price (India)'].str.replace('INR ', '', regex=False)
df['Launched Price (India)'] = df['Launched Price (India)'].str.replace(',', '', regex=False)
df['Launched Price (India)'] = pd.to_numeric(df['Launched Price (India)'], errors='coerce')

# Clean 'RAM'
df['RAM'] = df['RAM'].str.replace(' GB', '', regex=False)
df['RAM'] = pd.to_numer...",9,"The generated code successfully completes the task by loading the dataset, cleaning the 'Launched Price (India)' and 'RAM' columns, filtering the data, selecting the specified columns, and sorting the results.

Strong points:
- Correctly loads the CSV file.
- Accurately cleans both 'Launched Price (India)' (removing 'INR ' and commas, then converting to numeric with `errors='coerce'`) and 'RAM' (removing ' GB' and converting to numeric with `errors='coerce'`).
- Correctly filters for 'Apple' and `Launched Price (India)` less than 100,000.
- Selects the required 'Model Name', 'RAM', and 'Launched Price (India)' columns.
- Correctly sorts by 'Launched Price (India)' in ascending order (default for `sort_values`).

Areas for minor improvement:
- The cleaning step for 'RAM' uses `df['RAM'].str.replace(' GB', '', regex=False)`. While this works, the ground truth uses `'GB'`, which is slightly more robust if there isn't always a space before 'GB' (though typical in the given context, it's a minor detail). The ground truth also explicitly casts to `str` before cleaning, which handles potential non-string types more gracefully, although `str.replace` usually handles non-string values by coercing them to `NaN` if they're not strings, which is then handled by `to_numeric(errors='coerce')` anyway.
- The code lacks explicit `try-except` blocks for file loading, which was present in the ground truth for robustness (though not explicitly requested in the prompt's main requirements, it's good practice).
- The prompt asks to ""display the cleaned `RAM`"", the code stores it in `result`, but the final `print(result)` does display it, so this requirement is met.",gemini-2.0-flash,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv(""mobiles_2025.csv"")

# Clean the RAM column
df['RAM'] = df['RAM'].str.replace('GB', '', regex=False)
df['RAM'] = pd.to_numeric(df['RAM'], errors='coerce')

# Clean the Battery Capacity column
df['Battery Capacity'] = df['Battery Capacity'].str.replace('mAh', '', regex=False)
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'], errors='coerce')

# Create the scatter p...",9,"The generated code correctly addresses almost all requirements of the task. It properly loads the CSV, cleans both 'RAM' and 'Battery Capacity' columns by removing units and converting to numeric, handles conversion errors using `errors='coerce'`, and then generates a scatter plot. The plot correctly uses the cleaned data, colors points by 'Company Name', and includes a title and axis labels.

Minor areas for improvement:
1.  **Chaining cleaning operations**: While valid, applying `.str.replace` and then `pd.to_numeric` on the *same column* `df['RAM']` and `df['Battery Capacity']` overwrites the original, unclean data. In a real-world scenario, it's often safer or clearer to create new columns (e.g., `df['RAM_cleaned']`) for the transformed data, as done in the ground truth. This prevents potential issues if the original string content is needed elsewhere or if debugging.
2.  **Robustness for file not found**: The generated code doesn't include a `try-except` block for `FileNotFoundError` when loading the CSV, unlike the ground truth. While not explicitly asked for in the prompt's requirements, it's a good practice for robustness.
3.  **Stripping whitespace**: The ground truth includes `.str.strip()` after removing 'GB'/'mAh'. While not always necessary depending on the dataset, it's a good defensive programming practice to ensure there are no leading/trailing spaces left after replacement, which could interfere with `pd.to_numeric`. The generated code omits this.
4.  **Clarity on axis labels**: The ground truth labels the axes with units (e.g., 'RAM (GB)', 'Battery Capacity (mAh)'), which is good practice as it explicitly states what the numeric values represent post-cleaning. The generated code has just 'RAM' and 'Battery Capacity'.

Overall, the code is highly functional and correctly implements the core logic.",gemini-2.0-flash,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('titanic.csv')

# Display the first 5 rows
print(""First 5 rows:\n"", df.head())

# Show the shape of the dataset
print(""\nShape of the dataset:"", df.shape)

# List column names and their data types
print(""\nColumn names and data types:\n"", df.dtypes)

# Calculate survival rate
survival_rate = df['Survived'].mean() * 100
print(""\nSurvival Rate: {:.2f}%"".format(survival_rate))

# Show the count of missing values for each column
prin...",9,"The generated code is highly correct and addresses all requirements of the task. It successfully loads the dataset, displays the first 5 rows, shows the shape, lists column names and datatypes, calculates the survival rate, and counts missing values. The code quality is good, using standard pandas operations efficiently.

One minor area for improvement related to robustness is the lack of error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth. While not explicitly requested in ""basic exploratory data analysis on the titanic.csv dataset"" it is good practice for real-world scenarios.

Another minor point is printing `df.info()` vs `df.dtypes`. While `df.info()` provides a more comprehensive overview including non-null counts and memory usage, `df.dtypes` directly addresses ""List all column names and their data types"" as requested. Both are acceptable for this prompt.

The calculation `df['Survived'].mean() * 100` for survival rate is a more concise and equally correct way to achieve the same result as the ground truth's `sum() / total_passengers * 100`, demonstrating good pandas idiomatic usage.
Overall, a very strong solution.",gemini-2.0-flash,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""imdb_top_250.csv"")

# 1. List column names and data types
print(""Column Names and Data Types:"")
print(df.dtypes)

# 2. Calculate and report descriptive statistics for numerical columns
numerical_cols = ['Year', 'Rating', 'Runtime']
print(""\nDescriptive Statistics for Numerical Columns:"")
print(df[numerical_cols].describe())

# Calculate median
print(""\nMedian for Numerical Columns:"")
print(df[numerical_cols].median())

# Calcula...",6,"The generated code correctly loads the dataset, lists column names and data types, and reports the total number of movies. However, it fails to correctly calculate and report all the *required* descriptive statistics in a consolidated manner. Specifically, it uses `describe()` which includes mean, min, max, and std, but then separately calls `median()` and `std()` again, which is redundant for `std` and misses the specific ask for `median` *within the same reporting structure* as the other `describe` outputs (i.e. 'mean', 'median', 'std', 'min', 'max'). The `describe()` method also implicitly provides `50%` percentile which is the median, but the task asked for 'median' specifically labeled. More critically, it does not handle the common case where 'Runtime' might be a string (e.g., '142 min') and needs conversion to numeric, which would cause an error or incorrect statistics if not addressed. There's also no error handling for `FileNotFoundError`.",gemini-2.0-flash,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""imdb_top_250.csv"")

# Filter movies with Rating >= 9.0
filtered_df = df[df['Rating'] >= 9.0]

# Select Title and Rating columns
selected_data = filtered_df[['Title', 'Rating']]

# Sort by Rating in descending order
sorted_data = selected_data.sort_values(by='Rating', ascending=False)

# Display the result
print(sorted_data)
```",9,"The generated code correctly performs all the requested steps: it loads the CSV, filters movies with a rating greater than or equal to 9.0, selects the 'Title' and 'Rating' columns, and sorts the result by 'Rating' in descending order. It includes the necessary pandas import.

The primary difference from the ground truth is the lack of robust error handling (FileNotFoundError, KeyError, etc.). While the prompt asked for ""only the Python code without explanations"" and did not explicitly mention error handling as a requirement, in a real-world scenario, robust error handling is crucial for production-ready code. However, given the context of a benchmark focusing on the core data manipulation task, its absence doesn't detract from the correctness of the solution for the given requirements. The ground truth includes `DATASET_PATH` as a variable, which is good practice, but the generated code's direct use of the string literal `""imdb_top_250.csv""` is acceptable for this simple task.",gemini-2.0-flash,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv(""imdb_top_250.csv"")

# Histogram of Rating
plt.figure(figsize=(10, 6))
plt.hist(df['Rating'], bins=10, color='skyblue', edgecolor='black')
plt.title('Distribution of IMDb Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# Scatter plot of Rating vs Runtime
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Runtime', y='Rating', data=df, color='coral')
plt.title('R...",7,"The generated code correctly addresses the core requirements of loading the CSV, creating a histogram for `Rating`, and a scatter plot for `Rating` vs. `Runtime`, with appropriate titles and labels.

**Strengths:**
- Loads the dataset correctly.
- Generates both required plots using `matplotlib` and `seaborn`.
- Includes titles and axis labels for both plots.
- Uses good practices like `plt.figure(figsize)` for better plot appearance.

**Areas for Improvement (Why it didn't get a higher score):**
- **Robustness in Data Loading:** The generated code lacks error handling for `FileNotFoundError`. If the file is not present, the script will crash. The ground truth includes a `try-except` block for this.
- **Data Preprocessing for 'Runtime':** This is the most significant missing piece. The `Runtime` column in `imdb_top_250.csv` is typically in a string format (e.g., ""120 min""). The generated code attempts to directly use this column in `sns.scatterplot(x='Runtime', ...)`, which will likely result in an error or incorrect plot because `seaborn` expects numeric data for scatter plots. The ground truth code includes robust preprocessing to extract numeric values from the `Runtime` string and handle potential `NaN` values. Without this, the scatter plot part of the code is incomplete for the given dataset characteristics.
- **Clarity in `Runtime` axis label:** While ""Runtime (minutes)"" is intuitive, explicitly performing the unit extraction as in the ground truth makes the plot more accurate and robust.",gemini-2.0-flash,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
try:
    penguins = pd.read_csv(""penguins.csv"")
except FileNotFoundError:
    print(""Error: penguins.csv not found. Place it in the current directory."")
    exit()

# Report missing values per column
print(""Missing values per column:"")
print(penguins.isna().sum())

# Identify numerical columns with missing values that need to be dropped
numerical_cols_to_check = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']

# Drop r...",6,"The generated code correctly loads the dataset and handles `FileNotFoundError`. It also correctly identifies and drops rows with missing values in the specified numerical columns.

However, there are several issues impacting its correctness and completeness:
1.  **Reporting missing values (Requirement 1):** The generated code reports `penguins.isna().sum()`, which shows missing values for *all* columns, including those that might not have any. The ground truth filters this to show only columns with missing values. The task explicitly asked to ""Report the number of missing values (NA) for each column,"" which the generated code does, but the ground truth's approach of filtering for *only* columns with NAs is a better, more concise report. The generated code also doesn't explicitly state ""NA count"" or similar, just prints the series.
2.  **Descriptive Statistics (Requirement 2):**
    *   The generated code uses `penguins_cleaned[numerical_cols_to_check].describe()`. While `describe()` provides mean, median (quartiles), std, min, and max, the task specifically asked for ""mean, median, standard deviation, min, max"". The `describe()` output includes 25%, 50% (median), and 75% quartiles, which is more than what was asked, and presents it as a full table.
    *   Crucially, the code then *separately* prints mean, median, and std for each column individually in a very verbose, repetitive way, and *completely omits* min and max from this individual reporting. This is a significant deviation from the precise request for ""mean, median, standard deviation, min, max"" for these columns using the *filtered* data. The requested `min` and `max` are present in `describe()` but not explicitly reported as requested in the task's specific format using individual print statements.
3.  **Unique Values (Requirement 3):** The task asked to ""Count and report the number of unique values in the `species` column and the `island` column."" The generated code uses `value_counts()`, which lists each unique value and its count, not just the *number* of unique values. While `value_counts()` implies uniqueness, the task specifically asked for the ""number of unique values"" (i.e., `nunique()`). The ground truth correctly uses `nunique()`.

Overall, the code performs the core data loading and filtering but doesn't precisely fulfill the reporting requirements for missing values and descriptive statistics, and misinterprets the unique value count request. The manual printing of individual statistics is inefficient and misses the requested min/max compared to using `.agg()` or simply extracting from `.describe()`.",gemini-2.0-flash,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

# Load the dataset
penguins_df = pd.read_csv('penguins.csv')

# Filter the dataset
filtered_penguins = penguins_df[(penguins_df['species'] == 'Adelie') & (penguins_df['island'] == 'Torgersen')]

# Select specific columns
selected_columns = filtered_penguins[['culmen_length_mm', 'culmen_depth_mm']]

# Drop rows with missing values
cleaned_df = selected_columns.dropna()

# Display the first 5 rows
print(cleaned_df.head())
```",9,"The generated code is highly correct and functional. It successfully loads the dataset, filters it according to the specified conditions, selects the required columns, drops rows with missing values, and displays the first 5 rows. The code structure is clear and follows standard pandas operations.

The only minor point of improvement is the `dropna()` call. While `selected_columns.dropna()` works correctly in this specific case because the `selected_columns` DataFrame only contains `culmen_length_mm` and `culmen_depth_mm`, explicitly using `dropna(subset=['culmen_length_mm', 'culmen_depth_mm'])` as in the ground truth code is more robust and explicit. It ensures that only NaNs in *these specific columns* are considered for dropping, regardless of what other columns (if any) might have been present in `selected_columns` if the selection step was different or if the task had changed. This is a best practice for clarity and future maintainability, but it doesn't make the current code incorrect.

Additionally, the ground truth includes error handling for `FileNotFoundError`, which is good practice for production code but not explicitly required by the prompt's core task description. The generated code skips this, which is acceptable given the prompt's focus on the data manipulation steps. Similarly, the ground truth explicitly uses `print(cleaned_df.head(5))` while the generated code uses `print(cleaned_df.head())`, which defaults to 5 rows and is functionally identical for this task.",gemini-2.0-flash,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv(""penguins.csv"")

# Drop rows with any missing values in the specified columns
df_clean = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])

# Create the scatter plot
plt.figure(figsize=(8, 6))
sns.scatterplot(x='culmen_length_mm', y='culmen_depth_mm', hue='species', data=df_clean)
plt.title('Culmen Length vs. Culmen Depth by Species')
plt.xlabel('Culmen Leng...",9,"The generated code correctly loads the data, filters missing values as required, and generates the scatter plot with the correct axes, hue, title, and labels using `seaborn` and `matplotlib`. All requirements from the prompt were met.

Strong points:
- Correctly uses `pd.read_csv`
- Correctly filters rows using `dropna(subset=...)` for the specified columns.
- Uses `sns.scatterplot` effectively to plot `culmen_length_mm` vs `culmen_depth_mm` colored by `species`.
- Includes a title and correct axis labels.
- Includes all necessary imports.

Minor areas for consideration (but not detracting from the score for this specific task's requirements):
- The ground truth code wraps the logic in a function and includes error handling for `FileNotFoundError`, which is good practice for more robust applications but wasn't explicitly asked for in the prompt's requirements.
- The generated code directly executes the plotting logic, which is fine for a standalone script as requested.",gemini-2.0-flash,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
try:
    df = pd.read_csv(""penguins.csv"")
except FileNotFoundError:
    print(""Error: penguins.csv not found. Please ensure it is in the same directory as this script."")
    exit()


# Fill NA values with the mean for numerical columns
df['culmen_length_mm'] = df['culmen_length_mm'].fillna(df['culmen_length_mm'].mean())
df['culmen_depth_mm'] = df['culmen_depth_mm'].fillna(df['culmen_depth_mm'].mean())

# Calculate and print mea...",9,"The generated code correctly addresses all requirements of the task. It successfully loads the dataset, fills NA values in the specified columns with their respective means, calculates and prints the mean, median, and standard deviation for the cleaned columns, and finally prints the total number of rows and columns.

**Strong points:**
*   **Correctness:** All calculations and data manipulations are performed correctly.
*   **Completeness:** All parts of the task requirement are met.
*   **Readability:** The code is well-structured and easy to understand.
*   **Error Handling:** Includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice.

**Areas for minor improvement (not critical, hence the high score):**
*   **Redundancy in statistics calculation:** The statistical calculations for `culmen_length_mm` and `culmen_depth_mm` are done separately, leading to some repetitive code. The ground truth uses a loop, which is more scalable if more columns were to be processed.
*   **Hardcoding column names:** The column names `culmen_length_mm` and `culmen_depth_mm` are hardcoded multiple times. While acceptable for a small, specific task, using a list of columns (like `NUMERICAL_COLUMNS` in the ground truth) would make the code more maintainable and flexible for future expansions.",gemini-2.0-flash,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
penguins_df = pd.read_csv(""penguins.csv"")

# Drop rows where 'flipper_length_mm' is NA
penguins_df = penguins_df.dropna(subset=['flipper_length_mm'])

# Create a histogram of 'flipper_length_mm'
plt.figure(figsize=(8, 6))
plt.hist(penguins_df['flipper_length_mm'], bins=20)
plt.title('Distribution of Flipper Length (mm)')
plt.xlabel('Flipper Length (mm)')
plt.ylabel('Frequency')
plt.show()

# C...",8,"The generated code successfully completes all specified tasks: loading the data, handling NAs, generating a histogram for flipper length, and a bar chart for island counts, with appropriate titles and labels. The code is structured well and uses the required libraries.

Areas for improvement to reach a higher score:
1.  **Error Handling (File Not Found):** The generated code lacks `try-except` blocks for `FileNotFoundError` when loading the CSV, which is a good practice for robust code, as seen in the ground truth.
2.  **Explicit Plotting with Seaborn:** While `plt.hist` and `island_counts.plot(kind='bar')` work, the task description mentions using `matplotlib/seaborn`. The ground truth demonstrates using `sns.histplot` and `sns.countplot` which are often preferred for their enhanced aesthetics and features in data science contexts. For instance, `sns.histplot` automatically handles `kde`, and `sns.countplot` simplifies counting categories.
3.  **Aesthetics and Readability:** The ground truth includes `sns.set_theme`, explicit `figsize` for both plots, and fine-tuning of font sizes and grid lines, which contributes to more professional and readable visualizations. The generated code uses basic `figsize` but could benefit from these additional aesthetic touches.
4.  **`ylabel` for Histogram:** The histogram's `ylabel` is 'Frequency' in the generated code, which is acceptable, but 'Count' (as in the ground truth) is perhaps more common and directly intuitive for a histogram of counts.

Overall, it's a very good attempt that correctly fulfills the core requirements. The suggested improvements are mainly related to robustness, best practices in visualization library usage, and aesthetic enhancements.",gemini-2.0-flash,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
try:
    penguins_df = pd.read_csv(""penguins.csv"")
except FileNotFoundError:
    print(""Error: penguins.csv not found. Make sure the file is in the current directory."")
    exit()

# Filter for Torgersen island
torgersen_penguins = penguins_df[penguins_df['island'] == 'Torgersen']

# Count penguins of each species on Torgersen island
species_counts = torgersen_penguins['species'].value_counts()

# Print the counts
print(species_counts)
```",9,"The generated code correctly addresses all the core requirements of the task. It loads the dataset, filters for 'Torgersen' island, and counts the species occurrences within that subset, printing the result.

**Strong points:**
- **Correctness:** The logic is sound and produces the expected output.
- **Conciseness:** The code is more concise than the ground truth while still being readable.
- **Proper use of pandas:** `pd.read_csv`, boolean indexing for filtering, and `value_counts()` are used correctly and efficiently.
- **Error handling for FileNotFoundError:** It includes basic error handling for the common case of a missing file.

**Areas for improvement (minor):**
- **Column Existence Checks:** While not strictly required by the prompt, the ground truth includes checks for the existence of 'island' and 'species' columns. In a production environment or for more robust applications, these checks can prevent `KeyError` if the CSV structure deviates. However, for a typical benchmark scenario where the CSV format is assumed, this is often omitted for brevity.
- **Empty Filtered DataFrame:** The ground truth also handles the edge case where no penguins are found on 'Torgersen' island. The generated code would simply print an empty Series in that scenario, which is acceptable but less explicit than printing a message.
- **Print Statements:** The ground truth provides more verbose print statements for load success and filtering details, which can be helpful for debugging or user feedback in a larger application. The generated code directly prints the final species counts, which is perfectly fine for the given task.

Overall, it's a high-quality solution that efficiently solves the problem. The minor improvements are robustness considerations rather than critical flaws for this specific task.",gemini-2.0-flash,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# 1) Print the shape of the DataFrame
print(""Shape of the...",10,"The generated code is excellent. It correctly defines the DataFrame, prints its shape, and lists the data types of all columns as requested by the task. The code is clean, well-structured, and directly addresses all requirements without any unnecessary additions or omissions. Imports are handled correctly, and the output format is clear.",gemini-2.0-flash,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('titanic.csv')

# 1. Survival Count
survival_counts = df['Survived'].value_counts()
plt.figure(figsize=(6, 4))
survival_counts.plot(kind='bar')
plt.title('Survival Count')
plt.xlabel('Survived (0 = No, 1 = Yes)')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

# 2. Passenger Class Distribution
pclass_counts = df['Pclass'].value_counts()
plt.figure(figsize=(6, 4))
pclass_counts.plot(kind='bar...",8,"The generated code correctly addresses all requirements of the task. It creates four distinct plots for survival counts, passenger class distribution, age distribution, and gender distribution, each with appropriate titles and axis labels. It successfully handles missing age values by imputing them with the median.

Areas for minor improvement:
- **Survival Plot Labels:** The x-axis labels for the survival plot (0 and 1) could be made more descriptive (e.g., 'Non-survivor', 'Survivor') for better readability, as done in the ground truth.
- **Plot Customization:** While functional, the plots are quite basic. Adding features like `figsize`, `color`, `edgecolor`, `grid`, and `tight_layout` (as in the ground truth) would enhance their visual appeal and professionalism, but these are aesthetic choices beyond the core requirements.
- **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError`, which is good practice for robustness. The generated code assumes the file exists. While not explicitly asked for, it's a good touch for ""production-ready"" code.",gemini-2.0-flash,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]}

df = pd.DataFrame(data)

# Calculate and print the average 'Midterm_Score'
avg_midterm =...",9,"The generated code is highly correct and addresses all the requirements of the task. It properly creates the DataFrame, calculates the average midterm and final scores, and correctly identifies the subject with the highest average final score. The code quality is good, with necessary imports included and a clear structure.

The only minor points that prevent a perfect 10 are:
1. **Formatting of output:** The ground truth code formats the average scores to two decimal places (e.g., `:.2f`), which is good practice for numerical outputs like averages. The generated code prints them without specific formatting, which might lead to more decimal places than necessary.
2. **Additional detail in output:** The ground truth code also prints the actual average score alongside the subject with the highest average final score, providing more complete information. While not explicitly asked for, it enhances the output's utility.

These are minor improvements and do not detract from the core functionality and correctness of the solution.",gemini-2.0-flash,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
        'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
        'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
        ...",10,"The generated code successfully completes all aspects of the task. It correctly creates the DataFrame, sets 'Date' as the index, calculates the 'Daily_Range' column, accurately identifies and prints the date with the highest 'Close' price, and finally prints the full DataFrame including the new column. The code is well-structured, uses appropriate pandas functions, and includes necessary imports. The output format for the highest close price date is also consistent with common practices.",gemini-2.0-flash,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST...",10,"The generated code is correct, complete, and well-structured. It accurately creates the DataFrame, calculates the 'Total_Revenue' column, correctly identifies the best-selling product by quantity, and prints the first 5 rows of the updated DataFrame as requested. The imports are correct, and the code is easy to read and understand. It perfectly matches the requirements of the task.",gemini-2.0-flash,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate and print the average ""SessionDurationMinutes""
average_session_duration = df[""SessionDurationMinutes""].mean()
print...",9,"The generated code is highly correct and effectively solves all aspects of the task. It correctly defines the DataFrame, accurately calculates the average session duration, and correctly counts users by device type. The use of `.mean()` and `.value_counts()` are appropriate and efficient pandas methods for these operations.

Minor points for improvement (not critical enough to deduct more points):
- The `print` statements in the generated code are slightly less formatted than the ground truth (e.g., missing the ""1."" and ""2."" prefixes and the separator line), making the output marginally less clear for a benchmark comparison where exact output format might be a hidden expectation.
- There are no comments explaining the code, which is present in the ground truth for pedagogical purposes. While not strictly necessary for simple tasks, it's good practice.

Overall, the code is very good and directly answers the prompt's requirements.",gemini-2.0-flash,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

max_temp = df[""TemperatureCelsius""].max()
min_humidity = df[""HumidityPercent""]....",10,"The generated code is excellent. It correctly defines the DataFrame exactly as specified, and accurately calculates and prints both the maximum ""TemperatureCelsius"" and the minimum ""HumidityPercent"". The variable names are clear, and the f-strings for printing are appropriate. It demonstrates a complete understanding of the task requirements and produces the correct output. The only minor difference from the ground truth is the print statement wording, but this does not affect correctness or completeness of the solution.",gemini-2.0-flash,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# 1) Print the total number of ""Likes"" across all posts.
total_likes = df[""Likes""].sum()
print(f""Total Likes: {total_likes}"")

# ...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the total number of likes, and accurately identifies the PostType with the highest average shares. The code is concise, follows good practices (e.g., f-strings for printing), and directly addresses all requirements of the prompt. No improvements are needed.",gemini-2.0-flash,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",10,"The generated code is correct, complete, and adheres to good code quality practices. It correctly defines the DataFrame, calculates the average age, and counts patients by gender as requested in the prompt. The output format for printing is also reasonable. All requirements have been met.",gemini-2.0-flash,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}
df = pd.DataFrame(data)

# 1) Print the ""Title"" of the movie with the highest ""Rating"".
highest_rated_movi...",9,"The generated code correctly performs all the specified tasks. It correctly defines the DataFrame, identifies the highest-rated movie's title, and calculates the average year of movies. The `idxmax()` method is an efficient way to find the index of the maximum value, leading to concise code for finding the highest-rated movie. The calculation of the average year is also correct and straightforward.

**Strong points:**
*   **Correctness:** Both parts of the task are solved accurately.
*   **Conciseness:** The code is very efficient, especially for finding the highest-rated movie using `idxmax()`.
*   **Readability:** The code is easy to understand.
*   **Completeness:** All requirements of the prompt are addressed.

**Minor areas for improvement (to reach a perfect 10):**
*   **Output formatting:** The prompt asks to ""Print the 'Title' of the movie"" and ""Print the average 'Year'"". While the titles are printed, the generated code just prints the raw value. The ground truth code includes descriptive text (e.g., ""The title of the movie with the highest rating is: 'The Dark Knight'"") which makes the output more user-friendly and interpretable, aligning better with typical benchmark expectations for clear print statements.
*   **Robustness for duplicates in highest rating:** The `idxmax()` method will return the index of the *first* occurrence if there are multiple movies with the same highest rating. While the prompt implies finding ""the movie"" (singular), if the intention was to find all such movies, a different approach (like filtering as in the ground truth) would be needed. However, given the prompt's wording, this is not a strict error.",gemini-2.0-flash,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

# 1) Print the total number of cars
total_cars = len(df)
print(f""Tota...",9,"The generated code is highly correct and complete, successfully addressing all requirements. It correctly defines the DataFrame, accurately calculates the total number of cars using `len(df)` (which is a perfectly valid and common alternative to `df.shape[0]`), and correctly calculates the average ""PriceUSD"" using `df[""PriceUSD""].mean()`. The output uses f-strings for clear presentation.

The only minor point of difference, not a fault, is that the ground truth code formats the average price to two decimal places, which is good practice for currency. The generated code does not, but this was not explicitly requested in the prompt, so it doesn't detract from correctness.

Strong points:
- Correct DataFrame creation.
- Accurate calculation of total cars.
- Accurate calculation of average price.
- Clear and concise printing of results.
- Proper use of pandas functions.",gemini-2.0-flash,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd
import numpy as np

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}
df = pd.DataFrame(data)

# Find Pokemon with the highest HP
hi...",9,"The generated code correctly performs all requested operations: it defines the DataFrame as specified, identifies the Pokemon with the highest HP, and identifies the Pokemon with the lowest Defense. The approach for finding the max/min values using boolean indexing (`df[""HP""] == df[""HP""].max()`) is a valid and commonly used method, and `iloc[0]` correctly handles cases where multiple Pokemon might share the same max/min value (it picks the first one encountered).

**Strong Points:**
- **Correctness:** The code accurately solves the problem.
- **Conciseness:** The methods used are compact and efficient.
- **Readability:** The variable names are clear, and the comments indicate the purpose of each section.
- **Completeness:** All aspects of the task are addressed.

**Minor Area for Improvement (and why Ground Truth uses `idxmax`/`idxmin`):**
While the generated code's approach is correct, using `idxmax()` and `idxmin()` (as in the ground truth) is often slightly more performant and idiomatic in Pandas for finding the index of a single maximum/minimum value, especially in larger DataFrames. The generated code's boolean indexing (`df[""HP""] == df[""HP""].max()`) first creates a full boolean Series and then filters, which can be computationally more intensive than directly finding the index. However, for a small DataFrame like this, the performance difference is negligible. The `iloc[0]` is necessary in the generated code to extract the name, whereas `df.loc[idx, 'Name']` directly gets the scalar value using `idxmax()`/`idxmin()`.

The `import numpy as np` is present but not used in the final code, making it an unnecessary import. This is a minor point.

Overall, the generated code is excellent and functionally equivalent to the ground truth for this task.",gemini-2.0-flash,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd

# Load the dataset
titanic_df = pd.read_csv('titanic.csv')

# Identify columns with missing values
missing_values = titanic_df.isnull().sum()
print(""Missing values before cleaning:\n"", missing_values[missing_values > 0])

# Fill missing Age values with the median age
titanic_df['Age'] = titanic_df['Age'].fillna(titanic_df['Age'].median())

# Fill missing Embarked values with the most frequent port
titanic_df['Embarked'] = titanic_df['Embarked'].fillna(titanic_df['E...",8,"The generated code correctly addresses most of the requirements. It loads the data, handles missing 'Age' and 'Embarked' values appropriately, drops the 'Cabin' column, and extracts titles from the 'Name' column using `str.extract`. The output format also matches the ground truth.

Minor points for improvement (why it's not a 10):
1.  **Missing `re` import:** Although `str.extract` effectively handles regex, importing `re` explicitly (as in the ground truth) is a good practice if more complex regex operations were anticipated or if a custom function using `re` was needed. However, since `pandas.Series.str.extract` is used, `re` is implicitly handled by pandas.
2.  **Lack of `try-except` for file loading:** The ground truth includes a `try-except` block for `FileNotFoundError`, which makes the code more robust. The generated code assumes the file will always be present.
3.  **Title Standardization:** The generated code extracts the title but doesn't include the standardization step (e.g., replacing 'Mlle' with 'Miss', or grouping 'Rare' titles) which is present in the ground truth. While the prompt didn't explicitly demand this, it's a common and beneficial step in data cleaning when working with titles and makes the 'Title' column more functional for analysis. The ground truth performed this, indicating it might be a subtle part of ""proper cleaning"" for titles.

Overall, it's a very good solution that successfully completes the core task.",gemini-2.0-flash,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}
df = pd.DataFrame(data)

# Find the book wi...",9,"The generated code correctly defines the DataFrame, identifies the title of the book with the highest price, and calculates the total stock. It follows the task requirements precisely. The code quality is good, with proper imports and a clear structure. The only minor improvement would be to use more descriptive print statements, similar to the ground truth, to make the output clearer to the user, though the current output is technically correct according to the prompt given ""Print the 'Title' of the book..."" and ""Print the total number of books..."".",gemini-2.0-flash,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""water_potability.csv"")

# Display the first 10 rows
print(""First 10 rows:\n"", df.head(10))

# Show the shape of the dataset
print(""\nShape of the dataset (rows, columns):"", df.shape)

# List column names and their data types
print(""\nColumn names and data types:\n"", df.dtypes)

# Calculate the percentage of potable vs non-potable water samples
potability_counts = df['Potability'].value_counts(normalize=True) * 100
print(""\nPerce...",9,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, displays the head, shape, dtypes, calculates potability percentages, shows missing values, and provides summary statistics. The code is concise and uses appropriate pandas functions.

Areas for minor improvement:
1.  **Readability/Clarity of Output:** The ground truth code includes clear headers and separators (`--- 1. First 10 rows of the dataset ---`, `\n` + `=""*50` + `\n`) which significantly improve the readability of the output, especially when multiple pieces of information are printed. The generated code's print statements are less explicit (e.g., `print(""First 10 rows:\n"", df.head(10))`).
2.  **Robustness (File Not Found):** The ground truth includes error handling for `FileNotFoundError` when loading the CSV, which makes it more robust. The generated code lacks this.
3.  **Specific display of missing values:** The ground truth specifically prints only columns with missing values and a message if no missing values are found, which is a nice touch. The generated code prints all columns, even if they have zero missing values, which is acceptable but less focused.
4.  **`df.info()` vs `df.dtypes`**: While `df.dtypes` fulfills the request of listing column names and their data types, `df.info()` as used in the ground truth provides more comprehensive information (non-null counts, memory usage), which is generally preferred for this type of EDA summary.",gemini-2.0-flash,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv(""water_potability.csv"")

# 1. Identify columns with missing values
missing_values_before = df.isnull().sum()
original_shape = df.shape

# 2. Fill missing values
for col in df.columns:
    if df[col].isnull().any():
        if pd.api.types.is_numeric_dtype(df[col]):
            df[col].fillna(df[col].mean(), inplace=True)
        else:
            df[col].fillna(df[col].mode()[0], inplace=True)  # For non-numeri...",6,"The generated code addresses most of the requirements but has some significant shortcomings, particularly in the data range validation and the way it presents the summary.

**Correctness:**
*   **Loading and Missing Values:** Correctly loads the data and identifies/fills missing values. It's good that it handles non-numeric columns by using mode, although the prompt specifically asked for ""mean for numerical columns"".
*   **Duplicates:** Correctly identifies and removes duplicate rows.
*   **Data Range Validation:** This is the weakest point.
    *   The `in_range` function expects `column_name` to be ""pH"", but the actual column name is ""ph"" (lowercase). This will cause the pH range check to fail.
    *   It uses `count()` on a boolean series for `invalid_values`, which doesn't actually remove or modify the rows in the DataFrame. The task explicitly states ""Verify that all values are within reasonable ranges..."". The ground truth code successfully removes or filters out rows that are out of range. The generated code *identifies* out-of-range values but does not *correct* or *remove* them as implied by the task ""Verify that all values are within reasonable ranges"" which typically means ensuring the final dataset *only* contains such values.
    *   The `in_range` function unnecessarily uses `column_name` parameter for ""pH"", but for other columns, it directly accesses `df[col]` without using the `column_name` parameter inside the function.
*   **Summary:** The summary preparation is present, but the printing of it is entirely commented out. This means the user wouldn't see the ""before and after statistics"" as requested in the prompt.

**Code Quality:**
*   **Imports:** Proper imports.
*   **Structure:** Decent structure with comments indicating steps.
*   **Readability:** Generally readable.
*   **Error Handling:** Lacks basic error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes and is good practice.

**Completeness:**
*   **Missing Values:** Addressed.
*   **Duplicates:** Addressed.
*   **Data Ranges:** Identifies out-of-range values but does *not* fix or remove them from the dataset, which is a critical missing piece based on typical interpretations of ""verify that all values are within reasonable ranges"" in a data cleaning context. The ground truth demonstrates removal.
*   **Summary:** The logic for summary is present, but the display of the summary is commented out, thus failing to meet the ""Create a summary showing before and after statistics"" requirement.
*   **Display first 5 rows:** Addressed.
*   **Assumed CSV:** Correctly assumes CSV in current directory.

**Specific Areas for Improvement:**
1.  **Data Range Validation Implementation:** Modify the `in_range` function and its application to *filter out* rows with out-of-range values from the DataFrame, similar to the ground truth. Also, fix the case sensitivity for 'ph' column name.
2.  **Summary Display:** Uncomment the print statements for the summary statistics to meet the requirement of showing ""before and after statistics"".
3.  **Robustness:** Add `try-except` block for `FileNotFoundError` when loading the CSV.
4.  **Clarity on Filling Missing Values:** While it fills non-numeric with mode, the prompt specifically asked for mean for numerical. It's not necessarily a bad choice, but for strict adherence to the prompt, only mean for numerical would be needed. However, the current implementation is more generally robust.",gemini-2.0-flash,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv(""water_potability.csv"")

# 1. Potability Distribution Bar Chart
plt.figure(figsize=(6, 4))
sns.countplot(x='Potability', data=df)
plt.title('Distribution of Potable vs Non-Potable Water')
plt.xlabel('Potability (0: Non-Potable, 1: Potable)')
plt.ylabel('Count')
plt.show()

# 2. Histograms for pH, Hardness, and Chloramines
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.h...",8,"The generated code successfully creates all four required visualizations with proper titles and labels, addressing the core task requirements. It handles missing values in histogram and boxplot plotting by using `.dropna()` or by relying on Seaborn's default behavior, which is good. The code structure is clear and imports are correct.

Areas for improvement to reach a higher score (9 or 10):
- **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.
- **Plot Enhancements/Consistency:** The ground truth code has more aesthetic and information-rich enhancements such as `sns.set_style(""whitegrid"")`, `plt.grid`, `plt.tight_layout()` used more consistently and explicitly defined `fontsize` for titles and labels. While `tight_layout` is used for histograms in the generated code, it's missing for others. Adding gridlines, specific fontsizes, and consistent `figsize` choices would improve the overall quality and readability of the plots.
- **Labels for Potability on Bar/Box Plots:** The ground truth explicitly sets `xticks` with labels like 'Not Potable' and 'Potable' for clarity on plots involving 'Potability', whereas the generated code just uses the numerical labels (0, 1), which is less user-friendly for interpretation.
- **Redundant `numerical_features` selection:** For the correlation heatmap, `df.corr()` would already include only numerical features, so `df.select_dtypes(include=['number'])` before calculating correlation is not strictly necessary unless there's a specific reason to exclude some numerical columns.",gemini-2.0-flash,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.preprocessing import PolynomialFeatures

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1. Feature Engineering
df['alcohol_to_density'] = df['alcohol'] / df['density']
df['volatile_acidity_to_fixed_acidity'] = df['volatile acidity'] / df['fixed acid...",6,"The generated code addresses most of the requirements but has several significant issues that prevent it from scoring higher.

**Correctness:**
*   **Target Variable:** The `quality` column in the wine dataset is typically an ordinal or continuous variable, making it a regression problem. The generated code uses `RandomForestClassifier` for RFE, which is appropriate for classification, not regression. The `f_regression` score function used with `SelectKBest` is correct for regression, creating an inconsistency. This is a critical logical error.
*   **Feature Importance from RFE:** The code prints `selector_rfe.ranking_` but doesn't extract or display importance scores for the *selected* features, which was a specific requirement. The ranking tells you the order of elimination, but not the importance within the final selected set. The ground truth correctly retrains the estimator on the selected features to get their importances.
*   **Handling NaNs/Infs:** The feature engineering steps, particularly division, can introduce `inf` or `NaN` values. The code doesn't include any handling for these, which the ground truth correctly shows.
*   **Scalability/Practicality:** While `SelectKBest` for `f_regression` does not strictly require scaling, RFE (especially with distance-based estimators) and general good practice in ML often benefit from scaling features. The generated code omits scaling.

**Code Quality:**
*   **Imports:** Imports are generally good, but `train_test_split` is imported but not used, and `PolynomialFeatures` is imported but not used.
*   **Readability:** The code combines print statements for comparison with the final display of features, making it a bit less structured than the ground truth.
*   **Comments:** Lacks detailed comments explaining the rationale behind engineered features or the choices in feature selection parameters.

**Completeness:**
*   **Engineered Features:** It creates three new features but mentions `total_acidity` as a new feature in a comment but doesn't implement it. The engineered features are reasonable.
*   **Multiple Methods:** It correctly uses RFE and SelectKBest.
*   **Comparison:** It provides a basic set-based comparison, but the textual comparison and contrast of the methods' underlying principles is missing, which the ground truth provides effectively.
*   **Display:** It displays the selected features and their rankings/scores, but the RFE importance calculation is incomplete or incorrect as noted above.
*   **Number of Features:** It uses `k=6` for both methods, whereas the prompt suggested 5-7. Using a constant `N_FEATURES_TO_SELECT` as in the ground truth is good practice.

**Summary of Main Issues:**
1.  **Incorrect Estimator for Target Variable (Classification vs. Regression):** Using `RandomForestClassifier` for a regression problem.
2.  **Incomplete RFE Importance Display:** Doesn't provide actual importance scores for the selected RFE features.
3.  **Missing NaN/Inf Handling:** No robustness for potential division-by-zero or infinite values from feature engineering.
4.  **Lack of Feature Scaling:** A common best practice for many ML pipelines.

The code provides a basic framework but needs refinement to be robust and fully adhere to the problem requirements given the nature of the 'quality' target variable.",gemini-2.0-flash,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor
import xgboost as xgb
from sklearn.linear_model import LinearRegression
from sklearn.met...",6,"The generated code attempts to address all requirements of the task, but it has significant issues in data preprocessing and the overall pipeline structure which affect correctness and completeness.

**Correctness & Completeness Issues:**

*   **Missing Values and One-Hot Encoding:** The generated code uses `SimpleImputer` for numerical and categorical columns, but then it applies `LabelEncoder` to all categorical columns. This is incorrect for nominal categorical features like 'Platform', 'Genre', 'Publisher', 'Rating', 'Developer', 'Name' because it imposes an arbitrary ordinal relationship, which is detrimental to most tree-based models and especially linear models. The ground truth correctly uses `OneHotEncoder`.
*   **Pipeline Usage:** The most critical flaw is the *lack of a proper `Pipeline` for preprocessing and model fitting*. The generated code performs preprocessing (imputation, encoding, scaling) *before* splitting the data into train/test sets, or it applies scaling only to the train/test sets separately but without being encapsulated in a pipeline. This leads to data leakage (e.g., mean/mode of imputer, or min/max of scaler are calculated on the entire dataset before splitting) and makes the cross-validation and hyperparameter tuning incorrect as the preprocessing steps are not consistently applied within each CV fold. The ground truth correctly uses `ColumnTransformer` within a `Pipeline`.
*   **Cross-Validation Implementation:** The `evaluate_model` function uses `cross_val_score` directly on `X_train`, which is already preprocessed (encoded and scaled based on the full dataset or X_train *only*). This is not robust. For correct cross-validation, the preprocessing steps must be part of the pipeline *within* the `cross_val_score` (or manual KFold loop) to ensure data leakage is avoided and transformations are fitted only on the training folds. The ground truth correctly implements manual KFold loops with the `Pipeline` per fold.
*   **Hyperparameter Tuning:** While `GridSearchCV` is used, it's only applied to XGBoost and not the ""best individual model"" as requested. More importantly, it suffers from the same data leakage issues due to the lack of a proper pipeline.
*   **Feature Importance:** The feature importance plotting is attempted, but because `LabelEncoder` was used and scaling was applied, the interpretation of importances and the feature names displayed are not accurate. The ground truth correctly identifies features after one-hot encoding.
*   **Dropped Columns:** The ground truth drops 'Name' and 'Rank' as irrelevant. The generated code only drops 'Unnamed: 0' and attempts to encode 'Name', which is likely not useful for regression and adds noise. 'Rank' is also processed.
*   **User_Score vs. Critic_Score/Critic_Count/User_Count:** The ground truth correctly handles 'User_Score' specifically due to 'tbd' values and treats other numerical columns for imputation. The generated code applies `SimpleImputer` to `User_Score` and `Critic_Score` without special handling for the text, then `User_Count` and `Critic_Count`. Assuming 'User_Score' and 'Critic_Score' columns were already numeric for this imputation to work.
*   **`Unnamed: 0` column:** The generated code correctly drops 'Unnamed: 0' if it exists, which is a good practice for typical CSV imports, but it's not specified in the problem and not present in the provided ground truth's dataset.

**Code Quality:**

*   Imports are present but not very well organized.
*   Lack of comments for complex logic (especially around preprocessing).
*   Repetitive code for evaluation (ground truth uses a loop and stores results more cleanly).
*   No explicit random seed for `np.random` for broader reproducibility (only for models).

To improve, the generated code needs a complete overhaul of the data preprocessing strategy using `ColumnTransformer` and `Pipeline` within the `train_test_split` and cross-validation loops, along with using `OneHotEncoder` for nominal categorical features.",gemini-2.0-flash,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import PolynomialFeatures, StandardScaler, OneHotEncoder
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_squared_error
import pymc3 as pm
import arviz as az
imp...",6,"The generated code attempts to address all requirements but falls short in several aspects, primarily around correctness, robustness, and full implementation of advanced features, especially confidence/prediction intervals and proper validation.

Here's a breakdown:

**Strengths:**
*   **Good Imports:** Most necessary libraries are imported.
*   **Basic Structure:** The code is sectioned by task, which is helpful.
*   **Attempt at all tasks:** All 10 requirements are attempted.
*   **PyMC3 inclusion:** Demonstrates an effort to use a Bayesian framework.
*   **Survival Analysis:** Includes Kaplan-Meier, which is a good start.

**Weaknesses and Areas for Improvement:**

1.  **Data Loading and Preprocessing:**
    *   `df = df.dropna()` is too aggressive. The ground truth correctly identifies specific columns to drop NaNs from (`Year_of_Release`, `Global_Sales`). Dropping all NaNs can severely reduce data, especially as `Critic_Score`, `User_Score`, etc., often have many missing values.
    *   The `sales_by_year.asfreq('YS').fillna(method='ffill')` after grouping is questionable. `ffill` might not be appropriate for missing *years*. The ground truth handles this by using `pd.to_datetime` and `asfreq('YS')` directly after aggregation, which is more robust for time series.

2.  **ARIMA Modeling:**
    *   It uses `sm.tsa.arima.model.ARIMA` which requires manually specifying `order`. The prompt implicitly asks for a more robust method by expecting ""forecasting future global sales trends"", and `pmdarima.auto_arima` (used in ground truth) is the standard for automatically finding the best ARIMA model, especially for datasets where the order is not immediately obvious.
    *   `sales_by_year = df.groupby('Year')['Global_Sales'].sum()` does not create a true time series index required by `statsmodels`. The ground truth converts the index to a datetime index `yearly_sales.index = pd.to_datetime(yearly_sales.index, format='%Y')`. This fundamental setup issue will likely lead to incorrect time series analysis later or silent errors.

3.  **Seasonal Decomposition:**
    *   Uses `period=5` without justification. For yearly data, detecting a reliable ""seasonal"" component is difficult without a very long time series and careful consideration. The ground truth explains this limitation and makes a more informed attempt. The generated code just applies it.

4.  **Multivariate Regression:**
    *   **Interaction Terms:** `df_regression['Year_Genre_Action'] = df_regression['Year'] * df_regression['Genre_Action']` creates interaction with pre-selected one-hot encoded columns *after* `get_dummies`. This method is prone to issues if the selected dummy variables don't exist or if more nuanced interactions are needed. `statsmodels.formula.api` (used in ground truth) is much more convenient and robust for building complex formulas with interaction terms and categorical variables directly.
    *   **Data Preparation:** `pd.concat([df[['Year', 'Global_Sales']], df_dummies], axis=1)` is problematic as `df_dummies` is created from the original `df` rows, but `df[['Year', 'Global_Sales']]` might have rows that don't perfectly align after `dropna()`. More importantly, the `df_regression` also includes 'Year' as a feature, not necessarily filtered or processed for regression like the ground truth does by limiting categories or handling NaNs specific to regression.
    *   **Prediction Intervals:** The approximation `interval = norm.interval(0.95, loc=mean_pred, scale=std_err)` is *highly incorrect* for a prediction interval for OLS. It uses the mean of predictions and the RMSE across the *entire test set*. A proper prediction interval for a single new observation needs the standard error of the prediction for that specific observation, not the RMSE of the entire test set. `model_regression.get_prediction().summary_frame()` (as used in ground truth) is the correct way. The generated code also doesn't provide confidence intervals specific to coefficients.

5.  **Polynomial Regression with Regularization:**
    *   `X_poly = poly.fit_transform(df[['Year']])` applies polynomial features to the *entire original DataFrame* which is then split into train/test. This is not how `PolynomialFeatures` is typically used for a full regression model (`Year` is usually one of several features).
    *   The prediction interval approximation is again incorrect, similar to OLS. `sklearn` models don't directly provide PIs, and the approach taken is statistically unsound. The ground truth acknowledges this limitation and provides a basic, but more correct, approximation based on test residuals.

6.  **Bayesian Regression (PyMC3):**
    *   The model formulation `mu = alpha + beta * X_bayesian` only uses 'Year' as a predictor. This is a very simplified model and does not use `Genre`, `Platform`, `Publisher`, or their interactions, failing to meet the full ""multivariate regression"" expectation in the Bayesian context. The ground truth attempts a more complex model incorporating categorical effects.
    *   `pm.sample_posterior_predictive(trace, samples=500)` and then another `pm.sample_posterior_predictive` call. This is redundant.
    *   `interval_bayesian = np.percentile(ppc_samples, [2.5, 97.5], axis=0)` returns intervals for *every predicted point*. The code then prints just ""interval\_bayesian"" which is a large array, not a single interpretable interval for *a* prediction. The ground truth correctly shows how to interpret this for specific data points.

7.  **Change Point Detection:**
    *   `sales_by_year` is used, which is good.
    *   `change_location = algo.predict(pen=10)` uses an arbitrary penalty (`pen=10`). A more principled approach, like using `2 * np.log(n_samples)` for BIC-like behavior (as in ground truth), would be better.

8.  **Survival Analysis:**
    *   **Definition of Longevity:** `df['Age'] = 2016 - df['Year']` defines 'Age' as years since release until 2016. This is a very crude measure of 'longevity' or 'time at risk'. For games, the 'event' (game stops being profitable/sold) is not clearly defined. The ground truth's definition of longevity as `Year_of_Release_last_observed - Year_of_Release_release + 1` is a more robust (though still simplified) approach based on observed data for each game.
    *   **Event Status:** `event_observed=np.ones(len(df))` assumes *all* games have had their ""event"" by 2016. This essentially treats all observations as events at time T, which is not survival analysis, it's just a distribution of game durations. Survival analysis needs censored data (games still ""alive"" at the end of observation period). The ground truth correctly sets `Observed_Event` to 1 for all games (due to limitations of the dataset for true censoring), but its `Longevity_Years` is a better duration definition.
    *   **CoxPHFitter Missing:** The prompt asks for survival analysis, and the inclusion of Cox Proportional Hazards is standard for modeling covariates (like Genre). The generated code only implements Kaplan-Meier and a Log-Rank test, but doesn't build a regression model with covariates, failing to address the spirit of ""modeling game longevity"".

9.  **Time Series Cross-Validation:**
    *   **ARIMA TS-CV:** The `time_series_cv` function uses `ARIMA(train, order=(5, 1, 0))`. This hardcoded order might not be optimal for every fold, and `auto_arima` (used in GT) would dynamically fit the best model for each fold, which is more robust.
    *   **Multivariate TS-CV:**
        *   `X_train, X_test = X.iloc[train_index], X.iloc[test_index]` - This relies on `X` being sorted by time, which is not explicitly guaranteed for `df_regression`, potentially breaking the time series split. The ground truth explicitly `sort_values(by='Year_of_Release')` for the regression data.
        *   The function only validates the OLS model (`sm.OLS`). It doesn't validate Ridge/Lasso polynomial regression, which were explicitly implemented as part of the task.

10. **Overall:**
    *   Lack of graphical output. While the prompt states ""Provide only the Python code without explanations"", the ground truth includes plotting code which is essential for visualizing results and verifying models in practice. The generated code comments out `plt.show()` and `az.plot_trace`.
    *   Confidence and prediction intervals were largely incorrect or not fully implemented/explained. This was a key requirement.

In summary, the generated code demonstrates a superficial understanding of many advanced statistical techniques. It focuses on calling library functions without fully grasping the underlying statistical principles (e.g., proper PI calculation, time series indexing, or appropriate definition of ""longevity"" for survival analysis). The chosen implementations are often the simplest form, lacking the robustness and correctness seen in a truly expert solution.",gemini-2.0-flash,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from scipy.spatial.distance import cdist
from gap_statistic import O...",6,"The generated code attempts to address all aspects of the task, which is commendable. However, there are several issues that reduce its correctness and robustness, particularly in the preprocessing, clustering setup, and validation steps.

**Strong Points:**
*   **Comprehensive Task Coverage:** Most requested components (multiple clustering algorithms, dimensionality reduction, optimal cluster determination, consensus clustering, anomaly detection, association rules, recommendation, validation) are present.
*   **Feature Engineering:** Attempts to create the requested features ('Regional_Sales_Distribution', 'Market_Position', 'Publisher_Portfolio_Size').
*   **Use of Libraries:** Correctly uses `sklearn`, `matplotlib`, `seaborn`, `umap`, `mlxtend` for the specified tasks.
*   **Basic Visualization:** Includes a PCA scatter plot for clusters.

**Areas for Improvement/Corrections:**

1.  **Preprocessing Issues:**
    *   **`dropna()`:** A blanket `df.dropna(inplace=True)` at the beginning is overly aggressive and likely removes a significant portion of the data, especially rows with missing sales figures or other less critical columns like `Critic_Score`. The ground truth code handles `dropna` more selectively for crucial identifiers and imputes/fills for sales.
    *   **Feature Engineering `regional_sales_distribution`:** The custom function for `Regional_Sales_Distribution` creates a dictionary in a single column, which is not directly usable for numerical processing and scaling. The ground truth correctly creates separate proportion columns (`NA_Sales_Prop`, etc.).
    *   **`LabelEncoder` for Numerical-like Categoricals:** Applying `LabelEncoder` to `Year_of_Release` and `Rating` (if it were numeric) is problematic. These are ordinal or numerical in nature for their values, and `LabelEncoder` assigns arbitrary integer labels which implies an ordinal relationship that might not exist, especially for 'Rating'. `Year_of_Release` should be treated as numerical (which it already is before encoding) or one-hot encoded if treated categorically. The ground truth correctly treats `Year_of_Release` as numerical. `OneHotEncoder` is generally preferred for nominal categorical variables.
    *   **Missing numerical columns for scaling:** The `numerical_cols` for scaling do not include the newly engineered proportion features from `Regional_Sales_Distribution` (which were incorrectly generated anyway).
    *   **ColumnTransformer is missing:** The generated code doesn't use `ColumnTransformer` for robust preprocessing of mixed data types, leading to manual and less maintainable transformations.

2.  **Clustering and Optimal k-Selection:**
    *   **Fixed `n_clusters=5` for all algorithms:** The code hardcodes `n_clusters=5` for K-Means, Hierarchical, and GMM, making the ""Optimal Number of Clusters"" section redundant if those parameters aren't used. The optimal k analysis is performed but its result isn't applied to initialize the clustering models.
    *   **DBSCAN `eps` and `min_samples`:** DBSCAN is notoriously sensitive to parameters. A fixed `eps=0.5` and `min_samples=5` without any parameter estimation (like k-distance plot) is unlikely to yield meaningful results. The ground truth provides a heuristic for estimating `eps`.
    *   **`find_optimal_clusters` Function:**
        *   It calculates silhouette, elbow, and gap but doesn't actually **return** an `optimal_k` value determined by any of these methods usable by the main script. `n_clusters` from `OptimalK` is returned, but the other methods are left as raw scores.
        *   The elbow method plot (inertia) and silhouette plot are missing, which are crucial for visual interpretation.

3.  **Consensus Clustering:**
    *   **Simplified Implementation:** While the intention is there, the implementation is a ""simplified"" version and is not robust. It relies on `pairwise_distances` (which is not directly used in the current version) and then `AgglomerativeClustering` on `1 - co_occurrence_matrix`. This is a valid approach but the implementation can be more generic and handle potential noise labels from DBSCAN. The ground truth's approach is more explicit about counting matches and normalizing.
    *   It implicitly assumes that `DBSCAN_Cluster` will not contain -1 (noise) or handles it in `consensus_clustering` function itself, which might lead to issues when using `df[col].iloc[i] == df[col].iloc[j]`.

4.  **Cluster Profiling and Statistical Significance:**
    *   **`cluster_profiles` Function:** The `ttest_ind` is used to compare each cluster against ""all other clusters."" While this can indicate if a feature is significantly different for a cluster compared to the rest, it's not a direct pairwise comparison between all clusters. ANOVA (as in the ground truth) is more appropriate for comparing means across *multiple* groups simultaneously. Additionally, t-tests are for numerical variables; for categorical variables, chi-squared tests are needed, which are missing.
    *   **Output:** The `print(cluster_profiles_df)` just prints the dataframe, but a more descriptive output of the statistical significance would be beneficial, as shown in the ground truth.

5.  **Association Rule Mining:**
    *   **Initial Data:** `pd.get_dummies(df[categorical_cols])` will create a large sparse matrix depending on the cardinality of `Platform` and `Genre`. `Publisher` and `Year_of_Release` were in the task's general ARM requirements but excluded in `categorical_cols_for_rules` without an explicit reason given (though a fair reason is computational load).
    *   **`mlxtend` usage:** Correctly uses `apriori` and `association_rules`.

6.  **Recommendation System:**
    *   **Correctness:** The recommendation system is functional but requires `df` to have a 'Name' column and the 'Consensus_Cluster' column. It also assumes 'Global_Sales' is a good metric for ""similarity"" within a cluster, which is a reasonable heuristic.

7.  **Validation Metrics:**
    *   **DBSCAN Handling:** The `validate_clustering` function does not explicitly handle DBSCAN's noise points (-1 label) when calculating metrics. If -1 labels exist, `silhouette_score`, `davies_bouldin_score`, `calinski_harabasz_score` can produce errors or misleading results. The ground truth carefully filters out -1 labels before validation.
    *   **Individual Algorithm Validation:** The code only validates the `Consensus_Cluster`, missing the validation for individual clustering algorithms (K-Means, GMM, etc.) as requested (though the prompt asks for ""Validation metrics"" generally. The ground truth validates each one).

**Overall:** The code showcases a good understanding of the overall process but lacks the robust implementation details and error handling necessary for a production-ready or highly accurate analysis. The preprocessing choices are particularly weak and will impact the quality of all subsequent steps. The lack of proper application of optimal k from the analysis is also a significant flaw.",gemini-2.0-flash,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from...",7,"The generated code addresses most of the requirements, implementing various ensemble methods, Bayesian optimization, nested cross-validation, SHAP, and model calibration. However, there are several areas for improvement in correctness, completeness, and code quality.

**Strengths:**
*   **Comprehensive Ensembles:** Includes a good variety of ensemble methods (Voting, Bagging, Boosting, Stacking, CatBoost).
*   **Bayesian Optimization with Optuna:** Implements Optuna for hyperparameter tuning.
*   **Nested Cross-Validation:** Correctly structures nested CV, though the inner CV is not applied to all models as promised.
*   **SHAP Implementation:** Demonstrates SHAP value calculation and plotting.
*   **Model Calibration:** Includes `CalibratedClassifierCV`.
*   **Feature Engineering:** Attempts to handle missing values and create interaction features.

**Areas for Improvement/Weaknesses:**

1.  **Correctness in Preprocessing:**
    *   Replacing 0s with median: While a common approach, the given dataset often has 0s for `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI` which represent missing or invalid measurements rather than true zeros. The ground truth code correctly identifies this and replaces them with `np.nan` before median imputation. The generated code directly replaces 0s, which for some columns like `Pregnancies` (where 0 makes sense) might be incorrect if the intent was only for the specified medical columns. This is a subtle but important domain-specific detail.
    *   `PolynomialFeatures` are missing in the generated code's feature engineering, which the ground truth code used. The generated code's `create_interactions` is a simplified version of `PolynomialFeatures`.
    *   The generated code uses `X = scaler.fit_transform(X)` which converts `X` to a NumPy array. This is problematic for subsequent steps that expect a DataFrame (e.g., `shap.Explainer(model, data)`). The ground truth preserves the DataFrame structure.

2.  **Completeness/Correctness in Nested Cross-Validation:**
    *   The `nested_cv` function in the generated code is hardcoded to tune *only* XGBoost, and the `best_model` trained and evaluated within the loop is also *only* XGBoost. The prompt implies nested CV for *unbiased evaluation of all models/ensembles*, not just for tuning one model. The ground truth calculates metrics for *all* models/ensembles within each outer fold of the nested CV.
    *   The `nested_cv` function's `inner_cv` for Optuna is redundant; Optuna's `objective` function already handles internal cross-validation for hyperparameter search. The ground truth tuned XGBoost *outside* the nested CV using a single train/validation split for efficiency, and then used those tuned parameters within the nested CV for evaluation. This approach is more practical given the ""hard"" problem nature.
    *   The outer loop of nested CV often is for *evaluation* of the best model (potentially chosen after tuning) on truly unseen data. The generated code's `nested_cv` function is quite specific and doesn't align with the broader ""unbiased model evaluation"" intent for *all* implemented models.

3.  **Custom Ensemble:**
    *   The dynamic weighted ensemble code is a good start but is demonstrated *outside* the rigorous nested CV framework. To be truly robust, the weight assignment (e.g., based on AUC) would need to be done using an internal validation set within each fold of the nested CV, not on the `X_test` directly, which reintroduces data leakage for evaluation. The ground truth does this correctly by splitting `X_train` into `X_train_inner` and `X_val_inner` for weight calculation.

4.  **Model Calibration:**
    *   The example of calibration is very basic, demonstrating `CalibratedClassifierCV` for a single model (Logistic Regression) on the existing `X_train`, `y_train` where `X_train` is the entire training set from the `train_test_split`. For production, calibration should be done on out-of-fold predictions or a separate hold-out set to prevent overfitting the calibrator. The ground truth specifically applies calibration to the *best performing model from an outer fold* and calculates Brier scores, also showing the calibration curve.

5.  **Ensemble Diversity Analysis/Combination Strategies:**
    *   The `ensemble_diversity` function calculates ""disagreement"" using hard predictions. While a valid diversity measure, it's less common than correlation of probabilities (used in the ground truth) for soft ensembles.
    *   The ""model combination strategies"" section is mostly absent. The ground truth explicitly shows how to train a meta-learner on out-of-fold predictions obtained from the nested CV, which is a more advanced technique.

6.  **Code Quality/Clarity:**
    *   Missing `warnings` import and suppression.
    *   No proper print statements to indicate progress through different steps.
    *   The global `X, y` are used throughout, but `nested_cv` re-splits locally. This could lead to confusion.
    *   The SHAP explanation uses the `xgb` model directly, which was fitted *once* on the initial `X_train`. Ideally, SHAP would be applied to a model from the nested CV (e.g., the best model trained in an outer fold) to be consistent with the evaluation.
    *   Some comments are outdated or misleading (e.g., `best_model = XGBClassifier(...)` implying it takes `best_params` from inner CV for every model, but it's still hardcoded to be XGBoost).

The ground truth covers more metrics (precision, recall, F1, Brier score), includes more advanced feature engineering (`PolynomialFeatures`), properly handles missing data in a domain-aware way, and demonstrates a more rigorous and complete nested CV loop for evaluating all models and ensembles, including a full meta-learner example.",gemini-2.0-flash,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import arviz as az
import theano.tensor as tt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from causalinference import CausalModel
import statsmodels.api as sm
import statsmodels.formula.api as smf
from lifelines import KaplanMeierFitter
from lifelines.statistics import logrank_te...",5,"The generated code attempts to address all requirements but suffers from significant issues in correctness and completeness, particularly in the more complex statistical modeling aspects.

**Key Issues and Missing/Incorrect Implementations:**

*   **Hierarchical Bayesian Model (Section 2 & 3 - mixed effects):**
    *   The PyMC3 hierarchical model setup is fundamentally flawed. It tries to use `df['Age'].nunique()` as the shape for `a`, but `a` is then indexed by `df['Age'].unique().searchsorted(df['Age'])`, which is not a standard approach for group-level effects. A proper hierarchical model would define groups (e.g., age buckets) and estimate group-specific effects using those group indices. The `Age` column (which is scaled) is treated as individual variability leading to too many random effects, and the `base_effect` for `Glucose` is incorrectly defined.
    *   The generated code attempts mixed effects in `statsmodels` (Section 3), but the prompt explicitly requested mixed-effects models *using PyMC3/PyMC4*. The `groups=df['Pregnancies']` also seems arbitrary and might not represent a meaningful grouping variable for a mixed model.
    *   The `hierarchical_trace` analysis in Section 10 (`az.plot_posterior(hierarchical_trace, var_names=['a','b0','b1','b2'])`) inspects variables that are not defined in the hierarchical model (`b0`, `b1`, `b2`). This indicates a lack of coherence.

*   **Bayesian Logistic Regression with Informative Priors (Section 4):** This section is generally okay in its concept, but it's a very simple logistic regression and not integrated with the concepts of mixed effects or hierarchical modeling as requested. It uses arbitrary `b0`, `b1`, `b2` which are meant to be inferred coefficients, not fixed values.

*   **Survival Analysis (Section 6):**
    *   The simulation of `time_to_event` and `event_observed` is overly simplistic and random, not reflecting any underlying disease process or risk factors from the dataset. The ground truth provides a more structured simulation based on simulated risk.
    *   The prompt asked for *survival analysis techniques* (plural) and mentioned treating diabetes onset as time-to-event. While Kaplan-Meier is one technique, the ground truth uses Cox Proportional Hazards (a regression model), which is more advanced and aligns better with the modeling theme. The generated code stops at K-M and a log-rank test, which are exploratory.

*   **Gaussian Processes (Section 7):**
    *   The prompt asked for PyMC3/PyMC4 for GP, but the generated code uses `gpytorch` and `torch`. While `gpytorch` is a valid GP library, it deviates from the specified framework (PyMC3/PyMC4) in the prompt's context of Bayesian analysis.
    *   The example is very basic (Glucose vs BMI) and doesn't fully demonstrate a non-parametric modeling *of relationships between variables* in a broader sense. It merely sets up a simple GP regression.

*   **Causal Inference (Section 8):**
    *   The use of the `causalinference` library is a good attempt, but the `est_propensity()` call expects a binary treatment `D`, while `df['Glucose']` is continuous. This leads to an error (`D must be a binary numpy array or Series`). The method `StratifiedMatching` is also incorrectly called for a continuous treatment, as PSM typically works on binary treatments.
    *   The prompt specifically mentioned **propensity score matching** and **instrumental variables**. The generated code only attempts PSM (incorrectly) and completely omits Instrumental Variables (IV) analysis, which is a significant omission. The ground truth correctly implements a simplified 2SLS approach for IV.

*   **Posterior Predictive Checks (Section 10):** The generated code attempts to use `pm.plot_posterior_predictive_check(hierarchical_trace)`, but it does so without configuring the plot (e.g., showing the observed outcome distribution), making it less informative than the ground truth's `az.plot_ppc`.

*   **General Code Quality & Completeness:**
    *   Imports are a bit disorganized, and some (`theano.tensor`, `roc_auc_score`) are imported but not used or used incorrectly.
    *   The `df['Age'].nunique()` and `searchsorted` in the hierarchical model are not standard or robust ways to define grouping variables.
    *   Missing value handling is basic (`fillna(median)`) compared to the ground truth's more comprehensive approach of identifying specific '0' values as NaNs first.
    *   Variable `Age` is used as a numerical feature. Grouping `Age` into `AgeGroup` is done in Section 3 but not integrated into the hierarchical modeling, which was a core requirement.
    *   The output and printing of results are minimal, lacking summaries, plots, or clear indications of what was done, hindering clarity.

The code demonstrates some understanding of the individual components but fails to integrate them correctly or completely as specified, especially in the more advanced Bayesian and causal inference sections. The deviation from PyMC3 for GP and the incorrect application of PSM are major drawbacks.",gemini-2.0-flash,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.neighbors import NearestNeighbors
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from sklearn.metrics imp...",4,"The generated code attempts to address many parts of the prompt but falls short on correctness, completeness, and adherence to specific instructions.

**Primary Reasons for Failure & Critical Missing/Incorrect Aspects:**

1.  **Fundamental Data Handling Errors:**
    *   `data = data.dropna()` and `data = data.drop_duplicates()` are called immediately after loading. The ground truth specifically outlines replacing `0` with `NaN` in specific columns (Glucose, BP, Insulin, etc.) and then using `SimpleImputer(strategy='median')`. The generated code's approach is highly problematic for this dataset, as `0` values in these columns often represent missing data, not actual zero measurements (e.g., a person cannot have 0 blood pressure and be alive). Dropping these rows would severely reduce the dataset and distort analysis.
    *   The `StandardScaler` is applied to `data[numeric_columns]` *before* feature engineering like `Metabolic_Syndrome` is added to `data`. This means the engineered features are not scaled or are scaled improperly if `numeric_columns` is re-evaluated later. The ground truth scales *after* all feature engineering.

2.  **Incorrect/Missing Feature Engineering:**
    *   The prompt asked for ""advanced feature engineering including metabolic syndrome indicators."" The generated code's `Metabolic_Syndrome` only uses `BloodPressure > 130` and `Insulin > 150`. This is a *gross oversimplification and clinical inaccuracy* compared to the ground truth's approach of creating `High_Glucose`, `High_BP`, `High_BMI` and then a `Metabolic_Syndrome_Score` based on multiple components, which is closer to clinical definitions. The ground truth also explicitly listed `Glucose` as part of metabolic syndrome, which is entirely missed here.
    *   Missing `DiabetesPedigreeFunction` and `Age` from feature engineering (though they are numeric, they weren't explicitly used for *new* features).

3.  **Clustering Algorithm Issues:**
    *   **OPTICS:** The ground truth shows `OPTICS` being called with `min_samples` and `xi`, and `min_cluster_size`. The generated code uses default `OPTICS` which often produces a very high number of clusters or noise, making it less useful without parameters tuned visually. The ground truth also implies a conceptual approach for `eps` in DBSCAN and `OPTICS` based on `NearestNeighbors` plot rather than just arbitrary defaults.
    *   **Spectral Clustering:** The `assign_labels='discretize'` is a good attempt, but the ground truth explicitly uses `assign_labels='kmeans'` which is often preferred for more robust results with spectral clustering.
    *   **Parameter Determination:** While it attempts silhouette and elbow, it entirely misses the **Gap Statistic** (which it imports but doesn't correctly implement or use in a practical way for determining optimal K). The ground truth also uses BIC/AIC for GMM, which is crucial. `determine_optimal_clusters` only uses KMeans, not the other algorithms.

4.  **Dimensionality Reduction Deficiencies:**
    *   **UMAP installation:** The comment `# Requires pip install umap-learn` is missing, which is a key detail for users.
    *   **Autoencoder:** The autoencoder uses `activation='sigmoid'` for the output layer, which is incorrect for reconstructing scaled numerical data (which can be negative or positive). `linear` activation is appropriate for this, as shown in the ground truth. Also, the autoencoder training is suppressed (`verbose=0`) in GT, which is a good practice.
    *   The plots for visualizing dimensionality reduction are missing, which GT provides.

5.  **Consensus Clustering:**
    *   The generated version calculates a co-occurrence matrix but then simply runs *KMeans on the co-occurrence matrix*. This is not standard consensus clustering; typically, you perform hierarchical clustering on a distance matrix derived from the co-occurrence matrix to get stable clusters. The ground truth explicitly uses `linkage` on the co-occurrence matrix to derive and cluster. This is a major functional flaw.

6.  **Patient Phenotype Profiles & Statistical Testing:**
    *   The `patient_phenotype_profiles` only returns means, not standard deviations or other comprehensive statistics that define a profile.
    *   `perform_statistical_tests` uses `ttest_ind` which is only for *two* groups. For comparing *multiple* clusters (which is the purpose of phenotyping across all clusters), you need ANOVA or Kruskal-Wallis, as correctly used in the ground truth. This is a significant statistical error.
    *   Visualization of cluster profiles (box plots etc.) is missing.

7.  **Anomaly Detection:**
    *   Only Isolation Forest is implemented (partially - it returns indices, not scores or a combined flag as used in GT). The prompt asks for ""anomaly detection,"" which implies multiple methods. Ground truth includes `OneClassSVM` and `LocalOutlierFactor`.
    *   Visualization of anomalies on dimensionally reduced plots is missing.

8.  **Time-Series Clustering:**
    *   The generated code *attempts* a placeholder, but `time_series_clustering` is very simplistic, uses KMeans directly on grouped data (which is not suitable for complex temporal patterns), and does not discuss crucial concepts like DTW or specific time-series feature engineering, which the ground truth's *conceptual explanation* correctly addresses. The ground truth explicitly states it cannot be performed on *this* dataset, which is a critical piece of domain knowledge. The generated code's attempt to run it on `timestamp` (a non-existent column) is a functional flaw.

9.  **Association Rule Mining:**
    *   The `apyori` library is used, which is less common and powerful than `mlxtend` (used in GT).
    *   Discretization of continuous features is crucial for ARM but is either poorly or not implemented. `data.select_dtypes(exclude=np.number).copy()` misses all the powerful continuous features. The ground truth uses `qcut` and `cut` for intelligent binning. This makes the ARM effectively useless as implemented.

10. **Patient Risk Stratification:**
    *   The `risk_stratification` logic for assigning 'Low', 'Medium', 'High' based on `overall_risk` and arbitrary 0.5 is a very simplistic heuristic. The ground truth provides a more robust, albeit still heuristic, approach based on sorting cluster prevalence and then assigning levels. Crucially, the ground truth provides a `Clinical Interpretation` section which is explicitly requested and completely missing in the generated code.

11. **Code Quality & Completeness:**
    *   Missing `import matplotlib.pyplot as plt` and `import seaborn as sns` as required for visualizations.
    *   Many imports are missing (e.g., `SimpleImputer`, `chi2_contingency`, `kruskal`, `f_oneway`, `IsolationForest`, `OneClassSVM`, `LocalOutlierFactor`, `factor_analyzer` is imported but not used, `mlxtend` is required for good ARM but not used).
    *   No proper print statements to indicate progress or results, unlike the ground truth.
    *   Assumes 'Outcome' is present and directly uses it for risk stratification and feature engineering without explanation.
    *   The final `print` statements are just printing the results of a function call, not nicely formatted outputs with clinical interpretations.",gemini-2.0-flash,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import VotingRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
import xgboost as xgb
...",6,"The generated code attempts to address all requirements but suffers from several issues that limit its completeness and correctness.

**Correctness:**
- **Preprocessing:** The `preprocess` function is overly simplistic and incorrect. It merely drops NaNs and then re-assigns the same columns to themselves, effectively doing very little. It uses hardcoded column names, which makes it brittle. It fails to handle categorical features (like 'Regional indicator') which are crucial for this dataset, leading to important feature information being lost. The `StandardScaler` is applied directly without a `ColumnTransformer` or proper pipeline, which is a common error when mixing numerical and categorical features. The ground truth correctly uses `ColumnTransformer` and comprehensive imputation/scaling.
- **Model Training:** Models are trained directly on `X_train` and `y_train` without being part of a robust preprocessing pipeline. This means the models won't see the preprocessed data in the same way during evaluation or new predictions (e.g., categorical features are entirely missing if not handled). The ground truth correctly incorporates preprocessing into pipelines for every model.
- **Voting Regressor:** The ""soft voting"" weights are set to `[1,1,1]`, which is equivalent to ""hard voting"" (simple averaging). Soft voting typically implies using optimized or performance-based weights.
- **Custom Ensemble/Blending/Multi-level Stacking:** These sections have ""dummy"" or ""placeholder"" implementations, relying on very basic averaging or just copying existing models. They don't demonstrate sophisticated dynamic weight assignment, multi-layered prediction passing, or proper setup for these advanced techniques as required. The ground truth provides a concrete example for dynamic weight assignment based on OOF errors.
- **Model Calibration:** `CalibratedClassifierCV` is for classification, not regression, and the `method='isotonic'` is typically for classification probability calibration. For regression, calibration involves assessing prediction intervals and checking residual properties, which the generated code does not accurately address.
- **Uncertainty Quantification:** The implementation is a ""dummy"" using `np.random.rand`, which provides no real uncertainty quantification.
- **Diversity Analysis & Combination Optimization:** The implementation is a ""dummy"" average, not a real analysis or optimization technique. It explicitly states, ""Implement in more detail for a more serious implementation."" The ground truth provides a detailed example of correlation analysis and optimization using `scipy.optimize.minimize`.
- **SHAP/LIME:** The SHAP explanation uses `shap.Explainer(model, X)`, which is inefficient for tree-based models (where `shap.TreeExplainer` is better) and might face issues with preprocessed data if not handled carefully. The `shap.summary_plot` and `lime_explanation.show_in_notebook` lines are commented out, indicating they may not run or display correctly without further setup. The LIME 'explainer_data' is `X_train` but the instance to explain is `X_train[0]`, which might not be correctly preprocessed for the model if the raw data features changed.

**Code Quality:**
- **Imports:** Good job on most imports.
- **Structure:** The code is structured into functions, which is good. However, the lack of proper pipelines around the preprocessing step makes the overall structure less robust.
- **Clarity:** Many comments indicate ""dummy"" or ""placeholder"" implementations, which, while honest, signal a lack of complete implementation for complex requirements.
- **Hardcoding:** Features are hardcoded, limiting flexibility.

**Completeness:**
- **Advanced Preprocessing/Feature Engineering:** Falls short significantly. It lacks handling of categorical features and a robust pipeline for preprocessing. The feature engineering is minimal.
- **Multiple Ensemble Methods:** While it lists and trains various ensembles, the ""soft voting"" the implementation is functionally identical to hard voting.
- **Custom Ensemble with Dynamic Weight Assignment:** Only a placeholder.
- **Blending and Multi-level Stacking:** Only placeholders or very basic averaging.
- **Model Interpretation (SHAP/LIME):** Present but might have issues with data transformation and commented-out visualization.
- **Model Calibration and Uncertainty Quantification:** Incorrect use of `CalibratedClassifierCV` for regression and dummy uncertainty quantification.
- **Ensemble Diversity Analysis and Model Combination Optimization:** Only dummy implementations.

Overall, the code demonstrates an understanding of the concepts but fails to implement several advanced requirements correctly or completely. The preprocessing section is particularly weak and impacts the overall robustness of the modeling approach.",gemini-2.0-flash,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.neighbors import NearestNeighbors
from patsy import dmatrices
import theano.tensor as tt
from sklearn.preprocessing import StandardScaler
import arviz as az  # Corrected import
import matplotlib.pyplot as plt
import seaborn as sns
import math

# 1...",4,"The generated code attempts to address all aspects of the task, but significantly falls short in correctness and completeness for several key requirements.

**Correctness Issues:**
*   **Missing Value Handling:** The current implementation uses `fillna(df[col].median())` for all columns except the first, which is overly simplistic. It doesn't differentiate between numerical and categorical columns, potentially applying median imputation to strings if not careful and doesn't standardize numerical values, which is crucial for many of the models. The ground truth uses `SimpleImputer`, `StandardScaler`, and `OneHotEncoder` via `ColumnTransformer` for a comprehensive and correct approach.
*   **Hierarchical Bayesian Model:**
    *   The model defines `a[region_idx]` for regional intercepts but then calculates `mu` as `a[region_idx] + pm.Normal(""beta_gdp"", mu=0, sigma=1) * df['GDP'].values + pm.Normal(""beta_social"", mu=0, sigma=1) * df['SocialSupport'].values`. This is incorrect. `beta_gdp` and `beta_social` should be defined once as single parameters, not as `pm.Normal` within the `mu` calculation itself resulting in new Normal distributions for each data point. This makes the `mu` definition syntactically wrong and fundamentally misrepresents the model.
    *   It only includes GDP and SocialSupport as fixed effects, while the task implicitly asks for other factors (Healthy life expectancy, Freedom, Generosity, Corruption) which are included in the ground truth's hierarchical model.
    *   The priors are specified without clear justification relative to the task's requirement of ""informative priors based on happiness research literature.""
*   **Structural Equation Model (SEM):** The generated code uses `statsmodels.formula.api.ols` for SEM. While OLS regression can be a component of SEM, SEM traditionally involves latent variables, multiple indicators, and explicit causal path modeling often handled by specialized SEM software (e.g., `lavaan` in R, or `statmodels` with more specific structural equations). The task asked for ""Building structural equation models (SEM) to analyze causal pathways,"" which implies more than just a single OLS regression of all predictors on the outcome. The ground truth addresses this by approximating SEM using a series of interconnected PyMC3 models, explicitly modeling GPD -> Social Support and Social Support + GDP -> Happiness, which is a better approximation of SEM principles within the PyMC3 context.
*   **Instrumental Variables (IV):**
    *   The choice of `LifeExpectancy` as an instrument for `GDP` is highly questionable. It is very likely that `LifeExpectancy` directly affects `HappinessScore`, violating the exclusion restriction (instrument affects outcome only through the endogenous variable). The task notes that a true instrument might not be available in cross-sectional data, and the ground truth simulates one. The generated code uses one that is likely invalid.
    *   The PSM implementation is overly simplistic. It uses `predict_proba` on `X` directly, which includes `SocialSupport`, `LifeExpectancy`, etc., which are also used in the IV model. The matching logic, especially the `cutoff_pscore` and simply picking the `first available match`, is not robust and doesn't ensure good balance between groups.
    *   The `att` calculation is based on `df['GDP'] > df['GDP'].median()` which is a very crude definition of treatment, not aligned with real-world causal questions.
*   **Gaussian Processes:** While a GP model is set up, the `az.plot_gp_posterior` is missing, which would visualize the GP fit. The ground truth includes this important visualization.
*   **Bayesian Model Comparison:** The generated code directly calls `pm.waic(trace1, model=model1)` and `pm.loo(trace1, model=model1)`. In PyMC3 v3, `pm.waic` and `pm.loo` are deprecated in favor of `az.waic` and `az.loo` from ArviZ, which expect `arviz.InferenceData` objects. The generated code also attempts `az.bf.compare`, but the syntax for `models` is incorrect (it should be `dict` of models, not a tuple of models). The ground truth correctly uses `az.compare`.
*   **Sensitivity Analysis:** The sensitivity analysis portion is entirely non-Bayesian and simplistic. It uses an OLS model and just perturbations of `GDP` in the `predict` function. The task explicitly asks for ""robust inference"" and implicitly for Bayesian sensitivity analysis where one would typically vary prior distributions and observe changes in posterior distributions. The ground truth demonstrates this by re-running a model with wider priors and comparing posteriors.

**Completeness Issues:**
*   **Comprehensive Missing Value Handling:** As noted above, the implementation is too basic and not comprehensive enough.
*   **Informative Priors:** While a Bayesian regression with informative priors is included, the `mu` and `sigma` values for those priors are somewhat arbitrary (""Example based on assumption"") rather than truly derived from ""happiness research literature"" as requested. The ground truth explicitly states choices and rationale for informative priors.
*   **Mediation Analysis:** The mediation analysis is done using OLS models as in classic Baron and Kenny approaches, but the task implies a Bayesian approach and the ground truth uses the posteriors from the approximated SEM (Bayesian regressions) to calculate the indirect and total effects, including their uncertainty (HDI).
*   **Difference-in-Differences:** The simulation of the time component and policy is very basic and non-representative. The `df['Time']` and `df['Policy']`/`df['Treated']` are assigned randomly. A more robust simulation would ensure the `Treated` group is consistent across time and that the simulated policy effect is correctly applied to the `post` and `treated` observations. The ground truth has a more coherent simulation.
*   **Credible Intervals, Posterior Predictive Checks, Sensitivity Analysis:** While sections for these are present, their execution is either incomplete (PPC plot missing for GP, sensitivity analysis is non-Bayesian and simplistic) or not fully integrated as expected. The task explicitly asks for plots for PPC/GP, which missing in the generated code.

**Code Quality:**
*   Imports are mostly correct, but the `import arviz as az` is followed by a comment `# Corrected import` which implies a previous error, but it's okay.
*   The use of functions for each section improves readability, but the `if __name__ == ""__main__"":` block becomes very long and prints many summaries without proper labels or organization.
*   Minor issues like `pm.sample` having `cores=1` which slows down sampling, though understandable for portability.
*   Variable naming is generally good.
*   Lack of plots (e.g., for PPC, GP, and sensitivity analysis) makes the output less informative.
*   The use of `pm.summary` instead of `az.summary` for printing is an older PyMC3 practice, `az.summary` is preferred and more consistent with `arviz` being used for other comparison functions.",gemini-2.0-flash,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
import hdbscan
from sklearn.manifold import TSNE, LocallyLinearEmbedding, Isomap
import umap
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from gap_statistic import OptimalK
from sklearn.metrics import pairwise_distances
from sklearn...",6,"The generated code attempts to address all parts of the prompt but falls short in several areas, particularly regarding the complexity and correctness of the implementations requested in the prompt.

**Correctness (3/10):**
*   **Preprocessing and Feature Engineering:** The `preprocess_data` function explicitly drops the 'Regional indicator' column early on, which is a key categorical feature mentioned in the prompt for advanced feature engineering (""regional characteristics""). The ""Feature Engineering"" part adds very basic numerical additions/multiplications without much thought, and notably misses dummy encoding for the regional indicator as done in the ground truth. The missing value imputation is a simple mean for all numerical columns, which is less robust than specified (median for numerical, mode for categorical).
*   **Clustering:** The `apply_clustering` function, while structured to apply multiple algorithms, uses hardcoded `n_clusters=5` for K-Means, Hierarchical, GMM, and Spectral Clustering. The prompt explicitly asks for optimal parameter determination using silhouette analysis, gap statistic, elbow method, and stability analysis, which is not fully integrated into the clustering application itself. The `SpectralClustering` affinity parameter is fixed to 'nearest_neighbors' without exploring other options.
*   **Dimensionality Reduction:** The prompt asks for **autoencoders** for dimensionality reduction, but the generated code uses `PCA` as a placeholder (`'Autoencoder': PCA`). This is a critical omission. `LocallyLinearEmbedding` is used for ""ManifoldLearning"" which is a valid choice, but the prompt also asks for others like Isomap.
*   **Optimal Clustering Parameters:** The placeholder for `tune_kmeans` correctly shows silhouette analysis, but Gap Statistic and Elbow Method are only mentioned as ""Implement similarly,"" which means they are not implemented. Stability analysis is entirely missing.
*   **Consensus Clustering:** The implementation for consensus clustering is fundamentally flawed. It calculates a co-occurrence matrix `co_occurrence` by counting how many algorithms put two samples in the same cluster. This matrix actually represents a similarity matrix. However, it then applies `AgglomerativeClustering` *directly* to this similarity matrix, which is incorrect. Hierarchical clustering algorithms typically operate on a *distance* matrix, not a similarity matrix. The `linkage` function in `scipy.cluster.hierarchy` is usually used with condensed distance matrices derived from the co-occurrence (similarity) matrix (e.g., `distance = 1 - co_occurrence`). The current implementation would treat high similarity as high distance, leading to incorrect clustering.
*   **Country Profiles:** The `create_country_profiles` function attempts statistical significance testing but uses `ttest_ind` to compare each cluster against ""others_data"". While potentially informative, the prompt specifically asks for ANOVA which compares *all* groups simultaneously, and effect size analysis using Eta-squared, not Cohen's d. Cohen's d is suitable for two-group comparisons, but Eta-squared is more appropriate for ANOVA.
*   **Anomaly Detection:** `IsolationForest` and `LocalOutlierFactor` are used correctly, but `OneClassSVM` was also requested in the prompt and is missing.
*   **Network Analysis:** The network creation logic for `corr_matrix.iloc[idx_country1,idx_country2]` seems flawed. `corr_matrix` should be computed on the numerical features of the countries, and its `iloc` indexing relies on the order of countries in `df_processed`, which might not align directly with their index in `df`. A more robust approach would be to get the scaled features and then compute pairwise similarity there. The selection of `clusters['KMeans']` for node attributes in the network graph, when ""consensus_labels"" is available, is arbitrary and less ideal as the prompt specified ""relationships between countries based on happiness factors"" implying a more robust clustering.
*   **Association Rule Mining:** The comment `df_categorical = pd.get_dummies(df.drop('Country name', axis=1))` says ""convert all to categorical"", but `pd.get_dummies` on a numerical dataframe will perform one-hot encoding assuming each unique numerical value is a category. ARM requires discretization of numerical features into meaningful bins (e.g., 'low', 'medium', 'high') first, which was done in the ground truth using `pd.qcut`. This part is fundamentally incorrect for numerical data.
*   **Recommendation System and Policy Insights:** The recommendation system is very basic (just lists all countries in the same cluster based on KMeans, not consensus). The policy insights section is just a print statement based on a `policy_insights` function that tries to find 'significant features' using t-test and Cohen's d, which isn't the primary way to derive *policy insights* from cluster profiles. It lacks the rich, interpretive comparison across clusters (e.g., highest vs. lowest happiness clusters) and geopolitical validation as described in the prompt.

**Code Quality (6/10):**
*   **Imports:** The imports are mostly good, but `scipy.stats.ttest_ind` and `statsmodels.stats.effectsize.cohend` are imported even though `cohend` is not a standard function in `statsmodels.stats.effectsize` (it seems to be a custom or non-standard import based on the line).
*   **Structure:** The use of functions for each section is good, but there's no overarching class structure as in the ground truth, which would encapsulate the state and make the analysis more modular and reusable.
*   **Readability:** Variable names are generally clear. Comments are sparse and occasionally misleading (e.g., ""Replace PCA with Autoencoder if you have one"").
*   **Error Handling:** Basic try-except blocks are used, which is a good practice.
*   **Modularity:** The functions are somewhat modular, but the lack of a class means intermediate results like `df_processed`, `df_scaled`, `clusters`, `reduced_data`, etc., are in the global scope, making it harder to manage state.

**Completeness (7/10):**
*   **All sections are attempted:** The code has placeholders or basic implementations for almost all requested sections.
*   **Missing specific algorithms/techniques:** As noted above, `OneClassSVM` for anomaly detection is missing, and the autoencoder for dimensionality reduction is replaced by PCA. Gap statistic and stability analysis are missing.
*   **Depth of implementation:** Many implementations are shallow (e.g., consensus clustering, association rule mining, policy insights) and do not meet the ""advanced"" nature specified in the prompt. For instance, the ""policy insights"" section requires significant qualitative analysis and comparison, not just statistical output.

Overall, the generated code provides a structural outline for the task but fails to deliver on the ""advanced"" and ""expert"" level requirements in key areas due to incorrect or overly simplistic implementations, especially regarding consensus clustering, feature engineering, and the specific statistical/machine learning techniques requested. It would require significant work to become a robust solution.",gemini-2.0-flash,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX
from pmdarima import auto_arima
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import numpy as np

# 1. Load data, parse 'date', filter for 'Germany', and set 'date' as index.
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
df = df[df['location'] == 'Germany'].set_index('date')
df = df.sort_index()

# 2. Use 'new_cases' as the target variable. Consider 'string...",8,"The generated code correctly implements the core requirements of the task, including loading data, filtering, setting index, handling missing values (though somewhat simplified compared to ground truth), splitting data, using `auto_arima` for order selection, training SARIMAX with exogenous variables, forecasting, evaluating with RMSE, and plotting.

Strong points:
- Correctly uses `auto_arima` for order selection (`m=7` for seasonality).
- Includes exogenous variables in both `auto_arima` and `SARIMAX` fitting.
- Performs proper train-test split.
- Calculates RMSE and generates the plot as requested.
- Handles `parse_dates` during CSV loading.

Areas for improvement:
- **Missing Value Handling:** The generated code uses `interpolate(method='linear')` for all columns. While this is one valid method, the ground truth uses a more robust approach with `ffill()` and `bfill()` first, followed by `interpolate()` as a fallback, and includes checks for data integrity after imputation. For 'new_cases', forward fill is often preferred for time series as it uses past information. Negative forecasts are also not handled by explicitly setting them to 0 (though common in time series, `pmdarima` or `statsmodels` might not automatically zero them out).
- **Error Handling and Robustness:** The ground truth includes more robust error handling (e.g., `FileNotFoundError`, checks for sufficient data length, handling empty dataframes after dropping NaNs). The generated code assumes the CSV file exists and the data is always sufficient.
- **Plotting Detail:** The plot title and labels are good, but the ground truth adds more detail by including the training data in the plot and the RMSE in the title.
- **Clarity in Variable Naming/Steps:** While functional, the ground truth's use of more descriptive variable names for `y_train`, `X_train`, etc., and explicit comments separating the steps, makes the code slightly easier to read and understand, especially for complex time series workflows.
- **Suppressed Warnings:** The ground truth explicitly imports `warnings` and suppresses them for cleaner output, which can be useful when running `auto_arima`. The generated code uses `suppress_warnings=True` within `auto_arima`, but a general warning suppression might be beneficial if other parts of `statsmodels` produce warnings.",gemini-2.0-flash,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scikit_posthocs import posthoc_dunn

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Define quality groups
df['quality_tier'] = pd.cut(df['quality'], bins=[0, 4, 6, 10], labels=['low', 'medium', 'high'])

# Statistical tests and visualizations
results = {}
significant_features = []

for col in d...",4,"The generated code has several critical issues that make it largely incorrect and incomplete for the given task.

**Correctness Issues:**

1.  **Defining Quality Groups:** The `pd.cut` approach for defining quality groups `bins=[0, 4, 6, 10]` categorizes wines with quality 4 into 'low' and quality 6 into 'medium'. The ground truth explicitly states: 'low quality' wines (score <= 4), 'medium quality' (score 5-6), and 'high quality' (score >= 7). This is a fundamental misunderstanding of the problem's definition for quality tiers. The `cut` function with `right=True` (default) means `(0, 4]` which includes 4 in 'low', but `(4, 6]` means 4 is *not* included, so quality 4 would be an issue. If `right=False` was used, `[0, 4)` and `[4, 6)`. The ground truth used `df.apply` which correctly assigns `score <= 4` to 'low', `5 <= score <= 6` to 'medium', and `score >= 7` to 'high'.
2.  **Handling Missing Data in Shapiro-Wilk:** The generated code directly calls `stats.shapiro(df[df['quality_tier'] == 'low'][col])` without handling potential `NaN` values or empty arrays. If a group happens to have `NaN`s, `shapiro` might fail or produce incorrect results. More importantly, `shapiro` requires at least 3 data points. The code doesn't check for this, which could lead to errors.
3.  **Levene's Test for Homogeneity of Variances:** The generated code completely skips the check for homogeneity of variances (Levene's test), which is a crucial assumption for ANOVA. Without this check, using ANOVA solely based on normality is theoretically flawed. If data is normal but variances are unequal, Welch's ANOVA or a non-parametric test is more appropriate. The ground truth correctly implements Levene's test.
4.  **Post-hoc Test for Kruskal-Wallis:** The `scikit_posthocs.posthoc_dunn` library needs the data in a flat format with a corresponding `group_col`. The ground truth implements pairwise Mann-Whitney U tests with Bonferroni correction, which is a common and robust approach when Dunn's test (which `scikit_posthocs` provides) is not directly called. The `scikit_posthocs` `posthoc_dunn` function is a valid choice; however, the ground truth's manual implementation of Bonferroni-corrected pairwise Mann-Whitney is also acceptable and provides a slightly different perspective. The `scikit_posthocs` was not imported initially in the prompt specification.
5.  **Data for Tukey HSD:** The `pairwise_tukeyhsd` function requires `endog` (the dependent variable) and `groups` (the independent variable). The generated code `pairwise_tukeyhsd(df[col], df['quality_tier'], alpha=0.05)` is correct in its arguments. However, the ground truth also correctly concatenates the data to ensure correct group alignment in `statsmodels`, though `df[col]` and `df['quality_tier']` should work directly if indices align.
6.  **Error Handling:** The generated code lacks basic error handling, such as checking for the CSV file's existence, or handling cases where a quality tier might be empty for a particular feature (e.g., if there are no 'low' quality wines in the dataset for some reason).

**Completeness Issues:**

1.  **Missing Imports and Configuration:** While most necessary libraries are imported, `statsmodels.stats.multitest.multipletests` is missing, which is essential for manual Bonferroni correction. The ground truth also explicitly defines `ALPHA` as a configuration constant, making the code more readable and maintainable.
2.  **Insufficient Output/Summarization:** The summary section in the generated code is basic. The ground truth provides detailed print statements during the statistical tests (showing p-values, test type, significant status) and then a clear, organized summary of features with significant differences, including primary test results and post-hoc details. The generated code prints the `tukey_result` object directly, which is verbose, and the `dunn_result` dataframe. While correct, it doesn't present it as a concise textual summary.
3.  **Visualization Details:** While box plots are generated, the ground truth explicitly sets `plt.title`, `plt.xlabel`, `plt.ylabel`, `plt.grid`, `plt.tight_layout`, and uses `quality_order` for consistent plotting, making the visualizations more professional and readable. The generated code lacks some of these details and does not specify the order for the categorical variable `quality_tier` before plotting.

**Code Quality Issues:**

1.  **Readability/Comments:** The ground truth code has more extensive comments and better print statements to guide the user through the analysis steps.
2.  **Repetitive Data Extraction:** For each feature, data for 'low', 'medium', 'high' quality tiers are extracted repeatedly. While not a performance bottleneck for this dataset, it could be slightly more efficient.
3.  **Implicit Assumptions:** The loop `for col in df.columns[:-2]` implicitly assumes that the last two columns are `quality` and `quality_tier`, which is brittle. The ground truth explicitly extracts `chemical_properties` by excluding these columns, which is a safer approach.

Overall, the generated code attempts to solve the problem but contains several significant methodological and implementation flaws, particularly regarding statistical rigor (missing Levene's test, fragile normality checks) and incorrect quality group definitions.",gemini-2.0-flash,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import xgboost as xgb
from sklearn.metrics import roc_auc_score, f1_score
import matplotlib.pyplot as plt
import shap
from sklearn.impute import SimpleImputer

# Load data
df = pd.read_csv('synthetic_covid19_data.csv')

# 1. Data Loadin...",7,"The generated code addresses most of the requirements but has several issues that affect its correctness and robustness, particularly in data preprocessing and feature engineering.

**Areas for Improvement:**

1.  **Target Variable Definition:**
    *   The prompt specifies defining ""high\_daily\_death\_increase"" as `(df['new_deaths'] > df['location_90th_percentile_deaths']) & (df['new_deaths'] > 0)`.
    *   The generated code's `create_target` function defines it as `(location_df['new_deaths'] > threshold)`, where `threshold` is the 90th percentile of `new_deaths > 0`. However, it *does not* include the crucial condition `(location_df['new_deaths'] > 0)` in the final comparison for the target variable. This means a day with zero new deaths could be classified as `high_daily_death_increase` if a location's 90th percentile was 0 (e.g., if all its `new_deaths` were 0 or very small), which contradicts the definition. If `positive_deaths` is empty, `threshold` will be `NaN`, leading to `False` for all rows, which is acceptable but the missing `new_deaths > 0` condition is a more significant flaw.

2.  **Missing Value Handling:**
    *   The prompt specifically asks to ""Handle missing values from feature engineering (e.g., fill with 0 or backfill)"".
    *   The generated code uses a blanket `df.fillna(0, inplace=True)` for *all* NaN values in the entire DataFrame *after* feature engineering. This is problematic:
        *   It fills `NaN` values in `location` and `date` (if any, though unlikely after load) which would cause issues.
        *   Crucially, `new_deaths` and other original features might have legitimate `NaN`s that should be handled differently (e.g., imputation specific to those columns) rather than just 0, especially `reproduction_rate` which often has `NaN`s and is used in lag features.
        *   For rolling/lag features, `fillna(0)` can distort the data if `NaN`s are at the beginning of a sequence. The prompt mentioned ""backfill"" as an option, implying a more nuanced approach than just filling zeros everywhere. The ground truth carefully fills *only* engineered columns, prioritizing ffill within a group then fillna(0).

3.  **Feature Engineering Order and Grouping:**
    *   The generated code performs feature engineering *after* the initial `df.copy()`. If `df` is not sorted by `location` and `date` before feature engineering, the `rolling` and `shift` operations will produce incorrect results, as they operate on the full dataframe sequence rather than within each `location` group. The ground truth correctly sorts the data first.
    *   The `first_case_date` and `days_since_first_case` calculation is incorrect. `location_df[location_df['new_cases'] > 0]['date'].min()` gets the *first* date with >0 new cases WITHIN THAT *SUB-GROUP* (which is the current location's entire history). But `location_df['first_case_date'] = ...` assigns a single value to the whole column, which is incorrect for each row's context. This should be done carefully to assign the minimum date for each location to *all* rows of that location. The ground truth example for `days_since_first_case` uses `transform` on the `date` column *after* `groupby('location')` which correctly ensures `x.min()` is applied per group and broadcast back. The generated code also calls `astype(int)` on `days_since_first_case` which can fail if `(date - first_case_date)` results in a Timedelta that's `NaT`.

4.  **Vaccination Rate Calculation:**
    *   The feature selection section incorrectly calculates `vaccination_rate` on `X` *after* `X` has been created, meaning `people_fully_vaccinated` and `population` are copied and then modified. Also, `X.drop` and `numerical_features.remove` are modifying the lists in place, which is messy. It's better to define the full list of features *first* and then create `X`. The ground truth creates this derived feature upfront.

5.  **SHAP Feature Names:**
    *   The SHAP feature names extraction logic is incomplete and potentially incorrect. `feature_names.extend(grid_search.best_estimator_.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out().tolist())` only gets the OHE names for the categorical transformer, but it needs to apply to the *original* categorical features. If `remainder='passthrough'` is used, `feature_names` should also account for those. A more robust way is to get the `get_feature_names_out()` from the entire `ColumnTransformer` (if supported directly in the sklearn version used, otherwise manually combine as done in GT). The ground truth's approach is more robust by getting names from both parts of the transformer.

6.  **Efficiency and Good Practices:**
    *   Numerous calls to `.copy()` for intermediate DataFrame modifications (e.g., `df = create_target(df.copy())`, `df = feature_engineering(df.copy())`, `df = handle_missing_values(df.copy())`) are inefficient and unnecessary. Operations can often be performed in-place or chained without repeated copying.
    *   The time-based split is done after the features are already in `X`, but `X` is not explicitly sorted by date *before* the split, risking incorrect time splits if the original DataFrame was not sorted. The ground truth correctly sorts the `df` *before* slicing for time-based split.
    *   Imports are a bit scattered.

Despite these issues, the code correctly sets up the pipeline for scaling and encoding, uses `GridSearchCV` for hyperparameter tuning of XGBoost, evaluates with ROC-AUC and F1, and attempts SHAP explanation, demonstrating a good grasp of the overall task requirements. The issues are mainly in the data preparation and specific implementation details of feature engineering and handling missing values, which are critical for correctness.",gemini-2.0-flash,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.formula.api as smf
import statsmodels.api as sm

# 1. Load data, parse 'date', and calculate 'vaccination_percentage'
try:
    df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
except FileNotFoundError:
    print(""Error: synthetic_covid19_data.csv not found. Please make sure the file is in the same directory as the script, or provide the full path."")
    exit()
df['vaccination_percentage'] = df['people_fully_v...",7,"The generated code correctly attempts to solve most parts of the problem, including loading data, performing some imputation, building a mixed-effects model, and reporting results.

**Strong Points:**
*   Correctly uses `smf.mixedlm` for mixed-effects modeling.
*   Includes the interaction term as requested.
*   Attempts to parse dates and calculate vaccination percentage.
*   Prints the model summary, which includes fixed effects coefficients, standard errors, and p-values.
*   Reports the covariance matrix for random effects.
*   Mentions model assumptions.

**Areas for Improvement / Issues:**

1.  **Handling `vaccination_percentage` Calculation and Missing Values:**
    *   **Initial Calculation:** The generated code calculates `df['vaccination_percentage'] = df['people_fully_vaccinated'] / df['population']` *before* any imputation for `population` or `people_fully_vaccinated`. If `population` or `people_fully_vaccinated` has NaNs or zeros, this calculation will produce NaNs or Infs in `vaccination_percentage` from the start, before the imputation is even applied. The ground truth first imputes `population` and `people_fully_vaccinated` and then calculates `vaccination_percentage`, which is a more robust approach.
    *   **Imputation Strategy:** The imputation for `vaccination_percentage` first fills NaNs with `0` globally and then with the median grouped by location. This is problematic. If `vaccination_percentage` is truly missing, a `0` might be an incorrect assumption. The median imputation within groups (after calculating from potentially imputed `population` and `people_fully_vaccinated`) is better, but the initial `fillna(0)` is risky. The ground truth's linear interpolation within groups and then global median fallback is more standard for time-series-like data.
    *   **Clipping:** The `vaccination_percentage` is not capped at 100%, which can lead to values greater than 1. This is a common issue with raw `people_fully_vaccinated / population` calculations due to data inconsistencies. The ground truth correctly addresses this by clipping to 100.

2.  **Random Effects Specification:**
    *   The task explicitly asked for ""random intercepts for 'location' nested within 'continent'"". The generated code uses `groups=df[""continent""], re_formula=""~location""`. In `statsmodels.mixedlm`, `re_formula=""~location""` means random slopes for `location` *within* the groups defined by `continent`, *plus* a random intercept if `re_formula` is specified this way (it implies `~1+location` if `1` is not excluded) or just random `location` which acts as a random intercept if `location` is categorical. However, the standard way to specify nested random intercepts is to create a unique ID for each `(continent, location)` combination and use that as the `groups` variable, with `re_formula=""1""`. The ground truth implements this correctly by creating `continent_location_id`. The generated code's approach is different and may not capture the intended nesting.

3.  **Missing Value Handling Completeness:**
    *   The generated code's imputation strategy is less comprehensive than the ground truth's. It only uses median imputation. Linear interpolation, especially after sorting by date, is often preferred for time-series data to retain temporal trends.
    *   It doesn't handle `population` or `people_fully_vaccinated` missing values directly before `vaccination_percentage` calculation, leading to the issue mentioned in point 1.

4.  **Implicit vs. Explicit Interpretation:**
    *   The generated code provides comments for interpreting fixed effects and interaction, which is good. However, the ground truth provides a more explicit programmatic interpretation based on the actual p-values and coefficients, which is more robust and directly reflects the model output.

5.  **Robustness and Warnings:**
    *   The ground truth includes error handling for file not found and robust warning suppression for `statsmodels` convergence issues, which is good practice in a real-world scenario. The generated code has a `FileNotFoundError` check but no warning suppression for model fitting.

**Conclusion:** The generated code is a good start and demonstrates understanding of mixed-effects models. However, the details of data preprocessing (especially vaccination percentage calculation and comprehensive imputation) and the specific random effects structure requested in the prompt are not fully optimized or correctly implemented compared to the ground truth.",gemini-2.0-flash,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

# 1. Load the dataset
data = pd.read_csv(""clash_royale_cards.csv"")

# 2. Select features
features = ['elixirCost', 'Win Rate', 'Usage']
X = data[features]

# 3. Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. Determine optimal K u...",9,"The generated code is highly correct and complete, addressing almost all aspects of the task.

**Strong Points:**
- **Correct Imports:** All necessary libraries are imported.
- **Data Loading and Feature Selection:** Correctly loads the dataset and selects the specified features.
- **Feature Standardization:** Properly uses `StandardScaler` to standardize the features.
- **K-Means Application:** Correctly applies KMeans with the chosen `K`.
- **Dimensionality Reduction (PCA):** Correctly performs PCA to reduce dimensions to 2 components.
- **Visualization:** Creates a scatter plot of PCA components, colored by cluster, with appropriate labels and a colorbar.
- **K-selection method:** The code correctly implements both the Elbow method and Silhouette score for determining K, which is a good practice as it provides more options for the user.

**Areas for Improvement (reflected in the 9/10 score):**
- **Error Handling:** The ground truth code includes robust error handling for `FileNotFoundError` and missing columns. The generated code lacks this, which makes it less robust for real-world scenarios.
- **Explicit K-Choice Justification:** While the code plots both WCSS and Silhouette scores, it then *hardcodes* `K = 3` with a comment ""Based on elbow method, choosing K=3"". This assumes the user will interactively interpret the plots and then manually update the code. The prompt implies that `K` could be justified from the plots within the code's context, but since it's hardcoded, the plots essentially become informative outputs rather than a method applied *by the code* to determine K. The prompt implied using one of these methods to *justify* the choice of K, which the code does by explicitly stating K=3 (and then showing the plots). It doesn't programmatically select K, but for this kind of problem, visual inspection for K is very common.
- **Plot Labels for PCA:** The PCA plot labels are `Principal Component 1` and `Principal Component 2`. While correct, specifying the explained variance ratio in the axis labels (as done in the Ground Truth) (e.g., `Principal Component 1 (XX%)`) provides valuable context and enhances the plot's interpretability.

Overall, it's a very good solution that successfully achieves the core requirements. The minor points relate to robustness and enhanced output details.",gemini-2.0-flash,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import numpy as np


# Load the dataset
df = pd.read_csv(""clash_royale_cards.csv"")

# Select features and target
features = ['elixirCost', 'Usage', 'rarity']
target = 'Win Rate'...",9,"The generated code correctly addresses all requirements of the task. It loads the data, selects the specified features and target, preprocesses the 'rarity' column using one-hot encoding (specifically using `ColumnTransformer` and `OneHotEncoder` within a `Pipeline`, which is a robust and flexible approach), splits the data, trains a `RandomForestRegressor`, and evaluates it using R-squared and MSE. The proper imports are included.

The use of `Pipeline` and `ColumnTransformer` for preprocessing is a very good practice in production-ready code as it encapsulates the entire workflow and prevents data leakage.

Minor improvements could include:
1.  Adding error handling for `FileNotFoundError` when loading the CSV, similar to the ground truth.
2.  Handling potential missing values in the selected features or target columns, even if not explicitly required by the prompt, it's good practice.
3.  The `numpy` import is not strictly necessary for this specific code.",gemini-2.0-flash,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy import stats

# 1. Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 2. Identify unique categories in the 'rarity' column
rarity_categories = df['rarity'].unique()

# 3. Perform ANOVA test
# Null Hypothesis (H0): The means of the win rates are equal across all rarity categories.
# Alternative Hypothesis (H1): At least one rarity category has a different mean win rate.

win_rates_by_rarity = []
for rarity in rarity_categories:
    win_rates_by...",7,"The generated code correctly addresses most of the requirements but lacks robustness and thoroughness in data handling and clear hypothesis statement.

**Strong Points:**
*   Correctly loads the dataset.
*   Correctly identifies unique rarity categories.
*   Correctly performs the ANOVA test using `scipy.stats.f_oneway`.
*   Correctly prints the F-statistic and p-value.
*   Correctly interprets the p-value against the alpha level and states the conclusion.

**Areas for Improvement:**
*   **Missing Hypothesis Statement (Printing):** The task explicitly asks to ""state the null and alternative hypotheses for the test"" (point 4). While the code has comments indicating the hypotheses, it does not print them to the console, which is a requirement for stating them.
*   **Lack of Robust Error Handling for File Loading:** The ground truth code includes `try-except` blocks for `FileNotFoundError` and general exceptions during file loading, making it more robust. The generated code assumes the file will always be present.
*   **No Handling for Non-Numeric 'Win Rate':** The `Win Rate` column might contain non-numeric data. The ground truth explicitly converts it to numeric using `pd.to_numeric` with `errors='coerce'` and handles NaNs. The generated code assumes `Win Rate` is already clean numeric data, which is not a safe assumption in real-world datasets.
*   **No Data Cleaning for NaNs:** Related to the above, rows with NaN values in `Win Rate` or `rarity` are not explicitly dropped. ANOVA requires complete observations for each group.
*   **No Check for Sufficient Data/Groups:** The ground truth code includes checks to ensure there are at least two unique rarity categories and that each group has at least two observations. This prevents errors or misleading results if the dataset is too sparse for ANOVA. The generated code would fail or give incorrect results if, for instance, `win_rates_by_rarity` contains arrays of length 0 or 1, or if there's only one rarity category.
*   **Clarity of Output:** The output for F-statistic and P-value could be formatted for better readability (e.g., to a certain number of decimal places). The ground truth also explicitly prints the alpha value and uses separators (`---`) for better visual organization.

Overall, the core logic for ANOVA is sound, but it misses several crucial steps for a production-ready or robust analysis, especially concerning data preparation and error handling.",gemini-2.0-flash,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# 1. Load the dataset
iris_data = pd.read_csv(""iris.csv"")

# 2. Define features X and target y
X = iris_data[[""SepalLengthCm"", ""SepalWidthCm"", ""PetalLeng...",8,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, defines X and y, encodes the target variable, splits the data, trains the specified models, calculates and prints all required metrics, and identifies the best model based on F1-score.

Strong points:
- All required imports are present and correctly used.
- Data loading and feature/target selection are correct.
- Label Encoding is correctly applied.
- Data splitting uses the correct `test_size` and `random_state`.
- All three specified classifiers are trained with the correct parameters.
- All four required metrics (accuracy, precision, recall, F1-score, all macro-averaged) are calculated and printed.
- The best model is correctly identified based on the highest F1-score.
- The structure is logical and easy to follow.

Areas for minor improvement:
- **Missing `stratify` in `train_test_split`**: The ground truth uses `stratify=y_encoded` (or `stratify=y` from the generated code's perspective) during the train-test split. While not explicitly asked for in the prompt, it's a good practice for classification tasks, especially with potentially imbalanced classes, to ensure class distribution uniformity across splits. Its absence doesn't make the code ""incorrect"" per se based on the prompt, but it's a common best practice for reproducibility and robust evaluation.
- **`max_iter` for Logistic Regression**: The ground truth included `max_iter=200` for Logistic Regression. While the default `max_iter=100` might work for this dataset, sometimes it's necessary to increase it to ensure convergence, especially in more complex datasets. This was not explicitly stated in the prompt, but it's a detail worth noting from the ground truth.
- **Error Handling**: The ground truth included basic error handling for `FileNotFoundError` when loading the CSV, which is a good practice. The generated code lacks this. This is not explicitly asked by the prompt, so it doesn't reduce the score significantly, but it's a point of robustness.

Overall, the generated code met nearly all explicit requirements of the prompt and functions correctly. The missing `stratify` is the primary reason for not giving a perfect score, as it impacts the reproducibility and representativeness of the splits for classification problems.",gemini-2.0-flash,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv(""iris.csv"")

# Select features
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
X = data[features]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# K-Means clustering
kmeans = KMeans(n_clusters=3...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, selects and standardizes features, applies K-Means clustering with the specified parameters, performs PCA, and generates both required scatter plots. The use of `fit_predict` for K-Means labels is good, although `fit` followed by `kmeans.labels_` also works correctly. The plotting approach using loops for coloring is functional and provides good clarity for legends, though a direct `c=` mapping as in the ground truth can be more concise.

Minor points for improvement:
1.  The ground truth's use of `plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.8)` with a color bar is generally preferred for scatter plots with continuous or categorical colors as it's more direct and scalable than looping and plotting separate subsets, especially for visually representing labels effectively.
2.  The title for the first plot in the generated code (""PCA with K-Means Clustering"") is slightly less descriptive than the ground truth's (""PCA of Iris Dataset: Points Colored by K-Means Cluster Labels"") but is acceptable.
3.  The legends in the generated code for both plots are good, showing ""Cluster X"" and ""Species X"" which is helpful, but can sometimes clutter the plot if there are many categories. The ground truth's use of `colorbar` is a common and effective alternative for representing categorical labels on a scatter plot.
4.  The generated code creates a `pca_df` DataFrame, which isn't strictly necessary but doesn't harm the solution. It makes the plotting logic slightly more verbose by accessing DataFrame columns.

Overall, it's a very good solution that fully solves the problem.",gemini-2.0-flash,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# Load the dataset
df = pd.read_csv('iris.csv')

# Feature Engineering
df['PetalArea'] = df['PetalLengthCm'] * df['PetalWidthCm']
df['SepalArea'] = df['SepalLengthCm'] * df['SepalWidthCm']

# Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'PetalArea', 'SepalAr...",9,"The generated code correctly implements all specified requirements. It loads the dataset, performs the necessary feature engineering by creating `PetalArea` and `SepalArea`, defines `X` with the correct features (original four plus the two new ones), encodes the `Species` target, initializes and trains the `GradientBoostingClassifier` with the specified parameters, performs 5-fold cross-validation, and reports the mean accuracy. The imports are correct and the code structure is logical.

The only minor point for improvement, which keeps it from a perfect 10, is that the output message for the mean accuracy `print(f""Mean cross-validated accuracy: {cv_scores.mean()}"")` is a bit less verbose than the ground truth (which also prints individual fold scores). However, this is a very minor aesthetic difference and does not impact the core functionality or correctness. The `scoring='accuracy'` parameter in `cross_val_score` is implicitly handled by default for classification tasks when `scoring` is not specified, so its omission is not an error, but explicitly stating it can sometimes improve readability or prevent ambiguity depending on the context. Overall, it's a very strong and correct solution.",gemini-2.0-flash,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load the dataset
df = pd.read_csv(""heart.csv"")

# Define features (X) and target (y)
X = df.drop('HeartDisease', axis=1)
y = df['HeartDisease']

# Identi...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
*   **Correct Data Loading and Splitting:** Loads data correctly and splits training/testing sets with proper `random_state` and `test_size`.
*   **Correct Feature/Target Separation:** Correctly identifies `HeartDisease` as the target and the remaining as features (though the prompt explicitly listed features, `df.drop` achieves the same in this case as the omitted feature `HeartDisease` is the target).
*   **Correct Preprocessing:** Accurately defines and applies `OneHotEncoder` to categorical features and `StandardScaler` to numerical features using `ColumnTransformer`. `handle_unknown='ignore'` is correctly used for `OneHotEncoder`.
*   **Correct Model Training:** `RandomForestClassifier` is initialized with the specified `n_estimators` and `random_state`.
*   **Correct Evaluation Metrics:** Calculates accuracy, macro-averaged precision, recall, F1-score, and ROC-AUC score as required.
*   **Good Code Structure:** The code is well-structured and easy to follow.

**Areas for Minor Improvement:**
*   **Explicit Feature Definition:** While `X = df.drop('HeartDisease', axis=1)` works here because 'HeartDisease' is the only non-feature column, the prompt explicitly listed the features to be used (`Age`, `Sex`, etc.). The ground truth code explicitly defined these features for `X`, which is slightly more robust if the dataset had other columns not intended for features. This is a minor deviation but worth noting for strict adherence to requirements.
*   **Missing Pipeline:** The ground truth code uses a `Pipeline` which is generally considered a best practice for combining preprocessing and modeling steps, preventing data leakage and simplifying the workflow. The generated code manually performs `fit_transform` and `transform`, which is functional but less elegant and robust than a pipeline, especially in more complex scenarios. This is a minor point from a correctness perspective for this specific task but a definite improvement for code quality/best practices.
*   **Error Handling for File:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.

Overall, the generated code is highly functional and correct for the given task. The points for improvement are mostly about best practices and minor deviations from the implicit preferred solution style rather than outright errors.",gemini-2.0-flash,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

# 1. Load the dataset
df = pd.read_csv(""heart.csv"")

# 2. Select numerical features
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']
X = df[features]

# 3. Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. Dete...",9,"The generated code is very good and correctly addresses almost all requirements of the task.

**Strong points:**
*   **Correctness:** All steps are implemented correctly: data loading, feature selection, standardization, K-Means application, PCA reduction, and visualization.
*   **Feature Selection & Standardization:** Correctly selects the specified numerical features and applies `StandardScaler`.
*   **K-Means Parameters:** Uses `n_init='auto'` and `random_state=42` as required.
*   **K-Selection Methods:** Correctly calculates and plots WCSS (Elbow Method) and Silhouette Scores for the specified K range (2 to 5).
*   **PCA:** Correctly performs PCA to 2 components.
*   **Visualization:** Generates a scatter plot of the two principal components, colored by cluster labels, and adds a title.
*   **Imports:** All necessary imports are present.

**Areas for minor improvement:**
*   **K-Selection Justification (Implicit vs. Explicit):** The prompt asked to ""justify your choice of K."" While the code plots the elbow and silhouette methods, the choice `K=3` is hardcoded *after* the plots, with a comment ""Based on the elbow method plot and silhouette score, choose K=3"". The ground truth code provided a more explicit `print` statement stating the choice was based on visual inspection. This is a very minor point, but it slightly misses the explicit ""justification"" aspect on its own.
*   **Error Handling for CSV:** The ground truth includes a `try-except` block for `FileNotFoundError` for the CSV, which is good practice. The generated code assumes the file exists. This wasn't explicitly asked for as a hard requirement, but it improves robustness.
*   **Plot Legend:** The PCA plot in the generated code uses `colorbar` which is fine, but the prompt's implied visualization (and what the ground truth does) might lean towards a legend for individual cluster labels, which can sometimes be clearer than a continuous colorbar for discrete clusters, especially if the clusters are few. However, `colorbar` is valid.

Overall, it's a highly functional and well-structured solution. The issues are minor points of polish or robustness rather than core functional errors.",gemini-2.0-flash,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Create binary target variable
df['good'] = np.where(df['quality'] >= 7, 1, 0)

# Separate features and t...",9,"The generated code is very good and correctly addresses almost all requirements of the task.

Strong points:
- **Correct Binary Target:** The code correctly creates the binary target variable based on the quality threshold.
- **Preprocessing:** Handles missing values (though `X.mean()` might be less robust than median for skewed data, it's a valid attempt) and scales features appropriately.
- **Train/Test Split:** Performs an 80/20 train/test split.
- **Model Training:** Successfully trains a Logistic Regression model.
- **Evaluation:** Correctly calculates and prints accuracy, F1-score, and classification report.
- **Feature Importance:** Correctly identifies and reports feature importance based on model coefficients, including absolute coefficients for ranking.
- **Imports:** Includes all necessary imports.

Areas for minor improvement:
- **Missing Value Imputation:** While `X.fillna(X.mean())` works, `X.median()` is often a more robust choice for numerical data, especially if distributions are skewed, as it's less sensitive to outliers. The ground truth used `median`. For this dataset specifically, there are no missing values, so this part of the code primarily serves as good practice.
- **`stratify=y` in `train_test_split`:** The ground truth used `stratify=y` to ensure that the proportions of the target classes are maintained in both the training and testing sets. This is particularly important for imbalanced datasets, which this wine quality dataset can be for the binary classification (fewer 'good' wines). The generated code omits `stratify=y`, which is a minor oversight but could lead to less representative splits for the minority class.
- **Print Statements:** The ground truth included more informative print statements throughout the process, which is good practice for user understanding and debugging. The generated code's print statements are concise (only for final metrics and feature importance).
- **`matplotlib.pyplot`, `seaborn` imports:** These were imported but not used. While not a functional error, it's good practice to only import what's needed.",gemini-2.0-flash,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

# Load the dataset
df = pd.read_csv(""heart.csv"")

# Box plot of Oldpeak vs HeartDisease
plt.figure(figsize=(8, 6))
sns.boxplot(x='HeartDisease', y='Oldpeak', data=df)
plt.title('Oldpeak vs HeartDisease')
plt.show()

# Logistic regression
X = df[['Oldpeak']]
y = df['HeartDisease']
model = LogisticRegression()
model.fit(X, y)

# Report coeffici...",8,"The generated code correctly addresses all parts of the task.

**Strong points:**
*   Correctly loads the data.
*   Successfully generates the box plot.
*   Performs logistic regression, extracts the coefficient, and calculates the odds ratio.
*   Provides a basic interpretation of the odds ratio.
*   Accurately discretizes `Oldpeak` using `pd.qcut`.
*   Correctly calculates the proportion of `HeartDisease=1` within each bin.

**Areas for improvement:**
*   **Error Handling for Data Loading:** The ground truth includes `try-except` for `FileNotFoundError` for `heart.csv`, which is good practice. The generated code assumes the file exists.
*   **Box Plot Labels and Title:** While the box plot is generated, the ground truth provides more descriptive labels for the x-axis (`xticks` labels) and a more informative title. This improves readability significantly.
*   **Interpretation of Odds Ratio:** The interpretation provided is a good start, but the ground truth offers a more nuanced explanation, including how to phrase it when the odds *decrease* (though in this specific case, `Oldpeak` typically increases odds) and the phrasing ""holding other factors constant."" It also calculates the percentage increase/decrease, which is a common way to interpret odds ratios.
*   **`qcut` `duplicates='drop'`:** The ground truth includes `duplicates='drop'` in `pd.qcut` and a `try-except` block to handle potential `ValueError` if `qcut` cannot create unique bins (e.g., due to many identical values at quantile boundaries). While not strictly necessary for all datasets, it makes the code more robust.
*   **Output Clarity:** The ground truth includes more descriptive print statements and clear section headers (e.g., ""--- Box plot displayed ---"") which improves the readability of the overall script's output in a terminal.
*   **Visualization of Binned Proportions:** The ground truth includes an optional bar plot for the proportion of HeartDisease=1 per bin, which is a good visual summary of that step. While not explicitly required for a ""display"" prompt, it enhances the analysis.",gemini-2.0-flash,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# 1. Load the data
df = pd.read_csv(""spotify_2023.csv"")

# 2. Convert 'streams' to numeric and handle missing values
df['streams'] = pd.to_nume...",9,"The generated code correctly addresses almost all requirements of the task. It loads the data, correctly handles the `streams` column conversion and NaN values, creates the `is_popular` target variable using the 75th percentile, selects the specified features, performs one-hot encoding for `key` and `mode`, splits the data, trains a Logistic Regression model, and evaluates it using F1-score (macro), ROC AUC score, and displays the confusion matrix.

The use of `ColumnTransformer` and `Pipeline` is a strong point, representing a more robust and organized way to handle preprocessing and modeling in `sklearn`. This is good practice.

However, there is one minor issue: the `train_test_split` is not stratified. The ground truth code correctly uses `stratify=y` to ensure that the class distribution of the target variable (`is_popular`) is maintained in both the training and testing sets. While not explicitly asked for in the prompt, it is a recommended practice for imbalanced classification tasks (which predicting popularity often is) and ensures a more reliable evaluation. Since this was included in the ground truth and is a best practice, its omission is a minor deduction.

Additionally, the output format is slightly less detailed than the ground truth in terms of printing information about the data loading, shape, and specific steps, but it does print the required evaluation metrics. The prompt only asked for the *metrics* to be displayed, not the intermediate steps, so this is not a deduction in functionality, but could be clearer in a real-world scenario.",gemini-2.0-flash,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. Load data
df = pd.read_csv('spotify_2023.csv')

# 2. Select audio features and drop NaNs
features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%',
            'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm']
df_subset = df[features].copy()
df_subset = df_subset.dropna()

# 3. Standar...",8,"The generated code successfully completes the task and addresses all specified requirements.

**Strong Points:**
- **Correctness:** All steps are correctly implemented: data loading, feature selection, NaN handling, standardization, K-Means clustering (with correct parameters), PCA (with correct parameters), scatter plot generation, and calculation/display of cluster means.
- **Completeness:** All 7 requirements outlined in the task are met.
- **Code Quality:** The code is well-structured, readable, and uses appropriate libraries. Imports are correct.

**Areas for Improvement (to achieve a higher score):**
- **Robustness in Data Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust. The generated code assumes the file will always be present. While not strictly required by the prompt, it's good practice for production-ready code.
- **Clarity of `df_subset` usage:** The `df_subset` is modified in-place by `dropna()`, and then `scaled_features` is derived from it. Later, `df_subset` is used again to attach `clusters` and group by. This is technically correct, but if the original `df_subset` (before dropping NaNs) was needed for some reason, it wouldn't be available. The ground truth creates `df_original_for_analysis` explicitly for this purpose, which is slightly clearer for maintaining the unstandardized version for analysis after dropping NaNs. However, the current approach is functional and doesn't lose data pertinent to *this* task.
- **Print Statements:** The ground truth includes informative print statements about the number of rows before and after NaN dropping, which is helpful for debugging and understanding data preprocessing. This is a minor point but adds to user experience.
- **Plot Customization:** The plot is functional, but the ground truth adds a color bar with specific ticks, which enhances readability by clearly mapping cluster numbers to the colors. It also uses `s=50` and `alpha=0.7`, which are small aesthetic improvements.",gemini-2.0-flash,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. Load data
df = pd.read_csv('spotify_2023.csv', encoding='latin-1')

# 2. Convert 'streams' to numeric, coerce errors to NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# 3. Filter data
df_filtered = df[
    (df['streams'] > 10000000) & (df['released_year'].between(2010, 2023))
].copy()
df_filtered.dropna(subset=['streams'], inplace=True)


# 4. Group by 'released_year'
grouped_data = df_filtered.groupby('relea...",9,"The generated code successfully addresses almost all requirements of the task.

**Strong Points:**
- **Correctness:** The code correctly loads data, converts `streams` to numeric with error coercion, filters data based on both `streams` and `released_year` criteria, groups by year, calculates the average audio features, and generates the required line plots.
- **Completeness:** All explicit steps mentioned in the prompt (load data, convert streams, filter, group, calculate, plot) are covered.
- **Code Quality:** Proper imports are used. The use of `df.copy()` after filtering is good practice to prevent `SettingWithCopyWarning`. The plotting with `subplot` is well-implemented. The `xticks` rotation is also a good touch for readability.

**Areas for Improvement:**
- **Order of Filtering and `dropna`:** The generated code applies the `dropna(subset=['streams'])` *after* the initial filtering of `streams > 10000000`. This means that if a row had `streams` as NaN *before* the `pd.to_numeric` conversion, it would have already been excluded by `streams > 10000000` (as NaN > number is False). However, if there were valid numbers that became NaN *after* `pd.to_numeric` (e.g., 'abc' becomes NaN), those rows would correctly be excluded by the subsequent `dropna`. The ground truth's approach of `dropna` immediately after `pd.to_numeric` ensures that any `NaN` values *resulting from the conversion* are removed *before* any numerical comparisons, which is a slightly cleaner and more robust workflow when mixed or non-numeric types are expected in the 'streams' column. While the generated code's order might work for typical data, the ground truth's approach is more explicit about handling NaNs produced by `to_numeric` first. This is a minor point, as the outcome is largely the same for this specific filtering, but it's a subtlety in data cleaning workflows.
- **Error Handling for File Not Found:** The generated code lacks a `try-except` block for `FileNotFoundError` when loading the CSV, which is present in the ground truth and makes the code more robust.
- **Redundant `plt.tight_layout()`:** `plt.tight_layout()` is called three times, once for each subplot. It only needs to be called once at the end before `plt.show()` to adjust the entire figure's layout. Calling it multiple times on subplots does not achieve different results and just adds redundancy. The ground truth correctly calls it once.
- **Clarity of `grouped_data` usage:** While functionally correct, extracting `avg_danceability`, `avg_energy`, `avg_valence` into separate Series from `grouped_data` is a bit more verbose than directly using the `groupby` object for multiple columns, as shown in the ground truth `yearly_avg_features = df_filtered.groupby('released_year')[audio_features].mean()`. This is mainly a style/conciseness point.

Overall, the code is very good and effectively solves the problem. The suggested improvements are minor refinements rather than critical flaws.",gemini-2.0-flash,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder

# 1. Load the dataset
df = pd.read_csv(""youtube_trendings.csv"")

# 2. Clean 'Likes', 'Dislikes', and 'Video views'
for col in ['Likes', 'Dislikes', 'Video views']:
    df[col] = df[col].astype(str).str.replace(',', '', regex=False).astype(float)
    df[col] = ...",9,"The generated code is largely correct and addresses almost all requirements.

**Strengths:**
- Correctly loads the dataset.
- Successfully cleans `Likes`, `Dislikes`, and `Video views` by removing commas and converting to numeric, though the ground truth handles `NaN` values more robustly by filling with 0 *after* conversion.
- Treats the `published` year as a numeric feature correctly.
- Handles missing `Category` values and applies one-hot encoding using `OneHotEncoder` from `sklearn.preprocessing`, which is good practice. The use of `handle_unknown='ignore'` is a good touch.
- Correctly defines features `X` and target `y`.
- Splits the data into training and testing sets with the specified `test_size` and `random_state`.
- Trains a Linear Regression model and evaluates it using R-squared and MAE on the test set.
- Includes all necessary imports.

**Areas for Improvement (minor points):**
- **Missing `NaN` handling for `published` year:** While `pd.to_datetime` handles unparseable dates by producing `NaT`, `dt.year` on `NaT` will result in `NaN`. The code does not explicitly `fillna(0)` for `published` year, which might lead to issues if there are unparseable dates in the `published` column. The ground truth handles this more explicitly.
- **Robustness in cleaning `Likes`, `Dislikes`, `Video views`:** The generated code applies `astype(float)`
  `...astype(str).str.replace(',', '', regex=False).astype(float)`
  and then `fillna(0)`. While this works, explicitly using `pd.to_numeric(errors='coerce')` and *then* `fillna(0)` is generally more robust for handling non-numeric strings that are not just commas (as shown in the ground truth). The current approach would fail to convert ""abc"" to a number and then to 0.
- **Printout formatting:** The request asked for specific formatting for the printout (`R-squared (R2) Score: {r2:.4f}`, `Mean Absolute Error (MAE): {mae:.2f}`). The generated code uses slightly different formatting (`R-squared (R2): {r2}`, `Mean Absolute Error (MAE): {mae}`). This is a minor aesthetic point.
- **Handling of `category_df.columns` in `X` definition**: Using `list(category_df.columns)` is fine, but the ground truth's approach of concatenating dataframes (or letting `pd.get_dummies` append directly) can sometimes be cleaner depending on context. However, for the given task, this is acceptable.

Overall, the code is well-structured and performs the required tasks effectively, demonstrating a strong understanding of the problem and the libraries. The points for improvement are minor details that enhance robustness rather than correct critical errors.",gemini-2.0-flash,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv(""youtube_trendings.csv"")

# Clean 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '')
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce').fillna(0)

# Extract year from 'published' and ensure it's numeric
df['published'] = pd.to_datetime(df['published']).dt.year
df['published'] = pd.to_numeric(df['published'], errors='coerce').fillna(0).astype(int)


# ...",9,"The generated code successfully completes the task by loading the dataset, cleaning the 'Video views' column, processing the 'published' year, calculating the required metrics per year, and generating the two specified plots with appropriate titles and labels.

**Strong Points:**
- **Correct Data Loading:** Successfully loads the CSV.
- **Correct 'Video views' Cleaning:** Handles commas, converts to numeric, and fills NaNs with 0 correctly.
- **Correct 'published' Year Handling:** Correctly extracts the year, converts it to numeric, and fills NaNs.
- **Correct Grouping and Calculation:** The `groupby` and `agg` operations are spot on for calculating both average views and unique videos.
- **Correct Plotting:** Creates two subplots and plots the correct data on each, with relevant titles and axis labels. `plt.tight_layout()` is used for good presentation.
- **`ticklabel_format(style='plain', axis='y')`:** This is a good inclusion to prevent scientific notation on the y-axis for average views, which improves readability.

**Minor Areas for Improvement (preventing a perfect 10):**
- **Column Name Consistency for 'published':** The task description implies a `published` column, but typical YouTube datasets use `publishedAt`. The ground truth code correctly infers and handles `publishedAt` then creates a `published_year` column. The generated code assumes a column named `published`, but then reassigns the `published` column to the extracted year. While functional, it's slightly less robust or explicit than creating a new `published_year` column. However, given the prompt only mentioned `published` and not `publishedAt`, it's an understandable choice.
- **Error Handling:** The ground truth includes basic file loading error handling (e.g., `FileNotFoundError`), which is good practice for production-ready code. The generated code lacks this, which is common in benchmark responses but worth noting.
- **Clarity on `published` column:** The prompt stated ""Ensure the `published` column (year) is treated as a numeric or integer type."". The CSV file typically has a `publishedAt` column (full timestamp), and the task *implies* that the year needs to be extracted from it. The ground truth correctly assumes `publishedAt` and extracts the year into a new `published_year` column. The generated code directly renames the `published` column, which works if `published` refers to the datetime string column, but it's less explicit than the ground truth's handling. If the CSV literally had a `published` column that was already just the year, then the `pd.to_datetime` part would be misplaced. However, assuming the usual YouTube dataset structure, it successfully extracts the year.

Overall, the code is highly effective and directly addresses all core requirements of the task. The minor points are mostly about robustness and slightly better column naming conventions, rather than functional errors.",gemini-2.0-flash,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
from nltk.corpus import stopwords
from scipy.stats import pearsonr

# Load the dataset
try:
    df = pd.read_csv(""youtube_trendings.csv"", encoding='utf-8')
except FileNotFoundError:
    print(""Error: youtube_trendings.csv not found. Place the file in the current directory."")
    exit()
except UnicodeDecodeError:
    df = pd.read_csv(""youtube_trendings.csv"", encoding='latin1')


# Feature: title_word_count
df['title_word_coun...",9,"The generated code is highly correct and addresses almost all requirements of the task.

**Strong points:**
- **Correct Data Loading and Error Handling:** Handles `FileNotFoundError` and `UnicodeDecodeError` appropriately.
- **Accurate `title_word_count`:** Correctly calculates the word count for video titles.
- **Robust 'Video views' Cleaning:** Effectively removes commas, converts to numeric, and fills NaNs with 0.
- **Correct Correlation Calculation:** Uses `scipy.stats.pearsonr` which is a valid and robust way to calculate Pearson correlation.
- **Precise Date Filtering:** Correctly converts 'published' to datetime and filters for years 2023 and 2024.
- **Effective Text Processing:** Handles lowercasing, punctuation removal, and stop word removal correctly.
- **Correct Most Common Words:** Uses `collections.Counter` to find the 5 most common words as requested.
- **Proper Imports:** All necessary libraries are imported.

**Areas for Minor Improvement:**
- **NLTK Download:** The ground truth includes a `try-except` block to ensure NLTK stopwords are downloaded if not already present. While the prompt assumes the user has NLTK set up, for a robust solution, including `'nltk.download('stopwords')'` or a similar check would be beneficial to prevent `ResourceCorpusReader` errors if the stopwords haven't been downloaded. The current code assumes they are already downloaded.
- **Print Formatting for Correlation:** The ground truth prints the correlation formatted to `4` decimal places, while the generated code uses `2`. The prompt specifies to ""print the Pearson correlation coefficient,"" which implies a reasonable precision, but doesn't explicitly state 4 decimal places. This is a minor stylistic difference.
- **Handling of Non-String `Video` Titles:** The ground truth explicitly converts `df['Video']` to `str` before calculating `title_word_count`, which is a good practice to prevent errors if some entries in the 'Video' column are not strings. The generated code assumes 'Video' column entries are always strings. This could lead to a `TypeError` if there are numeric or other non-string types in that column.
- **Clarity of `Video views` cleaning:** While `df['Video views'] = df['Video views'].str.replace(',', '').astype(float).fillna(0)` is concise, breaking it into two or three lines as in the ground truth (`.astype(str).str.replace(...)`, then `pd.to_numeric(...)`, then `.fillna(0)`) would be slightly more robust against non-string types and potentially clearer for large data sets, although the current approach is perfectly functional for typical input.",gemini-2.0-flash,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# 1. Load data
df = pd.read_csv('mobiles_2025.csv')

# 2. Clean 'Launched Price (USA)' and create 'is_expensive_usa'
def clean_price(price_str):
    try:
       ...",6,"The generated code attempts to solve all parts of the task but has several inaccuracies and issues that prevent it from being fully correct and robust.

**Correctness Issues:**
1.  **`is_expensive_usa` Target Creation:**
    *   The ground truth explicitly states: ""1 if cleaned `Launched Price (USA)` > 1000, else 0. Drop rows where `is_expensive_usa` is NaN."" This implies that `is_expensive_usa` should be 0 if the price is <= 1000 and the price is valid.
    *   The generated code uses `(df['Launched Price (USA)'] > 1000).astype(int)`. This converts `True` to 1 and `False` to 0. However, it implicitly handles NaNs from `clean_price` incorrectly. If `clean_price` returns NaN, then `NaN > 1000` evaluates to `False`, and `.astype(int)` converts `False` to 0. This means rows with unparseable prices are assigned `is_expensive_usa = 0` instead of being dropped or flagged as NaN, which contradicts the prompt's requirement to ""Drop rows where `is_expensive_usa` is NaN"" (which should apply to cases where the original price couldn't be parsed). The ground truth correctly assigns `np.nan` if the price is unparseable and then drops those rows.
    *   The prompt also said ""errors to NaN"" for price cleaning, and then ""Drop rows where `is_expensive_usa` is NaN."" This is correctly implemented in GT by assigning `np.nan` when `pd.to_numeric` fails, and then dropping these rows. Generated code misinterprets this.

2.  **Dropping NaNs for Features and Target:**
    *   The prompt specifies: ""Drop rows with any NaNs in features or target."" The generated code drops NaNs for `is_expensive_usa` first but then uses `X['is_expensive_usa'] = y; X = X.dropna(); y = X['is_expensive_usa']; X = X.drop(columns=['is_expensive_usa'])`. This is an extremely convoluted and unusual way to handle `dropna` when `X` and `y` are already defined and intended to be kept in sync. It also only drops NaNs from the *initial* feature set *before* one-hot encoding. NaNs introduced by the cleaning of specific columns (e.g., `Mobile Weight` cleaning resulting in NaNs) are handled here, but the categorical columns are still objects before one-hot encoding so `dropna` on `X` at this point cannot catch NaNs in categorical columns if they exist. The ground truth correctly sets up `X` and `y` from newly created cleaned columns and then drops NaNs from `X` and `y` combined at the end.

3.  **Feature Cleaning (`Front Camera`, `Back Camera`):**
    *   The prompt specifies: ""For `Front Camera` and `Back Camera`, extract the first numerical MP value (default 0 if none)."" The generated code's `extract_megapixel` function `return float(camera_str.split(' ')[0])` is too simplistic. It assumes the number is always the first part of the string followed by a space. It will fail for more complex strings like ""20 MP"" (returns 20.0, which is correct) but also for ""Dual 12 MP"" (returns ""Dual"" which then errors out) or ""5.0MP"" (fails due to no space split). The ground truth uses `re.search(r'(\d+(\.\d+)?)\s*MP', ...)` which is much more robust for extracting a numerical MP value.

4.  **`astype(float)` for Cleaning:**
    *   The generated code uses `astype(float)` directly after `str.replace` for `Mobile Weight`, `RAM`, `Battery Capacity`, `Screen Size`. If the string after `replace` is empty or invalid (e.g., if the original value was `NaN` or unparseable text), this will raise an error or convert to NaN inappropriately. The `pd.to_numeric(errors='coerce')` in the ground truth is the standard and more robust way to handle this, converting unparseable values to `NaN`.

5.  **Pipeline with ColumnTransformer:**
    *   While using `Pipeline` and `ColumnTransformer` is generally good practice, the way it's used here is suboptimal for this specific task.
    *   `numerical_transformer = Pipeline(steps=[])` and `('num', numerical_transformer, numerical_features)` means numerical features are passed through without any imputation/transformation. This is fine if one expects no NaNs *after* data loading, but pre-cleaning before defining `X` and `y` and then dropping NaNs (as per ground truth) is a clearer approach, allowing `dropna` to work on the actual cleaned values. The way the generated code drops NaNs (`X = X.dropna()`) happens *before* the `ColumnTransformer`, so if any NaNs were left in `numerical_features` or `categorical_features` after the initial direct `astype(float)` conversions or `replace()` operations for features, those rows would be dropped *before* the Pipeline ever runs, but a more common approach for ColumnTransformer would be to include imputation steps within the numerical/categorical pipelines if one aims to retain rows with missing values. The current setup makes the `preprocessor` less useful for imputation, relying solely on prior `dropna`.
    *   The `OneHotEncoder` is usually applied to the raw categorical columns, which is what the `ColumnTransformer` does. However, because NaNs were theoretically handled *before* the split, the `handle_unknown='ignore'` is appropriate.

**Code Quality and Completeness:**
*   **Imports:** Proper imports are included.
*   **Structure:** The code largely follows the requested steps, though the implementation details are flawed.
*   **Verbosity:** The code lacks informative print statements about what's happening at each step (e.g., shape changes, number of rows dropped), making it harder to debug or understand its flow compared to the ground truth.
*   **Robustness:** The cleaning functions and `astype(float)` usage are not as robust as `pd.to_numeric(errors='coerce')` or regex for extracting MP values.
*   **`drop(columns=['Launched Price (USA)'])`:** This drops the original column after creating the target, which is fine, but the target creation logic was flawed.
*   **`Mobile Weight`, `RAM`, `Battery Capacity`, `Screen Size`:** These are cleaned by directly using `str.replace` and `astype(float)`. This is less robust than `pd.to_numeric(errors='coerce')` in the ground truth, which will correctly handle cases where the string might not be a valid number after replacement (e.g., 'Not Available').

Overall, the generated code understands the requirements but implements several critical cleaning and data preparation steps incorrectly or suboptimally, leading to potential data loss or incorrect data transformations. The `is_expensive_usa` creation and NaN handling are the biggest misses against the prompt's explicit instructions.",gemini-2.0-flash,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

def clean_price(price_str):
    """"""
    Removes currency symbols and commas from a price string, then converts it to numeric.
    Handles errors by returning NaN.
    """"""
    if isinstance(price_str, str):
        price_str = price_str.replace('PKR', '').replace('INR', '').replace('CNY', '').replace('USD', '').replace('AED', '').replace(',', '').strip()
        try:
            return pd.to_numeric(price_str)
        except ValueError:
          ...",9,"The generated code largely fulfills all requirements of the task with good correctness and structure.

**Strong points:**
*   **Correctness:** The code correctly loads the dataset, cleans all specified price columns, calculates the average global price for each model considering only available data, and identifies the top 5 models.
*   **Cleaning Function (`clean_price`):** The `clean_price` function correctly handles removing currency symbols and commas, and converts to numeric, returning `NaN` on errors. Its use of `pd.to_numeric` is a valid approach. It also correctly handles non-string inputs by returning `np.nan`.
*   **Average Global Price Calculation:** The `calculate_average_global_price` function correctly iterates through the specified price columns for each row, filters out `NaN` values, and calculates the mean of available prices. Applying this with `df.apply(..., axis=1)` is a correct method.
*   **Top 5 Identification:** Using `df.nlargest(5, 'Average Global Price')` is an efficient and clean way to get the top 5 models.
*   **Display:** The output format clearly shows the model name and average price.
*   **Error Handling (File Not Found):** Includes a `try-except` block for `FileNotFoundError`.

**Minor areas for improvement (preventing a perfect 10):**
*   **`clean_price` efficiency:** For simple string replacements, chaining `.replace()` is generally fine, but for a potentially larger set of symbols or more complex patterns, using `re.sub()` and iterating over a list of currency symbols (as in the ground truth) could be more robust and scalable if symbols were to change frequently or become more complex.
*   **`calculate_average_global_price` vs. `df.mean(axis=1)`:** While the user-defined `calculate_average_global_price` function works perfectly, `df[price_columns].mean(axis=1, skipna=True)` is generally more performant and idiomatic for DataFrame operations in pandas, especially on large datasets. The ground truth demonstrates this more efficient approach.
*   **Redundancy in `calculate_average_global_price`:** The explicit list creation and iteration for `valid_prices` is effectively what `df[price_columns].mean(axis=1, skipna=True)` does internally in a much more optimized C/Cython faster implementation.

These are minor points related to optimization and pandas idiomatic expressions rather than correctness or completeness, hence the high score.",gemini-2.0-flash,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# 1. Load the dataset
df = pd.read_csv(""mobiles_2025.csv"")

# 2. Clean the data
def clean_ram(ram_str):
    try:
        return float(ram_str.replace('GB', ''))
    except:
        return np.nan

def clean_battery(battery_str):
    try:
        return float(battery_str.replace('mAh', ''))
    except:
        return np.nan

def clean_price(price_str):
    try:
        return float(price_str.replace('USD ', '').repla...",8,"The generated code successfully completes most of the task requirements.

**Strong points:**
*   **Correctness:** Loads the data, cleans the specified columns (`RAM`, `Battery Capacity`, `Launched Price (USA)`) using custom functions that handle errors by returning `NaN`, and correctly drops rows with `NaN` in these cleaned columns.
*   **Filtering and Grouping:** Correctly filters for 'Apple' products and groups the data by 'Launched Year'.
*   **Plotting:** Generates box plots for each year, displaying RAM, Battery Capacity, and Launched Price as subplots. Titles and y-labels are generally appropriate. `plt.suptitle` and `plt.tight_layout` are used well.

**Areas for improvement:**
*   **Cleaning robustness:** While the custom cleaning functions handle errors, they might be less efficient or robust than using `pd.to_numeric` with `errors='coerce'` directly, especially for handling non-string inputs or more complex cleaning scenarios. For example, `df['RAM'].astype(str).str.replace('GB', '', regex=False).str.strip()` followed by `pd.to_numeric(...)` is often more robust than `apply` with a custom function.
*   **Handling `Launched Year` NaNs:** The task implicitly requires `Launched Year` to be numeric for grouping, but the cleaning steps don't explicitly handle potential NaNs or non-numeric types in `Launched Year` itself before grouping, which could cause issues if the column isn't clean. The ground truth code handles this.
*   **`Battery Capacity` renaming:** Renaming `Battery Capacity` to `Battery_Capacity` was unnecessary and slightly deviates from the original column name, although it doesn't break functionality. It introduces a minor inconsistency.
*   **Imports:** `numpy` is imported but not explicitly used in the core logic (`np.nan` is used, but could be `float('nan')` or `pd.NA` for better Pandas integration). `seaborn` was not imported, which was optional but could have been used for potentially more aesthetically pleasing plots with less code.
*   **Plot styling:** The plots are functional but lack specific styling (like a dark grid or specific colors) that could enhance readability, as seen in the ground truth using `plt.style.use`.",gemini-2.0-flash,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV, learning_curve, validation_curve
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neural_network i...",6,"The generated code attempts to address most of the requirements but falls short in several aspects, particularly regarding advanced preprocessing, proper pipeline integration, and comprehensive evaluation.

**Strengths:**
*   **Attempts at Advanced Preprocessing:** Includes some feature engineering (FamilySize, IsAlone, Title extraction) and attempts text mining from names using TF-IDF.
*   **Multiple Base Models and Tuning:** Uses Random Forest, Gradient Boosting, SVM, Neural Network, and HistGradientBoostingClassifier, with `GridSearchCV` for hyperparameter tuning.
*   **Multiple Imputation Strategies:** Attempts to use different imputers (mean, median, KNN) and compares models across these strategies.
*   **Ensemble Methods:** Implements a VotingClassifier.
*   **Feature Selection:** Includes `SelectFromModel` and `RFE`.
*   **Model Evaluation:** Uses stratified k-fold cross-validation, learning curves, and calculates various metrics.
*   **Interpretability:** Attempts to use SHAP and LIME.
*   **Statistical Significance:** Includes a function for `ttest_rel`.

**Weaknesses and Areas for Improvement:**

1.  **Data Preprocessing (Correctness & Completeness):**
    *   **Multiple Imputation:** The prompt explicitly asks for ""multiple imputation strategies,"" implying techniques like MICE (`IterativeImputer`), not just different simple imputers. The ground truth uses `IterativeImputer`. The generated code uses `SimpleImputer` (mean/median) and `KNNImputer`, which are single imputation methods, not ""multiple imputation"" in the statistical sense (multiple imputed datasets are created and analyzed).
    *   **Advanced Feature Engineering:**
        *   The handling of `Age_Band` and `Age_Filled` is confusing and seems to try to impute `Age` based on `Survived` means, which is data leakage if done before splitting or improperly. The ground truth's custom transformer is more robust.
        *   `Title` extraction is simplified; the ground truth has more comprehensive grouping.
        *   `Cabin` handling is completely missing, which is a key advanced feature engineering task for Titanic.
        *   `Fare_Per_Person` is missing.
        *   Polynomial features are mentioned in the prompt but not included in any preprocessing pipeline. The ground truth has this.
        *   Interaction terms are mentioned but not explicitly created beyond `FamilySize`.
    *   **Text Mining:** `ProcessedName` is created, but the TF-IDF vectorizer only processes the `ProcessedName` column. After `ColumnTransformer` is fitted, the vectorizer might output a sparse matrix. The subsequent `get_feature_names_out()` might not correctly capture these feature names, affecting feature selection and interpretability later. Also, the `OneHotEncoder` for categorical features is replaced by `pd.get_dummies` which is problematic within a `Pipeline` as `get_dummies` is a function, not a transformer. This likely breaks the pipeline.
    *   **`ColumnTransformer` Usage:** The `categorical_transformer` uses `pd.get_dummies` directly, which is not a `TransformerMixin` and breaks the `Pipeline` intended by `ColumnTransformer`. It should be `OneHotEncoder`.
    *   **Data Leakage in Preprocessing:** The function `create_age_bins(df)` is called on the entire `df` before the train-test split, which is a major data leakage. Similarly, `X['Embarked'] = X['Embarked'].fillna(X['Embarked'].mode()[0])` and `X = pd.get_dummies(X, columns=['Embarked'], drop_first=True)` are done on the full `X` before splitting, again leading to leakage. Preprocessing steps (imputation, scaling, one-hot encoding etc.) must be fitted *only* on the training data and then applied to both training and test data or done within pipelines that are fit on the training data. The ground truth correctly separates `X_train`, `X_test` and fits pipelines only on `X_train`.

2.  **Pipeline Integration and Model Tuning (Correctness):**
    *   The `tune_model` function passes the raw `X` to the `grid_search.fit(X, y)`. Given that `preprocessor` is *inside* the pipeline, this is theoretically correct. However, issues in `preprocessor` (like `pd.get_dummies`) will cause problems.
    *   The model tuning part repeatedly defines the same `best_rf_mean`, `best_rf_median` etc. The objective for a robust solution would be to train *one* best of each model type using the *best* preprocessing strategy found, or to combine strategies. The code trains 15 separate models (5 models x 3 imputation strategies) but doesn't explicitly compare which imputation strategy is best for each model systematically for final selection.

3.  **Ensemble Methods (Completeness):**
    *   Only a `VotingClassifier` is implemented; `StackingClassifier` is a key requirement for ""advanced ensemble"" but is missing. The `VotingClassifier` also attempts to include all 15 trained base models, which might be overkill and not necessarily yield the best performance.

4.  **Feature Selection (Correctness & Completeness):**
    *   The feature selection is performed on `preprocessor_mean.fit_transform(X)`, which again has data leakage issues applying `fit_transform` to the entire dataset `X`.
    *   `X_selected_rf`, `X_selected_gb`, `X_selected_rfe` are then used for evaluation but the `evaluate_model` function expects a pipeline, and `X_selected_rf` is already transformed data, making subsequent evaluation difficult if `best_rf_mean` (which *has* a preprocessor) is passed to it. This leads to inconsistency in how `evaluate_model` handles data.
    *   The impact of feature selection should be compared clearly (e.g., train chosen models on original features vs. selected features and compare performance). The current evaluation is unclear.

5.  **Extensive Model Evaluation (Completeness & Correctness):**
    *   **Learning Curves:** While plots are generated, they use `preprocessor_mean.fit_transform(X)` directly as input, which is incorrect. The `learning_curve` function automatically handles the `cv` splits; the pipeline for each model should be passed, allowing `fit_transform` to occur within each fold, not on the entire dataset upfront. In the ground truth, the pipeline (e.g., `stacking_classifier`) is passed directly to `learning_curve`.
    *   **Validation Curves:** Mentions ""validation curves"" in the prompt but they are completely missing from the generated code.
    *   **Performance Metrics:** While metrics are calculated, a summary comparison table or plot of all models (base and ensemble) and feature selection results would be very beneficial, as requested by ""compare all models...with comprehensive performance metrics"".
    *   **Statistical Significance:** The `compare_models` function is good, but it re-fits the model on `X_train` within each fold of the `compare_models` function, which is redundant if the models passed are already the `best_xx_mean` pipelines that contain the preprocessor. It needs to be more careful with how `X` is used here; `X` should be `X_train` from the train-test split for true CV.

6.  **Model Interpretability (Correctness):**
    *   **SHAP:** For `shap.summary_plot`, the `features` argument should typically be the data for which SHAP values were calculated (`shap_values`), not the original preprocessed data. Also, `get_feature_names_out()` called directly on the preprocessor step of a trained pipeline (`best_rf_mean.named_steps['preprocessor'].get_feature_names_out()`) might not return correct names if the `ColumnTransformer` itself has a `passthrough` or if `pd.get_dummies` was the last categorical step (as it's not a proper transformer in this context). The ground truth provides a `get_feature_names_after_preprocessing` helper for this.
    *   **LIME:** The `lime_explain` function's `feature_names` argument is problematic. `preprocessor.get_feature_names_out()` might not work correctly if `pd.get_dummies` was attempted. The `X_train` argument to `LimeTabularExplainer` should also be the processed training data, `X_instance` should be the processed instance. The `predict_fn` in LIME should take preprocessed data and pass it to the model. The current `predict_fn=model.predict_proba` implies `model` can directly handle unprocessed data, which it cannot if it includes a preprocessor in its pipeline. The ground truth correctly defines a `predict_proba_for_lime` that routes raw data through the full pipeline. The `exp_lime.show_in_notebook` is called, but the output is not shown in a plain script, and it expects a notebook environment.

Overall, the generated code shows a good attempt to cover the requirements but struggles with critical details in implementation, leading to potential bugs, data leakage, or incorrect analysis. The use of `pipeline` is a good concept, but its specific implementation with `pd.get_dummies` breaks it, and the inconsistent handling of `X` vs. preprocessed `X` across different evaluation stages adds complexity and potential errors.",gemini-2.0-flash,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
df = pd.read_csv(""imdb_top_250.csv"")

# Define features and target
X = df[['Year', 'Runtime']]
y = df['Rating']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Linear Regression model
mod...",7,"The generated code correctly implements the core requirements of loading data, defining features and target, splitting data, training a Linear Regression model, and evaluating it using R-squared and MAE. All specified imports are present and correctly used.

However, it lacks robustness and completeness in handling real-world data issues. Specifically, it does not include any error handling for `FileNotFoundError` when loading the CSV, which is a common and important consideration. More critically, it does not perform any data cleaning or type conversion for the 'Runtime' column (which often comes as 'X min') or handle potential non-numeric values in 'Year' or 'Rating'. If the `Runtime` column is not purely numeric or if `Year` or `Rating` contain non-numeric values, the code will likely fail or produce incorrect results due to `TypeError` or `ValueError` at the feature/target definition step or during model training. The ground truth code includes robust preprocessing for these aspects, demonstrating a more complete and production-ready solution. Without this preprocessing, the code might not run successfully with the provided dataset if `Runtime` is not already numeric, or it might produce NaNs leading to errors.",gemini-2.0-flash,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 1. Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 2. Select features for clustering
features = ['Rating', 'Runtime']
X = df[features]

# 3. Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. Apply K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
df['Cluster'] = kmeans...",7,"The generated code correctly implements most of the requirements: loading data, selecting and standardizing features, applying K-Means (K=3, random_state=42), visualizing clusters with original features, and attempting to report cluster means.

However, there are two significant issues that prevent a higher score:
1.  **Missing Data Cleaning/Error Handling for 'Runtime':** The `imdb_top_250.csv` dataset typically has the 'Runtime' column formatted as strings (e.g., ""142 min""). The ground truth includes robust code to convert this to numeric format and handle potential errors or missing values. The generated code directly uses `df['Runtime']`, which will likely cause a `ValueError` or `TypeError` during the `pd.to_numeric` conversion (which is implicitly done by `StandardScaler` if not pre-processed) or `mean()` calculation if the column isn't numeric, or simply lead to incorrect scaling/clustering. This is a critical oversight given the nature of the data.
2.  **Lack of Handling for Missing/Invalid 'Rating' or 'Runtime' Data:** Even if 'Runtime' was numeric, both features could have `NaN` values. The ground truth explicitly handles this with `df.dropna(subset=['Rating', 'Runtime'], inplace=True)`, ensuring clean data for clustering. The generated code does not include any such safeguard, which could lead to errors or unexpected behavior if `NaN`s are present.

Minor Improvements:
*   The mean calculation in step 6 uses a basic loop, which is less efficient and Pythonic than the `groupby().mean()` method used in the ground truth. While functionally correct for this small dataset, `groupby()` is preferred for larger datasets and cleaner code.
*   The ground truth includes better error handling for `FileNotFoundError` and missing columns, which improves robustness.
*   The plot could benefit from a legend or color bar title for clarity, similar to the ground truth.",gemini-2.0-flash,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter

# Download stopwords if not already downloaded
try:
    stop_words = set(stopwords.words('english'))
except LookupError:
    nltk.download('stopwords')
    stop_words = set(stopwords.words('english'))

nltk.download('punkt')

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create 'title_length' feature
df['title_length'] = d...",6,"The generated code correctly addresses most of the requirements but has several issues that affect its correctness and robustness.

**Positives:**
*   Loads the dataset correctly.
*   Calculates `title_length` feature.
*   Calculates and prints correlation.
*   Handles NLTK downloads for stopwords and punkt (though `punkt` download happens unconditionally).
*   Correctly identifies and prints the top 10 most common words along with frequencies.
*   The `preprocess_title` function attempts to apply the necessary steps (lowercase, remove punctuation, tokenize, remove stopwords).

**Areas for Improvement / Errors:**

1.  **Punctuation Removal (Critical):** The line `title = ''.join([char for char in title if char.isalnum() or char.isspace()])` for punctuation removal is flawed. `isalnum()` and `isspace()` are appropriate, but `string` module is not used as required by the prompt's `string` import (which the generated code does include, but then doesn't use `string.punctuation`). More importantly, the ground truth uses `re` which is more robust for general punctuation removal (e.g., handling non-ASCII punctuation, or cases where a character might be alphanumeric but not covered by a simple filter). The generated code's approach for punctuation removal is less comprehensive than `re.sub(r'[^a-z0-9\s]', '', text)`. A better way would be using `str.translate(str.maketrans('', '', string.punctuation))` as often seen, or the `re` based approach which handles more complex unwanted characters.
2.  **Order of Operations in Preprocessing (Critical):** The `preprocess_title` function first removes punctuation, then tokenizes, then removes stopwords. This is correct. However, the correlation calculation happens *before* any of the robust error handling for `Rating` (e.g., `pd.to_numeric` with `errors='coerce'` and `dropna`). This means if the `Rating` column has non-numeric values, the `corr()` operation might raise an error or produce an incorrect result. The ground truth correctly cleans the data *before* calculating correlation.
3.  **Redundant Stopwords Download Logic:** The `try-except LookupError` for `stopwords` is good, but `nltk.download('punkt')` is called unconditionally *after* that block. It would be better to wrap it in a `try-except` too to be consistent and avoid unnecessary downloads on subsequent runs if already downloaded.
4.  **No mention of ""Pearson"" correlation in print output:** The prompt specifically asks for ""Pearson correlation coefficient"". While `corr()` defaults to Pearson, it's good practice to explicitly state it in the print output as shown in the ground truth.
5.  **Efficiency of token joining for common words:** While functionally correct, `all_tokens = []` followed by a loop and `extend` is less Pythonic and potentially less efficient than a list comprehension like `all_words = [word for sublist in df['processed_title'] for word in sublist]` used in the ground truth.

To score higher, the code needs to:
*   Refine punctuation removal to be more robust and align with common practices or the prompt's implied need for `string` (or `re`).
*   Ensure data types are handled correctly for correlation calculation *before* attempting the correlation.
*   Standardize NLTK download checks.",gemini-2.0-flash,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load the dataset
penguins = pd.read_csv(""penguins.csv"")

# Define features and target
X = pengu...",4,"The generated code attempts to use pipelines and `ColumnTransformer` which is a good approach for complex preprocessing workflows, but it fails to correctly implement key requirements of the task, particularly around imputation and one-hot encoding within the pipeline.

**Major Issues:**

1.  **Incorrect Categorical Imputation and One-Hot Encoding:**
    *   The `pd.get_dummies` function cannot be directly used within a `Pipeline` or `ColumnTransformer` as a `Transformer` because it's a Pandas function, not a scikit-learn transformer with `fit`/`transform` methods. This fundamentally breaks the preprocessing pipeline for categorical features.
    *   The code tries to perform manual `fillna` and `pd.get_dummies` *outside* the `ColumnTransformer` and *after* defining it, which defeats the purpose of the pipeline structure and leads to processing issues (e.g., `X` is already modified before the pipeline tries to transform it).
    *   The `categorical_transformer` is defined with `('onehot', pd.get_dummies)` but the `ColumnTransformer` then uses `('cat', 'passthrough', categorical_features)`. This is a contradiction and indicates confusion about how `ColumnTransformer` works. `passthrough` means no transformation, so the defined `categorical_transformer` pipeline for one-hot encoding is never applied.
    *   The manual dropping of rows with `X = X.dropna(subset=['island'], axis=0)` is incorrect. The task specifically asked to drop rows *if mode imputation results in NA for island*. This implies handling within the imputation logic, not a blanket drop before imputation is fully resolved.
    *   The problem specifically asked for dropping rows where `island` is still NA *after* mode imputation (e.g., if all `island` values were NA). The generated code's `X.dropna(subset=['island'], axis=0)` happens before specific mode imputation, and the manual `fillna` for `island` and `sex` are done on `X` *before* the `ColumnTransformer` is even applied. This means the `SimpleImputer` in the pipeline is never used for these features.

2.  **Target Encoding:** The target variable `y` (`species`) is never label encoded. `RandomForestClassifier` works with string labels, but it's generally good practice for consistency, and the ground truth explicitly uses `LabelEncoder`. More importantly, `cross_val_score` expects input `X` and `y` where `y` is numerically encoded if necessary for the specific estimator. While `RandomForestClassifier` can sometimes handle string targets, consistent preprocessing is better.

3.  **Mixing Pipeline and Manual Preprocessing:** The code defines a `ColumnTransformer` and a `Pipeline`, but then manually performs `fillna` and `get_dummies` on `X` *before* passing it to `cross_val_score` with the `model` pipeline. This means the pipeline's preprocessing steps are largely redundant or incorrectly applied. The purpose of `Pipeline` and `ColumnTransformer` is to handle *all* preprocessing as a single step.

4.  **Inefficient `numerical_transformer.fit`:** `numerical_transformer.fit(penguins[numerical_features])` is called on the original `penguins` DataFrame *before* `cross_val_score` even starts. This is incorrect. The `Pipeline` (and its internal `ColumnTransformer`) should handle all `fit_transform` operations on the `X` passed to `cross_val_score` for each fold, ensuring proper data leakage prevention.

5.  **Missing `missing_values` in `SimpleImputer`:** While `np.nan` is default, explicitly passing `missing_values=np.nan` for `SimpleImputer` is good practice, especially given that `pd.get_dummies` is used, which might introduce `NaN` if not handled correctly. However, the core issue is the `pd.get_dummies` in the pipeline.

**Summary of Critical Failures:**

*   Failed to properly implement categorical imputation and one-hot encoding within the scikit-learn pipeline framework.
*   Mixed manual preprocessing with pipeline usage, leading to incorrect or redundant steps.
*   Missed the requirement for label encoding the target variable.
*   Did not correctly handle the `island` NA dropping condition based on mode imputation.",gemini-2.0-flash,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv(""penguins.csv"")

# Select features
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_subset = df[features].copy()

# Drop rows with missing values
df_subset = df_subset.dropna()

# Standardize the features
scaler = StandardScaler()
scaled_...",7,"The generated code largely achieves the task objectives, demonstrating good understanding of preprocessing, clustering, and dimensionality reduction.

**Strong Points:**
*   Correctly loads the dataset.
*   Correctly identifies and selects the specified features.
*   Handles missing values by dropping rows.
*   Applies `StandardScaler` for standardization.
*   Implements `KMeans` with the correct number of clusters, `n_init='auto'`, and `random_state=42`.
*   Performs `PCA` to 2 components.
*   Generates two scatter plots with correct axes (PC1, PC2) and titles.

**Areas for Improvement (Why it's not an 8-10):**
1.  **Species Alignment Issue:** The most critical flaw is in how `species_labels` is derived and aligned with `pca_df`.
    *   `df_subset = df[features].dropna().reset_index(drop=True)` is called *after* `scaled_features` and `principal_components` have already been computed from `df_subset` before its index reset. This re-processes the dropna, not ensuring consistency.
    *   `species_subset = df['species'].dropna().reset_index(drop=True)` creates a `species` Series that has dropped NAs *independently* from the feature set, meaning its rows might not align with the cleaned feature set `scaled_features`.
    *   `species_labels = species_subset[df[features].dropna().index]` tries to use the index of the re-cleaned feature dataframe to select from the independently cleaned species, which is problematic and leads to potential mismatches. The original dataframe's index used for the `species` selection is what matters, *after* the initial `df[features].dropna()` operation.
    *   The robust way, as shown in the ground truth, is to drop NAs from a DataFrame that *already contains both features and species*, ensuring their alignment.

2.  **Plotting Original Species:** The method for plotting `Original_Species` is less efficient and robust than using `seaborn` or direct `matplotlib` with `hue` parameter after adding `Original_Species` to `pca_df`. Iterating through `unique_species` and calling `plt.scatter` multiple times can be less efficient than a single call when all data is structured in a DataFrame. While it works, it's not the most ""Pythonic"" or efficient for this type of plot.

3.  **Code Structure/Robustness:** The ground truth includes `try-except` for file loading, which is a good practice for robustness in real-world scenarios, though not explicitly required by the prompt.

Overall, the core data science steps are correct, but the handling of the `species` column for plotting, especially after NA removal, introduces an alignment vulnerability. Fixing this alignment issue would push the score higher.",gemini-2.0-flash,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# 1. Load the dataset
df = pd.read_csv(""penguins.csv"")

# 2. Impute missing values
df['culmen_length_mm'].fillna(df['culmen_length_mm'].mean(), inplace=True)
df['culmen_depth_mm'].fillna(df['culmen_depth_mm'].mean(), inplace=True)
df['flipp...",7,"The generated code correctly addresses most of the task requirements but has a few areas for improvement.

**Strengths:**
*   Correctly loads the dataset.
*   Handles missing numerical values with their mean and 'sex' with its mode.
*   Correctly drops rows where 'island' is NA.
*   Accurately creates features `X` by combining numerical features and one-hot encoded 'island' and 'sex'.
*   Correctly label encodes the `species` target `y`.
*   Successfully trains a `GradientBoostingClassifier` with the specified parameters.
*   Extracts and prints feature importances with corresponding feature names.
*   Generates box plots for `flipper_length_mm` grouped by `sex` for each `species`.

**Areas for Improvement:**

1.  **Plotting Accuracy for Task 6:** The prompt specifically asked for ""For each `species`, create box plots to visualize the distribution of `flipper_length_mm` (using imputed data) grouped by the imputed `sex` column."" This implies a single plot showing `species` on the x-axis, `flipper_length_mm` on the y-axis, and `sex` as a hue, similar to the ground truth. The generated code produces separate plots for each species, with `sex` on the x-axis, which technically fulfills the ""for each species"" and ""grouped by sex"" but misses the implied overall comparison across species in a single plot that the phrasing ""For each `species`, create box plots... grouped by the imputed `sex` column"" usually suggests, and which the ground truth correctly interpreted with the `hue` parameter. While a valid visualization, it's not the most direct interpretation of the prompt's intent for cross-species comparison.

2.  **`df.copy()` Best Practice:** The ground truth uses `df_processed = df.copy()` to avoid `SettingWithCopyWarning`. While the generated code doesn't explicitly run into this warning in its current form, it's a good practice to adopt when performing modifications on a DataFrame derived from another.

3.  **Redundant Import:** `from sklearn.model_selection import train_test_split` is imported but not used.

4.  **Plot Titles:** The plots generated have titles, but the overall prompt asked for ""Ensure plots have titles."", which is met, but consistent naming and potentially more descriptive titles would enhance it.

The overall solution is very close to perfect, with the main discrepancy being the interpretation and execution of the final plotting requirement, which could be more concise and comparative.",gemini-2.0-flash,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer

# Load the dataset
penguins = pd.read_csv(""penguins.csv"")

# Numerical columns to impute
numerical_cols = [""culmen_length_mm"", ""culmen_depth_mm"", ""flipper_length_mm"", ""body_mass_g""]

# KNN Imputation for numerical columns
imputer = KNNImputer(n_neighbors=3)
penguins[numerical_cols] = imputer.fit_transform(penguins[numerical_cols])

# Categorical column to impute
categorical_col = ""sex""

# Impute with mode for...",8,"The generated code correctly performs all the required steps: loading the data, applying KNN imputation to the specified numerical columns, and imputing the `sex` column with its mode. It also successfully prints the number of remaining NA values to verify the imputation.

Strong points:
- Correctly uses `KNNImputer` with `n_neighbors=3`.
- Correctly applies the mode imputation for the 'sex' column using `fillna` and `mode()[0]`.
- Prints the final NA counts as requested.
- Concise and to the point.

Areas for improvement (to achieve a higher score):
- **Error Handling:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is good practice. The generated code lacks this, meaning it would crash if `penguins.csv` isn't found.
- **Verification Clarity:** While printing `penguins.isna().sum()` fulfills the requirement, explicitly verifying the NA count for the *specifically imputed* columns (as done in the ground truth's ""Optional"" section) can make the verification clearer, especially in a scenario with many columns where some might still have NAs from other categories. However, the current output is certainly acceptable per the prompt.",gemini-2.0-flash,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter for Adelie and Gentoo species
df_filtered = df[df['species'].isin(['Adelie', 'Gentoo'])]

# Handle NA values in culmen_length_mm by dropping rows
df_filtered = df_filtered.dropna(subset=['culmen_length_mm'])

# Separate the data for each species
adelie_culmen_length = df_filtered[df_filtered['species'] == 'Adelie']['culmen_length_mm']
gentoo_culmen_length = df_filtered[df_filtered...",9,"The generated code correctly performs all the required steps: loading the data, filtering species, handling NAs, performing the t-test, and interpreting the results. The code is concise and directly addresses the problem.

**Strong Points:**
*   **Correctness:** All steps are implemented accurately.
*   **Conciseness:** The code is straightforward and avoids unnecessary complexity.
*   **Completeness:** All aspects of the task are covered, including the final interpretation of the hypothesis test.

**Areas for Minor Improvement (Why not a 10):**
*   **Error Handling:** The code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. While the prompt implied the file would exist, robust code should account for this.
*   **Modularity:** The code is written as a script rather than encapsulated in a function, which makes it less reusable and testable compared to the ground truth's function-based approach.
*   **Assumptions for `ttest_ind`:** The `equal_var=True` parameter in `scipy.stats.ttest_ind` is often explicitly set for clarity and to indicate the assumption of equal variances (Student's t-test). While `True` is the default, explicitly stating it improves readability and ensures the user understands the specific t-test variant being performed.
*   **Informative Prints:** The ground truth includes more verbose print statements for progress tracking and detailed explanations of the steps being performed, which is beneficial for debugging and understanding, especially in a benchmark scenario where the output needs to be clear. For example, it prints the shape of the filtered dataframes, the number of dropped rows, and explicit means, which aids in validating intermediate steps.",gemini-2.0-flash,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Load the dataset
df = pd.read_csv(""penguins.csv"")

# Define features and target
features = [
    ""culmen_length_mm"",
    ""culmen_depth_mm"",
    ""flipper_length_mm"",
    ""body...",9,"The generated code correctly implements all the requirements of the task, including data loading, feature definition, comprehensive preprocessing (median imputation for numerical, mode imputation for 'sex', one-hot encoding for 'island' and 'sex', and label encoding for 'species'), stratified data splitting, Random Forest model training, and evaluation with accuracy and a classification report.

Strengths:
- All preprocessing steps are correctly applied as specified: median imputation for numerical features, mode imputation for 'sex', and one-hot encoding for 'island' and 'sex'.
- `species` is correctly label encoded.
- Data splitting is stratified by `species` and uses the correct `random_state`.
- The Random Forest Classifier is instantiated and trained with the specified `n_estimators` and `random_state`.
- Evaluation metrics (accuracy and classification report) are correctly calculated and printed.
- The use of `sparse_output=False` in `OneHotEncoder` is good practice for older `sklearn` versions or when direct dataframe concatenation is preferred.

Areas for minor improvement:
- The way `X` is modified in-place for numerical imputation (`X[numerical_features] = numerical_imputer.fit_transform(X[numerical_features])`) might trigger a `SettingWithCopyWarning` in pandas depending on how `X` was created (e.g., if `X = df[features]` returns a view). Using `.copy()` when splitting `X = df[features].copy()` would prevent this, which the ground truth does. Similarly, for `'sex'` imputation, `X.loc[:, ""sex""] = X[""sex""].fillna(X[""sex""].mode()[0])` would be a safer explicit modification.
- The OneHotEncoding for `island` and `sex` is done separately and then concatenated. While functionally correct, using `ColumnTransformer` within a `Pipeline` (as seen in the ground truth) is generally considered a more robust, cleaner, and often more efficient way to handle heterogeneous transformations, especially when dealing with many feature types or complex preprocessing steps. It also ensures proper alignment and prevents potential manual concatenation errors. However, for this specific task with only two categorical features, the manual approach is acceptable and correct.
- Passing `target_names=label_encoder.classes_` to `classification_report` (as the ground truth does) would provide more readable output for the report, showing actual species names instead of default numeric labels (0, 1, 2). This is a minor aesthetic improvement.

Overall, the code is highly correct and complete, meeting all requirements. The minor points are more about software engineering best practices for complex pipelines than functional correctness.",gemini-2.0-flash,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Calculate correlation coefficients
correlation_experien...",9,"The generated code correctly defines the DataFrame, calculates the Pearson correlation coefficients (using the default 'pearson' method of `.corr()`), prints them, and correctly determines the feature with the stronger absolute correlation.

**Strong points:**
*   Correctly sets up the DataFrame.
*   Accurately calculates both correlation coefficients.
*   Correctly uses `abs()` to compare correlation strength.
*   The logic for determining the stronger correlation is sound, including the tie-breaking condition.

**Areas for minor improvement:**
*   **Printing format for correlations:** The ground truth code formats the correlation coefficients to 4 decimal places using `:.4f`. The generated code prints them without specific formatting, which is slightly less clean for correlation values.
*   **Clarity in tie condition:** The tie condition `stronger_feature = ""Both features have the same correlation strength""` is functional but could be slightly more descriptive, perhaps indicating ""Both features have the same absolute correlation strength with PerformanceScore."" similar to the ground truth. This is a minor stylistic point.
*   **Explicit `method='pearson'` (optional but good practice):** Although `df.corr()` defaults to Pearson, explicitly stating `method='pearson'` can improve code readability and remove ambiguity, especially for someone who might not know the default. This is a very minor point and not strictly necessary for correctness here.",gemini-2.0-flash,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test
from scipy import stats
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pymc3 as pm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.neighbors import NearestNeighbors
import warnings
...",3,"The generated code attempts to address the prompt but fundamentally misunderstands key aspects of the task, particularly regarding survival analysis and advanced statistical modeling, leading to incorrect implementations.

**Key Failures and Missing Criticalities:**

1.  **Kaplan-Meier Survival Analysis Misapplication:**
    *   **Fundamental Error:** The prompt explicitly states to treat ""voyage as time-to-event data (with survival as event indicator)"". The ground truth correctly interprets this as a fixed time `T=1` for all observations, with `event_observed` being `1 - Survived` (death). This effectively models the *event of death during the voyage*.
    *   **Generated Code's Error:** The generated code uses `titanic['Age']` as the `duration_col` and `titanic['Survived']` as `event_observed` for Kaplan-Meier. This is incorrect for the defined task of ""voyage as time-to-event"". If 'Age' were the time variable, 'Survived' being the event would mean *survival beyond a certain age*, not surviving the actual voyage. Furthermore, Kaplan-Meier inherently models time-to-event data, and `titanic['Survived']` (1 for survived, 0 for died) is the *opposite* of what `lifelines` expects for `event_observed` (1 for observed event, 0 for censored). This is a critical misunderstanding of survival analysis basics and the prompt's specific framing.

2.  **Cox Proportional Hazards Misapplication:**
    *   **Same Fundamental Error:** Similar to KM, the generated code uses `duration_col='Age'` and `event_col='Survived'` for the Cox model. Again, this setup incorrectly models survival beyond an age rather than survival during the fixed-time voyage, which is what the prompt implies by ""voyage as time-to-event data"". The context of the Titanic dataset implies a single event (the sinking), not survival over varying durations (ages). The ground truth's approach of `T=1` effectively turns the Cox model into a logistic regression equivalent in this specific scenario, aligning with the prompt's implied fixed ""voyage time.""
    *   **Incorrect `event_col`:** Again, `Survived` (1=survived) used as `event_col` will be interpreted incorrectly by `lifelines` which expects 1 for event and 0 for censored.

3.  **Advanced Statistical Tests - Multivariate Analysis:**
    *   The prompt asked for ""multivariate analysis, interaction effects testing, and model diagnostics."" While OLS regression is a multivariate technique, it's typically used for continuous outcomes, not binary ""Survived"" outcomes in this context. Logistic regression (as used in the ground truth for some parts, and Bayesian logistic regression) would be more appropriate for a binary survival outcome.
    *   Model diagnostics are bare-bones (just a Q-Q plot for residuals, which is not suitable for a binary outcome from OLS).

4.  **Propensity Score Matching (PSM) Implementation:**
    *   **Treatment Definition:** The prompt asks for ""treatment effects (e.g., passenger class impact)"". The generated code defines `y = titanic[treatment].isin([1, 2]).astype(int)`. This makes 'Pclass 1 or 2' the ""treated"" group and 'Pclass 3' the ""control"". This is a valid choice, but the prompt implies a single treatment/control comparison rather than a composite group.
    *   **Matching Logic:** The matching part for ""Nearest Neighbors matching using propensity scores"" only matches treated instances to control instances. It doesn't combine them into a single matched dataframe or explicitly check covariate balance after matching, which is a crucial step in PSM.
    *   **Outcome Analysis:** It calculates ATT correctly for the chosen definition, but lacks further statistical testing on the matched groups, such as comparing survival rates or performing a chi-squared test.

5.  **Bootstrap Resampling:**
    *   The example provided (bootstrap CI for Kaplan-Meier median survival) is a good idea in isolation, but the implementation of `median_survival` is problematic due to the incorrect use of 'Age' as duration and 'Survived' as event, making the result meaningless in the context of the prompt.
    *   The prompt implies using bootstrapping for confidence intervals of ""statistical inference"" more broadly, such as for the Cox regression coefficients, which the ground truth does. The generated code only does one example, and a flawed one at that.

6.  **Bayesian Analysis:**
    *   The PyMC3 model is a simple logistic regression, which is okay as a start, but it doesn't align with ""probabilistic modeling of survival factors"" in the sense of time-to-event data. The ground truth, despite its `T=1` simplification, still models a Cox regression (which is a form of survival model), while the generated code switches to a simple binary outcome model. While Bayesian approaches to survival models (e.g., AFT models) exist in PyMC3, given the `T=1` interpretation, a logistic regression is the closest equivalent; however, the task implied probabilistic modeling of *survival factors* within the context of survival analysis.
    *   `pm.traceplot` and `pm.summary` are correctly used, but `cores=1` is typically discouraged for sampling unless necessary.

7.  **Model Validation and Sensitivity Analysis:**
    *   The ""sensitivity analysis"" is extremely basic: perturbing *one* variable (Age) and comparing its hazard ratio. This is not a comprehensive sensitivity analysis as requested. It doesn't validate model assumptions (like PH assumption, which `lifelines` can check), nor does it cover sensitivity to imputation strategies or prior choices in Bayesian models, as the ground truth does. The `duration_col='Age_perturbed'` is also problematic for the same reasons as the initial Cox model.

**Overall:** The generated code shows familiarity with libraries but struggles with the conceptual understanding of survival analysis and fails to meet the advanced requirements and rigor implied by the prompt. The misapplication of `duration_col` and `event_col` in `lifelines` functions is a critical flaw.",gemini-2.0-flash,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Create the DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
        'Final_Score': [90, 88, 82, 92, 80, 91, 8...",8,"The generated code correctly addresses all parts of the task. It successfully creates the DataFrame, calculates and prints the Pearson correlation, generates a scatter plot colored by subject with appropriate labels and legend, adds the 'Overall_Grade' column, and prints the grade distribution.

Strengths:
- **Correctness:** All calculations and visualizations are correct according to the prompt.
- **Clear Logic:** The steps are logically separated and easy to follow.
- **Successful Plotting:** The scatter plot is generated correctly with points colored by subject, a title, axis labels, and a legend.
- **Accurate Grade Assignment:** The `assign_grade` function correctly applies the grading logic.

Areas for Improvement:
- **Seaborn Usage:** The prompt explicitly mentioned ""seaborn for potentially better aesthetics but not required for coloring"". While the `matplotlib` solution works, `seaborn.scatterplot` (as used in the ground truth) would provide a more concise and often more aesthetically pleasing way to color by a categorical variable using the `hue` argument, without needing to manually iterate through subjects and manage a `color_map`. This is a minor point as the current implementation is functional.
- **Color Mapping Robustness:** The `colors` list in the generated code is hardcoded with only four colors. If there were more than four unique subjects in the `data` (e.g., 'Physics'), an `IndexError` would occur when `color_map[subject]` is accessed. Using seaborn or a more dynamic color palette (e.g., `plt.cm.get_cmap('viridis', len(subjects))`) would be more robust.
- **Output Formatting:** The ground truth includes more verbose print statements for clarity and progress tracking, which can be helpful in larger scripts. The generated code is more concise but slightly less communicative.
- **`pearsonr` unpack:** While `correlation, _ = pearsonr(...)` is correct, directly using `pearson_r_value = pearsonr(df['Attendance_Rate'], df['Final_Score'])[0]` would also work and is sometimes seen as slightly more explicit, though the unpacking is common.",gemini-2.0-flash,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']),
        'Ticker': ['TECHCO']*10,
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0],
        'Low': [149.5, 150.5, 1...",10,"The generated code is excellent. It correctly performs all requested operations: DataFrame creation, daily return calculation, 3-day and 5-day SMA calculation, plotting with all required labels and legend, and accurate identification of bullish crossover dates. The code is well-structured, uses proper imports, and produces the correct output. The plot is visually clear, and the crossover logic effectively captures the bullish signal.",gemini-2.0-flash,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUST0...",9,"The generated code correctly addresses all task requirements: DataFrame creation, calculation of total sales per product, generation of a horizontal bar chart, calculation of average order value, and identification of repeat customers. The code is well-structured and uses appropriate pandas and matplotlib functions.

Strengths:
- Correctly calculates and visualizes total sales revenue per product.
- Accurately calculates the average order value.
- Correctly identifies repeat customers and their purchase counts.
- All necessary imports are included.
- The use of `value_counts()` and boolean indexing for repeat customers is efficient and idiomatic pandas.

Areas for minor improvement:
- The `product_revenue.sort_values()` by default sorts in ascending order. While not strictly wrong (the user can still interpret it), typically for revenue charts, a descending sort is preferred to quickly see top performers. The ground truth sorts in descending order, making it slightly more intuitive for this type of visualization.
- The print statements could be a bit more elaborate for better readability, similar to the ground truth (e.g., adding headers for each section of output like ""--- Customers with More Than One Purchase ---"").
- The plot could benefit from a few more styling elements like a grid or `tight_layout()` for better presentation, though the core requirements are met.",gemini-2.0-flash,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd
import numpy as np

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Create DeviceType_Numeric
df[""DeviceType_Numeric""] = df[""DeviceType""].map({""Desktop"": 0, ""Mobile"": 1})

# ...",9,"The generated code is highly correct and complete. It successfully defines the DataFrame, creates both `DeviceType_Numeric` and `PagesPerMinute` features exactly as specified, including the crucial handling of potential division by zero for the latter. The feature selection is accurate, and the head of the resulting DataFrame is printed.

The only minor difference from the ground truth is the conciseness of the division-by-zero handling (`replace` and `fillna` chained) and the lack of explicit print statement for the head of the DataFrame (though it's implicitly done by `print(feature_df.head())`). Both approaches are functionally equivalent and correct. The ground truth breaks down the `PagesPerMinute` handling into separate steps, which can sometimes be clearer for complex operations, but the generated code's chaining is also a common and acceptable pandas idiom. The ground truth also explicitly stated `X` as the feature set, while the generated code used `feature_df`, which is a minor naming convention difference but doesn't affect correctness. Overall, it's a very strong submission.",gemini-2.0-flash,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26, 45, 28]
}
df = pd.DataFrame(data)
...",9,"The generated code correctly defines the DataFrame, calculates the mean and standard deviation of ""TemperatureCelsius"", identifies outliers based on the 1.5 standard deviation threshold, and prints the requested information for each identified outlier. The code is clean and directly addresses all parts of the prompt.

**Strong points:**
*   Correct DataFrame creation.
*   Accurate calculation of mean and standard deviation.
*   Correct application of the 1.5 standard deviation rule for outlier detection (both above and below the mean).
*   Correct looping through identified outliers and printing the required fields.
*   Handles the case where no outliers are found.

**Minor Improvements:**
*   The `Timestamp` column in the ground truth is converted to `datetime` objects, which is good practice for time-series data. While not strictly required for this specific outlier detection logic, it's a valuable step for proper time-series analysis and formatting. The generated code prints the timestamp as a string, which is acceptable given the input, but a `datetime` object would allow for more flexible formatting (as seen in the ground truth's `strftime`).
*   The output format for the outliers could be slightly more descriptive (e.g., adding ""Outlier"" or ""Anomalous temperature"") as in the ground truth, but this is a minor stylistic point not explicitly required by the prompt.",gemini-2.0-flash,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [10, 5, 25, 15, 8, 30, 9,...",9,"The generated code correctly defines the DataFrame, calculates the EngagementScore, and accurately identifies the PostType with the highest average EngagementScore for each platform. The printing format is slightly different from the ground truth (e.g., ""Average EngagementScore"" vs ""Highest Average EngagementScore""), but this is a minor stylistic difference and does not affect correctness. The use of `nlargest` within a `groupby().apply()` is an effective and concise way to achieve the desired result for step 2.

Strong points:
- **Correctness:** All tasks are completed accurately.
- **Conciseness:** The logic for finding the best post type per platform is quite elegant.
- **Readability:** The code is well-structured and easy to understand.

Minor points for consideration (not affecting the score significantly):
- The output string for ""Average EngagementScore"" could be slightly more descriptive matching the ""Highest Average EngagementScore"" from the problem description, though this is a minor detail.
- Formatting of the printed `engagement_score` to two decimal places (e.g., `f""{engagement_score:.2f}""`) would match the ground truth's precision and often be preferable in data analysis outputs.",gemini-2.0-flash,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu"", ""Injury"", ...",10,"The generated code correctly performs all the requested tasks:

1.  **DataFrame Definition**: The DataFrame is correctly initialized with the provided data.
2.  **LengthOfStay Calculation**: The `LengthOfStay` column is accurately calculated as `(DischargeDate - AdmissionDate).dt.days`.
3.  **Aggregation per Diagnosis**: The `groupby(""Diagnosis"").agg()` method is correctly used to calculate both `AverageLengthOfStay` and `PatientCount` for each unique diagnosis.
4.  **Printing Summary**: The summary information (Diagnosis, Average Length of Stay, Patient Count) is iterated through and printed clearly.
5.  **Longest Average Stay Identification**: `idxmax()` is correctly applied to `AverageLengthOfStay` to find the diagnosis with the longest average stay among them.

The code is clean, concise, and directly addresses all parts of the prompt. No unnecessary complexity or missing steps are present. It adheres to good practices by using standard pandas functionality.",gemini-2.0-flash,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",9,"The generated code correctly defines the DataFrame, calculates the ""WeightedRating"" as specified, adds it as a new column, and identifies the movie with the highest ""WeightedRating,"" printing its title and weighted rating. The code is concise and directly addresses all parts of the prompt.

One minor point for improvement, which is not a correctness issue but a robustness one, is the lack of a check for `total_votes == 0` before division. While the provided dataset guarantees a non-zero sum for 'Votes', in a more general scenario, this check would prevent a `ZeroDivisionError`. The ground truth code includes this robustness check, making it slightly more robust for arbitrary inputs. However, given the specific constraints of the task with a predefined dataset, its absence does not lead to an error.

Overall, it's a very good and efficient solution.",gemini-2.0-flash,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 10000, 8000]
}
df = pd...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"" using `groupby().agg()`, and then prints the resulting grouped DataFrame, fulfilling all requirements of the task. The code is concise, efficient, and demonstrates a good understanding of pandas operations.",gemini-2.0-flash,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63, 83],
    ""De...",10,"The generated code is excellent. It correctly defines the DataFrame, adds the ""TotalStats"" column, accurately identifies the Pokemon with the highest total stats, and correctly calculates the average speed for each ""Type1"". The code is concise, well-structured, and directly addresses all parts of the task. The use of `idxmax()` is efficient for finding the max value's index, and `groupby()` is correctly applied for the average calculation.",gemini-2.0-flash,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap

from scipy.cl...",6,"The generated code attempts to address most aspects of the prompt but falls short in several key areas compared to the ground truth, resulting in a less comprehensive and robust solution.

**Strengths:**
*   **Feature Engineering:** Includes family size, `IsAlone`, title extraction, ticket grouping, and name length, which aligns well with the prompt.
*   **Multiple Clustering Algorithms:** Implements K-Means, Hierarchical, DBSCAN, and GMM.
*   **Dimensionality Reduction:** Incorporates PCA, t-SNE, and UMAP for visualization.
*   **Visualization:** Basic visualizations for PCA and a heatmap for cluster means are present.
*   **Cluster Stability:** Attempts a form of consensus clustering.

**Weaknesses and Areas for Improvement:**

1.  **Missing Advanced Feature Engineering:**
    *   **Ground Truth (GT):** Includes `FarePerPerson`, `TicketPrefix` (with grouping), `CabinDeck`, `HasCabin`, `Age_Class`, `Fare_Class`, and `FarePerPerson_Class` as interaction features.
    *   **Generated Code (GC):** Introduces a `SocioEconomic` feature which is an interesting concept but less directly aligned with ""social network analysis"" or ""economic indicators"" compared to the GT's more granular and direct features like `FarePerPerson` or `CabinDeck` based on actual data. It misses the extensive interaction features and detailed text-based processing for `Ticket` and `Cabin`.

2.  **Preprocessing and Scaling Robustness:**
    *   **GT:** Uses `ColumnTransformer` and `Pipeline` for robust and reproducible preprocessing, handling numerical and categorical features separately (e.g., `StandardScaler` for numerical, `OneHotEncoder` for categorical), and handles imputation for `Fare` prior to `FarePerPerson` calculation.
    *   **GC:** Uses `LabelEncoder` for all categorical features, which is problematic for nominal categories (like `Embarked`, `Title`, `SocioEconomic`) as it introduces an artificial ordinal relationship that can mislead distance-based clustering algorithms. `OneHotEncoder` is generally preferred. It also imputes `Age` and `Embarked` but doesn't handle `Fare` or `Cabin` or `Ticket` related imputation directly in the preprocessing pipeline.

3.  **Optimal Cluster Determination:**
    *   **GT:** Explicitly calculates and plots Silhouette, Calinski-Harabasz, and Davies-Bouldin scores for all *algorithms* across a range of K.
    *   **GC:** Calculates Silhouette and Calinski-Harabasz but only prints them during the loop, making it hard to see the trends or choose an optimal K visually. It then *automatically* picks the 'optimal' K based solely on Silhouette for each algorithm, which might not always be the best approach (e.g., Elbow method for K-Means inertia is common). There are no comprehensive plots for these metrics across K values. Gap Statistic and Elbow method (beyond K-Means inertia which wasn't fully utilized for decision-making) were not implemented.

4.  **DBSCAN Implementation and Handling:**
    *   **GT:** Acknowledges and handles DBSCAN's noise points (-1 label) by excluding them from cluster characteristic analysis and visualizations, which is crucial for interpretability.
    *   **GC:** `X.groupby('DBSCAN_Cluster').mean()` includes noise points (-1), which can skew means and visualizations. It also has a basic check for `len(set(dbscan_labels)) > 1` but doesn't fully manage the noise labels in subsequent analysis.

5.  **Cluster Stability Analysis:**
    *   **GT:** Offers a simpler but direct and effective stability analysis for K-Means using Adjusted Rand Index across multiple runs with different random states.
    *   **GC:** Implements a more complex ""consensus clustering"" with `scipy.cluster.hierarchy.fcluster` on a consensus matrix. While theoretically advanced, its implementation might be an overkill for ""stability analysis"" as defined for a benchmark, and the chosen method (`linkage(squareform(distance_matrix), method='ward')`) may not be the most robust choice. It also uses a random state inside the loop for `KMeans(random_state=None)`, which is good for stability, but the overall `consensus_clustering` function's output and how it directly reflects 'stability' is less clear compared to direct ARI comparisons.

6.  **Comprehensive Visualizations:**
    *   **GT:** Provides dedicated plots for cluster distributions in reduced dimensions (PCA, t-SNE, UMAP), survival rates per cluster, and violin/count plots for key feature distributions, as well as a heatmap of *scaled* feature means.
    *   **GC:** Only provides a scatter plot for PCA with K-Means clusters and a heatmap of *unscaled* feature means for K-Means, which can be misleading as different features have different scales. It lacks similar visualizations for other algorithms, survival rate plots, and detailed feature distributions (violin/count plots) per cluster.

7.  **External Validation:**
    *   **GT:** Explicitly analyzes survival distributions segmented by each clustering result, including counts and percentages, and visualizes survival rates for each algorithm.
    *   **GC:** Calculates survival rates for each algorithm but only prints them and doesn't visualize them or provide a detailed breakdown (e.g., percentage survived vs. not survived).

Overall, the generated code implements the core components but lacks the depth, robustness, and comprehensive analysis/visualization aspects that the ground truth achieves, particularly in feature engineering, preprocessing, and the detailed presentation of clustering results.",gemini-2.0-flash,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dystopian""],
    ""Stock"": [...",8,"The generated code correctly defines the DataFrame and performs all requested calculations. It successfully groups by author to count books and calculate average price, and it accurately identifies the author with the most books in stock.

Strengths:
- **Correctness:** All calculations are performed correctly and the outputs match the task requirements.
- **Concise:** The use of `groupby().agg()` is efficient and clean for calculating the count and average price.
- **Accurate identification:** Successfully uses `idxmax()` and `max()` to find the author with the most stock and their stock count.

Areas for Improvement:
- **Printing format:** While the `author_summary` DataFrame is printed, it doesn't match the specific formatted output requested in the ground truth (e.g., ""Author: X | Books: Y | Avg Price: $Z.YY""). The generated code simply prints the DataFrame, which is functional but less tailored for human readability as specified by ""Print this information"".
- **Clarity of output:** Adding descriptive print statements for the `author_summary` output, similar to the ground truth, would make the output more user-friendly and aligned with the ""print this information"" requirement. The ground truth explicitly states ""Print this information, showing Author, Count of Books, and Average Price."" and then formats it clearly. The generated code prints a DataFrame which fulfills the information part, but not the specific ""showing Author, Count of Books, and Average Price"" format requested.",gemini-2.0-flash,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split, learning_curve, validation_curve
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sk...",6,"The generated code attempts to address most of the requirements but has significant shortcomings in implementation, especially concerning advanced preprocessing, correct application of cross-validation for model training/evaluation, and the structure of ensemble methods.

**Primary Issues:**

1.  **Data Preprocessing:**
    *   **Multiple Imputation Strategies:** The code implements `SimpleImputer` (mean, median) and `KNNImputer`, but it applies them separately and then arbitrarily picks `X_imputed_mean` to proceed. It doesn't implement a *multiple imputation strategy* where multiple imputed datasets are generated and combined, or more advanced techniques like MICE which can handle multivariate missing data through iterative imputation (which the ground truth uses `IterativeImputer`).
    *   **Outlier Detection/Treatment:** The `remove_outliers_iqr` function is defined but commented out and clearly stated as a placeholder. It also doesn't apply it to *all* columns, which the ground truth's `OutlierCapper` handles comprehensively within a pipeline.
    *   **Preprocessing Pipeline:** The preprocessing steps (imputation, outlier handling, feature engineering, scaling) are done sequentially outside a `Pipeline` or `ColumnTransformer`. This is a critical flaw as it means:
        *   Data leakage: `fit_transform` is applied to the *entire* dataset (`X`, `y`) before `train_test_split`. This means scaling parameters, imputation values, and polynomial features are learned from the test set, leading to an overly optimistic evaluation. The ground truth correctly fits the preprocessor *only* on the training data and transforms both training and test data.
        *   Inability to easily re-apply preprocessing: For cross-validation, each fold would need its own preprocessing fit, which is not done.

2.  **Model Optimization:**
    *   `optimize_model` function uses `X_scaled` and `y` (the entire dataset) for `GridSearchCV`. This is also data leakage and inappropriate for a proper evaluation. `GridSearchCV` should be performed on the training data. The ground truth correctly splits data first, then performs `GridSearchCV` on `X_train_processed_df`.

3.  **Ensemble Methods:**
    *   **Stacking:** The generated code's ""stacking_model"" is actually just a `VotingClassifier`. True Stacking involves training base models, generating predictions, and then training a meta-learner on those predictions. The ground truth correctly implements `StackingClassifier` with base estimators and a `final_estimator`.
    *   **Blending:** The blending implementation is a simple averaging of probabilities, which is a very basic form. More advanced blending involves a separate hold-out set for training the meta-learner on the base model predictions, similar to stacking but typically with a different train/validation split management. The generated code also re-trains base models inside the blending evaluation loop, which is inefficient and not how typically blending is implemented with a fixed meta-learner from a holdout set.

4.  **Advanced Evaluation:**
    *   **Cross-Validation:** The `evaluate_model` function re-splits the *entire* `X_scaled`, `y` for each call (`skf.split(X, y)`), re-fitting the model on `X_train`, `y_train` within the function. Since `X_scaled` was generated using the full `X`, this still implicitly introduces data leakage from the test set during the initial preprocessing. The splitting should happen *once* at the beginning (train/test), and CV should be applied *only* to the training set.
    *   **Bootstrapping:** The `bootstrap_confidence_interval` function is generic and assumes raw scores. It does not perform bootstrapping on the *test set predictions* of the *trained models*, which is the correct approach to estimate confidence intervals for model metrics.

5.  **Interpretability:**
    *   **LIME:** The LIME explanation `explanation.show_in_notebook(show_table=True)` will not display the explanation in the console output, which is generally what is expected in a script-based solution. The ground truth uses SHAP effectively and shows plots.

6.  **Model Calibration:**
    *   The model calibration is performed (`calibrated_xgb.fit(X_scaled, y)` and `calibration_curve(y, calibrated_xgb.predict_proba(X_scaled)[:, 1])`) using the *entire* `X_scaled` dataset. This is data leakage for calibration, as the calibration should be fitted on training data and evaluated on hold-out test data for an unbiased assessment of its performance. The ground truth correctly applies `CalibratedClassifierCV.fit` on `X_train_processed_df` and evaluates on `X_test_processed_df`.

**Minor Issues / Areas for Improvement:**

*   **Imports:** Some redundant imports (e.g., `DataConversionWarning` imported but not directly useful for this task).
*   **Code Structure:** The code is quite procedural. Using functions and classes (similar to the ground truth's `OutlierCapper` and preprocessing pipeline) for more complex tasks is better.
*   **Cloning Models:** When evaluating models in CV or ensembles, it's good practice to clone them (`sklearn.base.clone`) to ensure each fold or component gets a fresh, unfitted instance.
*   **Metric Choice:** While accuracy is included, the task emphasizes ROC AUC as a primary metric. The evaluation section starts printouts with accuracy and adds ROC AUC later.
*   **Plotting Functions:** The `plot_learning_curve` and `plot_validation_curve` functions return `plt`, but this is usually not necessary if they are just meant to display plots directly within the script.

**Conclusion:**
While the code attempts to cover all task requirements, the fundamental mishandling of data preprocessing, train-test splitting, cross-validation boundaries, and core ensemble definitions (especially stacking) leads to logically incorrect and often overly optimistic results due to data leakage. The implementation of advanced concepts like multiple imputation, outlier capping, and proper model calibration also falls short of the task's ""advanced"" criteria.",gemini-2.0-flash,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.impute import KNNImputer, SimpleImputer, IterativeImputer
from statsmodels.imputation import mice
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pymc3 as pm
import scipy.stats as stats
from sklearn.ensem...",6,"The generated code attempts to address many aspects of the task, but several implementations are either incomplete, incorrect, or significantly simplified compared to the requirements.

**Correctness:**
*   **Missing Data Imputation:** MICE implementation is problematic (`mice.MICEData(data.copy())` followed by `update_all(5)` and then just taking `mice_imputer.data.copy()` doesn't fully demonstrate MICE *imputation* but rather sets up the MICE framework. It doesn't show how to *use* the imputed datasets from MICE for comparison, just their std. The comparison of effectiveness is superficial (just `std()`).
*   **GAM:** The `statsmodels.formula.api.glm` is used, which is a Generalized *Linear* Model, not a Generalized *Additive* Model (GAM) which requires `pygam`. This is a critical functional error for a core task component.
*   **Bayesian Logistic Regression with PyMC3:** The `pm.math.dot(X_train, beta)` inside the `pm.Model` is problematic as `X_train` is a pandas DataFrame that hasn't been converted to a PyMC3 shared variable or theano tensor. While `pymc3` can sometimes handle pandas objects, it's not robust, and it's better practice to convert to numpy arrays explicitly, especially for `pm.set_data`. Also, there's no prediction on the test set or calculation of performance metrics (ROC AUC, classification report) for the Bayesian model, which was explicitly asked for.
*   **MANOVA:** The MANOVA implementation is completely incorrect. It attempts to combine residuals of separate OLS models on *different* dependent variables, which is not how MANOVA works. MANOVA requires a single model with multiple dependent variables and categorical independent variables (or factors). The provided `sms.manova` function is for comparing means of *residuals* which is not the standard application of MANOVA for comparing group means on multiple DVs. The task specified comparing group means across water quality parameters by potability.
*   **Robust Statistical Methods:** Winsorizing and Box-Cox are demonstrated, but the prompt asked to ""Apply robust statistical methods to handle outliers and non-normal distributions"" in the context of modeling, implying methods like Robust Linear Models (RLM) or similar, which were not explicitly implemented or related to the main modeling task.
*   **Model Diagnostics:** Only a Q-Q plot of residuals is shown, and influence measure `influence_plot` is imported but not used. There's no residual analysis or goodness-of-fit tests for the logistic regression beyond a simple score, and no other diagnostics requested (e.g., leverage plots). The `residuals = y_train - y_pred_prob` is not the correct way to calculate residuals for a logistic regression (deviance or Pearson residuals are appropriate).

**Code Quality:**
*   Lots of unused imports (`statsmodels.stats.multicomp`, `levene`, `smg`).
*   The overall structure is less clear than the ground truth, particularly how intermediate imputed datasets are handled.
*   No clear print statements or section headers to indicate progress and what's being done, making it harder to follow the execution flow.
*   Lack of comments explaining non-obvious steps.

**Completeness:**
*   **Missing Data Imputation:** Needs MICE *comparison* beyond 'std'.
*   **Statistical Models:** Failed on GAM. Failed to fully implement Bayesian model prediction and evaluation.
*   **Hypothesis Testing:** MANOVA implemented incorrectly. Permutation test and bootstrap CI are basic/correct for the example but missing advanced context.
*   **Anomaly Detection:** Implemented correctly for the chosen algorithms.
*   **Dimensionality Reduction:** PCA and Factor Analysis are correctly implemented with basic output.
*   **Robust Statistical Methods:** Partially implemented with winsorizing/box-cox, but missed the RLM aspect of the ground truth.
*   **Model Diagnostics:** Very weak. Missed residual analysis details, influence measures (leverage plots), and goodness-of-fit tests explicitly asked for (beyond just ROC AUC and classification report which are general model evaluations).
*   **Statistical Visualizations:** Only Q-Q plot is present from the advanced list. It misses leverage plots and diagnostic dashboards (which the ground truth implies via multiple plots for GAM and diagnostic plots). Missing pairplot, heatmap, boxplots.",gemini-2.0-flash,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from scipy.spatial.distance import cdist
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.manifold import TSNE
import umap
import matplotlib.pyplot as plt
impor...",7,"The generated code addresses most of the task requirements but lacks depth and completeness in several areas, especially compared to the ground truth.

**Strong Points:**
*   **Initialization:** Correctly loads data and performs basic imputation and scaling.
*   **Feature Engineering:** Attempts to engineer features, although the chosen features are less comprehensive than the ground truth's and might not be as domain-relevant.
*   **Multiple Clustering Algorithms:** Implements K-means, Hierarchical, DBSCAN, GMM, and Spectral Clustering.
*   **Dimensionality Reduction:** Includes PCA, t-SNE, and UMAP for visualization.
*   **Optimal Cluster Determination:** Attempts Elbow method and Silhouette analysis.
*   **Anomaly Detection:** Uses Isolation Forest.
*   **Visualization:** Uses Plotly for 2D/3D scatter plots.

**Areas for Improvement / Missing Features:**

1.  **Feature Engineering (Advanced):**
    *   The generated features (`TDS_TH_ratio`, `CF`, `alkalinity_balance`) are very basic. The ground truth goes into much more detail with `pH_Deviation`, `Hardness_Quality_Score`, `Contamination_Risk_Score`, and `WQI_Overall`, which are much more aligned with water quality analysis.
    *   No consideration for temporal patterns (though the dataset is static, the ground truth explicitly mentions its applicability and why it's not used here).

2.  **Preprocessing on Engineered Features:** The engineered features are based on `data_scaled` which is already scaled. This might not be right for `TDS_TH_ratio` as scaled values might not make sense for ratio calculations. The ground truth correctly performs feature engineering on *imputed* data and then scales *all* features.

3.  **Optimal Number of Clusters:**
    *   **Gap Statistic Implementation:** The `calculate_gap` function provided is highly simplified and appears incorrect in its calculation of `Wk` and `WkB`. It calculates `Wk` based on `d = cdist(data, data, 'euclidean')` using the entire dataset, not within-cluster variance. It also uses `cdist(refs.values, refs.values, 'euclidean')` for `nullDists`, which doesn't seem right for reference distribution variance. The ground truth only uses `silhouette_score`, `davies_bouldin_score`, `calinski_harabasz_score`, and `SSE` (Elbow), which are more standard and correctly implemented.
    *   **Davies-Bouldin and Calinski-Harabasz Indices:** These were mentioned in the prompt but not implemented for optimal cluster determination in the generated code.

4.  **Cluster Stability Analysis:**
    *   **Bootstrap Resampling:** The implemented `cluster_stability` calculates a very simple `agreement` score (`np.mean(labels == resampled_labels[:len(labels)])`). This doesn't account for permutations of cluster labels, which is a critical issue in comparing clustering results. A more robust method like ARI (Adjusted Rand Index) or a method that attempts to match clusters would be needed. The ground truth's ""consensus clustering"" with co-occurrence matrix and hierarchical clustering provides a much more robust and visual approach to stability.
    *   **Consensus Clustering:** Completely missing in the generated code.

5.  **Cluster Validation (External Criteria & Domain Knowledge):**
    *   The generated code has a placeholder comment. The ground truth explicitly shows how to display cluster profiles and then links them to WHO guidelines, providing a much more robust example of domain knowledge integration.

6.  **Cluster-based Anomaly Detection:**
    *   Only uses Isolation Forest on the *entire* `data_pca`. The ground truth also uses Isolation Forest but applies it *within each cluster*, which is a more refined cluster-based anomaly detection. It also includes ""Distance from Cluster Centroid"" as another method.

7.  **Visualization Dashboard:**
    *   Lacks the comprehensive nature described in the prompt. Missing radar charts for cluster profiles and interactive elements for cluster relationships beyond just the scatter plots.
    *   The 3D scatter plot function `create_3d_scatter` takes `data` as an array directly for x, y, z which is problematic as it assumes `data` has at least 3 columns for that. The ground truth correctly applies 3D dimensionality reduction to get 3 components.

8.  **Code Structure and Readability:**
    *   The ground truth uses more explicit section headers, configuration variables (e.g., `N_COMPONENTS_DR`, `K_RANGE`), making it more modular and easier to understand.
    *   Hardcoding `n_clusters=3` for all clustering algorithms before optimal K determination is found is not ideal, even if it's used for an example. The ground truth determines `optimal_k` first.
    *   Imports are a bit scattered at the top. The ground truth groups related imports.

9.  **Potability Handling:** The generated code automatically includes `Potability` in `data_imputed` and `data_scaled` and uses it for feature engineering (`data_engineered`). While the original `water_potability.csv` might have a 'Potability' column, the task implies clustering primarily on water quality parameters to derive natural groupings, and then validating against potability. The ground truth correctly separates `Potability` as a `y` variable for *later* validation.

Overall, the generated code fulfills the basic requirements but lacks the advanced detail, robustness, and comprehensive analysis that the ground truth provides, especially in feature engineering, stability analysis, optimal cluster determination, and detailed profiling.",gemini-2.0-flash,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import linregress

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Calculate correlation matrix
correlation_matrix = df.corr()['quality'].sort_values(ascending=False)

# Top 3 positively correlated features
top_3_positive = correlation_matrix[1:4]
print(""Top 3 Positively Correlated Features:\n"", top_3_positive)

# Top 3 negatively correlated features
top_3_negative = correlation_mat...",7,"The generated code correctly performs most of the requested tasks.

Strengths:
- Correctly loads the dataset.
- Calculates correlations with 'quality' and identifies top positive and negative features.
- Generates a correlation heatmap.
- Creates scatter plots with regression lines for the most correlated features using `seaborn.regplot`, which is a concise way to achieve this.

Areas for Improvement:
- **Error Handling:** Lacks robust error handling for file loading (`try-except` block). The ground truth code includes this, which is good practice.
- **Clarity in Correlation Calculation:** While `df.corr()['quality']` works, it automatically sorts by 'quality' correlation, which is then used for `top_3_positive` and `top_3_negative`. However, directly calculating `df.corr()` first and then selecting the 'quality' column (as in the ground truth) can sometimes be clearer, especially for understanding the full correlation matrix.
- **Documentation/Comments:** No comments explaining different sections of the code, which the ground truth provides.
- **Minor issue with `top_3_positive` slicing:** `correlation_matrix[1:4]` implicitly assumes 'quality' itself is at index 0 and takes the next three. This is generally okay if 'quality' is indeed the first entry after sorting. However, explicitly dropping the 'quality' self-correlation before taking the top N would be more robust and clearer, as shown in the ground truth (`quality_correlations.drop(TARGET_FEATURE)`).
- **Plot Customization:** The scatter plots are basic. The ground truth adds titles, labels, legends (for R-squared), and grid lines, making them more informative and visually appealing.
- **R-squared Value:** The `seaborn.regplot` directly plots the line, but it doesn't automatically display the R-squared value on the plot or in the console. The ground truth code calculates it using `linregress` and adds it to the plot's legend, which was a specific requirement for the linear regression analysis in the ground truth's detailed output.
- **Assumed `quality` column name:** The code directly uses 'quality' as a string literal. Using a variable (`TARGET_FEATURE` in ground truth) makes the code more configurable and less error-prone if the column name changes.

Overall, the code works and fulfills the core requirements, but it could be more robust, more thoroughly documented, and provide more detailed output (like R-squared on plots) to match the quality of the ground truth.",gemini-2.0-flash,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
from scipy import stats
import numpy as np

# Load the dataset
df = pd.read_csv(""video_games_sales.csv"")

# Data Cleaning
df = df.dropna()

# 1. Nintendo vs. Others (t-test)
nintendo_sales = df[df['Publisher'] == 'Nintendo']['Global_Sales']
other_sales = df[df['Publisher'] != 'Nintendo']['Global_Sales']
t_stat, p_value = stats.ttest_ind(nintendo_sales, other_sales)

alpha = 0.05
print(""1. Nintendo vs. Others:"")
print(f""   T-statistic: {t_stat}"")
print(f""   P-value: ...",6,"The generated code addresses most of the requirements but has several significant issues affecting its correctness and robustness.

**Positives:**
*   It correctly identifies and uses the `pandas` and `scipy.stats` libraries for data manipulation and statistical tests.
*   It attempts to perform all the requested statistical tests (t-test, chi-square, Mann-Whitney U test).
*   It calculates and displays p-values for each test, comparing them to the significance level for interpretation.
*   The chi-square test correctly filters for top 5 genres and platforms.

**Areas for Improvement / Issues:**

1.  **Data Cleaning:**
    *   **Over-aggressive `dropna()`:** The line `df = df.dropna()` drops *any* row with *any* missing value across *all* columns. This is too strong and will likely remove a large portion of the dataset, especially columns not relevant to the analysis (e.g., `User_Score` if 'tbd' is present and not handled). The ground truth code specifically addresses missing values in relevant columns (`Global_Sales`, `Year_of_Release`, `Publisher`, `Genre`, `Platform`) which is much more appropriate.
    *   **`Year_of_Release` handling:** The generated code does not handle the `Year_of_Release` column at all for cleaning (`NaN`s or type conversion to int). This will cause an error in the Mann-Whitney U test because `df['Year']` doesn't exist; it should be `df['Year_of_Release']`. Even if it were `Year_of_Release`, `df.dropna()` alone doesn't guarantee integer type, which could lead to issues.
    *   No error handling for `FileNotFoundError` for the CSV.

2.  **Interpretation:**
    *   For the first t-test (Nintendo vs. Others), the task specifically asks to test ""whether Nintendo games have significantly **higher** global sales."" The generated code uses a standard `ttest_ind` without the `alternative` argument, meaning it tests for a *difference* (two-sided), not specifically ""higher."" The interpretation also states ""significantly different"" rather than ""significantly higher."" This misses a key nuance of the statistical hypothesis.
    *   While the interpretations are correct based on the two-sided test, they lack the explicit Null and Alternative Hypothesis statements provided in the ground truth, which are good practice for clarity in hypothesis testing.

3.  **Robustness:**
    *   **Empty groups in tests:** The generated code does not include checks for whether the subsets for the t-tests or Mann-Whitney U tests are empty or have insufficient data (e.g., `len(group) < 2`). If, after the aggressive `dropna()`, a certain publisher or genre has very few entries, these tests could fail or produce warnings. The ground truth includes these crucial checks.
    *   **Missing `equal_var=False` for t-tests:** The `ttest_ind` defaults to `equal_var=True`. In real-world data science, it's often safer to assume unequal variances (Welch's t-test) unless there's a strong reason not to. The ground truth explicitly uses `equal_var=False`.

4.  **Minor Issues:**
    *   The problem statement specifies `Year_of_Release`, but the generated code refers to `df['Year']` in the Mann-Whitney U test, which is a `KeyError`. This is a critical logical bug.
    *   The formatting of the output for p-values and statistics is less consistent and less readable than the ground truth.

In summary, the generated code provides a functional but flawed attempt. The aggressive data cleaning, incorrect column name, and lack of robustness checks make it prone to errors and less reliable than the ground truth solution, directly impacting its correctness and robustness for a data science benchmark.",gemini-2.0-flash,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

# 1. Load the dataset and handle missing values
df = pd.read_csv(""video_games_sales.csv"")
df.dropna(inplace=True)

# 2. Create binary target variable
df['High_Sales...",7,"The generated code addresses most of the requirements but has several key issues that prevent a higher score.

**Positive Aspects:**
*   **Correct Model and Evaluation:** Correctly uses `RandomForestClassifier` and calculates all required evaluation metrics (accuracy, precision, recall, F1, confusion matrix).
*   **Target Variable Creation:** `High_Sales` is correctly created based on `Global_Sales > 1.0`.
*   **Additional Features:** `Total_Regional_Sales` and `Years_Since_Release` are correctly calculated.
*   **Feature Importance Display:** Attempts to display feature importance.

**Areas for Improvement/Correction:**

1.  **Missing Value Handling:**
    *   The `df.dropna(inplace=True)` approach is too simplistic and potentially destructive. The ground truth code specifically handles missing values in a more nuanced way, ensuring crucial columns (`Global_Sales`, `Year`, `Publisher`, regional sales) are addressed, and dropping rows only when necessary for modeling. A blanket `dropna()` might remove too many rows, even if some missing values could have been imputed or were in non-critical columns.
    *   The prompt implies that the `Year` column should be an integer after handling NaNs. The generated code doesn't explicitly convert `Year` to `int` after `dropna()`, which might lead to issues if `Year` is initially loaded as float due to NaNs.

2.  **Feature Selection for Training:**
    *   The prompt implies using the identified categorical columns, regional sales, `Total_Regional_Sales`, and `Years_Since_Release` as features. The generated code includes `Critic_Score`, `Critic_Count`, `User_Score`, `User_Count` in the feature set (`X`). While these are valid features from the dataset, the prompt did not explicitly ask for them, sticking to features directly tied to the primary requirements. This is a minor deviation but adds unintended complexity.

3.  **Random State for `train_test_split` Stratification:**
    *   The `train_test_split` call is missing `stratify=y`. This is crucial for classification tasks, especially when dealing with imbalanced classes (which 'High\_Sales' likely is), to ensure that the proportion of target classes is maintained in both training and testing sets.

4.  **Feature Importance Display:**
    *   While it attempts to display feature importance using `matplotlib.pyplot.bar`, it requires `plt.show()`, which is good. However, the ground truth code used `pd.Series` to sort and display the values, which is often more readable for initial inspection than a bar plot, especially when the number of features is high, and labels might overlap or be hard to read without significant tuning. The prompt didn't explicitly ask for a plot, just to ""Display feature importance,"" and `print(sorted_importance)` is a perfectly valid and often clearer way to satisfy the requirement if the plot isn't well-formatted.

5.  **Imports:**
    *   Unnecessary import of `matplotlib.pyplot as plt` and `numpy as np` if they are not explicitly used by the prompt's requirements (though `numpy` is often useful, it's not directly called in the displayed code logic). The prompt asks for ""proper imports"" and ""without explanations,"" so only directly used imports should be present.

To improve, the code should make missing value handling more robust and specific, ensure `train_test_split` is stratified, and perhaps reconsider the included features for `X` to strictly adhere to the prompt's implied list, or clearly state why additional features are included. The plotting of feature importances can be kept but should be made clearer, or simply listing them as done in the ground truth is sufficient.",gemini-2.0-flash,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# 1. Handle Missing Values
df.dropna(subset=['Year'], inplace=True)
df['Year'] = df['Year'].astype(int)
imputer = SimpleImputer(strategy='most_frequent')
df['Publisher'] = imputer.fit_tr...",6,"The generated code attempts to address all parts of the task, which is a good start.

**Correctness:**
*   **Missing Value Handling:** The `df.dropna(subset=['Year'], inplace=True)` for 'Year' is incorrect for this dataset. The ground truth imputes 'Year' with the median, which is more robust and prevents significant data loss. The generated code also imputes 'Developer' and 'Rating', which were neither in the ground truth's imputation nor explicitly mentioned in the task requirements, potentially introducing unnecessary steps or confusion.
*   **Feature Engineering:**
    *   `Sales_Ratio_NA_EU`: Correct.
    *   `Publisher_Avg_Sales`: Correct.
    *   `Genre_Market_Share`: The calculation `(genre_total_sales[x] / total_sales) * 100` uses `* 100`, which implies percentage, but the ground truth does not multiply by 100. Either is fine, but consistency with ""percentage of total sales"" might lean towards the generated code's approach. However, using `.map(lambda x: ...)` for this is less efficient and prone to errors than `transform()`.
    *   `Platform_Popularity`: Correct.
    *   `Sales_Momentum`: The `apply` method for `Sales_Momentum` is very inefficient compared to the ground truth's `transform` or direct vectorized subtraction.
*   **PCA:** It correctly performs PCA, scales features, and plots explained variance. However, `features_for_pca.fillna(numerical_features.mean())` before scaling is risky as `numerical_features` might still contain NaNs that `mean()` can't handle properly if all values are NaN. The ground truth handles NaNs comprehensively before PCA. The selection of `n_components` based on 95% variance is good but not explicitly part of the task which just said ""explain variance"".
*   **Polynomial Features:** Correctly applies polynomial features. However, it creates a new DataFrame `poly_df` but doesn't explicitly integrate it back into the main `df`, which the ground truth does with `pd.concat`. This makes the polynomial features unavailable for subsequent steps if the `df` isn't updated.
*   **Interaction Features:** The generated `df['Year_Genre_Interaction'] = df['Year'].astype(str) + '_' + df['Genre']` is *not* a numerical interaction feature. It's a string concatenation. The task asked for interaction features between Year and Genre, implying numerical interaction often achieved through one-hot encoding genre and multiplying by year. This is a significant deviation.
*   **Outlier Handling:** The `remove_outliers_iqr` function filters out rows containing outliers (`df_filtered = df[...]`), which means it *removes* data. The task specifically asked to ""handle outliers"" and the ground truth uses *capping* (`np.where`), which is often preferred to avoid data loss. This is a major difference in approach and directly affects the final dataset size.

**Code Quality:**
*   **Imports:** Proper.
*   **Structure:** Logical flow, but reliance on `.apply` for `Sales_Momentum` and the filtering for outlier handling indicate areas for optimization and consistency.
*   **Variable Naming:** Good.
*   **Comments:** Sparse compared to the ground truth, which provides valuable context for each step.

**Completeness:**
*   **Loading and MV Handling:** Partially complete, but the `dropna` for 'Year' is problematic, and 'Developer'/'Rating' are extra.
*   **Engineered Features:** All features are attempted but `Sales_Momentum` is inefficient, and `Genre_Market_Share` uses `map` instead of `transform`.
*   **Correlation & Heatmap:** Complete and correct.
*   **PCA:** Complete and correct, with an added sensible component selection.
*   **Polynomial Features:** Generated but not integrated into `df`.
*   **Interaction Features:** Incorrectly implemented as a string concatenation instead of a numerical interaction.
*   **Outlier Handling:** Implemented, but via *removal* (data loss) instead of *capping* (retraining).

**Areas for Improvement:**
1.  **Missing Value Handling for 'Year':** Change `dropna` to imputation (e.g., median) to preserve data, similar to the ground truth. Remove 'Developer' and 'Rating' imputation unless explicitly required.
2.  **Efficiency for Feature Engineering:** Use `transform()` for `Sales_Momentum` and `Genre_Market_Share` calculations to improve performance and code readability.
3.  **Polynomial Features Integration:** Integrate `poly_df` into the main `df` using `pd.concat` to make these features available.
4.  **Interaction Features Implementation:** Revisit the ""Year and Genre"" interaction. It should likely involve one-hot encoding 'Genre' and then multiplying the `Year` column by each of the one-hot encoded genre columns to create numerical interaction terms.
5.  **Outlier Handling Strategy:** Change the outlier handling from `removal` (filtering rows) to `capping` (modifying values) to avoid data loss, as is common practice and implied by the term ""handling outliers"" in many data analysis contexts, and as implemented in the ground truth.
6.  **Add Comments:** Enhance comments to explain the purpose of each major step and complex logic, similar to the ground truth.",gemini-2.0-flash,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import numpy as np
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv(""diabetes.csv"")

# 1. Data Cleaning: Replace zero values with NaN and then median imputation
cols_to_replace = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[cols_to_replace] = df[cols_to_replace].replace(0, np.NaN)

for col in cols_to_replace:
    df[col] = df[col].fillna(df[col].median())


# 2. T-test for Glucose levels between diabetic and non-diabetic patients
di...",8,"The generated code correctly performs all the requested statistical tests.

**Strengths:**
*   **Correctness of Statistical Tests:** All five statistical tests (t-test, Mann-Whitney U, chi-square, Pearson correlation, ANOVA) are correctly implemented using `scipy.stats`.
*   **Data Cleaning:** The initial data cleaning step correctly identifies the columns where 0s are biologically implausible and replaces them with NaN, followed by median imputation.
*   **Interpretation of Results:** Each test includes a clear interpretation of the p-value against the significance level (alpha = 0.05).
*   **Code Structure:** The code is well-structured, with clear comments indicating each section.

**Areas for Improvement (Why not a 9 or 10):**
*   **Robustness in Data Loading:** The ground truth includes a `try-except` block for file loading, which is good practice for robust code. The generated code assumes the file exists.
*   **BMI and Age Category Definition:** The ground truth uses `pd.cut` for defining BMI and Age categories, which is generally more robust and flexible than manual `if-elif-else` functions, especially for handling edge cases in bins (e.g., `right=True/False` for inclusive/exclusive boundaries). The `if-elif-else` approach is functional but less idiomatic for Pandas.
    *   For age groups, the generated code uses `30 <= age <= 50`, while the ground truth uses `[0, 29, 50, np.inf]` which means `(0, 29]`, `(29, 50]`, `(50, inf]`. The definitions are slightly different. The prompt's definition was `young: <30, middle: 30-50, older: >50`. The ground truth implements this as up to and including 29 for young, 30 to 50 for middle, and 51 onwards for older. The generated code correctly maps `<30`, `30-50`, `>50`.
    *   For BMI categories, the prompt stated `Underweight: <18.5, Normal: 18.5-24.9, Overweight: 25-29.9, Obese: >=30`. The generated code uses `18.5 <= bmi < 25`, `25 <= bmi < 30`, and `else: return 'Obese'`, which matches the prompt's definition correctly. The `pd.cut` in the ground truth with `right=False` (meaning `[a,b)`) also effectively creates these bounds `[0, 18.5)`, `[18.5, 25)`, `[25, 30)`, `[30, inf)`. Both are correct for the task.
*   **T-test `equal_var` parameter:** The ground truth specifies `equal_var=False` (Welch's t-test), which is generally a safer default when variance equality is not assumed or tested. The generated code defaults to `equal_var=True`, which is the standard Student's t-test. While often acceptable, explicitly considering Welch's is good practice.
*   **Clarity of Interpretation:** While interpretations are present, the ground truth provides a bit more detail (e.g., mentioning *higher* glucose levels where applicable, or positive/negative correlation).
*   **Initial Print Statements:** The ground truth includes print statements for loading and cleaning status, which provides useful feedback during execution.",gemini-2.0-flash,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_a...",7,"The generated code addresses most of the requirements but has some critical shortcomings related to data preprocessing and comprehensive evaluation.

**Strengths:**
- **Loads data and handles missing values:** Correctly replaces zeros with NaNs and then imputes with the median.
- **Feature Engineering:** Correctly creates `Glucose_BMI_ratio`, `Age_group`, and `Insulin_level`.
- **Model Training and Evaluation:** Implements multiple models, performs cross-validation, and calculates standard evaluation metrics (accuracy, precision, recall, F1, ROC-AUC).
- **Hyperparameter Tuning:** Uses `GridSearchCV` for Random Forest, which is good.
- **Visualization:** Generates confusion matrices and ROC curves.
- **Feature Importance:** Extracts and displays feature importance for tree-based models.

**Areas for Improvement/Weaknesses:**
- **Categorical Encoding:** The generated code uses `LabelEncoder` for `Age_group` and `Insulin_level`. This is incorrect for nominal categorical variables, as it introduces an arbitrary ordinal relationship that models might misinterpret. `OneHotEncoder` (or `pd.get_dummies`) should be used, especially before scaling. The ground truth correctly uses `OneHotEncoder` within a `ColumnTransformer`.
- **Preprocessing Pipeline:** The preprocessing (scaling, encoding) is done *before* model training, but without a `ColumnTransformer` or a `Pipeline`, handling numerical and categorical features simultaneously and correctly is more cumbersome and error-prone. The generated code scales `X_train` and `X_test` directly after Label Encoding, which means the scaled features are mixed with numerically encoded categorical features, and `StandardScaler` is applied to columns that also include these encoded categorical features, which is not ideal. A `ColumnTransformer` ensures proper handling for different column types.
- **Feature Names for Importance:** When using `LabelEncoder` and direct scaling, `X_train` and `X_test` lose their column names. This makes the feature importance visualization generic (`'Feature': feature_names`) rather than reflecting the actual preprocessed features, especially after `LabelEncoder` has been applied to categorical features before scaling. The ground truth correctly retains feature names including the one-hot encoded ones.
- **Visualization Scope:** The confusion matrix and ROC curve visualizations are generated *for each model in the initial loop*. While this meets the requirement, it floods the output with plots. The ground truth code consolidates plots (e.g., all ROC curves on one graph) which is better for comparison.
- **Code Structure and Robustness:** The ground truth code includes robust `try-except` for file loading, consistent `random_state` usage, and `warnings.filterwarnings('ignore')` for cleaner output, which are good practices missing in the generated code.
- **Best Model Selection:** The ground truth clearly identifies the ""best model"" for hyperparameter tuning and then evaluates it separately. The generated code simply tunes the Random Forest and re-evaluates it, which is acceptable, but doesn't explicitly compare all models post-tuning.",gemini-2.0-flash,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE
from sklearn.metrics import classification_report

# Load the dataset
data ...",7,"The generated code addresses most of the requirements but has several subtle issues and missed requirements that prevent it from being a higher score.

**Positive Aspects:**
*   **Comprehensive Coverage:** Most of the requested steps (loading, preprocessing, feature engineering, polynomial features, scaling, PCA, SelectKBest, RFE, SMOTE, correlation filtering, feature importance, visualization) are attempted.
*   **Correct Imports:** All necessary libraries are imported.
*   **Structured Code:** The code is reasonably well-structured with comments indicating different sections.
*   **Effective Missing Value Handling:** The approach of replacing zeros with NaNs and then imputing with medians is good.
*   **Polynomial Features:** The application of polynomial features is largely correct in its implementation, though the handling of the resulting dataframe could be cleaner.
*   **SMOTE Implementation:** Correctly applies SMOTE to deal with class imbalance.

**Areas for Improvement/Corrections:**
1.  **Missing Value Imputation:** The ground truth code used `SimpleImputer` which is more robust and standard for imputation. The generated code uses a loop with `fillna(data[col].median())`, which is functional but less efficient and less idiomatic for this task than `SimpleImputer`.
2.  **Feature Engineering Logic (Metabolic_Score, Risk_Factor_Count):**
    *   `Metabolic_Score`: The generated code uses multiplication (`Glucose * BMI * Insulin`), whereas the ground truth uses summation (`Glucose + BMI + Insulin`). The prompt asked for a ""combination"" and did not strictly specify. However, a sum is often more interpretable as a ""score"" in a biological context when variables are already on similar scales or represent additive effects. Multiplication can lead to very large numbers and different interpretations.
    *   `Risk_Factor_Count`: The generated code uses `(data[['BloodPressure', 'Glucose', 'BMI']] > data[['BloodPressure', 'Glucose', 'BMI']].mean()).sum(axis=1)`. This is a valid approach, but the ground truth used more defined medical thresholds (`> 140`, `> 90`, `> 30`, `> 50`), which aligns better with the concept of ""high-risk factors"". The ground truth's definition also included `Age`, which is a significant general risk factor.
3.  **Polynomial Feature Application:**
    *   The generated code uses `data = pd.concat([data, poly_df], axis=1)`. This adds the polynomial features *in addition to* the original features they were derived from (e.g., `Glucose` and `Glucose^2` will both be present, potentially `Glucose * Glucose`). The ground truth specifically addressed this by dropping the original columns first (`df = df.drop(columns=poly_cols)`) before concatenating, which helps avoid redundancy and multicollinearity for the original features if the polynomial features already cover them. While the prompt didn't explicitly demand dropping originals, it's a best practice to consider. Also, the `numerical_cols` for polynomial features in the generated code include `DiabetesPedigreeFunction`, which the ground truth excluded, possibly implying a more focused set of numerical interactions without including more complex features.
4.  **Scaling and Subsequent Steps (Critical Flow Issue):**
    *   The generated code scales the data (`scaled_df_standard` and `scaled_df_minmax`) but then *does not use these scaled versions* for PCA, SelectKBest, RFE, or correlation filtering.
    *   PCA is applied to `scaled_df_standard`, which is correct.
    *   However, **SelectKBest, RFE, and Correlation Filtering** are all applied to the original `X` (or `data.drop('Outcome', axis=1)`), which contains unscaled features and the original features which overlap with polynomial features. This is a significant flaw in the workflow. Scaling is typically done *before* dimensionality reduction (like PCA) and some feature selection methods (like RFE with distance-based models or SelectKBest using certain score functions like F-classif, though F-classif is robust to scaling for its interpretation, not its calculation of individual feature effects). Correlation filtering on unscaled data is okay, but performing it after scaling is also common. The ground truth correctly applied all these steps (`PCA`, `SelectKBest`, `RFE`, `Correlation Filtering`) to the *scaled* data (`X_processed` or `X_filtered_corr`). This leads to incorrect results for these steps.
5.  **Output of PCA:** The generated code prints `explained_variance_ratio` but doesn't explicitly print the cumulative explained variance or the number of components selected, which the ground truth did, providing richer explanation as requested by ""explain variance ratios.""
6.  **Correlation-based Feature Filtering Implementation:**
    *   The generated `correlation` function is correct in its logic.
    *   However, where it's applied, it is applied to `data.drop('Outcome', axis=1)`, meaning *unscaled* data and data that still contains duplicated information from polynomial features. The ground truth applied it to `X_processed` (already scaled by StandardScaler), and it did this *before* PCA and the other feature selection methods. This order is important because feature selection and dimensionality reduction should typically operate on a cleaned and appropriately preprocessed feature set. The generated code also drops the identified correlated features in `filtered_data` but then proceeds to use this `filtered_data` only for SMOTE, while the previous steps (PCA, KBest, RFE) may have used different data. This indicates a messy flow of processed data.
7.  **SMOTE and Feature Importance:**
    *   SMOTE is applied to `X_resampled`, but the columns names are lost during the resampling, so `feature_names = X_train.columns` likely won't work correctly if `X_train` is from the result of `X_resampled` (which has implicit integer columns). The ground truth ensured column names were retained for meaningful feature importance visualization.
    *   The prompt asked to use the SMOTE result for feature importance evaluation. The generated code does this, but the data flow for `X` and `y` before SMOTE could be clearer, ensuring `X` contains the features that have undergone all the transformations and selections decided upon so far. The ground truth explicitly used `X_filtered_corr` (the cleaned, scaled, and correlation-filtered dataset) as the input to SMOTE.

**Conclusion:**
The code demonstrates knowledge of Python libraries and machine learning concepts. However, the execution flow is not consistently applied across all steps, especially concerning feature scaling and the order in which features are chosen/transformed for downstream steps. The critical issue is the inconsistent use of scaled data for feature selection methods and the imperfect integration of polynomial features and correlation filtering into the overall data processing pipeline.",gemini-2.0-flash,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm

# --- 1. Load the data and handle missing values ---
df = pd.read_csv(""world-happiness-report-2021.csv"")
df = df.dropna()


# --- 2. T-test: Western Europe vs. Rest ---
western_europe = df[df[""Regional indicator""] == ""Western Europe""][""Ladder score""]
rest_of_world = df[df[""Regional indicator""] != ""Western Europe""][""Ladder score""]
t_stat, p...",6,"The generated code addresses most of the requirements but has several areas for improvement in terms of robustness, best practices, and the interpretation of results.

**Strengths:**
*   **Correct tests selected:** The code correctly identifies and uses `ttest_ind`, `mannwhitneyu`, `f_oneway` (implicitly via `statsmodels.anova_lm`), `pearsonr`, and `chi2_contingency`.
*   **Basic data loading and NA handling:** It loads the CSV and performs a `dropna()`.
*   **Prints p-values and initial interpretations:** It prints the p-values and a basic ""Reject/Fail to reject"" statement.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling:**
    *   **Ground Truth:** Explicitly identifies critical columns and drops NaNs only from those, reporting the number of rows dropped. This is a more robust approach, as dropping all NaNs (`df.dropna()`) can lead to loss of data if other columns have NaNs but are not relevant for the analysis.
    *   **Generated:** Uses a blanket `df.dropna()`, which might be overly aggressive and removes rows unnecessarily.

2.  **Significance Level (alpha):**
    *   **Ground Truth:** Defines `alpha` (0.05) once at the beginning and uses it consistently throughout, making it easy to change.
    *   **Generated:** Hardcodes `0.05` in every `if` statement, making it less maintainable.

3.  **Interpretation of Results:** This is the most significant weakness.
    *   **Ground Truth:** Provides much more detailed and nuanced interpretations, explaining what ""rejecting the null hypothesis"" means in the context of the specific test (e.g., ""Western European countries have significantly HIGHER happiness scores,"" ""significant POSITIVE correlation""). It also includes practical advice (e.g., ""Post-hoc tests would be needed"" for ANOVA).
    *   **Generated:** The interpretations are very generic (e.g., ""significantly different happiness scores,"" ""Significant correlation exists""). It does not specify the direction of the difference or correlation. For the t-tests, it just says ""significantly different"" rather than ""significantly higher/lower."" For the Mann-Whitney U test, it doesn't specify if top 25% has higher or lower GDP.

4.  **T-test Parameters (equal\_var):**
    *   **Ground Truth:** Uses `equal_var=False` (Welch's t-test) for independent sample t-tests, which is generally recommended unless you have strong evidence of equal variances, as it's more robust.
    *   **Generated:** Uses the default `equal_var=True`, which assumes equal variances. While not strictly wrong, it's less robust.

5.  **ANOVA Implementation:**
    *   **Ground Truth:** Uses `scipy.stats.f_oneway` directly, which is simpler for one-way ANOVA when you have groups as separate arrays.
    *   **Generated:** Uses `statsmodels.formula.api.ols` and `anova_lm`. While functional, this is more verbose and less direct for a simple one-way ANOVA as requested, potentially introducing an unnecessary dependency for this specific task part.

6.  **Chi-square Categories:**
    *   **Ground Truth:** Creates happiness categories based on quantiles (1/3, 2/3), ensuring a more balanced distribution of data into ""Low,"" ""Medium,"" and ""High"" groups.
    *   **Generated:** Uses arbitrary cutoffs (6 and 5) for happiness categories. This is less data-driven and might result in highly imbalanced groups, affecting the chi-square test's power or validity if cell counts become too low.

7.  **File Not Found Handling:**
    *   **Ground Truth:** Includes a `try-except` block for `FileNotFoundError`.
    *   **Generated:** Lacks this basic error handling.

8.  **Output Readability:**
    *   **Ground Truth:** Uses structured print statements with separators and formatted outputs, making the results easy to read and differentiate.
    *   **Generated:** Output is less structured, making it harder to distinguish results for different tests. Also, it doesn't print means/medians for comparison for the t-tests and Mann-Whitney U test, which are crucial for interpreting the direction of the difference.

To achieve a higher score, the generated code should focus on:
*   Implementing more robust missing value handling as seen in the ground truth.
*   Using a consistent `alpha` variable.
*   Providing detailed and context-specific interpretations for each test.
*   Considering `equal_var=False` for t-tests.
*   Using quantile-based categorization for the chi-square test.
*   Improving output formatting and including relevant summary statistics (means/medians) with test results.",gemini-2.0-flash,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, Gradient...",7,"The generated code addresses most of the requirements of the task.

**Strong points:**
- Implements data loading and preprocessing.
- Handles missing values using `SimpleImputer` and scaling using `StandardScaler`.
- Correctly identifies numerical and categorical features for preprocessing within a `Pipeline` and `ColumnTransformer`. This is a more robust way to handle preprocessing than the ground truth in cases where categorical features are actually used in the model.
- Trains and compares multiple regression models.
- Performs hyperparameter tuning using `GridSearchCV` for Gradient Boosting.
- Evaluates models using MAE, MSE, RMSE, and R² scores.
- Creates residual plots for all models.
- Implements cross-validation.
- Makes predictions for hypothetical countries.
- Includes proper imports.

**Areas for improvement:**
1.  **Feature selection for target and features:** The generated code includes 'Country name' and 'Regional indicator' in the initial `df` subset and then drops 'Country name' from `X`. More importantly, it then attempts to use `'Regional indicator'` as a categorical feature for encoding and inclusion in the model, even though the task explicitly stated: ""Create target variable using Ladder score and features using GDP per capita, Social support, Healthy life expectancy, Freedom, Generosity, and Perceptions of corruption"". The ground truth correctly identified and only used these numeric features. Including 'Regional indicator' deviates from the prompt's specific instructions for features, making the task harder to compare directly.
2.  **Hyperparameter tuning for specific model type:** The tuning `param_grid` is defined for `GradientBoostingRegressor`, which is good. However, when initializing `grid_search`, it uses `trained_models['Gradient Boosting']` directly. This `trained_models['Gradient Boosting']` is already a `Pipeline` (as `train_evaluate` returns a pipeline). When wrapping `grid_search` again within another `Pipeline`, it becomes `Pipeline -> Preprocessor -> Pipeline (Preprocessor -> Model) -> GridSearch`, which isn't the intended structure for tuning the `model` step within the initial pipeline. The correct approach would be to tune the `GradientBoostingRegressor` directly and then integrate it into a pipeline, or define the `param_grid` with `model__` prefix when `GridSearchCV` is used on the overall pipeline. The ground truth tunes the model directly then the results are integrated.
3.  **Feature Importance Plotting:** The feature importance plot function is designed to work with a `Pipeline`, but the way `feature_names` are generated assumes that the `OneHotEncoder` is always applied, even if there are no new categorical features from the data (which would be the case if the prompt's specified features were strictly followed). When the model is tuned, `pipeline_tune.named_steps['model']` is the `GridSearchCV` object, not the `GradientBoostingRegressor` directly. To get feature importances, one needs to access `pipeline_tune.named_steps['model'].best_estimator_.named_steps['model'].feature_importances_`. The current implementation `model.named_steps['model'].feature_importances_` might not access the feature importances correctly for all models or the tuned model within the pipeline structure. The current code for feature importance seems to assume `model` is a `Pipeline` but then accesses its `model` step directly. For the tuned model (which is a pipeline with `GridSearchCV` inside), it needs deeper access. The code includes a check `if hasattr(pipeline_tune.named_steps['model'], 'best_estimator_'):` but the subsequent call `plot_feature_importance(pipeline_tune, feature_names, ""Tuned Gradient Boosting"")` passes `pipeline_tune` which is a `Pipeline` and not the `best_estimator_` for plotting.
4.  **Handling of `random_state` in Ridge/Lasso:** The `random_state` parameter was not passed to `Ridge` and `Lasso` in the generated code, while the ground truth included it for reproducibility. While not strictly necessary for these deterministic models, it's good practice for consistency.
5.  **Hypothetical country prediction:** The `new_countries` DataFrame includes `'Regional indicator'`, which again deviates from the prompt's instruction about which features to use. While the model correctly handles it due to the `ColumnTransformer`, adhering strictly to the prompt's feature list (""GDP per capita, Social support, Healthy life expectancy, Freedom, Generosity, and Perceptions of corruption"") would have been better. Also, the example data for 'Generosity' includes a negative value (-0.1), which is not typical for a generosity score between 0 and 1, and could lead to less meaningful predictions.

Overall, the generated code demonstrates a strong understanding of the required ML workflow. The primary points of improvement revolve around strict adherence to the prompt's feature list and minor adjustments in accessing the tuned model's properties for feature importance plots.",gemini-2.0-flash,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
from skl...",7,"The generated code attempts to address all aspects of the prompt and successfully implements many of the requested steps. However, there are several inaccuracies, inefficiencies, and some missed requirements that prevent it from achieving a higher score.

**Positive Aspects:**
*   **Feature Engineering:** Successfully creates `Happiness_Efficiency`, `Social_Wellness_Index`, `Regional_Happiness_Rank`, and `GDP_vs_Regional_Average`.
*   **Polynomial Features:** Correctly applies `PolynomialFeatures`.
*   **Feature Scaling:** Implements `StandardScaler`, `MinMaxScaler`, and `RobustScaler`.
*   **Dimensionality Reduction:** Applies PCA and t-SNE correctly.
*   **Feature Selection:** Uses `SelectKBest` with `f_regression`, RFE (though with `RandomForestRegressor` as estimator instead of `LinearRegression` as in the ground truth for potential comparison), and `mutual_info_regression`.
*   **Interaction Features:** Creates interaction features.
*   **Feature Importance:** Uses Random Forest for feature importance and visualizes it.

**Areas for Improvement / Issues:**

1.  **Preprocessing Issues:**
    *   `df.dropna()` without specifying columns might drop too much data if other columns have NaNs not relevant to the core task. The ground truth handles NaNs in specific columns more judiciously (corruption) before a general drop.
    *   Dropping `'Standard error of ladder score', 'upperwhisker', 'lowerwhisker'` is good, but the prompt's ground truth keeps more columns for potential further analysis or features if needed. The ground truth also uses a `relevant_cols` list to explicitly state and manage the working set.
2.  **`Governance_Score` Calculation:** The generated code calculates `df['Governance_Score'] = df['Freedom to make life choices'] - df['Perceptions of corruption']`. The ground truth calculates `df['Governance_Score'] = df['Freedom to make life choices'] + (1 - df['Perceptions of corruption'])`. The GT's approach correctly inverts the perception of corruption score to make lower corruption (higher value) contribute positively to 'Governance_Score', which aligns with intuition. The generated code's subtraction means higher corruption *reduces* the score, which is also plausible but the `(1-corruption)` way is common. This is a subtle difference but worth noting for precision based on the task description's implied ""low Corruption"".
3.  **Polynomial Features Column Naming and Application:**
    *   The generated code applies polynomial features to `['Ladder score', 'Logged GDP per capita', 'Social support', 'Healthy life expectancy']`. The prompt requested it for ""key happiness factors"" which the ground truth expanded to `['Logged GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']`. The generated code missed some of these.
    *   The generated code uses `poly.get_feature_names_out(['Ladder', 'GDP', 'Support', 'Life'])` for column names. This is arbitrary and doesn't directly map to the original column names, which can lead to confusion later when referencing `X.columns`. The ground truth uses `poly.get_feature_names_out(key_happiness_factors)` which correctly uses the original column names for basis.
4.  **Feature Scaling Application:**
    *   The generated code applies scaling to `df` and concatenates new scaled *versions* of the columns back to the *main* DataFrame (`df = pd.concat([df, scaled_df], axis=1)`). This leads to a massive DataFrame with redundant, scaled versions of the same features for each scaler. The prompt implies applying each scaling method and then potentially using one (e.g., StandardScaler) for subsequent steps. The ground truth creates separate scaled DataFrames (`df_scaled_standard`, `df_scaled_minmax`, `df_scaled_robust`) and explicitly states it will use `df_scaled_standard` for the next steps. This is a much cleaner and more purposeful approach.
    *   The `scaler_cols` selection has hardcoded removals (`'Happiness_Efficiency'`, `'Regional_Happiness_Rank'`) which might not be necessary or correct depending on the full set of initial `numerical_cols`.
5.  **Dimensionality Reduction Input:** PCA and t-SNE are applied using `df[[f'{col}_StandardScaler' for col in scaler_cols]]`. This means it only uses the *scaled versions* of the *original* numerical columns, not the newly engineered or polynomial features that were created earlier and also belong in the feature set `X`. The ground truth correctly defines `X` as all numerical features (excluding target and identifiers) after all engineering and then applies scaling to `X`.
6.  **Feature Selection Input (`X` definition):**
    *   The `X` definition for feature selection `X = df.drop(columns=['Ladder score', 'Country name', 'Regional indicator'] + categorical_cols)` is problematic. Due to the way scaling was done (concatenating scaled columns), `X` now contains *all* the original numerical columns, *plus* three sets of scaled versions of those columns, *plus* polynomial features, *plus* PCA and t-SNE components. This `X` is huge, redundant, and includes features (PCA/t-SNE) that are outputs of dimensionality reduction, not inputs for feature selection applied to the raw/engineered features. The ground truth correctly defines `X` *after* all feature engineering and *before* scaling/dim reduction etc., and then applies scaling **to `X`** to get `X_scaled`, which is the correct input for feature selection.
    *   The `categorical_cols` list derived might still contain numerical `object` or `category` dtypes that are mistakenly included. The ground truth explicitly removes identifier columns and ensures `X` contains only numeric features.
7.  **RFE Estimator:** The prompt/ground truth uses `LinearRegression` as the estimator for RFE. The generated code uses `RandomForestRegressor`. While `RandomForestRegressor` can be used, `LinearRegression` is generally faster and more typical for initial RFE runs, especially if linearity is assumed or for direct comparison with ground truth.
8.  **Correlation Filtering:** The generated code uses `df[numerical_cols].corr()`. This `numerical_cols` does not include the polynomial features or engineered features. It should be applied to the full feature set `X` or `X_scaled` as defined correctly by the ground truth.
9.  **Mutual Information:** The `mutual_info_regression` is applied to `X`, which contains all the previously mentioned issues (scaled versions, PCA/tSNE components, etc.). It should be applied to the correctly defined feature set.
10. **Interaction Features Placement:** The interaction features are created too late in the generated code's flow, *after* the `X` and `y` are defined for feature selection, meaning these new features are not included in the feature selection steps or the RF importance calculation. The ground truth creates all engineered features *before* defining `X` and `y`.
11. **Visualization of PCA/t-SNE:** The prompt explicitly asks for visualization of PCA and t-SNE results. The generated code calculates them but doesn't plot them. The ground truth includes these plots.
12. **Print Statements/Feedback:** The generated code includes many print statements from the prompt for debugging, but it misses some useful ones (e.g., explained variance ratio for PCA, shape of DataFrames after processing).

To improve, the generated code needs to re-evaluate the flow of operations, especially how the main feature set `X` is constructed and passed through the various steps (scaling, dimensionality reduction, feature selection). The redundancy caused by concatenating scaled features to the main `df` is a significant structural issue. Additionally, better adherence to the specific inputs for polynomial features and more comprehensive plotting would elevate the score.",gemini-2.0-flash,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Parse the 'date' column
df['date'] = pd.to_datetime(df['date'])

# Filter for 'location' == 'USA'
usa_df = df[df['location'] == 'USA']

# Set the 'date' column as the index
usa_df = usa_df.set_index('dat...",8,"The generated code correctly performs all the specified steps for time series analysis, including loading and filtering data, resampling, decomposition, ADF testing, differencing if needed, and plotting ACF/PACF. The imports are correct, and the overall structure is logical.

However, there are a few areas for improvement:
1.  **Direct P-value Check for Differencing**: The code directly checks `adfuller(weekly_cases)[1] > 0.05` to decide on differencing. This is correct in principle, but the `adf_test` function already prints the result and it's generally better practice to capture the result of the first `adf_test` call to use it for the conditional check, rather than re-running `adfuller`. This avoids redundant computation and ensures consistency.
2.  **Robustness**: The code lacks error handling for cases like the file not being found or no data for 'USA' being present, which the ground truth code handles.
3.  **Plotting Decomposed Components**: While functional, `plt.subplot(411)`, `plt.subplot(412)`, etc., is a manual way to plot decomposition components. The ground truth leverages `decomposition.plot()`, which is a more concise and often preferred method provided by `statsmodels` for this specific task.
4.  **Clarity in Differencing Check**: If the original series is found to be stationary, the `weekly_cases_diff` variable should conceptually point to `weekly_cases` directly for plotting/further analysis, rather than relying on an `else` block where `weekly_cases_diff = weekly_cases` is assigned. The current implementation is correct, but re-running the ADF test within the `if` condition for the differenced series is a bit less streamlined than storing the result of the *first* ADF test and then making decisions.
5.  **Warnings**: The ground truth code includes warning suppression, which is good practice for cleaner output, though not strictly required for correctness.

Overall, it's a very good attempt that meets most requirements effectively.",gemini-2.0-flash,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
import numpy as np

# Load the dataset
df = pd.read_csv(""winequality-red.csv"")

# Categorize wines based on pH
def categorize_ph(pH):
    if pH < 3.2:
        return ""Low pH""
    elif 3.2 <= pH <= 3.4:
        return ""Medium pH""
    else:
        return ""High pH""

df['pH_group'] = df['pH'].apply(categorize_ph)

# Calculate average quality for each pH group
avg_quality = df.groupby('pH_group')['quality'].mean()
...",6,"The generated code largely addresses the requirements but has several areas for improvement, particularly regarding the correctness of pH categorization, robust handling of confidence intervals, and visualization quality.

**Correctness:**
*   **pH Categorization:** The categorization `3.2 <= pH <= 3.4` for ""Medium pH"" is inclusive on both ends, which is not ideal for continuous bins (e.g., what if pH was exactly 3.2? It could fall into ""Low pH"" or ""Medium pH"" depending on the implementation). The ground truth's `pd.cut` with `right=False` and explicit bins and labels is more robust and aligns better with standard binning practices (`[lower, upper)`). The prompt stated Low pH (< 3.2), Medium pH (3.2 to 3.4), and High pH (> 3.4), which implies exclusive bounds for the categories, not overlaps. The generated code has an overlap at 3.2 and 3.4.
*   **Confidence Interval Calculation:** The `calculate_confidence_interval` function is defined but not actually used by the plotting logic. The `quality_stats` DataFrame correctly calculates `ci_lower` and `ci_upper` using `apply` and `stats.t.ppf`, which is good. However, the `yerr` in `plt.errorbar` expects symmetric error lengths if only one array is provided, or two arrays for asymmetric errors. The way it's passed `[quality_stats['mean'] - quality_stats['ci_lower'], quality_stats['ci_upper'] - quality_stats['mean']]` is correct for asymmetric error bars, so this part is okay, but `plt.bar` (as used in GT) is often preferred for categorical means.
*   **Order of Categories:** The generated code does not explicitly order the `pH_group` categories. `groupby` will order them alphabetically ('High pH', 'Low pH', 'Medium pH'), which leads to an incorrectly ordered plot. The ground truth ensures proper categorical ordering, which is crucial for consistency.
*   **Edge Case for CI:** If a group has only one data point (`size=1`), `size-1` becomes 0, leading to a division by zero error in `stats.t.ppf` (degrees of freedom cannot be 0). The ground truth handles this case, preventing errors.

**Code Quality:**
*   **Readability:** The `categorize_ph` function is clear, but `pd.cut` is generally more idiomatic for binning in pandas.
*   **Configuration:** The absence of clear configuration for file paths, bins, and labels makes the code less flexible and harder to modify.
*   **Error Handling:** No error handling for `FileNotFoundError`.

**Completeness:**
*   **Output:** The average quality is printed and the ANOVA results are printed, which covers points 2 and 3.
*   **Visualization:** A visualization showing average qualities with error bars is produced. However, it uses `plt.errorbar` for points, not bars, which is less standard for categorical means. The prompt implied bar charts, but this isn't strictly forbidden. The lack of proper ordering of categories on the x-axis diminishes its completeness. Confidence intervals are calculated and used.
*   **Details in CI:** While the CI is calculated, the numerical output of these CIs is not explicitly printed as a table, which the ground truth does and is helpful for analysis.

**Areas for Improvement:**
1.  **Refine pH Categorization:** Use `pd.cut` with `right=False` and explicit bins `[0, 3.2, 3.4, np.inf]` and labels to accurately reflect the given conditions and ensure non-overlapping, continuous bins.
2.  **Order Categories:** Convert the 'pH_group' column to a pandas Categorical type with a specified order (`['Low pH', 'Medium pH', 'High pH']`) to ensure consistent grouping and plotting.
3.  **Robust CI Calculation:** Add a check for `df=0` (i.e., `count=1`) in the confidence interval calculation to prevent errors, perhaps by setting CI to just the mean in such cases.
4.  **Enhance Visualization:** Consider using `plt.bar` with `yerr` for the confidence intervals for a more common representation of categorical means. Explicitly set `ylim` if needed.
5.  **Add Configuration:** Define constants for the file path, bins, and labels at the top of the script for better maintainability.
6.  **Error Handling:** Include `try-except` for file loading.",gemini-2.0-flash,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load the dataset
data = pd.read_csv('synthetic...",10,"The generated code successfully addresses all requirements of the task. It correctly loads and parses the data, selects features and target, handles missing values through median imputation for all relevant columns (including the target), performs one-hot encoding for 'continent', and correctly scales numerical features using `StandardScaler`. The use of `ColumnTransformer` within `Pipeline` for preprocessing is a robust and efficient approach that ensures proper scaling and encoding without data leakage and makes the code very clean and maintainable. Both Linear Regression and Random Forest Regressor models are trained and evaluated using MAE, MSE, and R-squared on the test set, and the results are printed as requested. The imports are proper, and the code structure is logical and easy to follow. This is an excellent solution.",gemini-2.0-flash,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Impute missing values in relevant columns with the median
cols_to_impute = ['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']
for col in cols_to_impute:
    df[col].fillna(df[col].median(), inplace=True)


# 1. ANOVA test for reproduction_rate across continen...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being fully correct and robust.

**Correctness:**
*   **ANOVA:** The ANOVA test is performed correctly for the specified continents. However, it assumes `reproduction_rate` is available for all three specified continents. If any continent has no data for `reproduction_rate`, it would raise an error. The ground truth handles this more robustly by filtering `dropna()` and only appending if valid data is present.
*   **Pearson Correlation:** The Pearson correlation is correctly implemented. However, the imputation strategy for `new_cases` and `stringency_index` might still leave NaNs if the median itself was NaN or if the columns were entirely missing. But assuming valid data, it's correct.
*   **T-test (High Death Rate Category):** This is the most problematic part.
    *   The task states: ""use latest entry per country"" for calculating the death rate ratio. The generated code uses `df.groupby('location').tail(1)` which correctly gets the latest entry.
    *   The major issue is in merging the `high_death_rate_country` back to the original `df`. The instruction was to test if `median_age` is significantly different based on a country's high death rate status. This implies comparing the `median_age` of `high_death_rate_country` (which *is* a country-level attribute) against others. By merging back to the original `df`, which contains multiple entries per country (daily data), the `median_age` values for a single country (classified as high death rate or not) are replicated many times, leading to an inflated sample size and potentially incorrect p-values for the t-test. The t-test should be performed on the 'median_age' of the *countries* themselves, derived from the `latest_data` dataframe, not the full `df`.
    *   `df['high_death_rate_country'] = df['high_death_rate_country'].fillna(False)` is problematic. If a `location` was not present in `latest_data` (e.g., due to all its `total_deaths`/`population` being NaN), it would be `False`, which might not be the intended behavior. More importantly, the category is based on the *country*, not individual daily entries.

**Code Quality:**
*   **Imports:** Proper imports are present, including `statsmodels` which was requested, though not ultimately used in the same way as the ground truth.
*   **Structure:** The code is logically structured according to the task requirements.
*   **Missing Value Handling:** While median imputation is done, the robustness of checking for `any()` NaNs and `else` blocks for missing columns (like in GT) is absent. Also, it doesn't handle the case where the median itself might be NaN (e.g., if a column is entirely NaN).
*   **Clarity/Readability:** Variable names are descriptive.
*   **Error Handling:** Lacks robust error handling, e.g., for `pd.read_csv` if the file doesn't exist, or if specific columns are missing, or if groups for statistical tests are empty.

**Completeness:**
*   **Missing Value Handling:** The request implicitly asks for imputation of 'relevant columns' and lists them. It's done.
*   **ANOVA:** Done.
*   **Pearson:** Done.
*   **T-test:** The *logic* for creating the `high_death_rate_country` is problematic, as explained above, making the subsequent T-test incorrect. The T-test itself is called correctly, but on the wrong dataset (expanded from country-level to daily).
*   **Reporting:** Null hypotheses, p-values, and conclusions are printed.
*   **Alpha:** Uses `α=0.05`.

**Specific areas for improvement:**
1.  **T-test for high_death_rate_country:** The t-test for `median_age` should be performed on the `latest_data` dataframe (which represents unique countries) rather than merging the `high_death_rate_country` flag back to the entire `df`. This ensures that each country's `median_age` contributes only once to the analysis, reflecting a country-level comparison.
2.  **Robustness of Imputation:** Add checks for columns' existence before attempting to impute. Consider handling cases where `df[col].median()` might return `NaN`.
3.  **Robustness of Statistical Tests:** Add checks to ensure that there is sufficient data *after* filtering for the groups being compared (e.g., for ANOVA, ensure each continent group is not empty; for t-test, ensure both `high_death_rate` and `other_counties` groups have data). The ground truth code handles this better.
4.  **Clarity on `location` vs `country`:** The task uses `country`, the generated code uses `location` for `groupby`. While often interchangeable in such datasets, explicit consistency or clarification would be good. The ground truth used `country`.",gemini-2.0-flash,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select numerical columns
numerical_cols = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
df_numerical = df[numerical_cols]

# Calculate the correlation matrix
correlation_matrix = df_numerical.corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cm...",9,"The generated code correctly implements all the primary requirements of the task: loading the CSV, selecting the specified numerical columns, calculating the correlation matrix, and visualizing it with a heatmap. It uses the correct libraries (pandas, seaborn, matplotlib.pyplot) and produces the expected output.

Strong points:
- Correctly loads the data.
- Correctly identifies and selects the specified numerical columns.
- Accurately calculates the pairwise correlation matrix.
- Creates a good quality heatmap visualization with annotations and a clear colormap.

Areas for minor improvement (preventing a perfect 10):
- Lack of error handling (e.g., `FileNotFoundError` if the CSV isn't present, `KeyError` if columns are mistyped). The ground truth includes robust error handling, which is good practice for production-ready code.
- No docstrings or explicit function definition as shown in the ground truth for better reusability and readability, although the prompt only asked for ""Python code without explanations,"" which the generated code provides.
- The title of the plot is slightly less descriptive than the ground truth's: ""Pairwise Correlation Matrix of Selected Features"" vs. ""Pairwise Correlation Matrix of Numerical Clash Royale Card Attributes"". This is a minor stylistic point.

Overall, it's a very solid solution that directly addresses the core task.",gemini-2.0-flash,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

def analyze_clash_royale_cards(csv_file=""clash_royale_cards.csv""):
    """"""
    Loads a Clash Royale card dataset, groups by rarity, and calculates average elixir cost, win rate, and usage.

    Args:
      csv_file: The path to the CSV file containing the dataset.  Defaults to ""clash_royale_cards.csv"".

    Returns:
      A pandas DataFrame containing the average elixir cost, win rate, and usage for each card rarity.
    """"""

    try:
        df = pd.read_csv(csv_f...",9,"The generated code correctly addresses all aspects of the task, including loading the CSV, grouping by 'rarity', calculating the required averages, and displaying the results. It demonstrates good code quality by using a function to encapsulate the logic, providing a docstring, and including basic error handling for file not found and other general exceptions. The use of `if __name__ == '__main__'` for execution is also a good practice.

The only minor difference from the ground truth is the introduction of a function and error handling, which while technically not explicitly asked for by ""Provide only the Python code without explanations"" (which usually implies just the core logic), it does not detract from the core task solution and actually improves robustness. However, for a direct comparison against a very minimal ground truth, it adds slight verbosity. If the prompt strictly meant ""only the direct imperative steps"", then the added function and error handling are extra. But from a general good practice standpoint, they are beneficial. Given the primary goal of correctly solving the data manipulation task, it performs excellently.",gemini-2.0-flash,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# 1. Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 2. Create the target column
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)

# 3. Select features
features = ['elixirCost', 'Usage', 'rarity']
X = df[features]
y = df['is_high_win_rate']

# 4. One-hot encode the 'rarity' column
encoder = OneHotEncoder(handle_unknown='ignore', sparse_outpu...",9,"The generated code correctly addresses all requirements of the task: loading the dataset, creating the binary target column, selecting specified features, one-hot encoding the `rarity` column, and splitting the data into training and testing sets with the specified parameters. The imports are correct, and the code structure is logical.

The chosen method for one-hot encoding using `sklearn.preprocessing.OneHotEncoder` is robust and correctly implemented. The `handle_unknown='ignore'` and `sparse_output=False` are good practices. The use of `get_feature_names_out` to name the new columns is also correct.

The only minor area for improvement is handling `FileNotFoundError` for the CSV, which the ground truth code included, although it was not explicitly asked for in the prompt. However, given the prompt's focus on the data manipulation steps, its absence doesn't detract significantly from the solution's correctness or quality.",gemini-2.0-flash,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
iris_data = pd.read_csv('iris.csv')

# Define features (X)
X = iris_data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Define target (y)
y = iris_data['Species']

# Encode the categorical target variable
le = LabelEncoder()
y_encod...",9,"The generated code is highly correct and addresses almost all requirements of the task. It successfully loads the data, defines features and target, encodes the categorical target, splits the data with the specified parameters, trains the KNN classifier, makes predictions, and reports the accuracy.

Strong points:
- All required libraries are imported correctly.
- Data loading and feature/target selection are accurate.
- Label Encoding is correctly applied.
- `train_test_split` uses the correct `test_size` and `random_state`.
- KNN classifier is initialized with `n_neighbors=3` and trained correctly.
- Accuracy calculation and printing are correct.

Area for improvement:
- The ground truth includes `stratify=y_encoded` in `train_test_split`. While not explicitly stated as a requirement in the prompt, it's a good practice for classification tasks with potentially imbalanced classes to ensure representative splits. Its absence doesn't make the code ""incorrect"" based on the exact prompt, but it's a common and beneficial addition for robustness.
- The ground truth includes error handling for `FileNotFoundError`. While the prompt didn't explicitly ask for error handling, it's good practice for robust code.

Overall, excellent solution that fulfills the prompt's requirements very well.",gemini-2.0-flash,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
iris_df = pd.read_csv('iris.csv')

# Filter the data into two groups
versicolor = iris_df[iris_df['Species'] == 'Iris-versicolor']
virginica = iris_df[iris_df['Species'] == 'Iris-virginica']

# Null Hypothesis (H0): There is no difference in the mean PetalLengthCm between Iris-versicolor and Iris-virginica.
# Alternative Hypothesis (H1): There is a difference in the mean PetalLengthCm between Iris-versicolor and Iris-virgi...",8,"The generated code correctly performs all the core tasks: loading the data, filtering, performing the t-test, stating hypotheses, and interpreting the results based on the p-value and alpha.

**Strengths:**
*   Correctly loads the dataset using pandas.
*   Accurately filters the data for the specified species groups.
*   Correctly extracts the `PetalLengthCm` for the t-test.
*   Uses `scipy.stats.ttest_ind` appropriately for an independent two-sample t-test.
*   Clearly states the null and alternative hypotheses.
*   Correctly prints the t-statistic and p-value.
*   Provides a correct interpretation of the p-value against the alpha level of 0.05.

**Areas for Improvement to reach a higher score (9-10):**
*   **Error Handling (File Not Found):** The ground truth includes a `try-except FileNotFoundError` block for `iris.csv`, which is a good practice for robustness. The generated code lacks this.
*   **Clarity in Output:** The ground truth includes more descriptive print statements and separators (`---`, `***`) which enhance readability and user understanding of the different steps being presented in the output. For example, explicitly printing the significance level and the exact p-value and alpha in the conclusion statement, or giving more context before printing the t-statistic and p-value.
*   **Empty Group Check:** The ground truth also includes a check for empty filtered groups, which is a good robustness measure against potential typos in species names or unexpected data issues.",gemini-2.0-flash,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
iris = pd.read_csv('iris.csv')

# Drop the ID column
iris = iris.drop('Id', axis=1)

# Numerical features
numerical_features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

# Create box plots for each numerical feature, grouped by Species
for feature in numerical_features:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='Species', y=feature, data=iris)
    plt.title(f'Bo...",9,"The generated code correctly loads the dataset, identifies the numerical features, and generates the requested box plots. Each plot is grouped by 'Species' and has an appropriate title. The use of `seaborn.boxplot` simplifies the plotting process.

Strengths:
- Correctly loads the `iris.csv` dataset.
- Successfully generates four box plots, one for each numerical feature.
- Correctly groups the box plots by the `Species` column.
- Each plot has a title indicating the feature.
- Uses appropriate libraries (`pandas`, `matplotlib`, `seaborn`).
- The code is clean and readable.

Areas for Improvement (minor):
- The generated code attempts to `drop` the `Id` column. While `iris.csv` often has an 'Id' column that is not needed for this task, the prompt did not specify dropping it. This is a minor deviation and doesn't negatively impact the core task of generating plots for the specified features. If the `Id` column were one of the features needed for plotting, dropping it would be an error, but in this context, it's harmless.
- It lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth included. While not explicitly asked for in the prompt, robust code typically includes such handling.
- The ground truth also included `plt.xlabel`, `plt.ylabel`, `plt.grid`, and `plt.tight_layout` for aesthetic improvements and clarity, which the generated code omits. While optional, these enhance the presentation.",gemini-2.0-flash,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('heart.csv')

# Select features and target variable
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']
target = 'HeartDisease'

X = data[features]
y = data[target]

# Preprocess categorical features using o...",8,"The generated code correctly addresses almost all aspects of the task. It loads the data, selects the specified features and target, performs one-hot encoding on the specified categorical features, splits the data with the correct test size and random state, trains a Logistic Regression model, and reports the accuracy.

Strong points:
- Correctly loads the data.
- Correctly selects features and target.
- Correctly applies one-hot encoding to `Sex` and `ChestPainType`.
- Correctly concatenates the one-hot encoded features with the numerical features.
- Correctly splits the data using `train_test_split` with `test_size=0.25` and `random_state=42`.
- Correctly trains a `LogisticRegression` model.
- Correctly calculates and prints the accuracy.

Areas for minor improvement:
- The ground truth uses `ColumnTransformer` within a `Pipeline`, which is a more robust and standard practice for handling preprocessing and modeling steps, especially when dealing with various data types or complex preprocessing pipelines. The generated code manually handles the concatenation of numerical and encoded features, which works for this simple case but can become cumbersome for more complex scenarios.
- The `LogisticRegression` model might issue a `ConvergenceWarning` because `max_iter` is not specified and the default (100) might not be enough. The ground truth increases `max_iter` to `1000` to prevent this warning and ensure convergence. While not a functional error, it's a good practice for robust models.
- The formatting of the accuracy printout could be slightly improved (e.g., `:.4f` for consistent decimal places).",gemini-2.0-flash,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv(""heart.csv"")

# Null Hypothesis (H0): There is no significant difference in the mean Cholesterol levels between male and female patients.
# Alternative Hypothesis (H1): There is a significant difference in the mean Cholesterol levels between male and female patients.

# Create two groups based on the 'Sex' column
male_cholesterol = df[df['Sex'] == 'M']['Cholesterol']
female_cholesterol = df[df['Sex'] == 'F...",7,"The generated code correctly performs the independent two-sample t-test, separates data by sex, states hypotheses, prints the t-statistic and p-value, and interprets the result based on the given alpha. This covers most of the core requirements.

However, it lacks robustness and some best practices present in the ground truth:
1.  **Error Handling (File Loading):** The generated code assumes `heart.csv` exists and is correctly formatted. The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions during file loading, making it much more robust.
2.  **Data Validation (Columns):** The ground truth checks if the 'Sex' and 'Cholesterol' columns actually exist in the DataFrame, preventing potential `KeyError` if the CSV file's schema is unexpected.
3.  **Handling Missing Data:** The ground truth explicitly uses `.dropna()` on the cholesterol series for both groups. While `ttest_ind` generally handles `NaN` values by dropping them for the calculation, explicit handling is good practice and makes the intent clearer, especially if the subsequent use of these series would require non-NaN values.
4.  **Function Encapsulation:** The ground truth wraps the logic in a function `perform_cholesterol_t_test`. This makes the code reusable (e.g., for different file paths or alpha values), testable, and generally much cleaner and more organized. Directly runnable scripts are fine for simple tasks but less scalable.
5.  **Data Sufficiency Check:** The ground truth checks if there are at least two samples in each group before performing the t-test, which is a critical condition for a two-sample t-test. The generated code does not include this check.
6.  **Formatting of Output:** The ground truth formats the t-statistic and p-value to a specific number of decimal places, which makes the output cleaner and more professional.

Overall, it's a functional solution that meets the core statistical requirements but could be improved significantly in terms of robustness, error handling, and code organization.",gemini-2.0-flash,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Columns to treat for outliers
columns_to_treat = ['volatile acidity', 'residual sugar']

# Original statistics
original_stats = {}
for col in columns_to_treat:
    original_stats[col] = {
        'mean': df[col].mean(),
        'std': df[col].std()
    }

# IQR based outlier detection and capping
outlier_counts = {}
df_winsorized = df.copy() # Create a copy to modify

for col in column...",8,"The generated code correctly performs all the required tasks: identifying outliers using IQR, winsorizing them, and comparing statistics before and after treatment. It adheres to the task's requirements regarding imports and output.

**Strong Points:**
*   Correctly implements the IQR method for outlier detection.
*   Correctly applies winsorization by capping values at the calculated bounds.
*   Successfully calculates and compares mean and standard deviation before and after treatment.
*   Uses `df.copy()` to avoid modifying the original DataFrame, which is good practice.

**Areas for Improvement (to achieve a higher score):**
*   **Error Handling for File Loading:** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is a robust practice for real-world scripts. The generated code assumes the file will always be present.
*   **Output Clarity/Detail:** While it prints the required information, the output for outlier counts and statistical comparison is a bit concise. The ground truth provides more descriptive headers and formatting (e.g., ""Outlier Identification and Reporting"", ""Comparison for column: 'X'"", and formatted numbers with more decimal places), which can improve readability for a user.
*   **Intermediate Output:** The ground truth prints the calculated Q1, Q3, IQR, and bounds, which can be very helpful for debugging or understanding the outlier detection process. The generated code does not show these intermediate values.
*   **Consistency in `np.where` usage:** While correct, the winsorization in the ground truth is often written as a single line using nested `np.where` or a more complex condition which can be more concise, though the current two-step approach is also clear.

Overall, it's a solid solution that meets the core functional requirements. The suggestions are primarily about robustness, output formatting, and user experience, which would elevate it to a perfect score.",gemini-2.0-flash,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('heart.csv')

# Create the grouped bar chart
plt.figure(figsize=(10, 6))
sns.countplot(x='ChestPainType', hue='HeartDisease', data=df)

# Set plot title and labels
plt.title('Heart Disease Distribution by Chest Pain Type')
plt.xlabel('Chest Pain Type')
plt.ylabel('Count')
plt.legend(title='HeartDisease', labels=['No', 'Yes'])

# Show the plot
plt.show()
```",7,"The generated code correctly loads the data and creates a grouped bar chart using seaborn's `countplot` function, which is suitable for this task. It includes a title, labeled axes, and a legend, fulfilling most of the requirements.

However, there are a few areas for improvement:
1.  **Legend Labels:** The `plt.legend(title='HeartDisease', labels=['No', 'Yes'])` line attempts to relabel the legend, but `seaborn.countplot` generates its own legend based on the `hue` variable's values. For the labels to correctly appear as 'No Heart Disease' and 'Heart Disease' (or 'No' and 'Yes'), it's better practice to map the numerical `HeartDisease` values (0 and 1) to descriptive strings *before* plotting and then use that new column as the `hue`. As is, `countplot` will likely show '0' and '1' in the legend.
2.  **Robustness:** The code lacks error handling for `FileNotFoundError`, which is important for real-world applications.
3.  **Encapsulation:** While not strictly required by the prompt, wrapping the logic in a function (as in the ground truth) is better practice for reusability and modularity, and allows for passing the `file_path` as an argument.
4.  **`bbox_to_anchor` for legend:** For a potentially long legend title or labels, positioning the legend outside the plot area with `bbox_to_anchor` can prevent overlap, as shown in the ground truth.
5.  **No `main` block**: For standalone scripts, using `if __name__ == '__main__'` is good practice.",gemini-2.0-flash,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv(""spotify_2023.csv"", encoding='utf-8')
except FileNotFoundError:
    print(""Error: The file 'spotify_2023.csv' was not found in the current directory."")
    exit()
except Exception as e:
    print(f""Error loading the CSV file: {e}"")
    exit()

# Clean the 'in_shazam_charts' column
df['in_shazam_charts'] = df['in_shazam_charts'].str.replace(',', '', regex=False)
df['in_shazam_charts'] = pd.to_numeric(df['in_shazam_charts'...",9,"The generated code successfully completes almost all aspects of the task.

**Strong points:**
- Correctly loads the CSV and includes basic error handling for `FileNotFoundError`.
- Accurately cleans the `in_shazam_charts` column by removing commas and converting to numeric, coercing errors to NaN.
- Correctly reports the count of NaNs in `in_shazam_charts`.
- Correctly groups the data by `key` and `mode`.
- Accurately calculates the average `danceability_%` and `valence_%` for each group.
- Proper use of pandas for data manipulation.

**Areas for minor improvement:**
- The `encoding='utf-8'` in `pd.read_csv` is not strictly necessary for this dataset and can sometimes cause `UnicodeDecodeError` if the file truly isn't UTF-8, but for most standard CSVs, it's fine. The ground truth avoided this by just `pd.read_csv('spotify_2023.csv')`.
- The print statement for NaNs is ""NaN values in 'in_shazam_charts' after conversion"", which is slightly less descriptive than the ground truth's ""Count of NaNs in 'in_shazam_charts' after cleaning and coercion"", but functionally equivalent.
- The `print(grouped_data)` at the end displays the results, but the task asked to ""display"" them which is naturally handled by printing the dataframe. However, the ground truth added a descriptive header like `print(""\nAverage 'danceability_%' and 'valence_%' by 'key' and 'mode':"")` for better clarity in output, which is a minor polish.
- The ground truth also explicitly casts `in_shazam_charts` to `str` before `replace` for robustness, though `df['in_shazam_charts'].str.replace(...)` often works directly if the column is already object/string type. This is a very minor point.

Overall, a very robust and correct solution that fulfills nearly all requirements.",gemini-2.0-flash,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Load the dataset
df = pd.read_csv(""spotify_2023.csv"")

# Convert 'streams' to numeric, coercing errors to NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# Select features and target
X = df[['in_spotify_playlists', 'energy_%']]
y = df['streams']

# Drop rows with NaN in X or y
df_cleaned...",8,"The generated code correctly addresses most of the requirements. It successfully loads the dataset, converts the `streams` column to numeric, selects the correct features and target, splits the data, trains a Linear Regression model, and evaluates it using R2 and MSE.

However, there is a subtle issue with the NaN handling.
The instruction states: ""drop rows with NaN in `streams` or predictor columns for this task"".
The generated code first defines `X` and `y` using the original `df` (which might still contain NaNs in `in_spotify_playlists` or `energy_%` that were not coerced by `pd.to_numeric` on `streams`), then `pd.concat([X, y], axis=1).dropna()` is used. While this achieves the goal of dropping NaNs from X and y, concatenating them back together and then re-assigning X and y again is less direct than applying `dropna` directly to the `df` or a relevant subset of it *after* the `streams` coercion and *before* defining X and y in the first place, as done in the ground truth code. The ground truth's approach using `df.dropna(subset=model_columns)` is more robust and standard for handling NaNs across selected columns in the original DataFrame.

Additionally, the generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is a good practice in real-world scenarios and was included in the ground truth. The output format for R2 and MSE could also be slightly more descriptive or formatted (e.g., specific decimal places).

Overall, it's a very good attempt that produces functionally correct results for the core task, but lacks some robustness and optimal structure compared to the ground truth.",gemini-2.0-flash,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""spotify_2023.csv"")

# Combine released_year, released_month, and released_day into a single datetime column
def create_date(row):
    try:
        return pd.to_datetime(f""{row['released_year']}-{row['released_month']}-{row['released_day']}"", format='%Y-%m-%d', errors='raise')
    except ValueError:
        return pd.NaT

df['release_date'] = df.apply(create_date, axis=1)

# Calculate days_since_release
reference_date = pd.to_dat...",8,"The generated code correctly performs all the required tasks: loading the dataset, combining date components into a datetime column (handling errors with `NaT`), calculating `days_since_release`, and displaying the specified information for the first 5 tracks. The use of `df.apply` with a custom function for date conversion, while functional and correct for error handling, is generally less efficient than vectorized operations for larger datasets. The ground truth's approach using string concatenation and `pd.to_datetime` with `errors='coerce'` is typically more performant. However, for correctness, the generated code fulfills the requirements. The code also uses `astype('Int64')` for `days_since_release`, correctly handling potential `NaN` values resulting from `NaT` in `release_date`.",gemini-2.0-flash,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

def clean_and_aggregate_youtube_data(csv_file='youtube_trendings.csv'):
    """"""
    Loads, cleans, and aggregates YouTube trending data.

    Args:
        csv_file (str): Path to the CSV file.

    Returns:
        pandas.DataFrame: DataFrame containing the sum of video views and likes per category.
    """"""

    try:
        df = pd.read_csv(csv_file)
    except FileNotFoundError:
        print(f""Error: File not found at path {csv_file}"")
      ...",9,"The generated code correctly implements all specified requirements. It loads the CSV, cleans ""Video views"" and ""Likes"" columns (removing commas, converting to numeric, handling errors with NaNs, and filling NaNs with 0). It correctly fills missing ""Category"" values with ""Unknown"" and then groups by ""Category"" to calculate the sum of cleaned views and likes. The code is well-structured within a function and includes proper error handling for `FileNotFoundError`. The use of `agg` for grouping makes the code clean and readable.

Minor improvements could include:
1. The ground truth code added checks for column existence (`if 'Video views' in df.columns:`) before attempting operations. While this might be overkill for a specific dataset assumed to have these columns, it makes the code more robust against malformed or unexpected input files in a real-world scenario. The generated code currently assumes the columns always exist, which would raise `KeyError` if they don't.
2. The generated code uses `regex=True` for `str.replace(',', '', regex=True)` which is the default for `str.replace` in recent pandas versions when the first argument is a string (not a regex pattern). The ground truth code explicitly uses `regex=False`, which is slightly safer practice when replacing a literal substring (where you explicitly don't want regex behavior). However, for a simple comma, it doesn't make a practical difference.
Given the prompt's specific instructions and assumed data, these are minor points and do not detract from the code's successful execution of the task.",gemini-2.0-flash,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats
import numpy as np

# Load the dataset
df = pd.read_csv(""youtube_trendings.csv"")

# Clean the 'Likes' column
def clean_likes(likes_str):
    try:
        return float(likes_str.replace(',', ''))
    except:
        return np.nan

df['Likes'] = df['Likes'].apply(clean_likes)
df['Likes'] = df['Likes'].fillna(0)

# Fill missing 'Category' values
df['Category'] = df['Category'].fillna('Unknown')

# Create two groups
music_likes = df[df['Category'...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
- Loads the dataset correctly.
- Implements the cleaning of the `Likes` column by removing commas, converting to numeric, and filling NaNs with 0. The custom `clean_likes` function handles this well.
- Fills missing `Category` values with 'Unknown'.
- Correctly separates 'Music' and 'Sports' categories for the t-test.
- Performs the independent two-sample t-test with `equal_var=False` as is good practice and often needed.
- Clearly states the null and alternative hypotheses.
- Prints the t-statistic and p-value.
- Interprets the p-value against the significance level (alpha = 0.05) and states the conclusion.

**Areas for minor improvement:**
- The `clean_likes` function's `try-except` block catches a broad `except` without specifying the exception (e.g., `ValueError`). While it works for this case, it's generally better practice to catch specific exceptions.
- The prompt implicitly asked for a new cleaned column (as the ground truth shows `Likes_cleaned`). The generated code overwrites the original `Likes` column. While this is acceptable and might be considered more memory-efficient, creating a separate column can sometimes be beneficial for debugging or retaining original data. However, for this task, overwriting is fine.
- There's no error handling for `FileNotFoundError` during dataset loading, which is a good practice for robustness.
- The output doesn't match the level of detail and formatting of the ground truth (e.g., no explicit print statements for each step completed, no formatting of t-statistic and p-value to specific decimal places). While not explicitly required for a perfect score, it adds to readability and user experience.

Overall, it's a very solid solution that successfully completes the core requirements. The minor points are mostly about robustness, best practices, and output formatting rather than functional correctness for the stated problem.",gemini-2.0-flash,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
try:
    df = pd.read_csv(""youtube_trendings.csv"")
except FileNotFoundError:
    print(""Error: youtube_trendings.csv not found. Make sure the file is in the current directory."")
    exit()


# Function to safely convert to numeric and handle errors
def safe_numeric_conversion(series):
    series = series.astype(str).str.replace(',', '')
    try:
        series = pd.to_numeric(series, errors='raise')  # errors='raise' to catch c...",9,"The generated code successfully addresses all the requirements of the task. It correctly loads the dataset, cleans the `Likes` and `Dislikes` columns by removing commas, converting to numeric, and handling NaNs by filling them with 0. It then correctly calculates `TotalEngagement` and `LikeDislikeRatio` according to the specifications and displays the required columns.

The use of a `safe_numeric_conversion` function is a good practice for encapsulating the cleaning logic, although the `errors='raise'` followed by `errors='coerce'` in the `try-except` block for `pd.to_numeric` is redundant. `errors='coerce'` directly handles non-numeric values by converting them to NaN, which is what the task requires and what the ground truth does more concisely. This doesn't break functionality but adds unnecessary complexity.

Overall, it's a very good solution, just a minor point on the `safe_numeric_conversion` function's internal logic.",gemini-2.0-flash,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""mobiles_2025.csv"")

# Clean 'Launched Price (USA)'
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', '')
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'], errors='coerce')

# Clean 'Screen Size'
df['Screen Size'] = df['Screen Size'].str.replace(' inches', '')
df['Screen Size'] = pd.to_numeric(df['Screen Size'], errors='coerce')

# Group by 'Company Name' and 'La...",9,"The generated code correctly addresses all requirements of the task. It loads the CSV, cleans both the 'Launched Price (USA)' and 'Screen Size' columns by removing specified strings and converting to numeric, handles conversion errors by coercing to NaN, groups the data by 'Company Name' and 'Launched Year', and correctly calculates and displays the average of the cleaned columns. The code is concise and clear.

Areas for minor improvement (not critical for score):
1.  **Error Handling for File Loading:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions during CSV loading, which is good practice for robust code. The generated code assumes the file will always be present.
2.  **Explicit New Columns for Cleaned Data:** While the generated code directly overwrites the original columns, which is acceptable given the task, creating new columns for cleaned data (as done in the ground truth with 'Cleaned Price (USD)' and 'Cleaned Screen Size (inches)') can sometimes be beneficial for debugging or if the original raw data needs to be preserved for other operations. However, for this specific task, overwriting is efficient and doesn't hinder correctness.
3.  **Displaying Grouped Data:** The generated code directly prints the `grouped_data` which is a pandas DataFrame. The output is visually clear. The ground truth used `reset_index()` which can be useful if further operations are to be performed on the grouped data in a flat table structure, but `print(grouped_data)` is sufficient for displaying the aggregated data as requested.

Overall, a very good solution that perfectly fulfills the task requirements.",gemini-2.0-flash,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

def clean_camera_mp(camera_spec):
    """"""
    Extracts the first sequence of digits from a camera specification string
    and returns it as an integer. If no digits are found, returns 0.
    """"""
    if isinstance(camera_spec, str):
        match = re.search(r'(\d+)', camera_spec)
        if match:
            return int(match.group(1))
    return 0

# Load the dataset
df = pd.read_csv(""mobiles_2025.csv"")

# Clean Front Camera and Back Camera columns
df['...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, defines a function to clean camera specifications (extracting the first sequence of digits or returning 0), applies this function to create new cleaned columns, calculates the total camera MP, and displays the specified columns for the first 10 rows. The imports are correct and the code structure is clean and logical.

One minor area for improvement, though not critical for correctness given the context, is the handling of non-string inputs in the `clean_camera_mp` function. The generated code only proceeds if `isinstance(camera_spec, str)`. While this will correctly handle `NaN` or non-string inputs by returning 0, the ground truth explicitly uses `pd.isna` which is a more robust way to check for `NaN` values from a DataFrame column and ensures any non-string and non-NaN inputs are also handled gracefully by explicitly casting to string before regex. However, for typical CSV data, `NaN` and strings are the most common scenarios.

Additionally, the ground truth includes error handling for `FileNotFoundError` and adds print statements for progress, which are good practices for robust scripts, but not explicitly requested as part of the core task logic. The generated code correctly focuses solely on the core task logic.",gemini-2.0-flash,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# 1. Load the dataset
df = pd.read_csv(""mobiles_2025.csv"")

# 2. Clean the data
def clean_column(df, col, to_remove, data_type):
    df[col] = df[col].astype(str).str.replace(to_remove, '', regex=True)
    df[col] = df[col].str.replace(',', '', regex=False)

    try:
        df[col] = pd.to_numeric(df[col],...",7,"The generated code correctly addresses most of the task requirements, including loading data, splitting, training a model, and reporting metrics. The structure is clear, and necessary libraries are imported.

However, there's a significant issue in the `clean_column` function: it tries to remove multiple patterns (like 'USD ' and ',') by calling `replace` twice, but the `to_remove` parameter only handles one pattern. Specifically, for 'Launched Price (USA)', it correctly removes 'USD ' via `to_remove`, but the subsequent `df[col].str.replace(',', '', regex=False)` is a hardcoded call within the function, which means this comma removal will be applied to *all* columns being cleaned, including 'RAM', 'Battery Capacity', and 'Screen Size', even though it's only relevant for 'Launched Price (USA)'. This makes the cleaning logic for most columns incorrect.

Additionally, the `try-except ValueError` block for `pd.to_numeric` is redundant as `errors='coerce'` already handles conversion errors by setting them to NaN, so the `try-except` block adds no functional value and just duplicates the `pd.to_numeric` call. The original prompt also specified `regex=False` for string replacements where appropriate, which the generated code generally uses, but the parameter `to_remove` in `clean_column` uses `regex=True` by default, which can cause subtle issues if `to_remove` contains special regex characters. The ground truth correctly uses `regex=False` when the pattern is a literal string.

The code also doesn't handle the case where a column might be missing from the original DataFrame, which the ground truth does by checking `if col in df.columns`. While not explicitly required for this specific dataset, robust code would include such checks.",gemini-2.0-flash,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load the dataset
df = pd.read_csv(""titanic.csv"")

# Data cleaning: Handle missing values
df['Age'].fillna(df['Age'].median(), inplace=True)


# 1. Chi-Square Test (Pclass vs. Survival)
contingency_table = pd.crosstab(df['Pclass'], df['Survived'])
chi2, p_chi2, dof_chi2, expected = stats.chi2_contingency(contingency_table)

# Chi-square Effect Size (Cramer's V)
def cramers_v(confusion_matrix):
    chi2 = stats.chi2_contin...",7,"The generated code correctly implements the three required statistical tests (Chi-square, T-test, ANOVA) and calculates their respective effect sizes. It also includes proper imports and clear conclusions based on the p-values.

However, there are several areas for improvement:

**Missing/Incorrect/Improvements:**

1.  **Handling Missing Values (T-test and ANOVA):** The generated code fills missing 'Age' values with the median at the beginning. While this prevents errors, it's generally better practice for the T-test and ANOVA to `dropna()` on the specific columns relevant to the test, as imputing can bias results, especially for inferential statistics. The Ground Truth code demonstrates this by cleaning `df_age_cleaned` and `df_fare_cleaned` specifically for the test. The current approach of filling all NaNs at the start is acceptable for general data cleaning but less ideal for strict statistical testing where precision is required.

2.  **Cramer's V Calculation:** The `cramers_v` function provided by the generated code implements a more complex, bias-corrected version of Cramer's V, which is good. However, there's a slight issue: it expects a ""confusion matrix"" argument, but `chi2_contingency` already returns the actual `contingency_table`. The function should ideally take `contingency_table` directly or be simplified if it's meant for `chi2_contingency` output. The `min_dim` calculation in the ground truth is simpler and more standard for the basic Cramer's V often taught. The `phi2corr` and `rcorr`/`kcorr` in the generated code's Cramer's V are for a bias-corrected version, which is more advanced but could be confusing if not explicitly stated as such. The provided `confusion_matrix` argument to the function is incorrect; it should be `contingency_table`. It implicitly works because `stats.chi2_contingency(confusion_matrix)[0]` re-calculates chi-squared for the function, which is redundant.

3.  **Cohen's D Calculation (T-test):** The generated code calculates `cohens_d` as `(survivor_ages.mean() - non_survivor_ages.mean()) / survivor_ages.std()`. This is only correct if the standard deviation of the survivor ages is used as the pooled standard deviation. For an independent samples t-test, Cohen's d typically uses the *pooled* standard deviation of both groups, which is a more robust measure and accounts for the sample sizes and variances of both groups. The Ground Truth calculates this correctly (`pooled_std`).

4.  **Eta-squared Calculation (ANOVA):** The `eta_squared` function in the generated code has conceptual errors and the `ss_total` calculation is incorrect.
    *   `sst` inside the function refers to Sum of Squares Treatment/Between, not Total.
    *   `ss_total = sum((df['Fare'] - grand_mean_repeated)**2)` attempts to use `df['Fare']` directly but `grand_mean_repeated` was created with `total_sample_size` from `len(df['Fare'])`. This would work numerically but `df['Fare']` should probably be `all_fares` or at least `df_fare_cleaned['Fare']` if pre-cleaned.
    *   The `eta_squared` function definition incorrectly assumes `row_means` and `sample_sizes` are NumPy arrays allowing element-wise multiplication in `sst`. The way it's called outside the function (using `df.groupby('Pclass')['Fare'].mean().values` and `.size().values`) is correct for arrays, but the formula `sst / ss_total` is correct for eta-squared. However, the calculation of `ss_total` using `grand_mean_repeated` and `df['Fare']` is inconsistent.
    *   The Ground Truth's calculation of `ss_total` and `ss_between` is derived more directly from the definition of ANOVA terms and thus is more robust and standard.

5.  **Robustness and Assumptions:** The generated code does not perform Levene's test for equality of variances for the T-test or ANOVA, which is a crucial assumption for these parametric tests. The Ground Truth code includes Levene's test and adjusts the `equal_var` parameter for `ttest_ind` accordingly, making the T-test more statistically sound (Welch's t-test if variances are unequal). For ANOVA, a warning is issued if homogeneity of variance is violated. This demonstrates better statistical rigor.

6.  **Clarity of Output:** The Ground Truth provides more descriptive output, including detailed hypotheses, current values for specific tests (e.g., mean ages), and notes about post-hoc tests for ANOVA. The generated code's output is more concise but lacks some explanatory context.

7.  **File Not Found Handling:** The Ground Truth includes `try-except` for `pd.read_csv`, which is good practice for robust code outside of benchmark settings. While not strictly required by the prompt's core task, it adds to the quality.

Overall, the generated code satisfies the primary requirements but falls short in statistical robustness, accuracy of effect size calculations, and some best practices for data handling in statistical analysis.",gemini-2.0-flash,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv(""imdb_top_250.csv"")

# Group by Year and calculate average Rating and Runtime
yearly_stats = df.groupby(""Year"").agg({""Rating"": ""mean"", ""Runtime"": ""mean""})

# Create the line plot
plt.figure(figsize=(12, 6))
plt.plot(yearly_stats.index, yearly_stats[""Rating""], marker='o')
plt.title(""Average IMDB Rating Over the Years"")
plt.xlabel(""Year"")
plt.ylabel(""Average Rating"")
plt.grid(True)
plt.show()
```",6,"The generated code correctly loads the data, groups by year, calculates average rating and runtime, and plots the average rating. However, it lacks robustness in handling potential data inconsistencies (e.g., non-numeric 'Year' or 'Runtime' formats), which the ground truth code addresses. The generated code assumes clean `Year` and `Runtime` columns. Also, it doesn't include the function encapsulation and error handling present in the ground truth, which makes the ground truth more production-ready. Specific improvements would be
1. Adding error handling for file not found and missing columns.
2. Including data cleaning for 'Year' and 'Runtime' columns to handle cases where they might not be perfectly numeric (e.g., 'Year' as '(2000)' or 'Runtime' as '120 min'). The ground truth handles this effectively.
3. Encapsulating the logic within a function for better organization and reusability.",gemini-2.0-flash,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

# 1. Load the dataset
df = pd.read_csv(""imdb_top_250.csv"")

# 2. Create two groups based on the 'Year' column
group_before_1990 = df[df['Year'] < 1990]['Rating']
group_1990_and_later = df[df['Year'] >= 1990]['Rating']

# Null Hypothesis (H0): The mean rating of movies released before 1990 is equal to the mean rating of movies released in or after 1990.
# Alternative Hypothesis (H1): The mean rating of movies released before 1990 is different...",8,"The generated code correctly performs all the core steps required by the task: loading the dataset, creating the two groups, performing the t-test, stating the hypotheses, printing the results, and interpreting them based on the significance level.

**Strong points:**
*   Correctly uses `pandas` to load the CSV.
*   Correctly creates the two groups for `Year` using boolean indexing.
*   Correctly extracts the `Rating` series for each group.
*   Correctly uses `scipy.stats.ttest_ind` to perform the independent t-test.
*   Clearly states the null and alternative hypotheses.
*   Prints the t-statistic and p-value.
*   Correctly interprets the p-value against the alpha level of 0.05.

**Areas for Improvement (Why not a 9 or 10):**
1.  **Robustness in Data Loading/Handling:** The ground truth includes `try-except` for `FileNotFoundError` and explicit `pd.to_numeric` conversions with `errors='coerce'` and `dropna`. This makes the code more robust to common real-world issues like missing files or non-numeric data in 'Year'/'Rating' columns, which are often implicitly assumed to be clean. The generated code assumes the CSV always exists and that 'Year' and 'Rating' are already numeric.
2.  **Explicit `equal_var` in `ttest_ind`:** While `ttest_ind` defaults to `True` for `equal_var`, explicitly setting it (as in the ground truth) shows awareness of the assumption being made (Levene's test or similar pre-test for variance equality would be even better, but out of scope for this prompt).
3.  **Clarity of Output:** The ground truth adds more descriptive print statements (e.g., ""Dataset loaded successfully."", ""--- Hypotheses ---"", ""--- T-Test Results ---"", ""--- Interpretation ---""), which improves readability and user experience.
4.  **Edge Case Handling (Group Size):** The ground truth checks if each group has at least 2 samples, which is a critical condition for a t-test. The generated code assumes sufficient data in both groups.

Overall, the generated code is very good and functionally complete for the core requirements but lacks some of the robustness and defensive programming practices present in the expert-level ground truth.",gemini-2.0-flash,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv(""imdb_top_250.csv"")

# Create the 'Decade' column
def categorize_year(year):
    decade = (year // 10) * 10
    return f""{decade}s""

df['Decade'] = df['Year'].apply(categorize_year)

# Display the 'Title', 'Year', and 'Decade' for the first 10 movies
print(df[['Title', 'Year', 'Decade']].head(10))
```",9,"The generated code correctly loads the CSV, creates the `Decade` column using the specifed logic, and displays the requested columns for the first 10 movies. It fulfills all the stated requirements.

**Strong points:**
*   Correctly uses pandas for data loading and manipulation.
*   The logic for creating the `Decade` column (e.g., 1990-1999 -> '1990s') is accurate and efficiently implemented using floor division and f-strings.
*   Successfully displays the `Title`, `Year`, and `Decade` for the first 10 entries as requested.

**Minor areas for improvement (not critical for this task, but good practice):**
*   **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and `KeyError`, which robustifies the code against common issues. The generated code lacks explicit error handling for file not found or missing columns.
*   **Function Encapsulation:** The ground truth wraps the logic in functions (`perform_feature_engineering`, `main`), which improves modularity, reusability, and testability. For a simple script like this, it's not strictly necessary, but it's good practice for larger projects.
*   **Data Type Safety:** The ground truth explicitly converts `Year` to numeric and then integer, handling potential non-numeric entries with `errors='coerce'`. While the problem implicitly assumes clean data, robust code would account for this. The generated code assumes `Year` is already a numeric type that can be used directly for mathematical operations.",gemini-2.0-flash,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
penguins = pd.read_csv(""penguins.csv"")

# Numerical columns to impute with mean
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']

# Impute missing values in numerical columns with mean
for col in numerical_cols:
    penguins[col].fillna(penguins[col].mean(), inplace=True)

# Impute missing values in 'sex' column with mode
penguins['sex'].fillna(penguins['sex'].mode()[0], inplace=True)...",10,"The generated code correctly addresses all requirements of the task. It properly loads the data, imputes missing values in the specified numerical columns with their respective means, and imputes missing values in the `sex` column with its mode. It then correctly groups the data by `species` and imputed `sex` and calculates the average `body_mass_g` for each group, displaying the result. The code is concise and clear, uses proper imports (`pandas`), and effectively utilizes pandas functionalities for data manipulation. The absence of error handling is acceptable given the prompt's focus on the core data processing logic and simplicity for a benchmark.",gemini-2.0-flash,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# 1. Load the dataset
penguins = pd.read_csv(""penguins.csv"")

# 2. Filter data for Adelie and Gentoo penguins
penguins_filtered = penguins[penguins['species'].isin(['Adelie', 'Gentoo'])]

# 3. Remove rows with missing values in 'flipper_length_mm' or 'species'
penguins_filtered = penguins_filtered.dropna(subset=['flipper_length_mm', 'species'])

# 4. Perform independent two-sample t-test
adelie = penguins_filtered[penguins_filtered['species'...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
*   **Correct Logic:** It accurately loads the data, filters by species, handles NA values, performs the t-test, and interprets the results based on the p-value and alpha.
*   **Clear Hypotheses:** The null and alternative hypotheses are clearly stated.
*   **Proper Imports:** Necessary libraries (`pandas`, `scipy.stats`) are imported.
*   **Concise:** The code is straightforward and to the point.

**Areas for improvement (minor):**
*   **Error Handling for File Loading:** The ground truth includes `try-except` blocks for `FileNotFoundError` and other general exceptions when loading the CSV, which is good practice for robust code. The generated code assumes the file will always be present.
*   **`equal_var` parameter in `ttest_ind`:** The ground truth code discusses the `equal_var` parameter for `ttest_ind` and uses `equal_var=False` for robustness (Welch's t-test). The generated code uses the default `equal_var=True`, which assumes equal variances. While this might be an acceptable default for a simple prompt, explicitly mentioning or considering Welch's t-test (`equal_var=False`) can be a good detail depending on the context of the problem (e.g., if variances are typically unequal in similar real-world scenarios). For this problem, it's not explicitly stated, so `equal_var=True` is not wrong, but `equal_var=False` is often safer without prior knowledge.
*   **Edge Cases for Empty Groups:** The ground truth includes checks for empty or insufficient data (`len < 2`) in the species groups after filtering and NA removal, which prevents errors if the data ends up being too sparse. The generated code does not include these checks, which could lead to errors if `adelie` or `gentoo` series are empty.

Overall, the code is highly functional and correctly implements the core logic required by the task. The missing error handling and edge case checks are good practices but don't critical break the problem's core solution assuming a well-formed input file.",gemini-2.0-flash,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load the dataset
penguins = pd.read_csv(""penguins.csv"")

# Select features and target
X = penguins[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]
y = penguins['species']

# Drop rows with missing values in selected columns
Xy = pd.concat([X, ...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Correctness:** The code correctly loads the dataset, selects the specified features and target, handles missing values, encodes the target variable, splits the data, trains a Logistic Regression model with `max_iter=1000`, and calculates accuracy.
- **Completeness:** All the specified steps in the task description are covered.
- **Code Quality:** Imports are proper. The code is well-structured and easy to follow.

**Areas for Improvement (minor):**
- **Robustness in Data Loading:** The ground truth includes a `try-except` block for `FileNotFoundError`. While the task description implies the file exists, in a real-world scenario, this makes the code more robust. The generated code assumes the file will always be present.
- **Explicit Dropping of NA:** While the current method of combining `X` and `y` then dropping NA works, it's slightly less direct than how the ground truth handles it (i.e., operating on a selected subset `df_selected` and then splitting). The generated code's `Xy = pd.concat([X, y], axis=1)` could potentially be confusing if `X` and `y` were already slices before the `dropna()` call. However, in this case, `X` and `y` are slices directly from the original `penguins` DataFrame, so it works. The logic is sound but could be marginally cleaner.
- **Output Verbosity:** The ground truth includes `print` statements at various stages (e.g., data loaded, rows dropped, data split, model trained) which is good for debugging and understanding the flow in an educational context. The generated code only prints the final accuracy. This is a minor point, as the task only explicitly asked for the accuracy to be reported.

Overall, it's a very good solution that fulfills the core requirements effectively.",gemini-2.0-flash,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
penguins = pd.read_csv(""penguins.csv"")

# Drop rows with NA in 'body_mass_g' or 'species'
penguins = penguins.dropna(subset=['body_mass_g', 'species'])

# Calculate the average body mass for each species
avg_body_mass = penguins.groupby('species')['body_mass_g'].mean()

# Create a bar plot
plt.figure(figsize=(8, 6))  # Adjust figure size for better readability
sns.barplot(x=avg_body_mass.index...",9,"The generated code is highly correct and complete, successfully addressing all requirements of the task. It loads the data, correctly handles NA values in the specified columns, calculates the average body mass per species, and generates a well-formatted bar plot with appropriate labels and title.

**Strong points:**
*   **Correctness:** All steps are implemented accurately.
*   **Completeness:** All task requirements are met.
*   **Readability:** The code is clean and easy to understand.
*   **Plotting:** Uses Seaborn for a visually appealing plot and includes necessary `plt` adjustments like title, labels, x-tick rotation, and `tight_layout()`.

**Minor areas for improvement (to reach a perfect 10, align more with robust benchmark code):**
*   **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which makes the code more robust. The generated code assumes the file will always be present.
*   **Clarity in Plotting Data:** While `avg_body_mass.index` and `avg_body_mass.values` work perfectly, explicitly creating a DataFrame like `avg_body_mass.reset_index()` and passing it to `sns.barplot` (as done in the ground truth) can sometimes be slightly more explicit or flexible for other potential seaborn plot types that expect a DataFrame. However, for this bar plot, the current approach is perfectly fine and common.
*   **Initial/Final Status Prints:** The ground truth includes print statements to show the initial and final shapes of the DataFrame, and NA counts, which are very helpful for debugging and understanding data transformations in a larger project. These are not strictly part of the core task but are good practices in data analysis scripts.

Overall, it's an excellent solution that performs the task efficiently and correctly.",gemini-2.0-flash,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load the dataset
penguins_df = pd.read_csv(""penguins.csv"")

# 2. Drop rows with NA in specified columns
penguins_df = penguins_df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])

# 3. Create culmen_area feature
penguins_df['culmen_area'] = penguins_df['culmen_length_mm'] * penguins_df['culmen_depth_mm']

# 4. Calculate the correlation matrix
correlation_matrix = pe...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, handles NA values in the specified columns by dropping rows, creates the `culmen_area` feature, calculates the Pearson correlation matrix for the specified columns, and visualizes it as an annotated heatmap with the correct title.

**Strengths:**
*   All steps are correctly implemented.
*   Uses `dropna(subset=...)` as required.
*   Correctly calculates the `culmen_area` feature.
*   Accurately computes the Pearson correlation matrix for the specified columns.
*   Generates a clear and properly annotated heatmap with the requested title.
*   Good use of `pandas`, `seaborn`, and `matplotlib.pyplot`.

**Minor Improvement Area (not critical for score but good practice):**
*   The ground truth includes error handling for `FileNotFoundError` when loading the CSV and checks for empty DataFrames after dropping NAs. While not explicitly asked for in the prompt's main requirements, these are good practices for robust code. The generated code assumes the file exists and that data will remain after dropping NAs. For a benchmark, including these robust checks can be beneficial.",gemini-2.0-flash,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd

# 1. Load the dataset
penguins = pd.read_csv('penguins.csv')

# 2. Select features and target
features = [
    'culmen_length_mm',
    'culmen_depth_mm',
    'flipper_length_mm',
    'body_mass_g',
    'island',
    'sex',
]
target = 'species'

penguins = penguins[features + [target]]


# 3. Impute numerical features with the mean
numerical_features = [
    'culmen_length_mm',
    'culmen_depth_mm',
    'flipper_length_mm',
    'body_mass_g',
]
for feature in numer...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Correct Data Loading and Feature Selection:** Successfully loads the CSV and selects the specified features and target.
- **Correct Numerical Imputation:** Imputes missing numerical values with the mean of their respective columns.
- **Correct Sex Imputation and Mapping:** Correctly handles 'sex' column: imputes 'Unknown' for NA values and maps 'MALE', 'FEMALE', 'Unknown' to 0, 1, and -1, respectively.
- **Correct One-Hot Encoding:** Applies one-hot encoding to the 'island' column.
- **Correct Output:** Prints the shape of the feature matrix and the first 5 rows as requested.
- **Concise and Readable:** The code is quite concise and easy to follow.

**Areas for Improvement (Why not a 10):**
- **`drop_first=True` in `get_dummies`:** While `drop_first=True` is often used to prevent multicollinearity in linear models, the prompt did not specify this. The ground truth code did not use `drop_first=True`, implying that all dummy variables should be generated. Using `drop_first=True` effectively removes one of the island categories from the feature set, which technically deviates from simply ""applying one-hot encoding"" without further constraints. For a classification task, adding all dummy variables is often fine, as many models handle multicollinearity implicitly or it's not a severe issue.
- **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.
- **Clarity in `X` definition:** The generated code directly overwrites the `penguins` DataFrame. While functionally correct, it might be clearer and safer to create `X` and `y` from a `df_processed` DataFrame to keep the original selected features intact if needed for other operations. However, this is a minor readability/best practice point.

Overall, the generated code is very good and nearly perfect for the given task, with only a minor discrepancy in the `get_dummies` argument that could be interpreted as a slight deviation from the prompt's implied direct one-hot encoding.",gemini-2.0-flash,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

# Create the DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST...",9,"The generated code is highly correct and complete, addressing all the requirements of the task. It properly creates the DataFrame, calculates total spending per customer, correctly identifies and prints the top 3 customers by spending, and accurately calculates the total quantity sold per category. The code quality is also good, with necessary imports and a clear structure.

The only minor area for improvement, leading to a score of 9 instead of 10, is the formatting of the output for the 'Total Spending' for top customers. The ground truth code formats the spending to two decimal places and includes comma separators (e.g., ""$1,200.00""), which is a common and professional way to display monetary values. The generated code prints `Total Spending: ${spending}` without explicit formatting, which works but could be enhanced for better readability in a real-world scenario. However, this is a very minor point and does not detract from the functional correctness or completeness of the solution.",gemini-2.0-flash,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_s...",6,"The generated code addresses most requirements but has a few significant issues that reduce its correctness and completeness, particularly in the preprocessing and feature interpretation steps.

**Areas for Improvement:**

1.  **Preprocessing Pipeline Usage:**
    *   The generated code manually scales `X_train` and `X_test` *after* transforming with `preprocessor`. This breaks the concept of a single pipeline for preprocessing and modeling, makes it difficult to interpret coefficients correctly (as scaling happens outside the pipeline), and mixes transformed and untransformed columns if `remainder='passthrough'` is used and not all columns are handled by named transformers.
    *   The ground truth correctly uses a `Pipeline` that encapsulates both `preprocessor` and `LogisticRegression`, ensuring that all preprocessing steps are correctly applied together, both for training and later for evaluation or cross-validation. This also ensures that the `coef_` attribute of the `LogisticRegression` model can be mapped back to the features correctly.

2.  **Feature Selection and Engineering:**
    *   The ground truth code correctly identifies and drops `PassengerId`, `Name`, `Ticket`, and `Cabin` early, as they are either identifiers, highly sparse, or unique and not useful for modeling. The generated code uses `remainder='passthrough'` which means these columns are kept, and then attempts to scale them which is incorrect.
    *   The ground truth includes feature engineering (`FamilySize`), which is a common and beneficial step in Titanic survival prediction. The generated code misses this.
    *   `Pclass` and `Sex` are explicitly treated as categorical in the ground truth, which is correct given their nature. The generated code implicitly treats `Pclass` (and `Sex`) as numerical or passes them through if not handled by a named transformer, which is often not ideal for `Pclass` (ordinal but often better as categorical) and definitively wrong for `Sex` (binary categorical).

3.  **Feature Coefficient Interpretation:**
    *   The generated code's attempt to map coefficients back to feature names is highly problematic. It tries to manually reconstruct `feature_names` by concatenating lists and then slicing `feature_names[:len(coefficients)]`. This is fragile and almost certainly incorrect because of the `remainder='passthrough'` and the manual scaling. The coefficients won't accurately map to the original or even the correctly transformed features.
    *   The ground truth accurately gets the `encoded_feature_names` directly from the fitted `preprocessor` within the `Pipeline`, which is the correct and robust way to do this.

4.  **Cross-Validation:**
    *   The generated code performs `cross_val_score` on `X_train` and `y_train` *after* they have already been preprocessed and scaled. For robust cross-validation, the preprocessing steps should be *part of* each fold's training, meaning `cross_val_score` should be applied to the `Pipeline` using the *original* `X` and `y` data, as shown in the ground truth. This prevents data leakage from the test sets of the folds into the preprocessing steps.
    *   The generated code only performs cross-validation for accuracy. The ground truth evaluates multiple metrics during cross-validation, which is more comprehensive.

5.  **Visualization:**
    *   The ground truth uses `ConfusionMatrixDisplay.from_estimator` and `RocCurveDisplay.from_estimator`, which are more convenient and robust ways to generate these plots directly from the model pipeline, ensuring consistency with how the model was trained and evaluated. The generated code manually creates the plots, which is fine, but the `Display` objects are generally preferred for ease of use.

**Strong Points:**

*   Correctly identifies and applies logistic regression.
*   Includes all requested evaluation metrics.
*   Attempts visualizations for confusion matrix and ROC curve.
*   Attempts cross-validation.
*   Basic handling of missing values for Age/Fare and Embarked.",gemini-2.0-flash,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']),
        'Ticker': ['TECHCO']*8,
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
        'Close': [152.0, 151.0, 153.5, 155.0, 1...",10,"The generated code is entirely correct and complete. It successfully creates the DataFrame, sets the 'Date' as the index, correctly calculates the 3-day Simple Moving Average for 'Close' prices (handling NaNs for initial days as required), and accurately computes the percentage change in 'Volume' (also correctly handling the NaN for the first day). The code includes the necessary pandas import and prints the final DataFrame as requested. The structure is clean and readable.",gemini-2.0-flash,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]}

df = pd.DataFrame(data)

...",9,"The generated code correctly addresses all parts of the task: DataFrame creation, new column calculation, grouped average calculation, and conditional filtering with printing of results. The imports are correct, and the code structure is logical and readable. The main minor difference from the ground truth is the lack of additional print statements for intermediate steps or formatting, but the core requirements are met. The ground truth also included an `if not student_ids_meeting_criteria.empty:` check which is a good practice for robustness, but not strictly required by the prompt.",gemini-2.0-flash,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Filter the DataFrame
tech_high_performers = df[(df[""Dep...",9,"The generated code correctly achieves all the requirements of the task: it defines the DataFrame, filters it according to the specified conditions, and accurately calculates the average number of ""ProjectsCompleted"" for the filtered subset. The code is concise and uses appropriate pandas syntax for filtering and aggregation.

The only minor point for improvement, which doesn't affect correctness but could be considered good practice for robustness, is the lack of explicit handling for the edge case where `tech_high_performers` might be empty (e.g., if no employees matched the criteria). While `mean()` on an empty Series returns `NaN`, which is technically correct, sometimes a more user-friendly message or specific `if` condition might be preferred, as shown in the ground truth. However, for the given task description, its current implementation is perfectly valid.",gemini-2.0-flash,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate conversion rate for Desktop users
desktop_users = df[df[""DeviceType""] == ""Desktop""]
desktop_conversion_rate = deskt...",9,"The generated code accurately defines the DataFrame and correctly calculates and prints the conversion rates for both Desktop and Mobile users as required by the task. The code is clean, readable, and directly addresses all parts of the problem.

The only minor area where the ground truth code is slightly more robust is the explicit handling of `empty` DataFrames (i.e., if there were no 'Desktop' or 'Mobile' users). While this specific dataset guarantees non-empty subsets, in a general solution, adding checks like `if not desktop_users_df.empty:` would make the code more robust to diverse input data, preventing potential `NaN` results if a device type had no occurrences. However, for the given problem context, this edge case doesn't apply.",gemini-2.0-flash,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

# Calculate average AirQualityIndex for each SensorID
avg_aqi = df.groupby(""Sen...",10,"The generated code is excellent. It correctly defines the DataFrame as specified, accurately calculates the average ""AirQualityIndex"" for each unique ""SensorID"", and prints the results clearly. The code is concise, uses appropriate pandas functions (`groupby().mean()`), and demonstrates good readability. All requirements of the task have been met.",gemini-2.0-flash,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

df[""EngagementScore""] = df[""Likes""] + (df[""Shares""] * 2) + (df[""Comments""] * 3)

insta...",9,"The generated code correctly performs all the requested tasks: defining the DataFrame, calculating the `EngagementScore`, filtering for Instagram posts, and computing their average `EngagementScore`. The code is clean, concise, and uses appropriate pandas functionalities.

One minor suggestion for improvement, which is common in good practice but not strictly required by the prompt, would be to add a check for an empty `instagram_posts` DataFrame before attempting to calculate the mean, to prevent potential errors if no Instagram posts were present in the initial data. Also, as per the ground truth, printing the result with a descriptive message would make the output more user-friendly. The current output only shows `average_engagement_score` without context. However, these are minor points and do not detract from the core correctness of the solution.",gemini-2.0-flash,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the ""LengthOfStay"" column accurately, filters the DataFrame to include only patients with a ""Flu"" diagnosis, and correctly calculates and prints their average ""LengthOfStay"". The code is concise, imports necessary libraries, and fully addresses all requirements of the task. The use of `.dt.days` is canonical for converting timedelta into days. While the ground truth includes `print` statements for intermediate steps, the prompt only asked for the final output, which the generated code provides.",gemini-2.0-flash,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}
df = pd.DataFrame(data)

sci_fi_movies = df[df[""Genre""...",9,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates their average rating. It adheres to all the specified requirements. The code is concise and directly addresses the problem.

The only minor area for improvement, preventing a perfect 10, is the lack of print formatting, which usually enhances readability for numerical outputs (e.g., rounding to two decimal places and providing a descriptive label). Also, handling the edge case where no Sci-Fi movies might be found (though not strictly necessary for this specific dataset) is a good practice for robustness in real-world scenarios, which the ground truth code includes. However, for the given task and sample data, this is a very effective solution.",gemini-2.0-flash,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

toyota_cars = df[df[""Brand""] == ""Toyota""]
average_mileage = toyota_car...",9,"The generated code correctly performs all requested operations: it defines the DataFrame, filters for Toyota cars, and calculates their average mileage. The code quality is good, with clear variable names and concise operations. It correctly imports pandas and follows a logical flow.

**Areas for Minor Improvement (to reach a 10):**
1.  **Output Formatting:** The ground truth code formats the output string more explicitly: `print(f""The average Mileage for Toyota cars is: {average_mileage:.2f}"")`. The generated code just prints the raw number. While not strictly required by ""print the average Mileage"", a user-friendly message is generally preferred.
2.  **Robustness/Edge Cases:** The ground truth code includes a check `if not toyota_cars_df.empty:`. While this specific dataset guarantees Toyota cars, adding such a check makes the code more robust for general use cases where the filtered DataFrame might be empty. This is a best practice.

Overall, it's a very strong and correct solution that performs exactly as required.",gemini-2.0-flash,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 80...",9,"The generated code correctly defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat. All requirements of the task are met. The code is clean, readable, and follows good practices for pandas operations.

The only minor area for improvement, especially for a production-ready or more robust script, would be to add a check for an empty `fire_pokemon` DataFrame before attempting to calculate the mean, to prevent potential errors if no 'Fire' type Pokemon existed in the data. Additionally, the ground truth's print statement is slightly more informative by specifying what the number means and formatting it. However, for the given task and a data science benchmark, the current solution is highly effective and correct.",gemini-2.0-flash,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Load the dataset
df = pd.read_csv('titanic.csv')

# Data cleaning: Impute missing age values with the median
df['Age'].fillna(df['Age'].median(), inplace=True)


# 1. Multi-panel figure: Survival rates by demographic groups
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))

# Survival by Class
sns....",7,"The generated code largely addresses the requirements but has several areas for improvement to achieve a higher score, particularly in terms of sophistication, storytelling, and adherence to best practices for data visualization.

**Strengths:**
*   **Correctness:** All plots are generated and mostly convey the intended information.
*   **Completeness:** Attempts to address all five points of the task, including interactive plots and a dashboard.
*   **Appropriate Libraries:** Uses `pandas`, `matplotlib`, `seaborn`, and `plotly` as specified.
*   **Interactive Plot:** The Plotly scatter plot is interactive and includes hover data.

**Areas for Improvement:**
*   **Data Preprocessing and Categorical Handling:**
    *   The ground truth performs more robust preprocessing (e.g., `Pclass_Cat` for readable labels, `AgeGroup` with more appropriate bins and labels, converting `Survived` to descriptive categories). The generated code uses raw Pclass numbers and less descriptive age group labels, which is less ideal for visualization storytelling.
*   **Visualization Sophistication and Storytelling:**
    *   **Multi-panel figure (Task 1):** The plots are very basic `barplot`s. The ground truth enhances these with shared y-axes, consistent limits, and a clear overall title.
    *   **Violin Plots (Task 3):** The generated code creates two separate violin plots for age distribution. The ground truth uses `split=True` on a single violin plot, which is a more effective and common way to compare two distributions within a single aesthetic element, making the comparison clearer and more concise.
    *   **Interactive Scatter Plot (Task 4):** While interactive, the generated code uses `color='Pclass'`, which is less explicit about what the colors represent without a legend. The ground truth uses `color='Pclass_Cat'` and `symbol='Survived_Str'` for better differentiation and clearer legend labels, enhancing the storytelling. `hover_data` is also more comprehensive in the ground truth.
    *   **Dashboard (Task 5):**
        *   The generated dashboard uses Plotly subplots, which is good, but the selection of plots and their arrangement could be more impactful.
        *   The ground truth uses `matplotlib.gridspec` to create a more custom and effective layout, including different plot types (KDE, Histplot) and, crucially, a dedicated text panel for ""Key Insights,"" which directly addresses the ""storytelling"" aspect of the prompt.
        *   The Plotly dashboard in the generated code lacks proper annotations (e.g., clear custom titles for traces, consistent legends for survival status comparisons). The violin plots are added as separate traces but not styled for comparison like `split=True` in Seaborn.
*   **Code Quality and Readability:**
    *   The ground truth includes configuration for `sns.set_style` and `plt.rcParams` for consistent and higher-quality outputs.
    *   Error handling for file loading is present in the ground truth.
    *   Better `plt.tight_layout` and `suptitle` usage for overall figure titles.
    *   The ground truth adds `ylim` to bar plots for consistency.
*   **Clarity of Insights:** The ground truth explicitly includes a text box with ""Key Insights,"" directly fulfilling the dashboard requirement for ""proper annotations and insights."" The generated code doesn't explicitly provide this.",gemini-2.0-flash,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, ...",9,"The generated code correctly defines the DataFrame, filters for the ""Dystopian"" genre, and accurately calculates the average stock for that genre. It fulfills all the stated requirements of the task. The code is concise and directly addresses the problem.

The only minor area for improvement, preventing a perfect 10, is the lack of an f-string for the print statement `print(average_stock)`. The ground truth uses `print(f""Average stock for Dystopian books: {average_dystopian_stock:.2f}"")`, which provides more informative output and better formatting. While not strictly required by the prompt, it enhances user experience. Also, the ground truth includes an `if not dystopian_books_df.empty:` check, which is good practice to handle edge cases, though for this specific data, it's not strictly necessary as Dystopian books are present. Overall, it's a very good solution.",gemini-2.0-flash,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load the dataset
df = pd.read_csv(""water_potability.csv"")
df = df.dropna() # Drop rows with NaN values

alpha = 0.05

# T-tests for comparing means
print(""T-Tests for Comparing Means (Potable vs. Non-Potable):"")
for col in df.columns[:-1]:
    potable = df[df['Potability'] == 1][col]
    non_potable = df[df['Potability'] == 0][col]
    
    t_statistic, p_value = stats.ttest_ind(potable, non_potable)
    
    print(f""\n-...",6,"The generated code addresses most of the requirements but has several issues that reduce its overall quality and correctness.

Here's a breakdown:

**Correctness:**
*   **Missing Value Handling:** The code uses `df.dropna()` which drops rows with ANY NaN values. This is often an overly aggressive approach for real-world datasets and can drastically reduce the dataset size, leading to less reliable statistical results. The ground truth uses mean imputation, which is a more standard and robust initial approach for numerical data in statistical analysis.
*   **T-test:** The code performs the t-test correctly in principle. However, it uses `equal_var=True` by default (standard `ttest_ind`), which assumes equal variances between the two groups. The ground truth uses `equal_var=False` (Welch's t-test), which is generally safer and more robust when variances are unknown or unequal, as is often the case in real-world data. The interpretation lacks the mention of statistical significance level (alpha).
*   **Cohen's d:** The calculation of Cohen's d is correct for the standard (pooled standard deviation) method, but it's only calculated when the p-value is significant. Cohen's d is an effect size measure and can be calculated regardless of statistical significance; it just isn't typically interpreted in the same way if the difference isn't significant. Critically, it doesn't handle cases where `pooled_std` might be zero if all values are identical, leading to `ZeroDivisionError` (though less likely in real data).
*   **Chi-square:** The chi-square goodness-of-fit test setup and interpretation are correct for the assumed 50/50 distribution.
*   **Normality Tests:** The Shapiro-Wilk tests are generally correct. However, it hardcodes the list of columns to test, which isn't dynamic. It also doesn't consider the limitation of Shapiro-Wilk for very large sample sizes (though `water_potability.csv` might not exceed it, it's good practice to consider).

**Code Quality:**
*   **Missing Value Handling:** The `dropna()` approach is a critical flaw from a data science best practices perspective as it's often not the optimal strategy for missing numerical data.
*   **Hardcoding Columns:** Hardcoding column names for normality tests makes the code less reusable and flexible.
*   **Clarity of Output:** The output for interpretations could be more explicit (e.g., ""P-value < alpha, Reject H0"").
*   **Comments:** Comments are minimal compared to the ground truth, which provides helpful explanations for statistical choices.

**Completeness:**
*   **Interpretation Details:** The interpretations for t-tests lack explicit mention of the alpha level, and the null/alternative hypotheses are not stated, making them less robust for a full statistical report.
*   **Error Handling:** It lacks error handling for file not found, which the ground truth includes.
*   **Robustness:** The code is less robust due to `dropna()` and the potential `ZeroDivisionError` in Cohen's d for pathological cases.

Overall, while the generated code attempts to perform the requested tests, its approach to data preprocessing (missing values) and statistical choices (t-test equal variance, Cohen's d calculation edge cases) are not as robust or best-practice oriented as the ground truth.",gemini-2.0-flash,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
from sklear...",6,"The generated code addresses most of the requirements, but there are several critical issues and areas for improvement, particularly regarding data preprocessing and cross-validation methodology.

**Strong Points:**
*   Correctly implements the loading and initial setup.
*   Includes multiple classification algorithms (Logistic Regression, Random Forest, SVM).
*   Calculates and displays a comprehensive set of evaluation metrics (accuracy, precision, recall, F1-score, ROC-AUC).
*   Generates confusion matrices and ROC curves.
*   Attempts k-fold cross-validation.
*   Compares model performance and selects a best model.
*   Attempts to display feature importance for the best model (specifically for Random Forest).

**Areas for Improvement and Critical Issues:**

1.  **Data Preprocessing Order (Critical):**
    *   **Imputation and Scaling before Train-Test Split:** The code performs `SimpleImputer` and `StandardScaler` on the *entire dataset* (`data.iloc[:, :-1]` and `X`) *before* the `train_test_split`. This is a significant data leakage issue. The imputer and scaler learn parameters (mean/median, std) from the test set data, which invalidates the evaluation. Preprocessing steps that learn parameters (like imputation and scaling) MUST be `fit` only on the training data and then `transform` applied to both training and test data. The ground truth correctly handles this by fitting `imputer` and `scaler` on `X_train` (or `X_train_imputed`) and then transforming both `X_train` and `X_test`.

2.  **K-Fold Cross-Validation (Critical):**
    *   **Data Leakage in CV:** The `cross_val_score` is called using `X_scaled, y`. While `X_scaled` was the result of scaling the *full dataset* (which itself is a data leakage issue as mentioned above), even if `X_scaled` was correctly preprocessed, the `cross_val_score` still needs to handle preprocessing *within each fold*. The generated code directly passes the pre-scaled `X_scaled`, which means scaling is applied globally, not per fold. This is a common mistake. The correct way is to wrap the preprocessing steps and the model into a `Pipeline` and then pass the *unprocessed* `X` and `y` to `cross_val_score`, allowing the pipeline to re-run preprocessing for each fold. The ground truth demonstrates this correctly using `Pipeline`.
    *   **Limited CV Metrics:** The `cross_val_score` is only shown for `accuracy`. The task requested k-fold cross-validation to assess model stability, implying all key metrics should be considered. The ground truth explicitly shows how to get multiple metrics (accuracy, precision, recall, f1, roc_auc) from CV.
    *   **KFold vs StratifiedKFold:** The ground truth used `StratifiedKFold`, which is generally better for classification tasks, especially with imbalanced datasets, to ensure that each fold has a similar distribution of classes. The generated code used `KFold`.

3.  **Visualization:**
    *   **`ConfusionMatrixDisplay` and `RocCurveDisplay`:** The generated code directly uses `confusion_matrix` and `roc_curve` and then plots them manually. While functional, `sklearn` provides `ConfusionMatrixDisplay` and `RocCurveDisplay.from_estimator` which simplify plotting and often provide better default visual layouts. The ground truth uses `ConfusionMatrixDisplay` implicitly via `sns.heatmap` and `RocCurveDisplay.from_estimator`. The manual plots, while working, often need more tuning.

4.  **Feature Importance:**
    *   **Limited Scope:** The feature importance logic only covers `Random Forest`. The task asked to display feature importance for the *best model*. While logistic regression has coefficients that can serve this purpose, the code doesn't implement it. The ground truth provides examples for both Random Forest (feature\_importances\_) and Logistic Regression (coefficients). SVC typically doesn't have direct feature importance in the same way, but this should be mentioned.

5.  **Readability/Best Practices:**
    *   **Variable Naming:** `data` is used for the DataFrame, then `X`, `y`, `X_scaled`. Could be clearer by sticking to one main DataFrame name (`df`) and consistently using `X` and `y`.
    *   **Missing Values Imputation Strategy:** The generated code used `mean` for imputation, while the ground truth used `median`. For skewed numerical data (which is common in real-world datasets), median imputation is generally more robust to outliers than mean imputation. While not strictly ""wrong,"" median is often preferred.
    *   **Clarity of Output:** The output for ""Model Comparison"" combines everything onto one line, making it a bit dense. The ground truth presents results in a more structured table format.

To achieve a higher score, the generated code needs to prioritize fixing the data leakage issues in preprocessing and cross-validation, and then enhance the completeness of the cross-validation metrics and feature importance displays.",gemini-2.0-flash,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import PolynomialFeatures

# Loa...",7,"The generated code addresses most of the task requirements but has several areas for improvement in terms of robustness, completeness, and adherence to best practices.

**Strengths:**
*   **Imports:** Includes necessary libraries.
*   **Missing Value Handling:** Correctly imputes missing values using the mean.
*   **Feature Engineering:** Creates ratio features, interaction terms (though only one example), and polynomial features.
*   **Correlation Analysis:** Calculates the correlation matrix and identifies top correlated features.
*   **Feature Selection:** Implements both SelectKBest and RFE.
*   **Visualizations:** Creates correlation heatmap, feature importance plot, and pairwise scatter plots.
*   **Comparison:** Includes a basic comparison of selected features.

**Areas for Improvement (leading to a score of 7, not higher):**

1.  **Meaningful Feature Engineering:**
    *   **Limited Ratios/Interactions:** The ground truth created significantly more curated ratio and interaction features. The generated code only has 3 ratio and 1 interaction feature. While it's subjective what defines ""meaningful,"" a broader set was expected.
    *   **Polynomial Features:** The generated code applies polynomial features to *all* original numerical columns, potentially creating a very large number of features without much control (`poly_df = pd.DataFrame(poly_features, columns=[f""poly_{i}"" for i in range(poly_features.shape[1])])`). The `poly_df` column naming (`poly_0`, `poly_1`, etc.) makes them uninterpretable, which goes against the ""meaningful features"" spirit. The ground truth carefully selects a subset of columns for polynomial expansion and ensures interpretable names.

2.  **Correlation Analysis:**
    *   **Top 5 Definition:** The `top_5_potability = correlation_matrix['Potability'].abs().nlargest(6)[1:]` line correctly identifies the top 5, but the rest of the code doesn't consistently use these specific feature names for subsequent analysis or visualization for ""top correlated features"". For example, the `feature_importance` and `pairplot` section *recalculates* or uses potentially different ""top features"". The ground truth uses `top_5_correlated_features` consistently.

3.  **Feature Selection:**
    *   **Number of Features:** Both SelectKBest and RFE are hardcoded to select `k=5` features. The task asks for ""optimal features"" and the top 5 correlated features, but for selection techniques, it's usually beneficial to select more than just the top 5 to see a range. The ground truth uses `k=15` for these methods, which is more reasonable.
    *   **Consistency:** The feature selection processes (`X` and `y` definition, scaling) could be more organized. The ground truth separates `X` and `y` early and scales `X` once for all selection methods. The generated code redefines `X` and `y` and doesn't explicitly mention or apply scaling *before* SelectKBest or RFE, which is crucial for models sensitive to scale or distance-based metrics (though f\_classif is less sensitive for KBest, RFE with RFClassifier benefits from consistent scaling if other models might be used later).

4.  **Visualizations:**
    *   **Heatmap Detail:** The heatmap `annot=False` and lacks formatting (`fmt="".2f""` for values), making it less informative than the ground truth's. Also, the figure size is smaller, leading to tightly packed labels.
    *   **Feature Importance:** The x-axis labels for feature importance are too dense due to plotting *all* features, making it unreadable. The ground truth plots only the top N (25) features, which is more practical.
    *   **Pairwise Scatter Plots:** The `sns.pairplot(data, vars=top_feature_names, hue='Potability')` call uses `data` (which includes original and engineered, uninterpretable poly features) and `top_feature_names` which were only the top 5 *original* features if `correlation_matrix` was calculated on the *original* data. If `data` included the new poly features with names like `poly_0`, `poly_1`, etc., the `top_feature_names` would be uninterpretable, and the scatter plot would be uninformative if `top_5_potability` captured these generic names. The ground truth ensures proper identification and plotting of the *actual* top 5 *engineered* features.

5.  **Comparison and Analysis:**
    *   **Limited Analysis:** The comparison is very basic, just printing the selected features and their overlap. The ground truth provides more structured analysis: overlap, features unique to each, and a union of all selected features, going deeper into the overlap analysis.

**Overall:** The code demonstrates a reasonable attempt at the task but lacks the refinement, robust feature engineering practices, clear visualization presentation, and comprehensive analysis present in an expert solution.",gemini-2.0-flash,LLM_judge_config
