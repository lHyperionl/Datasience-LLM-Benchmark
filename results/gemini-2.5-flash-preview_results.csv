task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1) List all column names and identify data types
print(""Column Names and Data Types:"")
for col in df.columns:
    col_type = df[col].dtype
    if pd.api.types.is_numeric_dtype(col_type):
        print(f""- {col}: Numerical"")
    else:
        print(f""- {col}: Categorical/Other ({col_type})"")

print(""\n"")

# 2) Calculate and report basic descriptive statistics for numerical columns
print(""Descriptive Stati...",8,"The generated code is generally correct and solves the task by loading the winequality-red.csv dataset, identifying data types, calculating descriptive statistics for numerical columns, and reporting dataset dimensions. It also identifies potential categorical or ordinal columns, although it relies on a heuristic for this step. The code is well-structured, and the use of descriptive print statements helps in understanding the output. However, there are some areas for improvement to achieve a perfect score. Firstly, the code lacks error handling for the case where the dataset file is not found in the current directory, which is handled in the ground truth code. Secondly, the heuristic used to identify potential categorical or ordinal columns is simplistic and may not work for all datasets. A more robust approach would be to use domain knowledge or more sophisticated statistical methods. Lastly, the code could benefit from more comprehensive documentation, such as comments explaining the purpose of each section and the logic behind the heuristics used. Overall, the code is solid, but these improvements would enhance its robustness and maintainability.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('video_games_sales.csv')
except FileNotFoundError:
    print(""Error: video_games_sales.csv not found. Please ensure the file is in the current directory."")
    exit()

# 1) Display the first 10 rows
print(""First 10 rows of the dataset:"")
print(df.head(10))
print(""\n"" + ""=""*50 + ""\n"")

# 2) Show the shape of the dataset (rows and columns)
print(""Shape of the dataset (rows, columns):"")
print(df.shape)
print(""\n"" + ""=""*50 +...",8,"The generated code is quite close to the ground truth code in terms of functionality and structure. It properly loads the dataset, displays the first 10 rows, shows the shape of the dataset, lists column names and data types, displays the top 10 games by Global_Sales, shows unique values in the Platform and Genre columns, calculates summary statistics for sales columns, and identifies missing values in the dataset. However, to achieve a perfect score, the code could benefit from more detailed error handling, such as checking if the 'Global_Sales' column exists before sorting, and handling potential errors during the sorting process. Additionally, the code could include more descriptive print statements to provide context for the output. Overall, the code is well-structured, and the use of pandas is effective in performing the required data analysis tasks.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd
import numpy as np

def clean_preprocess_video_games_sales(file_path=""video_games_sales.csv""):
    """"""
    Cleans and preprocesses the video_games_sales.csv dataset.

    Args:
        file_path (str): The path to the video_games_sales.csv file.

    Returns:
        pandas.DataFrame: The cleaned and preprocessed DataFrame.
    """"""

    df = pd.read_csv(file_path)

    # 2) Handle missing values in the Year column by filling them with the median year
    median_year...",8,"The generated code is generally correct and complete in solving the task of cleaning and preprocessing the video_games_sales.csv dataset. It properly handles missing values in the 'Year' column, removes games with missing Publisher information, creates a new column 'Years_Since_Release', removes duplicate games, and filters out games with low Global_Sales. The code is well-structured and readable, with proper imports and a clear function definition. However, there are a few areas for improvement. Firstly, the code lacks error handling, specifically for cases where the input file is not found or other unexpected errors occur. Secondly, the code does not display the original dataset's shape, which is a requirement. Lastly, the code could benefit from more detailed comments and docstrings to explain the purpose of each section and the reasoning behind certain operations. Despite these minor issues, the code is strong and effectively solves the task at hand.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('video_games_sales.csv')
except FileNotFoundError:
    print(""Error: video_games_sales.csv not found. Please make sure the file is in the current directory."")
    exit()

# Set style for plots
sns.set_style(""whitegrid"")

# 1. Bar chart: Top 10 gaming platforms by total global sales
platform_sales = df.groupby('Platform')['Global_Sales'].sum().nlargest(10)
plt.figure(f...",9,"The generated code is very close to the ground truth code in terms of correctness and completeness. It successfully generates all the required visualizations with proper formatting and labels. The use of seaborn for setting the style and creating plots is a good practice, making the code more readable and visually appealing. However, there are a few minor discrepancies and improvements that could be made to achieve a perfect score. For instance, the ground truth code includes data cleaning and preprocessing steps that are not present in the generated code, which could potentially affect the quality of the visualizations. Additionally, the generated code uses seaborn's `histplot` and `lineplot` functions, which are equivalent to matplotlib's `hist` and `plot` functions but with additional features. Overall, the generated code is well-structured, and the visualizations are clear and informative. With a few minor adjustments, it could be considered even more effective and reliable.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('diabetes.csv')

# 1) Display the first 10 rows
print(""First 10 rows of the dataset:"")
print(df.head(10))
print(""\n"" + ""=""*50 + ""\n"")

# 2) Show the shape of the dataset (rows and columns)
print(""Shape of the dataset (rows, columns):"")
print(df.shape)
print(""\n"" + ""=""*50 + ""\n"")

# 3) List all column names and their data types
print(""Column names and their data types:"")
print(df.info())
print(""\n"" + ""=""*50 + ""\n"")

# 4) Calculate...",7,"The generated code successfully performs the required tasks for exploratory data analysis on the diabetes dataset, demonstrating good understanding of the task. It correctly loads the dataset, displays the first 10 rows, shows the shape, lists column names and data types, calculates the percentage of diabetic vs non-diabetic patients, displays the count of missing values for each column, and shows basic summary statistics for numerical columns. However, it lacks some completeness in addressing all requirements, such as not explicitly identifying columns with zero values that might represent missing data in a clear and structured manner like the ground truth code. Additionally, error handling is minimal and could be improved for robustness. The code organization is clear, and the use of pandas is efficient, but it could benefit from more comprehensive error checking and handling, especially for file loading and potential data-type issues. To achieve a higher score, the code should be enhanced with more comprehensive error handling, explicit handling of potential missing data (beyond just numerical columns), and clearer identification of columns with zero values that might indicate missing data.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler

# 1) Load the dataset
df = pd.read_csv('diabetes.csv')

# 2) Identify columns with zero values that represent missing data
# (Glucose, BloodPressure, SkinThickness, Insulin, BMI should not be zero)
missing_value_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

# 3) Replace zero values in these columns with NaN
for col in missing_value_cols:
    df[col] = df[col].replace(0, pd.NA)

# 4) Handle m...",8,"The generated code effectively solves the task of cleaning and preprocessing the diabetes.csv dataset. It properly loads the dataset, identifies and replaces zero values with NaN, handles missing values by filling them with the median, removes duplicate rows, creates a new categorical feature for BMI categories, and scales numerical features using StandardScaler. The code is well-structured, and the use of pandas and sklearn libraries is appropriate. However, there are a few areas for improvement to achieve a perfect score. Firstly, the code does not handle the case where the 'diabetes.csv' file is not found in the current directory, which is handled in the ground truth code. Secondly, the code uses `pd.NA` to represent missing values, which is not consistent with the ground truth code that uses `np.nan`. Lastly, the code does not print the original dataset shape for comparison, which is included in the ground truth code. Overall, the code is strong, and with a few minor adjustments, it can be even more robust and consistent with the ground truth code.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('diabetes.csv')

# 1) Correlation Heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt="".2f"")
plt.title('Correlation Heatmap of Diabetes Dataset')
plt.show()

# 2) Histograms for all numerical features
df.hist(bins=20, figsize=(15, 10), layout=(3, 3))
plt.suptitle('Histograms of Numerical Features')
plt.tight_layout(rect=[0, 0.03, 1, 0.95...",7,"The generated code is generally correct and solves the task of creating visualizations for the diabetes dataset. It covers various aspects such as correlation heatmap, histograms, box plots, pie chart, scatter plots, and pair plots, which aligns with the requirements. The code quality is also decent, with proper imports and a structured approach to creating the visualizations. However, there are a few areas where the code could be improved to achieve a higher score. Firstly, the code lacks consistency in its plotting style and formatting, which can make the output look disjointed. Secondly, some of the plots are created without explicitly setting titles and labels, which can make them less informative. Lastly, the code could benefit from additional error handling, especially when loading the dataset, to ensure it can handle potential issues such as a missing file. Overall, the code is a good start, but with some refinements, it can be even more effective and user-friendly.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('world-happiness-report-2021.csv')
except FileNotFoundError:
    print(""Error: 'world-happiness-report-2021.csv' not found. Please ensure the CSV file is in the current directory."")
    exit()

# 1) Display the first 10 rows
print(""First 10 rows of the dataset:"")
print(df.head(10))
print(""\n"" + ""=""*50 + ""\n"")

# 2) Show the shape of the dataset (rows and columns)
print(""Shape of the dataset (rows, columns):"")
print(df.sh...",9,"The generated code is very close to the ground truth code and successfully performs all the tasks required for the exploratory data analysis on the world-happiness-report-2021.csv dataset. It properly loads the dataset, displays the first 10 rows, shows the shape of the dataset, lists column names and their data types, displays unique regional indicators, shows the top 10 happiest countries, calculates summary statistics for key happiness factors, and identifies any missing values in the dataset. The code is well-structured, and the use of try-except blocks for handling the FileNotFoundError is good practice. The only minor difference is in the length of the separator strings used for clarity, but this does not affect the functionality of the code. To achieve a perfect score, the code could be made more robust by adding additional error handling for cases where the required columns are not found in the dataset, but this is a minor suggestion. Overall, the code is of high quality, and the tasks are addressed completely.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# 2) Handle missing values
# Identify numerical and categorical columns
numerical_cols = df.select_dtypes(include=['number']).columns
categorical_cols = df.select_dtypes(include=['object']).columns

# Fill missing numerical values with the mean
for col in numerical_cols:
    if df[col].isnull().any():
        df[col].fillna(df[col]....",8,"The generated code is generally correct and solves the task of cleaning and preprocessing the world-happiness-report-2021.csv dataset. It properly handles missing values, creates new derived features, removes outliers, applies scaling and encoding, and displays the cleaned dataset's information. The code quality is good, with proper imports and a clear structure. However, there are a few areas for improvement to achieve a higher score. Firstly, the generated code does not handle the case where the dataset file is not found, which is handled in the ground truth code. Secondly, the code does not print the initial shape of the dataset before cleaning, which is also present in the ground truth code. Additionally, the variable name 'Regional indicator_encoded' could be more descriptive, and the original 'Regional indicator' column is dropped without notice. Lastly, the code could benefit from more comments to explain the purpose of each section, especially for the more complex operations like standardizing numerical features and encoding the 'Regional indicator'. Overall, the code is strong, but these minor improvements would enhance its readability and robustness.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go

# 1) Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Preprocessing: Fill NaN values for 'Regional indicator'
df['Regional indicator'].fillna('Unknown Region', inplace=True)

# 2) Create a world map visualization showing happiness scores by country (use plotly)
fig_map = px.choropleth(df,
                        locations=""Count...",8,"The generated code is mostly correct and complete, addressing all the requirements of the task. It properly loads the dataset, creates a world map visualization, generates a horizontal bar chart, creates a correlation heatmap, plots scatter plots, generates box plots, and creates a radar chart showing the happiness profile of the top 5 happiest countries. The code also includes proper imports and is well-structured. However, there are a few minor issues that prevent it from being perfect: (1) In the correlation heatmap, the axis is not set to be equal, which might make the plot not a perfect square. (2) In the box plot, the x-axis label is 'Ladder score' which is not suitable, it should be 'Regional Indicator' for better understanding. (3) The radar chart normalization is optional but good practice, and the code could be improved by making it clear that this normalization is done for better visualization. (4) The code could benefit from more comments to explain the purpose of each section and the reasoning behind certain decisions. Overall, the code is strong, and with a few tweaks, it can be even better.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd

# 1) Load the dataset and parse the 'date' column as datetime objects
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2) Display the first 10 rows
print(""First 10 rows of the dataset:"")
print(df.head(10))
print(""\n"" + ""=""*50 + ""\n"")

# 3) Show the shape of the dataset (rows and columns)
print(""Shape of the dataset (rows, columns):"")
print(df.shape)
print(""\n"" + ""=""*50 + ""\n"")

# 4) List all column names and their data types
print(""Column nam...",8,"The generated code demonstrates a good understanding of the task requirements and implements the necessary steps for exploratory data analysis on the synthetic_covid19_data.csv dataset. It correctly loads the dataset, parses the 'date' column, displays the first 10 rows, shows the dataset shape, lists column names and data types, calculates the total sum of 'total_cases' and 'total_deaths' grouped by 'continent', identifies the top 5 'location' entries with the highest 'total_cases' based on the latest date, and calculates basic summary statistics for 'new_cases' and 'new_deaths'. However, there are a few areas where the code could be improved for higher scores. Firstly, error handling is minimal; for example, it does not check if the required columns exist before trying to access them, which could lead to errors if the dataset is missing expected columns. Secondly, the code could benefit from more explicit checks and handling for potential data types issues, such as ensuring that 'total_cases' and 'new_cases' are numeric before calculation. Lastly, the code's structure and comments are straightforward but could be enhanced with more detailed explanations of the reasoning behind certain steps, especially for more complex operations like grouping and sorting. Overall, the code is well-structured and effectively solves the task, but with some enhancements in error handling, data type checking, and commenting, it could reach a perfect score.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1) Calculate and report the average (mean) 'quality' of the red wines
average_quality = df['quality'].mean()
print(f""Average quality of red wines: {average_quality:.2f}"")

# 2) Group the wines by their 'quality' score and calculate the average 'alcohol' content for each quality group
average_alcohol_by_quality = df.groupby('quality')['alcohol'].mean()...",7,"The generated code is mostly correct and completes the task of analyzing the winequality-red.csv dataset, addressing all the main requirements. It correctly calculates the mean quality, groups the wines by their quality score to calculate the average alcohol content, and creates a bar chart to visualize the relationship between quality and average alcohol content. However, there are a few areas where the code can be improved to achieve a higher score. Firstly, the generated code lacks error handling, which is present in the ground truth code. It does not check if the file exists before attempting to read it, which could lead to errors if the file is not found. Additionally, the generated code omits setting a plot style, which results in a less aesthetically pleasing plot compared to the ground truth code. Furthermore, the generated code does not add data labels on top of the bars, making it less informative than the ground truth code. Overall, the code is well-structured and easy to follow, but with a few modifications, it can be even more robust and visually appealing.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

def clean_and_preprocess_covid_data(file_path='synthetic_covid19_data.csv'):
    """"""
    Cleans and preprocesses the synthetic_covid19_data.csv dataset.

    Args:
        file_path (str): The path to the CSV file.

    Returns:
        pandas.DataFrame: The cleaned and preprocessed DataFrame.
    """"""
    # 1) Load the dataset and parse the 'date' column as datetime objects.
    df = pd.read_csv(file_path, parse_dates=['date'])

    # 2) Handle m...",8,"The generated code effectively solves the task with a few minor adjustments needed to match the ground truth code's exact behavior, particularly in handling and displaying the cleaned dataset. The code structure is clear, and imports are proper. It addresses all the requirements with a good approach to handling missing values and creating new columns. However, to achieve a perfect score, it should ideally include error handling for the file not found scenario, as seen in the ground truth code, and perhaps a more detailed comment or docstring explaining the purpose of each step, especially for readers unfamiliar with the dataset or the processing steps. Additionally, the code could benefit from a check to ensure that the 'total_cases' and 'population' columns are not all zeros to avoid division by zero errors when calculating the 'mortality_rate' and 'vaccination_rate_per_capita', respectively.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Parse the 'date' column
df['date'] = pd.to_datetime(df['date'])

# 1. Time series line plot of 'new_cases' for India
plt.figure(figsize=(12, 6))
india_data = df[df['location'] == 'India'].sort_values('date')
sns.lineplot(x='date', y='new_cases', data=india_data)
plt.title('Daily New Cases in India Over Time')
plt.xlabel('Date')
plt.ylabel('New ...",8,"The generated code is largely correct and successfully solves the task by creating the required visualizations. It properly imports the necessary libraries, loads the dataset, and parses the 'date' column. The code is well-structured, and the use of seaborn for plotting is appropriate. However, there are a few minor issues that prevent it from being a perfect solution. For instance, the code could benefit from more informative error handling, such as checking if the 'synthetic_covid19_data.csv' file exists before attempting to load it. Additionally, some of the plot titles and labels could be more descriptive to better convey the meaning of each visualization. Furthermore, the code could be improved by adding more comments to explain the purpose of each section, making it easier for others to understand the logic and intentions behind the code. Overall, the code is a strong effort, and with a few refinements, it could achieve an even higher score.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 1) List all column names and identify the data type of each column
print(""Column Names and Data Types:"")
print(df.info())

# 2) Calculate and report basic descriptive statistics for numerical columns
numerical_columns = ['elixirCost', 'Win Rate', 'Usage']
print(""\nDescriptive Statistics for Numerical Columns:"")
for col in numerical_columns:
    print(f""\n--- {col} ---"")
    print(f""Mean: {df[col].mean...",8,"The generated code is generally correct and solves the task with some minor flaws. It correctly loads the dataset, lists column names, and calculates descriptive statistics for the specified numerical columns. However, it lacks error handling, which is a critical aspect of any data processing script. The code does not check if the file exists before attempting to load it, which could lead to errors if the file is not found. Additionally, it does not handle cases where the specified numerical columns may not exist in the dataset, which could result in KeyError. The code also uses `df.info()` to print column names and data types, which is not as explicit as listing each column name and its data type separately. Furthermore, the code calculates descriptive statistics for each numerical column individually, which could be more efficiently done using pandas' built-in aggregation functions. Overall, the code is well-structured and easy to follow, but it could benefit from additional error handling and more efficient statistical calculations.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Filter for elixirCost <= 3
filtered_df = df[df['elixirCost'] <= 3]

# Select 'Card' and 'Win Rate' columns and sort by 'Win Rate' descending
result = filtered_df[['Card', 'Win Rate']].sort_values(by='Win Rate', ascending=False)

# Display the result
print(result)
```",8,"The generated code is generally correct and solves the task with a good level of code quality. It properly imports the necessary library, loads the dataset, filters the cards based on the elixir cost, selects the required columns, and sorts the result by the win rate in descending order. The code is also relatively concise and easy to read. However, to achieve a perfect score, it could benefit from some error handling similar to the ground truth code, such as checking if the required columns exist before selecting them, and handling potential exceptions during the file loading process. Additionally, defining constants for the column names and the maximum elixir cost, as in the ground truth code, could improve readability and maintainability.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Generate the histogram for 'elixirCost'
plt.figure(figsize=(10, 6))
sns.histplot(df['elixirCost'], bins=range(1, 11), kde=False, stat='count', discrete=True)

# Add title and labels
plt.title('Distribution of Elixir Cost in Clash Royale Cards')
plt.xlabel('Elixir Cost')
plt.ylabel('Number of Cards')
plt.xticks(range(1, 11)) # Ensure all integer eli...",8,"The generated code is generally correct and solves the task by loading the dataset and generating a histogram for the `elixirCost` column as required. It also properly sets up the title, labels, and displays the plot. However, there are a few areas for improvement to achieve a perfect score. Firstly, the generated code lacks error handling, which is crucial for ensuring the script can handle potential errors such as a file not found or other exceptions during the loading process. Secondly, the code does not check if the `elixirCost` column exists in the DataFrame before attempting to plot it, which could lead to an AttributeError if the column is missing. Lastly, while the code is well-structured, it could benefit from more descriptive variable names and comments to improve readability, especially for someone unfamiliar with the code. Overall, the code is strong, but with these improvements, it could reach a score of 10.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# Load the iris dataset
df = pd.read_csv('iris.csv')

# 1) List all column names and identify the data type of each column
print(""Column Names and Data Types:"")
print(df.info())

# 2) Calculate and report basic descriptive statistics for numerical columns
numerical_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

print(""\nDescriptive Statistics for Numerical Columns:"")
for col in numerical_cols:
    print(f""\n--- {col} ---"")
    print(f""Me...",8,"The generated code is mostly correct and solves the task with some minor issues and improvements needed for a higher score. The code correctly loads the iris.csv dataset, lists column names and identifies their data types, calculates descriptive statistics for the specified numerical columns, and reports the number of unique species and their counts. However, it lacks error handling for file loading and column existence, which is crucial for robustness. The code also uses a more direct approach for calculating descriptive statistics, which is clear but may not be as flexible as using the `.agg()` method. Overall, the code is well-structured and easy to follow, but could benefit from additional error checking and handling for a higher score.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('iris.csv')

# Filter for 'Iris-setosa' species
setosa_df = df[df['Species'] == 'Iris-setosa']

# Select and display 'SepalLengthCm' and 'SepalWidthCm'
print(setosa_df[['SepalLengthCm', 'SepalWidthCm']])
```",8,"The generated code is essentially correct and solves the task, but it lacks some aspects of robustness and error handling found in the ground truth code. Specifically, it does not handle potential errors such as a missing file, unknown columns, or other exceptions that might occur during execution. The code structure is simple and straightforward, which is good, but it benefits from additional error handling mechanisms to make it more reliable. Additionally, while it meets the primary requirements, it does not explicitly display the steps or provide user-friendly error messages, which are present in the ground truth code. Overall, the code is a good start but can be improved with the addition of error handling and more detailed feedback mechanisms.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('iris.csv')

# Create the scatter plot
fig, ax = plt.subplots()

# Group by 'Species' and plot each group with a different color
for species, group in df.groupby('Species'):
    ax.scatter(group['SepalLengthCm'], group['SepalWidthCm'], label=species)

# Add title and labels
ax.set_title('Sepal Length vs. Sepal Width by Species')
ax.set_xlabel('Sepal Length (cm)')
ax.set_ylabel('Sepal Width (cm)')

...",8,"The generated code effectively loads the iris.csv dataset and creates a scatter plot to visualize the relationship between `SepalLengthCm` and `SepalWidthCm`, colored by the `Species` column, and includes a title and axis labels. It properly uses pandas for data manipulation and matplotlib for plotting. The code is concise and well-structured, making it easy to read and understand. However, to achieve a perfect score, it could benefit from additional error handling, such as checking if the required columns exist in the dataset before plotting, and considering transparency for better visibility of overlapping points. Additionally, including a grid in the plot for better readability and specifying the figure size would enhance the overall quality and readability of the plot.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('heart.csv')
except FileNotFoundError:
    print(""Error: heart.csv not found. Please ensure the file is in the current directory."")
    exit()

# 1) List all column names and identify the data type of each column
print(""Column Names and Data Types:"")
print(df.info())
print(""\n"" + ""=""*50 + ""\n"")

# 2) For the numerical columns Age, RestingBP, Cholesterol, and MaxHR,
#    calculate and report basic descriptive statistics
n...",8,"The generated code is generally correct and solves the task of loading the heart.csv dataset, identifying column data types, calculating descriptive statistics for numerical columns, and reporting value counts for the HeartDisease column. It handles file not found errors and checks for the existence of required columns before attempting to calculate statistics or report value counts. The code is well-structured and readable, with proper use of pandas functions and clear print statements for output.

However, to achieve a perfect score, there are a few areas for improvement. Firstly, the code could benefit from more comprehensive error handling, such as catching and handling specific exceptions that might occur during the execution of pandas functions, in addition to the FileNotFoundError. Secondly, the code could be made more robust by adding checks to ensure that the dataset is not empty before attempting to calculate statistics or report value counts. Lastly, while the code is readable, it could be slightly improved by adding comments to explain the purpose of each section, especially for someone not familiar with the pandas library. Overall, the code is strong, and with these minor adjustments, it could be even more robust and usuario-friendly.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# Filter the dataset
filtered_df = df[(df['Age'] > 50) & (df['HeartDisease'] == 1)]

# Select and display specified columns
result = filtered_df[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]
print(result)
```",6,"The generated code is mostly correct in terms of solving the task but lacks completeness and has room for improvement in code quality. It correctly loads the 'heart.csv' dataset, filters it based on the specified criteria (`Age` > 50 and `HeartDisease` == 1), and selects the required columns ('Age', 'Sex', 'ChestPainType', and 'Cholesterol') for display. However, it does not include any error handling, which is crucial for a robust data analysis script. Additionally, it does not provide any information about the number of records matching the criteria or the data types of the displayed columns, which is useful for understanding the dataset. The code structure is simple and straightforward, but it could benefit from being encapsulated in a function for better reusability and organization, similar to the ground truth code. To achieve a higher score, the code should be enhanced with error handling and additional informative output, and it should be structured in a more modular and reusable way.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Filter wines based on alcohol and pH
filtered_wines = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]

# Identify the wine with the highest quality from the filtered subset
highest_quality_wine = filtered_wines.loc[filtered_wines['quality'].idxmax()]

# Display all features of this specific wine
print(highest_quality_wine)
```",8,"The generated code is mostly correct and solves the task. It properly filters wines based on the given conditions, identifies the wine with the highest quality from the filtered subset, and displays all features of this specific wine. The code is well-structured and uses proper imports. However, it lacks some error handling and edge case management found in the ground truth code, such as handling the scenario where no wines meet the specified criteria or handling potential file not found errors. Additionally, the generated code does not provide as much informative output as the ground truth code, which includes the shape of the dataset and the number of wines meeting the criteria. To achieve a higher score, incorporating these additional features and error handling mechanisms would be beneficial.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('heart.csv')
except FileNotFoundError:
    print(""Error: heart.csv not found. Make sure the file is in the current directory."")
    exit()

# Set a style for better aesthetics
sns.set_style(""whitegrid"")

# Plot 1: Histogram for 'Age' distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['Age'], bins=15, kde=True, color='skyblue')
plt.title('Distribution of Age', fo...",8,"The generated code effectively solves the task by loading the heart.csv dataset using pandas and generating the required visualizations. It correctly imports the necessary libraries, structures the code in a clear manner, and addresses all the requirements. The code is well-organized, and the use of seaborn for plotting enhances the visual appeal of the histograms and bar charts. However, there are a few minor areas for improvement to achieve a perfect score. For instance, the generated code lacks error handling for potential Key Errors that might occur if the 'Age' or 'Sex' columns are missing from the dataset. Additionally, while the code assumes a specific encoding for the 'Sex' column (0 for Female, 1 for Male), it would be more robust to include a try-except block to handle any unexpected errors during the execution of the code. Overall, the generated code is strong, and with these minor adjustments, it can be nearly flawless.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('spotify_2023.csv')
except FileNotFoundError:
    print(""Error: 'spotify_2023.csv' not found. Please make sure the file is in the current directory."")
    exit()

# 1) List all column names and identify the data type of each
print(""Column Names and Data Types:"")
print(df.info())
print(""\n"" + ""=""*50 + ""\n"")

# 2) Convert the streams column to a numeric data type
# Some non-numeric strings might use ',' as a thousands sepa...",8,"The generated code is generally correct in solving the task, with some minor issues and areas for improvement. It correctly loads the dataset, lists column names and data types, converts the 'streams' column to a numeric data type, calculates descriptive statistics for numerical columns, and reports the total number of unique artists. However, there are a few areas where the code could be improved: 
1. Error handling in the file loading section is adequate, but the generated code does not provide as much feedback as the ground truth code does when the file is not found.
2. The handling of non-numeric strings in the 'streams' column is done correctly by removing commas, but this might not cover all possible cases (e.g., using spaces or other characters as thousand separators).
3. The calculation of descriptive statistics is done separately for each column, which is redundant and less efficient than using the `describe()` method provided by pandas, as shown in the ground truth code.
4. The counting of unique artists is innovative in handling cases where multiple artists are listed in one string, but it assumes that artists are always separated by commas and spaces, which might not be the case in all scenarios.
5. The code could benefit from more comments to explain the purpose of each section and the reasoning behind certain decisions, making it easier for others to understand and maintain. Overall, the code is well-structured and readable, but with some enhancements, it could reach a perfect score.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Filter for songs released in 2023
df_2023 = df[df['released_year'] == 2023]

# Select relevant columns
df_selected = df_2023[['track_name', 'artist(s)_name', 'streams']]

# Convert 'streams' to numeric, coercing errors to NaN
df_selected['streams'] = pd.to_numeric(df_selected['streams'], errors='coerce')

# Sort by 'streams' in descending order
df_sorted = df_selected.sort_values(by='streams', ascending=Fal...",8,"The generated code is correct and solves the task with minor omissions compared to the ground truth code. It properly loads the dataset, filters for songs released in 2023, selects the relevant columns, converts the 'streams' column to numeric, and sorts the data by 'streams' in descending order. However, it lacks the error handling for file not found and other exceptions, which is a crucial aspect for robust code. Additionally, it does not explicitly handle the case where the 'streams' column might contain NaN values after coercion, although the sorting operation will place these at the end. To achieve a perfect score, the generated code should include error handling and explicit handling of NaN values, making it more comprehensive and robust.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 1. Histogram for 'bpm'
plt.figure(figsize=(10, 6))
plt.hist(df['bpm'], bins=20, edgecolor='black')
plt.title('Distribution of BPM in Spotify 2023 Tracks')
plt.xlabel('BPM (Beats Per Minute)')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.tight_layout()
plt.show()

# 2. Scatter plot for 'danceability_%' vs 'energy_%'
plt.figure(figsize=(10, 6))
plt.scatter(df['dan...",8,"The generated code is generally correct and solves the task with some minor limitations. It correctly loads the dataset, generates a histogram for the 'bpm' column, and creates a scatter plot for 'danceability_%' vs 'energy_%. The code is well-structured, and the use of pandas and matplotlib is appropriate. However, there are a few areas where the code could be improved to achieve a higher score. Firstly, the generated code lacks error handling, which is a critical aspect of robust code. It does not handle cases where the dataset file is not found or if the columns 'bpm', 'danceability_%', or 'energy_%' are missing from the dataset. Secondly, the number of bins in the histogram is hardcoded to 20, which might not be optimal for all datasets. Finally, the code could benefit from more descriptive variable names and comments to improve readability and maintainability. Overall, the code is a good start, but with some additional error handling and refinements, it could be even more robust and effective.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: 'youtube_trendings.csv' not found. Make sure the file is in the current directory."")
    exit()

# 1) List all column names and their inferred data types
print(""Column names and their inferred data types:"")
print(df.info())
print(""\n"" + ""=""*50 + ""\n"")

# 2) Clean and convert 'Video views', 'Likes', and 'Dislikes' to numeric
columns_to...",8,"The generated code is mostly correct and solves the task. It properly loads the dataset, cleans the specified numeric columns by removing commas, converting to numeric, and filling NaNs with 0. It also calculates descriptive statistics for these cleaned columns, fills missing values in the 'Category' column with 'Unknown', and reports the count of unique values in the 'Category' column. However, the code lacks some details found in the ground truth code, such as providing a clear message when a column is not found during the cleaning process and not displaying the initial and final NaN counts for the 'Category' column. Additionally, the code does not include a final message indicating the completion of data processing. To achieve a perfect score, the code should be refined to include theseadditional details and messages.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '', regex=False)
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce')
df['Video views'] = df['Video views'].fillna(0)

# Filter for video views greater than 50,000,000
filtered_df = df[df['Video views'] > 50_000_000]

# Select and display specified columns
result = filtered_df[['Video', 'Video views']]

#...",9,"The generated code effectively solves the task with minor omissions compared to the ground truth code. It correctly loads the dataset, cleans the 'Video views' column by removing commas, converting to numeric, and handling conversion failures as NaN, then filling them with 0. The code properly filters the dataset for videos with views greater than 50,000,000, selects the required columns, and sorts the result by 'Video views' in descending order. The only notable difference is the absence of error handling for the case when the 'youtube_trendings.csv' file is not found in the current directory, which is implemented in the ground truth code. To achieve a perfect score, the generated code should include this error handling mechanism. Overall, the code is well-structured, and the use of pandas functions is appropriate for data manipulation and analysis.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Define columns to clean and convert
cols_to_clean = ['Likes', 'Video views']

for col in cols_to_clean:
    # Remove commas
    df[col] = df[col].astype(str).str.replace(',', '', regex=False)
    
    # Convert to numeric, coercing errors to NaN
    df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Fill NA/NaN values with 0
    df[col] = df[col].fillna(0)

# C...",9,"The generated code is very close to the ground truth code in terms of correctness, code quality, and completeness. It properly loads the YouTube trending dataset, cleans the 'Likes' and 'Video views' columns by removing commas, converting them to numeric types, and filling NaN values with 0. It then generates a scatter plot with the cleaned data, including a title and axis labels. The code structure is clear, and it handles potential errors during dataset loading. The only minor improvement could be adding some error handling for cases where the dataset file is not found or other exceptions occur during execution, which is already handled in the ground truth code. Overall, the generated code is well-structured and effectively solves the task.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: 'mobiles_2025.csv' not found. Please make sure the file is in the current directory."")
    exit()

# 1) Clean 'Mobile Weight'
df['Mobile Weight'] = df['Mobile Weight'].astype(str).str.replace('g', '', regex=False)
df['Mobile Weight'] = pd.to_numeric(df['Mobile Weight'], errors='coerce')

# 2) Clean 'RAM'
df['RAM'] = df['RAM'].astype(str).s...",8,"The generated code successfully completes the task by loading the dataset, cleaning the specified columns, listing data types, calculating descriptive statistics, and counting unique company names. It properly handles errors, assumes the CSV file is in the current directory, and uses pandas for data manipulation. However, to achieve a perfect score, the code could benefit from additional error handling, such as for cases where the specified columns do not exist in the dataset, and could be more concise by utilizing pandas' built-in functions more efficiently. Additionally, the code lacks a main function or a clear entry point, making it less modular and understandable. Overall, the code is well-structured and effectively solves the given task, but with some minor improvements, it could become more robust and Pythonic.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd

try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: mobiles_2025.csv not found. Make sure the file is in the current directory."")
    exit()

def clean_price(price_str):
    if isinstance(price_str, str):
        price_str = price_str.replace('INR ', '').replace(',', '')
        try:
            return float(price_str)
        except ValueError:
            return None
    return None

def clean_ram(ram_str):
    if isinstance(...",8,"The generated code is generally correct and solves the task. It properly loads the CSV file, cleans the 'Launched Price (India)' and 'RAM' columns, filters the dataset for Apple phones with an Indian price less than 100,000, selects the specified columns, and sorts the resulting data by the cleaned 'Launched Price (India)' in ascending order. The code is well-structured, and the use of functions to clean the price and RAM is a good practice. However, there are a few areas for improvement to achieve a perfect score. Firstly, the code could benefit from more comprehensive error handling, such as catching specific exceptions that may occur during the file loading and data cleaning process. Secondly, the code could be more robust by adding checks for potential edge cases, such as an empty dataset or missing columns. Additionally, the code could be improved by including more descriptive variable names and comments to enhance readability and understandability. Lastly, the code does not handle the case where the 'Launched Price (India)_cleaned' column is entirely NaN, which might lead to an empty result set. Overall, the code is strong, and with a few tweaks, it can be even more robust and efficient.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: 'mobiles_2025.csv' not found. Please ensure the CSV file is in the same directory."")
    exit()

# 1. Clean the 'RAM' column
def clean_ram(ram_str):
    if isinstance(ram_str, str):
        try:
            return float(ram_str.replace('GB', '').strip())
        except ValueError:
            return np.nan
 ...",8,"The generated code is mostly correct and solves the task with some minor adjustments needed for better alignment with the ground truth code. It correctly loads the dataset, cleans the 'RAM' and 'Battery Capacity' columns, creates a scatter plot to visualize the relationship between these two variables, and colors the points according to the 'Company Name' column. The code structure is clear, and it handles potential errors such as the file not found exception. However, there are some differences in the approach, such as using a function to clean the columns and manually coloring the points in the scatter plot instead of using seaborn's functionality. Additionally, the code does not include a check for the 'Company Name' column to ensure it exists before attempting to color the points, which could lead to an error if the column is missing. Overall, the code is well-structured, but a few adjustments could make it more comprehensive and aligned with the ground truth code.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 5 rows
df = pd.read_csv('titanic.csv')
print(""First 5 rows of the dataset:"")
print(df.head())
print(""\n"" + ""=""*50 + ""\n"")

# 2) Show the shape of the dataset (rows and columns)
print(""Shape of the dataset (rows, columns):"")
print(df.shape)
print(""\n"" + ""=""*50 + ""\n"")

# 3) List all column names and their data types
print(""Column names and their data types:"")
print(df.info())
print(""\n"" + ""=""*50 + ""\n"")

# 4) Calculate the...",8,"The generated code is mostly correct and completes the task of performing basic exploratory data analysis on the titanic.csv dataset. It properly loads the dataset, displays the first 5 rows, shows the shape of the dataset, lists column names and their data types, calculates the survival rate, and identifies missing values correctly. However, there are a few minor issues that prevent it from receiving a perfect score. For instance, the generated code does not handle the case where the 'titanic.csv' file is not found in the current directory, which could lead to errors. Additionally, the survival rate calculation in the generated code uses the mean of the 'Survived' column instead of counting the number of 1s (representing survivors) and dividing by the total number of passengers. With these minor adjustments, the code could achieve a higher score.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 1) List all column names and their data types
print(""Column Names and Data Types:"")
print(df.info())

# 2) Calculate and report basic descriptive statistics
print(""\nDescriptive Statistics for Numerical Columns:"")
for col in ['Year', 'Rating', 'Runtime']:
    if pd.api.types.is_numeric_dtype(df[col]): # Ensure the column is numeric
        print(f""\n--- {col} ---"")
        print(f""Mean: {df[col].mean()}"")
 ...",8,"The generated code is mostly correct and Complete, as it solves the task by loading the dataset, listing column names and data types, calculating descriptive statistics for the specified numerical columns, and reporting the total number of movies. The code quality is also good, with proper imports and a clear structure. However, there are a few areas for improvement to achieve a perfect score. Firstly, the code lacks error handling, which is essential for a robust solution. It does not handle potential exceptions that may occur during the file loading process, such as a `FileNotFoundError`. Secondly, the code does not preprocess the 'Runtime' column, which may be in a string format and needs to be converted to a numeric type for statistical calculations. Lastly, the code could benefit from more descriptive variable names and comments to enhance readability and maintainability. Overall, the code is a good start, and with these minor improvements, it can be considered almost perfect.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Filter for movies with Rating >= 9.0
filtered_df = df[df['Rating'] >= 9.0]

# Select 'Title' and 'Rating' columns and sort by 'Rating' in descending order
result = filtered_df[['Title', 'Rating']].sort_values(by='Rating', ascending=False)

# Display the result
print(result)
```",8,"The generated code is overall correct and effective in solving the task. It properly loads the CSV dataset, filters movies with a `Rating` greater than or equal to 9.0, selects the `Title` and `Rating` columns, and sorts the result by `Rating` in descending order. The code quality is good, with proper imports and a straightforward structure. However, to achieve a perfect score, the code should include error handling to manage potential exceptions such as a `FileNotFoundError` if the CSV file is not found, and a `KeyError` if the expected columns are missing from the dataset. Additionally, including a try-except block to catch any other unexpected errors during processing would make the code more robust.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 1. Create a histogram for 'Rating'
plt.figure(figsize=(10, 6))
sns.histplot(df['Rating'], bins=10, kde=True)
plt.title('Distribution of Movie Ratings (IMDb Top 250)')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.show()

# 2. Create a scatter plot for 'Rating' vs 'Runtime'
plt.figure(figsize=(12, 7))
sns.scatterplot(x=df...",8,"The generated code is mostly correct and effectively solves the task. It properly loads the dataset, creates a histogram for the 'Rating' column, and generates a scatter plot for the relationship between 'Rating' and 'Runtime'. The code is well-structured and easy to understand, with proper imports and a clear separation of concerns. However, there are a few areas for improvement. Firstly, the code does not handle potential errors, such as a missing 'Runtime' column in the dataset. Secondly, the code does not provide as much detail in the plot titles and axis labels as the ground truth code, which makes the plots slightly less informative. Lastly, the code could benefit from more consistent styling, such as using the same alpha value for the grid in both plots. Overall, the generated code is strong, but with a few minor tweaks, it could be even more robust and effective.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# 1) Report the number of missing values (NA) for each column
missing_values = df.isnull().sum()
print(""Number of missing values per column:"")
print(missing_values)
print(""\n"")

# 2) For the numerical columns culmen_length_mm, culmen_depth_mm, flipper_length_mm, and body_mass_g,
# first drop any rows where *any* of these four specific columns have a missing value.
# Then, calculate and report basic descriptive st...",8,"The generated code effectively solves the task by loading the penguins.csv dataset using pandas, reporting the number of missing values per column, filtering out rows with missing values in specific numerical columns, calculating descriptive statistics for these columns, and counting unique values in the 'species' and 'island' columns. The code is well-structured, and imports are proper. However, to achieve a perfect score, it should include error handling for file loading, provide more comprehensive descriptive statistics (e.g., using `.agg()` for multiple statistics at once), and offer a more detailed report, such as the original number of rows before filtering and the number of rows after filtering. Additionally, the code could benefit from comments explaining the purpose of each section and the logic behind the filtering and statistical calculations. Overall, the code is clear and effective but lacks some enhancements for robustness and detail.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the dataset
filtered_df = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')]

# Select specific columns
selected_columns_df = filtered_df[['culmen_length_mm', 'culmen_depth_mm']]

# Drop rows with missing values in the selected columns
cleaned_df = selected_columns_df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm'])

# Display the first 5 rows
print(cleaned_df.head())
```",8,"The generated code is mostly correct and solves the task, correctly filtering the dataset, selecting the specified columns, dropping rows with missing values, and displaying the first 5 rows. However, it lacks error handling for cases where the CSV file is not found or other exceptions occur during loading. Additionally, it does not provide any feedback or messages to the user if the file is not found or if the filtering process does not yield any results. The code structure is clean and follows good practices, but including some basic error handling and user feedback would improve its quality and user experience.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows with any missing values in the relevant columns
df_cleaned = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])

# Create the scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=df_cleaned,
    x='culmen_length_mm',
    y='culmen_depth_mm',
    hue='species',
    palette='viridis',
    s=100,  # size of points
    a...",8,"The generated code is overall correct and solves the task. It properly loads the dataset, filters out rows with missing values, and generates a scatter plot colored by species. The code is well-structured, and imports are proper. However, to achieve a perfect score, a few improvements can be suggested. Firstly, the code could benefit from a more informative error message when the file is not found, similar to the ground truth code. Additionally, the code lacks a check to ensure data remains after filtering out missing values, which is a good practice to handle potential edge cases. Furthermore, the figure size and layout adjustments could be more customized for better visualization. Despite these minor suggestions, the code is clear, concise, and effectively accomplishes the required tasks.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

def analyze_penguins_data(file_path='penguins.csv'):
    """"""
    Loads the penguins.csv dataset, cleans specified numerical columns,
    calculates and prints descriptive statistics, and dataset dimensions.

    Args:
        file_path (str): The path to the penguins.csv file.
    """"""
    try:
        # 1) Read the dataset
        df = pd.read_csv(file_path)

        # Columns to clean and analyze
        numerical_cols = ['culmen_length_mm', 'culmen_depth_mm']

  ...",9,"The generated code is very close to meeting all the requirements and is well-structured. It correctly loads the penguins.csv dataset, fills NA values in the specified numerical columns with their means, calculates and prints the mean, median, and standard deviation for these columns, and reports the dataset dimensions. The code is also properly commented, and error handling is implemented to handle the case where the dataset file is not found. One of the areas for minor improvement is that the generated code does not provide as detailed a summary of the NA imputation process as the ground truth code does, but this does not significantly impact the overall functionality. Additionally, the generated code uses a try-except block to catch any unexpected errors, which is a good practice but may mask the specific error message that could be useful for debugging. Overall, the code is clear, concise, and effectively solves the task.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'penguins.csv' not found. Make sure the CSV file is in the current directory."")
    exit()

# 2) Drop rows where flipper_length_mm is NA
df_filtered = df.dropna(subset=['flipper_length_mm'])

# 3) Create a histogram of the flipper_length_mm column
plt.figure(figsize=(10, 6))
sns.histplot(df_filtered['flipp...",8,"The generated code is generally correct and completes the task as requested. It properly loads the dataset, drops rows with NA values in the `flipper_length_mm` column, creates a histogram for the distribution of flipper length, and generates a bar chart for penguin counts by island. However, there are a few areas that could be improved to enhance code quality and completeness:

1. **Error Handling**: While the generated code handles the `FileNotFoundError` by exiting the script, it lacks comprehensive error handling. The ground truth code provides more robust error handling for loading the dataset, which is a good practice.

2. **Code Structure and Comments**: The generated code could benefit from additional comments explaining the purpose of each section and the reasoning behind certain choices (e.g., the choice of figure sizes, the use of seaborn for plotting). The ground truth code includes these elements, making it easier to understand and follow.

3. **Seaborn Theme**: The generated code does not set a professional style for the plots using seaborn, as seen in the ground truth code. This addition would improve the visual appeal and consistency of the generated plots.

4. **Grid and Layout**: While the generated code includes a grid for the histogram, it could also benefit from adjusting the layout to prevent overlapping of labels, similar to the ground truth code's use of `plt.tight_layout()`.

5. **BAR Chart Implementation**: The generated code uses `sns.barplot` directly, which is correct but could be improved by using `sns.countplot` as in the ground truth code for a more straightforward count plot.

Overall, the generated code demonstrates a good understanding of the task requirements but could be enhanced with more comprehensive error handling, additional comments, and a few cosmetic improvements to the plotting code.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: penguins.csv not found. Make sure the file is in the current directory."")
    exit()

# 2) Filter the data to include only penguins from the 'Torgersen' island
torgersen_penguins = df[df['island'] == 'Torgersen']

# 3) For this filtered subset, count and print the number of penguins belonging to each species
species_counts = torgersen_penguins['species'].valu...",7,"The generated code is mostly correct and solves the task with some minor omissions and potential improvements. Here's a breakdown of its strengths and weaknesses:

1. **Correctness**: The code correctly loads the dataset, filters the data for penguins from the 'Torgersen' island, and counts the number of penguins belonging to each species. However, it lacks error handling for cases where the 'island' or 'species' columns are missing from the dataset, which is handled in the ground truth code.

2. **Code quality**: The code is straightforward and easy to follow. It properly imports the pandas library and structures the code into logical steps. However, it could benefit from more comprehensive error handling and checks for the existence of required columns.

3. **Completeness**: The code addresses the main requirements but misses some edge cases, such as handling the situation where no penguins are found on 'Torgersen' island or when the 'species' column is missing from the filtered dataset. It also lacks the initial success message for loading the dataset and a clear header for the species counts, making the output slightly less informative than the ground truth code.

To improve the code and achieve a higher score, it's recommended to enhance error handling, add checks for the existence of required columns, and include more detailed output to make the results clearer. Specifically, adding try-except blocks for operations that could potentially raise errors (like checking for the existence of columns) and including messages for different scenarios (like no penguins found on 'Torgersen' island) would make the code more robust and user-friendly.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# 1) Print the shape of the DataFrame
print(""Shape of the...",9,"The generated code scores high because it correctly defines the DataFrame, prints its shape, and lists the data types of all columns, which are the main requirements of the task. The code structure is clear, and it properly imports the necessary libraries. The only minor improvement that could be suggested is adding a separator between the shape and data types output for better readability, similar to the ground truth code. Additionally, including a brief comment explaining the purpose of each section of the code could enhance its readability and maintainability. Overall, the code is well-organized, and its functionality matches the requirements of the task.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('titanic.csv')

# 1. Bar chart: Count of survivors vs non-survivors
plt.figure(figsize=(6, 4))
df['Survived'].value_counts().plot(kind='bar', color=['skyblue', 'lightcoral'])
plt.title('Count of Survivors vs Non-Survivors')
plt.xlabel('Survived')
plt.ylabel('Count')
plt.xticks(ticks=[0, 1], labels=['Non-Survived', 'Survived'], rotation=0)
plt.tight_layout()
plt.show()

# 2. Bar chart: Distribution ...",8,"The generated code effectively solves the task by creating the required visualizations for the titanic.csv dataset. It demonstrates good code quality with proper imports and a clean structure. The code addresses all the requirements, including creating bar charts for survivors vs non-survivors, passenger classes, and passengers by gender, as well as a histogram for passenger ages with proper handling of missing values. However, there are a few minor areas for improvement to achieve a higher score. For instance, the code could benefit from more descriptive variable names and comments to enhance readability and understanding. Additionally, some minor adjustments in the plot customization (e.g., grid visibility, axis label clarity) could further refine the visualizations. Overall, the code is well-organized and effectively communicates the required insights from the dataset.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]}
df = pd.DataFrame(data)

# 1) Calculate and print the average 'Mid...",8,"The generated code is largely correct and complete, solving the task as required. It properly imports the necessary library, creates the DataFrame, calculates and prints the average 'Midterm_Score' and 'Final_Score' for all students, and identifies and prints the 'Subject' with the highest average 'Final_Score'. The code structure is clear, and it uses the `mean()` method correctly for calculations. However, to achieve a perfect score, it could benefit from a few minor improvements, such as adding a comment to explain the purpose of each section of the code and including error handling for potential edge cases like an empty DataFrame. Additionally, printing the highest average score value for the subject with the highest average final score, as done in the ground truth code, would make the output more informative.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
        'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
        'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
        ...",9,"The generated code is very close to the ground truth code and correctly solves the task. It properly creates the DataFrame with 'Date' as the index, calculates the 'Daily_Range', finds the date with the highest 'Close' price, and prints the updated DataFrame. The code is well-structured, and the imports are correct. The only minor deviation from the ground truth code is the absence of the comment that using `inplace=True` modifies the DataFrame directly, but this does not affect the functionality of the code. To achieve a perfect score, the code could benefit from additional comments explaining the purpose of each section and the calculations being performed. Overall, the code is clear, concise, and effectively solves the given task.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Data for the DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CU...",9,"The generated code is highly accurate and effectively solves the task. It correctly creates the DataFrame, calculates the 'Total_Revenue' for each transaction, identifies the best-selling product by quantity, and prints the first 5 rows of the DataFrame including the new 'Total_Revenue' column. The code quality is good, with proper imports and a straightforward structure. The only minor area for improvement is that the code could benefit from adding a comment or two to explain the purpose of each section, particularly for someone not familiar with the task. Additionally, the variable name `most_sold_product` could be more descriptive, such as `best_selling_product`, to maintain consistency with the ground truth code. Overall, the generated code is strong and effectively accomplishes the required tasks.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 1) Calculate and print the average ""SessionDurationMinutes""
average_session_duration = df[""SessionDura...",9,"The generated code is very close to the ground truth code and correctly defines the DataFrame, calculates the average session duration, and counts the users by device type. The only minor differences are in the print statements, where the generated code does not include a separator line and does not explicitly label the output as ""1. Average Session Duration"" and ""2. Number of users per Device Type"" like the ground truth code. Additionally, the generated code could benefit from comments to explain the purpose of each section, especially for someone not familiar with the code. The structure is clear, and the imports are proper. Overall, the code is well-organized, and the required tasks are completed accurately. With a few minor adjustments for clarity and formatting, the generated code could achieve a perfect score.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

# Create the pandas DataFrame
df = pd.DataFrame(data)

# 1) Find an...",9,"The generated code is very close to the ground truth code and accurately solves the task. It correctly defines the DataFrame and finds the maximum temperature and minimum humidity, printing the results as required. The code is well-structured, and the use of pandas is proper for data manipulation and analysis. The only minor area for improvement is the lack of explicit comments or section headers to further explain the data analysis steps, but this is a minor criticism given the overall quality and correctness of the code. The code is complete in terms of addressing all requirements, and the structure is clear and easy to follow. With minor adjustments for comments and readability, this code could achieve a perfect score.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}

# Create the pandas DataFrame
df = pd.DataFrame(data)

# 1) Print the total number of ""Likes"" across all posts
total_...",9,"The generated code is very close to the ground truth code and accurately solves the task. It correctly defines the pandas DataFrame, calculates the total number of ""Likes"" across all posts, and identifies the ""PostType"" with the highest average number of ""Shares"". The code is also well-structured and readable. The only minor suggestion for improvement is to add optional print statements to verify the creation of the DataFrame, as seen in the ground truth code, which can be helpful for debugging purposes. Additionally, the code does not handle potential errors, such as an empty DataFrame, which could be considered in a more robust implementation. Overall, the generated code is of high quality and effectively addresses all requirements.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male""...",9,"The generated code is very close to the ground truth code and correctly defines the DataFrame, calculates the average age, and counts patients by gender. The code is well-structured, and the use of pandas library is proper. The only minor issue is that the generated code is missing the printing of the units (""years"") when displaying the average age, which is present in the ground truth code. Additionally, the print statement for the number of patients by gender is slightly mismatched in the wording compared to the ground truth code. Overall, the code is clear, concise, and effectively solves the task. With a few minor adjustments, it could be almost identical to the ground truth code.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}
df = pd.DataFrame(data)

# 1) Print the ""Title"" of the movie with the highest ""Rating"".
highest_rating_mov...",9,"The generated code is very close to the ground truth code and correctly solves the task. It properly defines the pandas DataFrame, identifies the highest-rated movie by title, and calculates the average release year. The code is well-structured and concise, using efficient pandas methods such as `idxmax()` and `mean()`. The only slight improvement that could be made is to handle the case where there are multiple movies with the same highest rating, as the ground truth code does. Additionally, the generated code could benefit from including some basic error handling, such as checking if the DataFrame is empty before attempting to access its columns. Overall, the code is solid and effectively addresses all requirements.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

# Create the pandas DataFrame
df = pd.DataFrame(data)

# ...",9,"The generated code is very close to the ground truth code in terms of correctness, code quality, and completeness. It correctly defines the DataFrame, prints the total number of cars, and calculates the average ""PriceUSD"" of all cars. The code is well-structured, and proper imports are used. The only minor differences are in the formatting of the average price output, where the ground truth code uses `:.2f` to format the price to two decimal places, while the generated code uses `:,.2f` which includes commas as thousand separators. Additionally, the generated code uses `len(df)` to get the total number of cars, which is a more straightforward approach than `df.shape[0]` used in the ground truth code. Overall, the generated code is clear, concise, and effectively solves the task.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}
df = pd.DataFrame(data)

# 1) Print the name of the Pokemon with the highest ""HP""...",9,"The generated code is very close to the ground truth code and effectively solves the task. It correctly defines the DataFrame, identifies, and prints the names of the Pokémon with the highest HP and lowest Defense. The code structure is clear and concise, and it properly imports the necessary pandas library. The only minor improvement that could be made is adding some comments to explain the purpose of each section of the code, making it slightly more readable and understandable for someone else. Additionally, the generated code does not include any error handling, which could be useful in case the input data is not as expected. However, overall, the code is well-structured, and its functionality matches the requirements of the task.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('titanic.csv')

# Identify columns with missing values
missing_values_before = df.isnull().sum()

# 2) Fill missing Age values with the median age
median_age = df['Age'].median()
df['Age'].fillna(median_age, inplace=True)

# 3) Fill missing Embarked values with the most frequent port
most_frequent_embarked = df['Embarked'].mode()[0]
df['Embarked'].fillna(most_frequent_embarked, inplace=True)

# 4) Drop the Cabin column due to exc...",6,"The generated code is largely correct and addresses most of the requirements, such as loading the dataset, filling missing values, dropping the Cabin column, and creating a new 'Title' column. However, there are a few areas where it falls short of the ground truth code. First, the identification of columns with missing values is done but not displayed, which is a minor oversight. More significantly, the extraction of the 'Title' from the 'Name' column is done using a different method than the ground truth code, which might not be as accurate or robust. The generated code uses a simpler string splitting approach, which may not handle all possible name formats. Additionally, the code is missing the standardization of less common titles, which is a good practice for feature engineering. To improve, the code should incorporate a more sophisticated method for title extraction and consider adding title standardization. Overall, the code is a good start, but some refinements are needed to match the ground truth code's level of completeness and accuracy.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}
df = pd.DataFrame(data)

# 1) Print the ""T...",9,"The generated code accurately defines the DataFrame as specified in the task and successfully identifies the most expensive book by title and calculates the total stock. The code structure is clear, and it properly uses pandas for data manipulation. The only minor improvement could be adding a check for an empty DataFrame to handle potential edge cases, as shown in the ground truth code. However, this is a typical consideration and not strictly necessary for this specific dataset. Overall, the code is concise, readable, and effectively solves the problem posed by the task.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('water_potability.csv')

# 1) Display the first 10 rows
print(""First 10 rows of the dataset:"")
print(df.head(10))
print(""\n"" + ""=""*50 + ""\n"")

# 2) Show the shape of the dataset (rows and columns)
print(""Shape of the dataset (rows, columns):"")
print(df.shape)
print(""\n"" + ""=""*50 + ""\n"")

# 3) List all column names and their data types
print(""Column names and their data types:"")
print(df.info())
print(""\n"" + ""=""*50 + ""\n"")

# 4) C...",8,"The generated code successfully performs the tasks outlined in the benchmark, including loading the dataset, displaying the first 10 rows, showing the shape of the dataset, listing column names and their data types, calculating the percentage of potable vs non-potable water samples, displaying the count of missing values for each column, and showing basic summary statistics for all numerical columns. The code is well-structured, and the use of pandas functions like `head()`, `shape`, `info()`, `value_counts()`, `isnull().sum()`, and `describe()` is appropriate. However, to achieve a score of 10, the code could benefit from additional error handling, such as checking if the 'Potability' column exists before attempting to calculate the percentage of potable vs non-potable water samples, and handling potential exceptions that may occur during the loading of the dataset. Additionally, the code could include more detailed comments or documentation to explain its purpose and the logic behind each section.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Store initial statistics
initial_shape = df.shape
initial_missing_counts = df.isnull().sum()

# 1) Identify columns with missing values
columns_with_missing_values = df.columns[df.isnull().any()].tolist()

# 2) Fill missing values using appropriate strategies (mean for numerical columns)
for col in columns_with_missing_values:
    if pd.api.types.is_numeric_dtype(df[col]):
        df[...",8,"The generated code addresses the majority of the requirements for the data cleaning task, including loading the dataset, identifying and filling missing values, removing duplicates, and verifying data ranges. It also provides a comprehensive summary of the cleaning process and displays the first 5 rows of the cleaned dataset. However, there are a few areas for improvement to achieve a perfect score. Firstly, the code does not handle the case where the 'water_potability.csv' file is not found in the current directory, which is addressed in the ground truth code. Secondly, the code uses arbitrary value ranges for water quality parameters, which may not be accurate or realistic. It would be better to use known, established ranges for each parameter. Finally, the code could benefit from more detailed comments and documentation to explain the reasoning behind certain steps and the cleaning process as a whole. Overall, the code is well-structured and effective, but with a few adjustments, it could be even more robust and reliable.",gemini_2.5_flash_preview_5_20,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('water_potability.csv')
except FileNotFoundError:
    print(""Error: water_potability.csv not found. Make sure the file is in the current directory."")
    exit()

# Handle missing values (if any) - A common approach for this dataset
# Filling NaNs with the mean of their respective columns
for col in ['ph', 'Hardness', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_...",8,"The generated code is mostly correct and effectively solves the task. It creates the required visualizations, including a bar chart, histograms, a boxplot, and a correlation heatmap, which demonstrates a good understanding of the problem. The code structure is clear, and the use of pandas, matplotlib, and seaborn is appropriate for data visualization and manipulation.

However, there are a few areas for improvement to achieve a higher score. Firstly, the generated code does not handle missing values in the same way as the ground truth code; it fills NaNs with the mean of their respective columns, which might not be the most appropriate strategy for all datasets. The ground truth code handles missing values in plots by using `seaborn's histplot` and `boxplot` functions, which automatically ignore NaNs.

Secondly, the generated code does not fully match the ground truth code in terms of plot titles, axis labels, and legends. While the titles and labels are generally correct, some minor variations in wording or formatting might be desirable for consistency.

Lastly, the generated code uses a different palette in the boxplot (`'pastel'`) compared to the ground truth code (`'coolwarm'`), which might affect the visual appearance of the plot. Using a consistent palette throughout the code can enhance the overall visual cohesion.

To achieve a perfect score, the generated code should be revised to address these minor issues and ensure consistency with the ground truth code.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.linear_model import LinearRegression

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Separate features (X) and target (y)
X = df.drop('quality', axis=1)
y = df['quality']

# 1. Create at least thre...",9,"The generated code effectively solves the task by creating new meaningful features, applying two different feature selection techniques (RFE with RandomForestRegressor and SelectKBest with f_regression), comparing and contrasting the selected feature sets, and displaying the final selected features with their importance scores or rankings. The code is well-structured and properly imports the necessary libraries. It also handles potential issues such as division by zero andNaN/inf values. The only areas for improvement are minor: the code could benefit from more detailed comments explaining the purpose of each section and the reasoning behind the feature engineering and selection decisions. Additionally, the code could be made more modular by breaking it down into separate functions for feature engineering, feature selection, and comparison. However, these are minor suggestions, and the code is overall of high quality and effectively addresses all requirements.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, Votin...",8,"The generated code demonstrates a good understanding of the task and implements most of the required ensemble methods, hyperparameter tuning, and evaluations. It correctly loads and preprocesses the dataset, trains and evaluates various models, including a voting ensemble and a stacking ensemble with a linear regression meta-learner. The code also performs feature importance plots for some models and residual analysis. However, there are a few areas for improvement to achieve a perfect score. Firstly, the code could benefit from more detailed comments and docstrings to explain the purpose and logic behind different sections, making it easier for others to understand. Secondly, some of the variable names could be more descriptive, and the code could be slightly more organized, with functions for specific tasks to improve readability. Lastly, while the code handles some potential issues like missing values, it could be more robust by adding error handling for cases like file not found, unsuccessful model training, etc. Overall, the code is well-structured, and the tasks are completed effectively, but with a bit of refinement, it could be even more robust and easier to maintain.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
import statsmodels.api as sm
from sklearn.linear_model import Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.model_selection import TimeSeriesSplit
import pymc3 as pm
import arviz as az
from ruptures import Binseg
from lifelines import KaplanMeierFitter, AftRegrHazard
from sklearn.metrics import mean...",6,"The generated code attempts to solve the task but has several issues that prevent it from being a high-quality solution. 

Correctness is partially addressed, as the code implements various models such as ARIMA, multivariate regression, polynomial regression, Bayesian regression, and survival analysis. However, the implementation is incomplete, and some aspects, like change point detection and time series cross-validation, are not thoroughly executed.

Code quality is decent, with proper imports and a attempts at structuring the code. However, the organization could be improved, and there are some redundant or inefficient sections. 

Compared to the ground truth code, the generated code lacks completeness in several areas. For instance, the change point detection using Ruptures is not fully implemented, and the survival analysis is highly simplified and not accurately representative of game longevity. Furthermore, the time series cross-validation is not adequately performed, especially for the ARIMA model.

To improve the score, the code should be thoroughly reviewed to address these gaps. This includes completing the change point detection, enhancing the survival analysis to accurately model game longevity, and properly implementing time series cross-validation for all models. Additionally, code organization, redundancy reduction, and efficiency improvements are necessary to enhance overall quality.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.manifold import TSNE
import umap
from scipy.cluster.hierarchy import dendrogram, linkage
from gap_statistic import OptimalK
from sklearn.neighbors import NearestNeighbors
...",8,"The generated code demonstrates a strong understanding of the task and successfully implements advanced clustering and market segmentation analysis on the video_games_sales.csv dataset. It properly loads and preprocesses the dataset, creates features like 'Regional_Sales_Distribution', 'Market_Position', and 'Publisher_Portfolio_Size', and implements multiple clustering algorithms (K-Means, DBSCAN, Hierarchical Clustering, and Gaussian Mixture Models). The code also uses advanced techniques like t-SNE and UMAP for dimensionality reduction and visualization, determines the optimal number of clusters using silhouette analysis, elbow method, and gap statistic, and performs consensus clustering to identify stable clusters across different algorithms. Additionally, it creates detailed cluster profiles with statistical significance testing, implements anomaly detection using Isolation Forest, and uses association rule mining to find patterns between game attributes.

However, there are a few areas for improvement. The code could benefit from more comprehensive error handling and logging to ensure that issues are caught and reported appropriately. Furthermore, some sections of the code, such as the association rule mining and recommendation system, could be more concise and efficient. Finally, the code could benefit from more comments and documentation to explain the purpose and functionality of each section, making it easier for others to understand and maintain. Overall, the code is well-structured and effective, but with some refinements, it could be even more robust and efficient.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold
from sklearn.preprocessing import StandardScaler, QuantileTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, brier_score_loss, roc_curve, calibration_curve
from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostCla...",6,"The generated code attempts to solve the task by implementing advanced ensemble learning and model optimization for diabetes prediction. It covers several aspects including loading and preprocessing the dataset, implementing various ensemble methods (Voting Classifier, Bagging, Boosting, Stacking), hyperparameter tuning using Optuna, and model interpretation using SHAP values. However, there are some areas that need improvement:

1. **Incomplete Implementation**: The code seems to be incomplete in certain areas, such as the implementation of custom ensemble methods with dynamic weight assignment and the model calibration section, which lacks detailed calibration plots and reliability analysis.

2. **Lack of Nested Cross-Validation**: The code performs hyperparameter tuning using Optuna but does not fully utilize nested cross-validation for unbiased model evaluation, which is crucial for ensuring the reliability of the models.

3. **Inefficient Model Interpretation**: The SHAP interpretation is performed but could be more comprehensive, including dependence plots for more features and a clearer explanation of the importance of each feature.

4. **Code Organization and Structure**: The code could benefit from better organization and structure, especially in the model implementation and interpretation sections, to improve readability and maintainability.

5. **Missing Diversity Analysis**: The ensemble diversity analysis is mentioned but not fully implemented, which is important for understanding how diverse the ensemble members are and how they contribute to the overall performance.

To achieve a higher score, the code should address these areas, ensuring that all requirements are fully met, and the implementation is more comprehensive, efficient, and organized.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import pymc as pm
import arviz as az
import aesara.tensor as at
from sklearn.preprocessing import StandardScaler, QuantileTransformer
from sklearn.impute import SimpleImputer
import lifelines
from sklearn.linear_model import LogisticRegression
from causalinference import CausalModel
from causalinference.match import Match
from statsmodels.sandbox.regression.gmm import IV2SLS
import scipy.stats as stats

# 1. Load and preprocess the dataset with co...",8,"The generated code is well-structured and addresses most of the requirements for advanced statistical modeling and Bayesian analysis on the diabetes dataset. It correctly implements hierarchical Bayesian models, Gaussian processes, causal inference methods (though in a simplified manner), and performs Bayesian model comparison using WAIC and LOO cross-validation. The code also includes comprehensive missing value handling, implements MCMC sampling for posterior inference, and applies survival analysis techniques.

However, there are a few areas for improvement to achieve a higher score. Firstly, the code could benefit from more detailed comments and explanations, especially for the more complex sections such as the Gaussian process implementation and the causal inference methods. This would make the code easier to understand and follow for others.

Secondly, the code uses both PyMC and statsmodels for different aspects of the analysis. While this is not necessarily a problem, it would be more consistent to use a single library throughout the code. Furthermore, some functions and models (e.g., the IV2SLS estimation) are not properly implemented or are commented out, which reduces the reliability and completeness of the code.

Lastly, the code could benefit from more robust error handling and input validation, especially when reading in the dataset or performing file operations. This would make the code more robust and able to handle unexpected issues.

Overall, the generated code provides a good foundation for advanced statistical modeling and Bayesian analysis, but with some refinements and additional work, it could be even more effective and reliable.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE, UMAP
from sklearn.ens...",8,"The generated code is generally correct and addresses most of the requirements for advanced clustering and patient segmentation analysis on the diabetes.csv dataset. It implements multiple clustering algorithms, including K-Means, DBSCAN, Hierarchical Clustering, Gaussian Mixture Models, Spectral Clustering, and OPTICS, and performs dimensionality reduction using techniques like t-SNE, UMAP, and autoencoders. The code also includes consensus clustering, patient phenotyping, anomaly detection, association rule mining, and patient risk stratification. The structure is mostly clear, and the use of comments helps in understanding the purpose of each section. However, there are a few areas where improvements can be made for a higher score. Firstly, the code could benefit from more comprehensive error handling, especially when dealing with file operations and data loading. Secondly, some sections, such as the time-series clustering, are placeholders and would need to be fully implemented if the dataset were to be temporal. Lastly, the clinical interpretation section could be more detailed and nuanced, reflecting a deeper understanding of the diabetes dataset and its implications. Overall, the code is solid and demonstrates a good grasp of the necessary techniques, but with some refinements, it could be even more effective and comprehensive.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.preprocessing import StandardScaler, QuantileTransformer, LabelEncoder
from sklearn.ensemble import (
    VotingRegressor,
    BaggingRegressor,
    AdaBoostRegressor,
    GradientBoostingRegressor,
    RandomForestRegressor,
)
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, HuberRegressor
from sklearn.svm imp...",8,"The generated code is well-structured and covers most of the requirements specified in the task, including data preprocessing, implementation of various ensemble methods (voting regressor, bagging, stacking, and custom ensemble), hyperparameter tuning using Optuna, model interpretation using SHAP and LIME, and model calibration and uncertainty quantification. The code also includes a comprehensive analysis of ensemble diversity and model combination optimization. However, there are a few areas where improvements can be made to achieve a perfect score. Firstly, the code could benefit from more detailed comments and docstrings to explain the purpose and logic behind each section. Secondly, some of the model evaluation metrics are not consistently formatted, which can make it harder to compare results across different models. Lastly, the code could be further optimized for performance, particularly in the hyperparameter tuning and ensemble evaluation sections, where computationally expensive operations are performed. Overall, the generated code provides a solid foundation for solving the task, but with some refinements, it can be even more effective and efficient.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc as pm
import arviz as az
import bambi as bm
import statsmodels.api as sm
from statsmodels.sandbox.regression.gmm import IV2SLS
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors
import seaborn as sns
import matplotlib.pyplot as plt

# Set random seed for reproducibility
np.random.seed(42)

# Load the dataset
try:
    df = pd.read_csv('world-happiness-report-2021.csv')
except FileNotFoundE...",7,"The generated code demonstrates a good understanding of the task requirements and attempts to address each aspect, including loading and preprocessing the dataset, implementing hierarchical Bayesian models, structural equation models, causal inference, and Gaussian processes. However, there are a few areas that need improvement to achieve a higher score.

1. **Structural Equation Models (SEM):** The code attempts to use Bambi/PyMC for SEM, which is a correct approach. However, the example models provided are simplified and do not fully capture the complexity of SEM, which involves latent variables and multiple pathways. A more comprehensive implementation of SEM would enhance the code's score.

2. **Causal Inference with Instrumental Variables (IV):** The IV analysis is simulated due to the lack of a valid instrument in the real dataset. While this is acknowledged in the code, a more realistic example or discussion on how to identify or simulate IVs would strengthen the section.

3. **Completeness of Model Comparison:** The code compares models using LOO-CV and WAIC but does not explicitly compute Bayes factors, which could provide more nuanced comparisons between models. Adding this or discuss its limitations would make the code more comprehensive.

4. **Code Organization and Comments:** While the code is generally well-structured, more comments explaining the reasoning behind certain choices (e.g., the decision to use a particular distribution for a variable) and how the different sections relate to the overall analysis would improve readability and maintainability.

5. **Visualization and Results Interpretation:** The code includes some visualization but could benefit from more detailed explanations of what the results mean in the context of the problem. This would help users understand the implications of the findings and how they relate to the research questions.

Overall, the code provides a solid foundation for addressing the task's requirements but could be enhanced with more detailed implementations of certain methods, additional explanations, and a focus on realism and practicality in examples, especially for complex analyses like SEM and causal inference.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MiniBatchKMeans
from sklearn.mixture import GaussianMixture
from sklearn.cluster import SpectralClustering
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.manifold imp...",8,"The generated code has made significant progress in tackling the advanced clustering and country segmentation analysis task. It correctly loads and preprocesses the dataset, implements multiple clustering algorithms (K-Means, DBSCAN, Hierarchical Clustering, Gaussian Mixture Models, Spectral Clustering, and HDBSCAN), applies advanced dimensionality reduction techniques (t-SNE, UMAP, autoencoders, and manifold learning), and performs consensus clustering to identify robust country groupings across different algorithms. The code also includes detailed country profiles for each cluster with statistical significance testing and effect size analysis, implements anomaly detection, and applies network analysis to understand relationships between countries based on happiness factors.

However, there are a few areas that could be improved for a higher score:
1. **Code Organization**: The code is a bit lengthy and could benefit from being modularized into separate functions for each step of the analysis. This would improve readability and maintainability.
2. **Error Handling**: While the code attempts to handle some errors, it could be more robust in error handling, especially when dealing with external data files or library versions.
3. **Visualization**: The code hints at visualizing clustering results but does not fully implement it. Adding proper visualization with DR results and coloring by consensus cluster would enhance the understanding of the analysis outcomes.
4. **Recommendation System**: The recommendation system is a good start but could be refined, possibly by incorporating more features or using a more advanced recommendation algorithm.
5. **Association Rule Mining**: The association rule mining section could benefit from more parameter tuning and exploration of different mining algorithms to discover more relevant patterns.

Overall, the code demonstrates a good grasp of the required techniques and is asolid foundation for further development and refinement.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error
import numpy as np
import pmdarima as pm
import matplotlib.pyplot as plt

# 1) Load data, parse 'date', filter for 'Germany', and set 'date' as index.
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
df_germany = df[df['location'] == 'Germany'].copy()
df_germany.set_index('date', inplace=True)

# 2) Use 'new_cases' as the target variable. Conside...",8,"The generated code is generally correct and effectively solves the task of performing advanced time series forecasting using SARIMA for 'new_cases' in Germany. It properly loads the data, handles missing values, splits the data into training and testing sets, implements a SARIMA model with auto_arima to determine the orders, trains the model, forecasts the test set, and evaluates the forecast using RMSE. The code also plots the actual vs. forecasted values for the test period, which is a good practice for visualizing the model's performance.

However, there are a few areas that could be improved upon to achieve a higher score:

1. **Error Handling**: The code lacks robust error handling. For example, it does not check if the file 'synthetic_covid19_data.csv' exists before trying to read it. Adding try-except blocks to handle potential errors, such as file not found or unable to parse the 'date' column, would make the code more reliable.

2. **Data Preparation**: The code assumes that the data is already clean and does not require any further preprocessing. However, in real-world scenarios, data cleaning and preprocessing are crucial steps. The code could benefits from additional checks and handling for outliers, inconsistent data, or missing values that may not be handled by the current forward and backward filling approach.

3. **Model Selection and Tuning**: While the code uses auto_arima to determine the SARIMA orders, it does not provide any tuning or validation of the model. Consider using techniques like cross-validation to evaluate the model's performance on unseen data and hyperparameter tuning to find the best model parameters.

4. **Code Organization and Comments**: The code could be more organized with clear sections and comments explaining the purpose of each section. This would make it easier for others to understand the code and for the author to maintain it.

5. **Plot Customization**: The plot could be customized to make it more informative. For example, adding a title, labels, and a grid could make the plot more readable. Additionally, considering using different colors or linestyles to differentiate between the actual and forecasted values could improve the visual appeal of the plot.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.multicomp import pairwise_tukeyhsd

# Load the dataset
try:
    df = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: 'winequality-red.csv' not found. Please ensure the file is in the current directory."")
    exit()

# 1) Define 'low quality' wines (score <= 4), 'medium quality' (score 5-6), and 'high quality' (score >= 7)
def assign...",7,"The generated code demonstrates a good understanding of the task requirements and attempts to address each point. It correctly categorizes quality groups, performs statistical tests (ANOVA or Kruskal-Wallis based on normality assumptions), and conducts post-hoc tests (Tukey's HSD for ANOVA and a simplified approach for Kruskal-Wallis). The code also creates box plots for significant features and summarizes the results. However, there are areas for improvement to achieve a higher score.

1. **Import Statements**: The code is missing the import statement for `matplotlib` and `seaborn` is used but not explicitly imported for all its functionalities. Adding the necessary import statements for all libraries used would enhance code readability and maintainability.

2. **Error Handling**: While the code attempts to handle file not found errors, it lacks comprehensive error handling for other potential issues, such as data type mismatches or missing values that could affect statistical tests. Enhancing error handling mechanisms would make the code more robust.

3. **Code Structure and Comments**: The code could benefit from more comments explaining the purpose of each section, especially the statistical tests and post-hoc analyses. This would make the code easier to understand for others and for the author upon reviewing. Additionally, the code structure is generally clear, but some functions or methods could be defined to enhance modularity and reusability.

4. **Post-hoc Tests for Kruskal-Wallis**: The current implementation for post-hoc tests after Kruskal-Wallis is a simplified approach using pairwise Mann-Whitney U tests with Bonferroni correction. While this is a reasonable approximation, it does not fully capture the spirit of Dunn's test, which is more suitable for non-parametric tests. Implementing or mentioning Dunn's test would align the code more closely with best practices for statistical analysis.

5. **Summary and Visualization**: The code generates box plots for significant features, which is a good visualization approach. However, the summary of results could be more detailed, including perhaps a tabular summary of p-values and significant differences for easier comparison.

Addressing these points would further enhance the code's quality, completeness, and correctness, bringing it closer to the ground truth code's standards.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import xgboost as xgb
from sklearn.metrics import roc_auc_score, f1_score, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import shap

# Suppress SettingWithCopyWarning
pd.options.mode.chained_assignment = None

# ...",8,"The generated code generally addresses the task's requirements, demonstrating a good understanding of the problem. It correctly loads the data, parses the 'date', engineers the required features, handles missing values, selects features, encodes 'continent', scales numerical features, trains an XGBoost classifier with hyperparameter tuning, evaluates the model using ROC-AUC and F1-score, and uses SHAP for feature importance. The code is well-structured, and the use of comments explains the purpose of each section. However, there are a few areas for improvement to achieve a perfect score. Firstly, the code could benefit from more robust handling of missing values, especially in the engineered features. Secondly, the hyperparameter tuning could be more extensive, exploring a wider range of parameters. Lastly, the evaluation section could include more metrics to provide a comprehensive understanding of the model's performance. Despite these minor issues, the code is well-organized, and the author has demonstrated a strong grasp of the necessary techniques to solve the problem.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.formula.api as smf
from scipy.interpolate import interp1d

def advanced_statistical_modeling(file_path='synthetic_covid19_data.csv'):
    """"""
    Performs advanced statistical modeling to assess the impact of 'stringency_index'
    and 'people_fully_vaccinated' on 'reproduction_rate', considering 'location'
    and 'continent' effects.

    Args:
        file_path (str): Path to the synthetic_covid19_data.csv file.

    Returns:...",8,"The generated code is generally correct and solves the task by implementing a mixed-effects linear regression model to assess the impact of 'stringency_index' and 'people_fully_vaccinated' on 'reproduction_rate', considering 'location' and 'continent' effects. The code is well-structured, and it properly handles missing values, calculates 'vaccination_percentage', and fits the model using statsmodels.

However, there are some areas for improvement to achieve a higher score. Firstly, the code could benefit from more detailed error handling, especially when loading the data or fitting the model. It would be good to provide more informative error messages to help diagnose issues.

Secondly, the code could be more comprehensive in its interpretation of the model results. While it reports the fixed effects coefficients, standard errors, and p-values, it could also provide more insight into the significance and direction of these effects, as well as the variance components for the random effects.

Thirdly, the code assumes that the data is in a specific format and does not include any data validation or cleaning steps. In a real-world scenario, it would be essential to include these steps to ensure that the data is suitable for analysis.

Finally, the code could be more modular, with separate functions for each step of the analysis (e.g., data loading, missing value imputation, model fitting, and result interpretation). This would make the code more reusable and easier to maintain.

Overall, the generated code is a good start, but with some refinements, it could be even more effective and reliable.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

# 1) Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 2) Select the features and handle potential missing values
features = ['elixirCost', 'Win Rate', 'Usage']
df_selected = df[features].copy()
df_selected.dropna(inplace=True) # Drop rows with missing valu...",9,"The generated code is highly effective in solving the task of performing K-Means clustering on the clash_royale_cards.csv dataset, covering all the necessary steps from loading the dataset to visualizing the clusters using PCA. The code quality is excellent, with proper imports, a clear structure, and good use of comments (though there are no explicit comments in the code, the variable and function names are descriptive). 

The code correctly handles potential missing values by dropping rows with missing values in the selected features, which is a good practice. It also uses the Elbow Method and Silhouette Score to determine the appropriate number of clusters (K) for K-Means, providing a comprehensive analysis. 

One area for slight improvement is in the visualization part. The code could benefit from more detailed labels and titles in the plots, making them easier to understand at a glance. Additionally, the code could include more comments to explain the rationale behind certain steps, especially for readers who might not be familiar with the K-Means clustering and PCA process. 

Overall, the code is well-structured, readable, and effectively solves the given task, making it a strong candidate for the benchmark.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select features and target
features = ['elixirCost', 'Usage', 'rarity']
target = 'Win Rate'

X = df[features]
y...",9,"The generated code is very close to the ground truth code in terms of correctness, code quality, and completeness. It correctly loads the dataset, selects the features and target, preprocesses the 'rarity' column using one-hot encoding, splits the data into training and testing sets, trains a regression model (RandomForestRegressor), and evaluates the model using R-squared and MSE. The code structure is clear, and the use of a pipeline with preprocessing and model is a good practice. However, there are a couple of minor issues: (1) The generated code is missing the error handling for the case when the dataset file is not found, which is present in the ground truth code. (2) The generated code does not print the evaluation results in a formatted section like the ground truth code, which makes it slightly less readable. Overall, the generated code is very strong, and with these minor adjustments, it could be considered almost perfect.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy import stats

def analyze_win_rate_by_rarity(file_path='clash_royale_cards.csv'):
    """"""
    Performs an ANOVA test on the 'clash_royale_cards.csv' dataset to determine if there is a
    significant difference in mean 'Win Rate' across different card 'rarity' categories.

    Args:
        file_path (str): The path to the CSV dataset.
    """"""
    # 1) Load the dataset
    df = pd.read_csv(file_path)

    # 2) Identify the unique categories in the `rarity...",8,"The generated code effectively performs the required statistical test on the dataset to compare the mean 'Win Rate' across different card 'rarity' categories. It properly loads the dataset, identifies unique 'rarity' categories, performs an ANOVA test, states the null and alternative hypotheses, and interprets the p-value based on a significance level of alpha = 0.05. The code is well-structured, and the use of a function to encapsulate the analysis is a good practice. However, there are a few areas for improvement to achieve a perfect score. Firstly, the code does not handle potential errors that might occur when loading the dataset, such as a FileNotFoundError or other exceptions that could be encountered. Secondly, the code does not ensure that the 'Win Rate' column is numeric or handle missing values, which could lead to incorrect results. Lastly, the code could benefit from more detailed output, such as printing the number of observations in each 'rarity' category and the mean 'Win Rate' for each category, to provide a more comprehensive understanding of the results.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
df = pd.read_csv('iris.csv')

# Define features X and target y
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm...",9,"The generated code is very close to the ground truth code and correctly solves the task. It properly loads the dataset, preprocesses the features and target, splits the data, trains the three classification models, evaluates their performance using the required metrics, and identifies the best model based on the highest F1-score. The code structure is clear, and the use of dictionaries to store models and their performance metrics is efficient. The only minor issues are that the code does not include a try-except block to handle the case where the 'iris.csv' file is not found, and the model comparison section at the end is a bit redundant as the best model has already been identified earlier. Overall, the code is well-organized, and the only improvements needed are to add error handling and possibly remove the redundant model comparison section.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv('iris.csv')

# 2) Select the features
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
X = df[features]
y_true = df['Species']

# 3) Standardize these features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4) Apply ...",9,"The generated code is very close to the ground truth code and correctly solves the task of performing K-Means clustering and PCA on the iris dataset. It loads the dataset, selects and standardizes the features, applies K-Means clustering, performs PCA, and generates two scatter plots as required. The code is well-structured, and the imports are proper. The only minor deviation is the figure size in the plots, which is slightly different from the ground truth code. Additionally, the generated code uses `s=50` to set the marker size in the scatter plots, which is not present in the ground truth code. Overall, the code is of high quality, and the only improvements needed are to match the figure sizes exactly and consider adding the marker size parameter to match the ground truth code.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# 1) Load the dataset
df = pd.read_csv('iris.csv')

# 2) Create two new features
df['PetalArea'] = df['PetalLengthCm'] * df['PetalWidthCm']
df['SepalArea'] = df['SepalLengthCm'] * df['SepalWidthCm']

# 3) Define features X
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'PetalArea', 'Sep...",9,"The generated code is very close to the ground truth code and effectively solves the task. It correctly loads the dataset, creates the two new features (`PetalArea` and `SepalArea`), defines the features `X` and target `y`, encodes the target variable, trains a Gradient Boosting Classifier, performs 5-fold cross-validation, and reports the mean cross-validated accuracy score. The code quality is also good, with proper imports and a clear structure. The only minor issue is that the generated code does not print the individual cross-validation scores for each fold, which is included in the ground truth code for transparency. To achieve a perfect score, the code could be slightly modified to include this additional print statement, but overall, the generated code is strong and effective.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# 1) Load the dataset
try:
    df = pd.read_csv('heart.csv')
except FileNotFoundError:
    print(""Error: heart.csv no...",9,"The generated code is very close to the ground truth code in terms of correctness, code quality, and completeness. It correctly loads the dataset, defines the features and target, preprocesses the data using one-hot encoding for categorical features and standard scaling for numerical features, splits the data into training and testing sets, trains a RandomForestClassifier model, and evaluates the model on the test set using the required metrics. The code is well-structured, and the imports are proper. However, there are a few minor differences and areas for improvement. For instance, the generated code does not include a stratified split in the train_test_split function, which might lead to biased models if the classes are imbalanced. Additionally, it would be beneficial to include more print statements or logging to provide more feedback during the training and evaluation process, such as the model training progress or any potential errors that might occur. Nevertheless, the generated code is overall of high quality and effectively solves the task.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

# 1) Load the dataset
try:
    df = pd.read_csv('heart.csv')
except FileNotFoundError:
    print(""Error: heart.csv not found. Make sure the file is in the current directory."")
    exit()

# 2) Select the following numerical features for clustering
selected_features = ['...",8,"The generated code is generally correct and solves the task with minor adjustments needed for completeness and code quality. It properly loads the dataset, selects and standardizes the specified features, determines an appropriate number of clusters using the elbow method and silhouette scores, applies K-Means clustering, reduces dimensionality using PCA, and visualizes the clusters. However, there are a few areas for improvement to achieve a perfect score. Firstly, the code could benefit from more informative print statements or logging to track the progress and key findings, such as the chosen value of K and the explained variance ratio by the principal components. Secondly, the code assumes K=3 is chosen based on the plots, but in a real scenario, this decision should be more explicitly justified by comparing the WCSS and silhouette scores. Lastly, while the code is structured well, some comments could be added to explain the reasoning behind certain steps, especially for readers not familiar with the K-Means algorithm and PCA. Overall, the code is well-structured, and with these minor adjustments, it would be even more robust and easier to understand.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1) Create binary target variable
df['quality_binary'] = (df['quality'] >= 7).astype(int)
df = df.drop('quality', axis=1) # Drop original quality column

# Def...",8,"The generated code is overall well-structured and correctly solves the task of developing a binary classification model using the winequality-red.csv dataset. It properly creates a binary target variable, preprocesses the data, splits it into training and testing sets, trains a logistic regression model, evaluates its performance, and reports the most important features. The code quality is good, with proper imports and a clear structure. However, there are a few areas for improvement to achieve a perfect score. Firstly, the code could benefit from more detailed comments to explain the purpose of each section, especially for someone not familiar with the dataset or the task. Secondly, the code does not handle missing values, which, although not present in this dataset, is a good practice to include in case of future datasets with missing values. Lastly, the code could be made more robust by including error handling for cases like the dataset file not being found or the model not converging. Additionally, the classification report and feature importance are not formatted as nicely as in the ground truth code, but they still convey the necessary information. Overall, the code is strong, and with a few tweaks, it could be even more robust and user-friendly.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

# 1) Load the dataset
df = pd.read_csv('heart.csv')

# 2) Create a box plot
plt.figure(figsize=(8, 6))
sns.boxplot(x='HeartDisease', y='Oldpeak', data=df)
plt.title('Distribution of Oldpeak by Heart Disease Status')
plt.xlabel('Heart Disease (0: No, 1: Yes)')
plt.ylabel('Oldpeak')
plt.show()

# 3) Perform a logistic regression
X = df[['Oldpea...",8,"The generated code is mostly correct and addresses all the requirements of the task. It correctly loads the dataset, creates a box plot of `Oldpeak` grouped by `HeartDisease`, performs logistic regression with `Oldpeak` predicting `HeartDisease`, reports and interprets the coefficient, discretizes `Oldpeak` into three equally sized bins, and calculates the proportion of patients with `HeartDisease`=1 within each bin. The code is well-structured, and the use of proper imports and libraries is evident. However, there are a few minor issues that prevent it from scoring a 10. For instance, the code does not include any error handling for file loading, and it lacks detailed information about the dataset, such as the first few rows and summary statistics. Additionally, the interpretation of the logistic regression coefficient could be more detailed, explaining the conditions under which the odds ratio would indicate an increase or decrease in the risk of heart disease. Overall, the code is strong, but a few enhancements would make it more robust and informative.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# 1) Load data
try:
    df = pd.read_csv('spotify_2023.csv')
except FileNotFoundError:
    print(""Error: spotify_2023.csv not found. Please make sure the CSV file is in th...",8,"The generated code is largely correct and solves the task as required. It properly loads the data, converts the 'streams' column to numeric, creates a binary target variable 'is_popular', selects the specified features, one-hot encodes categorical features, splits the data into training and testing sets, trains a Logistic Regression model, and evaluates the model using F1-score (macro) and ROC AUC score. The code is well-structured, and the use of comments and print statements helps in understanding the code's flow. However, there are a few areas for improvement: (1) The generated code does not drop the original 'streams' column after converting it to numeric and removing NaN values. It would be better to rename the original column or explicitly drop it to avoid confusion. (2) The code uses `OneHotEncoder` from scikit-learn, which is not necessary since `pd.get_dummies` can also be used for one-hot encoding. (3) The generated code plots the confusion matrix, which is not requested in the task. While it can be helpful for visualizing the results, it is not necessary for the task at hand. (4) The code could benefit from more comments explaining the reasoning behind certain steps, such as the choice of 75th percentile for creating the 'is_popular' target variable. Overall, the code is strong, and with a few adjustments, it can be even more effective and easier to understand.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load data
try:
    df = pd.read_csv('spotify_2023.csv')
except FileNotFoundError:
    print(""Error: 'spotify_2023.csv' not found. Make sure the CSV file is in the same directory."")
    exit()

# 2) Select audio features and drop NaNs
audio_features = [
    'danceability_%', 'energy_%', 'valence_%', 'acousticn...",9,"The generated code is very close to the ground truth code and correctly solves the task. It properly imports necessary libraries, loads the data, selects and standardizes the audio features, applies K-Means clustering, reduces features using PCA, and visualizes the clusters. The code also calculates and displays the mean values of the original audio features for each cluster. The structure is clear, and the use of comments is minimal but sufficient. The primary area for improvement is in adding more detailed comments to explain the purpose of each section of the code, which would make it easier to understand for others. Additionally, the error handling in the data loading section is similar to the ground truth code, but it does not provide as much feedback about the number of rows dropped due to NaNs as the ground truth code does. Overall, the code is well-organized, and the requirements are adequately addressed, which justifies a high score.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1) Load data
try:
    df = pd.read_csv('spotify_2023.csv')
except FileNotFoundError:
    print(""Error: 'spotify_2023.csv' not found. Make sure the CSV file is in the same directory."")
    exit()

# 2) Convert 'streams' to numeric (coerce errors to NaN)
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# 3) Filter for songs with 'streams' greater than 10,000,000 and where 'released_year' is between 2010 and 2023, inclus...",8,"The generated code is generally correct and solves the task with some minor deviations from the ground truth code. It correctly loads the data, converts the 'streams' column to numeric, filters the data based on the specified conditions, groups the data by 'released_year', calculates the average audio features, and creates line plots for each feature. However, there are a few areas where the code can be improved to match the ground truth code more closely. 

Firstly, the code does not print out the shape of the dataset at various stages, which can be helpful for debugging and understanding the data. Secondly, the code uses a different approach to create the plots, using separate `plt.figure()` calls for each plot, whereas the ground truth code uses subplots to display all three plots in a single figure. This makes the generated code less concise and more difficult to compare the three plots directly. 

Additionally, the generated code does not include error handling for the case where the filtered dataset is empty after applying the filters, which is handled in the ground truth code. Lastly, the code uses a fixed color for the energy plot, whereas the ground truth code uses a style ('seaborn-v0_8-darkgrid') that automatically selects colors for the plots. 

Overall, the generated code is solid and effective, but with a few adjustments to match the ground truth code more closely, it could be even stronger.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder

# 1) Load the dataset
try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: youtube_trendings.csv not found. Please ensure the file is in the current directory."")
    exit()

# 2) For Likes, Dislikes, and Video views: r...",8,"The generated code is generally correct and solves the task as required. It properly loads the dataset, cleans the 'Likes', 'Dislikes', and 'Video views' columns, treats the 'published' column as a numeric feature, fills missing values in the 'Category' column, and applies one-hot encoding. The code also defines the features X and target y, splits the data into training and testing sets, trains a Linear Regression model, and evaluates the model on the test set, reporting the R-squared score and Mean Absolute Error. However, there are a few areas where the code can be improved for completeness and code quality. For instance, the generated code does not ensure that the cleaned columns are of numeric type after cleaning, which is explicitly done in the ground truth code. Additionally, the handling of the 'published' column is simplified and may not accurately represent the year, as it directly converts the 'published' column to datetime and extracts the year, which might not be the best approach for all formats of the 'published' date. Furthermore, the code could benefit from more comments to explain the reasoning behind certain steps, making it easier for others to understand the code's logic. Overall, the code is solid, but with a few tweaks, it can be even more robust and readable.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '', regex=False)
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce')
df['Video views'] = df['Video views'].fillna(0)

# Ensure 'published' column is numeric (assuming it's already the year)
# If 'published' is a full date string, uncomment and use the following:
# df['publ...",6,"The generated code demonstrates a good understanding of the task and correctly solves it to some extent. However, there are several areas that need improvement to achieve a higher score. 

1. **Correctness and Completeness**: The code misses the step of handling the 'published' column as a numeric or integer type, similar to the ground truth code. It directly converts the 'published' column to numeric, which might not work if the column contains date strings. Additionally, the code does not filter out the year 0 if it's an artifact of NaN filling and not a real year, which is a requirement. 

2. **Code Quality**: The code lacks proper error handling, unlike the ground truth code, which checks for file not found errors and other exceptions. The generated code also does not provide informative print statements to indicate the progress of the analysis, making it less transparent.

3. **Structure and Readability**: The code could benefit from being more structured and readable. The ground truth code is more modular, with clear sections for each step of the analysis. The generated code combines these steps into a more linear sequence, making it slightly harder to follow.

4. **Figure Customization**: The generated code does not include some of the customization details present in the ground truth code, such as setting the figure title, using specific colors for the lines, and adjusting the layout to prevent title overlap. While the plots are generated, they could look more polished with additional customization.

To improve the score, the generated code should address these areas, particularly focusing on handling the 'published' column correctly, enhancing error handling, and adding more structural clarity and plot customization.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
from nltk.corpus import stopwords
import string

# Load the dataset
try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: youtube_trendings.csv not found. Please ensure the file is in the current directory."")
    exit()

# 2) Create 'title_word_count'
df['title_word_count'] = df['Video'].apply(lambda x: len(str(x).split()))

# 3) Clean 'Video views'
df['Video views'] = df['Video views...",9,"The generated code is very close to the ground truth code and successfully solves the task. It correctly loads the dataset, creates the `title_word_count` feature, cleans the `Video views` column, calculates the Pearson correlation coefficient, filters the dataset for videos published in 2023 or 2024, processes the titles of these videos, and finds the 5 most common words. The code structure is also good, and it uses proper imports. However, there are a few minor differences and potential improvements. For instance, the ground truth code checks if the 'youtube_trendings.csv' file exists before attempting to load it, and it handles the case where no video titles are found for 2023/2024 or no words remain after processing. The generated code does not include these checks, which is why it does not receive a perfect score. Nevertheless, the code is well-written, and with a few adjustments to include these checks and handle edge cases, it could be considered nearly flawless.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score
import re

# 1) Load data
try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: mobiles_2025.csv not found. Please ensure the file is in the current directory."")
    exit()

# Make a copy to avoid SettingWithCopyWarning
df_cleaned = df.copy()

# 2) Clean `Launched Price (USA)`...",8,"The generated code solves the task with high accuracy, demonstrating a good understanding of the requirements. It correctly loads the data, cleans the specified features, handles categorical variables through one-hot encoding, defines features and targets, splits the data, trains a RandomForestClassifier, and evaluates the model using F1-score and ROC AUC score. The code is well-structured, and imports are proper. However, there are a few areas for improvement to achieve a perfect score. Firstly, the code could benefit from more comprehensive error handling, especially when dealing with file operations and potential data cleaning issues. Secondly, while the code is generally well-structured, some sections could be further optimized for readability, such as the feature cleaning and extraction steps. Lastly, including comments to explain the reasoning behind certain choices (like the selection of hyperparameters for the RandomForestClassifier) would enhance the code's readability and maintainability. Overall, the code is strong, and with these minor adjustments, it could be considered exemplary.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

def clean_price(price_str):
    if pd.isna(price_str):
        return np.nan
    price_str = str(price_str).replace('PKR', '').replace('INR', '').replace('CNY', '').replace('USD', '').replace('AED', '').replace(',', '').strip()
    try:
        return float(price_str)
    except ValueError:
        return np.nan

df = pd.read_csv('mobiles_2025.csv')

price_columns = ['Launched Price (Pakistan)', 'Launched Price (India)', 'Launched Price (China)',...",8,"The generated code is overall correct and achieves the task of performing a price analysis on the mobiles_2025.csv dataset. It correctly loads the dataset, defines a function to clean price columns, calculates the Average Global Price for each mobile phone model, identifies the top 5 models with the highest Average Global Price, and displays the results. The code is well-structured, and the use of pandas and numpy is appropriate for the task.

However, there are a few areas for improvement to achieve a higher score. Firstly, the generated code lacks error handling for the case where the CSV file is not found. Although the ground truth code includes this, it is a good practice to include it in the generated code as well. Secondly, the code does not print any warning messages when a price column is not found in the dataset, which is handled in the ground truth code. Adding these features would make the generated code more robust.

Additionally, the code could benefit from more comments to explain the purpose of each section and the logic behind the calculations. This would make the code easier to understand for someone reading it for the first time. Lastly, the code uses the `apply` function to calculate the Average Global Price, which can be slower for large datasets. Using vectorized operations or the `numpy` library could improve performance.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1) Load the dataset
try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: 'mobiles_2025.csv' not found. Please ensure the file is in the current directory."")
    exit()

# 2) Clean 'RAM', 'Battery Capacity', and 'Launched Price (USA)'
def clean_numeric_column(df, column_name, chars_to_remove):
    if column_name in df.columns:
        df[column_name] = df[column_name].astype(str).str.replace(char...",6,"The generated code attempts to solve the task but has several areas for improvement. Here are the key strengths and weaknesses:

1. **Correctness**: The code correctly loads the dataset, cleans the specified columns, filters for Apple products, and generates box plots for each launched year. However, it lacks proper error handling and does not provide a professional-style plot layout as requested in the ground truth code.

2. **Code quality**: The code uses proper imports and defines a reusable function for cleaning numeric columns. However, the code structure could be improved by separating the data cleaning, filtering, and plotting into distinct sections or functions for better readability and maintainability.

3. **Completeness**: The code addresses all the requirements but misses some details such as setting up a professional-style plot layout, adding titles to each subplot, and including y-axis labels. It also lacks the initial print statements for dataset loading and cleaning progress that were present in the ground truth code.

To achieve a higher score, the code should be refined to include these missing details, improve the structure for better readability, and enhance error handling for a more robust solution. Specifically, adding more print statements for progress, improving the plot layout, and ensuring all required titles and labels are included would enhance the code's quality and completeness.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import (
    train_test_split, GridSearchCV, RandomizedSearchCV,
    StratifiedKFold, learning_curve, validation_curve
)
from sklearn.preprocessing import (
    StandardScaler, OneHotEncoder, PolynomialFeatures
)
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import (
    RandomForestClassifier, GradientBoost...",8,"The generated code is generally correct and addresses most of the requirements, including comprehensive data preprocessing, building and tuning multiple base models, creating stacking and voting ensemble classifiers, implementing feature selection, and performing extensive model evaluation. It also includes model interpretability techniques using SHAP. The code is well-structured, and proper imports are used. However, there are a few areas for improvement to achieve a higher score. Firstly, the code could benefit from more comprehensive documentation, including comments that explain the purpose of each section and the reasoning behind certain design choices. Secondly, the model evaluation section could be expanded to include more metrics and statistical tests to provide a more comprehensive understanding of the models' performance. Lastly, the code could be optimized for better performance, particularly in the preprocessing and feature selection steps, where complex operations are performed on large datasets. Additionally, the SHAP explanation section could be improved by handling cases where the feature names are not perfectly aligned after selection, which might occur with complex pipelines and dynamic feature selection. Overall, the code provides a solid foundation for building and evaluating advanced ensemble learning models, but with some refinements, it could be even more effective and robust.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Define features X and target y
X = df[['Year', 'Runtime']]
y = df['Rating']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Linear Regression model...",6,"The generated code successfully loads the dataset, defines features and target, splits the data, trains a Linear Regression model, and makes predictions to evaluate the model using R-squared and Mean Absolute Error. However, there are areas for improvement to achieve a higher score. The primary issue is that the code lacks error handling and data preprocessing steps present in the ground truth code, such as checking for required columns, cleaning the 'Runtime' column, and handling missing values. Additionally, the code does not provide any feedback or output during the execution, which makes it difficult to diagnose any potential issues. To improve the code quality and completeness, it is recommended to include these missing features and provide more informative output. The code structure is simple and straightforward, but it can be further enhanced by adding comments to explain the purpose of each section and using a more consistent naming convention. Overall, the code demonstrates a good understanding of the task but requires some refinements to match the completeness and robustness of the ground truth code.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 2) Select the features `Rating` and `Runtime` for clustering
features = df[['Rating', 'Runtime']]

# Keep a copy of original features for plotting
original_features = features.copy()

# 3) Standardize these features using StandardScaler
scaler = StandardScaler()
scaled_features = scaler.fit...",9,"The generated code is very close to the ground truth code and effectively solves the task of performing K-Means clustering on the given dataset. It correctly loads the dataset, selects and standardizes the features, applies K-Means clustering, visualizes the clusters, and reports the mean rating and runtime for each cluster. The code is well-structured, and the use of comments is minimal but sufficient to understand the logic. However, there are a few minor differences and potential improvements:
- The generated code does not include data cleaning steps for the 'Runtime' column, which might be necessary if the column contains string values that need to be converted to numeric.
- The column name in the generated code is 'Cluster' instead of 'cluster_label' used in the ground truth code. While this does not affect the functionality, it's a minor inconsistency.
- The generated code uses a for loop to calculate and print the mean rating and runtime for each cluster, which is not as concise as the groupby operation used in the ground truth code.
- The figure size in the generated code is set to (10, 6) instead of (10, 7) used in the ground truth code, which might slightly affect the plot's appearance.
Overall, the generated code is strong, and with a few minor adjustments, it could be even more concise and consistent with the ground truth code.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
from scipy.stats import pearsonr

# Ensure NLTK stopwords resource is available
try:
    stopwords.words('english')
except LookupError:
    nltk.download('stopwords')
try:
    word_tokenize(""test"")
except LookupError:
    nltk.download('punkt')


def analyze_titles(filepath=""imdb_top_250.csv""):
    """"""
    Performs text analysis on movi...",8,"The generated code is well-structured, and it correctly solves the task with minor adjustments needed for completeness and correctness. It properly imports the required libraries, ensures the necessary NLTK resources are available, and performs the necessary text analysis on the movie titles. The code is also readable, and the use of a function to analyze the titles enhances modularity. However, there are a few areas that could be improved for a perfect score. Firstly, the code does not handle potential NaN values in the 'Title' column during the preprocessing steps, which could lead to errors. Secondly, the code does not ensure that the 'Rating' column is numeric before calculating the Pearson correlation coefficient, which might result in an error if the column contains non-numeric data. Lastly, the code could benefit from more detailed error handling and logging to make it more robust. Despite these minor issues, the code is well-written and effectively accomplishes the task at hand.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""penguins.csv not found. Please make sure the fi...",8,"The generated code is mostly correct and solves the task with some minor deviations from the ground truth code. It properly loads the dataset, imputes missing values in numerical and categorical features, one-hot encodes categorical features, scales numerical features, trains a RandomForestClassifier model, and evaluates the model using 5-fold cross-validated accuracy score. The code is well-structured, and the use of pipelines and column transformers makes it efficient and readable.

However, there are a few areas where the generated code deviates from the ground truth code. The ground truth code explicitly handles the case where the mode imputation results in NA for the 'island' feature by dropping those rows, whereas the generated code performs this check but does so after the pipeline preprocessing step, which might lead to slightly different results due to the order of operations. Additionally, the generated code does not exactly replicate the ground truth code's handling of the 'species' target variable, as it directly uses the `LabelEncoder` on the target variable without checking for NA values, unlike the ground truth code which drops rows with NA target values before training the model.

To achieve a perfect score, the generated code should be further refined to exactly match the ground truth code's handling of these edge cases, particularly the NA values in the 'species' target variable and the mode imputation for the 'island' feature. Additionally, including more comments and documentation would enhance the code's readability and maintainability.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Select the features
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_selected = df[features + ['species']].copy()

# 3) Drop any rows where any of these four selected features have missing values (NA).
df_cleaned ...",8,"The generated code is mostly correct and complete, solving the task of performing K-Means clustering and PCA on the penguins.csv dataset. It properly loads the dataset, selects and cleans the features, standardizes them, applies K-Means clustering, and creates scatter plots of the principal components colored by both K-Means labels and original species labels. The code quality is good, with proper imports and a clear structure. However, there are a few areas that could be improved for a higher score: (1) Error handling is missing, such as checking if the 'penguins.csv' file exists before trying to load it, which could prevent the program from crashing if the file is not found. (2) The code could benefit from more descriptive variable names, especially for the dataframes and arrays, to improve readability. (3) The second scatter plot could be simplified by using seaborn's scatterplot function, similar to the first plot, which would make the code more consistent and easier to understand. (4) Adding a brief explanation or comment at the beginning of the code about the purpose and the steps being performed would enhance readability and understandability for anyone reading the code.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('penguins.csv')

# Impute missing values
# Numerical columns: culmen_length_mm, culmen_depth_mm, flipper_length_mm, body_mass_g
numerical_cols = ['culmen_l...",8,"The generated code is generally correct and solves the task as requested. It properly loads the dataset, imputes missing values, defines features, trains a GradientBoostingClassifier, extracts and prints feature importances, and generates box plots of flipper_length_mm by sex for each species. The code quality is good, with proper imports and a clear structure. However, there are a few areas for improvement to achieve a perfect score. Firstly, the code could benefit from more comments, especially in complex sections like the ColumnTransformer and the loop for generating box plots. Secondly, the code does not handle the case where the penguins.csv file is not found, which is handled in the ground truth code. Lastly, the code could be more consistent in its naming conventions and spacing. Additionally, the ground truth code provides more detailed and informative plots, such as a single box plot for all species and a legend for the sex, which are missing in the generated code. Overall, the code is strong, but with a few tweaks, it could be even better.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer

def clean_penguins_data(file_path='penguins.csv'):
    """"""
    Performs advanced data cleaning on the penguins dataset.

    Args:
        file_path (str): The path to the penguins.csv file.

    Returns:
        pandas.DataFrame: The cleaned DataFrame.
    """"""
    # 1) Load the dataset using pandas
    df = pd.read_csv(file_path)

    # 2) For the numerical columns, impute NA values using K-Nearest Neighbors
    numerical_cols...",9,"The generated code is very close to solving the task correctly. It properly loads the dataset, applies K-Nearest Neighbors imputation to the specified numerical columns, imputes NA values in the 'sex' column with its mode, and verifies the number of remaining NA values in each column. The code is well-structured, and the use of a function `clean_penguins_data` is a good practice for encapsulating the data cleaning logic. However, to achieve a perfect score, the code could benefit from a few improvements: (1) Error handling for the case when the 'penguins.csv' file is not found, similar to the ground truth code. (2) Verification that the numerical columns are indeed numerical and can be imputed by KNNImputer without errors. (3) Consideration of the case when the 'sex' column is entirely NA or empty, which is already partially addressed but could be more explicitly handled for clarity. Overall, the code is robust and effective for the task at hand.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset using pandas.
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: penguins.csv not found. Please make sure the file is in the current directory."")
    exit()

# 2) Filter the dataset to include only 'Adelie' and 'Gentoo' species.
adelie_gentoo_df = df[df['species'].isin(['Adelie', 'Gentoo'])].copy() # .copy() to avoid SettingWithCopyWarning

# 3) Handle NA values in the `culmen_length_m...",8,"The generated code is generally correct and solves the task as required. It properly loads the dataset, filters for 'Adelie' and 'Gentoo' species, handles NA values in the `culmen_length_mm` column, performs an independent two-sample t-test, and interprets the result based on the p-value. The code is also well-structured and readable.

However, there are a few areas for improvement to achieve a higher score. Firstly, the code assumes unequal variances for the t-test, which is specified using `equal_var=False` in the `stats.ttest_ind` function. In the ground truth code, it is assumed that equal variances are a reasonable assumption, specified with `equal_var=True`. This should be made explicit or a justification provided for the choice.

Secondly, the generated code does not print the mean culmen lengths for 'Adelie' and 'Gentoo' penguins, which are provided in the ground truth code for context and completeness.

Lastly, while the code is generally well-structured, it lacks the robust error handling seen in the ground truth code, such as checking if the dataset was loaded successfully and providing detailed error messages. Adding such checks can enhance the code's reliability and user experience.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'pe...",8,"The generated code generally solves the task correctly, showcasing a good understanding of the requirements. It properly loads the dataset, defines features and a target, performs the specified preprocessing (median imputation for numeric features, mode imputation for 'sex', and one-hot encoding for 'island' and 'sex'), splits the data into training and testing sets while ensuring stratification by the target, trains a RandomForestClassifier, and evaluates the model with an accuracy score and a classification report. The code is well-structured, and the use of pipelines and transformers from scikit-learn is appropriate for the task. However, there are a few minor improvements that could be made for enhanced code quality and completeness. For instance, error handling could be more comprehensive, particularly for cases where the dataset might be missing or is of the wrong format. Additionally, the code could benefit from more comments explaining the reasoning behind certain preprocessing decisions, especially for handling missing values in categorical variables. Lastly, while the code achieves a high score, explicitly stating that it meets all requirements and providing a clear summary of its strengths and weaknesses would enhance its overall quality.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Calculate Pearson correlation co...",9,"The generated code is very close to the ground truth code and correctly defines the DataFrame, calculates the Pearson correlation coefficients, prints them, and determines which feature has a stronger absolute correlation with ""PerformanceScore"". The code is well-structured and easy to follow. However, to achieve a perfect score, the generated code could benefit from a few minor improvements. For instance, it does not include a import statement for the numpy library, which is good practice for data science tasks, even though it's not strictly necessary for this specific task. Additionally, the code could include some comments to explain the purpose of each section, making it even more readable and maintainable. Overall, the generated code is robust and effectively solves the given task.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from lifelines import KaplanMeierFitter
from lifelines import CoxPHFitter
from lifelines.datasets import load_dd
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols
import pymc3 as pm
import arviz as az
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors...",6,"The generated code demonstrates a good understanding of the task requirements, covering topics like Kaplan-Meier survival analysis, Cox proportional hazards regression, propensity score matching, and Bayesian analysis using PyMC3. However, there are several areas that need improvement to achieve a higher score. 

1. **Data Preprocessing**: The code lacks comprehensive data preprocessing steps. For instance, it assumes that all categorical variables should be one-hot encoded, which might not be the case for all datasets. Additionally, there's no handling for missing values in the target variable or in the features used for modeling.

2. **Model Validation**: While the code performs some model diagnostics (e.g., checking proportional hazards assumptions), it lacks more comprehensive validation techniques such as cross-validation, concordance index calculation for model performance evaluation, and sensitivity analysis to assess the robustness of the models.

3. **Code Organization and Structure**: The code is not well-organized, with each section being a continuous block of code without clear separation or documentation. This makes it difficult to follow and maintain.

4. **Documentation and Comments**: There are very few comments explaining the purpose of each section of the code, making it challenging for someone else to understand the logic and implement similar tasks.

5. **Error Handling**: The code does not include robust error handling. For instance, it assumes that the `titanic.csv` file will always be found in the current directory, and it does not handle cases where the file is missing or cannot be read.

6. **Statistical Rigor**: While the code attempts to cover various statistical methods, it lacks the statistical rigor expected in a comprehensive data science benchmark. This includes not fully addressing interaction effects, not performing multivariate analysis with more nuanced techniques, and not fully validating model assumptions.

7. **Bayesian Analysis**: The Bayesian analysis section is simplified and lacks the complexity and nuance typically required in real-world applications, such as handling prior distributions morethoughtfully and exploring the model's posterior distributions in greater depth.

To achieve a higher score, the code should be refined to address these areas, incorporating more sophisticated data preprocessing, model validation techniques, improved code organization, and a emphasis on statistical rigor and depth in the analysis.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# 1. Create the DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
    'Final_Score': [90, 88,...",8,"The generated code is mostly correct and complete, solving the task with some minor adjustments to the ground truth code. It correctly creates the DataFrame, calculates the Pearson correlation coefficient, generates a scatter plot colored by subject, and assigns overall grades based on the final score. The code quality is good, with proper imports and a clear structure. However, there are a few areas where it could be improved to achieve a perfect score. Firstly, the generated code does not print the initial DataFrame, which is a useful step for verification. Secondly, the correlation coefficient is printed with only two decimal places, whereas the ground truth code prints it with four decimal places for more precision. Additionally, the generated code uses a different method for assigning grades, which, while correct, is less concise than the np.select method used in the ground truth code. Lastly, the legend in the scatter plot could be placed outside the plot similar to the ground truth code for better readability. With these minor adjustments, the code could achieve a near-perfect score.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']),
        'Ticker': ['TECHCO']*10,
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0],
        'Low': [149.5, 150.5, 1...",9,"The generated code is very close to the ground truth code and successfully solves the task. It correctly creates a DataFrame, calculates daily returns, 3-day and 5-day simple moving averages, generates a line plot with the close price and both SMAs, and identifies the dates where the 3-day SMA crossed above the 5-day SMA. The code is well-structured, and the use of comments is minimal but sufficient. The only minor issues are the different line styles used for the 3-day and 5-day SMAs in the plot and the figsize parameter in the plt.figure function. Additionally, the code could benefit from more descriptive variable names and additional comments to explain the logic behind the code, especially in the section where bullish cross dates are calculated. However, these are minor suggestions, and the code is overall of high quality.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. Create the DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CU...",9,"The generated code overall accurately solves the task, demonstrating strong correctness. It properly creates a pandas DataFrame, calculates total sales revenue by product, generates a horizontal bar chart, calculates the average order value, and identifies repeat customers. The code quality is high, with proper imports and a clear structure. The only areas for improvement are minor, such as the lack of printing the original DataFrame and the average order value details, which are present in the ground truth code but not in the generated code. Additionally, the generated code uses `df.groupby('CustomerID')['OrderID'].nunique()` to count the number of purchases for each customer, which is slightly different from the ground truth code that uses `df['CustomerID'].value_counts()`. However, both methods achieve the same result. Overall, the generated code is very strong, and with a few minor adjustments, it could be considered perfect.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 1) Create a new numerical feature ""DeviceType_Numeric""
df[""DeviceType_Numeric""] = df[""DeviceType""].map...",9,"The generated code is very close to the ground truth code and correctly defines the DataFrame, creates the ""DeviceType_Numeric"" and ""PagesPerMinute"" features as specified, selects the correct final features, and prints the head of the resulting DataFrame. The code quality is also good, with proper use of pandas and dictionary mapping for feature creation. The only minor area for improvement is the handling of division by zero in the ""PagesPerMinute"" feature creation, where the ground truth code explicitly replaces infinities with NaNs before filling them with 0, whereas the generated code directly replaces infinities with 0. However, the generated code's approach also effectively handles division by zero and produces the same result as the ground truth code. Overall, the generated code is well-structured, readable, and effective in solving the task.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26, 45, 28]
}
d...",8,"The generated code is largely correct and completes the task of defining a DataFrame for environmental sensor readings and performing outlier detection on the ""TemperatureCelsius"" column. It accurately calculates the mean and standard deviation of the temperatures and identifies outliers based on a 1.5 standard deviation threshold. The code structure is straightforward, and it properly uses pandas for data manipulation.

However, to achieve a perfect score, there are a few improvements that could be made. Firstly, the code does not convert the 'Timestamp' column to datetime objects, which is a good practice for handling time-series data. Additionally, the code could benefit from more descriptive variable names and comments to improve readability. The output could also be enhanced by including the mean and standard deviation of the temperatures in the printed outlier information, similar to the ground truth code.

Overall, the generated code is strong and effectively solves the task, but with a few refinements, it could be even more robust and user-friendly.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [1...",9,"The generated code accurately defines the DataFrame, calculates the ""EngagementScore"", and for each unique ""Platform"", determines the ""PostType"" with the highest average ""EngagementScore"" and prints the results. The code structure is clear, and the use of pandas functions is appropriate. The only minor improvement that could be suggested is in the naming of variables; for example, ""average_engagement"" and ""highest_engagement_per_platform"" could be more descriptive. Additionally, the code could benefit from including comments to explain the purpose of each section, making it easier to understand for someone reading the code for the first time. Overall, the code is well-organized, and the logic is correct, making it a strong solution to the task.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""C...",7,"The generated code is generally correct and solves the task, but it lacks in code quality and completeness. Here are the specific issues:

1. **Lack of structure and comments**: The code is not well-structured, and there are no comments to explain what each section is doing. This makes it difficult to understand the code's logic and intent.
2. **Insufficient printing**: The code only prints the diagnosis summary, but it does not provide the detailed information required, such as the average length of stay and patient count for each diagnosis.
3. **No identification of the diagnosis with the longest average length of stay**: The code finds the index of the diagnosis with the longest average length of stay, but it does not print the corresponding diagnosis name and average length of stay.
4. **No reset index**: The `groupby` and `agg` methods return a DataFrame with a multi-index, but the code does not reset the index to make it easier to work with.
5. **Missing formatting**: The printed output is not formatted, making it difficult to read and understand.

To improve the code, the developer should add comments and structure to explain the code's logic, print the required information, and identify the diagnosis with the longest average length of stay. Additionally, resetting the index and formatting the output would make the code more readable and user-friendly.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",9,"The generated code is mostly correct, efficient, and well-structured. It correctly defines the DataFrame, calculates the WeightedRating for each movie, adds this rating as a new column, and identifies the movie with the highest WeightedRating. The code is concise and uses vectorized operations provided by pandas, which is a good practice for performance. However, there are a few minor aspects that could be improved for a perfect score. Firstly, the code does not handle the potential division by zero error, which is a critical aspect to consider. Although the provided dataset does not contain zero total votes, it is a good practice to include a check for this condition. Secondly, the code could benefit from including comments to explain the purpose of each section, especially for someone not familiar with the task. Finally, the code uses `.5f` for formatting the WeightedRating, which might not provide enough decimal places for all cases; using a more flexible approach, such as `.6f` or allowing the user to specify the number of decimal places, could be more versatile. Overall, the code is solid, and with these minor adjustments, it could be even more robust and user-friendly.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 10000, 8000]
}

df = p...",9,"The generated code is very close to the ground truth code and effectively solves the task. It correctly defines the DataFrame, calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"", and presents the results in a new grouped DataFrame. The code is also well-structured and easy to follow. The only minor improvement that could be suggested is to include a clear and descriptive print statement, similar to the one in the ground truth code, to explain the purpose of the printed DataFrame. Overall, the code is of high quality, and the only reason it's not a perfect 10 is that it doesn't include a descriptive print statement.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 8...",8,"The generated code is generally correct and solves the task with minor omissions. It correctly defines the DataFrame, adds the ""TotalStats"" column, identifies the Pokémon with the highest total stats, and calculates the average speed by Type1. The code is well-structured, and proper imports are used. However, there are a few areas for improvement to achieve a higher score. Firstly, the code does not print the highest ""TotalStats"" value, which is part of the requirement. Secondly, the code does not include a clear separation between the different steps of the task, making it slightly harder to read. Lastly, it would be beneficial to add a print statement at the beginning to display the original DataFrame for comparison. With these minor adjustments, the code could achieve a score of 9 or 10.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
import re
from itertools import combinations
from sklearn.preprocessing import StandardScaler, QuantileTransformer
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from scipy.optimize import curve_fit
from scipy.spatial.distance impor...",6,"The generated code demonstrates a good understanding of the task, implementing advanced feature engineering, multiple clustering algorithms, and comprehensive analysis of passenger segments. It correctly solves the task and addresses most requirements. However, there are some areas for improvement to achieve a higher score. 

1. **Code Quality and Structure**: The code could benefit from better organization, with clear sections for data loading, preprocessing, feature engineering, clustering, and visualization. Some functions or methods could be defined to improve readability and maintainability.

2. **Completeness**: While the code covers a wide range of aspects, it lacks a detailed exploration of the dendrogram for Hierarchical Clustering and a more thorough analysis of the consensus clustering results. Additionally, the code could benefit from more robust evaluation metrics and a more detailed validation of the clustering results using external criteria.

3. **Error Handling**: The code could be improved with more comprehensive error handling, especially when dealing with missing values, invalid inputs, or unexpected errors during data processing and clustering.

4. **Documentation**: The code could benefit from more comments and docstrings to explain the purpose of each section, the reasoning behind certain choices, and the expected output.

5. **Visualization**: While the code includes some visualizations, it could be expanded to provide a more comprehensive understanding of the data and the clustering results. This includes more detailed plots for each clustering algorithm and a comparison of the results.

6. **Tuning of Hyperparameters**: The code could benefit from a more systematic approach to tuning hyperparameters, especially for the clustering algorithms, to ensure that the best possible results are achieved.

By addressing these areas, the code can be further improved to achieve a higher score and provide a more comprehensive and robust solution to the task.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dyst...",8,"The generated code generally solves the task, demonstrates good structure, and is quite complete. It correctly defines the DataFrame, calculates the book count and average price per author, prints this information, and identifies the author with the most books in stock. The code quality is good, with proper imports and a clear, if somewhat concise, approach to the problem. However, there are a few areas for improvement to achieve a perfect score. Firstly, the code could benefit from more descriptive variable names (e.g., `author_summary` could be `author_book_statistics`). Secondly, the printing of the author summary could be more formatted for better readability. Finally, a separator or header for the author with the most books in stock section would enhance the overall clarity of the output. Additionally, the generated code lacks the optional step of displaying the full DataFrame for reference, which is included in the ground truth code for completeness.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
from scipy import stats
from sklearn.model_selection import (
    train_test_split, GridSearchCV, StratifiedKFold,
    validation_curve, learning_curve
)
from sklearn.preprocessing import (
    StandardScaler, QuantileTransformer, PolynomialFeatures
)
from sklearn.impute import KNNImputer
from sklearn.ensemble import (
    RandomForestClassifier, GradientBoostingClassifier, VotingCl...",7,"The generated code attempts to solve the task by implementing data preprocessing, building and optimizing multiple base models, creating ensemble models, and performing advanced evaluations. It correctly uses various scikit-learn classes and functions for these tasks. However, there are areas for improvement to achieve a higher score.

1. **Inconsistent Ensemble Model Handling**: The code creates ensemble models but does not consistently handle their predictions, especially for the blending model. The blending model prediction is attempted to be integrated into the evaluation loop but is not properly handled, leading to potential inconsistencies in the evaluation results.

2. **Lack of Comprehensive Error Handling**: While the code attempts to handle some errors, it does not provide comprehensive error handling. This could lead to the script crashing unexpectedly if an error occurs during execution, which is crucial for a robust data science pipeline.

3. **SHAP Interpretability**: The code computes SHAP values but does not provide a clear explanation or visualization of the results. While it generates SHAP summary and dependency plots, these are not integrated into a coherent interpretation of the model's performance and feature importance.

4. **Model Calibration**: The model calibration section is limited to an example with the RandomForest model and uses a simple sigmoid calibration. More comprehensive calibration techniques and a clear evaluation of their impact on the model's performance are missing.

5. **Code Organization and Documentation**: The code could benefit from better organization and documentation. This includes using more descriptive variable names, adding comments to explain the purpose of each section, and considering the use of functions to organize the code further.

6. **Statistical Significance Testing**: The statistical significance testing is performed but could be more robust. It lacks a clear explanation of the statistical tests used, the interpretation of their results, and does not consider multiple testing corrections.

Overall, the code provides a good start but requires refinement in several areas to achieve a higher score, especially in terms of handling ensemble models, improving error handling, enhancing model interpretability, and organizing the code.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.genmod.generalized_additive_model import GLMGAM
from statsmodels.genmod import families
import pymc3 as pm
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, KNNImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFacto...",6,"The generated code attempts to solve the advanced statistical modeling and anomaly detection task for water quality analysis but falls short in several aspects, mainly due to incomplete implementation of some methods and a lack of comprehensive model diagnostics. Here are the primary reasons for this rating:

1. **Inadequate Implementation of Some Methods**: The code partially implements various statistical models and techniques but lacks completion in certain areas, such as fully incorporating Generalized Additive Models (GAM) with automatic smoothing, and detailed diagnostics for all models.
2. **Insufficient Model Diagnostics**: While the code attempts to perform some model diagnostics, it misses providing a comprehensive suite of diagnostics, including detailed residual analyses, influence measures, and goodness-of-fit tests for all models.
3. **Incomplete Anomaly Detection**: The anomaly detection section is present but could be more robust with additional techniques and a more detailed evaluation of the results.
4. **Missing Visualizations**: Although the code includes some basic visualizations, it does not fully leverage the potential of advanced statistical visualizations to effectively communicate the findings.
5. **Code Organization and Comments**: The code could benefit from better organization, more detailed comments, and documentation to enhance readability and understandability.

To reach a higher score, the code should be thoroughly reviewed and revised to address these gaps, including:
- Completing the implementation of all required statistical models and techniques.
- Enhancing model diagnostics to include a wide range of analyses.
- Improving the anomaly detection section with additional methods and a more in-depth analysis.
- Incorporating a broader range of visualizations to effectively communicate the results.
- Refactoring the code for better organization, comments, and documentation.",gemini_2.5_flash_preview_5_20,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import (KMeans, AgglomerativeClustering, DBSCAN,
                             GaussianMixture, SpectralClustering)
from sklearn.metrics import (silhouette_score, davies_bouldin_score,
                             calinski_harabasz_score, pairwise_distances)
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.manifold import TSNE
import umap
from scip...",6,"The generated code attempts to solve the task of comprehensive water quality clustering and segmentation analysis on the water_potability.csv dataset. It covers many aspects, including feature engineering, multiple clustering algorithms, dimensionality reduction, cluster stability analysis, and anomaly detection. However, there are some shortcomings and areas for improvement.

1. **Incompletely implemented feature engineering**: The code lacks a thorough and detailed feature engineering process, which is a critical component of water quality analysis. It only includes a few basic indices and ratios, which might not fully capture the complexity of water quality data.

2. **Insufficient handling of missing values**: The code drops rows with missing values instead of imputing them, which might lead to loss of valuable information, especially if the missing values are not randomly missing.

3. **Lack of comprehensive validation techniques**: While the code touches upon silhouette score, Davies-Bouldin index, and Calinski-Harabasz index for evaluating the quality of clusters, it does not fully explore other validation techniques such as gaps statistic, elbow method, or external validation using domain knowledge.

4. **Limited depth in cluster stability analysis**: The code's approach to cluster stability analysis is basic and could be expanded upon. Techniques like bootstrap resampling and consensus clustering could be more thoroughly explored for a more robust analysis.

5. **Anomaly detection could be more sophisticated**: The code's anomaly detection method is based on isolation forest, which is a good start, but could be strengthened by exploring other methods or combining them for a more rigorous analysis.

6. **Visualizations could be more informative**: While the code generates various plots, they could be more informative and detailed, potentially including more context or interactive elements to better convey the findings.

7. **Code organization and documentation**: The code could benefit from better organization, including more comments and docstrings to explain the purpose and logic behind different sections, making it easier to understand and maintain.

Overall, the code demonstrates a good starting point but requires further development to fully address the complexity of the task and to achieve a higher score.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import linregress

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Calculate correlations with 'quality'
correlations = df.corr()['quality'].sort_values(ascending=False)

# Identify top 3 positive and negative correlations
top_3_positive = correlations[1:4]  # Exclude 'quality' itself
top_3_negative = correlations[-3:]

# Print top 3 positive and negative correlations
print(""Top 3 M...",7,"The generated code is mostly correct and addresses the task requirements, but there are a few areas that need improvement to achieve a higher score. 

1. Correctness: The code calculates correlations correctly, identifies the top 3 positively and negatively correlated features, creates a correlation matrix heatmap, and generates scatter plots with regression lines. However, it does not print the correlation coefficients of the top features, which is a minor oversight.

2. Code quality: The code structure is generally good, but it can be improved. For example, the code for creating scatter plots with regression lines is duplicated, which can be refactored into a separate function to improve readability and maintainability. Additionally, some variable names, such as `correlations`, could be more descriptive.

3. Completeness: The code addresses all the requirements, but it could benefit from more informative print statements and error handling, similar to the ground truth code.

To improve the score, the following suggestions can be made:
- Refactor the code to reduce duplication, especially in the scatter plot creation section.
- Add more informative print statements to explain the process and results.
- Consider adding error handling for file loading and correlation calculation.
- Use more descriptive variable names to improve readability.
- Print the correlation coefficients of the top features for completeness.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
from scipy import stats
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('video_games_sales.csv')
except FileNotFoundError:
    print(""Error: 'video_games_sales.csv' not found. Make sure the file is in the current directory."")
    exit()

# Data Cleaning
df.dropna(inplace=True)

# Convert 'Year_of_Release' to integer
df['Year_of_Release'] = df['Year_of_Release'].astype(int)

# 1. Test whether Nintendo games have significantly higher global sales comp...",8,"The generated code is mostly correct and completes the task by performing the required hypothesis tests on the video_games_sales.csv dataset. It accurately loads the dataset, cleans missing values, and performs t-tests, a chi-square test, and a Mann-Whitney U test as specified. The code structure is clear, and the use of descriptive print statements helps in understanding the results. However, there are a few areas for improvement to achieve a higher score. Firstly, the code does not handle the case where the t-test or Mann-Whitney U test cannot be performed due to insufficient data in one or both groups, which is addressed in the ground truth code. Secondly, the code could benefit from more detailed error handling, such as catching and handling potential errors during file loading or statistical tests. Additionally, the code does not provide a summary of the cleaned dataset, which is included in the ground truth code for transparency. Despite these minor shortcomings, the generated code effectively solves the task and is well-structured, making it a strong candidate with room for further refinement.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('video_games_sales.csv')
except FileNotFoundError:
    print(""Error: 'video_games_sales.csv' not found. Make sure the file is in the current directory....",7,"The generated code is mostly correct and addresses all the requirements of the task. It properly loads the dataset, handles missing values, creates a binary target variable, encodes categorical variables, and trains a Random Forest classifier. The code also evaluates the model using accuracy, precision, recall, F1-score, and confusion matrix, and displays feature importance. However, there are a few areas where the code could be improved. Firstly, the handling of missing values is quite arbitrary and could be more sophisticated, such as using imputation strategies based on the data type of each column. Secondly, the code does not handle the case where the 'Global_Sales' column contains non-numeric values, which could lead to errors when creating the 'High_Sales' target variable. Lastly, the code could benefit from more comments and docstrings to explain the purpose of each section and the reasoning behind certain decisions. Overall, the code is well-structured and easy to follow, but could use some refinement in terms of handling edge cases and adding more documentation.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.decomposition import PCA

def analyze_video_game_sales(file_path='video_games_sales.csv'):
    # 1) Load the dataset and handle missing values
    df = pd.read_csv(file_path)

    # Handle missing values: Fill numerical with median, categorical with mode
    for col in ['Publisher', 'Genre', 'Platform', 'Rating']:...",7,"The generated code solves the task, and its correctness is generally acceptable. However, there are areas for improvement to enhance code quality, completeness, and alignment with best practices. Here are some key points:

1. **Inaccurate Handling of Missing Values**: The code uses a simplistic approach to handle missing values, which might not be sufficient for all datasets. It fills categorical columns with mode and numerical columns with median, which could lead to information loss or inaccuracies if not done carefully.

2. **Insufficient Outlier Handling**: While the code attempts to handle outliers using the IQR method, it does so in a straightforward manner without considering the potential impact on the analysis or modeling stages. More sophisticated methods or considerations might be necessary depending on the dataset and analysis goals.

3. **Limited Feature Engineering**: The generated code creates several engineered features but misses others mentioned in the task, such as 'Sales_Momentum' which is correctly implemented but not fully aligned with the task's description. It also does not create polynomial features for a comprehensive analysis.

4. **Incomplete Analysis**: The code performs correlation analysis and includes a basic PCA implementation, which is a good start. However, it lacks a deeper exploration of the results, such as visualizations or further statistical analysis, which could provide more insights into the data.

5. **Code Organization and Comments**: The code could benefit from better organization and more comments, especially in complex functions or sections. This would make it easier for readers to understand the logic and intent behind the code.

6. **Error Handling**: The code lacks robust error handling. For instance, it does not check if the input file exists or if the necessary libraries are installed, which could lead to errors if not handled.

7. **Interaction Features**: The interaction features between 'Year' and 'Genre' are simplified and might not capture the full complexity of the relationship between these variables. A more detailed approach could be beneficial.

To achieve a higher score, the code should address these areas by enhancing its methods for handling missing values, outliers, and by including a broader range of feature engineering techniques. Additionally, incorporating more detailed analyses and robust error handling mechanisms would improve its overall quality and reliability.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import ttest_ind, mannwhitneyu, chi2_contingency, pearsonr, f_oneway

# Load the dataset
try:
    df = pd.read_csv('diabetes.csv')
except FileNotFoundError:
    print(""Error: diabetes.csv not found. Make sure the file is in the current directory."")
    exit()

# 1) Clean zero values: replace with NaN and then median imputation
columns_to_impute = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI...",8,"The generated code is generally correct and solves the task by performing the required hypothesis testing and statistical analysis on the diabetes.csv dataset. It properly handles data preparation, including cleaning zero values and performing median imputation. The code also correctly implements the specified statistical tests (t-test, Mann-Whitney U test, chi-square test, Pearson correlation, and ANOVA) and displays the results with proper interpretation.

However, there are a few areas for improvement. Firstly, the code could benefit from more descriptive variable names and comments to enhance readability and understanding. Secondly, the code does not handle potential errors that may occur during the execution of statistical tests, such as the ANOVA test which requires at least two groups to compare. The code should include try-except blocks to handle such scenarios.

Additionally, the code could be more concise and efficient by using vectorized operations provided by pandas and NumPy, which could reduce the need for explicit loops. Finally, the code does not provide a comprehensive summary of the findings, and it would be beneficial to include a conclusion or a summary of the results at the end.

Overall, the code is well-structured, and the statistical tests are correctly implemented, but there is room for improvement in terms of readability, error handling, and conciseness.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_scor...",7,"The generated code is mostly correct and solves the task, but there are a few areas that could be improved for better code quality, completeness, and to match the ground truth code more closely. Here are the key points:

1. **Incorrect handling of missing values**: The code replaces zeros with the median, but it does not explicitly state this handling as part of the preprocessing step. The ground truth code clearly mentions replacing zeros with the median of each respective column.

2. **Missing Hyperparameter Tuning for All Models**: The ground truth code performs hyperparameter tuning for the Random Forest model using GridSearchCV, but the generated code only does this for the Random Forest model. It would be beneficial to include hyperparameter tuning for other models as well for a more comprehensive comparison.

3. **Lack of Feature Importance for All Tree-Based Models**: The generated code only displays feature importance for the tuned Random Forest model and Gradient Boosting model if it exists. It should include all tree-based models for a fair comparison.

4. **Insufficient Visualization**: While the code includes some visualizations like confusion matrices and ROC curves, it could benefit from more detailed visualizations, such as different plots for each model's performance metrics.

5. **Code Structure and Comments**: The code could benefit from more comments explaining the purpose of each section and how it relates to the task. The structure is generally clear, but additional comments would improve readability and understanding.

6. **Model Evaluation Metrics**: The generated code computes and displays various evaluation metrics for each model. However, it might be more useful to include these metrics in a summarized table or diagram for easier comparison.

7. **Error Handling**: The code only checks for the existence of the 'diabetes.csv' file. It could benefit from more robust error handling, especially when dealing with file operations and model training.

Overall, the code is a good start and demonstrates a clear understanding of the task. With some refinements in handling missing values, incorporating hyperparameter tuning for all models, and enhancing the visualization and error handling, it could score higher.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('diabetes.csv')
except FileNotFoundError:
    print(""Error: diabetes.cs...",8,"The generated code successfully performs advanced feature engineering and dimensionality reduction on the diabetes.csv dataset, addressing most of the requirements specified in the task. It correctly loads and preprocesses the dataset, creates engineered features, applies polynomial features, performs feature scaling, and applies Principal Component Analysis (PCA). The code also implements correlation-based feature filtering, selects features using SelectKBest and Recursive Feature Elimination (RFE), and handles class imbalance using SMOTE. Furthermore, it evaluates feature importance using Random Forest and visualizes the top features.

However, there are a few areas where the code can be improved to achieve a higher score. Firstly, the code does not provide a clear explanation of the reasoning behind certain engineering decisions, such as the selection of coefficients for the 'Metabolic_Score' feature. Secondly, the code could benefit from more detailed comments and documentation to explain the purpose of each section and the logic behind the feature engineering and selection process. Finally, the code could be organized into separate functions or modules to improve readability and maintainability, especially for more complex tasks.

Additionally, the code does not fully match the ground truth code in some aspects, such as the implementation of PCA, where the ground truth code selects the number of components that explain 95% of the variance, whereas the generated code selects the number of components based on the cumulative sum of the explained variance ratio. While the generated code achieves similar results, it would be better to align with the ground truth code for consistency and accuracy.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Handle missing values (if any)
# For simplicity, we'll drop rows with any missing values.
# A more robust approach might involve imputation depending on the context.
df.dropna(inplace=True)

# 1. Test whether Western European countries have significantly higher happiness scores compared to all other r...",8,"The generated code generally solves the task by performing the required statistical tests on the world-happiness-report-2021.csv dataset. It demonstrates a good understanding of the necessary imports, structure, and completeness in addressing the requirements. However, there are a few areas that could be improved for a perfect score. 

Firstly, the handling of missing values is simplistic and could be improved with more robust methods like imputation, especially if the dataset contains a significant number of missing values. Secondly, the code could benefit from more detailed and informative print statements, providing clearer interpretations of the results, especially for complex statistical tests like ANOVA and chi-square tests. 

Additionally, the code could be more modular, breaking down the tests into separate functions for better readability and maintainability. This would also make it easier to replicate the tests or modify them without affecting the rest of the code. Lastly, while the code uses the correct statistical tests, it does not explicitly state the significance level (α) for all tests, which is an important detail for interpreting the results. 

Overall, the code is well-structured, and the use of pandas and statsmodels libraries is appropriate for the task. With some improvements in handling missing values, adding more informative print statements, modularity, and explicitly stating the significance level, the code could achieve a perfect score.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# 1. Load the dataset and pre...",8,"The generated code is quite comprehensive and addresses most of the requirements specified in the task, including data loading and preprocessing, model training and comparison, hyperparameter tuning, evaluation using multiple metrics, creation of residual plots and feature importance visualizations, and implementation of cross-validation. The code also makes predictions for new hypothetical countries and provides a basic analysis of the results. However, there are a few areas where the code could be improved to achieve a perfect score. Firstly, the handling of missing values is limited to filling NaNs with the mean for numerical columns, which might not be the best approach for all datasets. Additionally, the code could benefit from more detailed documentation and comments to explain the purpose of each section and the reasoning behind certain decisions. Finally, the code could be more flexible by allowing the user to easily switch between different models and hyperparameter tuning techniques. Overall, the code is well-structured and effectively solves the task, but with a few tweaks, it could be even more robust and user-friendly.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_classif, RFE, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import seaborn as sns

# Suppress warnings for cleaner outp...",7,"The generated code demonstrates a good understanding of the task requirements, including creating engineered features, applying polynomial transformations, performing feature scaling, dimensionality reduction, and evaluating feature importance. The code is generally well-structured and readable. However, there are several areas that could be improved to achieve a higher score. 

One of the main issues is the incomplete implementation of some steps, such as the correlation-based feature filtering, which only plots the correlation matrix but does not actually remove highly correlated features. Additionally, the t-SNE visualization is not correctly implemented as it tries to visualize the t-SNE result directly without checking if the t-SNE transformation was successful.

Another area for improvement is the handling of missing values and non-finite values. While the code attempts to drop rows with NaNs, it does not handle the case where a feature becomes non-finite after scaling or polynomial transformation. This could lead to errors or incorrect results in subsequent steps.

The code could also benefit from more detailed comments and explanations, especially for the more complex steps like polynomial feature transformation and t-SNE visualization. This would make the code easier to understand for others and for the author themselves when reviewing the code.

Finally, the code could be more efficient by using more vectorized operations and by avoiding the creation of temporary DataFrames whenever possible. This would make the code more concise and faster to execute.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# 1) Load the dataset, parse the 'date' column, and filter for 'location' == 'USA'.
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
df_usa = df[df['location'] == 'USA'].copy()

# 2) Set the 'date' column as the index for the filtered data.
df_usa.set_...",8,"The generated code is overall correct and effectively solves the task of performing time series analysis on the 'new_cases' for the location 'USA' from the synthetic_covid19_data.csv dataset. It correctly loads the dataset, filters for the specified location, sets the 'date' column as the index, resamples the data to a weekly frequency, performs time series decomposition, conducts the Augmented Dickey-Fuller (ADF) test, applies first-order differencing if necessary, and plots the ACF and PACF of the stationary time series. The code quality is also good, with proper imports and a logical structure. However, there are a few areas for improvement to achieve a higher score. Firstly, the code lacks documentation, which would make it easier to understand the purpose of each section and how the code is intended to be used. Secondly, some of the variable names could be more descriptive, which would improve the readability of the code. Finally, the code does not handle potential errors as robustly as the ground truth code, such as handling missing values or files that do not exist. Overall, the code is strong, but with some additional attention to detail and best practices, it could be even more robust and maintainable.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1) Categorize wines based on 'pH' values
def categorize_ph(pH):
    if pH < 3.2:
        return 'Low pH'
    elif 3.2 <= pH <= 3.4:
        return 'Medium pH'
    else:
        return 'High pH'

df['ph_group'] = df['pH'].apply(categorize_ph)

# Ensure the order of categories for consistent plotting
ph_group_order = ['Low pH', ...",7,"The generated code is mostly correct and solves the task, but there are some areas for improvement to achieve a higher score. 

One of the primary strengths of the generated code is that it correctly categorizes wines based on 'pH' values, calculates the average 'quality' for each pH group, performs an ANOVA test, and creates a visualization showing average qualities with error bars representing 95% confidence intervals. The code is also well-structured and uses proper imports.

However, there are a few areas for improvement. Firstly, the code does not handle the case where the CSV file is not found in the current directory, which is handled in the ground truth code. 

Secondly, the calculation of the 95% confidence intervals in the generated code is simplified and does not account for the degrees of freedom, which is a more accurate approach. 

Lastly, the visualization in the generated code could be improved by adding more labels and a title, and by using a more consistent color scheme.

To achieve a higher score, the generated code should address these areas and provide a more robust and accurate solution.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load the dataset
df = pd.read_csv('synthetic_c...",8,"The generated code is largely correct and solves the task with some minor deviations from the ground truth code. It properly imports necessary libraries, structures the code in a logical manner, and addresses all requirements. The use of pipelines and transformers from scikit-learn simplifies the preprocessing and modeling steps. However, there are a couple of areas where the code slightly differs from the ground truth, such as handling missing values in the target variable and encoding the 'continent' column. Specifically, the generated code uses the median of the target variable to impute missing values, whereas the ground truth code imputes missing values in both the features and the target variable using the median. Additionally, the generated code uses a simple imputer with the most frequent strategy for the 'continent' column, whereas the ground truth code uses one-hot encoding after imputing with the most frequent value. Despite these minor deviations, the code is well-organized and effectively solves the regression modeling task. To achieve a perfect score, the generated code should align more closely with the ground truth code in terms of handling missing values and encoding categorical variables.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Load the dataset
try:
    df = pd.read_csv('synthetic_covid19_data.csv')
except FileNotFoundError:
    print(""Error: 'synthetic_covid19_data.csv' not found. Please make sure the file is in the correct directory."")
    exit()

# Define columns for imputation
impute_cols = [
    'reproduction_rate',
    'stringency_index',
    'median_age',
    'total_deaths',
    'populati...",8,"The generated code is overall correct and addresses the main requirements of the task, including ANOVA for 'reproduction_rate' by continent, Pearson correlation for 'stringency_index' and 'new_cases', and a t-test for 'median_age' based on a derived high death rate category. It also properly handles missing values and includes necessary imports. However, there are a few areas for improvement to achieve a perfect score. Firstly, the code could benefit from more comprehensive error handling, especially when dealing with file not found errors or insufficient data for certain tests. Secondly, some of the variable names and comments could be more descriptive to enhance code readability. Lastly, the code assumes equal variances for the t-test, which might not always be the case; using Welch's t-test could provide more robust results. Overall, the code is well-structured and effectively solves the problem, but with some adjustments, it could be even more reliable and efficient.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select numerical columns
numerical_cols = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
numerical_df = df[numerical_cols]

# Calculate the pairwise correlation matrix
correlation_matrix = numerical_df.corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot...",8,"The generated code is mostly correct and solves the task, demonstrating a good understanding of the required steps. It correctly loads the dataset, selects the numerical columns, calculates the pairwise correlation matrix, and visualizes it using a heatmap. The code quality is also good, with proper imports and a clear structure. However, to achieve a higher score, the code could benefit from including error handling mechanisms similar to the ground truth code, such as checking if the specified columns exist in the dataset and handling potential exceptions like file not found errors. Additionally, the generated code lacks a clear function definition and docstrings, which are important for readability and maintainability. Overall, the code is strong, but minor enhancements in error handling and code organization would be beneficial.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Group by 'rarity' and calculate the mean of specified columns
average_stats_by_rarity = df.groupby('rarity')[['elixirCost', 'Win Rate %', 'Usage %']].mean()

# Display the calculated average values
print(average_stats_by_rarity)
```",6,"The generated code is partially correct and solves the task to some extent. It correctly loads the dataset and groups the data by the `rarity` column. However, it fails to address the task requirement of calculating the average `elixirCost`, `Win Rate`, and `Usage` as specified in the ground truth code. The generated code calculates the mean of `elixirCost`, but with incorrect column names for `Win Rate` and `Usage` (`Win Rate %` and `Usage %`), which do not match the required columns. To improve the code, the generated code should be modified to use the correct column names as specified in the ground truth code. Additionally, the code could benefit from more descriptive variable names and comments to enhance readability and understandability.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Create the binary target column 'is_high_win_rate'
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)

# Select features
features = ['elixirCost', 'Usage', 'rarity']
X = df[features]
y = df['is_high_win_rate']

# Apply one-hot encoding to 'rarity'
encoder = OneHotEncoder(handle_unknown='ignore'...",8,"The generated code is generally correct and complete, solving the task of preparing data from the clash_royale_cards.csv for a classification task. It correctly loads the dataset, creates the `is_high_win_rate` target column, selects the specified features, applies one-hot encoding to the categorical `rarity` column, and splits the data into training and testing sets with the specified parameters. The code quality is good, with proper imports and a clear structure. However, there is room for improvement in terms of conciseness and alignment with the ground truth code's approach for one-hot encoding. Specifically, the generated code uses `OneHotEncoder` from scikit-learn, which is not necessary when using `pd.get_dummies` as in the ground truth code. Additionally, the generated code does not include any error handling or verification steps, such as checking for the existence of the CSV file or printing shapes to verify the split, which are present in the ground truth code. Overall, the code is strong, but a few adjustments could make it more efficient and consistent with best practices.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 1) Load the dataset
df = pd.read_csv('iris.csv')

# 2) Define features X
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# 3) Define the target y
y = df['Species']

# 4) Encode the categorical Species target variable into numerical labels
label_...",9,"The generated code effectively solves the task with minor omissions compared to the ground truth code. It correctly loads the dataset, selects the specified features and target, encodes the categorical target variable, splits the data into training and testing sets, trains a K-Nearest Neighbors classifier, and reports the accuracy score. The code is well-structured, and proper imports are included. However, to achieve a perfect score, it would be beneficial to add error handling for the case when the 'iris.csv' file is not found, similar to the ground truth code. Additionally, the code could benefit from including a comment or description explaining the purpose of each section, which would enhance readability and understandability. Overall, the generated code is robust and effectively accomplishes the assigned task with only minor suggestions for improvement.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
df = pd.read_csv('iris.csv')

# 2) Filter the data to get two groups
versicolor_petal_length = df[df['Species'] == 'Iris-versicolor']['PetalLengthCm']
virginica_petal_length = df[df['Species'] == 'Iris-virginica']['PetalLengthCm']

# 3) Perform an independent two-sample t-test

# 4) Clearly state the null and alternative hypotheses for this test.
# Null Hypothesis (H0): There is no statistically significant difference i...",9,"The generated code is very close to the ground truth code and correctly solves the task. It properly loads the dataset, filters the data for the specified species, performs an independent two-sample t-test, states the null and alternative hypotheses, and interprets the result based on a significance level of alpha = 0.05. The code is well-structured and has proper imports. The only minor issues are that it lacks clear print statements for the null and alternative hypotheses, and the edge case handling for empty groups is missing. With these minor adjustments, the code could be considered perfect.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: 'iris.csv' not found. Make sure the file is in the the current directory."")
    exit()

# Define the numerical features
numerical_features = [
    'SepalLengthCm',
    'SepalWidthCm',
    'PetalLengthCm',
    'PetalWidthCm'
]

# Create box plots for each numerical feature, grouped by 'Species'
for feature in nume...",8,"The generated code is very close to the ground truth code and effectively solves the given task. It correctly loads the iris dataset, generates box plots for each of the four numerical features, and ensures that the plots are grouped by the 'Species' column. The code also includes proper imports, uses a try-except block for error handling during dataset loading, and displays each plot with a clear title and axis labels. However, to achieve a perfect score, the generated code could benefit from more explicit error handling for cases other than a FileNotFoundError, as seen in the ground truth code. Additionally, the palette argument in the seaborn.boxplot function is not mentioned in the requirements, so while it improves the plot's aesthetic, it's not strictly necessary for the task at hand. Overall, the code is well-structured, and with minor adjustments for more comprehensive error handling, it would be nearly flawless.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# 1) Load the dataset
try:
    df = pd.read_csv(""heart.csv"")
except FileNotFoundError:
    print(""Error: heart.csv not found. Make sure the file is in the current directory."")
    exit()

#...",9,"The generated code is very close to the ground truth code and correctly solves the task. It properly loads the dataset, selects the specified features and target variable, one-hot encodes the categorical features, splits the data into training and testing sets, trains a Logistic Regression model, and reports the accuracy score. The code is well-structured, and imports are correct. The only minor issues are the lack of a specific error message when the file is not found (beyond what's provided by the `exit()` function) and the use of `solver='liblinear'` in the Logistic Regression model, which is not necessary and could be removed for default behavior. Additionally, the model accuracy is printed without mentioning it's on the test set, which is a minor oversight. Overall, the code is strong, and these are minor suggestions for further improvement.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
try:
    df = pd.read_csv('heart.csv')
except FileNotFoundError:
    print(""Error: heart.csv not found. Make sure the file is in the current directory."")
    exit()

# 2) Create two groups based on the Sex column
male_cholesterol = df[df['Sex'] == 'M']['Cholesterol'].dropna()
female_cholesterol = df[df['Sex'] == 'F']['Cholesterol'].dropna()

# 4) State the null and alternative hypotheses
print(""Null Hypothesis (H0): The...",8,"The generated code is largely correct and effectively solves the task. It properly loads the dataset, creates groups based on the `Sex` column, performs an independent two-sample t-test, states the null and alternative hypotheses, reports the t-statistic and p-value, and interprets the result based on a significance level of alpha = 0.05. The code is well-structured, and the use of try-except blocks for error handling is a good practice. However, there are a few minor issues that prevent it from being a perfect score. Firstly, the code lacks clear function definition like the ground truth code, which makes it less modular and reusable. Secondly, it does not check if the required columns ('Sex' and 'Cholesterol') exist in the dataset, which could lead to errors if these columns are missing. Lastly, the code could benefit from more detailed error messages and handling, especially in the t-test result interpretation section, to provide clearer conclusions based on the p-value. Overall, the code is strong, but with a few adjustments, it can be made even more robust and comprehensive.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: 'winequality-red.csv' not found. Please make sure the CSV file is in the current directory."")
    exit()

# Columns to analyze
columns_to_analyze = ['volatile acidity', 'residual sugar']

# Store results for comparison
original_stats = {}
winsorized_stats = {}

# Create a copy for winsorization
df_winsorized = df.copy()

for col in colu...",8,"The generated code is generally correct in solving the task, accurately implementing the IQR method for outlier identification, and performing winsorization. The code structure is clean, and proper imports are used. However, there are a few areas for improvement to achieve a higher score. Firstly, the code lacks detailed reporting for each step of the process, similar to the ground truth code, which provides a clearer understanding of the outlier identification and winsorization process. Secondly, the code does not calculate the percentage change in mean and standard deviation after winsorization, which is presented in the ground truth code for better comparison. Lastly, while the code creates a copy of the DataFrame for winsorization, it does not include a print statement to show the first few rows of the winsorized dataset, similar to the ground truth code, to visually verify the changes. Addressing these points can enhance the code's completeness and transparency.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('heart.csv')
except FileNotFoundError:
    print(""Error: 'heart.csv' not found. Make sure the file is in the current directory."")
    exit()

# Group the data to count HeartDisease for each ChestPainType
grouped_data = df.groupby(['ChestPainType', 'HeartDisease']).size().unstack(fill_value=0)

# Create the grouped bar chart
plt.figure(figsize=(10, 6))
grouped_data.plo...",8,"The generated code accurately solves the task by loading the dataset, creating a grouped bar chart showing the count of `HeartDisease` for each category in the `ChestPainType` column, and includes proper labels, title, and legend. The code structure is clear, and the use of seaborn for plotting is appropriate. However, there are a few areas for improvement to achieve a higher score. Firstly, the generated code lacks the handling of potential errors that may occur during dataset loading, similar to the ground truth code. It only checks for a `FileNotFoundError` and exits the program, which may not be sufficient for all error scenarios. Additionally, the code does not ensure that the required columns ('ChestPainType' and 'HeartDisease') exist in the dataset, which could lead to errors if these columns are missing. Finally, while the code creates a grouped bar chart, it does not explicitly map the `HeartDisease` values to more descriptive labels ('No Heart Disease' and 'Heart Disease') as seen in the ground truth code, which could improve the readability of the legend.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('spotify_2023.csv')
except FileNotFoundError:
    print(""Error: spotify_2023.csv not found. Make sure the file is in the current directory."")
    exit()

# Clean 'in_shazam_charts' column
df['in_shazam_charts_cleaned'] = df['in_shazam_charts'].astype(str).str.replace(',', '', regex=False)
df['in_shazam_charts_cleaned'] = pd.to_numeric(df['in_shazam_charts_cleaned'], errors='coerce')

nan_count = df['in_shazam_charts_clea...",7,"The generated code generally solves the task with some minor adjustments needed for compliance with the requirements. It correctly loads the dataset, cleans the `in_shazam_charts` column, groups the data by `key` and `mode`, and calculates the average `danceability_%` and `valence_%` for each group. However, there are a few areas for improvement:
- The generated code creates a new column `in_shazam_charts_cleaned` instead of cleaning the original `in_shazam_charts` column as required. This is not a critical error but deviates from the ground truth code.
- The warning message for NaNs is printed as a warning, whereas the ground truth code prints it as an information message labeling it as the count of NaNs after cleaning and coercion.
- The cleaning of `danceability_%` and `valence_%` columns is not required according to the task description, as it assumes these columns are already numeric. The generated code's approach to convert them to numeric might not be necessary if the dataset is correctly loaded.
- The structure and imports are proper, and the code is well-organized. However, to achieve a higher score, the code should adhere more closely to the ground truth in terms of column handling and message formatting.
- To fully meet the requirements, the generated code should be adjusted to either clean the original `in_shazam_charts` column or provide a clear reason for creating a new column. Additionally, the message about NaNs should be formatted to match the ground truth code's level of detail.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# 1) Load the dataset
try:
    df = pd.read_csv('spotify_2023.csv')
except FileNotFoundError:
    print(""Error: 'spotify_2023.csv' not found. Make sure the file is in the current directory."")
    exit()

# 2) Ensure the `streams` column is numeric
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')...",9,"The generated code is highly accurate and effectively solves the given task. It properly loads the dataset, ensures the `streams` column is numeric, selects the correct features and target, splits the data into training and testing sets, trains a Linear Regression model, and evaluates the model on the test set, reporting the R-squared (R2) score and Mean Squared Error (MSE). The code quality is good, with proper imports and a clear structure. The only area for slight improvement is in error handling during file loading, as it simply exits the program without providing additional diagnostic information. Additionally, it would be beneficial to include print statements or comments to explain the steps of the process, making the code even more readable and understandable. Overall, the code is well-structured, and its functionality aligns closely with the ground truth code.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

def feature_engineer_spotify(file_path='spotify_2023.csv'):
    # 1) Load the dataset
    df = pd.read_csv(file_path)

    # 2) Combine released_year, released_month, and released_day into a single datetime column
    def create_release_date(row):
        try:
            return pd.to_datetime(f""{int(row['released_year'])}-{int(row['released_month'])}-{int(row['released_day'])}"")
        except (ValueError, TypeError):
            return pd.NaT

    df['release_dat...",8,"The generated code demonstrates a good understanding of the task and implements the required feature engineering steps. It correctly loads the dataset, combines the release date columns into a single datetime column, creates a new feature for the number of days since release, and displays the specified columns for the first 5 tracks. The code is well-structured, and the use of a function to encapsulate the feature engineering process is a good practice. However, there are a few areas for improvement to achieve a perfect score. Firstly, the code does not handle potential errors during CSV loading, which is crucial for a robust implementation. Secondly, the code uses a simple try-except block to handle date conversion errors, which might not be sufficient for all edge cases. Lastly, the code creates a dummy dataset for testing, which is not necessary and makes the code less relevant to the actual task. Overall, the code is solid, but with some additional error handling and refinement, it can be even better.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Video views' and 'Likes' columns
for col in ['Video views', 'Likes']:
    df[col] = df[col].astype(str).str.replace(',', '', regex=False)
    df[col] = pd.to_numeric(df[col], errors='coerce')
    df[col] = df[col].fillna(0)

# Fill missing values in 'Category'
df['Category'] = df['Category'].fillna('Unknown')

# Group by 'Category' and calculate sums
category_summary = df.groupby('Category')[['V...",8,"The generated code is effective in solving the task with some minor areas for improvement. It correctly loads the dataset, cleans the 'Video views' and 'Likes' columns by removing commas and converting them to numeric types, fills missing values in the 'Category' column with 'Unknown', and then groups the data by 'Category' to calculate the sum of cleaned 'Video views' and 'Likes'. The code is well-structured and readable. However, to achieve a perfect score, it would be beneficial to include error handling for cases like a missing file or columns not found in the dataset, as seen in the ground truth code. Additionally, the generated code assumes that both 'Video views' and 'Likes' columns are always present, which is not handled explicitly like in the ground truth code.Overall, the code is solid and effective for its intended purpose with a minor need for additional error handling and column existence checks.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats
import numpy as np

# 1) Load the dataset
try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: youtube_trendings.csv not found. Please make sure the file is in the current directory."")
    exit()

# 2) For the Likes column:
df['Likes'] = df['Likes'].astype(str).str.replace(',', '')
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce')
df['Likes'] = df['Likes'].fillna(0)

# 3) Fill any missing valu...",8,"The generated code performs well, accurately solving the task of loading the dataset, cleaning the `Likes` column, filling missing values in the `Category` column, creating groups for the t-test, performing the independent two-sample t-test, and interpreting the results based on a significance level of alpha = 0.05. It correctly implements the null and alternative hypotheses for the test and provides a clear conclusion based on the p-value. However, there are a few areas for improvement to reach a perfect score. Firstly, the code could benefit from more detailed error handling, especially when dealing with file not found errors. Secondly, the code could utilize more descriptive variable names, which would enhance readability. Lastly, including a check to ensure that both groups have at least two samples before performing the t-test, as shown in the ground truth code, would make the code more robust. Despite these minor suggestions, the generated code is well-structured and effectively addresses all the requirements of the task.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: 'youtube_trendings.csv' not found. Make sure the file is in the current directory."")
    exit()

# Function to safely convert to numeric, handling commas and NaNs
def safe_numeric_conversion(series):
    cleaned_series = series.astype(str).str.replace(',', '', regex=False)
    numeric_series = pd.to_numeric(cleaned_series, errors='coe...",8,"The generated code is largely correct and effectively solves the task of loading the youtube_trendings.csv dataset, cleaning the 'Likes' and 'Dislikes' columns, creating new columns for 'TotalEngagement' and 'LikeDislikeRatio', and displaying the required columns. It properly handles errors, such as missing files and non-numeric values, and uses a function to simplify the numeric conversion process. However, to achieve a perfect score, the code could benefit from additional error handling for cases beyond the specified file not found error, and including more explicit comments or docstrings to explain the purpose of each section and the function. Additionally, the code could be more complete by including optional steps, such as displaying data types and a sample of the DataFrame after feature engineering, to provide a comprehensive verification of the cleaning and new column creation process.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean 'Launched Price (USA)' column
def clean_price(price_str):
    try:
        if isinstance(price_str, str):
            return float(price_str.replace('USD ', '').replace(',', ''))
        return float(price_str)
    except ValueError:
        return pd.NA

df['Launched Price (USA)'] = df['Launched Price (USA)'].apply(clean_price)

# Clean 'Screen Size' column
def clean_screen_size(size_str):
    try:
 ...",6,"The generated code demonstrates a good understanding of the task, correctly loading the dataset and cleaning the specified columns. However, it falls short in a few areas. Firstly, the code does not handle conversion errors as explicitly as the ground truth code does, relying on `pd.NA` which might not be as readable or explicit for someone familiar with the `errors='coerce'` parameter in `pd.to_numeric`. Secondly, the code does not provide informative print statements at each step, making it less transparent how the data is being processed. Additionally, the code does not calculate and display the number of NaN values introduced during cleaning, which is a crucial step for data quality assessment. Furthermore, the column names in the aggregated data frame could be more descriptive, following the same pattern as the ground truth code. Finally, the generated code does not include error handling for file not found or general exceptions during the loading process, which is a good practice for robustness. Addressing these points can enhance the code's readability, maintainability, and robustness.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

def clean_camera_mp(camera_spec):
    if pd.isna(camera_spec):
        return 0
    match = re.search(r'(\d+)', str(camera_spec))
    if match:
        return int(match.group(1))
    return 0

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Apply the cleaning function to create new columns
df['Cleaned Front Camera'] = df['Front Camera'].apply(clean_camera_mp)
df['Cleaned Back Camera'] = df['Back Camera'].apply(clean_camera_mp)

# Create 'Total ...",9,"The generated code is nearly identical to the ground truth code in terms of functionality and structure. It correctly loads the CSV dataset, cleans the camera specifications by extracting the first sequence of digits as MP values, creates a new feature for total camera MP, and displays the required columns for the first 10 rows. The code quality is also high, with proper imports and a clear, concise structure. The only minor improvement that could be suggested is the addition of a try-except block to handle the case where the 'mobiles_2025.csv' file is not found in the current directory, similar to the ground truth code. Additionally, including a print statement to confirm the dataset has been loaded successfully, as seen in the ground truth code, could enhance the code's readability and provide feedback to the user. Overall, the generated code is very strong and effective in solving the given task.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# 1) Load the dataset
try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: mobiles_2025.csv not found. Make sure the file is in the current directory."")
    exit()

# 2) Clean specified columns
def clean_numeric_column(series, suffix, dtype=float):
    cleaned_series = p...",8,"The generated code is largely correct and completes the task of building a regression model using the specified dataset. It properly loads the dataset, cleans the specified columns, defines features and a target variable, splits the data into training and testing sets, trains a Linear Regression model, and evaluates the model on the test set. The code is well-structured, and the use of a function to clean numeric columns is a good practice. However, there are a few areas for improvement to achieve a perfect score. Firstly, the code does not handle the case where the file 'mobiles_2025.csv' is found but is empty, which could lead to errors downstream. Secondly, the code does not check if the required columns are present in the dataset before attempting to clean them, which could result in errors if the columns are missing. Additionally, the code could benefit from more comprehensive error handling and potentially include a check for the presence of any other required columns after cleaning. Finally, while the code reports the R-squared score and Mean Absolute Error, it would be beneficial to include these metrics in a more structured output, such as a dictionary or a DataFrame, for easier interpretation and further analysis.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
import scipy.stats as stats
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('titanic.csv')
except FileNotFoundError:
    print(""Error: 'titanic.csv' not found. Make sure the file is in the current directory."")
    exit()

alpha = 0.05

# 1. Chi-Square Test: Pclass vs. Survival
print(""--- Chi-Square Test: Pclass vs. Survival ---"")
contingency_table = pd.crosstab(df['Pclass'], df['Survived'])
chi2, p_value_chi2, dof, expected = stats.chi2_contingency(...",8,"The generated code is quite accurate and effectively solves the task of performing statistical analysis and hypothesis testing on the titanic.csv dataset. It correctly conducts a chi-square test, t-test, and ANOVA, calculates effect sizes for each test, and interprets the results with proper statistical methodology. The code structure is generally good, with clear section headers and print statements that help in understanding the output. However, there are a few areas that could be improved to achieve a perfect score. Firstly, the code could benefit from more robust error handling, especially when dealing with missing values or empty datasets. Additionally, some calculations, such as the effect sizes, could be wrapped in functions to improve code readability and reusability. Finally, the code could benefit from more detailed comments explaining the purpose of each section and the statistical concepts being applied, especially for readers without a strong statistical background. Overall, the code is well-structured, and with a few refinements, it could be even more effective and easier to understand.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Group by Year and calculate average Rating and Runtime
yearly_stats = df.groupby('Year').agg(
    Avg_Rating=('Rating', 'mean'),
    Avg_Runtime=('Runtime', 'mean')
).reset_index()

# Create a line plot for average Rating over Year
plt.figure(figsize=(12, 6))
sns.lineplot(data=yearly_stats, x='Year', y='Avg_Rating')
plt.title('Trend of Average IMDb Ratin...",6,"The generated code is generally correct in terms of solving the task, with a good structure and proper imports. However, it falls short in a few areas. Firstly, it lacks error handling mechanisms, which is crucial for a robust data analysis code. The ground truth code includes try-except blocks to handle potential errors such as a missing file, missing columns, and conversion errors, which are not present in the generated code. Secondly, the generated code does not clean the 'Year' and 'Runtime' columns as thoroughly as the ground truth code, which might lead to incorrect calculations if the data is not properly cleaned. Lastly, the generated code uses seaborn for plotting, which is not specified in the task, and does not include a title and labeled axes for the plot as required. To improve, the generated code should incorporate error handling, thorough data cleaning, and adhere to the specified plotting requirements.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 2) Create two groups of movies based on the Year column
group_before_1990 = df[df['Year'] < 1990]['Rating']
group_1990_and_after = df[df['Year'] >= 1990]['Rating']

# 3) Perform an independent two-sample t-test

# 4) Clearly state the null and alternative hypotheses for this test.
# Null Hypothesis (H0): There is no statistically significant difference in the mean Rating between m...",6,"The generated code attempts to solve the task but falls short in several areas. 
1. Correctness: It mostly solves the task but fails to handle cases where the 'Year' or 'Rating' columns are not in numeric format, which could lead to errors if the dataset is not as expected. 
2. Code quality: The structure is somewhat disorganized, and it lacks clear statements about ensuring the 'Year' and 'Rating' columns are properly formatted before proceeding with the analysis. 
3. Completeness: The code misses crucial steps like checking for sufficient data in each group before performing the t-test and stating the null and alternative hypotheses clearly. 
Specifically, the generated code assumes the 'Year' and 'Rating' columns are already in numeric format, which may not always be the case. It also directly performs the t-test without checking if there are enough data points in each group, which is a critical assumption for the t-test to be valid. Additionally, using Welch's t-test without a clear reason for assuming unequal variances is a weakness. The code lacks the explicit statement of the null and alternative hypotheses as seen in the ground truth code, which makes the purpose of the t-test less clear. 

To improve, the generated code should include error handling for non-numeric data, check for sufficient data in each group, and provide clear statements of the null and alternative hypotheses. It should also consider the assumption of equal variances more carefully.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create the Decade column
df['Decade'] = (df['Year'] // 10 * 10).astype(str) + 's'

# Display Title, Year, and Decade for the first 10 movies
print(df[['Title', 'Year', 'Decade']].head(10))
```",6,"The generated code is mostly correct and solves the task, but it lacks proper error handling and feature engineering functionality. It correctly loads the dataset and creates the `Decade` column, but it does not handle potential errors such as a missing file or columns. The code also does not ensure that the `Year` column is numeric before performing the division, which could lead to errors if the column contains non-numeric values. Additionally, the code does not drop rows where the `Year` is NaN after coercion, which could result in incorrect decade calculations. The structure and completeness of the code are also improved by the use of a `main` function and the display of the first 10 movies, but it could be further enhanced by adding more error checking and handling. To achieve a higher score, the code should be modified to include these additional features.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Numerical columns for imputation
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']

# Impute missing numerical values with the mean
for col in numerical_cols:
    mean_val = df[col].mean()
    df[col].fillna(mean_val, inplace=True)

# Impute missing categorical 'sex' values with the mode
mode_sex = df['sex'].mode()[0]
df['sex'].fillna(mode_sex, inplace=True)

# Group b...",8,"The generated code is mostly correct and complete in solving the task. It properly loads the dataset, imputes missing numerical values with their means, imputes missing categorical 'sex' values with the mode, groups the data by species and imputed sex, and calculates the average body_mass_g for each group. The code quality is good with proper imports and a straightforward structure. However, to achieve a perfect score, the code could benefit from error handling similar to the ground truth code to handle potential exceptions such as a missing file, unknown columns, or other unexpected errors. Additionally, displaying the initial and final count of missing values for verification, as shown in the ground truth code, could enhance the code's completeness and robustness. Overall, the generated code is a strong solution that just needs a bit of extra error handling and verification steps to make it completely reliable.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: penguins.csv not found. Make sure the file is in the current directory."")
    exit()

# 2) Filter the data to include only records where species is 'Adelie' or 'Gentoo'
filtered_df = df[df['species'].isin(['Adelie', 'Gentoo'])].copy()

# 3) Within this filtered dataset, remove any rows where flipper_length_mm or species is missing (NA)...",9,"The generated code is very close to the ground truth code and correctly solves the task. It properly imports the necessary libraries, loads the dataset, filters the data for 'Adelie' and 'Gentoo' species, removes rows with NAs in 'flipper_length_mm' or 'species', performs an independent two-sample t-test, states the hypotheses, reports the t-statistic and p-value, and interprets the result based on a significance level of alpha = 0.05. The code is also well-structured and easy to follow. The only minor improvement that could be made is to include error handling for the case where one or both species groups are empty after filtering and NA removal, as well as to check if there are at least 2 data points for each species group before performing the t-test. Additionally, the code could benefit from more detailed comments to explain the purpose of each section, but overall, the code is strong and effectively solves the problem.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Select the features X and target y
feature_columns = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
target_column = 'species'

X = df[feature_columns]
y = df[target_column]

# 3) Drop ...",8,"The generated code is mostly correct and solves the task efficiently. It properly loads the dataset, selects the specified features and target, drops rows with missing values, encodes the categorical target, splits the data into training and testing sets, trains a logistic regression model, and reports the accuracy score. However, there are a few areas that could be improved for a higher score. Firstly, the generated code does not include any error handling, such as checking if the required columns exist in the dataset or handling the case where the CSV file is not found. Secondly, the code does not provide as much feedback as the ground truth code, such as printing the number of rows dropped due to missing values or the encoded classes. Lastly, the code could benefit from more comments to explain the purpose of each section, although it is generally well-structured and easy to follow. Overall, the code is solid and effectively solves the task, but with a few enhancements, it could be even more robust and informative.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'penguins.csv' not found. Make sure the file is in the current directory."")
    exit()

# 2) Drop rows where `body_mass_g` or `species` is NA
df_filtered = df.dropna(subset=['body_mass_g', 'species'])

# 3) For each `species`, calculate the average `body_mass_g`
average_body_mass = df_filtered.groupby('spe...",9,"The generated code is very close to solving the task correctly and efficiently. It properly imports the necessary libraries, loads the dataset, drops rows with missing values in the specified columns, calculates the average body mass for each species, and creates a bar plot to visualize the results. The code is well-structured and easy to follow.

The primary strengths of the generated code include:
- Correct handling of missing values by dropping rows where `body_mass_g` or `species` is NA.
- Accurate calculation of the average `body_mass_g` for each `species` using the `groupby` and `mean` functions.
- Clear and readable plotting with a suitable title, labeled axes, and a grid for better visualization.

However, to achieve a perfect score of 10, a few minor improvements could be made:
- Adding more informative print statements to track the dataset loading, NA removal, and calculation steps, similar to the ground truth code, would enhance transparency and debugging capabilities.
- Considering error handling for cases where the `groupby` operation might return an empty DataFrame (in case all species have NA values for `body_mass_g`) could make the code more robust.
- Including a comment or docstring to explain the purpose of the code and its main sections would improve readability and maintainability.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'penguins.csv' not found. Make sure the file is in the current directory."")
    exit()

# 2) Drop rows where culmen_length_mm, culmen_depth_mm, flipper_length_mm, or body_mass_g are NA
df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'], inplace=True)

# 3) Create a...",8,"The generated code is generally correct and solves the task, demonstrating a good understanding of the required operations. It correctly loads the dataset, drops rows with NA values in the specified columns, creates the `culmen_area` feature, calculates the Pearson correlation matrix, and visualizes it as a heatmap. The code structure is clear, and the use of pandas and seaborn libraries is appropriate. However, to achieve a perfect score, the code could benefit from additional error handling, such as checking if the dataset is empty after dropping NA values, and including a message to confirm that the dataset has been loaded successfully. Additionally, the generated code does not include print statements to confirm the number of rows before and after dropping NA values, which could be helpful for debugging purposes. Overall, the code is well-organized, and minor enhancements would be necessary to match the completeness and robustness of the ground truth code.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Select the features and target
features = [
    'culmen_length_mm',
    'culmen_depth_mm',
    'flipper_length_mm',
    'body_mass_g',
    'island',
    'sex'
]
target = 'species'

X = df[features].copy()
y = df[target].copy()

# 3) Impute numerical features with the mean
numerical_cols = [
    'culmen_length_mm',
    'culmen_depth_mm',
    'flipper_length_m...",8,"The generated code is mostly correct and complete in solving the task. It properly loads the dataset, selects the specified features and target, imputes NA values in numerical features with the mean, handles the 'sex' column by imputing and mapping values as required, applies one-hot encoding to the 'island' column, and prints the shape and first 5 rows of the processed features. The code quality is also good, with proper use of pandas and a clear structure. However, there are a few minor areas that could be improved for a perfect score. For instance, the generated code does not handle the case where the dataset cannot be loaded, which is addressed in the ground truth code with a try-except block. Additionally, the code does not print the value counts for the target variable, which could be useful for confirming it's ready for the classification task. Overall, the generated code is strong and demonstrates a good understanding of the requirements, but a few additional checks and prints could enhance its completeness and robustness.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

# Create the DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST...",8,"The generated code is generally correct and solves the task. It properly creates the DataFrame, calculates the total spending for each 'CustomerID', identifies and prints the top 3 customers by total spending, and calculates the total quantity of items sold per category. The code structure is clear, and necessary imports are included. However, there are a few areas that could improve the code quality and completeness. Firstly, the code could benefit from more informative print statements, such as those in the ground truth code, to provide a clear separation between different sections of the output. Secondly, the code does not explicitly handle the case where there are less than 3 customers, it simply prints the top customers without any message indicating that there might be less than 3. Lastly, the code could include some error handling to ensure that the DataFrame is not empty and that the calculations are performed on the correct data. Overall, the code is solid, but with a few additional touches, it could be even more robust and user-friendly.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix...",8,"The generated code effectively addresses the task of building and evaluating a logistic regression model for survival prediction on the titanic dataset. It correctly performs data preprocessing, including handling missing values, encoding categorical variables, and feature scaling. The code also splits the data into training and testing sets, trains a logistic regression model, evaluates the model using multiple metrics, and visualizes the results. Additionally, it performs cross-validation to assess model stability.

However, there are a few areas where the code could be improved. Firstly, the code does not provide a clear display of feature coefficients and their interpretation as described in the ground truth code. While it prints the coefficients, it lacks the detailed explanation provided in the ground truth code. Secondly, the code could benefit from more detailed comments and docstrings to explain the purpose of each section and the reasoning behind certain preprocessing decisions. Finally, the code does not display the ROC-AUC score with the same level of detail as the ground truth code, which includes a plot of the ROC curve with a random classifier line for comparison. 

Overall, the generated code is well-structured and effectively solves the task, but with some improvements in documentation and result interpretation, it could be even more comprehensive and easier to understand.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']),
        'Ticker': ['TECHCO']*8,
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
        'Close': [152.0, 151.0, 153.5, 155.0, 1...",8,"The generated code is generally correct and solves the task by creating a pandas DataFrame, setting the 'Date' as the index, calculating the 3-day simple moving average (SMA) of the 'Close' price, and calculating the percentage change in 'Volume' from the previous day. The code also prints the resulting DataFrame with the new columns. However, there are a couple of areas for improvement. Firstly, the code is missing a clear description or comment explaining the purpose of the code and the calculations being performed. Secondly, the calculation for 'Volume_Pct_Change' is multiplied by 100, which is not necessary as per the ground truth code. This minor deviation could lead to confusion if the percentage values are being used in further calculations. Overall, the code is well-structured, and the use of pandas is proper, but a few tweaks can improve its clarity and adherence to the ground truth code.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]}

df...",9,"The generated code is very close to the ground truth code and correctly solves the task. It properly creates a pandas DataFrame, calculates the 'Score_Improvement', and identifies the students who meet the specified score and attendance criteria. The code is well-structured, and the imports are correct. The only minor difference is in the print statement for the average improvement per subject, where the generated code uses ""Average Score Improvement by Subject"" instead of ""Average Score Improvement per Subject"" used in the ground truth code. Additionally, the generated code directly prints the list of student IDs meeting the criteria, whereas the ground truth code checks if the result is not empty before printing. Overall, the code is clear, concise, and effectively solves the problem. To reach a perfect score, the code could be slightly modified to match the ground truth code's print statements and handling of empty result sets.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# 1) Filter the DataFrame
filtered...",8,"The generated code is generally correct, properly defines the DataFrame, filters it based on the specified criteria, and calculates the average number of ""ProjectsCompleted"" for the filtered subset. The code quality is good, with proper imports and a clear structure. However, there are a few areas for improvement to achieve a higher score. Firstly, the code does not handle the edge case where no employees match the filtering criteria. It would be beneficial to include a check to handle this situation, similar to the ground truth code, to provide a more robust solution. Secondly, the code could benefit from additional comments to explain the purpose of each section, making it easier for others to understand the solution. Lastly, the code could be made more flexible by considering potential errors that might occur during the execution, such as missing or invalid data. Overall, the generated code is a strong start, and with a few adjustments, it can be even more effective and reliable.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate conversion rate for Desktop users
desktop_users_df = df[df['DeviceType'] == 'Desktop']
deskt...",8,"The generated code is mostly correct and solves the task, creating the DataFrame and calculating the conversion rates for Desktop and Mobile users. The code quality is also good, with proper imports and a straightforward structure. However, there are a few areas for improvement to reach a perfect score. Firstly, the generated code does not handle the edge case where there are no users of a particular device type, which could result in NaN or errors. The ground truth code checks if the filtered DataFrames are empty before calculating the conversion rates, providing a warning message if no users are found. Secondly, the generated code uses a simple division, which could lead to precision issues if the number of users is very small. The ground truth code uses the `mean()` method on a boolean Series, which directly calculates the proportion of True values and is more precise. Lastly, the generated code could benefit from adding comments to explain the purpose of each section, making the code easier to understand for others. Nonetheless, the code is well-structured and effectively solves the task, earning an 8 out of 10.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

# Define the data
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

# Create the DataFrame
df = pd.DataFrame(data)

# Calculate and print the average Air...",9,"The generated code is excellent and effectively solves the task. It correctly defines the pandas DataFrame, calculates the average ""AirQualityIndex"" for each unique ""SensorID"", and prints the results. The code is well-structured, and proper imports are used. The only minor improvement that could be suggested is to format the average values to two decimal places for clarity, as shown in the ground truth code. This would make the output more readable. Additionally, considering error handling or edge cases, such as non-numeric values or missing data in the ""AirQualityIndex"" column, could further enhance the code's robustness. Nevertheless, the generated code is very close to the ground truth and effectively accomplishes the task as specified.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

# 1) Create a new feature ""EngagementScore""
df[""EngagementScore...",8,"The generated code accurately defines the DataFrame, calculates the ""EngagementScore"" feature, filters the DataFrame for Instagram posts, and calculates their average ""EngagementScore"". This shows a good understanding of the task requirements. However, to achieve a perfect score, the code could benefit from some improvements in code quality and completeness. For instance, the generated code does not handle the case where there are no Instagram posts in the DataFrame, which could lead to an error when trying to calculate the mean of an empty DataFrame. The ground truth code includes a check for this scenario and prints a meaningful message if no Instagram posts are found. Additionally, the generated code lacks optional print statements for verification, which can be helpful for debugging and understanding the intermediate results. Overall, the code is well-structured, and the calculations are correct, but a few enhancements would make it more robust and comprehensive.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male""...",8,"The generated code accurately defines the DataFrame, calculates the ""LengthOfStay"" column, filters the DataFrame for patients with a ""Flu"" diagnosis, and calculates the average ""LengthOfStay"" for these patients. It correctly solves the task and demonstrates good code quality with proper imports and structure. However, to achieve a perfect score, the code could benefit from additional print statements to display the intermediate results, such as the DataFrame with the ""LengthOfStay"" column and the filtered DataFrame for Flu patients, similar to the ground truth code. This would provide a clear understanding of the data transformation and filtering process. Additionally, considering the use of the `.copy()` method when filtering the DataFrame to avoid the SettingWithCopyWarning, as seen in the ground truth code, would further enhance the code's robustness.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}
df = pd.DataFrame(data)

# 1) Filter the DataFrame to...",9,"The generated code is very close to the ground truth code and correctly defines the DataFrame, filters for Sci-Fi movies, and calculates their average rating. The primary difference is the lack of a check for the edge case where no Sci-Fi movies are found in the DataFrame, which is handled in the ground truth code. The code is well-structured, and proper imports are used. The only improvement would be to add a check for the empty DataFrame case to make the code more robust. Overall, the code is clear, concise, and effectively solves the task.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

# 1) Filter the DataFrame to include only cars of the ""Toyota"" brand....",9,"The generated code accurately defines the DataFrame and filters it to include only cars of the ""Toyota"" brand, correctly calculating and printing the average ""Mileage"" for these cars. It closely follows the ground truth code, missing only the optional print statements for the original and filtered DataFrames, which does not affect its functionality. The code is well-structured, and the use of descriptive variable names like `toyota_cars` enhances readability. However, to achieve a perfect score, it could benefit from including some basic error handling, such as checking if the `toyota_cars` DataFrame is empty before calculating the average mileage to avoid potential errors. Nevertheless, the code effectively solves the task with high quality and completeness.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defen...",9,"The generated code is very close to the ground truth code and successfully solves the task. It correctly defines the DataFrame, filters for Fire-type Pokemon, and calculates their average Attack stat. The code quality is good with proper imports and structure. The only minor improvement that could be made is to handle the edge case where no Fire-type Pokemon are found, similar to the ground truth code. This would make the code more robust and handle all possible scenarios. Additionally, considering the division of the task into steps with comments, as seen in the ground truth code, could enhance readability and understanding for others. Overall, the generated code is solid and effectively achieves the desired outcome.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go

# Load the dataset
df = pd.read_csv('titanic.csv')

# Data cleaning and preparation
df['Age'] = df['Age'].fillna(df['Age'].median())
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])
df['Fare'] = df['Fare'].fillna(df['Fare'].median())
df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 12, 18, 60, 100],...",7,"The generated code solves the task with some minor discrepancies in the visualization and composition of the dashboard. It correctly addresses all requirements, including creating a multi-panel figure, a heatmap, violin plots, an interactive scatter plot, and a comprehensive dashboard. However, there are a few areas that could be improved for code quality and completeness.

1. **Inconsistencies in Data Preprocessing**: The generated code uses different methods for handling missing values in the 'Age' and 'Fare' columns compared to the ground truth code. While both methods are valid, consistency in approach would improve code readability and maintainability.

2. **Visualization Choices**: The selection of specific visualizations and their configurations could be more aligned with the insights the data is supposed to convey. For example, the use of a pie chart for survival rates by gender in the dashboard might not be the most effective choice for comparing rates.

3. **Dashboard Composition**: The layout and composition of the dashboard could be more intuitive and easier to navigate. The current arrangement seems to prioritize visual appeal over the logical grouping of related insights.

4. **Interactivity and Filterability**: While the code includes an interactive scatter plot, enhancing the dashboard with more interactive elements or filters could significantly improve its utility for exploratory data analysis.

5. **Annotations and Insights**: The annotations provided in the dashboard are informative but could be more strategically placed and designed for better readability and impact.

To achieve a higher score, addressing these areas would be crucial. This includes standardizing data preprocessing methods, optimizing visualization choices for clarity and insight, refining the dashboard's layout for better logic and user experience, and enhancing interactivity and filterability.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10]...",9,"The generated code is very close to the ground truth code and successfully solves the task. It correctly defines the DataFrame, filters for the ""Dystopian"" genre, and calculates the average stock for those books. The code is well-structured, and the use of pandas for data manipulation is appropriate. The only minor improvement could be to add a check to handle the edge case where no ""Dystopian"" books are found in the inventory, similar to the ground truth code. This would make the code more robust and user-friendly by providing a clear message when no books match the specified genre. Overall, the code is concise, readable, and effectively addresses all requirements.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
from scipy import stats
import numpy as np

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Drop rows with missing values
df.dropna(inplace=True)

alpha = 0.05

# 1) Conduct t-tests to compare mean values of each water quality parameter
print(""--- T-tests for Water Quality Parameters ---"")
numerical_features = [
    'ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate',
    'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity'
]

potable_df ...",8,"The generated code is overall well-structured and addresses the key requirements of the task, including performing t-tests for comparing mean values of water quality parameters, conducting a chi-square goodness-of-fit test for the distribution of potability, and applying Shapiro-Wilk normality tests. It also calculates Cohen's d for significant differences found by t-tests, which is a strong point. However, there are a few areas for improvement to reach a perfect score. Firstly, the code does not explicitly handle missing values in the dataset, instead opting to drop rows with missing values, which might not be desirable in all scenarios. The ground truth code demonstrates a more nuanced approach by imputing missing values with the mean of their respective columns. Secondly, while the code correctly identifies numerical features and performs the required statistical tests, it does not provide as detailed an interpretation of the results as the ground truth code, including effects size interpretations for Cohen's d based on specific thresholds. Lastly, the code assumes a uniform distribution for the expected frequencies in the chi-square goodness-of-fit test, which might not always be the case, depending on the specific expectations of the analysis. Overall, the code is solid and demonstrates a good understanding of the task requirements, but with some adjustments, it could be even more comprehensive and robust.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1...",8,"The generated code is generally correct and addresses most of the requirements for the task. It performs data preprocessing, trains multiple classification algorithms, evaluates models with various metrics, creates confusion matrices and ROC curves, and performs k-fold cross-validation. The code is well-structured and uses proper imports.

However, there are a few areas for improvement. Firstly, the code does not handle the case where the 'water_potability.csv' file is not found as robustly as the ground truth code. It simply prints an error message and exits, whereas the ground truth code provides more informative error handling.

 Secondly, the code does not implement the imputation and scaling steps within the k-fold cross-validation loop, which means that the cross-validation results might not accurately reflect the model's performance on unseen data. This is a critical oversight, as the goal of cross-validation is to evaluate the model's ability to generalize to new, unseen data.

 Lastly, the code could benefit from more detailed commenting and documentation, especially in the more complex sections, to make it easier for someone else to understand the code's logic and intent.

 Overall, the generated code is solid and demonstrates a good understanding of the task requirements, but with a few key adjustments, it could be even stronger.",gemini_2.5_flash_preview_5_20,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from itertools import combinations

# Load the dataset
try:
    df = pd.read_csv('water_potability.csv')
except FileNotFoundError:
    print(""water_potability.csv not found. Make sure the file is in the current directory."")
  ...",8,"The generated code generally addresses the task requirements, including feature engineering, correlation analysis, feature selection, and visualization. It correctly implements techniques like ratio creation, interaction terms, polynomial features, and applies SelectKBest and RFE for feature selection. The code is also well-structured, and the use of libraries like pandas, numpy, matplotlib, and seaborn is appropriate for data manipulation and visualization.

However, there are a few areas for improvement to reach a perfect score. Firstly, the handling of missing values could be more robust, as the current implementation fills all missing values with the mean, which might not be suitable for all datasets, especially if there are multiple types of missing values. Secondly, the code could benefit from more comprehensive error handling, such as checking if the file 'water_potability.csv' exists before attempting to read it, and handling potential errors during the feature engineering and selection processes. Additionally, the visualization sections could be more detailed, with clearer annotations and titles for better understanding.

In terms of completeness, the code covers most of the requirements but misses a detailed comparison of the feature sets selected by different methods, including a more in-depth analysis of the overlap and unique features selected by each method. Finally, the code could benefit from more comments and docstrings to explain the purpose of each section and the reasoning behind certain decisions, making it easier for others to understand and modify the code.",gemini_2.5_flash_preview_5_20,LLM_judge_config
