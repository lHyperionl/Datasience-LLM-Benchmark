task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('winequality-red.csv', sep=';')

# 1. Column names and data types
print(""\nColumn Names and Data Types:"")
print(df.dtypes)

# 2. Descriptive statistics for numerical columns
print(""\nDescriptive Statistics:"")
print(df.describe())

# 3. Dataset dimensions
print(""\nDataset Dimensions:"")
print(f""Number of rows: {df.shape[0]}"")
print(f""Number of columns: {df.shape[1]}"")

# 4. Identify categorical/ordinal columns
print(...",6,"The generated code correctly loads the dataset, prints column names and data types, provides descriptive statistics, and lists dataset dimensions. These are strong points. However, there are several areas for improvement to achieve a higher score.

**Areas for Improvement:**

1.  **File Loading Robustness:** The ground truth includes error handling for `FileNotFoundError`, which is crucial for real-world scripts. The generated code lacks this.
2.  **Delimiter:** The `winequality-red.csv` dataset is typically comma-separated, not semicolon-separated. The generated `sep=';'` is incorrect for the common version of this dataset and would cause issues if the user has the standard CSV. The ground truth correctly assumes the default comma-separator.
3.  **Data Type Identification Granularity:**
    *   The ground truth explicitly iterates through columns to identify if they are ""Numerical"" or ""Categorical/Ordinal"" using `pd.api.types` functions. This is a more robust and explicit way to categorize columns.
    *   The generated code simply prints `df.dtypes`, which shows pandas dtypes but doesn't explicitly categorize them for the user as ""numerical"" or ""categorical/ordinal"" as requested by the prompt.
4.  **Categorical/Ordinal Identification Sophistication:** The prompt specifically asked to ""Identify any columns that appear to be categorical or ordinal.""
    *   The ground truth identifies these columns based on their Pandas dtype heuristics (`is_object_dtype` or `is_categorical_dtype`). It also includes a crucial note about `quality` being ordinal despite being numerical.
    *   The generated code attempts to identify categorical/ordinal columns by printing unique values and counts. While this is a common *exploratory* method, it doesn't directly fulfill the ""identify"" request as well as the dtype-based approach, especially for ""ordinal"" which is often numerical but discrete. It assumes that columns with few unique values *are* categorical, which isn't always true, and doesn't handle actual categorical dtypes if present. The final point in the ground truth regarding 'quality' being ordinal despite numerical is a critical detail missed by the generated code's approach.
5.  **Descriptive Statistics Context:** The prompt asked for descriptive statistics ""For all numerical columns"". `df.describe()` by default *only* works on numerical columns, but the generated code doesn't explicitly state or confirm this. The ground truth explicitly filters `df[numerical_columns].describe()` which reinforces the constraint.",claude-3.5-sonnet,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('video_games_sales.csv')

# First 10 rows
print(""\nFirst 10 rows:"")
print(df.head(10))

# Shape
print(""\nDataset Shape:"")
print(df.shape)

# Column info
print(""\nColumn Info:"")
print(df.info())

# Top 10 games by global sales
print(""\nTop 10 Games by Global Sales:"")
print(df.nlargest(10, 'Global_Sales')[['Name', 'Global_Sales']])

# Unique platforms and genres
print(""\nUnique Platforms:"")
print(df['Platform'].uniqu...",7,"The generated code correctly addresses most of the task requirements. It loads the dataset, displays the first 10 rows, shows the shape, lists column names and data types (though df.info() returns None and is printed, which is slightly odd but functionally okay), identifies the top 10 games, shows unique values for 'Platform' and 'Genre', calculates sales statistics, and identifies missing values.

However, there are a few areas for improvement to match the robustness and clarity of the ground truth:
1.  **Error Handling**: The ground truth includes `try-except FileNotFoundError` for loading the CSV, which is good practice for robustness. The generated code lacks this.
2.  **Clarity of Output**: The ground truth uses more descriptive print statements and separators (`print(""=""*80)`) to clearly delineate each section of the EDA, making the output much easier to read and understand. The generated code's output is more concise but less user-friendly for presentation.
3.  **`df.info()` printing**: `print(df.info())` will print the info to stdout, and then `print(None)` because `df.info()` returns `None`. While not a critical error, just `df.info()` is the standard way.
4.  **Top 10 Games Columns**: The ground truth displays more relevant columns (`Name`, `Platform`, `Genre`, `Publisher`, `Global_Sales`) for the top 10 games, which makes the output more informative. The generated code only shows `Name` and `Global_Sales`.
5.  **Robustness for Missing Columns**: The ground truth includes checks (`if 'Global_Sales' in df.columns:`) before attempting operations on specific columns. This makes the code more robust against variations in dataset schema.
6.  **Unnecessary Import**: `import numpy as np` is not used in the generated code.

Overall, it's a good attempt that covers the core functionality, but it could be improved in terms of user experience, robustness, and small code quality details.",claude-3.5-sonnet,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing years with median
median_year = df['Year'].median()
df['Year'] = df['Year'].fillna(median_year)

# Remove rows with missing Publisher
df = df.dropna(subset=['Publisher'])

# Create Years_Since_Release column
df['Years_Since_Release'] = 2023 - df['Year']

# Remove duplicates
df = df.drop_duplicates(subset=['Name', 'Platform'])

# Filter low sales
df = df[df['Global_Sales'] ...",7,"The generated code addresses most of the requirements but has a few omissions and potential issues that prevent it from being perfect.

**Positive Aspects:**
*   Correctly loads the dataset.
*   Handles missing 'Year' values with the median.
*   Correctly removes rows with missing 'Publisher'.
*   Correctly creates the `Years_Since_Release` column.
*   Correctly removes duplicates based on 'Name' and 'Platform'.
*   Correctly filters out games with `Global_Sales` less than 0.1 million.
*   Correctly displays the final shape and head.

**Areas for Improvement:**
*   **Data Type Handling for 'Year'**: The ground truth explicitly converts 'Year' to numeric with `errors='coerce'` *before* filling NaNs and then to `int` *after*. The generated code relies on `median()` working directly on potentially mixed types and doesn't explicitly convert to `int` afterward. While `fillna()` might handle this in some cases, robustly ensuring the column is numeric first is better practice, especially if the 'Year' column contains non-numeric strings.
*   **Negative Years_Since_Release**: The ground truth accounts for potential future release years (if data is messy or `CURRENT_YEAR` is less than `df['Year']`) by ensuring `Years_Since_Release` is non-negative. The generated code does not include this safeguard.
*   **Data Type Handling for 'Global_Sales'**: Similar to 'Year', `Global_Sales` also needs robust numeric conversion before filtering. The ground truth includes `pd.to_numeric(df['Global_Sales'], errors='coerce')` and `dropna` for it. The generated code implicitly assumes 'Global_Sales' is already numeric, which might lead to errors if it's not.
*   **Verbosity/Informative Output**: The ground truth includes print statements after each major step, showing the dataset's shape changes. This is very useful for debugging and understanding the pipeline, which the generated code lacks.
*   **Error Handling/Robustness**: The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions, making the script more robust. The generated code lacks this.
*   **Constants**: Using `CURRENT_YEAR = 2023` as a constant is better practice than hardcoding `2023` directly in the calculation, as seen in the ground truth.",claude-3.5-sonnet,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('video_games_sales.csv')

# Set figure style
plt.style.use('seaborn')

# Create subplots
plt.figure(figsize=(15, 10))

# 1. Bar chart - Top 10 platforms by global sales
plt.subplot(2, 3, 1)
platform_sales = df.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False).head(10)
platform_sales.plot(kind='bar')
plt.title('Top 10 Gaming Platforms by Global Sales')
plt.xlab...",7,"The generated code addresses most of the requirements and successfully generates the requested plots. However, there are several areas for improvement, particularly regarding data handling, plot formatting, and subplot organization.

**Strengths:**
*   **Correct plot types:** All specified plot types (bar, pie, histogram, scatter, line) are used.
*   **Basic labeling:** Most plots have titles and axis labels.
*   **Subplot usage:** The code attempts to arrange plots using `subplot`, which is good practice.
*   **Plot titles and labels (mostly):** The plots have appropriate titles and labels.

**Areas for Improvement:**
*   **Data Cleaning and Preprocessing:** The most significant missing piece is the lack of robust data cleaning for `Year_of_Release`, `NA_Sales`, `EU_Sales`, and `Global_Sales`. The ground truth includes `pd.to_numeric` with `errors='coerce'` and `dropna`, which is crucial for handling non-numeric entries and `NaN` values, especially for sales and year columns. The generated code assumes clean numeric data, which is often not the case in real-world datasets like this one.
*   **`Year` column:** The generated code uses `df['Year']` for the line plot, but the dataset typically has `Year_of_Release`. This would likely cause an error if the column name isn't exactly 'Year'. The ground truth correctly uses `Year_of_Release`.
*   **Pie Chart Readability:** The pie chart can become unreadable if there are too many genres with small counts. The ground truth code's approach of consolidating small categories into an ""Other"" category vastly improves readability for datasets with many unique categorical values.
*   **Histogram Range:** The histogram of Global\_Sales often benefits from limiting the x-axis or using logarithmic bins, as sales data is typically heavily skewed. The ground truth uses `range=(0, df['Global_Sales'].quantile(0.99))` to focus on the majority of the data.
*   **Global Layout (Subplots):** The `plt.figure(figsize=(15, 10))` is applied globally, but then `subplot(2, 3, X)` is used. This implies 6 plots, but only 5 are generated. Whilematplotlib handles this, using a 2x3 layout for 5 plots leaves one empty spot, which might not be ideal. The ground truth creates individual figures for each plot, which is also a valid approach for distinct visualizations.
*   **`plt.style.use('seaborn')`:** While not an error, this is a general style setting, not a specific requirement, and doesn't fundamentally impact the core task of creating the plots as specified.
*   **`plt.close()`:** This is generally not needed if `plt.show()` is the last action and the script is exiting, as `plt.show()` usually blocks until plots are closed. It's more relevant in interactive sessions or when managing many figures.",claude-3.5-sonnet,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('diabetes.csv')

# Display first 10 rows
print(""\nFirst 10 rows:"")
print(df.head(10))

# Show dataset shape
print(""\nDataset shape:"")
print(df.shape)

# Display column names and data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# Calculate percentage of diabetic vs non-diabetic
print(""\nDiabetes outcome distribution:"")
print(df['Outcome'].value_counts(normalize=True) * 100)

# Count missing value...",7,"The generated code provides a functional solution to most of the requirements. It successfully loads the data, displays head, shape, dtypes, missing values, and summary statistics, and calculates outcome percentages.

However, there are several areas for improvement:

1.  **Robustness (File Loading):** The ground truth includes error handling for `FileNotFoundError`, which is good practice for real-world scenarios. The generated code lacks this.
2.  **`df.info()` vs `df.dtypes`:** While `df.dtypes` shows data types, `df.info()` (used in the ground truth) also provides non-null counts, which is more comprehensive for understanding a dataset's structure and potential missing values at a glance. The task explicitly asked to ""List all column names and their data types"", and `df.info()` does this more completely, showing non-null counts.
3.  **Specific Zero-Value Check:** The ground truth specifically lists columns where zero values might represent missing data (`COLUMNS_TO_CHECK_FOR_ZERO_AS_MISSING`). The generated code checks *all* columns for zeros, which can be misleading for columns like 'Pregnancies' or 'Outcome' (where zero is a legitimate value). The prompt implies a more targeted identification of *potential* missing data.
4.  **Readability/Separation:** The ground truth uses print statements to clearly delineate each section (e.g., ""--- 1. First 10 rows...""), which improves readability when running the script. The generated code just uses newlines and simple headings.
5.  **Target Column Check:** The ground truth includes a check to ensure the `TARGET_COLUMN` actually exists, which is important for robustness. The generated code assumes 'Outcome' is always present without a check.",claude-3.5-sonnet,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Load dataset
df = pd.read_csv('diabetes.csv')

# Replace 0s with NaN in specific columns
zero_not_allowed = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[zero_not_allowed] = df[zero_not_allowed].replace(0, np.nan)

# Fill missing values with median
df = df.fillna(df.median())

# Remove duplicates
df = df.drop_duplicates()

# Create BMI category
def get_bmi_category(bmi):
    ...",7,"The generated code largely addresses the requirements but has several areas for improvement to match the robustness and completeness of the ground truth.

**Strong Points:**
*   Correctly identifies and replaces zero values in specified columns with NaNs.
*   Correctly fills missing values using the median of each column.
*   Successfully removes duplicate rows.
*   Applies `StandardScaler` to numerical features.
*   Creates the 'BMI\_Category' column, although the implementation is slightly less efficient than `pd.cut`.
*   Displays the requested output (shape and head).

**Areas for Improvement:**
*   **Error Handling for File Loading:** The generated code lacks `try-except` for file loading, which is crucial for robust code.
*   **Column Existence Check:** It assumes all specified columns (`zero_not_allowed`, `numerical_cols`) always exist. The ground truth code includes checks for column existence before attempting operations, making it more robust to variations in the dataset.
*   **Efficiency of `fillna`:** `df = df.fillna(df.median())` is concise but might fill *all* NaN values in the DataFrame with the median of *their respective columns*, even if a NaN was not originally a '0' in `cols_to_replace_zero`. While in this specific problem and order of operations it works out because NaNs were explicitly introduced only in `cols_to_replace_zero`, the ground truth's approach of iterating and filling `cols_to_replace_zero` specifically is more controlled, especially in more complex scenarios.
*   **Efficiency of BMI Categorization:** Using `df.apply(get_bmi_category)` is less efficient than `pd.cut` for creating categorical bins. `pd.cut` is vectorized and generally preferred for this task in pandas.
*   **Dynamic Numerical Column Selection for Scaling:** The numerical columns are hardcoded, including the 'Outcome' column in the scaling list. While the problem implicitly suggests scaling all numeric features except the target 'Outcome', the generated code does not exclude 'Outcome' and if the dataset structure changes, this hardcoded list could break. The ground truth dynamically selects all numerical columns and then explicitly removes 'Outcome', which is more robust.
*   **Print Statements:** The ground truth includes more informative print statements describing the steps taken, which is good for debugging and understanding the code's flow.",claude-3.5-sonnet,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('diabetes.csv')

# Set style
plt.style.use('seaborn')
sns.set_palette('husl')

# Create correlation heatmap
plt.figure(figsize=(10,8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Heatmap')
plt.tight_layout()
plt.show()

# Create histograms
df.hist(figsize=(12,8), bins=30)
plt.suptitle('Distribution of Features')
plt.tight_layout()
plt.show(...",8,"The generated code successfully addresses most of the requirements and produces the requested visualizations.

**Strong Points:**
- **Correctness:** All plots are generated and largely display the correct data.
- **Completeness:** All 7 visualization types requested in the prompt are present.
- **Imports:** Necessary libraries are imported at the beginning.
- **Clarity:** The code is well-structured and easy to follow.

**Areas for Improvement to achieve a higher score:**
- **Loading Dataset Feedback:** The prompt explicitly asks for ""proper imports and assume the CSV file is in the current directory."" The ground truth includes `try-except` for `FileNotFoundError` and prints dataset info (`head`, `info`, `describe`). While not explicitly asked for, adding error handling for file loading is good practice and was included in the ground truth.
- **Correlation Heatmap:** The prompt mentions ""relationships between all numerical variables"". The generated code uses `df.corr()`, which by default will include the 'Outcome' column in the correlation calculation and heatmap. The ground truth explicitly excludes 'Outcome' when calculating correlation: `df[numerical_cols].corr()`. While 'Outcome' is a numerical variable, often in correlation analysis, you focus on independent variables' relationships. However, including it is not strictly wrong. It also lacks specific x and y tick rotations for better readability, which the ground truth included.
- **Histograms:** The `df.hist()` method provides a quick way to plot all histograms, but it can be less flexible for customization compared to iterating through columns and using `sns.histplot` (as seen in the ground truth), especially for adding KDE or fine-tuning bins per plot if needed. The ground truth also explicitly labeled each histogram with its feature name.
- **Box Plots:** The titles for individual subplots are missing in the generated code, which the ground truth included (e.g., `f'{col} by Diabetes Outcome'`). Also, the x-axis label for box plots could be more descriptive (e.g., ""0: Non-Diabetic, 1: Diabetic"") as in the ground truth.
- **Pie Chart Labels:** The pie chart's labels are just '0' and '1'. The ground truth used more descriptive labels like 'Non-Diabetic (0)' and 'Diabetic (1)'.
- **Plot Customization/Labels:** Some titles and labels are a bit generic or missing context. For example, the `Correlation Heatmap` title could specify ""of Numerical Features"" as in the ground truth. Scatter plots' legends could have more descriptive labels ('Non-Diabetic', 'Diabetic') instead of just '0' and '1'.
- **Consistent Styling:** While `plt.style.use('seaborn')` and `sns.set_palette('husl')` are good, some plots could benefit from specific styling choices (e.g., `sns.set_style(""whitegrid"")` in GT) or specific color palettes for consistency across similar plots.
- **No verbose output:** The ground truth includes print statements for loading and completion, which is a nice touch for user feedback.",claude-3.5-sonnet,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Display first 10 rows
print(""\nFirst 10 rows:"")
print(df.head(10))

# Show shape
print(""\nDataset shape:"")
print(df.shape)

# Display column names and data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# Regional indicators and country counts
print(""\nCountries per region:"")
print(df.groupby('Regional indicator')['Country name'].count().sort_values(ascending=F...",7,"The generated code correctly addresses most of the requirements.

**Strong Points:**
*   Correctly loads the dataset.
*   Successfully displays the first 10 rows, shape, column names and data types, top 10 happiest countries, summary statistics for key factors, and missing values.
*   The use of `df.nlargest(10, 'Ladder score')` is an efficient way to get the top 10 happiest countries.
*   Successfully calculates the count of countries per region, though the specific column used for counting differs from the ground truth (`Country name` vs. `value_counts()` on the indicator itself).

**Areas for Improvement (Why it's not higher):**
*   **Robustness (File Not Found):** The ground truth includes a `try-except FileNotFoundError` block, which makes the code more robust if the CSV file is not present. The generated code will simply raise an error and stop.
*   **Clarity and Structure:** The ground truth code explicitly separates each task with clear print statements and separators, making the output much easier to read and understand, especially in a benchmark scenario where individual tasks are being evaluated. The generated code's print statements are less distinct.
*   **Regional Indicator Count:** While `df.groupby('Regional indicator')['Country name'].count()` works, `df['Regional indicator'].value_counts()` is a more direct and common pandas idiom for counting unique occurrences within a single Series, which is what the prompt asked for (""count of countries per region""). The ground truth's approach using `value_counts()` directly on the 'Regional indicator' column implicitly counts countries per region because each row is a country. Also, the prompt asks for ""unique regional indicators AND count of countries per region"", where `value_counts()` naturally handles both by showing the unique indicators and their counts.
*   **Column Absence Handling:** The ground truth code includes checks for the presence of columns before attempting operations (e.g., `'Regional indicator' in df.columns`, `'Ladder score' in df.columns`). This makes the code more robust against variations in dataset schema. The generated code assumes all specified columns exist.
*   **`df.info()` vs `df.dtypes`:** For task 3 (Column names and data types), `df.info()` from the ground truth provides a more comprehensive summary including non-null counts and memory usage, which is generally preferred for understanding a DataFrame's structure. `df.dtypes` only shows the data types.
*   **Unnecessary Import:** `import numpy as np` is included but not used in the generated code.

Overall, it's a good attempt that covers the core functionality, but lacks the robustness, comprehensive output formatting, and slightly more idiomatic pandas usage seen in the ground truth.",claude-3.5-sonnet,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Load dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Handle missing values
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
categorical_cols = df.select_dtypes(include=['object']).columns

for col in numerical_cols:
    df[col].fillna(df[col].mean(), inplace=True)
    
for col in categorical_cols:
    df[col].fillna(df[col].mode()[0], inplace=Tr...",8,"The generated code largely addresses all requirements of the task.

**Strong Points:**
- **Missing Value Handling:** Correctly identifies and fills missing numerical values with the mean and categorical values with the mode.
- **Feature Engineering:**
    - Correctly calculates 'Happiness_Category' based on the specified ranges.
    - Correctly creates 'GDP_Category' using `pd.qcut` for quartiles.
    - Accurately calculates 'Well_being_Score' as the average of the three specified columns.
- **Outlier Removal:** Implements the IQR method correctly for 'Ladder score'.
- **Standardization:** Uses `StandardScaler` on numerical features.
- **Encoding:** Correctly applies `LabelEncoder` to 'Regional indicator'.
- **Display:** Shows the shape and first 5 rows as requested.
- **Imports:** Includes all necessary imports.

**Areas for Improvement (Why not a 9 or 10):**
- **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code assumes the file will always be present, which could lead to a crash if it's not.
- **Dynamic Numerical Column Selection for Standardization:** The generated code hardcodes the `numerical_cols` list for standardization. While correct for this specific dataset and the features created, a more robust approach (like in the ground truth) would be to dynamically select all numerical columns (excluding those that are no longer numerical or are identifiers) after all feature engineering, ensuring newly created numerical features are also considered without manual updates. In this specific case, 'Ladder score' is included in the hardcoded list, but it's also the column used for outlier removal and its values would have changed. This leads to applying scaling on a column that might have already been modified by outlier removal, which is fine, but the *list itself* could be more generalized.
- **Clarity of Numerical Column Selection for Scaling:** The hardcoded list for `numerical_cols` in the standardization step might unintentionally exclude other numerical columns that might benefit from scaling or include columns that shouldn't be scaled (though in this case, it seems to capture the relevant ones). A more dynamic selection would be safer and more flexible.
- **Redundancy in `numerical_cols` for Scaling:** The `numerical_cols` variable is defined twice, once for missing value handling and again for standardization, with different implicit meanings. While not an error, it could be clearer.",claude-3.5-sonnet,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Load data
df = pd.read_csv('world-happiness-report-2021.csv')

# 1. World Map Visualization
fig_map = px.choropleth(df, 
                       locations='Country name',
                       locationmode='country names',
                       color='Ladder score',
                      ...",7,"The generated code successfully creates all the requested visualizations. It demonstrates good use of `plotly`, `matplotlib`, and `seaborn` as required.

However, there are several areas for improvement:

1.  **Column Renaming:** The ground truth code renames the columns for better readability (`Ladder score` to `Happiness Score`, `Logged GDP per capita` to `GDP per Capita`, etc.). The generated code does not do this, leading to longer, less user-friendly labels on several plots.
2.  **Consistency in Plotly vs Matplotlib:** The task explicitly asks to ""use plotly or folium"" for the world map and then ""matplotlib, seaborn, and plotly for visualizations"". The generated code correctly uses Plotly for the map and radar chart, and Matplotlib/Seaborn for others.
3.  **Radar Chart Scaling:** The ground truth code correctly applies `MinMaxScaler` to normalize the radar chart features, which is crucial for a meaningful radar chart comparison across different scales. The generated code does not scale the features, leading to a potentially unreadable radar chart if the feature values have vastly different ranges (e.g., GDP per capita vs. perceptions of corruption). While plotly's `go.Scatterpolar` can handle unscaled data, visually comparing different magnitudes on the same axis is difficult without normalization. The `range=[0,1]` is set, but the values themselves are not scaled to fit this range.
4.  **Scatter Plot Labels/Titles:** The scatter plots lack explicit `xlabel` and `ylabel` calls in the generated code, relying on the `ax.set_title` which is good, but full labels would be better.
5.  **Plot Aesthetics:** While the generated code uses `tight_layout()`, the ground truth code includes more aesthetic enhancements like `sns.set_style`, `plt.rcParams`, and more specific color palettes or adjustments for better readability (e.g., `xticks` rotation for box plot, `heatmap` customization).
6.  **Error Handling:** The ground truth code includes basic error handling for `FileNotFoundError` when loading the CSV, which is good practice. The generated code does not.
7.  **Labels on Radar Chart:** The ground truth code sets `yticklabels` for the radar chart axes (0.2, 0.4, etc.), which helps in understanding the scaled values. The generated code just has `range=[0,1]` which is visually distinct but doesn't have the labels.
8.  **Redundant `plt.show()`:** There's a redundant `plt.show()` at the very end of the generated code after the plotly `fig.show()`.

The most critical missing piece is the normalization for the radar chart, which can significantly misrepresent the data if not applied. The lack of column renaming also reduces the overall polish and readability of the plots.",claude-3.5-sonnet,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

# Display first 10 rows
print(""\nFirst 10 rows:"")
print(df.head(10))

# Display shape
print(""\nDataset shape:"")
print(df.shape)

# Display column info
print(""\nColumn info:"")
print(df.dtypes)

# Group by continent
print(""\nTotal cases and deaths by continent:"")
continent_totals = df.groupby('continent')[['total_cases','total_deaths']].sum()
print(c...",8,"The generated code correctly addresses most of the requirements.

**Strong points:**
- Correctly loads the CSV and parses the 'date' column.
- Accurately displays the first 10 rows and dataset shape.
- Correctly calculates and displays continental summaries for total cases and deaths.
- Correctly identifies the top 5 locations by total cases based on the latest date.
- Accurately calculates basic summary statistics for 'new_cases' and 'new_deaths'.
- Correctly counts and displays missing values.
- The use of `df.groupby('location').last()` for the latest data per location is concise and effective.

**Areas for improvement:**
- **Error Handling (Task 1):** The ground truth includes `try-except` blocks for file loading, which is robust. The generated code lacks this.
- **Column Dtypes/Info (Task 4):** The ground truth uses `df.info()` which is more comprehensive as it shows non-null counts and memory usage in addition to dtypes. The generated code uses `df.dtypes` which only shows data types. The prompt explicitly asked for *all column names and their data types*, and while `df.dtypes` fulfills this, `df.info()` is generally preferred for ""info"" queries because it provides more context.
- **Output Clarity:** While the code's logic is mostly correct, the print statements in the ground truth are more descriptive, clearly labeling each section of output, which improves readability for the user. For instance, the ground truth explicitly prints ""--- Column names and their data types ---"" before calling `df.info()`.
- **Numpy Import:** `import numpy as np` is present but not used in the provided code snippet. While not an error, it's unnecessary clutter.

Overall, it's a very good attempt that fulfills the core requirements, with minor areas where it could be more robust and provide more comprehensive output as per best practices in data analysis scripting.",claude-3.5-sonnet,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Read the dataset
df = pd.read_csv('winequality-red.csv', sep=';')

# Calculate mean quality
mean_quality = df['quality'].mean()
print(f""Average wine quality: {mean_quality:.2f}"")

# Group by quality and calculate mean alcohol
quality_alcohol = df.groupby('quality')['alcohol'].mean()
print(""\nAverage alcohol content by quality:"")
print(quality_alcohol)

# Create bar plot
plt.figure(figsize=(10,6))
sns.barplot(x...",8,"The generated code successfully addresses all the core requirements of the task: calculating the mean quality, grouping by quality to find average alcohol, and creating a relevant bar chart.

**Strong Points:**
- **Correctness:** All calculations (mean quality, mean alcohol per quality) are correct.
- **Completeness:** It completes all three main steps requested in the prompt.
- **Proper Imports:** Uses `pandas`, `matplotlib.pyplot`, and `seaborn` as required.
- **Clear Variable Names:** Variable names like `mean_quality` and `quality_alcohol` are descriptive.
- **Concise Grouping:** The `groupby().mean()` operation is handled efficiently.
- **Visualization:** The bar chart effectively visualizes the required relationship.

**Areas for Improvement (to reach a 9 or 10):**
- **Robust File Handling:** The ground truth includes a `try-except FileNotFoundError` block for file loading, which is a good practice for robustness. The generated code lacks this.
- **CSV Separator:** The generated code uses `sep=';'` in `pd.read_csv`. While this might be correct for some `winequality-red.csv` files, the standard UCI dataset for wine quality uses a comma (`,`) as a separator. If the file is indeed comma-separated, this line would cause an error or misinterpretation. Assuming the standard dataset, `sep=';'` is an issue.
- **Plot Customization:**
    - The ground truth code includes more detailed plot customizations (e.g., `plt.figure(figsize=(10, 6))` is present in both, but ground truth also has `sns.set_style(""whitegrid"")`, `fontsize` for titles and labels, `xticks`/`yticks` font sizes, `tight_layout`, and data labels on bars). While the generated code's plot is functional, more customization (especially data labels and better axis labeling) would improve readability and presentation, aligning closer to a professional data science analysis.
    - The ground truth explicitly states which column is x and which is y for `sns.barplot` (e.g., `x='quality', y='alcohol', data=average_alcohol_by_quality`), which is clearer than passing `quality_alcohol.index` and `quality_alcohol.values` directly, especially if `quality_alcohol` wasn't a `Series`. Although `quality_alcohol` is indeed a Series, using the `data` argument with named columns is generally more idiomatic for `seaborn`.",claude-3.5-sonnet,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# Load and parse data
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

# Fill missing values
df['hosp_patients'].fillna(0, inplace=True)
df['icu_patients'].fillna(0, inplace=True)
numeric_cols = ['positive_rate', 'reproduction_rate']
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())

# Calculate mortality rate
df['mortality_rate'] = (df['total_deaths'] / df['total_cases']) * 100
df['mortali...",7,"The generated code correctly addresses most of the requirements.

**Strong points:**
*   Correctly loads the data and parses the 'date' column.
*   Handles missing values for 'hosp_patients' and 'icu_patients' by filling with 0.
*   Correctly calculates 'mortality_rate' and handles division by zero (NaNs and infs).
*   Correctly calculates 'vaccination_rate_per_capita' and handles division by zero (NaNs and infs).
*   Filters the dataset for the year 2021.
*   Displays the shape and head of the resulting DataFrame.
*   Uses `inplace=True` where appropriate for efficiency.

**Areas for improvement:**
1.  **Missing Value Handling for `positive_rate` and `reproduction_rate`:** The generated code attempts to fill `NaN` values with the mean using `df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())`. While this works, it implicitly assumes that these columns are already numeric. The ground truth explicitly converts these columns to numeric first using `pd.to_numeric(..., errors='coerce')` before calculating the mean, which is more robust, especially if the synthetic data could have non-numeric entries in these columns. This is a subtle but important robustness improvement.
2.  **`SettingWithCopyWarning` Precaution:** The ground truth code uses `.copy()` after filtering (`df[df['date'].dt.year == 2021].copy()`) to prevent a potential `SettingWithCopyWarning` when subsequent operations are performed on the filtered DataFrame. While not always strictly necessary depending on pandas' internal optimizations, it's good practice to explicitly create a copy when filtering to avoid unexpected behavior or warnings. The generated code does not include this and assigns directly back to `df`.
3.  **Clarity of Output:** The ground truth code provides more descriptive print statements for the shape and head, which is helpful in a benchmark scenario to confirm what's being displayed. The generated code's print statements are concise but less informative.
4.  **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness in a real-world scenario. The generated code assumes the file will always be present.

Overall, the code is very close to perfect, with minor improvements in robustness and clarity discussed above.",claude-3.5-sonnet,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# Load and parse data
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

# Set style
plt.style.use('seaborn')

# Create figure with subplots
fig = plt.figure(figsize=(20,15))

# 1. Time series line plot for India
ax1 = plt.subplot(2,3,1)
india_data = df[df['location']=='India']
ax1.plot(india_data['date'], india_data['new_cases'])
ax1.set_title('...",6,"The generated code addresses most of the requirements of the prompt, creating the specified plots. However, there are several areas for improvement, especially regarding the accuracy of the data aggregation for some plots and adherence to the specific plotting libraries requested.

**Strengths:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Data Loading and Parsing:** Handles the initial data loading and date parsing correctly.
*   **Plot Types:** All requested plot types (time series, bar, scatter, histogram, box plot) are generated.
*   **Subplots:** Uses subplots which is a good approach for showing multiple visualizations.
*   **Basic Labels/Titles:** Includes basic titles and labels for most plots.

**Areas for Improvement:**
*   **Bar Chart (Total Deaths by Continent):** The ground truth correctly identifies that `total_deaths` should be summed *after* taking the latest entry per location/country to avoid overcounting. The generated code uses `df.groupby('continent')['total_deaths'].max()`, which is incorrect for summing total deaths across a continent. It should sum the *latest* `total_deaths` for each *country* within that continent. This is a significant logical error in data aggregation.
*   **Scatter Plot (GDP vs. Vaccinations):** The `total_vaccinations` and `gdp_per_capita` should be taken from the *latest* entry for each `location` to represent a current state. While `latest_data = df.sort_values('date').groupby('location').last()` partially addresses this, it implicitly creates a new DataFrame and the scatter plot is then directly called on two series from this new DataFrame. The ground truth explicitly creates `latest_data_for_scatter` and drops NaNs to ensure cleaner data for the plot. The generated code implicitly handles NaNs by `last()` which usually skips them if there are non-NaN values, but for a scatter plot, it's good practice to be explicit. Also, the prompt mentioned using `hue` and `size` for the scatter plot to add more information (continent and population), which the generated code misses.
*   **Matplotlib vs. Seaborn Usage:** The prompt explicitly states ""Use matplotlib and seaborn for plotting."" While `plt.plot` and `df.hist` are valid `matplotlib` calls, `sns.lineplot`, `sns.barplot`, `sns.scatterplot`, `sns.histplot`, and `sns.boxplot` generally offer better default aesthetics and convenience for data-driven plots than pure `matplotlib` functions. The generated code primarily uses `matplotlib`'s direct plotting functions (`plt.plot`, `df.hist`, `ax.scatter`) where `seaborn` might have been a better choice for consistency and aesthetic quality (e.g., for time series and scatter plots). The box plot correctly uses seaborn.
*   **Missing Scatter Plot Enhancements:** The scatter plot in the generated code misses the `hue='continent'` and `size='population'` parameters which were implied by the ground truth and are good practice for visual clarity in such plots, relating to the `total_vaccinations` and `gdp_per_capita` from the latest entry for each location.
*   **Histogram Data Handling:** The ground truth explicitly drops NaNs (`dropna()`) before plotting the histogram, which is robust. The generated code doesn't explicitly handle NaNs in the histogram, though `hist()` usually ignores them.
*   **Redundant `datetime` Import:** `from datetime import datetime` is imported but not used.
*   **Plotting Style:** While `plt.style.use('seaborn')` is fine, `sns.set_theme(style=""whitegrid"")` from the ground truth is more common when mixing Seaborn plots and ensures a consistent modern aesthetic for all plots.
*   **Error Handling:** The ground truth includes basic error handling for file loading, which is a good practice for robustness. The generated code lacks this.",claude-3.5-sonnet,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('clash_royale_cards.csv')

# 1. Column names and data types
print(""\nColumn Names and Data Types:"")
print(df.dtypes)

# 2. Descriptive statistics for numerical columns
print(""\nDescriptive Statistics:"")
numerical_cols = ['elixirCost', 'Win Rate', 'Usage'] 
print(df[numerical_cols].describe())

# 3. Total number of cards
print(""\nTotal number of cards:"", len(df))
```",7,"The generated code correctly loads the dataset, identifies column data types, calculates descriptive statistics for the specified columns, and reports the total number of rows. It uses appropriate pandas functions for these tasks.

However, there are a few areas for improvement:
- **Error Handling**: The generated code lacks explicit error handling for `FileNotFoundError` when loading the CSV, which is crucial for robust code. The ground truth includes a `try-except` block for this.
- **Precision in Output**: While `df.dtypes` is correct for identifying data types, the prompt asked to ""list all column names AND identify the data type of each column."" The ground truth explicitly prints column names first, then data types, providing a clearer separation as requested by the prompt's wording.
- **Specific Statistics**: The prompt asked for ""mean, median, standard deviation, min, max"". The generated code uses `df[numerical_cols].describe()`, which provides these along with 'count', '25%', '50%', '75%'. While it includes the requested statistics, it also includes extra ones. The ground truth used `.agg(['mean', 'median', 'std', 'min', 'max'])` which is more precise to the prompt's request.
- **Clarity of Output**: The print statements are a bit terse. While functional, they could be more descriptive (e.g., ""Total number of cards (rows) in the dataset:""). The ground truth provides more verbose and structured output which enhances readability.
- **Unnecessary Import**: `import numpy as np` is present but not used.

Overall, it's a solid attempt that addresses the core requirements but could be improved in terms of robustness, output specificity, and adherence to exact output details in the prompt.",claude-3.5-sonnet,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

df = pd.read_csv('clash_royale_cards.csv')
low_elixir = df[df['elixirCost'] <= 3][['Card', 'Win Rate']]
sorted_cards = low_elixir.sort_values('Win Rate', ascending=False)
print(sorted_cards)
```",9,"The generated code correctly implements all the requirements of the task: it loads the CSV, filters by 'elixirCost' less than or equal to 3, selects the 'Card' and 'Win Rate' columns, and sorts the results by 'Win Rate' in descending order. The code is concise and efficient.

The only minor area for improvement, preventing a perfect 10, is the lack of error handling for file not found or missing columns, which the ground truth gracefully handles. However, given the prompt only asked for ""only the Python code without explanations"" and assumed the CSV file would be present, the current code is highly effective for the stated task.",claude-3.5-sonnet,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('clash_royale_cards.csv')

plt.figure(figsize=(10,6))
plt.hist(df['elixirCost'], bins=range(1,11), align='left', rwidth=0.8)
plt.title('Distribution of Card Elixir Costs in Clash Royale')
plt.xlabel('Elixir Cost')
plt.ylabel('Number of Cards')
plt.xticks(range(1,10))
plt.grid(axis='y', alpha=0.75)
plt.show()
```",8,"The generated code correctly loads the CSV, creates a histogram of the `elixirCost` column, and includes a title and labeled axes, fulfilling all core requirements of the task.

**Strengths:**
*   **Correctness:** Successfully generates a histogram for the specified column.
*   **Completeness:** Addresses all stated requirements: loading CSV, histogram, title, and labeled axes.
*   **Imports:** Includes all necessary imports (`pandas`, `matplotlib.pyplot`, `seaborn`).
*   **Clarity:** The code is concise and easy to understand.
*   **Plotting:** Uses appropriate `matplotlib` functions for plotting and customization.

**Areas for Improvement (minor):**
*   **Robustness:** The code lacks error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth. This is a good practice for production-ready code.
*   **`xticks`:** While `plt.xticks(range(1,10))` works, the ground truth uses `range(1,11)` which sets ticks up to 10 inclusive for `elixirCost`, which is usually 1-10. The generated code also sets `bins=range(1,11)` which seems to imply 1-10, but the `xticks` only go up to 9. The ground truth also explicitly sets `bins=range(1,12)` which better covers the domain of 1 to 10. `bins=range(1,11)` would mean bins for 1-2, 2-3, ..., 9-10. This is a subtle difference in the bins and x-axis ticks.
*   **Seaborn usage:** The problem statement mentioned ""Use matplotlib or seaborn for plotting."" While `hist` is `matplotlib`, `seaborn.histplot` (used in the ground truth) often provides a more aesthetically pleasing default. This isn't a critical issue, as `matplotlib` is perfectly valid.",claude-3.5-sonnet,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('iris.csv')

# 1) Column names and data types
print(""Column Names and Data Types:"")
print(df.dtypes)
print(""\n"")

# 2) Descriptive statistics for numerical columns
numerical_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
print(""Descriptive Statistics:"")
print(df[numerical_cols].describe())
print(""\n"")

# 3) Species counts
print(""Species Counts:"")
print(df['Species'].value_counts())
```",7,"The generated code correctly addresses the core requirements of loading the data, identifying column types, calculating descriptive statistics for the specified numerical columns, and reporting unique species counts. It uses appropriate `pandas` functions for these tasks.

However, there are several areas for improvement as compared to the ground truth:

1.  **Robustness (File Loading):** The generated code lacks error handling for `FileNotFoundError` or other exceptions during CSV loading. The ground truth includes a `try-except` block which is crucial for robust applications.
2.  **Output Clarity and Formatting:** While the output is correct, the ground truth provides more descriptive headers and separators, making the output much clearer and easier to read. For example, adding titles like ""--- Column Names and Data Types ---"" significantly improves user experience.
3.  **Specific Statistical Metrics:** The task explicitly asked for 'mean, median, standard deviation, min, max'. While `df.describe()` provides these (and more), the ground truth specifically targets and shows only these requested metrics using `.agg()`, which is a more precise fulfillment of the prompt.
4.  **Completeness of Column Identification:** The prompt asked to ""List all column names and identify the data type of each column."" The generated code only prints `df.dtypes`, which shows data types but doesn't explicitly *list* the column names in a separate line as `df.columns.tolist()` does in the ground truth, which is a minor detail but reflects a slight incompleteness.
5.  **Handling Missing Columns (Numerical Stats):** The ground truth includes a check to ensure the `numerical_columns` actually exist in the DataFrame before attempting to calculate statistics, providing a warning if any are missing. The generated code would raise a `KeyError` if one of the specified columns was not found in the DataFrame.
6.  **`numpy` import:** `numpy` is imported (`import numpy as np`), but it is not used anywhere in the provided code. While harmless, it's unnecessary.

Overall, the generated code is functional and largely correct, but it lacks the robustness, user-friendliness, and precise adherence to output specifications seen in the ground truth.",claude-3.5-sonnet,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

df = pd.read_csv('iris.csv')
filtered_df = df[df['Species'] == 'Iris-setosa'][['SepalLengthCm', 'SepalWidthCm']]
print(filtered_df)
```",8,"The generated code correctly performs all the specified operations: loading the CSV, filtering for 'Iris-setosa' species, and selecting and displaying the specified columns. It's concise and achieves the task requirements.

However, it lacks error handling (e.g., `FileNotFoundError`, `KeyError`), which is present in the ground truth and is generally good practice for robust code involving file I/O and data manipulation. The ground truth also uses intermediate variables which can improve readability for complex operations, although the generated code is simple enough that this isn't a major detractor. The prompt explicitly asked for *proper imports* which is satisfied, but it also implied a complete solution addressing all requirements, and robust error handling is part of a complete, production-ready solution in a real-world scenario.",claude-3.5-sonnet,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('iris.csv')

plt.figure(figsize=(10,6))
for species in df['Species'].unique():
    subset = df[df['Species'] == species]
    plt.scatter(subset['SepalLengthCm'], subset['SepalWidthCm'], label=species)

plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)') 
plt.title('Iris Sepal Length vs Width by Species')
plt.legend()
plt.show()
```",9,"The generated code correctly addresses all the requirements of the task. It properly loads the CSV, creates a scatter plot of `SepalLengthCm` vs `SepalWidthCm`, colors the points by `Species`, and includes a title and axis labels. The imports are correct.

Minor improvements could be made for robustness and best practices:
1. **Error Handling:** The ground truth code includes robust `try-except` blocks for `FileNotFoundError`, `KeyError`, and general `Exception`, which is good practice for real-world applications. The generated code lacks this.
2. **Axis Labels:** While the generated code uses 'Sepal Length (cm)' and 'Sepal Width (cm)', the task specifically asked for `SepalLengthCm` and `SepalWidthCm` as labels, matching the column names. This is a very minor deviation but worth noting for exact adherence.
3. **Clarity/Readability:** Calling the main logic within a function (`main()` in ground truth) is generally better for code organization and reusability, though for this simple script, it's not strictly necessary.

Overall, it's a very good and correct solution to the problem statement.",claude-3.5-sonnet,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('heart.csv')

# 1. Column names and data types
print(""Column Names and Data Types:"")
print(df.dtypes)
print(""\n"")

# 2. Descriptive statistics for numerical columns
num_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']
print(""Descriptive Statistics:"")
print(df[num_cols].describe())
print(""\n"")

# 3. Count of heart disease values
print(""Heart Disease Counts:"")
print(df['HeartDisease'].value_counts())
```",8,"The generated code correctly performs all the requested tasks: loading the CSV, printing column data types, calculating descriptive statistics for the specified numerical columns, and reporting value counts for 'HeartDisease'.

Strong points:
- Correctly uses pandas for loading data and performing operations.
- Accurately identifies and uses the specified numerical columns for descriptive statistics.
- Correctly uses `value_counts()` for the `HeartDisease` column.
- The code is concise and readable.

Areas for improvement (to achieve a higher score):
- Lack of error handling for `FileNotFoundError` when loading the CSV. The ground truth includes this, which is good practice in real-world scenarios.
- The descriptive statistics output from `.describe()` includes more metrics than explicitly requested (e.g., count, 25%, 75%). While not incorrect, the prompt specifically asked for ""mean, median, standard deviation, min, max"". The ground truth selectively reports these, mapping '50%' to 'median' for clarity.
- The output formatting could be slightly clearer, especially for the descriptive statistics part. The ground truth adds more descriptive print statements (e.g., ""--- Column Names and Data Types ---"").
- The ground truth explicitly lists column names and then their data types, while the generated code only prints `df.dtypes`, which fulfills the data type part but not explicitly ""List all column names"". However, `df.dtypes` does implicitly show them.",claude-3.5-sonnet,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

df = pd.read_csv('heart.csv')
filtered_df = df[(df['Age'] > 50) & (df['HeartDisease'] == 1)]
result = filtered_df[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]
print(result)
```",9,"The generated code correctly applies the filtering conditions for `Age` and `HeartDisease`, and accurately selects and displays the required columns. It addresses all core requirements of the task. The strong points are its conciseness and directness in solving the problem.

Areas for minor improvement (preventing a perfect 10):
1.  **Error Handling**: The ground truth includes `try-except` blocks for `FileNotFoundError`, `KeyError`, and general exceptions, which makes the code more robust. The generated code would crash if `heart.csv` doesn't exist or if a required column is missing.
2.  **Function Encapsulation**: The ground truth encapsulates the logic within a function, which is generally good practice for reusability and organization, especially in larger scripts. However, for this specific quick task, the direct script is also acceptable. The prompt asked for ""Python code without explanations"", implying a direct script was fine.
3.  **Docstrings/Comments**: While not explicitly asked for in the output, the ground truth includes docstrings and comments which improve readability and maintainability.

Overall, the generated code directly solves the problem presented efficiently and correctly. The score reflects its accurate functionality with minor stylistic/robustness improvements possible.",claude-3.5-sonnet,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

# Read data
df = pd.read_csv('winequality-red.csv')

# Filter wines
filtered_wines = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]

# Get wine with highest quality from filtered set
best_wine = filtered_wines.loc[filtered_wines['quality'].idxmax()]

# Display all features
print(best_wine)
```",8,"The generated code correctly implements all the core requirements of the task: filtering with compound conditions, finding the maximum quality wine in the subset, and displaying its features. The code is concise and directly addresses the problem.

Areas for improvement:
- **Error Handling**: The generated code lacks any error handling (e.g., `FileNotFoundError` if 'winequality-red.csv' is missing). This is a significant omission for robust code.
- **Modularity/Functions**: The code is not encapsulated within a function, which makes it less reusable and harder to test. The ground truth uses a function, which is better practice.
- **Edge Case Handling**: It doesn't explicitly handle the case where `filtered_wines` might be empty after the initial filter (though `idxmax()` would likely raise an error in that scenario, it's not gracefully handled).
- **Clarity/Readability**: While concise, adding print statements to indicate progress or the number of filtered wines (as in the ground truth) would improve user understanding.
- **Output Formatting**: The `print(best_wine)` might truncate output depending on the number of columns. The ground truth uses `.to_string()` for comprehensive display.

Overall, it's a functionally correct solution for the given task but could be made more robust, user-friendly, and production-ready with the suggested improvements.",claude-3.5-sonnet,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('heart.csv')

# Create figure with 2 subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))

# Plot age histogram
sns.histplot(data=df, x='Age', bins=20, ax=ax1)
ax1.set_title('Distribution of Age')
ax1.set_xlabel('Age')
ax1.set_ylabel('Count')

# Plot sex distribution
sex_counts = df['Sex'].value_counts()
ax2.bar(sex_counts.index, sex_counts.values)
ax2.set_title('Distributio...",7,"The generated code correctly loads the dataset, generates a histogram for `Age`, and a bar chart for `Sex`. It uses `matplotlib` and `seaborn` appropriately and includes titles and labeled axes for both plots.

However, there are a few areas for improvement:
1.  **Sex Labels for Bar Chart**: The `Sex` column in `heart.csv` typically contains numerical values (0 for Female, 1 for Male). The generated code plots these numerical values directly on the x-axis, which is less readable than using 'Male' and 'Female' labels. The ground truth code correctly addresses this by mapping the numerical values to descriptive strings.
2.  **Robustness**: The generated code lacks error handling for `FileNotFoundError` or `KeyError`, which is present in the ground truth. While not explicitly requested in the prompt, it's a good practice for robust code.
3.  **Visualization Enhancement**: The histogram could benefit from `kde=True` like in the ground truth for a smoother representation of density, and the bar chart for `Sex` could use a `countplot` directly from seaborn for potentially more visually appealing defaults (though `value_counts()` and `bar()` is functionally correct). Also, the ground truth explicitly uses `sns.histplot` and `sns.countplot` which are often preferred from a seaborn context, whereas the generated code combines `sns.histplot` and `plt.bar`.

Overall, it's a good attempt that meets most requirements, but could be enhanced for better readability, robustness, and more idiomatic seaborn usage for the bar chart.",claude-3.5-sonnet,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('spotify_2023.csv')

# 1) List columns and data types
print(""\nColumn Names and Data Types:"")
print(df.dtypes)

# 2) Convert streams to numeric
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
print(""\nNumber of NaN values in streams column:"", df['streams'].isna().sum())

# 3) Descriptive statistics for numeric columns
numeric_cols = ['streams', 'bpm', 'danceability_%', 'valence_%', 'energy_%']
print(""\nD...",8,"The generated code successfully completes the majority of the tasks.

**Strengths:**
- Correctly loads the CSV.
- Accurately lists column names and data types using `df.dtypes`.
- Correctly converts the `streams` column to numeric with `errors='coerce'` and reports NaN count.
- Calculates and prints descriptive statistics for the specified numerical columns using `describe()`.
- Correctly counts and reports the unique artists in `artist(s)_name`.

**Areas for Improvement:**
- **Error Handling for File Loading:** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is a robust practice for real-world scenarios. The generated code lacks this.
- **Reporting NaNs in `streams`:** While it correctly counts NaNs, the task also subtly implied reporting *if* any NaNs were introduced or already present. The ground truth provides more verbose output to clarify this, including original and new data types for `streams`, and example rows with NaNs if they exist. The generated code simply prints the number of NaNs.
- **Specific Statistics Reporting:** The task asked for ""mean, median, std, min, max"". While `describe()` includes these, it also includes other quartiles. The ground truth explicitly selects these exact statistics from the `describe()` output, which is slightly more precise adherence to the prompt.
- **Output Clarity:** The ground truth provides more descriptive print statements and clear section headers, making the output easier to read and understand. While the generated code's output is functional, it's less user-friendly.

Overall, it's a very good attempt that fulfills most criteria. The main deductions are for the lack of robust file loading error handling and slightly less detailed output in a few places as compared to the ground truth's thoroughness.",claude-3.5-sonnet,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

df = pd.read_csv('spotify_2023.csv')
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

filtered_df = df[df['released_year'] == 2023][['track_name', 'artist(s)_name', 'streams']]
filtered_df = filtered_df.sort_values('streams', ascending=False)

print(filtered_df)
```",9,"The generated code correctly loads the CSV, filters by `released_year` == 2023, selects the specified columns, ensures `streams` is numeric, and sorts by `streams` in descending order. It's concise and efficient.

Strong points:
- All task requirements are met.
- Correct use of `pd.to_numeric` with `errors='coerce'`.
- Efficient chaining of operations for filtering and column selection.
- Correct sorting order.

Areas for minor improvement (not critical for functionality):
- The ground truth includes error handling for `FileNotFoundError`, which is good practice for robustness but not explicitly required by the prompt's functional task description. The generated code skips this.
- The prompt asks to ""display"" the columns, and `print(filtered_df)` accomplishes this. The ground truth adds a descriptive print statement, which is a nice touch but not strictly required by the prompt.
- The ground truth explicitly uses `.loc` for filtering to avoid `SettingWithCopyWarning`, although in this specific case, the chained indexing `df[...][...]` might create a copy anyway, making `SettingWithCopyWarning` less relevant for the final `filtered_df`. However, for more complex operations, using `.loc` can prevent subtle issues.",claude-3.5-sonnet,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('spotify_2023.csv')

# Create figure with 2 subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))

# Histogram
ax1.hist(df['bpm'], bins=30, color='green', alpha=0.7)
ax1.set_title('Distribution of BPM')
ax1.set_xlabel('Beats Per Minute')
ax1.set_ylabel('Frequency')

# Scatter plot
ax2.scatter(df['danceability_%'], df['energy_%'], alpha=0.5)
ax2.set_title('Danceability vs Energy')
ax2.set_xlabel('D...",9,"The generated code correctly addresses all requirements of the task. It loads the CSV, generates a histogram for `bpm`, and a scatter plot for `danceability_%` vs `energy_%`. Both plots have appropriate titles and axis labels. The use of `subplots` is an efficient way to display multiple plots in one figure, which is good code quality. The only minor point is the general title ""Danceability vs Energy"" for the scatter plot, which is slightly less descriptive than the ground truth's ""Relationship between `danceability_%` and `energy_%`"". However, it's still correct and clear enough. Error handling for file not found or missing columns (present in ground truth) is not included, which is typical for a direct task solution without explicit instructions for robustness, but worth noting for a comprehensive expert evaluation.",claude-3.5-sonnet,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('youtube_trendings.csv')

# List columns and dtypes
print(df.dtypes)
print(""\n"")

# Clean numeric columns
for col in ['Video views', 'Likes', 'Dislikes']:
    df[col] = pd.to_numeric(df[col].str.replace(',',''), errors='coerce')
    df[col] = df[col].fillna(0)

# Get descriptive stats
print(""Descriptive Statistics:"")
print(df[['Video views', 'Likes', 'Dislikes']].describe())
print(""\n"")

# Fill missing categories
df['...",9,"The generated code correctly performs all the requested tasks. It successfully loads the data, cleans the specified numeric columns by removing commas, converting to numeric, and filling NaNs with 0. It also calculates descriptive statistics for these columns. Furthermore, it correctly fills missing 'Category' values and reports the unique category count.

Improvements:
1.  **Column Names and Inferred Data Types (1):** The task specifically asked to ""List all column names and their inferred data types."" While `df.dtypes` is output, the ground truth uses `df.info()` which provides a more comprehensive output including non-null counts, which is generally more informative for understanding data quality. The prompt specifically asked for listing *all* column names, and `df.dtypes` does this, so it's a minor point, but `df.info()` is often preferred.
2.  **Descriptive Statistics Output (3):** The general `describe()` method includes more statistics than requested (e.g., count, 25%, 50%, 75% quartiles). While this isn't strictly incorrect, the task asked for ""mean, median, std, min, max"". The ground truth explicitly prints only these, which is a closer match to the request.

Overall, the code is concise, effective, and directly addresses the prompt's requirements. The minor points above are about slight variations in how information is presented or how much information is provided, rather than outright errors.",claude-3.5-sonnet,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

df = pd.read_csv('youtube_trendings.csv')
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)
filtered_df = df[df['Video views'] > 50000000]
result = filtered_df[['Video', 'Video views']].sort_values('Video views', ascending=False)
print(result)
```",9,"The generated code is highly correct and concise, efficiently addressing all requirements of the task.

**Strong points:**
- **Correctness:** It correctly loads the data, cleans the 'Video views' column (removes commas, converts to numeric, handles errors by coercing to NaN and filling NaNs with 0), filters by views > 50,000,000, selects the specified columns, and sorts the result in descending order.
- **Conciseness:** The cleaning and conversion steps for 'Video views' are combined into a single, efficient line. Similarly, filtering, column selection, and sorting are chained effectively.
- **Completeness:** All steps outlined in the task description are implemented.

**Minor points for improvement (not penalizing much given the constraints of the task):**
- **Error Handling:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is good practice. The generated code assumes the file exists. For a robust production script, error handling would be beneficial.
- **Readability/Breaking Down Steps:** While its conciseness is a strength, for highly complex tasks, breaking down some chained operations into separate lines with intermediate variables (as seen in the ground truth for `filtered_df`, `selected_columns_df`, `sorted_df`) can sometimes improve readability and debuggability, especially for beginners or larger codebases. However, for this specific task, the generated code's conciseness is perfectly acceptable.

Overall, the generated code is a very strong solution.",claude-3.5-sonnet,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('youtube_trendings.csv')

df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',',''), errors='coerce')
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',',''), errors='coerce')

df['Likes'] = df['Likes'].fillna(0)
df['Video views'] = df['Video views'].fillna(0)

plt.figure(figsize=(10,6))
plt.scatter(df['Video views'], df['Likes'])
plt.title('Video Views vs. Likes')
plt.xlabel('Video Views (cleaned)'...",10,"The generated code correctly addresses all requirements outlined in the task. It properly loads the CSV, cleans the 'Likes' and 'Video views' columns by removing commas, converting to numeric, and filling NaNs with 0. It then generates a scatter plot with the specified title and axis labels. The imports are correct, and the code structure is clean and efficient for the given task. It achieves the desired outcome effectively.",claude-3.5-sonnet,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the data
df = pd.read_csv('mobiles_2025.csv')

# Clean Mobile Weight
df['Mobile Weight'] = pd.to_numeric(df['Mobile Weight'].str.replace('g',''), errors='coerce')

# Clean RAM 
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB',''), errors='coerce')

# Clean Battery Capacity
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh',''), errors='coerce')

# Clean Launched Price
df['Launched Price (USA)'] = pd.to_nume...",9,"The generated code successfully completes almost all aspects of the task.

**Strong Points:**
*   **Correctness:** All cleaning steps for `Mobile Weight`, `RAM`, `Battery Capacity`, and `Launched Price (USA)` are correctly implemented, including removal of specific strings and conversion to numeric types with `errors='coerce'`.
*   **Data Types:** Correctly prints column names and their inferred data types after cleaning.
*   **Descriptive Statistics:** Correctly calculates and prints descriptive statistics for the specified numeric columns.
*   **Unique Count:** Correctly calculates and prints the count of unique values in the `Company Name` column.
*   **Conciseness:** The code is quite concise and directly addresses the requirements without unnecessary complexity.

**Areas for Minor Improvement:**
*   **Error Handling (File Loading):** The ground truth includes robust error handling for `FileNotFoundError` during CSV loading, which is good practice for production-ready code. The generated code assumes the file will always be present, which is acceptable for a direct script but less robust.
*   **Specific Statistics:** The ground truth specifically asks for mean, median, std, min, max. The generated code uses `.describe()`, which provides these plus count, 25th, and 75th percentiles. While `.describe()` is often perfectly fine, explicitly filtering for the requested statistics (mean, 50% (median), std, min, max) as shown in the ground truth (`.loc[['mean', '50%', 'std', 'min', 'max']]`) would have made it a perfect match to the prompt's `report basic descriptive statistics (mean, median, std, min, max)`.
*   **Output Clarity:** The ground truth added more descriptive print statements (e.g., ""--- Loading Data from..."", ""Cleaning 'RAM' column..."", ""Analysis Complete""), which improves the user experience by indicating progress and completion of different stages. This is a minor aesthetic point but contributes to better feedback for the user.

Overall, the generated code is highly effective and directly addresses the core requirements of the task. The minor points mentioned are mostly about robustness and output formatting, not functional correctness.",claude-3.5-sonnet,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

df = pd.read_csv('mobiles_2025.csv')

df['Launched Price (India)'] = pd.to_numeric(df['Launched Price (India)'].str.replace('INR ','').str.replace(',',''), errors='coerce')
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB','').str.strip(), errors='coerce')

filtered_df = df[
    (df['Company Name'] == 'Apple') & 
    (df['Launched Price (India)'] < 100000)
][['Model Name', 'RAM', 'Launched Price (India)']]

result = filtered_df.sort_values('La...",9,"The generated code correctly performs all the requested tasks: loading the CSV, cleaning the specified columns with error handling, filtering by company and price, selecting the correct columns, and sorting the final output. The code is concise and efficient.

One minor point of improvement could be the handling of `astype(str)` before the string operations on the columns (`Launched Price (India)` and `RAM`). While `str.replace` usually handles non-string types by raising an error or returning `NaN`, explicitly converting to string first might prevent potential issues with mixed types, though in many pandas versions `str` accessor methods are robust enough. The ground truth includes this for robustness. Additionally, the `numpy` import is not strictly necessary as `pd.to_numeric` handles coercing errors to `NaN` without needing `np.nan`. These are minor, as the code works correctly as is. Error handling for the `read_csv` operation is omitted, but the task described filtering and processing, not robust file handling.",claude-3.5-sonnet,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean RAM column
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB','').str.strip(), errors='coerce')

# Clean Battery Capacity column 
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh','').str.strip(), errors='coerce')

# Create scatter plot
plt.figure(figsize=(10,6))
sns.scatterplot(data=df, x='RAM', y='Battery Capacity', hu...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong Points:**
*   **Correctness:**
    *   Successfully loads the `mobiles_2025.csv` dataset.
    *   Correctly cleans the `RAM` column by removing 'GB' and converting to numeric, handling errors with `errors='coerce'`.
    *   Correctly cleans the `Battery Capacity` column by removing 'mAh' and converting to numeric, handling errors with `errors='coerce'`.
    *   Generates a scatter plot with `Battery Capacity` on the y-axis and `RAM` on the x-axis.
    *   Correctly colors the points by `Company Name` using `hue`.
    *   Includes a clear title for the plot.
    *   Includes correct labels for both x-axis and y-axis.
    *   Uses appropriate libraries (`pandas`, `matplotlib.pyplot`, `seaborn`).
*   **Code Quality:**
    *   Imports are properly placed at the top.
    *   Code is well-structured and easy to read.
    *   Uses `sns.scatterplot` which is generally preferred for its aesthetics.

**Minor Area for Improvement (preventing a perfect 10):**
*   **Robustness in Cleaning:** The `astype(str)` step before string operations was present in the ground truth but is missing in the generated code. While `str.replace` usually works fine on mixed-type columns, explicitly converting to string `df['RAM'].astype(str).str.replace(...)` ensures robustness, especially if there might be non-string values that aren't immediately coercible to numbers during the `str.replace` step. In this specific case, `pd.to_numeric` would handle non-string values, but for general string cleaning, `astype(str)` is a best practice. However, given the nature of the data (likely string-based numbers), it's not a critical error.

Overall, an excellent solution that fulfills all the core requirements of the task.",claude-3.5-sonnet,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('titanic.csv')

# Display first 5 rows
print(""\nFirst 5 rows:"")
print(df.head())

# Show shape
print(""\nDataset shape:"")
print(df.shape)

# Show column names and data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# Calculate survival rate
survival_rate = (df['Survived'].mean() * 100)
print(f""\nSurvival rate: {survival_rate:.2f}%"")

# Show missing values
print(""\nMissing values count:"")
print(df.is...",8,"The generated code is largely correct and addresses most of the requirements.

**Strong points:**
*   **Correctness:** It correctly loads the dataset, displays the first 5 rows, shows the shape, calculates the survival rate, and counts missing values.
*   **Conciseness:** The `df['Survived'].mean() * 100` for survival rate calculation is more concise than the ground truth's `sum() / total_passengers`.
*   **Completeness:** All five requested tasks are addressed.

**Areas for improvement (why not a 10):**
*   **Error Handling:** The ground truth includes `try-except FileNotFoundError`, which is good practice for robust code dealing with file I/O. The generated code lacks this.
*   **Clarity of Output (`df.info()` vs `df.dtypes`):** While `df.dtypes` fulfills the request of ""List all column names and their data types,"" `df.info()` (used in the ground truth) provides a more comprehensive summary, including non-null counts and memory usage, which is typically more valuable for EDA. The prompt asked for ""List all column names and their data types,"" and both satisfy this but `df.info()` is generally preferred in real-world EDA for its richness.
*   **Unnecessary Import:** `import numpy as np` is included but not used. While not a critical issue, it indicates slight inefficiency or a copy-paste without full review.
*   **Output Formatting:** The ground truth includes more explicit print statements (e.g., ""1. First 5 rows of the dataset:"") which makes the output clearer and easier to follow, especially when presenting EDA results. The generated code's print statements are slightly less descriptive (e.g., just ""First 5 rows:"").",claude-3.5-sonnet,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('imdb_top_250.csv')

# 1) Column names and data types
print(""\nColumn Names and Data Types:"")
print(df.dtypes)

# 2) Descriptive statistics for numerical columns
print(""\nDescriptive Statistics:"")
numeric_cols = ['Year', 'Rating', 'Runtime']
print(df[numeric_cols].describe())

# 3) Total number of movies
print(f""\nTotal number of movies: {len(df)}"")
```",6,"The generated code correctly loads the dataset, lists column names and data types, and reports the total number of movies. For the descriptive statistics, it correctly calls `.describe()` on the specified columns.

However, it falls short of fully meeting the requirements in a few crucial areas:
1.  **""Report basic descriptive statistics (mean, median, standard deviation, min, max)""**: While `df[numeric_cols].describe()` provides these, it also includes 'count', '25%', and '75%' which were not explicitly requested. More importantly, it doesn't handle the 'Runtime' column effectively if it's in a string format (e.g., '142 min'). The ground truth code includes robust preprocessing to convert 'Runtime' to numeric, which is a common real-world scenario for this dataset. This missing preprocessing would cause an error or incorrect statistics if 'Runtime' is not purely numeric.
2.  **Robustness**: There's no error handling for `FileNotFoundError` if 'imdb_top_250.csv' is not present. This makes the code less robust than good production code.
3.  **Specific Statistics**: The task asked for specific statistics (mean, median, std, min, max). While `describe()` gives these, directly extracting them would be more precise to the requirement, as demonstrated in the ground truth using `.loc` and renaming '50%' to 'median'.

Overall, the code is a good starting point but lacks the robustness and attention to specific output formats/preprocessing details present in the ground truth.",claude-3.5-sonnet,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

df = pd.read_csv('imdb_top_250.csv')
high_rated = df[df['Rating'] >= 9.0][['Title', 'Rating']]
high_rated_sorted = high_rated.sort_values('Rating', ascending=False)
print(high_rated_sorted)
```",9,"The generated code correctly solves all aspects of the task: it loads the CSV, filters by rating, selects the specified columns, and sorts the data in descending order. The imports are correct, and the code is concise and efficient.

The primary reason it's not a perfect 10 is the lack of error handling (e.g., `FileNotFoundError`, `KeyError`), which the ground truth gracefully includes. While the prompt didn't explicitly ask for error handling, it's a critical component for robust data science applications and often implicitly expected for ""production-ready"" code. However, given the explicit task requirements, the generated code fulfills them perfectly.",claude-3.5-sonnet,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
df = pd.read_csv('imdb_top_250.csv')

# Set style
plt.style.use('seaborn')

# Create histogram
plt.figure(figsize=(10,6))
plt.hist(df['Rating'], bins=20, edgecolor='black')
plt.title('Distribution of IMDB Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

# Create scatter plot
plt.figure(figsize=(10,6))
plt.scatter(df['Runtime'], df['Rating'], alpha=0.5)
plt.tit...",6,"The generated code correctly addresses the core requirements of loading the CSV, plotting a histogram for 'Rating', and a scatter plot for 'Rating' vs. 'Runtime', including titles and labels. However, it lacks robustness in handling potential data issues, specifically with the 'Runtime' column, which often needs cleaning (e.g., converting '120 min' to 120). The Ground Truth code includes essential error handling for file not found and data preprocessing for the 'Runtime' column, which are critical for real-world data science tasks. The generated code assumes the 'Runtime' column is directly numeric, which is unlikely for this dataset.

Improvements needed:
- Implement robust error handling for `FileNotFoundError` when loading the CSV.
- Add preprocessing steps for the 'Runtime' column to extract numeric values (e.g., converting '120 min' to 120) and handle non-numeric entries gracefully, as the scatter plot will fail or produce incorrect results if 'Runtime' is not purely numeric.
- Consider adding more descriptive titles and labels for the plots, similar to the Ground Truth, which provides context like ""(IMDb Top 250)"".
- While `plt.style.use('seaborn')` is used, the histogram uses `plt.hist` directly, which misses out on some of Seaborn's benefits like `sns.histplot` with KDE.",claude-3.5-sonnet,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('penguins.csv')

# 1) Count missing values
print(""\nMissing values per column:"")
print(df.isnull().sum())

# 2) Drop rows with NA in numerical columns and get stats
num_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_clean = df.dropna(subset=num_cols)

print(""\nDescriptive statistics for numerical columns:"")
print(df_clean[num_cols].describe())

# 3) Count unique values
print(""\nU...",8,"The generated code correctly addresses the core requirements of the task. It properly loads the dataset, reports missing values per column, correctly filters rows based on NAs in the specified numerical columns, and calculates descriptive statistics for these filtered columns. It also correctly counts and reports unique values for 'species' and 'island'.

**Strong Points:**
*   Correctly loads the CSV using `pd.read_csv`.
*   Uses `df.isnull().sum()` to report missing values.
*   Correctly uses `df.dropna(subset=num_cols)` to filter rows based on the specified columns.
*   Leverages `df_clean[num_cols].describe()` to efficiently get the required descriptive statistics (mean, median (as 50% percentile), std, min, max).
*   Correctly uses `nunique()` to count unique values for 'species' and 'island'.
*   Includes all necessary imports (`pandas`).

**Areas for Improvement to reach a 10:**
*   **Error Handling for File Loading:** The ground truth includes robust `try-except` blocks for `FileNotFoundError` and general exceptions during file loading. The generated code lacks this, which is crucial for production-ready scripts.
*   **Specificity of Descriptive Statistics:** While `describe()` provides mean, std, min, max, and quantiles (including median as 50%), the prompt specifically asked for `mean, median, standard deviation, min, max`. Using `agg(['mean', 'median', 'std', 'min', 'max'])` as in the ground truth would be more precise to the prompt's request, rather than relying on `describe()` output which might include other statistics like 25% and 75% quartiles not explicitly asked for.
*   **Informative Output:** The ground truth provides more detailed print statements, indicating what each section of output represents (e.g., ""--- Missing Values Report ---"", ""Original number of rows:"", ""Number of rows after dropping NAs...""). The generated code's print statements are more concise but less descriptive for a user.
*   **Existence Checks for Columns:** The ground truth includes checks like `if 'species' in df.columns:` before accessing columns, which makes the code more robust if the dataset schema changes unexpectedly.
*   **Redundant Import:** `import numpy as np` is not strictly necessary as `pandas` handles all operations. While not harmful, it's good practice to only import what's needed.",claude-3.5-sonnet,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"import pandas as pd

df = pd.read_csv('penguins.csv')
filtered_df = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')]
culmen_df = filtered_df[['culmen_length_mm', 'culmen_depth_mm']]
final_df = culmen_df.dropna()
print(final_df.head())",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, filters the DataFrame, selects the specified columns, drops rows with missing values in *those selected columns*, and displays the first 5 rows.

The only minor point of difference/improvement compared to the ground truth would be the `dropna()` call. The ground truth explicitly uses `subset=['culmen_length_mm', 'culmen_depth_mm']`. While in this specific case, since `culmen_df` *only* contains these two columns, `dropna()` without `subset` will have the same effect. However, explicitly defining the `subset` in `dropna()` is often considered slightly more robust or clearer in intent, especially if `culmen_df` was potentially derived from a larger DataFrame but implicitly *only* contained these two columns. For this specific problem, it doesn't lead to incorrect results, but it's a subtle point of best practice. The ground truth also included error handling for `FileNotFoundError`, which is good practice for production code but not explicitly asked for by the prompt's requirements.",claude-3.5-sonnet,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('penguins.csv')
df_clean = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])

plt.figure(figsize=(10,6))
sns.scatterplot(data=df_clean, x='culmen_length_mm', y='culmen_depth_mm', hue='species')
plt.title('Penguin Culmen Measurements by Species')
plt.xlabel('Culmen Length (mm)')
plt.ylabel('Culmen Depth (mm)')
plt.show()
```",8,"The generated code correctly addresses all requirements of the task. It loads the CSV, filters out rows with N/A values in the specified columns, creates a scatter plot with `culmen_length_mm` vs `culmen_depth_mm` colored by `species`, and includes a title and axis labels. The use of seaborn simplifies the plotting process.

Areas for improvement:
- The generated code doesn't include error handling for `FileNotFoundError` or other exceptions when loading the CSV, which could make it more robust.
- The ground truth code wraps the logic in a function and uses an `if __name__ == '__main__':` block, which is good practice for reusability and script organization. The generated code is more of a script, which is acceptable given the prompt asked for ""Python code without explanations"" but demonstrates a minor difference in software engineering practices.
- The `plt.figure(figsize=(10,6))` is a good addition for plot sizing but doesn't explicitly meet any requirement from the prompt. However, it enhances the plot's presentation.

Overall, it's a solid solution that effectively solves the core problem.",claude-3.5-sonnet,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"import pandas as pd
import numpy as np

# Read the dataset
df = pd.read_csv('penguins.csv')

# Fill NA values with mean for specified columns
df['culmen_length_mm'] = df['culmen_length_mm'].fillna(df['culmen_length_mm'].mean())
df['culmen_depth_mm'] = df['culmen_depth_mm'].fillna(df['culmen_depth_mm'].mean())

# Calculate and print statistics for culmen_length_mm
print(""\nculmen_length_mm statistics:"")
print(f""Mean: {df['culmen_length_mm'].mean():.2f}"")
print(f""Median: {df['culmen_length_mm'].me...",8,"The generated code correctly addresses all the requirements of the task. It successfully loads the dataset, fills NA values in the specified numerical columns with their respective means, calculates and prints the mean, median, and standard deviation for these cleaned columns, and finally prints the total number of rows and columns.

**Strong points:**
*   **Correctness:** All calculations and data manipulations are accurate.
*   **Completeness:** All specified tasks are covered.
*   **Concise:** The code is relatively concise and to the point.

**Areas for improvement (to achieve a higher score like 9 or 10):**
*   **Error Handling:** The ground truth includes `try-except` block for `FileNotFoundError`. The generated code does not include any error handling for file not found, which could lead to a `FileNotFoundError` if `penguins.csv` is missing.
*   **Readability/Robustness:** Using hardcoded column names multiple times could be less robust if column names change or if more columns need to be processed. The ground truth uses a list `NUMERICAL_COLUMNS` and iterates, which is more scalable and less prone to typos.
*   **Output Formatting:** While functional, the output formatting of the generated code is less descriptive than the ground truth. The ground truth provides more context (e.g., ""--- NA Imputation Summary ---"", ""Statistics for 'culmen_length_mm':"").
*   **Unnecessary Import:** `import numpy as np` is present but not used.",claude-3.5-sonnet,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('penguins.csv')

# Drop NA values in flipper_length_mm
df = df.dropna(subset=['flipper_length_mm'])

# Create histogram
plt.figure(figsize=(10,6))
plt.hist(df['flipper_length_mm'], bins=20, edgecolor='black')
plt.title('Distribution of Flipper Length (mm)')
plt.xlabel('Flipper Length (mm)')
plt.ylabel('Count')
plt.show()

# Create bar chart
plt.figure(figsize=(10,6))
island_counts = ...",8,"The generated code correctly addresses all the core requirements of the task. It loads the dataset, handles NA values for `flipper_length_mm`, creates a histogram for flipper length, and generates a bar chart for island counts, all with appropriate titles and labels.

**Strong points:**
*   Correctly loads the data and drops NA values.
*   Generates both required plots with correct data.
*   Applies correct titles and x-axis labels for the histogram.
*   Applies correct titles and x and y axis labels for the bar chart.
*   Uses `plt.show()` to display the plots.

**Areas for improvement (to reach a 9 or 10):**
*   **Robustness in loading data:** The ground truth includes error handling for `FileNotFoundError`, which makes the code more robust. The generated code lacks this.
*   **Plotting library choices:** While `matplotlib.pyplot.hist` works, using `seaborn.histplot` (as in the ground truth) often provides a more aesthetically pleasing and informative plot out-of-the-box (e.g., with KDE). Similarly, `seaborn.countplot` is often a direct and concise way to create count plots.
*   **Aesthetics and readability:** The ground truth code includes `sns.set_theme` for professional styling, sets specific `figsize` for better plot sizing, and adds `plt.tight_layout()` for both plots to prevent labels from overlapping, and adjusts font sizes. The generated code only uses `tight_layout` for the second plot and doesn't explicitly set grid styles or font sizes.
*   **Informative print statements:** The ground truth provides more verbose output regarding data loading and NA handling (e.g., how many rows were dropped), which is helpful for debugging and understanding data processing.

Overall, it's a very good attempt that fully completes the core functional requirements. The suggested improvements are mainly for robustness, code style, and enhanced visualization.",claude-3.5-sonnet,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter for Torgersen island
torgersen_df = df[df['island'] == 'Torgersen']

# Count species on Torgersen
species_counts = torgersen_df['species'].value_counts()
print(species_counts)
```",7,"The generated code correctly implements the core logic required by the task: loading the data, filtering for 'Torgersen' island, and counting species. It produces the expected output when the `penguins.csv` file is present and well-formed.

However, it lacks robust error handling and input validation present in the ground truth. Specifically:
- **File Not Found:** It doesn't handle the `FileNotFoundError` if `penguins.csv` is missing, which could cause a program crash.
- **Missing Columns:** There's no check for the existence of 'island' or 'species' columns. If these columns are not present, it would raise a `KeyError`.
- **Empty Filter Result:** It doesn't explicitly handle the case where no penguins are found on 'Torgersen' island (though `value_counts()` on an empty series would result in an empty series, which is technically correct but less informative than an explicit message).
- **Output Clarity:** While it prints the species counts, the ground truth provides clearer descriptive print statements for each step, enhancing user understanding.

The brevity is good for simple scripts, but for production-ready or more robust data analysis code, the error checking and descriptive outputs of the ground truth are highly beneficial.",claude-3.5-sonnet,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4], 
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

print(df.shape)
print(df.dtypes)
```",9,"The generated code correctly performs all the requested tasks: defining the DataFrame, printing its shape, and listing the data types of each column. The code is concise and functionally equivalent to the ground truth.

The only minor point of difference, not a correctness issue, is the lack of descriptive print statements (e.g., ""Shape of the DataFrame:"", ""Data types of each column:"") which were present in the ground truth for better readability. However, the prompt specifically asked for ""only Python code"" and the core requirements were met without these extra print statements. Therefore, this is a very strong submission.",claude-3.5-sonnet,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Read the data
df = pd.read_csv('titanic.csv')

# Set figure size
plt.figure(figsize=(12,8))

# Plot 1: Survival Count
plt.subplot(2,2,1)
survival_counts = df['Survived'].value_counts()
plt.bar(['Did Not Survive', 'Survived'], survival_counts)
plt.title('Survival Count')
plt.ylabel('Number of Passengers')

# Plot 2: Passenger Class Distribution
plt.subplot(2,2,2)
pclass_counts = df['Pclass'].value_counts().sort_index()
plt.bar(['1st...",7,"The generated code largely addresses the requirements but has some significant issues.

**Strengths:**
*   Correctly identifies the need for pandas and matplotlib.
*   Successfully reads the CSV file.
*   Generates four distinct plots as requested.
*   Uses `value_counts()` for categorical data correctly.
*   Uses `subplot` to display all plots in one figure, which is a good design choice for a quick overview.
*   Includes titles and y-axis labels.

**Areas for Improvement/Issues:**

1.  **Missing Age Handling:** The most critical problem is `plt.hist(df['Age'].dropna(), bins=20, edgecolor='black')`. The requirement was to ""handle missing values appropriately."" Dropping missing values is a valid approach for a simple display, but the ground truth code filled missing values with the median, which is a more robust and generally preferred method for age data in Titanic, as it retains more data. `dropna()` simply removes the rows with missing age, potentially losing valuable information from other columns. This impacts the ""handle missing values appropriately"" criteria.
2.  **X-axis Labels for Bar Charts:**
    *   **Survival Count:** `plt.bar(['Did Not Survive', 'Survived'], survival_counts)` works because `value_counts()` for `Survived` (0 and 1) naturally aligns with the provided labels. However, this is hardcoding based on `value_counts()` index.
    *   **Passenger Class:** `plt.bar(['1st Class', '2nd Class', '3rd Class'], pclass_counts)` is also hardcoded. While it happens to align, it's brittle if the unique classes change or are represented differently in the data. It's better to use `pclass_counts.index` directly and map them if custom labels are needed (similar to the ground truth for ""Survived"").
    *   **Gender:** `plt.bar(gender_counts.index, gender_counts)` is correct and good for gender, as it uses the actual index values ('male', 'female'). This inconsistency is noted.
3.  **X-axis Labels Missing:** No x-axis labels are explicitly set for the first, second, and fourth plots, only titles and y-axis labels. The task explicitly asked for ""proper titles and axis labels."" While `subplot` might make it seem redundant for some, providing them explicitly adds clarity.
4.  **No `plt.show()` per plot:** The ground truth code `plt.show()` after each plot. The generated code uses subplots and one `plt.show()` at the end, which is perfectly acceptable and often preferred for a single figure display. The `matplotlib.figure(figsize=(12,8))` outside the subplots will apply to the entire figure.
5.  **Robustness (File Not Found):** The generated code does not include error handling for `FileNotFoundError` when reading the CSV, unlike the ground truth. While not explicitly asked for in the prompt, it's a good practice for robustness.
6.  **Readability/Separation:** While `subplot` is efficient, separating each plot into its own block (as in the ground truth) often makes the code more modular and easier to read/debug, especially for complex visualization requirements that might involve more specific parameters for each chart. However, subplots are also a valid approach.

The primary deduction comes from the age handling and the missing x-axis labels on most categorical plots.",claude-3.5-sonnet,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]}

df = pd.DataFrame(data)

print(f""Average Midterm Score: {df['Midterm_Score'].mean():.2f}...",9,"The generated code successfully accomplishes all parts of the task. It correctly creates the DataFrame, calculates and prints the average midterm and final scores, and identifies the subject with the highest average final score. The code is concise and uses appropriate pandas methods.

One minor point that could improve the output precision is to match the ground truth's exact phrasing for the print statements, for example, ""Average Midterm Score for all students"". Also, the ground truth explicitly uses variable assignments for `average_midterm_score` and `average_final_score` before printing, which can sometimes improve readability for complex calculations, though direct printing as done in the generated code is perfectly acceptable for these simple averages. The generated code's print statement for the subject with the highest average final score is slightly more concise than the ground truth while still providing all requested information, which is a good thing.",claude-3.5-sonnet,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
        'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
        'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
        'Volume': [1000000, 120...",9,"The generated code successfully completes almost all aspects of the task. It correctly creates the DataFrame with the 'Date' column as the index, calculates the 'Daily_Range' column, and finds the date with the highest 'Close' price. The DataFrame is also printed with the new column.

The primary area for improvement is the formatting of the printed date for the highest close price. The ground truth code uses `strftime` to format the datetime index, which is generally a best practice for readability. The generated code prints the raw datetime object, which is less user-friendly. Additionally, the ground truth explicitly adds a newline character for better separation of output, which is a minor stylistic choice.

Overall, it's a very good solution that correctly implements the core logic.",claude-3.5-sonnet,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002'...",10,"The generated code is excellent. It correctly creates the DataFrame, calculates the 'Total_Revenue' column, accurately identifies the best-selling product by quantity, and prints the first 5 rows of the DataFrame, fulfilling all requirements of the task. The code is clean, well-structured, and uses standard pandas operations efficiently. The variable names are clear, and the f-strings for printing are well-formatted. All necessary imports are included.",claude-3.5-sonnet,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"import pandas as pd
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

print(""Average Session Duration:"", df[""SessionDurationMinutes""].mean())
print(""\nDevice Type Count:"")
print(df[""DeviceType""].value_counts(...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It successfully defines the DataFrame, calculates the average session duration, and counts users by device type using appropriate pandas functions (`.mean()` and `.value_counts()`).

The code quality is good, with correct imports and a clear structure. The only minor area for improvement, preventing a perfect score, is the lack of explicit formatting for the average session duration output (e.g., `:.2f` for two decimal places) and the missing separator line between outputs, which were present in the ground truth for better readability and a more polished presentation. While not strictly required for correctness, these small details improve output clarity for the user.",claude-3.5-sonnet,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

print(f""Maximum Temperature: {df['TemperatureCelsius'].max()}"")
print(f""Minimum Humidity:...",10,"The generated code is excellent. It correctly defines the DataFrame with the specified data. It then accurately finds the maximum ""TemperatureCelsius"" and the minimum ""HumidityPercent"" using the `.max()` and `.min()` pandas methods, respectively, and prints them as requested. The code is concise, imports necessary libraries, and directly addresses all requirements of the task.",claude-3.5-sonnet,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""], 
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

print(f""Total likes: {df['Likes'].sum()}"")

avg_shares = df.groupby('PostType')['Shares'].mean()
print(f""Post type with highest ...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates and prints the total number of likes, and precisely identifies and prints the `PostType` with the highest average number of `Shares`. The code is concise, efficient, and directly addresses all parts of the prompt with correct pandas operations. The variable names are clear, and the f-strings for printing provide good readability for the output.",claude-3.5-sonnet,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",9,"The generated code correctly defines the DataFrame, calculates the average age, and counts patients by gender, fulfilling all requirements of the task. The code is concise and directly addresses the problem.

The only minor area for improvement that prevents a perfect score is the output formatting for the average age. The ground truth code formats the average age to two decimal places, which is good practice for numerical outputs like averages, and includes units (""years""). While the generated code's output is correct, adding similar formatting and context would improve its readability and user-friendliness, aligning more closely with best practices for presenting data analysis results. The ground truth also adds slightly more descriptive print statements for clarity. However, these are minor aesthetic and readability improvements rather than functional correctness issues.",claude-3.5-sonnet,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""], 
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}
df = pd.DataFrame(data)

print(df.loc[df['Rating'].idxmax(), 'Title'])
print(df['Year'].mean())
```",9,"The generated code correctly performs all the required tasks: it defines the DataFrame, prints the title of the movie with the highest rating, and prints the average year. The use of `idxmax()` is a concise and efficient way to find the index of the maximum rating, which is then used to retrieve the corresponding title. Calculating the mean of the 'Year' column is also straightforward and correct.

The code is very concise and efficient. The only minor point that prevents a perfect 10 is the lack of explicit print statements, which are present in the ground truth for clarity (e.g., ""The title of the movie with the highest rating is:""). While not strictly required by the prompt, adding these would improve user readability for the output. However, functionally, the code is entirely correct and follows best practices for pandas operations.",claude-3.5-sonnet,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

print(""Total number of cars:"", len(df))
print(""Average price:"", df[""PriceUSD""]....",9,"The generated code correctly defines the DataFrame, accurately calculates the total number of cars, and computes the average ""PriceUSD"". It effectively addresses all specified requirements. The code is concise and functional.

One minor area for improvement relates to the output format of the average price. While correct, the ground truth code formats the average price to two decimal places and includes a dollar sign, which is good practice for currency values. The generated code does not include this formatting, which is a slight deviation from a ""perfect"" user output, but doesn't impact correctness of the calculation.

Another minor point is using `df.shape[0]` for total rows is generally preferred over `len(df)` for larger DataFrames as `shape` is often faster, though `len(df)` is perfectly acceptable and widely understood for DataFrame row count. Overall, it's a very good solution.",claude-3.5-sonnet,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""], 
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}
df = pd.DataFrame(data)

print(df.loc[df['HP'].idxmax(), 'Name'])
print(df.loc[d...",10,"The generated code correctly defines the DataFrame and accurately identifies and prints the names of the Pokemon with the highest HP and lowest Defense. The code is concise and directly addresses all requirements of the task. It demonstrates proper use of pandas functions like `idxmax()` and `idxmin()` combined with `loc` for efficient data retrieval. The imports are correct, and the structure is clean.",claude-3.5-sonnet,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('titanic.csv')

# Check missing values
print(""Missing values:\n"", df.isnull().sum())

# Fill missing Age values with median
df['Age'] = df['Age'].fillna(df['Age'].median())

# Fill missing Embarked values with mode
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])

# Drop Cabin column
df = df.drop('Cabin', axis=1)

# Extract titles from Name
df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.', expa...",9,"The generated code correctly performs all the required data cleaning steps. It handles missing values for 'Age' and 'Embarked', drops the 'Cabin' column, and correctly extracts and normalizes titles. The use of `str.extract` for title extraction is concise and effective.

Minor improvements could be:
- Adding a `try-except FileNotFoundError` block for robust file loading, similar to the ground truth.
- The prompt explicitly asks to display the cleaned dataset's shape and *the first 5 rows* as two separate print statements, which the generated code does slightly differently by combining them more with `\n` but still presents the correct information. The ground truth formatted these more distinctly.
- The `np` import is not strictly necessary as `numpy` functions are not explicitly used, although pandas relies on it underneath. This is a very minor point and not incorrect.

Overall, it's a very good solution that addresses all requirements effectively and concisely.",claude-3.5-sonnet,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""], 
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}

df = pd.DataFrame(data)

print(df.loc[df...",9,"The generated code correctly performs all the required tasks. It accurately defines the DataFrame, identifies the title of the book with the highest price, and calculates the total stock. The code is concise and efficient, demonstrating a good understanding of pandas operations like `idxmax()` and `sum()`. The only minor improvement would be to add descriptive print statements for clarity, similar to the ground truth code, to make the output more user-friendly. However, this doesn't affect the correctness or functionality of the code based on the core requirements.",claude-3.5-sonnet,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('water_potability.csv')

# Display first 10 rows
print(""\nFirst 10 rows:"")
print(df.head(10))

# Show shape
print(""\nDataset Shape:"")
print(df.shape)

# List columns and data types
print(""\nColumns and Data Types:"")
print(df.dtypes)

# Calculate potable vs non-potable percentages
print(""\nPotability Distribution:"")
potability_dist = df['Potability'].value_counts(normalize=True) * 100
print(potability_dist)

# Count...",7,"The generated code correctly performs most of the requested tasks.

**Strong Points:**
*   Correctly loads the dataset.
*   Correctly displays the first 10 rows, shape, and summary statistics.
*   Correctly calculates and displays potability percentages using `normalize=True`, which is concise.
*   Correctly counts missing values.

**Areas for Improvement:**
*   **Error Handling (Critical Missing Point):** The generated code lacks any error handling for `FileNotFoundError` or other exceptions when loading the CSV. The ground truth includes robust `try-except` blocks, which are crucial for real-world applications. This is a significant omission.
*   **Clarity for Column Names/Data Types:** While `df.dtypes` is correct, `df.info()` as used in the ground truth provides a more comprehensive overview including non-null counts, which is generally more informative for EDA.
*   **Display of Missing Values:** The generated code shows all columns for missing values, even those with zero missing values. The ground truth provides a cleaner output by filtering to show only columns with actual missing values (`missing_values[missing_values > 0]`), and also adds a confirmation message if no missing values are found.
*   **Interpretation for Potability:** The ground truth code explicitly states ""Interpretation: 1 = Potable, 0 = Non-Potable"", which enhances clarity for the user.
*   **Formatting and Readability:** The ground truth uses separators (`""=""*50`) and more descriptive print statements, making the output much easier to navigate and understand, especially when running all parts of the EDA consecutively.
*   **Unnecessary Import:** `import numpy as np` is not used in the generated code, making it an unnecessary import.

The lack of error handling is a major drawback that prevents a higher score, as it's a fundamental aspect of robust code. The minor improvements in output formatting and detail would also enhance the user experience significantly.",claude-3.5-sonnet,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('water_potability.csv')

# Store original info
original_shape = df.shape
original_nulls = df.isnull().sum()

# Identify and fill missing values
for col in df.columns:
    if df[col].isnull().any():
        df[col].fillna(df[col].mean(), inplace=True)

# Remove duplicates
df.drop_duplicates(inplace=True)

# Define reasonable ranges for parameters
ranges = {
    'ph': (0, 14),
    'Hardness': (0, 1000),
    'Solids':...",7,"The generated code correctly addresses most of the requirements but has some shortcomings in robustness and adherence to detail.

**Strengths:**
*   Correctly loads the dataset.
*   Identifies and fills missing values using the mean, although not exclusively for numerical columns, which could lead to issues if there were non-numeric columns with NaNs (though not the case for this dataset).
*   Correctly removes duplicate rows.
*   Attempts to verify data ranges, which is a good step.
*   Provides a summary showing before/after shapes and missing values.
*   Displays the first 5 rows of the cleaned dataset.

**Areas for Improvement:**
*   **Missing Value Strategy Robustness:** The code iterates through all columns to fill missing values with the mean (`df[col].fillna(df[col].mean(), inplace=True)`). While this works for the given dataset (as all columns with missing values are numerical), it's not a robust approach. If there were categorical columns with missing values, trying to fill them with the `mean()` would raise an error or produce meaningless results. The ground truth specifically selects numerical columns for mean imputation.
*   **FileNotFoundError Handling:** The ground truth includes `try-except FileNotFoundError` for loading the CSV, which is good practice for robustness. The generated code lacks this.
*   **Clarity in Summary:** While it provides before/after statistics, the print statements are less informative than the ground truth. For example, the ground truth explicitly states ""Missing Values Before Cleaning:"" and only shows columns with >0 missing values, making it clearer. The generated code prints `original_nulls` which will show all columns, even those without missing values, making it a bit verbose.
*   **Range Validation Detail:** The range validation is implemented, but the ranges defined are somewhat arbitrary (e.g., Hardness up to 1000, Turbidity up to 100). While the prompt asks for ""reasonable ranges,"" these are quite wide and don't necessarily reflect tighter, more realistic physical limits or typical ranges for potable water. The code also doesn't explicitly state how many rows were removed due to out-of-range values, unlike the ground truth, which provides this valuable diagnostic information.
*   **Task Order and Detail:** The Ground Truth more explicitly follows the task order and provides clearer in-line comments for each step. The generated code lumps some summary steps together implicitly.
*   **Numpy Import:** `numpy` is imported, but `np` is not used in the code. It's not a functional issue, but good practice to remove unused imports.",claude-3.5-sonnet,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Read the dataset
df = pd.read_csv('water_potability.csv')

# Set style
plt.style.use('seaborn')

# Create figure with subplots
fig = plt.figure(figsize=(15, 10))

# Bar chart of potable vs non-potable
plt.subplot(2, 2, 1)
df['Potability'].value_counts().plot(kind='bar')
plt.title('Distribution of Water Potability')
plt.xlabel('Potability (0=Not Potable, 1=Potable)')
plt.ylabel('Count')

# Histograms
plt.subplo...",6,"The generated code attempts to address all aspects of the task, meeting the basic requirements for each visualization. However, there are significant areas for improvement in terms of code quality, visualization clarity, and best practices.

**Areas for Improvement:**

*   **Subplot Handling:** The approach of combining all plots into a single `fig` with `plt.subplot` is problematic for the histograms. `df[['ph', 'Hardness', 'Chloramines']].hist()` generates *its own* figure and subplots, independently of the `plt.subplot(2, 2, 2)` call. This results in the histograms being displayed in a separate, unformatted window instead of within the intended subplot grid. This is a critical structural issue.
*   **Histogram Presentation:** Even if the histograms were correctly placed, `df.hist()` for multiple columns creates separate plots for each, which isn't ideal when the request was for ""histograms for pH, Hardness, and Chloramines showing their distributions"" implying they should be presented clearly, potentially on the same subplot row as done in the ground truth for better comparison and organization. The ground truth uses `seaborn.histplot` which offers more control and better default aesthetics.
*   **Missing Value Handling:** While `seaborn` plotting functions often handle `NaN`s by ignoring them, the generated code doesn't explicitly demonstrate awareness or strategy for missing values. For instance, the ground truth explicitly uses `.dropna()` for `histplot` to be clear, though Seaborn's `histplot` would handle it. For correlation, `df.corr()` handles NaNs pairwise, which is acceptable, but it's good to be aware.
*   **Readability and Customization:**
    *   The `matplotlib.pyplot` bar chart for potability is very basic. `seaborn.countplot` (used in ground truth) is more visually appealing and specifically designed for categorical counts.
    *   Plot titles, labels, and legends are present but could be more descriptive (e.g., specifying units or clearer descriptions for axes).
    *   The overall aesthetic of the plots in the generated code is less polished compared to the ground truth, particularly due to the `plt.style.use('seaborn')` which is a generic style, instead of specific color palettes and `set_style` for `seaborn`.
*   **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError`, which is good practice for robust code. The generated code lacks this.
*   **Redundant `plt.tight_layout()`:** The `plt.tight_layout()` is called twice and positioned incorrectly after the histogram subplot which generates its own figure, reducing its effectiveness there. It should be called once, at the very end before `plt.show()`, after all subplots are defined.

In summary, the code provides a basic attempt at each visualization but suffers from fundamental issues in subplot management and the effectiveness of the chosen plotting functions, leading to a suboptimal and confusing output for the user.",claude-3.5-sonnet,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# Load data
df = pd.read_csv('winequality-red.csv', sep=';')
X = df.drop('quality', axis=1)
y = df['quality']

# Create new features
X['total_acidity'] = X['fixed acidity'] + X['volatile acidity'] + X['citric acid']
X['acidity_r...",7,"The generated code addresses most aspects of the prompt, including feature engineering, using two distinct feature selection methods (RFE and SelectKBest), and comparing their results.

**Strengths:**
- **Feature Engineering:** Creates 4 new features, which is more than the required ""at least three"". The chosen features ('total_acidity', 'acidity_ratio', 'sugar_acid_ratio', 'alcohol_density_ratio') are meaningful.
- **Multiple Feature Selection Methods:** Correctly implements RFE with RandomForestRegressor and SelectKBest with `f_regression`.
- **Comparison:** Attempts to compare the selected features using set intersections, which is a good approach.
- **Code Structure:** The code is reasonably well-structured and uses appropriate imports.
- **Handling of NaN/Inf:** Although not explicitly shown, `pd.read_csv` without `sep=';'` might incorrectly parse `winequality-red.csv` if it's comma-separated, but the ground truth indicates it's comma-separated. The generated code uses `sep=';'` which is specific to another common format of this dataset. For the `winequality-red.csv` provided, it's comma-separated. This could lead to a `FileNotFoundError` or incorrect parsing.

**Areas for Improvement to achieve a higher score:**

1.  **Dataset Loading:** The `pd.read_csv('winequality-red.csv', sep=';')` line is likely incorrect for the standard `winequality-red.csv` dataset, which is usually comma-separated. This would lead to incorrect data loading and subsequent errors or poor results. The ground truth correctly assumes it's comma-separated. This is a critical error for execution correctness.
2.  **Robustness in Feature Engineering:** The engineered features `acidity_ratio` and `sugar_acid_ratio` involve division. Like the ground truth, it's good practice to add a small epsilon to denominators to prevent division by zero errors or `inf` values, which can then cause issues in `StandardScaler` or subsequent models. The generated code doesn't handle this.
3.  **Correctness of RFE Feature Importance Display:** The RFE selected features are displayed with 'Rank', but the prompt asks for ""importance scores or rankings"". While rank is given, getting importance scores for the *selected* features (by retraining the estimator on just those features after RFE) would be more informative and align better with typical RFE output analysis, similar to how the ground truth does it. The current display of `rfe_features` shows all features with their rank, not just the selected ones with their performance-based importance.
4.  **Display of SelectKBest Scores:** For SelectKBest, the scores are correctly calculated, but the output `skb_selected` only shows the top 6 features with their scores. It would be clearer to present the selected features with their scores explicitly, confirming they are the ones identified by the selector. The current output structure is fine but could be more explicit in stating that these are the *selected* features and their scores.
5.  **Direct Comparison and Contrasting:** While the code identifies common features, the prompt also asks to ""Compare and contrast the feature sets selected by each method"". The ground truth provides excellent textual explanation of why differences might arise (e.g., filter vs. wrapper methods, univariate vs. multivariate). The generated code only provides the lists of common features but lacks this crucial qualitative comparison and reasoning.
6.  **""Final Selected Features with Importance Scores/Rankings"":** The prompt asks for this. The generated code calculates `final_features` based on common features across RFE, SelectKBest, and *Random Forest's inherent feature importances (which wasn't explicitly asked for as a separate method)*. While including RF importance is fine, the prompt specifically asked for RFE and SelectKBest. The final display correctly shows importance for the `final_features` but the methodology for defining `final_features` could be made clearer if it deviates from the ""top 5-7 selected by RFE and SelectKBest"". For example, if the common set is less than 5-7 features, what then? The number of features to select (`n_features_to_select=6` and `k=6`) is fixed.
7.  **Unnecessary Import:** `train_test_split` is imported but not used. It's a minor point, but good practice to remove unused imports.

The most significant flaw is the incorrect `sep=';'` in `pd.read_csv`, which would likely make the code non-functional as intended. The lack of robustness for division by zero in engineered features is also a notable omission.",claude-3.5-sonnet,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.base import BaseEstimator, TransformerMixin
impor...",6,"The generated code attempts to address many aspects of the prompt but falls short in several key areas, impacting its correctness and completeness, particularly in the preprocessing and evaluation phases.

**Correctness:**
*   **Preprocessing:** The use of `LabelEncoder` for features like 'Platform' and 'Genre' is generally not recommended for tree-based models (and especially not for linear models in stacking) unless the cardinality is very low or inherent ordinality exists. One-hot encoding is usually preferred to prevent the model from inferring a false ordinal relationship. The `dropna()` approach for missing values is too simplistic and can lead to significant data loss without proper analysis. The ground truth uses `SimpleImputer` and `OneHotEncoder` via `ColumnTransformer` and `Pipeline`, which is a much more robust and standard approach.
*   **Target Creation:** The prompt asks to ""Create a regression target to predict Global_Sales"". The code assumes it's already there and drops 'Global_Sales' from X but doesn't explicitly mention or reinforce this step effectively.
*   **Cross-Validation:** The cross-validation for individual models is done using `cross_val_score` on `X_train_scaled`. This is okay, but then the models are fitted on `X_train_scaled` and evaluated on `X_test_scaled` (a single split). The prompt explicitly asks to ""Use cross-validation with 5 folds to evaluate each model."" This means the reported MAE, MSE, RMSE, R should ideally be `cv_scores` for these metrics, not just R and then a single test set evaluation. The ground truth correctly implements a full 5-fold CV for each model, showing metrics per fold and then averaging them.
*   **Hyperparameter Tuning:** Tuning is only done for the ""best model,"" and only if it's `RandomForest`. If another model is chosen as best (e.g., XGBoost, GB) based on `cv_scores`, it won't be tuned, which misses the point. The ground truth dynamically selects the best model and applies a defined tuning grid for it.
*   **Feature Importance:** The feature importance plot is only generated if the `best_model` happens to be a `RandomForestRegressor`. The prompt requested it for ""ensemble models"" generally, implying potentially all the tree-based ones. Also, the scaling applied earlier means `best_model.feature_importances_` might relate to scaled features, but the `X.columns` used for plotting are original. In the ground truth, the preprocessor is explicitly used to get the transformed feature names, which is more accurate.
*   **Stacking Ensemble:** The stacking implementation manually creates `meta_features_train` and `meta_features_test` by training base models on `X_train_scaled` and predicting on `X_train_scaled` (for training meta-learner) and `X_test_scaled` (for final prediction). A proper `StackingRegressor` from `sklearn.ensemble` uses out-of-fold predictions from base models to train the meta-learner, preventing data leakage and overfitting. The generated code's stacking approach is a common naive implementation but not the `sklearn` standard one, and it misinterprets the prompt's request for `StackingRegressor`. The ground truth correctly uses `sklearn.ensemble.StackingRegressor`.
*   **Residual Plots:** The residual plot is generated using `best_model.predict(X_test_scaled)`. This means it only shows residuals for the `best_model` (and only if it was `RandomForest` and tuning occurred, otherwise it's just the untuned RF), not necessarily the overall best or desired model for analysis (e.g., the Stacking Ensemble which often performs better). The prompt allows for analysis of ""prediction errors"", so using a good final model is implicit.

**Code Quality:**
*   **Imports:** Mostly correct, but `BaseEstimator` and `TransformerMixin` are imported but not used. `xgb` is imported explicitly while `XGBRegressor` from `xgboost` is imported. It's inconsistent.
*   **Structure:** The flow is somewhat linear and logical, but the separation of concerns (e.g., preprocessing logic from model evaluation) is not as clean as in the ground truth using `Pipeline` and `ColumnTransformer`.
*   **Reproducibility:** A `random_state` is set for models, but not for data splitting (`train_test_split`) or k-fold CV which affects overall reproducibility.
*   **Readability:** Variable names are clear.

**Completeness:**
*   **Multiple Ensemble Methods:** All listed methods (RF, GB, XGBoost, AdaBoost) are present.
*   **Cross-Validation:** Partially implemented for R2, but not for all metrics per fold as requested.
*   **Voting Ensemble:** Implemented correctly using `VotingRegressor`.
*   **Hyperparameter Tuning:** Only for one model and condition-dependent.
*   **Feature Importance Plots:** Only for one model and condition-dependent.
*   **Evaluation Metrics:** MAE, MSE, RMSE, R are calculated for the single test set, but not comprehensively across CV folds for all models as implied by points 4 and 8.
*   **Stacking Ensemble:** Misinterpreted and implemented manually instead of using `sklearn.ensemble.StackingRegressor`, which is a key part of the prompt.
*   **Residual Plots:** Present, but tied to a specific model.
*   **Data Preprocessing:** Simplistic (`dropna`, `LabelEncoder`) compared to the robust requirements implied by ""handling missing values and encoding categorical variables"" for a real-world dataset. The ground truth's `ColumnTransformer` and `Pipeline` setup is superior.
*   **""Create a regression target to predict Global_Sales""**: This step is generally done implicitly by selecting `Global_Sales` as `y`, but the prompt includes it as a distinct step. The generated code handles it correctly in terms of variable assignment but doesn't explicitly highlight it.",claude-3.5-sonnet,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import TimeSeriesSplit
import pymc3 as pm
import ruptures as rpt
from lifelines import KaplanMeierFitter
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# L...",3,"The generated code attempts to address many requirements but fails on several critical aspects, leading to a low score.

1.  **Correctness:**
    *   **ARIMA:** The `fit_arima` function uses `statsmodels.tsa.arima.model.ARIMA`, which is a lower-level API. `pmdarima.auto_arima` (used in ground truth) is specified in the task as ""or similar"" and provides a much more robust and automated way to find the best ARIMA model, especially for complex datasets. The `order=(1,1,1)` is hardcoded and may not be optimal.
    *   **Seasonal Decomposition:** The `period` argument is used `yearly_sales` directly, which is problematic for yearly data as a period of 5 years might not be appropriate for all datasets, and `extrapolate_trend='freq'` is not a standard `seasonal_decompose` parameter; it should be `extrapolate_trend='freq'` which is not valid, and `freq` is not present in the new version.
    *   **Multivariate Regression:** The code generates interaction terms for *all* features using `PolynomialFeatures(degree=2, include_bias=False)`. This will create an enormous number of features from `Year`, `Genre`, `Platform`, `Publisher` and their encoded versions, leading to issues like multicollinearity and poor model interpretability, especially for a large number of categorical levels. The ground truth carefully selects a smaller subset of interaction terms. There's no actual application of the multivariate model, only preparation of `X_scaled`. The task asked for *building* the model to predict `Global_Sales`.
    *   **Bayesian Regression:** The `X_scaled` for Bayesian model uses the excessively large feature set from the multivariate regression prep. This will cause the `pm.sample` to be extremely slow or crash due to memory issues, especially with default sampling parameters. The ground truth smartly limits the features for Bayesian regression for practical reasons.
    *   **Survival Analysis:** The definition of 'game longevity' and 'event' is incorrect and arbitrary (`df['Global_Sales'] > sales_threshold`). The task implies a time-based 'longevity' (years a game remains profitable), not just a sales threshold at a single point. The ground truth correctly defines longevity as the span of years a game was active in the dataset, and the event as simply being observed during that period for KM, and then uses Cox PH with relevant covariates. The provided implementation does not align with industry-standard survival analysis definitions.
    *   **Confidence/Prediction Intervals:** The `calc_prediction_interval` function is only defined for Ridge and Lasso, and it's a very simplistic calculation based on residuals, assuming homoscedasticity and normality, which is not robust. It does not provide CIs or PIs for ARIMA, multivariate regression, or Bayesian models robustly. The ground truth provides more specific and correct methods for each model type where applicable (e.g., `get_forecast().conf_int()` for ARIMA, `summary_frame` for `statsmodels.OLS`).
    *   **Validation:** Time series cross-validation is performed, but only for `LinearRegression` using the massively inflated `X_scaled`. It's not applied to ARIMA or the other regression models robustly. There's no specific validation for the Bayesian or survival models.
    *   **Data Preparation:** The initial data loading has `df['Year']` which should be `df['Year_of_Release']` as per the CSV context often used in such datasets. This will likely cause a `KeyError`.

2.  **Code Quality:**
    *   **Imports:** Mostly correct, but not all imported libraries are fully utilized or utilized correctly (e.g., `pymc3` is used but likely crashes with the large X, `statsmodels.formula.api` is missing for explicit formula handling in multivariate regression).
    *   **Readability/Structure:** Functions are defined, but the flow isn't always logical. Many parts are missing error handling (e.g., `pm.sample` can fail).
    *   **Modularity:** Some parts are put into functions (`fit_arima`, `create_interaction_terms`), which is good, but the overall structure isn't very robust.

3.  **Completeness:**
    *   **ARIMA:** Only basic ARIMA (hardcoded order) is implemented, not robust forecasting using auto_arima or similar.
    *   **Seasonal Decomposition:** Implemented, but potentially incorrectly applied to yearly data and with wrong parameters.
    *   **Multivariate Regression:** The model is prepared with `pd.get_dummies` and `PolynomialFeatures` but *not actually built or fitted* in a meaningful way to predict global sales beyond setting up `X_scaled`. The formula interface with interaction terms is missing.
    *   **Regularization:** Ridge and Lasso models are fitted, but their coefficients or proper evaluation beyond storing the models isn't clear.
    *   **Bayesian Regression:** The model setup is there, but it's likely impractical due to the massive `X_scaled`. No summary or post-processing of results is shown for PyMC3 to interpret uncertainty.
    *   **Change Point Detection:** Implemented using `ruptures`, which is good.
    *   **Survival Analysis:** Kaplan-Meier is implemented, but the 'longevity' and 'event' definitions are fundamentally flawed for this task. CoxPH model (a key part of survival analysis for covariates) is completely missing.
    *   **Confidence/Prediction Intervals:** Very weak and limited implementation.
    *   **Time Series Cross-Validation:** Limited application and questionable data preparation for the cross-validation.
    *   **Plotting:** Basic plots are generated at the end, but they are not detailed enough to show all model results (e.g., multivariate model, Bayesian results).

In summary, the code provides fragments of solutions for most requirements but often implements them incorrectly, inefficiently, or incompletely, missing crucial domain understanding (e.g., for survival analysis or appropriate feature engineering for complex models). The severe issues with practical model building (especially multivariate and Bayesian regression due to feature explosion) and the flawed survival analysis significantly detract from its utility.",claude-3.5-sonnet,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.manifold import TSNE
import umap
from scipy.stats import chi2_contingency, ttest_ind
from scipy.cluster.hierarchy import dendrogram, linkage
from mlxtend.frequent_patterns import apriori, assoc...",5,"The generated code attempts to address most of the requirements but falls short on several critical aspects, especially regarding data preprocessing, the depth of clustering analysis, and proper integration of the generated components.

**Primary Reasons for Failure/Critical Issues:**

1.  **Incomplete Data Preprocessing:**
    *   **Missing Value Handling:** The code loads the CSV but doesn't handle missing values at all. The ground truth explicitly addresses this for 'Name', 'Genre', 'Platform', 'Publisher', and 'Year_of_Release', and then fills sales NaNs with 0. The generated code's `create_features` function will likely fail or produce NaNs if `total_sales` is zero or if sales columns have NaNs, leading to division by zero errors or incorrect ratios.
    *   **Feature Engineering:**
        *   `Regional_Sales_Distribution`: The generated code calculates `_Ratio` based on `sum(axis=1)`. This is problematic if `Global_Sales` is a distinct column, as it often is, indicating total sales. The ground truth correctly uses `Global_Sales` for proportion calculation and handles division by zero.
        *   `Publisher_Portfolio_Size`: The generated code calculates `transform('count')` on `Name`, which gives the total number of games *per publisher in the dataframe*, not the *unique* number of games. This is a significant functional difference from `nunique()` in the ground truth.
    *   **Categorical Feature Handling for Clustering:** The generated code completely ignores categorical features (`Genre`, `Platform`, `Publisher`) for clustering. It only selects `cluster_features` as numerical ones (`Global_Sales`, ratios, Market_Position, Publisher_Portfolio_Size), which is a major oversight given the task explicitly mentions ""advanced clustering and market segmentation analysis"" on game data. The ground truth correctly uses `ColumnTransformer` and `OneHotEncoder`.

2.  **Lack of Advanced Techniques / Incorrect Implementation:**
    *   **Optimal Cluster Determination:** Only K-Means is used for optimal k determination (silhouette, elbow from inertia). The task explicitly asks for this for *multiple* clustering algorithms. The ground truth performs this for K-Means, GMM, and Hierarchical.
    *   **DBSCAN Parameter Tuning:** The generated code hardcodes `eps = 0.3` and `min_samples = 5`. The ground truth demonstrates a heuristic approach using `NearestNeighbors` and plotting for `k-distance`, which is a common and necessary step for DBSCAN in real-world scenarios. Without proper tuning, DBSCAN often yields poor results (e.g., all noise or one giant cluster).
    *   **Consensus Clustering:** The implementation is highly simplistic. `consensus_matrix /= len(cluster_labels)` implies a simple average, but the further use of this matrix is not specified. The ground truth correctly defines a co-occurrence matrix by counting agreements and then uses hierarchical clustering on a derived distance matrix, which is a standard approach for consensus. The generated code doesn't actually produce a final set of consensus cluster labels.
    *   **Cluster Profiling and Statistical Significance:** The `profile_clusters` function only calculates means for numerical features. It completely omits profiling categorical features (e.g., top genres, platforms, publishers per cluster). More critically, it lacks *statistical significance testing* (ANOVA, Chi-squared) which is a direct requirement of the prompt (""Create detailed cluster profiles with statistical significance testing""). This is a huge gap.

3.  **Missing or Incomplete Requirements:**
    *   **Gap Statistic:** This method for optimal cluster determination is mentioned in the prompt but completely omitted in the generated code.
    *   **External Validation Metrics:** The prompt asks for ""internal and external validation metrics."" The generated code only provides internal (silhouette, calinski). External metrics (e.g., Adjusted Rand Index, Normalized Mutual Information, etc., if ground truth labels were available, which they aren't for this problem, but the prompt asks for both) are completely missing. The term ""external"" is a bit ambiguous given the input, but a complete solution would at least acknowledge this.
    *   **Dendrogram:** The `dendrogram` import is there, but it's never used.
    *   **`ttest_ind`:** Imported but unused. `f_oneway` and `chi2_contingency` (used in ground truth) are more appropriate for comparing multiple clusters.
    *   **Integration and Flow:** The code is structured into many separate functions, but the main script flow doesn't always integrate them meaningfully or pass `df` around consistently with added columns. For example, `cluster_labels` is a tuple of arrays, and it's not clear how these would be added back to the original `df` for profiling in the `profile_clusters` function without manual assignment. This makes the code harder to run and use.

The code attempts to cover the requirements but often implements simpler versions or misses crucial details specified in the prompt or expected in an ""advanced"" analysis, particularly in data preprocessing, statistical validation, and the depth of clustering techniques.",claude-3.5-sonnet,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.ensemble import VotingClassifier, BaggingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, roc_auc_score, classif...",6,"The generated code attempts to address many of the requirements but falls short in completeness and depth, especially when compared to the ground truth.

**Strengths:**
*   **Basic Ensemble Implementation:** Includes Voting, Bagging, and mentions base learners (RF, GB, XGB, LGBM, CatBoost).
*   **Bayesian Optimization:** Utilizes Optuna for hyperparameter tuning.
*   **SHAP Interpretation:** Includes SHAP for model interpretation.
*   **Model Calibration:** Uses `CalibratedClassifierCV`.
*   **Nested Cross-Validation:** Attempts to implement nested CV, which is good.
*   **Custom Ensemble:** Includes a `DynamicWeightedEnsemble` class.

**Areas for Improvement & Missing/Incomplete Aspects:**

1.  **Comprehensive Feature Engineering:**
    *   **Ground Truth:** Handles 0 values as NaNs, imputes medians, applies `PolynomialFeatures` for interaction terms, and creates custom ratios like `Glucose_Insulin_Ratio`.
    *   **Generated:** Only adds a `BMI_Age_Ratio` and `Glucose_BMI_Interaction`. It doesn't handle the logical 0 values in the dataset which are critical for preprocessing `diabetes.csv`.
2.  **Advanced Ensemble Methods & Stacking (Detailed Implementation):**
    *   **Ground Truth:** Explicitly defines base estimators, then implements Voting (soft/hard), Bagging with *different* base estimators (DT, LR, SVC), AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost. It also has a well-defined `StackingClassifier` with base learners and a meta-learner.
    *   **Generated:** Only defines base models, a generic `VotingClassifier` (no hard voting example), and mentions `base_models` for stacking but *doesn't actually implement or use* the `StackingClassifier` from `sklearn.ensemble`. It defines `base_models` and `meta_model` but then doesn't combine them.
3.  **Bayesian Optimization Scope:**
    *   **Ground Truth:** Conducts Optuna tuning for `XGBoost` using a train-validation split *before* nested CV and then *uses* the optimized parameters in the subsequent models.
    *   **Generated:** The `objective` function for Optuna is for `XGBClassifier`, but the optimized parameters (`best_params`) are obtained but *not used* in any subsequent model training during the nested CV or final evaluation. This makes the optimization step ineffective.
4.  **Nested Cross-Validation (`KFold` vs. `StratifiedKFold`):**
    *   **Ground Truth:** Uses `StratifiedKFold` for both outer and inner loops, which is crucial for imbalanced datasets like diabetes to ensure class distribution is preserved.
    *   **Generated:** Uses `KFold`, which doesn't guarantee stratification and could lead to biased folds.
5.  **Nested Cross-Validation Logic & Evaluation:**
    *   **Ground Truth:** Iterates through outer folds, trains *all* models/ensembles *within* each fold, records metrics for *each* model/ensemble, and then averages them at the end. This provides an unbiased evaluation for all implemented methods.
    *   **Generated:** The nested CV loop only trains and evaluates the `voting_clf` *within the inner loop* for hyperparameter selection purpose, but the main model training and evaluation are done *outside* the nested CV loop on a single `train_test_split`. This means the reported `final_predictions` and `ROC-AUC Score` are from a single split, not a truly unbiased nested CV evaluation. The ""nested_scores"" only evaluate the `voting_clf` in an inner loop context and thus don't represent the overall performance of all models.
6.  **Custom Ensemble - Dynamic Weights:**
    *   **Ground Truth:** Implements a dynamic weighting based on validation `AUC` *within* a fold, then applies these weights to the test set predictions.
    *   **Generated:** The `DynamicWeightedEnsemble` uses `cross_val_score` on `X, y` to calculate weights. This means the weights are calculated *before* any train-test splitting and are inherently optimistic for the models used. The `fit` method then retrains the models on the *entire* `X, y` (or X_train based on execution order). It's not truly representing a proper out-of-fold weighting in a robust CV setup.
7.  **Model Interpretation (SHAP):**
    *   **Ground Truth:** Shows different SHAP plot types (bar, beeswarm, dependence, force) for global and local interpretability.
    *   **Generated:** Calculates `shap_values` but only mentions `explainer = shap.TreeExplainer(xgb)` and `shap_values = explainer.shap_values(X_test)`. It doesn't actually **display** any of the SHAP plots or differentiate between global/local interpretation types.
8.  **Model Calibration & Reliability Analysis:**
    *   **Ground Truth:** Performs calibration using `CalibratedClassifierCV` (both isotonic and sigmoid), calculates Brier scores, and explicitly plots reliability diagrams (`calibration_curve`) and probability histograms for calibrated vs. uncalibrated models.
    *   **Generated:** Initializes `CalibratedClassifierCV` and fits it but *doesn't evaluate* the calibrated model's performance (e.g., Brier score) or plot reliability curves.
9.  **Ensemble Diversity Analysis and Model Combination Strategies:**
    *   **Ground Truth:** Explicitly collects OOF predictions from all models across all folds, calculates and visualizes a correlation matrix of these predictions, and trains a meta-learner on the OOF predictions.
    *   **Generated:** Completely missing this section.

The code provides isolated implementations of several concepts but fails to integrate them cohesively into a comprehensive solution as outlined by the task requirements, particularly in the robust nested CV framework, comprehensive feature engineering, and detailed ensemble evaluation and diversity analysis.",claude-3.5-sonnet,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import numpy as np
import pandas as pd
import pymc as pm
import arviz as az
import lifelines
import scipy.stats as stats
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import GPy
from scipy.interpolate import interp1d
from statsmodels.stats.outliers_influence import variance_inflation_factor
import warnings
warnings.filterwarnings('ignore')

# Load and preprocess data
df = pd.read_csv('diab...",3,"The generated code attempts to address many requirements but falls short in implementation correctness and completeness for several key components.

**Primary reasons for failure:**

1.  **PyMC3 vs. PyMC4 (PyMC):** The ground truth explicitly requested PyMC3 or PyMC4, and the generated code uses `pymc` which implies PyMC4 (or the latest PyMC). While the prompt mentions ""PyMC3 or PyMC4"", the PyMC4 implementation in the generated code is largely incorrect for the given task and often tries to apply PyMC3 patterns to PyMC4, leading to errors or conceptual mismatches. For example, `pm.sample` returns `arviz.InferenceData` in PyMC4, but the calls to `pm.compute_waic()` and `pm.compute_loo()` are incorrect syntax for PyMC4 (they should be called on the `InferenceData` object via ArviZ like `az.waic` or `az.loo`). The `pm.model.PyMC3DeprecationWarning` suppression in the ground truth implies a PyMC3 environment, which was a strong hint. If PyMC4 was chosen, the implementation needed to be natively PyMC4 compliant.

2.  **Hierarchical Model Implementation:**
    *   The `hierarchical_model` in the generated code attempts to define `alpha` with `shape=len(X_train)`, making it a varying intercept per *data point*, not per *group*. A hierarchical model requires a defined grouping variable (e.g., `Age_Group` as in the ground truth) to properly model group-level variability. The dataset does not have an inherent group variable, which the ground truth correctly addressed by creating one. The generated code entirely misses this crucial aspect.
    *   `pm.math.sigmoid` is not needed in PyMC4's `Bernoulli` likelihood when `p` is directly provided; `pm.invlogit` (or letting `pm.Bernoulli` handle `logits` argument) is preferred, but even then, the setup is incorrect for a proper hierarchical logistic regression.

3.  **Missing Value Handling:** The generated code uses `df.fillna(df.mean())`, which is a simplistic approach. The ground truth correctly identifies specific columns where 0s represent missing values (a common issue in this dataset) and imputes them with the median, then scales the data. The generated code's mean imputation for specific columns and lack of 0-to-NaN conversion is inadequate.

4.  **Mixed-Effects Model:** The implementation `u = pm.Normal('u', mu=0, sigma=1, shape=len(np.unique(y_train)))` where `y_train` is the binary outcome (0 or 1) means `u` will only have two ""random effects"" (one for 0 and one for 1), which makes no sense as a mixed-effects model for individual variability. A mixed-effects model inherently requires a grouping variable (e.g., patient ID, hospital, or age group) that isn't present in the dataset and needs to be simulated or identified, similar to the hierarchical model. The `u[y_train.astype(int)]` indexing is also problematic.

5.  **Survival Analysis:**
    *   The simulation of `time_to_onset` and `censored` is rudimentary (`np.random.exponential`, `np.random.binomial`) and doesn't explicitly link to the `Outcome` variable (diabetes presence) in a meaningful way, which the ground truth does by correlating time to risk.
    *   It only performs Kaplan-Meier (which is descriptive) but does not implement a Cox Proportional Hazards model or a Bayesian survival model, missing the ""techniques"" part of the requirement.

6.  **Gaussian Processes:** The generated code uses the separate `GPy` library, whereas the prompt implies using a PyMC3/PyMC4 GP module (as the ground truth correctly demonstrates). The GPy implementation is also minimal, just fitting a model without showing how it's used for actual non-parametric modeling of *relationships* or for inference, or how it integrates with the rest of the Bayesian framework.

7.  **Causal Inference:**
    *   `propensity_score_matching` attempts to use `pm.logistic.Logistic()`, which is not a function/class for fitting a logistic regression directly for propensity scores in PyMC or ArviZ. This section is completely broken.
    *   No instrumental variables method is implemented.

8.  **Model Comparison:** The calls `pm.compute_waic()` and `pm.compute_loo()` are incorrect syntax for PyMC4 (or PyMC3). These operations are typically performed using ArviZ functions (`az.waic`, `az.loo`) on the `InferenceData` object returned by `pm.sample`.

9.  **Completeness & Correctness Issues:** Many sections are either implemented incorrectly (e.g., PyMC syntax), conceptually flawed (hierarchical/mixed effects without proper grouping), or completely missing (instrumental variables, proper causal inference). Imports (`pymc` instead of `pymc3` or `pymc4` explicitly, `GPy` instead of a PyMC GP), and general structure are quite messy, with code blocks mixed in with general setup and then individual model attempts.

In summary, while the code identifies the categories, its actual implementation frequently misunderstands the core concepts of Bayesian hierarchical/mixed models, the specific PyMC syntax for complex models, and the proper statistical methods required for survival analysis and causal inference beyond simple function calls.",claude-3.5-sonnet,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering, OPTICS
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.ensemble import IsolationForest
from tensorflow.keras.layers import Input, Dense
...",5,"The generated code attempts to address many aspects of the prompt but falls short in several critical areas, lacking the depth and completeness required for ""advanced"" analysis.

**Correctness:**
*   **Dataset:** The generated code assumes 'Weight' and 'Height' columns are present for BMI calculation, which are NOT in `diabetes.csv`. This immediately breaks the code and prevents it from running successfully with the given dataset. This is a fundamental error.
*   **Feature Engineering:** The `MetabolicSyndrome` calculation is oversimplified (e.g., BloodPressure > 130 is systolic, but the actual column is general BP). It also uses a very basic definition and doesn't align with the more robust approach in the ground truth.
*   **Clustering Parameter Optimization:** The generated code uses hardcoded values (e.g., `n_clusters=4` for KMeans, GMM, Hierarchical, Spectral), completely missing the explicit requirement to ""Determine optimal clustering parameters using silhouette analysis, gap statistic, and elbow method with statistical validation."" This is a major omission.
*   **Consensus Clustering:** While it attempts a consensus matrix, it then uses `hdbscan.HDBSCAN` on this matrix. `HDBSCAN` is a density-based clustering algorithm. Applying it directly to a co-occurrence/similarity matrix (which represents patient-patient similarity, not density in feature space) is conceptually incorrect for traditional consensus clustering. The ground truth correctly uses hierarchical clustering on the similarity-derived distance matrix.
*   The generated `final_clusters` might not be directly comparable to the input `cluster_results` if `HDBSCAN` identifies a different number of clusters or handles noise differently, which would affect subsequent steps like profiling.
*   **Anomaly Detection:** Only Isolation Forest is implemented, missing One-Class SVM and LOF.
*   **Time-Series Clustering:** This crucial part is completely absent. The prompt explicitly asks for conceptual outlining if data doesn't support it, which the generated code fails to do.
*   **Statistical Significance Testing for Phenotypes:** The code only includes `ttest_ind` and `chi2_contingency` in imports but doesn't implement comprehensive statistical testing for feature differences across clusters. It doesn't mention or show how to interpret the significance for continuous vs. categorical features.
*   **Clinical Interpretation:** The risk stratification system is very basic, and the required ""clinical interpretation"" is missing.

**Code Quality:**
*   **Imports:** Some imports are present but not used (e.g., `MinMaxScaler`, `PCA`, `calinski_harabasz_score`, `ttest_ind`, `chi2_contingency`, `multipletests`, `seaborn`). Conversely, `SimpleImputer` (crucial for dealing with 0s) is missing.
*   **Hardcoding vs. Configuration:** The hardcoded number of clusters is a significant quality issue given the requirement for optimization.
*   **Clarity:** The `calculate_risk_score` function, while an attempt, uses arbitrary weights and divisions (e.g., `Glucose / 200`, `BMI / 40`, `Age / 100`) without clear clinical justification or normalization based on feature ranges. This makes the risk score less interpretable.
*   **Modularity:** Functions are used, but the overall flow can be clearer.

**Completeness:**
*   **Advanced Feature Engineering:** The prompt requested ""advanced feature engineering including metabolic syndrome indicators."" The generated code's definition of metabolic syndrome is too simplistic and erroneous given the dataset. It misses the strategy to handle 0s as missing values.
*   **Optimal Clustering Parameters:** This is mostly missing (only hardcoded k values).
*   **Gap Statistic and Elbow Method with Statistical Validation:** Not implemented.
*   **DBSCAN/OPTICS Parameter Determination:** No attempt to estimate `eps` for DBSCAN or `min_samples` for OPTICS in a principled way.
*   **Anomalies:** Only one algorithm used.
*   **Time-Series Clustering:** Completely missing.
*   **Association Rule Mining Details:** While `apriori` and `association_rules` imported, the discretization of continuous features for ARM is very basic, and the resulting `transactions` DataFrame only includes `AgeGroup` (binned), `Outcome`, and `MetabolicSyndrome`, missing other numeric features vital for diabetes analysis (e.g., Glucose, BMI).
*   The prompt asked for ""detailed patient phenotype profiles for each cluster with statistical significance testing"". The generated solution provides basic means but lacks proper statistical validation comparing clusters.
*   ""Clinical interpretation"" for risk stratification is largely absent beyond assigning labels.

Overall, the generated code provides a skeleton with many missing or incorrectly implemented components necessary to fulfill the ""advanced"" nature and specific requirements of the prompt. The inability to run due to the `Weight`/`Height` assumption is a critical flaw.",claude-3.5-sonnet,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor
from sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBRegressor
from lightgbm i...",3,"The generated code attempts to address several aspects of the prompt but falls significantly short in terms of correctness, completeness, and advanced implementation details.

**Key Failures and Missing/Incorrect Implementations:**

1.  **Comprehensive Preprocessing & Feature Engineering:**
    *   **Preprocessing:** The code completely ignores categorical features (`Regional indicator`) and numerical features that might need imputation (`Standard error of ladder score`, etc.). It only applies `RobustScaler` to all columns indiscriminately, which is insufficient. The ground truth uses `ColumnTransformer` with `SimpleImputer` and `OneHotEncoder` for proper handling of mixed data types.
    *   **Feature Engineering:** While it adds three engineered features, the prompt asked for ""advanced feature engineering,"" which typically includes handling dates, interactions, polynomial features etc., beyond simple multiplication/division, and most importantly, it should be part of the preprocessing pipeline for consistent application during cross-validation. The current implementation is rudimentary.

2.  **Multiple Ensemble Methods:**
    *   **Voting Regressor:** It only implements a single `VotingRegressor` with three models, without explicitly showing ""hard"" (simple average) and ""soft"" (weighted average) voting as specified in the ground truth.
    *   **Bagging:** Entirely missing. The prompt explicitly asked for ""Bagging with different base estimators.""
    *   **Boosting:** While `XGBoost`, `LightGBM`, `CatBoost`, `GradientBoosting` are present, they are only used as base models; the prompt implies demonstrating them as standalone boosting algorithms.
    *   **Bagging:** Missing.
    *   **AdaBoost:** Used as a base model, but not demonstrated as a standalone boosting algorithm within a pipeline.

3.  **Stacking Ensemble:**
    *   Completely missing. The prompt explicitly asked for creating a stacking ensemble with base learners and a meta-learner.

4.  **Bayesian Optimization with Optuna:**
    *   The `objective` function for Optuna is implemented, but it only optimizes `RandomForestRegressor` hyperparameters. The prompt asked for ""Bayesian optimization for hyperparameter tuning using Optuna *or Hyperopt*"". The ground truth demonstrates it for XGBoost, which is more common and complex.
    *   It also calls `cross_val_score` directly on the `X_train`, `y_train` without preprocessing steps, which is incorrect if those steps are part of the full pipeline (as they should be). The ground truth correctly integrates the preprocessor into the model pipeline for Optuna.

5.  **Nested Cross-Validation:**
    *   The code does *not* implement nested cross-validation. It performs a single `train_test_split` and then fits models on `X_train`, evaluating on `X_test`. Nested CV involves an outer loop for robust evaluation and an inner loop for hyperparameter tuning. This is a critical omission. The ground truth correctly demonstrates the structure of nested CV.

6.  **Custom Ensemble with Dynamic Weight Assignment:**
    *   The `DynamicWeightedEnsemble` class attempts this. However, its `fit` method trains each model on the *entire* `X`, `y` given to it, then calculates weights based on error on that *same* training data. This is a severe form of data leakage. Dynamic weighting should be based on *out-of-fold* predictions to get a truly unbiased error estimate for weight calculation. The ground truth correctly uses `cross_val_predict` to generate OOF predictions for this purpose.

7.  **Advanced Techniques: Blending and Multi-level Stacking:**
    *   Completely missing.

8.  **Model Interpretation (SHAP and LIME):**
    *   **Initialization:** SHAP `TreeExplainer` is used but on `optimized_rf` which might not be the most representative model from the ensemble context.
    *   **Data for SHAP:** `shap_values = explainer.shap_values(X_test)` is called directly on `X_test` without preprocessing/scaling. If `X_test` is the raw data and the model expects scaled data, this will lead to incorrect or error-prone results. The ground truth correctly transforms the data before passing it to SHAP explainer.
    *   **Visualization:** SHAP plots are imported but not actually generated or shown, which is part of the ""implement"" requirement.
    *   **LIME:** `lime.lime_tabular.LimeTabularExplainer` is correctly initialized, but `predict_fn` is missing for how the LIME explainer would call the model. Also, the explanation (`explanation.show_in_notebook()`) is not actually displayed in the provided script.

9.  **Model Calibration and Uncertainty Quantification:**
    *   Completely missing.

10. **Ensemble Diversity Analysis and Model Combination Optimization:**
    *   Completely missing.

**Code Quality:**

*   **Imports:** Mostly correct, but `ColumnTransformer`, `SimpleImputer`, `Pipeline`, `StackingRegressor`, `CalibratedRegressorCV` from `sklearn` (essential for the task) are missing.
*   **Structure:** The `main` function is good, but the class `DynamicWeightedEnsemble` has a critical flaw in its `fit` method as discussed.
*   **Robustness:** No error handling for file not found.
*   **Logging/Prints:** Minimal output, making it hard to follow intermediate steps of execution or validation.

Overall, the generated code only superficially touches upon some requirements and fails to implement the ""advanced"" aspects or even the basic robust methodologies like proper data pipelines and nested CV. The critical data leakage in the dynamic weighted ensemble and the absence of proper preprocessing within pipelines are significant flaws.",claude-3.5-sonnet,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc4 as pm
import tensorflow as tf
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.matching import PropensityScoreMatcher
import causalinference as ci
import linearmodels as lm
import seaborn as sns
import matplotlib.pyplot as plt

# Load and preprocess data
df = pd.read_csv('world-happiness-report-2021.csv')

# Missing value handling
num_impute...",2,"The generated code attempts to address all parts of the prompt but fails significantly in execution, correctness, and completeness for a data science benchmark.

**Major Failures and Critical Issues:**

1.  **PyMC3/PyMC4 Usage:** The prompt explicitly asked for PyMC3 or PyMC4. The generated code imports `pymc4 as pm` but then attempts to use `pm.Model`, `pm.Normal`, `pm.HalfNormal`, `pm.sample`, `pm.waic`, `pm.loo` etc., all of which are PyMC3 syntax. PyMC4 uses a TensorFlow Probability backend and has a very different API (`tfd.Normal`, `tfd.HalfNormal`, `tfp.experimental.mcmc.sample_chain`). This fundamental misunderstanding of the specified PyMC library makes much of the Bayesian modeling code non-functional or conceptually incorrect for PyMC4. The provided PyMC4 code would likely throw numerous errors and not run.

2.  **Missing Data Handling:**
    *   The `KNNImputer` is applied only to `select_dtypes(include=['float64', 'int64'])`, completely ignoring categorical columns and potentially the target variable if it's not a float/int.
    *   The original `df` is not updated with imputed values; `df_imputed` is created, and then `df_scaled` is created from `df_imputed`, but the rest of the code still refers to variables using their original column names, which would not exist in the scaled data or would refer to the un-imputed, unscaled `df`. This creates data inconsistencies.
    *   No handling for the `Regional_indicator` or `Country_name` columns which are essential for the hierarchical model and later analysis.

3.  **Data Inconsistency & Column Names:** The prompt states `Logged_GDP_per_capita` and `Ladder_score`. The generated code assumes `Log GDP per capita` and `Life Ladder`. This would lead to `KeyError` unless the CSV columns happen to be exactly those, which they are not based on the prompt's context and the file name given (`world-happiness-report-2021.csv`). The ground truth specifically handles this by renaming columns.

4.  **Hierarchical Model:**
    *   It uses `df['Region'].unique()` for `shape` but then `df['Region'].codes` for indexing. `df['Region']` does not exist in the processed dataframe, and `.codes` would only work if it was a CategoricalDtype, which it isn't made into.
    *   The `mu` calculation uses `df['Log GDP per capita']` and `df['Social support']` directly from the original `df`, not the `df_scaled` that was produced, leading to errors or incorrect scaling.
    *   No informative priors for the hierarchical model as requested by the prompt (it uses `mu=0, sd=1` which are standard non-informative defaults).

5.  **Structural Equation Model (SEM):**
    *   Uses `statsmodels.api.SEM` which is a much simpler SEM functionality compared to dedicated SEM packages (e.g., `lavaan` in R, `statsmodels.formula.api` for specific paths). It also tries to define measurement models (`social =~ ...`, `economic =~ ...`) using variables like `social_support`, `freedom`, `generosity`, `gdp`, `healthy_life` which likely don't directly map to factors it defines or the scaled dataframe.
    *   More importantly, building SEM using PyMC3/PyMC4 was explicitly requested as a stronger implementation of SEM, which the generated code failed to do beyond simple regressions for mediation.

6.  **Causal Inference - IV:**
    *   The instrument is arbitrary (`latitude`) and not related to the dataset. The prompt emphasized *simulating* a plausible instrument if one doesn't exist, and the ground truth demonstrated how to do this.
    *   It uses `Predicted GDP` directly in the second stage, which implies 2SLS, but the `statsmodels` setup for 2SLS (using `IV2SLS` or building the structure correctly using `formula.api` with instrumental syntax) is not correctly implemented.

7.  **Propensity Score Matching (PSM):**
    *   Imports `sklearn.matching.PropensityScoreMatcher` which does not exist in `scikit-learn`.
    *   No implementation of matching/weighting and calculating the ATE.

8.  **Bayesian Regression (Informative Priors):**
    *   The priors are used, but again, the `df` variables are used, not the `df_scaled`, leading to incorrect scaling.

9.  **Gaussian Processes (GP):**
    *   Uses `pm.gp.Marginal` but the `pm.gp` namespace is specific to PyMC3. PyMC4 GPs are handled differently via a TensorFlow Probability GP library.
    *   The likelihood needs `X` and `y` to be passed as arguments, but it refers to `df[['Log GDP per capita']].values` and `df['Life Ladder'].values` which are unscaled.

10. **Mediation Analysis:**
    *   Uses `statsmodels.OLS` for paths, which is okay for traditional frequentist mediation but does not leverage Bayesian methods as implied by the overall task request (which asked for PyMC3/4).
    *   Does not calculate standard indirect, direct, and total effects with credible intervals using a Bayesian approach (e.g., using posterior samples).

11. **Difference-in-Differences (DiD):**
    *   The simulation of `post` and `treated` is entirely random (`np.random.binomial`) and is not conceptually sound for simulating a panel structure or treatment based on actual data characteristics, as the ground truth did. It does not create a pre/post structure or a truly treated vs. control group based on the data. It just adds two random binary columns.

12. **Model Comparison:**
    *   As with other PyMC functions, `pm.waic` and `pm.loo` are PyMC3 methods and would not work directly with a PyMC4 trace (if `pm.sample` had even worked for PyMC4).

13. **Posterior Predictive Checks:**
    *   Again, `pm.sample_posterior_predictive` is PyMC3 syntax.

14. **Sensitivity Analysis:**
    *   The sensitivity analysis is hardcoded to `beta` and `Log GDP per capita` only, not a general approach.
    *   It does not present a comparison of the results of the sensitivity analysis.

Overall, the code appears to be a direct translation of task requirements into function names and some imports, but the internal logic and implementation details are severely lacking or incorrect, particularly regarding the chosen PyMC library (`PyMC4` vs. `PyMC3` syntax) and consistent data handling. It would not execute as intended and would require substantial debugging and correction to achieve any of the stated goals.",claude-3.5-sonnet,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.manifold import TSNE, LocallyLinearEmbedding, Isomap
import umap
import hdbscan
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
im...",5,"The generated code attempts to address several requirements but falls short in correctness, completeness, and structure.

**Correctness:**
*   **Feature Engineering:** The `engineer_features` function creates some new features, but they are not advanced or particularly meaningful (e.g., `Health_Index = (Healthy life expectancy + Perceptions of corruption)/2`). The original task asked for ""advanced feature engineering including happiness profiles and regional characteristics,"" which would imply more sophisticated transformations and interactions or the incorporation of regional DUMMY variables.
*   **Scaling:** The `scale_features` function is implemented, but the subsequent dimensionality reduction and clustering are applied to `X`, which is `df_scaled` (the scaled version of *original* features plus the newly engineered ones), not the full processed dataframe (`df`). The original `df` is still passed to `analyze_clusters`. This mix-up in data frames makes the data flow incorrect.
*   **Dimensionality Reduction:** `LocallyLinearEmbedding` is included but `Isomap` (Manifold Learning) and autoencoders are implemented differently in the ground truth (autoencoders are present but their structure and objective could be more robustly defined). The task specifically asked for ""manifold learning"", and `LLE` covers that, but it misses the specific mention of `Isomap` from the ground truth. Crucially, the code performs dimensionality reduction but does not use these reduced dimensions for *visualization* in conjunction with clusters, as requested.
*   **Optimal Clustering Parameters:** The task explicitly asks for using silhouette analysis, gap statistic, elbow method, and stability analysis for *optimal parameter determination*. The generated code simply picks `n_clusters=5` for most algorithms without any methodology for determining optimality. `silhouette_score` and `calinski_harabasz_score` are imported but not used to determine optimal k.
*   **Consensus Clustering:** The `consensus_matrix` calculation is flawed. `AgglomerativeClustering` on `consensus` matrix directly is not a standard way to derive consensus clusters; typically, a distance metric (1 - similarity) is used, and then hierarchical clustering is applied to *that* distance matrix, or a different consensus algorithm (like one based on voting) is used.
*   **Anomaly Detection:** Only `IsolationForest` and `LocalOutlierFactor` are implemented. `OneClassSVM` was in the ground truth and is a common technique for this purpose, which is missing.
*   **Network Analysis:** `pdist` and `squareform` are used to create a similarity matrix, but the entire `X` (scaled features) is used to create the network. The ground truth, correctly, built the network based on a similarity *threshold* to avoid highly dense, unusable graphs and then analyzed centrality measures. The generated code creates a completely dense graph (all nodes connected) and doesn't define what ""similarity"" means for edge weights; it just converts all distances directly into a graph without filtering. The task also asked for ""relationships between countries based on happiness factors,"" implying names of countries and specific factors as nodes/edges, not just numerical indices.
*   **Association Rule Mining:** `discretize_features` attempts to discretize but is not applied to the correct dataframe (`df_arm` in ground truth for *original* features, not `X` which contains scaled values). Also, the ground truth explicitly states which features from the original dataset are used for ARM, ensuring meaningful interpretation.
*   **Cluster Profiling:** `analyze_clusters` just returns mean, std, median. It doesn't include ""statistical significance testing and effect size analysis"" using ANOVA, which was a specific requirement.
*   **Recommendation System/Policy Insights:** The `recommend_countries` function is rudimentary (just finding countries in the same cluster). The policy insights are completely missing; there's no comparison of high vs. low happiness clusters or geopolitical validation.

**Code Quality:**
*   **Imports:** Imports are generally correct for the functionalities attempted.
*   **Structure:** The code is structured as a script, not a class, which was implied by the task's complexity and the ground truth's structure. A class-based structure would encapsulate state and methods better, leading to more organized and reusable code.
*   **Readability/Comments:** Lacks detailed comments and explanations, especially for complex operations like consensus clustering or autoencoder setup.

**Completeness:**
*   **Feature Engineering:** Missing regional characteristics (one-hot encoding of region).
*   **Clustering:** Optimal parameter determination missing.
*   **Dimensionality Reduction:** No visualization with cluster labels. Autoencoder implementation is basic (relu for bottleneck, sigmoid for output with raw input `X` not scaled to `[0,1]`). Missing `Isomap`.
*   **Optimal Parameter Determination:** The most significant omission. The code hardcodes `n_clusters=5` without justification.
*   **Consensus Clustering:** Implementation is incorrect and doesn't follow standard methods or aim to find an optimal number of consensus clusters.
*   **Detailed Country Profiles:** Statistical significance testing and effect size analysis are completely missing.
*   **Anomaly Detection:** Missing `OneClassSVM`.
*   **Network Analysis:** No thresholding or filtering of edges, making the network graphs likely uninterpretable for ""understanding relationships."" Lacks centrality measures analysis.
*   **Association Rule Mining:** Discretization is applied to scaled data, which makes interpretation difficult. Does not mention specific features to discretize.
*   **Recommendation System and Policy Insights:** Very minimal recommendation system. Policy insights, geopolitical validation, and comparison of high/low happiness clusters are largely absent.

Overall, the generated code provides a skeleton with many of the required components but fails to correctly or comprehensively implement the advanced aspects and analytical depth requested in the prompt.",claude-3.5-sonnet,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from pmdarima import auto_arima
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Load and prepare data
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])
df = df[df['location'] == 'Germany'].set_index('date')

# Prepare target and exogenous variables
target = df['new_cases']
exog_vars = df[['stringency_index', 'people_fully_v...",8,"The generated code correctly implements the core requirements of the task: data loading and preparation, filtering for Germany, handling missing values (though not as robustly as the ground truth), data splitting, auto_arima for order selection, SARIMA model training with exogenous variables, forecasting, RMSE calculation, and plotting.

Strong points:
- Correct use of `pmdarima` for `auto_arima` and `statsmodels.tsa.statespace.sarimax` for SARIMA.
- Proper handling of target and exogenous variables throughout the process.
- Correct calculation and display of RMSE.
- Clear and concise plotting of results.
- `stepwise=True` and `m=7` are correctly used in `auto_arima`.

Areas for improvement to reach a higher score:
1.  **Missing Value Handling:** The generated code uses `ffill` for `target` and `interpolate` for `exog_vars`. While these are valid methods, they are not as robust as the ground truth's combined `ffill`, `bfill`, and then `interpolate` fallback for `exog_vars` (or the combined `data` dataframe approach). The `target.fillna(method='ffill')` might not handle leading NaNs if they exist. The ground truth's approach is more comprehensive for time series data.
2.  **Error Handling:** The generated code lacks explicit error handling for file not found or insufficient data, which is present in the ground truth. This makes the code less robust in real-world scenarios.
3.  **Negative Forecasts:** The generated code does not explicitly handle negative forecasts for 'new_cases', which are illogical. The ground truth includes `forecast_values[forecast_values < 0] = 0`.
4.  **Clarity in Plots:** While the plot is decent, adding a title that includes the RMSE value would be a nice touch, as done in the ground truth. Also, plotting the training data alongside actual and forecasted test data provides a more complete picture of the model's performance context.
5.  **Warnings Suppression:** While `auto_arima` has `suppress_warnings=True`, there isn't a general warning suppression `warnings.filterwarnings(""ignore"")` at the beginning of the script, which can be useful for noisy outputs from `statsmodels`.
6.  **`results.forecast().predicted_mean` vs `results.forecast()`:** The ground truth correctly uses `forecast_obj = sarima_results.get_forecast(steps=forecast_steps, exog=X_test); forecast_values = forecast_obj.predicted_mean`. The generated code directly calls `results.forecast(steps=30, exog=test_exog)`, which technically works but `get_forecast` explicitly returns a `PredictionResultsWrapper` object which then has `predicted_mean`, `se_mean`, `conf_int`, etc. It's often good practice to use `predicted_mean` for clarity.
7.  **`enforce_stationarity` and `enforce_invertibility` in SARIMAX:** The ground truth explicitly sets `enforce_stationarity=False` and `enforce_invertibility=False` in `SARIMAX` initialization, which is often crucial when `auto_arima` determines `d` or `D` to be greater than 0, preventing potential convergence issues. The generated code omits these.",claude-3.5-sonnet,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scikit_posthocs import posthoc_dunn
import matplotlib.pyplot as plt
import seaborn as sns

# Read data
df = pd.read_csv('winequality-red.csv', sep=';')

# Define quality groups
df['quality_category'] = pd.cut(df['quality'], 
                              bins=[-np.inf, 4, 6, np.inf],
                              labels=['Low', 'Medium', 'High'])

# Function to ...",6,"The generated code correctly categorizes quality groups and attempts to apply appropriate statistical tests and visualizations. However, there are several key issues and areas for improvement:

1.  **Issue with `check_normality`**: The `check_normality` function is flawed. It implicitly assumes that if *any* single group for a given feature fails the Shapiro-Wilk test (p < 0.05), then the *entire feature* is non-normal, leading to Kruskal-Wallis. For ANOVA, all groups need to be normal. More importantly, it doesn't account for homogeneity of variances (Levene's test), which is a crucial assumption for ANOVA (or the use of Welch's ANOVA if variances are unequal but data is normal). The ground truth correctly addresses this by checking for normality in *all* groups and then checking for homogeneity of variance.
2.  **`scikit_posthocs` for Dunn's Test**: While `scikit_posthocs` is a valid library for Dunn's test, it introduces an external dependency not explicitly listed in the prompt's `scipy.stats` or `statsmodels` requirements. The ground truth provides a `scipy.stats` based alternative using pairwise Mann-Whitney U with Bonferroni correction, which is a common and acceptable non-parametric post-hoc for Kruskal-Wallis when Dunn's test is not directly available via core libraries.
3.  **Handling of Missing Values (`dropna`)**: The code doesn't explicitly handle missing values before performing statistical tests. While `pandas.cut` might handle NaNs gracefully for categorization, the statistical functions `shapiro`, `f_oneway`, and `kruskal` can be sensitive to NaNs or empty arrays if a group happens to be empty due to filtering or missing data. The ground truth explicitly uses `.dropna()` when preparing data for tests and includes checks for sufficient data points in groups.
4.  **Data Loading (`sep=';'`)**: The prompt states ""assume the CSV file is in the current directory"" and typically `winequality-red.csv` uses a semicolon as a separator. The generated code correctly infers `sep=';'`, which is good, but it's an implicit assumption not 100% derivable from the prompt text, whereas the ground truth defaults to comma and could be problematic if the file indeed used semicolons without modification.
5.  **Completeness of Output/Summary**: The summary printing for statistical tests is very brief (`Significant differences found...`) and does not explicitly show which *pairs* are significant in the post-hoc results in a readable format for Dunn's test (it prints the full DataFrame, which isn't as concise as expected for a summary). For Tukey, it prints the full `pairwise_tukeyhsd` result object, which is good. The prompt asked to ""Summarize which features significantly differ between groups"", and while it identifies the features, the detail for the post-hoc results is less user-friendly for Dunn's.
6.  **Box Plot Subplots**: The logic for creating subplots `plt.figure(figsize=(15, 5*((len(significant_features)+2)//3)))` and `plt.subplot((len(significant_features)+2)//3, 3, i+1)` is good for dynamic subplot generation based on the number of significant features.
7.  **Minor**: The `quality_category` labels (`Low`, `Medium`, `High`) could ideally be ordered as a categorical type for consistent plotting order, similar to the ground truth, though `pd.cut` might maintain order implicitly if the bins are ordered.

To improve, the generated code should:
*   Implement a robust normality check across all groups and incorporate Levene's test for homogeneity of variance before deciding between ANOVA/Kruskal-Wallis.
*   Explicitly handle potential `ValueError`s or insufficient data for statistical tests (e.g., groups with fewer than 3 samples for Shapiro-Wilk or 2 for pairwise comparisons).
*   Adjust the post-hoc summary for Dunn's test to be more concise and highlight significant pairs.
*   Ensure the use of standard libraries or clearly noted external dependencies.",claude-3.5-sonnet,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score, f1_score
import xgboost as xgb
import shap

# Load and prepare data
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

# Create target variable
def create_target(group):
    mask = g...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being a fully correct and robust solution.

**Positive Aspects:**
*   Correctly loads and parses the date data.
*   Attempts to create the target variable based on the 90th percentile of `new_deaths > 0` for each location.
*   Includes most of the requested feature engineering: rolling averages/stds, lag features, and days since first case.
*   Handles missing values using `fillna(0)` which aligns with the prompt's suggestion.
*   Selects relevant features for modeling.
*   Performs time-based train-test splitting.
*   Uses `ColumnTransformer` for preprocessing (scaling and one-hot encoding).
*   Trains an XGBoost classifier and uses `GridSearchCV` for hyperparameter tuning.
*   Evaluates with ROC-AUC and F1-score.
*   Attempts to use SHAP for feature importance.

**Areas for Improvement / Issues:**

1.  **Target Variable Definition (Critical Bug):** The `create_target` function is incorrect.
    *   `df.groupby('location').apply(create_target).reset_index(level=0, drop=True)` will return a Series where the index aligns with the original DataFrame. However, the logic *within* `create_target` is flawed.
    *   `threshold = group.loc[mask, 'new_deaths'].quantile(0.9)` is correct for calculating the threshold.
    *   The problem lies in `(group['new_deaths'] > threshold).astype(int)`. This comparison is made against *all* `new_deaths` in the group, not just the `new_deaths > 0` that were used to calculate the threshold. The prompt explicitly states: ""A 'high_daily_death_increase' is defined as a day where 'new_deaths' for a 'location' is in the top 10% of its own historical 'new_deaths' *(excluding days with zero new deaths)*"". So, days with `new_deaths=0` should *not* be marked as `high_daily_death_increase`, even if the threshold is met. The current implementation might mark a day with `new_deaths=0` as 1 if the threshold is negative or zero, which is not the intent. The ground truth correctly handles this by ensuring `new_deaths > 0` for the target.

2.  **Feature Engineering - Rolling Operations (Correctness):**
    *   `df.groupby('location')[col].rolling(7).mean().reset_index(0, drop=True)`: `reset_index(0, drop=True)` will flatten the MultiIndex created by `groupby().rolling()`, but it **loses the original DataFrame's index alignment if there are gaps in locations or dates**. The correct way to apply rolling operations within groups and maintain original DataFrame structure is to use `transform()` or `apply()` and ensure the index is preserved, as shown in the ground truth. `transform()` is generally more efficient for this.
    *   This issue could lead to misaligned features if not handled carefully, though in simple cases it might appear to work.

3.  **Feature Engineering - Days Since First Case (Robustness/Correctness):**
    *   The current implementation `df.apply(lambda x: (x['date'] - first_case[x['location']]).days if x['location'] in first_case else 0, axis=1)` is very inefficient for large datasets due to row-wise `apply`.
    *   It also assumes `first_case[x['location']]` will always exist, and sets `0` if `x['location']` is not in `first_case`, which could happen if a location has no `new_cases > 0`. This is not strictly correct; `days_since_first_case` should be relative to the *first date data is available* for that location, or more precisely, the first date a case was recorded for that location (as the prompt asks `since_first_case`). The ground truth calculates `(x - x.min()).dt.days` within a `groupby('location').transform()`, which is much more robust and efficient.

4.  **Missing Value Handling (Completeness):** The prompt explicitly suggests handling all numerical NaNs (likely from feature engineering) and mentions ""fill with 0 or backfill"". While `fillna(0)` is used, it's applied only to a pre-defined `numeric_cols` list. The ground truth dynamically identifies *all* engineered columns and then handles NaNs in *all* numerical columns to be safe, including potential `population` or other base data NaNs.
    *   The `vaccination_rate` calculation (`df['people_fully_vaccinated'] / df['population']`) correctly handles potential division by zero *implicitly* due to how `fillna(0)` and `replace([np.inf, -np.inf], 0)` work, but it's not explicitly done during the calculation, which can lead to `inf` values initially. The ground truth is more explicit about handling `inf` values.

5.  **SHAP Feature Names (Completeness/Correctness):**
    *   The SHAP explanation part is problematic. `X_test` after `preprocessor.transform` is a NumPy array.
    *   `model.best_estimator_` is the `GridSearchCV` object itself, not the trained XGBoost classifier. `shap.TreeExplainer` needs the actual XGBoost model. It should be `model.best_estimator_.best_params_` for a direct XGBoost model, or if the `GridSearchCV` structure is retained, `model.best_estimator_` might *contain* the best model if it's directly an XGBoost object, but typically after `GridSearchCV`, `best_estimator_` refers to the best *pipeline* or *model* depending on what was passed to `GridSearchCV`. In this case, `xgb.XGBClassifier` was passed directly, so `model.best_estimator_` *is* the best XGBoost instance. This is okay.
    *   However, the `feature_names` list for SHAP is constructed manually using `preprocessor.named_transformers_['cat'].categories_[0][1:]`. This assumes `drop='first'` was used in `OneHotEncoder`, which it was, but it's brittle. If the number or order of numerical features changes, this list will be incorrect. The ground truth shows a more robust way to get feature names after transformation using `get_feature_names_out()` from the `OneHotEncoder` and by transforming data into a DataFrame with column names for clarity.

6.  **Code Structure and Imports:**
    *   The code uses `xgboost as xgb` which is good.
    *   The definition of functions (`create_target`, `create_features`) is okay but not strictly necessary for this task and can sometimes make the data flow less clear compared to direct pandas operations (as in the ground truth).
    *   Missing `matplotlib.pyplot` import for `shap.summary_plot` if `show=False` and `plt.show()` are to be used explicitly (though SHAP might call `plt.show()` internally for simple cases). The ground truth correctly imports `matplotlib.pyplot`.

7.  **Data Split Date (Potential Issue):** `df['date'].quantile(0.8)` might not give a clean date for splitting, especially if multiple locations share the same date or if data is sparse. A cleaner approach is to sort the entire dataset by date and use an `iloc` split based on a percentage of rows, which guarantees the last X% of *data points* are used for testing, maintaining temporal order. The ground truth uses this more robust approach.

In summary, while the code attempts to fulfill all requirements and gets close on many, the critical bug in target variable definition and the fragility/inefficiency in feature engineering and SHAP feature naming significantly reduce its correctness and robustness.",claude-3.5-sonnet,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from statsmodels.regression.mixed_linear_model import MixedLM
import warnings
warnings.filterwarnings('ignore')

# Load and prepare data
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])
df['vaccination_percentage'] = df['people_fully_vaccinated'] / df['population'] * 100

# Handle missing values by location
cols_to_impute = ['reproduction_rate', 'stringency_index', 'vaccination_percentage']
for col in cols_to_...",6,"The generated code attempts to address all aspects of the task, but it has significant issues with correctness and completeness, particularly regarding the handling of random effects and missing values.

**Correctness Issues:**
1.  **Missing Value Imputation:** The imputation strategy for missing values (`.interpolate(method='linear').fillna(x.median())`) is problematic.
    *   `fillna(x.median())` within a `transform` function for `groupby` will apply the median for *each group* to any remaining NaNs *after interpolation*. This is better than a global median, but it doesn't handle the edge cases well. The ground truth's ffill/bfill for `population` and `people_fully_vaccinated` followed by interpolate and a global median fallback is more robust for time series.
    *   There is no capping of `vaccination_percentage` at 100%, which can lead to unrealistic values if `people_fully_vaccinated` exceeds `population`.
2.  **Random Effects Specification:** This is the most critical error. The task clearly states ""Random effects: random intercepts for 'location' nested within 'continent'.""
    *   The generated code uses `groups=df['location_id']` for the primary grouping variable and `re_formula='1'`, which correctly specifies random intercepts for *each location*.
    *   However, it then adds `vc_formula={'continent': '0 + C(continent_id)'}`. This `vc_formula` is used for *variance components* (typically for random slopes or more complex random structures), not for defining nested random intercepts in the way `MixedLM` expects for `groups`. In `statsmodels.MixedLM`, if you want nesting like 'location within continent', you usually create a combined ID (`continent_location_id` in the ground truth).
    *   The way it's set up effectively models random intercepts for `location` and then attempts to model *additional* random effects variance component for `continent` as a separate factor, which is not the same as random intercepts for `location` *nested within* `continent`. The variance component reporting also doesn't match the expectation (""Location (nested)"" vs. the actual variance structure).
3.  **Interaction Term Centering:** While centering predictors before interaction is a common practice in some fields to reduce multicollinearity and aid interpretation of main effects, the prompt *did not ask for it*. It adds unnecessary complexity and deviates from a straightforward interpretation unless specifically requested or if it's causing numerical instability. The ground truth directly uses the variables and their interaction.
4.  **No `smf.mixedlm`:** The generated code uses `from statsmodels.regression.mixed_linear_model import MixedLM` and manually creates `exog` and `endog`. The prompt implies using `statsmodels.formula.api` (like `smf.mixedlm`) by stating ""Implement a mixed-effects linear regression model"" and ""Fit the model using statsmodels."" While not strictly an error, `smf.mixedlm` simplifies formula-based model definition and interaction term creation, making the code cleaner and less error-prone for variable selection.
5.  **Lack of Robust Error Handling for File Loading:** No `try-except` block for `pd.read_csv`, which is standard good practice.
6.  **Partial Reporting of Fixed Effects:** `print(results.summary().tables[1])` is good, but the detailed interpretation part (which the ground truth provides) is missing.

**Completeness Issues:**
1.  **Interpretation:** The generated code lacks any detailed interpretation of fixed effects, which was explicitly requested (""Interpret the significance and direction of these effects.""). It only prints the summary table.
2.  **Random Effects Reporting:** It reports `np.sqrt(results.cov_re.iloc[0,0])` as ""Location (nested):"" and then `pd.DataFrame(results.cov_re_unscaled)`.
    *   `results.cov_re` refers to the structured covariance between the random effect components. For a simple random intercept (per `location_id`), this would be a 1x1 matrix, and `iloc[0,0]` would be the variance. Reporting `np.sqrt` gives the standard deviation, not the variance (""variance components"" was requested, not standard deviations).
    *   `results.cov_re_unscaled` is the unscaled random effects covariance matrix, often tied to the identity matrix or other structures. The ground truth correctly extracts `random_intercept_variance = model_results.cov_re.iloc[0, 0]` directly for the variance (as asked) and explains its meaning.
3.  **Discussion of Model Assumptions:** The generated code includes commented-out code for plotting, but the *discussion* of assumptions (linearity, normality, homoscedasticity, independence, normality of random effects) and a brief description of how they could be checked (as in the ground truth) is missing from the *executable report*. The prompt asked ""Include proper imports. Provide only Python code."" This implies that the discussion should be part of the output, not just comments.

**Code Quality (Minor):**
*   `warnings.filterwarnings('ignore')` is overly broad. It's better to ignore specific warnings like in the ground truth for more targeted suppression.
*   The centering of predictors leads to the need for manual creation of the `exog` dataframe, which is less idiomatic for `statsmodels.formula.api` models.

In summary, the most significant failures are the incorrect implementation of nested random effects and the insufficient handling of missing values and interpretation/reporting.",claude-3.5-sonnet,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Load and prepare data
df = pd.read_csv('clash_royale_cards.csv')
features = ['elixirCost', 'Win Rate', 'Usage']
X = df[features]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Elbow method to find optimal K
wcss = []
for k in range(2,8):
    kmeans = KM...",7,"The generated code correctly addresses most of the task requirements: loading data, selecting features, standardizing, applying K-Means, performing PCA, and visualizing. The initial steps of data loading and preparation are concisely handled. The elbow method plot is generated, which is good. There are a few areas for improvement:

1.  **K-Means `n_init` parameter**: The `KMeans` initialization in the generated code implicitly uses the default `n_init='warn'` behavior (or `n_init=10` in newer versions before 1.2, but the warning would still appear if not explicitly set to 'auto' or an integer). The ground truth explicitly sets `n_init=10` which is good practice for reproducibility and to suppress deprecation warnings in newer scikit-learn versions (where `n_init='auto'` is recommended). While it might run, the warning is distracting and indicates a potential for non-reproducible results if `n_init` is not properly handled.
2.  **Clarity in K-choice for the Elbow Method**: The generated code presents the elbow plot but then *arbitrarily* picks `k=4` without any clear justification or instruction for the user to interpret the plot. The task explicitly asked to ""justify your choice of K"". The ground truth provides more context around *how* to interpret the elbow plot and makes an explicit choice based on that.
3.  **Visualization Labels**: While the PCA plot is generated, the axis labels are generic (`First Principal Component`, `Second Principal Component`). The ground truth includes the explained variance ratio in the axis labels, which adds significant contextual value to the plot, allowing the viewer to understand how much variance each component captures.
4.  **Robustness**: The generated code lacks error handling for the `FileNotFoundError` when loading the CSV, unlike the ground truth which gracefully handles this common issue. It also doesn't perform checks for missing features in the dataframe, which the ground truth does. These checks make the code more robust to unexpected input.
5.  **Small Details**: The ground truth includes `n_init=10` for K-Means and `random_state=42` for reproducibility, which is good practice. The generated code also uses `random_state=42` which is good.

Overall, it's a good attempt that covers the core logic, but could benefit from improved robustness, clearer justification in the K-choice step, and more informative plot labels.",claude-3.5-sonnet,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error

# Load data
df = pd.read_csv('clash_royale_cards.csv')

# Prepare features and target
X = df[['elixirCost', 'Usage', 'rarity']]
y = df['Win Rate']

# Create preprocessing pipel...",9,"The generated code correctly addresses all requirements of the task. It loads the data, selects the specified features and target, preprocesses `rarity` using one-hot encoding (though using `ColumnTransformer` and `OneHotEncoder` which is very robust), splits the data, trains a `RandomForestRegressor`, and evaluates it using R2 and MSE.

Strong points:
- **Correctness:** The logic for data loading, feature selection, preprocessing, model training, and evaluation is sound.
- **Completeness:** All aspects of the task are covered.
- **Code Quality:** The use of `ColumnTransformer` and `Pipeline` is excellent for MLOps best practices, making the preprocessing steps explicit and integrated with the model training. This is a very clean and scalable approach.
- **Reproducibility:** `random_state=42` is used for both `train_test_split` and the model initialization.

Areas for minor improvements (not critical, hence the high score):
- **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError` and checks for missing columns. The generated code does not include these, which could lead to a crash if the CSV is missing or columns are misnamed.
- **Clarity of Output:** The ground truth includes more descriptive print statements for the evaluation results, which can be helpful for understanding the output.
- **Missing Value Handling:** Neither the generated code nor the ground truth explicitly handles potential missing values *within* the selected features or target before training, beyond what `pd.read_csv` or the model's default behavior might do. For robustness, explicit steps for imputation or dropping NaNs are usually good practice in real-world scenarios.",claude-3.5-sonnet,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy import stats

# Load data
df = pd.read_csv('clash_royale_cards.csv')

# Get unique rarity categories
rarity_categories = df['rarity'].unique()

# Create lists of win rates for each rarity
rarity_groups = [df[df['rarity'] == rarity]['Win Rate'] for rarity in rarity_categories]

# Perform one-way ANOVA
f_statistic, p_value = stats.f_oneway(*rarity_groups)

# Print results
print(""Null Hypothesis: Mean win rates are equal across all rarity categories"")
print(...",6,"The generated code correctly addresses the core requirements of loading data, performing ANOVA, stating hypotheses, and interpreting results based on the p-value. It is concise and generally follows the requested structure.

However, there are several crucial improvements needed to make it robust and production-ready:

1.  **Error Handling for File Not Found:** The generated code lacks a `try-except` block for `FileNotFoundError` when loading the CSV, which is a common and important consideration.
2.  **Data Cleaning/Type Conversion:** It implicitly assumes `Win Rate` is already numeric. The ground truth explicitly converts `Win Rate` to numeric and handles potential `NaN` values, which is best practice for real-world datasets. Without this, the ANOVA could fail or produce incorrect results if `Win Rate` contains non-numeric entries or NaNs.
3.  **Handling insufficient data:** The ground truth code includes checks for empty DataFrames after cleaning, checks if enough unique rarity categories exist, and ensures each group passed to `f_oneway` has at least two observations. The generated code lacks these vital checks, making it vulnerable to errors or misleading results if the dataset is not perfectly clean or does not have enough data points per category.
4.  **Hypothesis Clarity:** While the hypotheses are stated, the ground truth's phrasing is slightly more formal and precise.
5.  **Hardcoded Alpha:** The significance level `0.05` is hardcoded in the `if` condition for interpretation. It's better practice to define `alpha` as a variable at the top, as done in the ground truth, for readability and easy modification.",claude-3.5-sonnet,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load data
df = pd.read_csv('iris.csv')

# Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = df[...",9,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, defines features and target, encodes the target, splits the data, trains the specified models, calculates and prints the required metrics (accuracy, precision, recall, F1-score), and identifies the best model based on F1-score.

Strong points:
- All required imports are present and correctly used.
- Data loading, feature/target definition, and target encoding are correctly implemented.
- Data splitting uses the correct `test_size` and `random_state`.
- All three specified classifiers are used with the correct parameters (Logistic Regression default, SVC linear kernel with `random_state=42`, Random Forest with `n_estimators=100`, `random_state=42`).
- Evaluation metrics are calculated and printed correctly for each model, with macro averaging as required.
- The best model is identified based on the highest F1-score.

Area for minor improvement:
- The `LogisticRegression` model might benefit from `max_iter` being explicitly set (e.g., `max_iter=200`) to avoid potential `ConvergenceWarning` for some datasets, although it's not strictly required by the prompt's ""default parameters"" clause. The ground truth example used `max_iter=200`.
- The `train_test_split` in the generated code does not include `stratify=y`. While for the Iris dataset with its balanced classes this might not cause an issue, it's generally good practice to stratify the split when dealing with classification tasks to ensure the class distribution is maintained in both training and test sets. This was present in the ground truth code and is a commonly recommended practice for robust model evaluation.",claude-3.5-sonnet,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Load and prepare data
df = pd.read_csv('iris.csv')
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
clus...",9,"The generated code correctly implements all specified steps: loading data, selecting and standardizing features, applying K-Means (with correct parameters), performing PCA, and generating both required scatter plots. The use of `subplot` to show both plots side-by-side is a good approach for comparison.

Areas for minor improvement:
1.  **Plot Titles:** The specified titles were ""PCA of Iris Dataset: Points Colored by K-Means Cluster Labels"" and ""PCA of Iris Dataset: Points Colored by True Species Labels"". The generated titles (""K-Means Clustering Results"" and ""True Species Labels"") are accurate but slightly less descriptive than requested.
2.  **Colorbar:** The ground truth code included colorbars which enhance readability, especially for the species plot where the ticks could be mapped to actual species names. This was not explicitly requested for *both* plots but is good practice. The generated code didn't include colorbars.
3.  **Error Handling:** The ground truth included basic error handling for `FileNotFoundError`, which is good practice for robustness, though not strictly required by the prompt.

Overall, it's a very strong submission that meets the core requirements effectively. The minor deviations are aesthetic or best practice additions rather than fundamental correctness issues.",claude-3.5-sonnet,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# Load data
df = pd.read_csv('iris.csv')

# Create new features
df['PetalArea'] = df['PetalLengthCm'] * df['PetalWidthCm']
df['SepalArea'] = df['SepalLengthCm'] * df['SepalWidthCm']

# Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'PetalArea', 'SepalArea']]
y...",10,"The generated code is excellent. It correctly addresses all requirements of the task: loading the dataset, creating the specified new features, defining `X` with both original and new features, encoding the target `y`, initializing and training the Gradient Boosting Classifier with the correct parameters, performing 5-fold cross-validation, and reporting the mean accuracy. The variable names are clear, and the code structure is logical. It includes all necessary imports and directly solves the problem. The only minor difference from the ground truth is the absence of error handling for file not found, but since the task description implies the file is present, this is not a deduction point for correctness. It also omits printing the raw CV scores in each fold, but the task specifically asked for the _mean_ score.",claude-3.5-sonnet,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load data
data = pd.read_csv('heart.csv')

# Define features and target
X = data.drop('HeartDise...",8,"The generated code correctly addresses most of the requirements. It loads the data, defines features and target, performs one-hot encoding for categorical features and scaling for numerical features using a `ColumnTransformer` and `Pipeline`, splits the data, trains a `RandomForestClassifier`, and evaluates the model using the specified metrics.

Strong points:
- Correctly uses `ColumnTransformer` and `Pipeline` for preprocessing, which is good practice.
- Correctly calculates and prints all required metrics.
- Uses `random_state` for reproducibility in both `train_test_split` and `RandomForestClassifier`.

Areas for improvement:
- **Feature selection**: The prompt explicitly listed the features to be used (`Age`, `Sex`, etc.). The generated code uses `X = data.drop('HeartDisease', axis=1)`, which implicitly selects all other columns. While in this specific dataset 'HeartDisease' is the only non-feature column, it's better to explicitly select the features as listed in the prompt to prevent issues if there were other irrelevant columns.
- **`OneHotEncoder` parameters**: The ground truth uses `handle_unknown='ignore'` for `OneHotEncoder`, which is robust if new categories appear in the test set. The generated code uses `drop='first'` which is acceptable for avoiding multicollinearity but `handle_unknown='ignore'` is generally a safer default. The prompt did not specify `drop='first'`.
- **`train_test_split` stratification**: The ground truth includes `stratify=y` in `train_test_split`, which is good practice for classification tasks, especially with imbalanced datasets, to ensure that both train and test sets have a similar distribution of the target variable. The generated code omits this. While not explicitly asked for, it's a detail that often differentiates robust solutions.",claude-3.5-sonnet,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

# Load and prepare data
df = pd.read_csv('heart.csv')
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']
X = df[features]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Elbow method
wcss = []
si...",9,"The generated code successfully performs all required steps: data loading, feature selection and standardization, K-Means clustering with K chosen via elbow and silhouette methods, PCA dimensionality reduction, and visualization.

**Strong points:**
- Correctly loads data and selects specified features.
- Implements `StandardScaler` for feature standardization.
- Correctly uses `n_init='auto'` and `random_state=42` for KMeans.
- Plots both WCSS and silhouette scores for K determination.
- Performs PCA to 2 components.
- Visualizes the clusters using PCA components, correctly coloring points by cluster labels.
- Includes all necessary imports.

**Areas for minor improvement (preventing a perfect 10):**
- The task explicitly asked to ""justify your choice of K"" and the ground truth code included a print statement acknowledging the visual inspection. While the plots are there, explicitly stating the chosen K and its reasoning (even if it's an ""example choice"") as part of the output would align closer with the ""justify"" requirement. The generated code implicitly uses `k=3` but doesn't explicitly state it as the *chosen* K in the output based on the plots, just proceeds with it.
- The principal component labels on the scatter plot are simply 'First Principal Component' and 'Second Principal Component'. While correct, adding 'PC1' and 'PC2' as per the ground truth (and common practice) is a minor stylistic improvement.
- Lacks error handling for `FileNotFoundError` when loading the CSV, which was present in the ground truth and is good practice. This wasn't explicitly stated as a hard requirement but is a good production-ready aspect.

Overall, it's a very good solution and meets nearly all requirements comprehensively.",claude-3.5-sonnet,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report

# Load and prepare data
df = pd.read_csv('winequality-red.csv', sep=';')

# Create binary target
df['quality_binary'] = (df['quality'] >= 7).astype(int)

# Separate features and target
X = df.drop(['quality', 'quality_bi...",6,"The generated code correctly addresses most of the task requirements, but it has some notable shortcomings that prevent it from achieving a higher score.

**Positive Aspects:**
*   Correctly loads the data (though it incorrectly assumes a semicolon separator).
*   Correctly creates the binary target variable.
*   Separates features and target appropriately.
*   Handles missing values (though perhaps not as robustly as median imputation).
*   Scales features using `StandardScaler`.
*   Splits data into train/test sets.
*   Trains a `LogisticRegression` model.
*   Evaluates the model using accuracy, F1-score, and classification report.
*   Attempts to report feature importance based on coefficients.

**Areas for Improvement/Shortcomings:**

1.  **DataLoader Issue:** The `pd.read_csv('winequality-red.csv', sep=';')` part is incorrect. The `winequality-red.csv` dataset is typically comma-separated, not semicolon-separated. This will lead to the data not being loaded properly and subsequent errors or incorrect results if the file actually exists as a comma-separated file. **This is a significant flaw as the entire output depends on correct data loading.**
2.  **Missing Value Handling:** While `X.fillna(X.mean(), inplace=True)` is a valid approach, median imputation is often preferred for skewed data or when dealing with outliers, which might be the case in some features of this dataset. The ground truth's check for missing values before imputation is also good practice, though less critical.
3.  **Data Split Stratification:** The `train_test_split` does not use `stratify=y`. For imbalanced datasets (which this binary classification on wine quality often is, as 'good' wines are fewer), stratification is crucial to ensure both train and test sets have representative distributions of the target classes. Without it, the model training and evaluation might be biased.
4.  **Scaling and DataFrame Preservation:** The generated code applies `scaler.fit_transform(X)` and then splits `X_scaled`, meaning `X_train` and `X_test` lose their column names. While `LogisticRegression` can handle NumPy arrays, the subsequent feature importance reporting (`X.columns` used with `model.coef_[0]`) implicitly assumes the order of columns remains consistent, which it does in this case. However, for better readability and robustness, especially when dealing with feature importance, it's generally better to maintain DataFrame structures or explicitly map coefficients back to feature names using the original column names. The ground truth does this by converting scaled arrays back to DataFrames.
5.  **Feature Importance - Absolute Value:** The generated code uses `abs(model.coef_[0])` for feature importance, which is good for ranking by magnitude. However, it doesn't show the original sign of the coefficient. The ground truth code provides both the coefficient and its absolute value, which gives a more complete picture of the feature's relationship with the target (positive vs. negative impact).
6.  **Code Structure and Comments:** The code is functional but lacks comments explaining different sections, which would improve readability for complex tasks. The ground truth code is well-structured with clear comments and print statements indicating progress.
7.  **Reproducibility (Solver/Max_iter):** The generated code doesn't specify `solver='liblinear'` or `max_iter=1000` for `LogisticRegression`. While `liblinear` is a common default for small datasets, explicitly setting it and `max_iter` can aid reproducibility and convergence stability. The model likely converges with defaults, but it's a minor point.",claude-3.5-sonnet,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

# Load data
df = pd.read_csv('heart.csv')

# Create box plot
plt.figure(figsize=(8,6))
sns.boxplot(x='HeartDisease', y='Oldpeak', data=df)
plt.title('Distribution of Oldpeak by Heart Disease Status')
plt.show()

# Logistic regression
X = df['Oldpeak'].values.reshape(-1,1)
y = df['HeartDisease'].values
log_reg = LogisticRegression()
log_reg.fi...",8,"The generated code correctly addresses most of the requirements but has some minor shortcomings.

**Strong Points:**
*   **Correct Data Loading and Basic EDA:** Loads the data, creates the required box plot, and handles basic setup correctly.
*   **Correct Logistic Regression:** Performs the logistic regression with `Oldpeak` as the independent variable, extracts the coefficient, and calculates the odds ratio, which are the core calculations.
*   **Correct Binning:** Uses `pd.qcut` to discretize `Oldpeak` into three equally sized bins as requested.
*   **Correct Proportion Calculation:** Accurately calculates the proportion of `HeartDisease`=1 within each `Oldpeak` bin.

**Areas for Improvement / Missing Aspects:**
*   **Interpretation of Coefficient/Odds Ratio (Missing):** The prompt explicitly requested to ""interpret its meaning in the context of odds ratios."" The generated code prints the coefficient and odds ratio but does not provide any textual interpretation, which was a specific requirement.
*   **Plot Customization/Labels (Minor):** While the box plot is created, it lacks proper x-axis labels ('No Heart Disease', 'Heart Disease') as specified in the ground truth for better readability.
*   **Error Handling (Missing):** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is good practice. The generated code assumes the file exists.
*   **Robustness for `pd.qcut` (Missing):** The ground truth includes a `try-except` block for `pd.qcut` with `duplicates='drop'` and a fallback to `pd.cut` in case `qcut` fails (e.g., due to identical values preventing unique quantiles). This makes the ground truth more robust. The generated code lacks this.
*   **Visualization of Binned Proportions (Missing):** The ground truth optionally visualizes the proportions of heart disease by bin, which adds value to the analysis, though the prompt didn't explicitly demand a plot for this step; it just asked to ""display"" the proportion. The prompt also mentioned including proper imports but the generated code doesn't explicitly import `numpy` which is later used for `np.exp`. (Self-correction: `numpy` *is* imported at the top, so this is fine).

Overall, it's a solid attempt that gets the core calculations right. The main deduction is for missing the explicit interpretation, which was a key part of the requirements for the logistic regression section.",claude-3.5-sonnet,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

# Load data
df = pd.read_csv('spotify_2023.csv')

# Convert streams to numeric and drop NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams'])

# Create binary target variable
strea...",8,"The generated code largely fulfills the requirements of the task.

**Strong Points:**
- Correctly loads data, converts `streams` to numeric, and handles NaNs.
- Accurately computes the 75th percentile for `streams` and creates the `is_popular` target variable.
- Selects the specified numerical and categorical features.
- Correctly performs one-hot encoding on 'key' and 'mode'.
- Splits the data into training and testing sets with the correct `test_size` and `random_state`.
- Trains a `LogisticRegression` model with `max_iter=1000`.
- Calculates and prints the F1-score (macro) and confusion matrix.

**Areas for Improvement / Minor Issues:**
- **ROC AUC Score Calculation:** The `roc_auc_score` function expects probabilities of the positive class (`predict_proba[:, 1]`) as its second argument, not just the binary predictions (`predict`). The current implementation for ROC AUC will raise an error or produce incorrect results if `y_pred` is used directly here. This is a common mistake for this metric.
- **`OneHotEncoder` `drop='first'`:** While `drop='first'` is often used to avoid multicollinearity, the prompt did not specify this. The ground truth used `drop_first=False` (default behavior of `pd.get_dummies` without `drop_first=True`), which technically aligns more closely if absolute adherence to the prompt's simplicity is desired. However, this is a minor deviation and often a good practice.
- **Combining Dummies and Numerical Features:** When using `OneHotEncoder`, the `pd.DataFrame(encoded_cats, ...)` part assumes that the index of `df[numeric_features]` and `encoded_cats` matches after `dropna`. This can be a point of failure if the original `df` has a non-default index and `dropna` or other operations reset it. Explicitly handling indices or using `reset_index(drop=True)` on `df` before split, or combining using `pd.get_dummies` on the full dataframe, would make it more robust. The ground truth's approach of `pd.get_dummies` on the `X` dataframe (which already has the selected features) is less prone to index misalignment if `X` is appropriately extracted from the DataFrame.
- **Prints for Clarity:** The generated code only prints the final evaluation metrics. Adding print statements for intermediate steps (e.g., data loading success, rows dropped, target variable creation, feature columns after encoding, data split shapes) would improve its completeness in terms of user feedback, as seen in the ground truth.
- **Error Handling:** The ground truth includes basic error handling for file loading, which the generated code lacks. While not explicitly asked for, it is good practice.

Overall, the code is very close to perfect, with the ROC AUC calculation being the most significant functional issue. The other points are more about robustness and best practices rather than strict adherence to the prompt's core requirements.",claude-3.5-sonnet,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Load data
df = pd.read_csv('spotify_2023.csv')

# Select features and drop NaNs
features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%', 
           'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm']
df_features = df[features].dropna()

# Standardize features
scaler = StandardScaler()
fe...",9,"The generated code is highly correct and complete, successfully addressing almost all requirements of the task.

**Strong Points:**
*   **Correctness:** All core steps are implemented correctly: data loading, feature selection, NaN handling, StandardScaler, K-Means (K=4, random_state=42, n_init='auto'), PCA (2 components, random_state=42), scatter plot coloring, and calculation/display of cluster means of *original* features.
*   **Completeness:** All explicit requirements from the prompt (imports, specific parameters for K-Means and PCA, plot title/labels, mean calculation) are met.
*   **Code Quality:** The code is clean, readable, and follows a logical flow. Imports are correct.

**Areas for Minor Improvement (for a perfect 10):**
*   **Robust Data Loading:** The ground truth includes a `try-except FileNotFoundError` block for `pd.read_csv`. While not explicitly requested as a requirement, this adds robustness to the script, which is good practice in real-world scenarios. The generated code assumes the CSV is always present.
*   **NaN Handling Output:** The ground truth includes print statements showing how many rows were dropped due to NaNs. This provides useful information about the data processing step. While the generated code correctly drops NaNs, it doesn't provide this informative output.
*   **`df_features` Index:** When `dropna()` is called on `df[features]`, the index might become non-contiguous. When `features_scaled` is created and later `df_features['Cluster'] = clusters` is executed, pandas will attempt to align by index. If the index of `df_features` (after `dropna`) doesn't perfectly align with the `clusters` array (which is a simple numpy array from `fit_predict`), it could lead to misalignment issues. The ground truth handles this by storing the original (unstandardized) data in `df_original_for_analysis` *before* scaling and attaching clusters to *that* DataFrame, ensuring the index and original data for mean calculation are consistent. In this specific case, since `clusters` is derived from `features_scaled` which was derived from `df_features.dropna()`, the alignment might work due to implicit pandas behavior, but it's less explicit and potentially fragile. The ground truth's `df_original_for_analysis = df_features.copy()` before scaling guarantees that the `df_features` used for `dropna` and then for mean calculation retains its original index and shape to receive the `cluster` column.

These are minor points, and the generated code is very good overall.",claude-3.5-sonnet,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('spotify_2023.csv')

# Convert streams to numeric
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# Filter data
df_filtered = df[
    (df['streams'] > 10000000) & 
    (df['released_year'] >= 2010) & 
    (df['released_year'] <= 2023)
].dropna(subset=['streams'])

# Group by year and calculate means
yearly_means = df_filtered.groupby('released_year').agg({
    'danceability_%': 'mean',
    ...",9,"The generated code correctly addresses almost all aspects of the task. It loads the data, correctly converts `streams` to numeric, and filters the data based on the specified conditions (`streams` > 10M, `released_year` between 2010 and 2023). It then correctly groups the data by `released_year` and calculates the mean of the specified audio features. Finally, it generates the requested line plots for each audio feature.

**Strong Points:**
*   Correct use of `pd.to_numeric` with `errors='coerce'`.
*   Efficient filtering using boolean indexing.
*   Correct aggregation using `groupby` and `agg`.
*   Proper use of `matplotlib.pyplot` for creating subplots and line plots with titles and labels.
*   `plt.tight_layout()` is used for better plot presentation.

**Areas for Improvement (minor):**
*   The filtering for NaN `streams` is applied *after* the initial filtering of `streams` > 10M and `released_year` range. While this works, it's slightly more robust to drop NaNs immediately after `pd.to_numeric` if `streams` is a key column, as shown in the ground truth. The ground truth dropped NaNs *before* the numerical comparison. In this specific case, it doesn't cause an error because `NaN > 10000000` is `False`, so NaN rows won't pass the `streams` filter anyway. However, it's good practice to handle missing values explicitly before filtering on the numeric value.
*   The plot layout uses `(3,1,x)` meaning 3 rows and 1 column, making the plots vertical. The ground truth used `(1,3,x)` for 1 row and 3 columns, which is often preferable for comparing trends across similar metrics side-by-side, especially if the years are many. The prompt didn't specify the layout, so `(3,1,x)` is technically correct but might be less readable for long x-axes.
*   No error handling for `FileNotFoundError` for the CSV, which is a good practice for robust code.
*   Minor detail: The `released_year` x-axis labels might get crowded if there are many years and they aren't rotated, which the ground truth did. The generated code's current `3,1,x` subplot layout actually helps manage x-axis crowding compared to `1,3,x` *without* rotation.

Overall, solid implementation that meets the core requirements. The minor points are mostly about robustness and presentation refinements.",claude-3.5-sonnet,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder

# Load data
df = pd.read_csv('youtube_trendings.csv')

# Clean numeric columns
for col in ['Likes', 'Dislikes', 'Video views']:
    df[col] = df[col].str.replace(',', '').fillna('0')
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

# Extract ye...",7,"The generated code correctly addresses most of the requirements. It loads the data, performs the specified cleaning on `Likes`, `Dislikes`, and `Video views`, extracts the year from `published`, handles missing categories, and performs one-hot encoding. It defines `X` and `y` correctly, splits the data, trains a Linear Regression model, and reports R2 and MAE.

Areas for improvement to achieve a higher score:

1.  **Robustness of Numeric Cleaning:** The line `df[col] = df[col].str.replace(',', '').fillna('0')` is problematic. `fillna('0')` applies to string NaNs *before* `to_numeric`, which could lead to incorrect parsing if the original column had string NaNs (though less likely in this dataset). The ground truth's approach of `pd.to_numeric(df[col], errors='coerce')` followed by `fillna(0)` is more robust as it explicitly caters to non-numeric values turning into `NaN` first, which then correctly become 0. While it might work for this specific dataset, it's not universally robust.
2.  **Handling `published` column NaNs:** The generated code `df['published_year'] = pd.to_datetime(df['published']).dt.year` does not explicitly handle potential `NaT` values that could arise if `pd.to_datetime` fails for some rows. This could propagate NaNs into `published_year` which would then affect the model training. The ground truth `fillna(0)` is a safer approach for this benchmark.
3.  **OneHotEncoder Output:** `encoder.get_feature_names(['Category'])` is deprecated. The recommended way is `encoder.get_feature_names_out(['Category'])`. While this doesn't break the code, it's a best practice to use the current API.
4.  **Redundant `reset_index(drop=True)`:** The `reset_index(drop=True)` for `df[['Dislikes', 'Video views', 'published_year']]` before concatenation with `category_df` is generally not needed if the original DataFrame `df` has a default integer index and `category_df` also has a default integer index, as `pd.concat` will align by index. While it doesn't cause a bug here, it adds unnecessary complexity.
5.  **Error Handling:** The ground truth includes `try-except` for file loading and checks for column existence, which makes the code more robust to unexpected input data. This was not formally required by the prompt, but it is a mark of higher quality code.

Overall, the code performs the task correctly for the given dataset structure, but some parts could be made more robust and follow more current best practices.",claude-3.5-sonnet,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('youtube_trendings.csv')

# Clean video views
df['Video views'] = df['Video views'].str.replace(',', '').astype(float).fillna(0)

# Convert published year to numeric
df['published'] = pd.to_numeric(df['published'], errors='coerce')

# Group by year and calculate metrics
yearly_stats = df.groupby('published').agg({
    'Video views': 'mean',
    'Video title': 'nunique'
}).reset_index()

# Create figure wi...",6,"The generated code addresses most of the requirements but has critical issues that prevent it from being production-ready or fully correct for the given dataset.

**Positive points:**
*   Correctly loads the dataset.
*   Correctly processes the `Video views` column (removes commas, converts to numeric, fills NaNs).
*   Correctly groups by the `published` year and calculates the mean of 'Video views' and the count of unique 'Video title'.
*   Creates two subplots with appropriate titles and axis labels.

**Areas for Improvement / Critical Issues:**
1.  **`published` column handling:** The prompt specified the `published` column (year) should be treated as numeric. The dataset typically has a `publishedAt` column which contains a full datetime string, *not* just the year in a column named `published`. The generated code assumes a `published` column already exists and contains numeric year data. This is incorrect for the typical `youtube_trendings.csv` dataset, which usually has `publishedAt`. It should parse the year from `publishedAt`. This is a major correctness issue as the code will likely fail or produce incorrect results if `published` isn't a pre-processed year column. The `df['published'] = pd.to_numeric(df['published'], errors='coerce')` line is likely to produce all NaNs if `published` is not present, or if it's present but not numeric (e.g., if a column named `published` oddly contained non-numeric data).
2.  **`Video name` vs `Video title`:** The prompt requested `Video` (or `Video title` in context) for unique count, and the solution used `Video title` which is likely correct. However, `Video name` was used in the `agg` function in the generated code while `Video title` was used in the plotting. Based on the prompt's `Video` titles metric, `Video title` is generally the correct column name in these datasets. This is a minor inconsistency (`df.groupby('published').agg({'Video views': 'mean', 'Video title': 'nunique'})` would be robust).
3.  **Missing `Video` column:** The prompt specified calculating the total number of unique videos using the `Video` titles/column. The generated code uses `'Video title'` in the groupby aggregation, which is likely the correct column name. However, the prompt might have intended `Video` as the column, which is usually `Video title`. This is a slight ambiguity but the generated code's choice is reasonable.
4.  **Error Handling:** The code lacks any error handling for file not found, which is good practice for data loading tasks. (Though the prompt didn't explicitly ask for it, it's a mark of robust code).
5.  **Direct execution:** The code is written as a script that runs directly, rather than a function. While this wasn't explicitly forbidden, a function would make the code reusable.

**Recommendation:** The most critical fix is to correctly extract the year from the `publishedAt` column (or whatever the actual datetime column is named) before grouping.",claude-3.5-sonnet,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
from nltk.corpus import stopwords
import numpy as np

# Load data
df = pd.read_csv('youtube_trendings.csv')

# Create title word count feature
df['title_word_count'] = df['Video'].str.split().str.len()

# Clean video views
df['Video views'] = df['Video views'].str.replace(',', '').astype(float).fillna(0)

# Calculate correlation
correlation = df['title_word_count'].corr(df['Video views'])
print(f'Correlation coefficient: {co...",9,"The generated code successfully completes almost all aspects of the task.

**Strong Points:**
- **Correctness:**
    - Loads the dataset.
    - Accurately creates `title_word_count`.
    - Correctly cleans `Video views` (removes commas, converts to numeric, fills NaNs).
    - Calculates and prints the Pearson correlation coefficient.
    - Filters the dataset for the specified years (2023, 2024).
    - Processes titles correctly for lowercase, punctuation removal, and stop word filtering.
    - Finds and prints the 5 most common words.
- **Code Quality:**
    - Uses appropriate imports (`pandas`, `re`, `collections.Counter`, `nltk.corpus.stopwords`).
    - Code is well-structured and readable.
- **Completeness:** Addresses most of the requirements.

**Areas for Minor Improvement:**
- **NLTK Download:** The ground truth includes a robust way to ensure NLTK stopwords are downloaded (`try-except` with `nltk.download`). The generated code assumes `stopwords` are already downloaded, which might lead to an error if not. This is a common practical consideration for NLTK.
- **File not found:** The ground truth also includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.
- **Slightly less robust `Video views` cleaning:** The ground truth uses `df['Video views'].astype(str).str.replace(',', '', regex=False)` which is slightly more robust if the column is not already string. The generated code `df['Video views'].str.replace(',', '')` might error if the column is not object/string type initially (though for this specific dataset, it likely works).
- **Presentation of correlation:** The ground truth prints a more descriptive string for the correlation output.
- **Word processing detail:** While correct for the given task, the ground truth explicitly uses `word.isalpha()` in the final word processing step, which an edge case where titles might contain numbers or mixed alphanumeric strings that shouldn't be counted as words. This is a minor detail and the generated code is still correct based on the prompt's `remove punctuation` requirement.",claude-3.5-sonnet,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score
import re

# Load data
df = pd.read_csv('mobiles_2025.csv')

# Clean price and create target
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''), errors='coerce')
df['is_expensive_usa...",7,"The generated code largely addresses the requirements of the task. However, there are several areas where it falls short compared to the ground truth, primarily concerning robustness, data handling, and subtle data cleaning nuances.

**Strengths:**
*   Correctly loads the data.
*   Correctly identifies and performs one-hot encoding for `Company Name` and `Processor`.
*   Correctly splits the data, trains a RandomForestClassifier, and evaluates using F1-score and ROC AUC.
*   The overall flow of the machine learning pipeline is correct.

**Areas for Improvement/Weaknesses:**

1.  **Target Variable Creation (`is_expensive_usa`):**
    *   **Issue:** The generated code uses `(df['Launched Price (USA)'] > 1000).astype(float)`. This creates `1.0` and `0.0`. While technically correct for boolean conversion, the ground truth explicitly uses `apply` with `pd.notna(x)` and then `astype(int)` to ensure `0` or `1` for the target and correctly handles `NaN` values *before* dropping, making the logic more explicit for the `> 1000` condition considering the original `NaNs`. The generated code does `dropna` *after* this step, which might leave some `NaN` in `is_expensive_usa` if `pd.to_numeric` failed for values <= 1000 but not ""> 1000"".
    *   **Ground Truth Difference:** The ground truth sets `0` if `x <= 1000` and `pd.notna(x)`, which is a more robust way to define the 0 class, whereas the generated code just converts `False` to `0.0`. If a price was unparseable *but* was less than or equal to 1000, the ground truth would correctly mark it as `NaN` to be dropped, while the generated code's `astype(float)` on `False` would turn it into `0.0` potentially incorrectly.

2.  **Handling of `NaN`s during Feature Cleaning:**
    *   **Issue:** When cleaning numeric features like `Mobile Weight`, `RAM`, etc., the generated code uses `pd.to_numeric(..., errors='coerce')`. However, it doesn't immediately handle these `NaN`s. The `X` and `y` NaN drop is only done at the very end (`mask = ~(X.isna().any(axis=1) | y.isna())`). This can lead to `NaN` propagation through the `pd.concat` step and potential issues if `pd.concat` were to include `NaN`s in a problematic way (though `dropna` at the end resolves it), or if the `OneHotEncoder` encountered `NaN`s (which it handles with `handle_unknown='ignore'`, but it's not ideal for input data).
    *   **Ground Truth Difference:** The ground truth implicitly allows `NaN`s in cleaned numeric features until the final `X`, `y` definition and `dropna` on the combined dataframe, which is acceptable but the logic of creating `_cleaned` columns and then selecting from them is clearer.

3.  **OneHotEncoder Input and `pd.concat`:**
    *   **Issue:** The generated code uses `df[categorical_features]` directly for `OneHotEncoder`. If these columns contained `NaN`s, `OneHotEncoder` would convert them to `NaN`s in the `encoded_features` array. `handle_unknown='ignore'` is for unseen categories, not `NaN`s. While the final `dropna` on `X` and `y` will remove rows with NaNs in the encoded features, it's a bit less explicit.
    *   **Issue:** `encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))` creates a new DataFrame with a *new index*. When `pd.concat([df[numerical_features], encoded_df], axis=1)` is performed, if `df` had any rows dropped earlier (e.g., during the `is_expensive_usa` `dropna`), the indices of `df[numerical_features]` and `encoded_df` might no longer align, leading to misaligned data or more `NaN`s. This is a common and subtle bug when not resetting indices or ensuring alignment. The ground truth avoided this by performing operations on `df_cleaned` and then correctly selecting columns for X and y, ensuring all operations are on the same, consistent dataframe until the final `dropna`.

4.  **Camera Feature Cleaning (`extract_first_mp`):**
    *   **Issue:** The `re.findall(r'(\d+(?:\.\d+)?)', str(x))[0]` might throw an `IndexError` if no number is found, which is then caught by a generic `except`. This works functionally but is less specific than `re.search` and explicit group extraction.
    *   **Ground Truth Difference:** The ground truth's `extract_mp` function is more robust, specifically looking for `MP` (e.g., `12 MP`) which is more aligned with the typical format, and explicitly returns `0.0` if no `MP` value is found, not just any numerical string. The generated code's `extract_first_mp` would extract `123` from ""123mAh"" or ""123g"" if column names were swapped or data was messy, which is less precise for camera MP.

5.  **Robustness (e.g., `FileNotFoundError`, `stratify`):**
    *   The generated code doesn't include `try-except` for file loading.
    *   It also omits `stratify=y` in `train_test_split`, which is good practice for classification tasks to maintain class balance in splits.

6.  **Code Comments and Readability:**
    *   The ground truth provides excellent comments and print statements indicating progress and dataset shape changes, which greatly aids understanding and debugging. The generated code is more concise but less informative.

Overall, the generated code performs the core tasks and would likely produce a working ML model. However, its robustness, precise handling of edge cases (especially `NaN`s and index alignment during `pd.concat`), and the subtlety of its data cleaning (e.g., camera MP extraction) are not as strong as the ground truth. The `pd.concat` with potentially misaligned indices due to prior `dropna` on `df` for the target is the most critical correctness flaw.",claude-3.5-sonnet,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

def clean_price(price):
    try:
        price = str(price)
        price = price.replace('PKR', '').replace('INR', '').replace('CNY', '')
        price = price.replace('USD', '').replace('AED', '').replace(',', '')
        return float(price.strip())
    except:
        return np.nan

# Load dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean price columns
price_columns = ['Launched Price (Pakistan)', 'Launched Price (India)', 
               ...",8,"The generated code correctly addresses most of the task requirements.

**Strong Points:**
- **Correct Data Loading:** Successfully loads the CSV.
- **Accurate Cleaning Function:** The `clean_price` function correctly removes currency symbols and commas, and handles `ValueError` by returning `np.nan`, which is crucial for the subsequent steps.
- **Correct Column Application:** Applies the cleaning function to all specified price columns.
- **Correct Average Calculation:** Calculates `Average Global Price` using `mean(axis=1, skipna=True)`, which correctly ignores NaN values.
- **Correct Top 5 Identification:** Uses `nlargest(5, 'Average Global Price')` to efficiently identify the top 5 models by `Average Global Price`.
- **Correct Display:** Displays the `Model Name` and `Average Global Price` for the top 5, and `to_string(index=False)` provides clean output.

**Areas for Improvement:**
- **Robustness in `clean_price`:** The `clean_price` function does not explicitly handle cases where the input is already `NaN` or `None`. While `str(price)` might convert `NaN` to the string 'nan', it's better practice to check for `pd.isna(price)` directly at the beginning, as in the ground truth, to avoid unnecessary string operations and ensure consistency.
- **Error Handling for File Loading:** The code assumes the CSV file will always be present. Adding a `try-except FileNotFoundError` block for `pd.read_csv` would make it more robust, as seen in the ground truth.
- **Warning for Missing Columns:** The ground truth includes a warning if a specified `price_column` is not found in the DataFrame. While not strictly required by the prompt, it's good practice for real-world data handling.
- **Clarity in `clean_price`:** The concatenation of `.replace()` calls is efficient but can be slightly less readable than an iterative approach for multiple currency symbols, especially if the list of symbols grows. The ground truth's loop over `currency_symbols` is arguably more maintainable.
- **Displaying all top models:** While `df.nlargest(5, 'Average Global Price')` will pick the top 5, if there are ties for the 5th spot, `nlargest` might return more than 5 rows. The prompt asked for ""the top 5"", so it's good that it implicitly handles it. The ground truth explicitly uses `.head(5)` after sorting, which guarantees exactly 5 rows (unless less than 5 models exist in total).

Overall, excellent solution that fulfills the core requirements effectively and efficiently. The minor improvements are mostly about robustness and defensive programming, which are important for production-level code.",claude-3.5-sonnet,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('mobiles_2025.csv')

# Clean columns
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''), errors='coerce')

# Drop rows with NaN in cleaned colu...",6,"The generated code correctly addresses loading the data, cleaning the specified columns, handling NaNs, and filtering for Apple products. The imports are proper.

However, it fails on a critical aspect of the plotting requirement. The task explicitly asks for box plots *for each Launched Year* to ""visualize and compare the distributions of the cleaned RAM, Battery Capacity, and Launched Price (USA)"". This implies generating distinct sets of box plots *per year*, showing the distribution of the three metrics *within that year*. The generated code, conversely, creates three box plots where the x-axis is 'Launched Year', showing the trend of each metric across all years, rather than the distribution within each year for all three metrics simultaneously. This is a significant deviation from the prompt's visualization requirement.

Specifically:
- **Plotting:** The prompt asked for ""For each Launched Year, generate box plots to visualize and compare the distributions... Use subplots for clarity (e.g., one row of three subplots, or three separate figures)."" This means iterating through each unique `Launched Year` and, *for each year*, plotting the RAM, Battery Capacity, and Price distributions. The generated code plots RAM vs. Year, Battery Capacity vs. Year, and Price vs. Year on separate subplots, which does not fulfill the requirement of showing distributions *within* each year.
- **Titles and Labels:** While titles and y-labels are present, they assume the incorrect multi-year plotting approach. The titles should ideally reflect the year for the plots.
- **Handling `Launched Year` NaNs:** The ground truth explicitly cleans `Launched Year` and handles NaNs before converting to int. The generated code implicitly assumes `Launched Year` is clean and numeric, which might not always be the case and could lead to errors if the column contains non-numeric values.
- **Robustness:** The ground truth includes error handling for file loading and checks for empty filtered dataframes, making it more robust. The generated code lacks these.",claude-3.5-sonnet,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_val_score, learning_curve, validation_curve
from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures
from sklearn.impute import SimpleImputer, IterativeImputer
from sklearn.feature_selection import SelectKBest, RFE, SelectFromModel
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, V...",4,"The generated code attempts to address many aspects of the prompt but falls short on implementing them correctly, robustly, or completely.

**Critical Issues and Missing Components:**

1.  **Data Preprocessing and Pipelines:**
    *   **Custom Transformer Missing:** The prompt explicitly asked for a custom transformer for `Name` and `Cabin` feature engineering (`FeatureEngineerTransformer`) and the generated code instead implements this within a `preprocess_data` function, which is not integrated into a `Pipeline`-based `ColumnTransformer` approach. This means the feature engineering steps are applied *before* scaling and imputation, and not as part of a robust, reproducible pipeline.
    *   **Multiple Imputation:** The prompt specified **multiple imputation strategies**. The generated code only uses `IterativeImputer` but doesn't integrate it into the `ColumnTransformer` properly, leading to `X_train_scaled`/`X_test_scaled` being passed to `engineer_features` *before* the imputers would have handled missing values, resulting in errors or incorrect behavior for features like 'Age' and 'Fare' which are known to have missing values. The ground truth correctly uses `IterativeImputer` within a numerical pipeline inside `ColumnTransformer`.
    *   **Polynomial Features Positioning:** Polynomial features are expected to be applied to numerical features *after* scaling and imputation within the preprocessing pipeline. The generated code applies `PolynomialFeatures` incorrectly within `engineer_features` on `X_train_scaled`, which is a NumPy array without feature names, making interaction terms (`Age_Class`, `Fare_Class`) hardcoded and unscalable when combined with `PolynomialFeatures`. The ground truth integrates `PolynomialFeatures` correctly into the numerical pipeline.
    *   **Text Mining from Names:** The prompt specifically requested ""text mining from names."" While it extracts a 'Title', it's a very basic extraction and the ground truth's `FeatureEngineerTransformer` performs more advanced text processing like grouping rare titles and creating 'Is_Married' along with 'Name_Length'.
    *   **Handling Missing Values with Multiple Imputation Strategies:** The generated code only uses `IterativeImputer` for numerical features and implicitly `SimpleImputer` for categorical ones (via `LabelEncoder` sometimes handling NaN, but not robustly), but doesn't explicitly showcase or compare *multiple* imputation strategies. The prompt asked for ""multiple imputation strategies,"" implying a comparison or flexible choice, which is not present.
    *   **LabelEncoder vs. OneHotEncoder:** `LabelEncoder` is used for categorical features. While it might work for some models, `OneHotEncoder` (as used in the ground truth) is almost always preferred for non-ordinal categorical features to prevent models from inferring an arbitrary ordinal relationship. `ColumnTransformer` (missing from the generated code) is key here for consistent preprocessing.

2.  **Model Building and Tuning:**
    *   **`GridSearchCV` vs. `RandomizedSearchCV`:** The prompt asked for `GridSearchCV *or* RandomizedSearchCV`. The generated code uses `GridSearchCV` exclusively with very limited parameter grids, which is inefficient for a task requesting ""advanced"" tuning. `RandomizedSearchCV` (used in the ground truth) is generally preferred for broader hyperparameter search.
    *   **Pipeline Integration:** The crucial part of `GridSearchCV`/`RandomizedSearchCV` with preprocessing steps is to wrap the *entire preprocessing (ColumnTransformer) and model* into a `Pipeline`. The generated code performs preprocessing *before* tuning, which means the hyperparameter search is done on already processed fixed data, potentially leading to data leakage if not handled perfectly, and makes the model less portable as it doesn't contain the preprocessing steps within itself. The ground truth correctly defines a pipeline `preprocessor -> classifier` and tunes over parameters prefixed with `classifier__`.

3.  **Feature Selection:**
    *   **Integration with Pipeline:** Feature selection methods should ideally be integrated into a pipeline for proper cross-validation and evaluation. The generated code applies them *after* preprocessing, which makes it harder to properly compare their impact within a cross-validation loop.
    *   **Comparison of Impact:** The prompt asked to ""compare their impact."" The generated code runs `SelectKBest`, `RFE`, `SelectFromModel` but only performs basic evaluation (accuracy, etc.) and doesn't clearly *compare* the performance of models trained with different feature sets or integrate this comparison into the overall model evaluation framework.

4.  **Model Evaluation and Interpretability:**
    *   **Stratified K-Fold Cross-Validation:** While `StratifiedKFold` is imported, the `GridSearchCV` uses `cv=5` (default KFold). The custom CV strategy (e.g., `cv=cv`) is not consistently passed to all evaluation steps. The prompt emphasizes ""extensive model evaluation with stratified k-fold cross-validation.""
    *   **Learning/Validation Curves:** These are correctly attempted but might face issues due to the non-pipeline preprocessing approach. `validation_curve` needs a `param_name` that exists within the model/pipeline, and the current structure might break this.
    *   **SHAP/LIME:**
        *   **SHAP:** The generated code uses `shap.TreeExplainer` for the ""best model,"" but the best model might not be tree-based (e.g., SVM or MLP). The ground truth correctly applies `TreeExplainer` specifically to XGBoost and uses `shap.force_plot` for single instance explanation, which is missing.
        *   **LIME:** There's no LIME implementation in the generated code at all, which was a specific requirement for model interpretability.
    *   **Performance Metrics & Statistical Significance:** While `roc_auc_score` is used, the statistical significance testing (McNemar's test) is implemented incorrectly by using `confusion_matrix(base_predictions == y_test, curr_predictions == y_test)`. McNemar's test compares the correct/incorrect predictions of two *different* classifiers on the *same* examples, specifically focusing on disagreements. The conversion to `True`/`False` for the confusion matrix input is not how McNemar's test is typically applied for comparing models. A paired t-test on cross-validation scores (as in the ground truth) is more appropriate for comparing overall model performance across folds.

5.  **Code Quality & Structure:**
    *   **Function-based Approach:** While separating logic into functions is good, the global `X_train_engineered`, `X_test_engineered` approach breaks reproducibility if functions are called out of order or if `engineer_features` mutates `X_train_scaled`/`X_test_scaled` instead of returning a new array, leading to unexpected side effects. A robust pipeline ensures immutability and reproducibility.
    *   **Missing Imports:** `sklearn.compose.ColumnTransformer` and `lime` are missing. `warnings` and `seaborn` are also missing, which impacts some ground truth functionalities like `warnings.filterwarnings` and plotting.
    *   **Lack of Readability:** The code has a large `main` function that orchestrates everything, making it less modular and harder to debug compared to the ground truth's more structured approach of defining pipelines and then orchestrating within the `main` script.

The generated code attempts to hit many points but lacks the depth, correctness, and robust structure found in the ground truth implementation, particularly concerning advanced preprocessing and proper pipeline integration for both tuning and model interpretability.",claude-3.5-sonnet,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load data
df = pd.read_csv('imdb_top_250.csv')

# Define features and target
X = df[['Year', 'Runtime']]
y = df['Rating']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Make pr...",7,"The generated code correctly implements the core components of the task: loading data, defining features and target, splitting data, training a Linear Regression model, and evaluating with R2 and MAE. The imports are correct, and the structure is logical.

However, there's a significant missing piece regarding data preparation, specifically for the 'Runtime' column. The `imdb_top_250.csv` dataset typically has 'Runtime' in a format like '120 min'. The ground truth code handles this by removing ' min' and converting to numeric. The generated code directly tries to use 'Runtime' without this crucial preprocessing, which would likely lead to an error if `pd.to_numeric` is not implicitly handling it (or `read_csv` if it's smart enough, but usually it isn't for mixed types) or incorrect results if it just coerces to NaN. It also lacks any robust error handling for file loading or missing columns.

To achieve a higher score, the code needs to include:
1.  **Robust data cleaning for 'Runtime'**: Convert 'X min' format to numeric.
2.  **Error Handling**: Add `try-except` for file loading and check for column existence.
3.  **Handling of non-numeric data/Na NaNs**: The current code just assigns the columns, but if 'Year' or 'Rating' (though unlikely for these) or 'Runtime' have non-numeric values, `LinearRegression` will fail. The ground truth explicitly uses `pd.to_numeric(errors='coerce')` and `dropna`.",claude-3.5-sonnet,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Load data
df = pd.read_csv('imdb_top_250.csv')

# Select features
X = df[['Rating', 'Runtime']]

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Perform K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# Create scatter plot
plt.figure(figsize=(1...",6,"The generated code performs most of the required steps correctly. It loads the dataset, selects features, standardizes them, applies K-Means with the correct parameters, creates a scatter plot, and calculates cluster means.

However, there are a few critical omissions and issues that prevent a higher score:
1.  **Missing Data Cleaning for 'Runtime'**: The most significant issue is the complete lack of data cleaning for the 'Runtime' column. The `imdb_top_250.csv` dataset typically has 'Runtime' in a string format like '142 min'. The generated code directly selects this column, which will cause `StandardScaler` or K-Means to fail if it's not numeric, or produce incorrect results if pandas manages to convert parts of it but not all. The Ground Truth proactively handles this by removing ' min' and converting to numeric, including error handling for `NaN` values. This is a common real-world data science task requirement.
2.  **Lack of Robustness**: No error handling for `FileNotFoundError` for the CSV, or for missing 'Rating'/'Runtime' columns, and no handling for `NaN` values in 'Rating' or 'Runtime' after the *assumed* (but not performed) numeric conversion.
3.  **Plotting Original Features**: While the scatter plot uses `df['Runtime']` and `df['Rating']`, which are the original non-standardized features, the previous steps don't ensure these are clean numeric columns at that point due to the lack of cleaning.

To improve, the code needs to incorporate robust data loading and cleaning, specifically for the 'Runtime' column to ensure it's numeric before standardization and plotting.",claude-3.5-sonnet,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import nltk

# Download stopwords if needed
try:
    stopwords.words('english')
except LookupError:
    nltk.download('stopwords')
    nltk.download('punkt')

# Load dataset
df = pd.read_csv('imdb_top_250.csv')

# Create title_length feature
df['title_length'] = df['Title'].str.len()

# Calculate correlation
correlation = df['title_length'].corr(df...",7,"The generated code addresses most of the task requirements but has a few shortcomings.

**Strengths:**
*   Correctly loads the dataset.
*   Successfully creates the `title_length` feature.
*   Calculates and prints the correlation coefficient (though it uses `pd.DataFrame.corr()` which uses Pearson by default, it doesn't explicitly specify it and lacks robustness for non-numeric or missing `Rating` values).
*   Correctly performs lowercase conversion and tokenization.
*   Effectively removes stopwords.
*   Accurately finds and prints the 10 most common words with their frequencies.
*   Handles NLTK downloads for both `stopwords` and `punkt`.

**Areas for Improvement:**
*   **Punctuation Removal:** The method `title = ''.join(char for char in title if char not in string.punctuation)` is generally fine for standard ASCII punctuation but might not be as robust as a regex-based approach (`re.sub`) for all non-alphanumeric characters or more complex text. The prompt specified ""characters that are not alphanumeric or whitespace,"" which this method doesn't fully cover (e.g., it removes '!', but doesn't handle special Unicode characters if they were present, nor does it explicitly keep whitespace). The Ground Truth uses `re.sub(r'[^a-z0-9\s]', '', text)` which is more precise for the given requirement.
*   **Correlation Calculation Robustness:** The code calculates `df['title_length'].corr(df['Rating'])` directly. It does not handle potential `NaN` values in the `Rating` column if it's not purely numeric, nor does it explicitly convert `Rating` to numeric with `errors='coerce'` as a best practice before calculating correlation, which the Ground Truth does. This can lead to errors or incorrect correlations if the `Rating` column contains non-numeric entries.
*   **Separate Processing Steps:** The prompt asked for ""Preprocess all movie titles: convert to lowercase, remove punctuation... and tokenize into words."" followed by ""Remove common English stop words from the tokenized titles."" The generated code combines all these steps within a single `preprocess_title` function and then applies it, which is efficient but makes individual step verification or reuse slightly less modular compared to the Ground Truth's more explicit, chained DataFrame operations (e.g., `df['processed_title'] = ...`, `df['tokenized_title'] = ...`, `df['filtered_title'] = ...`). While not strictly incorrect, the Ground Truth more directly reflects the sequential steps specified in the prompt.
*   **Output Format for Correlation:** The printed correlation output is ""Correlation between title length and rating: X.XXX"". The prompt didn't strictly define the output format, but the Ground Truth uses ""Pearson correlation between title\_length and Rating: X.XXXX"", which is more precise in its description and uses 4 decimal places.

To achieve a higher score, the generated code should incorporate robust handling for the `Rating` column (conversion to numeric, handling NaNs) and improve the punctuation removal to align more precisely with the ""not alphanumeric or whitespace"" requirement, possibly using `re`.",claude-3.5-sonnet,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer

# Load data
df = pd.read_csv('penguins.csv')

# Split features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 
        'body_mass_g', 'island', 'sex']]
y = df['species']

# Separate numerical and categorical co...",8,"The generated code addresses most of the requirements correctly.

**Strong points:**
*   Correctly loads the dataset and defines features and target.
*   Correctly uses `SimpleImputer` with 'median' strategy for numerical features.
*   Correctly uses `SimpleImputer` with 'most_frequent' strategy for categorical features.
*   Correctly performs one-hot encoding using `pd.get_dummies`.
*   Correctly scales numerical features using `StandardScaler`.
*   Correctly handles the target variable (`species`) with `LabelEncoder`.
*   Correctly trains a `RandomForestClassifier` and evaluates with 5-fold cross-validation, printing the mean accuracy.

**Areas for improvement:**
*   **Imputation of `island` with all NA values:** The prompt specifically stated: ""(Note: if a row still has NA in `island` after mode imputation, e.g. if all were NA, it can be dropped, or a placeholder like 'Unknown' can be used if mode is NA itself. For this task, if mode imputation results in NA for island, drop those rows)."" The generated code's `if all values are NA` check for `island` is generic (`X.dropna(subset=['island'])`) and might not strictly adhere to the nuances of 'if mode imputation results in NA for island'. While `SimpleImputer(strategy='most_frequent')` will generally handle this by dropping the column if all values are NA, the explicit handling of the edge case for `island` when *all* values are missing or the *mode itself is NA* was a specific prompt requirement that could be more robustly implemented as in the ground truth. The `X.dropna(subset=['island'])` after imputation is a good general safeguard, but it doesn't explicitly check for the `mode` being NA.
*   **Target NA handling:** The prompt doesn't explicitly ask for handling NAs in the target variable (`species`), but it's good practice as performed in the ground truth code. The generated code implicitly handles this because `LabelEncoder` will fail if there are NaNs, or the `fit_transform` will error out if `y` contains NAs. This is a minor point, but it's robustness.
*   **Use of `.copy()`:** The generated code does not use `.copy()` when creating `X` from `df`, which can lead to `SettingWithCopyWarning` if subsequent operations modify `X` in a way that Pandas thinks might affect `df`. While not critical for this specific example because it eventually reassigns `X`, it's generally good practice to explicitly copy.
*   **Printing format:** The prompt asked for `:.4f` for the accuracy, while the generated code uses `:.3f`. This is a minor stylistic point.

Overall, the code is very close to excellent and performs the main task accurately. The primary deduction is for the slightly less robust handling of the specific `island` NA scenario compared to the ground truth and the `SettingWithCopyWarning` potential.",claude-3.5-sonnet,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Load and prepare data
df = pd.read_csv('penguins.csv')
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
X = df[features].dropna()
species = df['species'][X.index]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# K-Mea...",7,"The generated code correctly performs the K-Means clustering and PCA as requested. It loads the dataset, selects the features, handles missing values (though the implementation of this step is less robust than the ground truth), standardizes the data, applies K-Means and PCA, and generates two plots.

However, there are a few areas for improvement:

1.  **NA Handling and Species Alignment**: The line `X = df[features].dropna()` correctly drops rows with NA in the selected features. However, `species = df['species'][X.index]` relies on `X.index` which is an `Int64Index` and assumes the original `df['species']` has not been re-indexed. This works here because `dropna` on a subset preserves the original index, but it's less explicit and potentially fragile compared to creating a cleaned DataFrame that includes `species` from the start, as done in the ground truth. The ground truth's approach of `df_cleaned = df_selected.dropna(subset=selected_features)` ensures all subsequent operations (including `species` retrieval) are perfectly aligned to the cleaned data.
2.  **Plotting**:
    *   The plots are generated as subplots within a single figure, which is acceptable but the request specifically asked for ""Create a scatter plot... Add a title to this plot"" and ""Create a *second* scatter plot... Add a title to this plot *for comparison*."" This phrasing usually implies two separate figures, or at least two distinct plots that can be easily compared. The ground truth's solution of using `plt.show()` after each plot block is a clearer interpretation of ""create a second scatter plot"". While `subplot` is technically `a second scatter plot`, the user's intent might be two distinct visual outputs.
    *   The `hue` parameter in `sns.scatterplot` is a much cleaner and more expressive way to handle coloring by categories (K-Means labels or species) and automatically creates a legend. `plt.scatter` with `c=labels` is functional but less feature-rich for categorical plotting without manual legend creation.
    *   The second plot uses `c=pd.factorize(species)[0]` and `cmap='viridis'`. While `factorize` correctly turns string labels into numerical ones, 'viridis' is a sequential colormap. For categorical data like species, a qualitative (distinct) colormap like 'tab10' (used by the ground truth) or similar is more appropriate for visual distinction.
3.  **Clarity and Readability**: The ground truth explicitly creates a `pca_df` DataFrame, adding `KMeans_Cluster` and `Original_Species` columns. This makes the plotting code very readable and less prone to indexing issues (`X_pca[:, 0]`, `X_pca[:, 1]` vs `pca_df['PC1']`, `pca_df['PC2']`).
4.  **Imports**: While `import numpy as np` is present, it's not strictly used in the final code. It's a minor point but indicates a slightly less refined set of imports.

Overall, the code is correct in its core functionality but could be improved in terms of robustness, best practices for plotting categorical data, and overall code clarity/readability, especially regarding data alignment after NA handling.",claude-3.5-sonnet,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('penguins.csv')

# Drop rows where island is NA
df = df.dropna(subset=['island'])

# Impute numeric columns with mean
numeric_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numeric_cols:
    df[col] = df[col].fillna(d...",7,"The generated code correctly addresses most of the requirements.

**Strong points:**
*   **Data Loading and Imputation:** Correctly loads the data, drops rows where `island` is NA, imputes numerical columns with their mean, and `sex` with its mode.
*   **Target Encoding:** Correctly label encodes the `species` column.
*   **Gradient Boosting Model:** Successfully trains a `GradientBoostingClassifier` with the specified parameters.
*   **Feature Importance:** Extracts and prints feature importances, associating them with feature names and sorting them, which is good for readability.

**Areas for Improvement (Why it didn't get a higher score):**
*   **Feature Definitions (`X`) Incorrect:** The most significant issue is in the definition of features `X`. The task explicitly states: ""Define features `X` using the four imputed numerical measurements, plus one-hot encoded `island` and one-hot encoded imputed `sex`."" The generated code uses `pd.get_dummies(df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']])`. This applies one-hot encoding to *all* these columns, including the numerical ones, which is incorrect. The numerical columns should be kept as numerical and concatenated with the one-hot encoded categorical columns. This fundamentally changes the feature set used for training the model and for calculating feature importances.
*   **Box Plot Structure:** The box plot requirement is: ""For each `species`, create box plots to visualize the distribution of `flipper_length_mm` (using imputed data) grouped by the imputed `sex` column."" The generated code creates separate subplots for each species, which is acceptable, but the ground truth's use of `hue='sex'` in a single `sns.boxplot` call against `x='species'` is generally more direct and provides a cleaner comparison across species on one plot, while still showing the grouping by sex. The generated code also hardcodes `plt.subplot(1, 3, i+1)`, which assumes there are exactly 3 unique species. While true for this dataset, it's not robust if the number of species changes. A more robust solution would dynamically adjust subplot layout (e.g., `nrows = (len(species_unique) + ncols - 1) // ncols`).

**Minor Points (Not impacting score significantly but good practice):**
*   **File Not Found Handling:** The generated code lacks a `try-except` block for `pd.read_csv`, which is good practice for robustness in case the file is missing.
*   **Plot Titles/Labels:** While individual subplot titles are present, a single main title for the collective figure would improve clarity. The `species_data` subsetting for plotting is fine, but using `hue` as in the ground truth is often more efficient.",claude-3.5-sonnet,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer

# Load the dataset
df = pd.read_csv('penguins.csv')

# Separate numerical and categorical columns
num_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
cat_cols = ['sex']

# KNN imputation for numerical columns
imputer = KNNImputer(n_neighbors=3)
df[num_cols] = imputer.fit_transform(df[num_cols])

# Mode imputation for sex column 
df['sex'] = df['sex'].fillna(df['sex'].mode()[...",9,"The generated code correctly addresses all parts of the task: loading the dataset, applying KNN imputation to the specified numerical columns, imputing the 'sex' column with its mode, and printing the remaining NA values.

**Strong Points:**
- Correctly uses `KNNImputer` for numerical columns with `n_neighbors=3`.
- Correctly uses `fillna` with `mode()[0]` for the 'sex' column.
- Imports are correct and necessary.
- The output format for NA counts is clear.

**Areas for Minor Improvement (to reach a perfect 10):**
- **Error Handling for File Not Found:** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code assumes the file exists. While the prompt states to assume the CSV is in the current directory, robust code would include this.
- **Informative Print Statements:** The ground truth provides more verbose print statements (e.g., ""Dataset loaded successfully."", ""KNN imputation applied to numerical columns."") which makes the execution flow clearer to the user. The generated code is concise but less informative during execution.
- **Verification of No NAs:** The final verification step in the ground truth explicitly checks NA counts for the *imputed* columns, which is a very clear way to confirm the imputation worked as expected. The generated code just prints all NA counts, which is sufficient but less direct in its verification than the ground truth's approach.

Overall, it's a very good solution that flawlessly completes the core requirements of the task. The minor points are about robustness and verbosity, not fundamental correctness.",claude-3.5-sonnet,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Load data
df = pd.read_csv('penguins.csv')

# Filter for Adelie and Gentoo species
df_filtered = df[df['species'].isin(['Adelie', 'Gentoo'])]

# Drop NA values in culmen_length_mm column
df_clean = df_filtered.dropna(subset=['culmen_length_mm'])

# Get culmen lengths for each species
adelie_lengths = df_clean[df_clean['species'] == 'Adelie']['culmen_length_mm']
gentoo_lengths = df_clean[df_clean['species'] == 'Gentoo']['culmen_length_mm']
...",9,"The generated code correctly performs all the required steps: loading data, filtering, handling NAs, performing the t-test, and printing the results with interpretation.

**Strong Points:**
- **Correctness:** All logical steps (load, filter, NA handling, t-test, interpretation) are executed accurately.
- **Conciseness:** The code is quite concise and readable, directly addressing the prompt's requirements.
- **Proper Use of Libraries:** `pandas` for data manipulation and `scipy.stats` for the t-test are used correctly.
- **Interpretation:** The conclusion based on the p-value and significance level of 0.05 is correct.

**Areas for Minor Improvement (why not a 10):**
- **Error Handling:** The ground truth includes `try-except` blocks for file loading, which is good practice for robustness. The generated code assumes the file exists.
- **Readability/Formatting of P-value:** The primary `print` for the p-value uses `:.4e` which outputs in scientific notation (e.g., `1.2345e-10`). While numerically correct, using `:.4f` (like for the t-statistic) would make it more comparable to the alpha value for a human reader, especially since the ground truth also uses `:.4f` consistently. For very small p-values, scientific notation is appropriate, but for arbitrary p-values, `:.4f` is often clearer for direct comparison to `0.05`.
- **Modularity:** The ground truth wraps the logic in a function, `perform_penguin_culmen_ttest`, which makes the code reusable and more organized, especially in larger projects. For a simple script, direct execution is fine, but a function is generally better practice.
- **Comments/Docstrings:** The ground truth includes comments for each step and a docstring for the function, enhancing explainability. While the generated code is clear enough for this specific task, adding comments explaining the purpose of each step can improve maintainability.",claude-3.5-sonnet,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.impute import SimpleImputer

# Load data
df = pd.read_csv('penguins.csv')

# Separate features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 
        'body_mass_g', 'island', 'sex']]
y = ...",9,"The generated code is very good and largely achieves all the requirements.

**Strong Points:**
*   **Correctness:** It correctly loads the data, defines features and target, performs the specified preprocessing steps (median imputation for numerical, mode imputation for 'sex', one-hot encoding for 'island' and 'sex'), label encodes the target, splits the data stratifying by `species`, trains a `RandomForestClassifier` with the specified parameters, makes predictions, and prints the accuracy and classification report.
*   **Completeness:** All explicit requirements from the task description are met.
*   **Clarity:** The code is well-structured and easy to understand. Variable names are appropriate.

**Minor Areas for Improvement/Comparison with Ground Truth:**
*   **Imputation of numerical features:** The line `X[num_features] = num_imputer.fit_transform(X[num_features])` for numerical imputation works but copies `X[num_features]` before imputation. A more robust way (especially if `X` wasn't a copy initially) would be `X.loc[:, num_features] = num_imputer.fit_transform(X[num_features])` to avoid `SettingWithCopyWarning` in more complex scenarios. However, in this specific case, `X` is already a copy, so it works.
*   **Imputation of 'sex':** While `X['sex'].fillna(X['sex'].mode()[0])` directly imputes `sex` with the mode, the ground truth uses `SimpleImputer(strategy='most_frequent')` within a `Pipeline`/`ColumnTransformer`. Both achieve the same result for this specific case, but using `SimpleImputer` is more aligned with standard `sklearn` preprocessing practices and can be more easily integrated into a robust pipeline for multiple categorical features if needed.
*   **Pipeline Usage:** The ground truth code uses `ColumnTransformer` and `Pipeline` for preprocessing and model training. While the generated code gets the job done correctly, using `Pipeline` is generally considered a best practice in `sklearn` for several reasons:
    *   **Prevents Data Leakage:** Ensures that preprocessing steps (like imputation and scaling) learned only from the training data are applied to the test data. The generated code applies `fit_transform` on `X` (the full dataset) for numerical imputation and `fillna` on `X` for sex imputation *before* the train-test split, which is a subtle form of data leakage. While for median/mode imputation it might not have a drastic impact, it's not ideal practice.
    *   **Reproducibility and Maintainability:** Makes it easier to reproduce the entire machine learning workflow.
    *   **Cleaner Code:** Chaining steps simplifies the process.
    The generated code handles `fit_transform` before splitting which works because it only uses `fit` and `transform` on the entire dataset for `SimpleImputer` and `fillna` (which doesn't 'learn' from the data in the same way, but still operates on the full `X`). For `LabelEncoder` on `y`, it's generally fine to fit on the full `y` as it's just mapping categories.

Despite the point about potential data leakage with the imputation being performed on the full `X` before the split (which is a common mistake but doesn't lead to incorrect results for `median` and `most_frequent` strategies in this specific context), the code perfectly satisfies all explicit requirements of the prompt and produces the correct output. The choice not to use `ColumnTransformer` and `Pipeline` might be a stylistic difference or a simplification, but it doesn't lead to incorrect results for the given task requirements.",claude-3.5-sonnet,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4], 
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

corr_years = df[""PerformanceScore""].corr(df[""YearsExper...",8,"The generated code correctly defines the DataFrame, calculates the specified Pearson correlation coefficients, and prints them. It also correctly identifies which feature has a stronger absolute correlation with ""PerformanceScore"".

**Strong Points:**
*   **Correct DataFrame Creation:** The DataFrame is precisely as specified.
*   **Correct Correlation Calculation:** It correctly uses `df['Series1'].corr(df['Series2'])` which defaults to Pearson correlation.
*   **Correct Printing of Correlations:** The correlation values are printed as requested.
*   **Correct Absolute Correlation Comparison:** It correctly uses `abs()` to compare the strength of correlations, which is crucial.

**Areas for Improvement:**
*   **Specificity in Printing:** The print statements for the correlations could be more explicit, e.g., ""Pearson correlation between PerformanceScore and YearsExperience:"" instead of ""Correlation with Years Experience:"". While not a critical error, it improves clarity.
*   **Tie-breaking Condition:** The `else` block only covers the case where `abs(corr_training)` is greater than or equal to `abs(corr_years)`. It would be slightly more robust to explicitly handle the `elif abs(corr_training) > abs(corr_years):` and an `else:` for when they are exactly equal, as shown in the ground truth. This is a minor edge case but demonstrates complete logic.
*   **Precision in Output:** The ground truth formats the output to 4 decimal places (`:.4f`), while the generated code uses 3 (`:.3f`). Both are acceptable for printing, but consistency or explicit requirement handling would be better.

Overall, the code is highly functional and addresses almost all aspects of the task effectively. The minor points are mostly about refinement and handling of edge cases for even greater robustness.",claude-3.5-sonnet,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test
from scipy import stats
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
import pymc3 as pm
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.matching import MatchingEstimator

#...",3,"The generated code attempts to address several requirements but falls short in critical areas, leading to incorrect or incomplete implementations of advanced statistical methods.

**Major Issues:**

1.  **Fundamental Misinterpretation of Survival Analysis (Kaplan-Meier & CoxPH):** The most egregious error is `df['duration'] = 1`. This completely invalidates the core premise of survival analysis, which relies on *varying* time-to-event data. By setting duration to 1 for all observations, Kaplan-Meier curves become meaningless (a single step at time=1), and the Cox APH model effectively reduces to a logistic regression on the binary outcome 'Survived', making the survival analysis components void of their intended statistical rigor. This is a critical failure.

2.  **Incorrect Kaplan-Meier Usage:** Due to `duration=1`, the KM plots are just single points at time 1, providing no meaningful survival curve over time. The interpretation is completely lost. Also, `event_col='Survived'` implies 'Survived' is the event indicator, but in standard survival analysis, 1 usually means the event occurred (e.g., death), and 0 means censored/survived. The prompt explicitly states ""survival as event indicator"", which might be interpreted as 1 for survival. However, in CoxPH, `event_col` typically indicates the *occurrence* of the event (e.g., death or failure), thus `Survived=0` for death would be the correct event indicator if `event_col=E` is used. The ground truth correctly uses `E = 1 - df_encoded['Survived']` for death as an event. The generated code uses `df['Survived']` directly, which means 'survival' is treated as the event. This is generally the inverse of how survival models are typically applied.

3.  **Incomplete/Incorrect CoxPH Model Diagnostics:** While `cph.check_assumptions()` is called, it won't yield meaningful results with a constant duration of 1, as noted in the ground truth. It also lacks deeper diagnostics like testing for interaction effects within CoxPH.

4.  **Flawed Propensity Score Matching:**
    *   It uses `sklearn.matching.MatchingEstimator`, which is not a standard scikit-learn module. This class appears to be a custom or deprecated one, leading to ""module 'sklearn.matching' has no attribute 'MatchingEstimator'"". This portion of the code is non-functional.
    *   Even if a matching estimator were available, the implementation doesn't perform standardization of covariates, or balance checks (like SMD), which are crucial for valid PSM.
    *   It defines `treatment = df['Pclass'] == 1` but does not specify a control group clearly (e.g., Pclass=3 as in the ground truth).

5.  **Simplified Bootstrap Analysis:** The bootstrap analysis is performed on a `sm.Logit` model, not the CoxPH model or other survival analysis metrics (like C-index or hazard ratios), which were explicitly asked for. It calculates CIs for logistic regression coefficients, which is tangential to the survival analysis focus.

6.  **Oversimplified Bayesian Analysis:**
    *   The Bayesian model uses PyMC3 for a logistic regression, but it's not directly a ""probabilistic modeling of survival factors"" in the sense of time-to-event data. It's a binary outcome model on survival.
    *   The priors `sd=10` are quite broad and might not be optimal.
    *   `return_inferencedata=False` is deprecated.
    *   It lacks posterior predictive checks or evaluation of convergence diagnostics which are crucial for Bayesian analysis.
    *   It does not present a summary of the trace (e.g., `az.summary`) or plots, which are standard for PyMC3 outputs.

7.  **Missing or Inadequate Advanced Statistical Tests:**
    *   Multivariate analysis/interaction effects were partially attempted with `sm.Logit`, but not within the context of survival models where they are particularly relevant.
    *   The prompt asked for model diagnostics for *survival models*, not just logistic regression.
    *   Interaction effects testing within CoxPH (as in the ground truth) is missing.
    *   Only VIF is present, but missing other model diagnostics relevant to survival analysis.

8.  **No Clear Sensitivity Analysis for Survival Models:** The sensitivity analysis focuses on `sm.Logit` models with different features, not on the survival models (CoxPH or Bayesian survival) or critical assumptions (like imputation strategy, or prior sensitivity for Bayesian survival models) as requested.

9.  **Import Issues:** `sklearn.matching` is not a standard module, leading to an ImportError.

**Score Justification:**

The code demonstrates an understanding of the components requested (Kaplan-Meier, CoxPH, PSM, Bootstrap, Bayesian), but the implementation quality is very low due to fundamental misunderstandings of survival analysis and incorrect use of libraries or oversimplifications that negate the intent of the prompt's requirements. The `duration=1` issue is a critical flaw that renders the core survival analysis section unusable. Several advanced methods are either incorrectly implemented, non-functional, or applied to simpler non-survival models, failing to meet the ""advanced statistical modeling and survival analysis"" requirement.",claude-3.5-sonnet,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Create DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
        'Final_Score': [90, 88, 82, 92, ...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong Points:**
- **Correct DataFrame Creation:** The DataFrame is correctly initialized as per the task description.
- **Accurate Correlation Calculation:** The Pearson correlation coefficient is calculated and printed correctly.
- **Effective Scatter Plot:** The scatter plot correctly uses 'Attendance_Rate' on the x-axis and 'Final_Score' on the y-axis. It successfully colors points by 'Subject' and includes a legend, title, and axis labels. The use of `plt.scatter` in a loop iterating through unique subjects is a valid approach for coloring.
- **Correct Grade Assignment:** The `Overall_Grade` column is correctly assigned based on the `Final_Score` using a function and `apply()`.
- **Accurate Grade Distribution:** The count of students in each grade category is correctly calculated and printed, sorted by index (which produces A, B, C, D order).
- **Necessary Imports:** All required libraries (`pandas`, `matplotlib.pyplot`, `seaborn`, `scipy.stats`) are correctly imported.

**Areas for Minor Improvement (for a perfect 10):**
- **Plot method for coloring:** While the loop-based plotting is functional, using `seaborn.scatterplot` with the `hue` parameter (as in the ground truth) is often more concise and robust for this type of plot, and it handles the legend setup automatically. The prompt explicitly mentioned `seaborn for potentially better aesthetics but not required for coloring`, so the current implementation is acceptable but a common improvement point would be `seaborn.scatterplot(..., hue='Subject')`.
- **Formatting of correlation output:** The ground truth used `.4f` for formatting the correlation, while the generated code used `.3f`. This is a minor stylistic difference, but consistency with the ground truth's level of precision would be ideal if aiming for identical output.
- **No explicit print statements for task steps:** While the code's output is correct, the ground truth included descriptive print statements (e.g., ""--- Creating DataFrame ---"") which makes the execution flow clearer when running the script. This isn't a functional requirement but contributes to user-friendliness of the script's output.",claude-3.5-sonnet,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']),
        'Ticker': ['TECHCO']*10,
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0],
        'Low': [149.5, 150.5, 150.0...",10,"The generated code successfully addresses all aspects of the task. It correctly creates the DataFrame, calculates daily returns, 3-day and 5-day simple moving averages. The plotting section is accurate, displaying all required lines with proper labels, title, and legend. Crucially, it correctly identifies and prints the dates for bullish crossovers by checking the conditions for an actual crossover (current day SMA_3Day > SMA_5Day AND previous day SMA_3Day <= SMA_5Day). The code is well-structured and uses appropriate pandas and matplotlib functions efficiently. The output format for the bullish crossover dates is also good.",claude-3.5-sonnet,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUST002',...",9,"The generated code correctly addresses all parts of the task. It successfully creates the DataFrame, calculates total sales revenue per product, generates and displays the horizontal bar chart, calculates the average order value, and identifies repeat customers. The imports are correct, and the code structure is logical.

Minor areas for improvement:
1.  **Readability of product_revenue sort:** The `product_revenue` is sorted in ascending order (`.sort_values()`), which means the smallest revenue items will be at the top of the bar chart. While not strictly incorrect, it's generally more intuitive for bar charts showing totals like this to be sorted in descending order to easily see the top performers. The Ground Truth uses `ascending=False`.
2.  **Output formatting for repeat customers:** While the repeat customers are correctly identified, the output is a pandas Series. The Ground Truth provides a more human-readable formatted printout, including a header, which improves the clarity for a user seeing the results.
3.  **Missing print statements for DataFrame and intermediate calculations:** The Ground Truth includes print statements for the initial DataFrame and the `product_revenue` Series, which can be useful for debugging and understanding the intermediate steps. This is a minor point but adds to the completeness of the output.
4. **Plot customization:** The plot is functional but could benefit from a chosen color (like `color='skyblue'` in the ground truth) and a grid for better readability. These are minor stylistic choices.",claude-3.5-sonnet,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd

# Create original DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Create DeviceType_Numeric feature
df['DeviceType_Numeric'] = (df['DeviceType'] == 'Mobile').astyp...",8,"The generated code correctly defines the DataFrame and performs the required feature engineering steps.

**Strengths:**
- **Correct DataFrame Initialization:** The initial DataFrame is created exactly as specified.
- **Accurate Feature Creation:**
    - `DeviceType_Numeric` is correctly mapped using a concise boolean-to-integer conversion.
    - `PagesPerMinute` is calculated correctly.
    - The division by zero handling for `PagesPerMinute` is almost entirely correct, replacing `np.inf`, `-np.inf`, and `np.nan` with 0 in a single line.
- **Correct Feature Selection:** The final set of features for `X` (named `features` in the generated code) is correctly selected.
- **Correct Output:** The head of the feature DataFrame is printed as required.

**Areas for Improvement (why not a 10):**
- **Missing `numpy` Import:** The generated code uses `np.inf` and `np.nan` without importing `numpy`. This would lead to a `NameError`. While the intent is clear, the code as provided is not runnable without this import.
- **Readability/Explicitness for `DeviceType_Numeric`:** While `(df['DeviceType'] == 'Mobile').astype(int)` is functional and concise, the ground truth's approach using `map` with an explicit dictionary `{""Desktop"": 0, ""Mobile"": 1}` can sometimes be more readable and maintainable for more complex mappings, and explicitly shows the mapping for both values. For this particular binary case, both are acceptable, but the explicit mapping can be a slight edge in clarity.
- **Redundant `np.nan` in `replace` for `PagesPerMinute` (minor):** If `np.inf` and `-np.inf` are replaced with `np.nan` first, then `np.nan` is replaced with 0, it's a two-step process to ensure all problematic values become 0. The generated code's single line `replace([np.inf, -np.inf, np.nan], 0)` is efficient and handles all cases directly, though if there were existing NaNs *before* the division, they would also be caught here. The ground truth's explicit `fillna` after `replace` can sometimes be marginally clearer about intent, but the generated code's approach is perfectly valid and concise for this scenario where NaNs are primarily generated by the division itself.

Overall, the generated code is very good, with the main missing piece being the `numpy` import, which is a critical detail for execution.",claude-3.5-sonnet,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd
import numpy as np

# Create DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, ...",9,"The generated code correctly defines the DataFrame, calculates the mean and standard deviation for 'TemperatureCelsius', identifies outliers based on the 1.5 standard deviation threshold, and prints the required details for each outlier. The logic `abs(df[""TemperatureCelsius""] - temp_mean) > 1.5 * temp_std` is a concise and correct way to identify values more than 1.5 standard deviations away from the mean in either direction.

One minor improvement would be to add a check for `outliers.empty` before iterating, similar to the ground truth. This makes the output more user-friendly if no outliers are found but isn't strictly necessary for correctness, as an empty `iterrows()` loop will simply not execute. Also, the `import numpy as np` is not strictly needed as `pandas` handles `mean()` and `std()` internally. Formatting the temperature to one decimal place as in the ground truth is a nice-to-have but not a core requirement.",claude-3.5-sonnet,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [10, 5, 25, 15, 8, 30, 9, 20, 6]
}
...",10,"The generated code correctly defines the DataFrame, calculates the `EngagementScore`, and accurately identifies the `PostType` with the highest average `EngagementScore` for each `Platform`. The method `result.loc[result.groupby('Platform')['EngagementScore'].idxmax()]` is an elegant and efficient way to achieve the desired grouping and selection, making the code concise and readable. The formatting of the output also matches the requirement of displaying the score with two decimal places. This solution is efficient, correct, and complete.",claude-3.5-sonnet,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]), 
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu"", ""Injury"",...",9,"The generated code correctly performs all requested operations: it defines the DataFrame, calculates LengthOfStay, computes average LengthOfStay and patient count per diagnosis, and identifies the diagnosis with the longest average stay.

**Strong points:**
*   **Correctness:** All calculations are accurate and the final results match the expected output.
*   **Conciseness:** The code is more concise than the ground truth while still being readable. Using `.agg()` with a dictionary and `rename()` is an efficient way to achieve the desired aggregations and column names.
*   **Clarity:** Variable names are descriptive (`diagnosis_stats`, `longest_stay`).

**Areas for minor improvement:**
*   **Printing requirements:** The task explicitly asked to ""Print this information (Diagnosis, Average LengthOfStay, Patient Count)"". While `print(diagnosis_stats)` does output this, it doesn't format it exactly as specified in the ground truth (e.g., ""Diagnosis, Average LengthOfStay, Patient Count"" as column headers with specific formatting, or iterating through and printing each row with explicit labels like ""days""). The current output is a pandas DataFrame representation, which is generally acceptable and often preferred in data science, but it deviates slightly from the literal interpretation of the prompt's printing format.
*   **Function encapsulation:** The ground truth encapsulates the logic within a function, which is good practice for reusability and organization, especially in larger scripts. The generated code is top-level script, which is fine for small tasks but less modular.
*   **Output detail for longest stay:** The ground truth prints both the diagnosis and its average length of stay. The generated code only prints the diagnosis. Adding the actual average length of stay would be a minor enhancement for completeness.

Overall, it's a very good solution that successfully completes the core requirements of the task efficiently and correctly. The minor points are more about stylistic preferences or slight deviations from explicit printing format requests rather than functional errors.",claude-3.5-sonnet,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",10,"The generated code perfectly addresses all aspects of the task. It correctly defines the DataFrame, calculates the ""WeightedRating"" precisely as specified, adds it as a new column, and accurately identifies and prints the ""Title"" and ""WeightedRating"" of the movie with the highest ""WeightedRating"". The code is concise and efficient, utilizing pandas' vectorized operations effectively for the calculation and a clear `loc` selection for the final output. It demonstrates a strong understanding of pandas functionalities required for the task.",claude-3.5-sonnet,LLM_judge_config
H58,hard,Data Aggregation,,"import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 10000, 8000]
}
df = pd.DataFrame...",9,"The generated code correctly defines the DataFrame, calculates the average ""PriceUSD"" and ""Mileage"" per brand, and presents this in a new grouped DataFrame. It uses `groupby()` followed by `agg()` which is an efficient and standard way to perform such operations in pandas. The use of `reset_index()` is good practice if the brand needs to be a column rather than the index, which is often the desired output for printing or further processing.

One minor point for improvement, especially if the task implied specific column names for the aggregated results (e.g., ""Average_PriceUSD"" as in the ground truth), is that the generated code implicitly names them ""PriceUSD"" and ""Mileage"". While this is perfectly functional, explicit naming via named aggregations in `agg()` can make the output more readable and less ambiguous. However, given the prompt did not explicitly ask for specific column names for the averages, the current naming is acceptable.
Also, the print statement could include a descriptive message, similar to the ground truth, for better user understanding. These are minor stylistic choices rather than correctness issues.",claude-3.5-sonnet,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63, 83],
    ""De...",9,"The generated code correctly performs all the required tasks:
1. Defines the DataFrame as specified.
2. Correctly adds the ""TotalStats"" column.
3. Identifies and prints the name of the Pokemon with the highest ""TotalStats"".
4. Calculates and prints the average ""Speed"" for each ""Type1"".

The code is concise and efficient, demonstrating good understanding of pandas operations.

**Areas for minor improvement (why not a 10):**
*   **Clarity of Output:** The ground truth code includes print statements that clearly label what each output represents (e.g., ""Original DataFrame:"", ""Pokmon with the highest 'TotalStats':""). The generated code simply prints the results without much context, which might be less user-friendly in a real application. While the prompt said ""Provide only Python code"" and didn't explicitly ask for output labels, adding them would improve the readability and interpretability of the results during execution.
*   **Comments:** While not strictly necessary for this small example, good commenting practices (like those in the ground truth) make code more maintainable and understandable, especially for complex operations or if shared with others.
Overall, a very strong solution that correctly addresses all core requirements.",claude-3.5-sonnet,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.manifold import TSNE
import umap
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.s...",6,"The generated code addresses many aspects of the prompt but lacks the depth and comprehensive implementation present in the ground truth.

**Strengths:**
*   **Attempts advanced feature engineering:** Includes `FamilySize`, `IsAlone`, `TicketGroup`, `Title` extraction, and a rudimentary social network feature (`NetworkDegree`) using `networkx`.
*   **Applies multiple clustering algorithms:** K-Means, Hierarchical, DBSCAN, Gaussian Mixture Models are used.
*   **Includes dimensionality reduction:** PCA, t-SNE, UMAP are applied for visualization.
*   **Analyzes survival patterns:** Plots survival rates per cluster.
*   **Attempts cluster profiling:** Creates a heatmap of feature means per cluster.
*   **Includes basic optimal cluster determination:** Uses silhouette and Calinski-Harabasz scores.

**Areas for Improvement:**
*   **Feature Engineering Depth:**
    *   **Social Network:** The `NetworkDegree` feature is quite basic. The ground truth's `TicketGroupSize` and `FarePerPerson` derived from `TicketGroupSize` are more directly applicable for socioeconomic analysis. The `networkx` use here is a good thought but needs more refined features (e.g., connected components, centrality).
    *   **Text-based features:** Missing `NameLength` and `TicketPrefix` from the ground truth.
    *   **Socioeconomic:** The `Title` handling is too simplistic (only `LabelEncoder`). The ground truth groups rare titles and handles Mlle/Ms/Mme effectively.
    *   **Interaction features:** Missing `Age_Class`, `Fare_Class`, `FarePerPerson_Class` which are useful interaction terms.
    *   **Cabin features:** Missing `CabinDeck` and `HasCabin`.
*   **Preprocessing:**
    *   **Imputation:** Only uses `fillna(X.mean())` which is not always appropriate (e.g., for `Embarked` or `Age` where median might be better, or mode for categorical). The ground truth uses `SimpleImputer` and `ColumnTransformer` which is a more robust and professional approach, handling numerical and categorical imputation separately.
    *   **Categorical Encoding:** Uses `LabelEncoder` for `Sex`, `Embarked`, `Title`. This imposes an arbitrary ordinality which is incorrect for nominal features and can negatively impact clustering. One-Hot Encoding (as in ground truth) is the correct approach here. The `ColumnTransformer` in the ground truth is also superior for managing different transformations.
*   **Optimal Cluster Determination:**
    *   Only uses silhouette and Calinski-Harabasz, missing Davies-Bouldin, Elbow Method (Inertia), and Gap Statistic (though Gap Statistic is not implemented in either directly, it was mentioned in the prompt).
    *   The method `get_optimal_clusters` only demonstrates it for K-Means, not for other algorithms as requested by ""multiple validation metrics for *multiple clustering algorithms*"".
*   **Missing Clustering Algorithms:** While it includes K-Means, Hierarchical, DBSCAN, GMM, it misses `AgglomerativeClustering` specifically mentioned in the ground truth and generally preferred for Hierarchical in sklearn.
*   **DBSCAN Parameter Tuning:** The `eps` and `min_samples` for DBSCAN are crucial and highly data-dependent; using fixed values (`0.5`, `5`) might not yield meaningful clusters. The ground truth correctly points this out, although it also uses fixed values for demonstration. The handling of DBSCAN noise points (-1) is also not as thorough in the analysis and visualization.
*   **Cluster Stability Analysis:** A crucial requirement (""Implement cluster stability analysis and consensus clustering"") is very weakly implemented. The ""consensus_matrix"" function provided is a simple co-occurrence matrix, *not* a method to derive ""consensus clusters"" (e.g., using hierarchical clustering on the consensus matrix, or more advanced ensemble methods). It doesn't perform ""stability analysis"" in the sense of repeat runs with different initializations and measuring ARI, which the ground truth does.
*   **Visualizations:**
    *   The plots for PCA, t-SNE, UMAP only show K-Means clusters; the prompt implies comprehensive visualizations *for each segment*, which should encompass all algorithms (as shown in the ground truth for each clustering algorithm).
    *   Missing visualizations like Violin plots for feature distributions by cluster for different features (Age, Fare, Pclass, Sex).
    *   The cluster profiles heatmap `sns.heatmap(cluster_profiles, annot=True, cmap='coolwarm', center=0)` is for the *mean* of original features, which might not be appropriate if features have widely different scales. The ground truth does this on *scaled* features for better comparison.
*   **External Validation:** While survival patterns are analyzed, the explicit ""external validation"" criteria using survival outcomes for each clustering algorithm (as distinct from just plotting survival rates) is less explicit and less thorough than the ground truth.
*   **Code Structure and Comments:** Lacks clear sectioning and detailed comments compared to the ground truth, making it harder to follow. `warnings.filterwarnings` and setting plot styles (dpi, etc.) are good practices missing.
*   **UMAP Import:** `import umap` is correct, but the ground truth `import umap.umap_ as umap` specifically handles the common `umap-learn` library structure.

Overall, the generated code performs a reasonable first pass at the problem, but significant improvements are needed in feature engineering, preprocessing robustness, the depth of cluster validation and analysis, and the implementation of stability/consensus clustering to meet the ""comprehensive"" and ""expert"" level implied by the prompt.",claude-3.5-sonnet,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dystopian""],
    ""Stock"": [...",9,"The generated code correctly defines the DataFrame, calculates the count of books and average price per author, and identifies the author with the most books in stock.

**Strong points:**
- **Correctness:** All three main requirements of the task (DataFrame creation, per-author stats, and author with most stock) are correctly implemented.
- **Conciseness:** The `agg` function with `rename` is a clean way to get the author statistics.
- **Readability:** The code is well-structured and easy to understand.

**Areas for improvement (minor):**
- **Output formatting for author statistics:** While `print(author_stats)` correctly displays the information, the ground truth uses a `for` loop with an f-string for more explicit and custom formatting (e.g., aligning columns, adding currency symbols, and a clear header). The generated code's output for author statistics is a pandas DataFrame representation, which is functional but less ""printed information"" and more ""raw data representation.""
- **Clarity of `author_stock` output:** The second print statement `print(f""\nAuthor with most books in stock: {max_stock_author} ({author_stock[max_stock_author]} books)"")` is good, but the ground truth separates the author name and the count, which can be slightly clearer.

Overall, the code is highly functional and accurate, meeting nearly all aspects of the prompt. The minor stylistic differences in printing output are the only reason for not giving a perfect 10.",claude-3.5-sonnet,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, learning_curve, validation_curve, StratifiedKFold
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classifi...",6,"The generated code attempts to address most requirements but falls short in implementation depth and correctness for several advanced elements.

**Strengths:**
*   **Data Preprocessing:** It includes multiple imputation strategies (KNN, Iterative, Simple) and outlier detection (Isolation Forest), which is good. It also includes polynomial feature engineering.
*   **Base Models & Optimization:** It correctly identifies and optimizes base models using GridSearchCV.
*   **Ensemble Methods:** It implements Voting and Stacking classifiers.
*   **Evaluation:** It includes basic model evaluation (classification report, ROC AUC), learning curves, SHAP (for a tree model), paired t-tests for statistical significance, and calibration curves.
*   **Dashboard:** It attempts to create a dashboard with ROC curves, feature importance, and performance metrics.

**Weaknesses and Areas for Improvement:**

1.  **Preprocessing Workflow (Correctness & Completeness):**
    *   **Multiple Imputation Strategies (Implementation):** The prompt asked for ""multiple imputation strategies,"" implying a method to combine results from multiple imputed datasets (e.g., following Rubin's rules). The generated code applies them sequentially or in isolation (`X_knn`, `X_iter`, `X_simple`) without clearly integrating them into a single, comprehensive preprocessing pipeline that handles the variations or combines their benefits. The `X_clean` is derived only from `X_knn`, negating the use of other imputation strategies.
    *   **Outlier Treatment Logic:** The `IsolationForest` is used for detection, but the ""treatment"" is simply removing outliers. The ground truth offers a more robust ""capping"" method, which is often preferable to outright removal as it retains more data.
    *   **Data Leakage in Preprocessing:** The `IsolationForest` and `PolynomialFeatures` are fitted on the *entire* dataset (`X_knn`, `X_clean`) before the train-test split. This is a significant data leakage issue. The scaler is correctly applied after the split, but preprocessing steps like imputation, outlier capping, and feature engineering should ideally be part of a `Pipeline` or applied only on the training data and then transformed on the test data. The ground truth addresses this by putting the whole preprocessing chain into a `ColumnTransformer` within a `Pipeline`.
    *   **Feature Names after PolyFeatures:** The `feature_names` variable in the generated code is used for SHAP, but `X_test_scaled` (a numpy array) is passed to SHAP, not a DataFrame with these names. This will cause SHAP plots to show generic feature names (e.g., f0, f1). The ground truth correctly converts processed arrays back to DataFrames with proper column names, enabling meaningful SHAP plots.

2.  **Base Model Optimization (Completeness):**
    *   **Hyperparameter Grids:** The grids provided are very limited for NN and SVM (they are completely missing parameters for NN and very basic for SVM `C` value, without `kernel`). Also, the generated code *doesn't optimize NN and SVM at all*; it only optimizes RF, GB, and XGBoost. The task explicitly asked for *all* base models to be optimized.
    *   **MLPClassifier `max_iter`:** The `MLPClassifier` is set with a default `max_iter`, but if it doesn't converge, it could fail silently or with warnings. It's good practice to handle `ConvergenceWarning` as the ground truth does.

3.  **Ensemble Methods (Completeness):**
    *   **Blending:** The task specifically asked for blending. The generated code did not implement blending.
    *   **Ensemble Training:** Basic `VotingClassifier` and `StackingClassifier` are present, but their hyperparameter tuning is not performed, which is part of ""optimization.""

4.  **Advanced Evaluation (Correctness & Completeness):**
    *   **Cross-validation strategy:** `StratifiedKFold` is only used for the statistical significance testing, not consistently as the `cv` object during GridSearchCV or `learning_curve` definition, which could lead to non-stratified folds if the data split is imbalanced.
    *   **Learning Curves:** The code plots learning curves but incorrectly passes `X_train` and `y_train` directly for the curve plotting which should be using `X_train_scaled`.
    *   **Validation Curves:** The task asked for ""validation curves,"" but the generated code doesn't explicitly implement them (only learning curves). The ground truth provides an example.
    *   **Bootstrap Sampling:** The task required bootstrap sampling for confidence intervals. This is completely missing from the generated code.
    *   **Prediction Confidence Intervals:** Linked to bootstrap sampling, the code does not generate or display prediction confidence intervals.

5.  **Model Interpretability (Correctness):**
    *   **SHAP:** While SHAP is used, the initial `shap_values` would be a list for binary classification, and `feature_names` would not correctly map to `X_test_scaled` if the data isn't a DataFrame with column names, leading to less interpretable plots. The ground truth handles this by getting the correct shap values (for class 1) and ensuring `X_test_processed_df` has proper column names.

6.  **Statistical Significance Testing:**
    *   It correctly uses `cross_val_score` and `ttest_rel`. However, it doesn't identify the ""best model"" to compare others against systematically or present results clearly.

7.  **Model Calibration:**
    *   It implements `CalibratedClassifierCV` and `calibration_curve`, which is good.

8.  **Dashboard (Completeness & Clarity):**
    *   The dashboard function is a good attempt at centralizing visualization.
    *   **Feature Importance:** The feature importance plot only shows `rf_cv.best_estimator_.feature_importances_`, not consistently from SHAP, nor from other tree models like GB or XGBoost. The prompt asks for ""feature importance analysis"" broadly, which should ideally include SHAP as the primary method across chosen models, not just one model's internal importance.
    *   **Prediction Confidence Intervals:** No display of prediction confidence intervals.
    *   **Comprehensive Metrics:** The dashboard presents a limited set of metrics (Accuracy, ROC AUC, Brier Score) compared to the ""performance metrics"" requested in the prompt, which implies a comprehensive view including precision, recall, F1, etc.
    *   The presentation of the dashboard with `plt.subplot` and the `create_dashboard` function is okay, but it lacks the cohesiveness and interactivity of an actual dashboard (which might be beyond the scope of a pure Python script but could be hinted at by cleaner separations).

Overall, the generated code has good intentions and covers many components, but the implementation often lacks the robustness, consistency, and correctness required for an ""advanced"" system, particularly regarding data leakage, full optimization of all models, and comprehensive evaluation details like bootstrap CIs and more nuanced preprocessing.",claude-3.5-sonnet,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.model_selection import train_test_split...",6,"The generated code attempts to address all requirements but lacks completeness and depth in several areas, leading to a score of 6.

**Strengths:**
*   **Structure:** The code is generally well-structured with functions attempting to encapsulate different logical parts of the task.
*   **Core Implementations:** It correctly identifies and imports necessary libraries for various statistical tasks (imputation, modeling, anomaly detection, dimensionality reduction, visualization).
*   **Partial Implementation of Requirements:** It includes placeholders or basic implementations for most of the requested functionalities like multiple imputation, logistic regression, GAM, Bayesian modeling, anomaly detection, PCA/Factor Analysis, robust methods, and diagnostics.

**Areas for Improvement / Missing Aspects:**
1.  **Missing Data Imputation Comparison:** The `compare_imputation` function is defined but its ""comparison"" part is only implicit (by returning `imputed_dfs`). The task requested *comparison of effectiveness*, which typically involves statistical examination (e.g., comparing distributions, descriptive statistics, or model performance on different imputed datasets). The ground truth provides detailed `describe()` comparisons for each method and explicitly states *which* imputed dataset will be used, and *why*.
2.  **Bayesian Logistic Regression (PyMC3):** The implementation is highly superficial:
    *   It only uses `pm.glm.GLM.from_formula('Potability ~ ph + Hardness + Solids', df)`. This is a very basic use of `pm.glm` and doesn't represent a *multivariate statistical model* in the sense of building it from scratch with defined priors as demonstrated in the ground truth. The task specifically asked for PyMC3 to build a Bayesian logistic regression, which implies more control over model specification (priors, likelihood) than just `pm.glm`.
    *   It performs `pm.sample` but doesn't do any post-sampling analysis like `arviz.plot_trace`, `arviz.summary`, or *prediction* from the posterior, as requested in the ground truth.
3.  **Hypothesis Testing:**
    *   **MANOVA:** While `MANOVA` is imported and called, the formula `ph + Hardness + Solids ~ Potability` is too restrictive. The ground truth correctly included *all other features* as dependent variables, which is more aligned with the idea of a comprehensive multivariate test.
    *   **Permutation Test:** The `permutation_test` from `scipy.stats` is called, but it's applied incorrectly. `(X_scaled[y==0], X_scaled[y==1])` passes pandas Series/DataFrames, not single variables. A permutation test typically compares a single variable (e.g., 'ph') between two groups, or performs a more complex test on a statistic. The provided code does not specify *what* exactly it's testing differences in, and `permutation_test` in scipy.stats expects callable functions. The ground truth correctly demonstrated a permutation test for the mean difference of a specific feature ('ph') between the two potability groups, along with visual analysis and interpretation.
    *   **Bootstrap Confidence Intervals:** Completely missing from the generated code, while explicitly requested.
4.  **Robust Statistical Methods:** The code calls `RLM` but provides no context, interpretation, or demonstration of *why* it's robust compared to OLS (as demonstrated in the ground truth). The task asks to handle non-normal distributions, and RLM is a good example, but its application needs to be shown for insight.
5.  **Model Diagnostics:**
    *   **Residual Analysis:** The `model_diagnostics` function creates a generic `OLS` model for residuals, which is inappropriate for a *logistic regression* model's residuals. Logistic regression residuals (deviance, Pearson) are not Gaussian and require different diagnostic plots (e.g., deviance residuals vs. fitted values). The Q-Q plot for *unspecified* `X_scaled[:, 0]` is also not a proper residual Q-Q plot.
    *   **Influence Measures:** Cook's distance, leverage plots, and identification of influential points for logistic regression are completely missing, despite being explicitly requested.
    *   **Goodness-of-Fit Tests:** Hosmer-Lemeshow test, AIC/BIC interpretation, and ROC Curves (beyond just fitting the model) are missing for logistic regression.
6.  **Advanced Statistical Visualizations:**
    *   The `create_diagnostics_plots` function includes some plots (Q-Q, residual, PCA, heatmap) but misses several specific ones requested like *leverage plots*, and the concept of ""diagnostic dashboards"" (which the ground truth attempts with more comprehensive plots and organization).
    *   EDA plots like pairplots and boxplots by potability group are also missing.
7.  **Anomaly Detection:** While `IsolationForest`, `LOF`, `OneClassSVM` are used, there's no reporting of the *number of anomalies detected* or visualization of *anomaly scores*, which are crucial for interpreting these methods and were present in the ground truth.
8.  **Output and Interpretation:** The generated code lacks print statements for results, summaries, and interpretations that guide the user through the analysis, making it less useful as a complete solution. The ground truth provides extensive print statements for clarity.
9.  **Imports:** While most imports are present, `arviz` is missing for PyMC3 diagnostics, and `statsmodels.multivariate` is imported *inside* a function, which is fine but less conventional.

To improve, the generated code needs to expand on the depth of implementation for each requirement, especially for Bayesian modeling, hypothesis testing, and model diagnostics. It should also include more output and visualization to make the analysis comprehensive, as was done in the ground truth. Many functions in the generated code are defined but not called, or only partially implemented, preventing a comprehensive evaluation of their outputs.",claude-3.5-sonnet,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import umap
from sklearn.manifold import TSNE
from scipy.cluster.hierarchy import dendrogram, linkage
from sci...",6,"The generated code attempts to address many aspects of the prompt but lacks depth and correctness in several key areas, particularly in complex parts like feature engineering, cluster validation, stability analysis, and anomaly detection.

**Strengths:**
*   Correctly loads data and handles missing values (though simple mean imputation might not be optimal).
*   Implements multiple clustering algorithms (K-Means, Hierarchical, DBSCAN, GMM, Spectral).
*   Includes multiple dimensionality reduction techniques (PCA, t-SNE, UMAP).
*   Attempts cluster validation using silhouette, Davies-Bouldin, and Calinski-Harabasz scores.
*   Provides some visualization using Matplotlib and Plotly.

**Weaknesses and Areas for Improvement:**

1.  **Feature Engineering (Limited & Basic):**
    *   The ""Water Quality Index"" (WQI) is a very simplistic weighted sum lacking domain-specific formulation. Real WQI calculations often involve sub-indices and normalization per parameter based on guidelines.
    *   `Chemical_Balance` (Sulfate/Chloramines) and `Contamination_Risk` are also very basic ratios/products, not sophisticated. The prompt asked for ""advanced features including water quality indices, chemical balance ratios, contamination risk scores."" The ground truth provides more nuanced calculations (e.g., pH deviation, integrated risk scores).
    *   ""Temporal patterns if applicable"" was part of the prompt, and the generated code simply omits it without acknowledging that the dataset is static, whereas the ground truth explicitly mentions its applicability.

2.  **Optimal Number of Clusters (Insufficient):**
    *   It only uses one criterion (`np.argmax(sil_scores)`) to determine the optimal K. The prompt explicitly requested ""multiple validation techniques (silhouette analysis, gap statistic, elbow method, Davies-Bouldin index, Calinski-Harabasz index)"". The elbow method is missing from the automatic selection, and there's no visual plot generated for these metrics to allow for human interpretation, which is crucial for elbow and gap statistic. The ground truth plots all metrics and selects an ""optimal_k"" based on visual inspection.

3.  **DBSCAN Parameter Tuning (Fixed & Unjustified):**
    *   DBSCAN uses fixed `eps=0.5` and `min_samples=5`. DBSCAN is highly sensitive to these parameters, and they should ideally be determined empirically (e.g., k-distance plot for epsilon), especially for a comprehensive analysis. The ground truth includes the k-distance plot for `eps` estimation.

4.  **Cluster Stability Analysis (Incorrect/Incomplete):**
    *   The `bootstrap_clustering` function only computes the mean/std of Silhouette score on bootstrapped samples, which is a very weak measure of cluster stability. It doesn't assess if specific points or clusters consistently remain together. The prompt asked for ""bootstrap resampling AND consensus clustering."" Consensus clustering (which involves co-occurrence matrices) is entirely missing. The ground truth correctly implements a form of consensus clustering with a dendrogram.

5.  **Cluster-based Anomaly Detection (Basic & Flawed):**
    *   The `detect_anomalies` function uses Z-scores on the entire scaled data, which is not ""cluster-based anomaly detection."" It identifies global outliers, not anomalies *within* each cluster's distribution. True cluster-based anomaly detection usually involves something like distance from centroid *within each cluster* or applying isolation forest *per cluster*. The ground truth properly implements both of these.

6.  **Domain-Specific Water Quality Profiling (Missing Depth):**
    *   It generates `cluster_profiles` by taking the mean of features, which is good. However, it doesn't explicitly link these profiles to water quality ""standards"" or discuss ""contamination risks and potability rates"" for each cluster as requested. The ground truth provides examples of WHO guidelines and suggests interpreting the profiles against them. The generated code also doesn't explicitly calculate potability rates per cluster.

7.  **Visualization Dashboard (Needs Improvement):**
    *   The visualizations are basic. The Matplotlib plots show DR results and validation metrics, but they are not a ""dashboard.""
    *   The Plotly section creates subplots, but the content is somewhat arbitrary (e.g., `pca.components_[0]` as ""Feature importance"" is a bit misleading without context, and comparing it to the DR plots).
    *   Radar charts for cluster profiles, 3D interactive visualizations of DR with clusters, and interactive potability rate charts are specifically requested but missing from Plotly. The ground truth generates these specific interactive plots.

8.  **Factor Analysis:**
    *   Factor Analysis was requested as a dimensionality reduction technique, but it is not implemented beyond `PCA`. It's listed in imports but not used.

9.  **General Code Quality/Completeness:**
    *   Lack of comments for complex sections.
    *   No explicit configuration section for parameters like `RANDOM_STATE`, `K_RANGE`, `N_BOOTSTRAPS`, etc., which makes the code less flexible.
    *   Outputs for ""optimal K"", stability scores are printed but not explicitly interpreted or used to inform the user about the findings.
    *   The Dendrogram is plotted but doesn't explicitly mention the number of clusters it reveals or its interpretation for stability.",claude-3.5-sonnet,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
import numpy as np
from sklearn.linear_model import LinearRegression

# Read the dataset
df = pd.read_csv('winequality-red.csv', sep=';')

# Calculate correlations with quality
correlations = df.corr()['quality'].sort_values(ascending=False)

# Print top 3 positive and negative correlations
print(""\nTop 3 Positive Correlations:"")
print(correlations[1:4])
print(""\nTop 3 Negative Correlation...",7,"The generated code successfully addresses most of the task requirements, including calculating correlations, identifying top positive/negative features, creating a heatmap, and generating scatter plots with regression lines.

However, there are a few areas for improvement:
1.  **Reading CSV:** The `sep=';'` argument in `pd.read_csv` is incorrect for the `winequality-red.csv` dataset, which is comma-separated. This will cause the entire dataset to be loaded as a single column, leading to incorrect correlation calculations and plots. This is a critical error affecting the correctness of the analytical results.
2.  **Top N selection:** The slicing `correlations[1:4]` and `correlations[-3:]` is fragile. `correlations[1:4]` correctly gets the top 3 positive *excluding* 'quality' itself (assuming 'quality' is the highest, which it usually is). However, `correlations[-3:]` might include 'quality' if there are fewer than three negative correlations or if 'quality' ended up somewhere in the middle due to floating point inaccuracies if the dataset was smaller. A more robust way, as shown in the ground truth, is to `drop` the target feature first and then use `head(3)` and `tail(3)`.
3.  **Linear Regression Library:** While `sklearn.linear_model.LinearRegression` is a valid choice, `scipy.stats.linregress` is often preferred for simple 2-variable linear regression as it directly provides the r-value (correlation coefficient squared, R) and p-value, which are useful for interpreting the regression. `sklearn` requires more steps to extract R (e.g., using `reg.score(x, y)`).
4.  **Error Handling:** The ground truth includes robust error handling for file not found, which is good practice for generalizable scripts. The generated code lacks this.
5.  **Output Clarity:** The ground truth provides more print statements for clarity throughout the execution, showing intermediate results like the full correlation list and the identified most positive/negative features with their coefficients.

Despite the issues, the code's overall structure, use of appropriate libraries for visualization and regression, and logical flow are good. The one critical error with reading the CSV, however, significantly impacts the correctness of the results.",claude-3.5-sonnet,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Load and clean data
df = pd.read_csv('video_games_sales.csv')
df = df.dropna()

# Convert Year to numeric, dropping non-numeric values
df['Year'] = pd.to_numeric(df['Year'], errors='coerce')

# Test 1: Nintendo vs Other Publishers
nintendo_sales = df[df['Publisher'] == 'Nintendo']['Global_Sales']
other_sales = df[df['Publisher'] != 'Nintendo']['Global_Sales']

t_stat, p_v...",6,"The generated code addresses most of the requirements but has several shortcomings regarding robustness, detailed interpretation, and specific test parameters.

**Strengths:**
*   Correctly uses `pandas` for data manipulation and `scipy.stats` for statistical tests.
*   Performs all requested statistical tests (t-test, chi-square, Mann-Whitney U).
*   Correctly identifies top 5 genres and platforms for the chi-square test.
*   Provides p-values and a boolean interpretation of significance.

**Areas for Improvement:**
1.  **Data Cleaning:**
    *   The `df.dropna()` is too aggressive. The ground truth code specifically drops NaNs in relevant columns (`Global_Sales`, `Year_of_Release`, `Publisher`, `Genre`, `Platform`) for each test, which is more robust and prevents unnecessary data loss for columns not used in a specific test (e.g., `User_Score`).
    *   The generated code assumes `Year` column, whereas the dataset has `Year_of_Release`. This renaming needs to be explicitly handled or the correct column name used. The `pd.Series.astype(int)` is also better than `pd.to_numeric` for year conversion once NaNs are handled.
    *   No error handling for `FileNotFoundError` as in the ground truth.

2.  **Hypothesis Testing & Interpretation:**
    *   **Missing Hypotheses:** The ground truth explicitly states the Null (H0) and Alternative (H1) hypotheses for each test, which is crucial for good statistical practice and interpretation. The generated code only provides a boolean `Significant difference` or `Significant association`.
    *   **Interpretation Detail:** The ground truth provides a natural language interpretation (""Nintendo games have significantly higher..."" or ""There is no significant difference...""). The generated code only prints `True`/`False` for significance.
    *   **T-test Parameters:** The first t-test for Nintendo sales specifies `alternative='greater'` in the ground truth, as the hypothesis is ""significantly *higher*"". The generated code uses the default `two-sided`, which might not align with the specific hypothesis about ""higher sales."" Similarly, `equal_var=False` (Welch's t-test) is often preferred unless variance equality is confirmed, which the ground truth uses; the generated code uses the default `equal_var=True`.
    *   **Mann-Whitney U Test:** The problem statement for the Mann-Whitney U test mentions ""significantly *different* sales,"" which implies a two-sided test. The generated code correctly uses `alternative='two-sided'`.
    *   **Missing Data Checks for Tests:** The ground truth includes checks like `if len(nintendo_sales) < 2` before performing statistical tests. While not strictly necessary for this dataset given its size, it's good practice for robust code that might run on smaller or sparser subsets of data.
    *   **Consistency in Alpha:** While the generated code consistently uses `0.05` inline, the ground truth defines `ALPHA = 0.05` as a constant. This is better practice for maintainability.

3.  **Output Formatting:** The ground truth's output is much clearer with headers and descriptive print statements, making it easier to read and understand the results of each test. The generated code's output is more concise but less informative.

Overall, the generated code gets most of the statistical tests right but lacks the robustness, detailed interpretation, and best practices seen in the ground truth, particularly in data cleaning specifics and hypothesis statement/interpretation.",claude-3.5-sonnet,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('video_games_sales.csv')

# Handle missing values
df['Year'].fillna(df['Year'].mean(), inplace=True)
df['Publishe...",7,"The generated code largely addresses the requirements of the task. It correctly loads the data, creates the target variable, encodes categorical features, generates new features, splits data, trains a Random Forest classifier, and evaluates the model with the specified metrics. It also attempts to display feature importance.

However, there are a few significant issues that prevent a higher score:
1.  **Missing Value Handling:** The ground truth code correctly identifies and handles missing values by dropping rows where `Global_Sales`, `Year`, `Publisher`, and regional sales are NaN. The generated code only handles `Year` (with `mean` imputation, which is problematic for integer `Year`) and `Publisher` (with an 'Unknown' string). It completely misses handling NaNs in regional sales columns, which are then used to calculate `Total_Regional_Sales`, potentially leading to NaNs in the new feature. This is a critical data quality issue.
2.  **Feature Selection:** The generated code's `features` list omits `NA_Sales`, `EU_Sales`, `JP_Sales`, and `Other_Sales` from the model training. While `Total_Regional_Sales` captures some of this information, dropping the individual regional sales quantities removes potentially valuable granular information the model could use. The ground truth model correctly includes all individual regional sales columns *and* the `Total_Regional_Sales`.
3.  **Visualizations (Optional but included):** While not explicitly asked for, the generated code includes Seaborn/Matplotlib for visualization. However, the confusion matrix plot includes `plt.show()` which is good for interactive environments but often problematic in non-interactive batch processing or within environments like Jupyter notebooks where `plt.show()` can block execution or create multiple redundant plots. Similarly for feature importance plot, `plt.show()` is present. The prompt asked for ""display"" feature importance, which can be done by printing, as in the ground truth, without relying on visual plots.
4.  **Error Handling:** The ground truth code includes basic error handling for `FileNotFoundError` when loading the CSV, which is a good practice. The generated code lacks this.

Areas for improvement:
*   Rethink missing value strategy for `Year` and ensure regional sales NaNs are handled. For `Year`, dropping rows or a more sophisticated imputation if NaNs are abundant (e.g., based on other game attributes) would be better than mean imputation, especially since it's used to calculate `Years_Since_Release`.
*   Include all relevant numerical features (`NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`) in the `features` list in addition to `Total_Regional_Sales`.
*   For the task, printing the feature importance (as done in the ground truth) is sufficient and generally more robust for automated evaluation than requiring plot display. If plots are included, consider removing `plt.show()` if the environment handles plot rendering automatically.
*   Add basic `try-except` for file loading.",claude-3.5-sonnet,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('video_games_sales.csv')

# Handle missing values
df['Year'].fillna(df['Year'].median(), inplace=True)
df['Publisher'].fillna('Unknown', inplace=True)

# Create engineered features
df['Sales_Ratio_NA_EU'] = df['NA_Sales'] / df['EU_Sales']
df['Sales_Ratio_NA_EU...",7,"The generated code addresses most of the requirements but has several critical issues and omissions.

**Correctness:**
- **Missing Value Handling:** The missing value handling for 'Publisher' is problematic. Replacing with 'Unknown' is a valid strategy, but the ground truth uses `SimpleImputer(strategy='most_frequent')` which is often a more robust choice for categorical data.
- **Sales_Ratio_NA_EU:** The generated code directly divides `df['NA_Sales'] / df['EU_Sales']` without handling potential `0` in the denominator, which can lead to `np.inf`. While it attempts to handle `np.inf` by replacing with `np.nan` and then `0`, this is less robust than adding a small epsilon as in the ground truth, which avoids division by zero entirely.
- **PCA:** The PCA part lacks the visualization of explained variance and transformation of data into principal components as specified in the task (""explain variance"").
- **Polynomial Features:** The polynomial feature creation is correct, but it creates a new DataFrame `df_poly` and doesn't explicitly `concat` it back to the original `df`. This effectively means these features are created but not added to the main dataframe for subsequent steps.
- **Interaction Features:** The approach for interaction features using `mul` and `get_dummies` is clever but might produce a very sparse matrix if there are many genres. It also doesn't handle the case where `Year` might have `NaN` values before multiplication (though it should be handled by previous steps, robust code would consider it).
- **Outlier Handling:** The outlier handling function `remove_outliers` *removes* rows containing outliers. The task requested to ""handle outliers,"" and the ground truth **caps** them using the IQR bounds, which is generally preferred over removal as it preserves more data points. Removing outliers can significantly reduce the dataset size.
- **Extra Content:** The generated code includes ""Feature importances for sales prediction"" using RandomForestRegressor, which was not requested in the task. This makes the code less focused on the specific requirements.

**Code Quality:**
- **Imports:** Imports are correct.
- **Structure:** Code is generally well-structured by task.
- **Readability:** Variables are named reasonably.
- **Error Handling:** Lacks robust error handling like `try-except` for file loading.

**Completeness:**
- **Missing Value Handling (Category):** The ground truth also handles the `Genre` missing values, which the generated code omits.
- **PCA Visualization/Explanation:** Insufficiently addresses the ""explain variance"" part of PCA without a plot or clear explanation of the explained variance ratio of components.
- **Polynomial Features Integration:** Not fully integrated into the main dataframe.
- **Outlier Handling Strategy:** Does not match the standard ""capping"" method often implied by ""handle"" in data science contexts when not specified, and the ground truth explicitly uses capping. Removing data is a drastic step.
- **Missing Sales Columns in Outlier Handling:** The `sales_columns` for outlier handling are limited to original sales. Engineered sales features like `Sales_Ratio_NA_EU`, `Publisher_Avg_Sales`, `Genre_Market_Share`, `Sales_Momentum` are not included, which is a significant oversight given they are numerical and can also contain outliers.
- **`plt.show()`:** The generated code lacks `plt.show()` after plots, meaning they won't display automatically in some environments.",claude-3.5-sonnet,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import chi2_contingency
import seaborn as sns
from sklearn.preprocessing import KBinsDiscretizer

# Load and clean data
df = pd.read_csv('diabetes.csv')
zero_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
for col in zero_cols:
    df[col] = df[col].replace(0, np.nan)
    df[col] = df[col].fillna(df[col].median())

# T-test for glucose levels
diabetic = df[df['Outcome']==1]['Glucose']...",7,"The generated code correctly performs all the requested statistical tests and handles the data cleaning as specified. The interpretation of the p-values is also largely correct.

However, there are several areas for improvement:
1.  **Readability and Structure:** The ground truth code uses clear print statements and headers to delineate each step and its interpretation, making the overall output much more readable and user-friendly. The generated code's print statements are more concise but less informative about *what* is being tested at each step or the specific conclusion.
2.  **Robustness in Data Loading:** The ground truth includes a `try-except` block for file loading, which is good practice for robustness. The generated code assumes the file exists.
3.  **Detailed Interpretation:** While the generated code states ""Significant difference..."" or ""No significant difference..."", the ground truth provides a more nuanced interpretation, including the direction of the difference (e.g., ""diabetic patients have significantly higher glucose levels"") or the nature of the association. This is crucial for a complete understanding of the results.
4.  **Specific `equal_var` in t-test:** The ground truth code specifically sets `equal_var=False` for the t-test (Welch's t-test), which is often a more robust choice when assumptions about equal variances are not explicitly checked or met. The generated code uses the default, which is `equal_var=True` (Student's t-test). While not strictly ""wrong"" without assuming anything about variance equality, Welch's is often preferred in practice.
5.  **BMI Category Binning:** The ground truth uses `right=False` for BMI binning (e.g., [18.5, 24.9)), which is the standard for BMI categories. The generated code uses `pd.cut` with default `right=True`, which means the bins are (0, 18.5], (18.5, 24.9], etc. This might slightly alter the categorization at the boundary values. The task simply asked for the ranges, but standard practice should be followed.
6.  **Missing `KBinsDiscretizer` and `seaborn`:** The imports `seaborn` and `KBinsDiscretizer` are not used in the provided code, which indicates unnecessary imports.

Overall, the code is functional and produces the correct statistical outputs, but lacks the polished presentation, detailed interpretation, and some best practices found in the ground truth.",claude-3.5-sonnet,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_auc_score, confusion_matrix,...",7,"The generated code addresses most of the requirements but has several areas for improvement in terms of correctness, completeness, and code quality.

**Strong Points:**
*   Correctly loads data and replaces zeros with median in specified columns.
*   Creates the three requested new features: 'Glucose\_BMI\_ratio', 'Age\_group', and 'Insulin\_level'.
*   Trains and compares multiple classification models.
*   Performs cross-validation.
*   Attempts hyperparameter tuning for the best model.
*   Evaluates models using accuracy, precision, recall, F1-score, and ROC-AUC.
*   Generates confusion matrices and ROC curves.
*   Displays feature importance for Random Forest (tree-based).

**Areas for Improvement:**

1.  **Categorical Encoding:** The generated code uses `LabelEncoder` for 'Age\_group' and 'Insulin\_level'. This is problematic as it introduces an artificial ordinal relationship between categories, which can mislead models like Logistic Regression and SVM. `OneHotEncoder` or `pd.get_dummies` should be used for nominal categorical variables. The ground truth correctly uses `OneHotEncoder` within a `ColumnTransformer`.
2.  **Preprocessing Pipeline (Completeness/Correctness):** The generated code applies `StandardScaler` *after* dropping the target variable, but before the categorical encoding is properly handled (as mentioned above). This means scaling is applied to the integer-encoded categorical features, which is incorrect. A `ColumnTransformer` (as seen in the ground truth) is essential for handling numerical scaling and categorical encoding separately and correctly for different columns. This also means feature names for importance plotting later are incorrect as they don't reflect the one-hot encoded features.
3.  **Hyperparameter Tuning Logic:**
    *   The tuning logic `if best_model_name == 'Random Forest':`
        is flawed. It assumes Random Forest will always be the best model based on the initial run. A more robust approach would be to dynamically select the `best_model_name` *from all models*, and then define a `param_grid` specifically for that determined best model type. The ground truth explicitly states it *chooses* Random Forest for tuning, but a more dynamic approach would inspect the results dictionary for the best performing model.
    *   The `best_model` variable is only assigned if `Random Forest` is the best, meaning tuning results for other `best_model_name` are not considered or updated. The evaluation metrics and visualizations don't use the *tuned* best model *unless* it was explicitly Random Forest, and they don't clearly distinguish between tuned and untuned models in the final output. The ground truth explicitly re-evaluates the tuned model and incorporates its metrics and ROC curve.
4.  **Feature Importance (Completeness):** The task specifically requested feature importance for *tree-based models* (plural). The solution only provides it for Random Forest, not Gradient Boosting. Furthermore, because of the incorrect categorical encoding and scaling, the feature names in the importance plot will be for the raw features, not the expanded features after one-hot encoding, which can be misleading.
5.  **Code Structure/Readability:** While not terrible, the generated code combines cross-validation, training, prediction, and metric storage in one loop, making it a bit dense. The ground truth separates phases more clearly (e.g., dedicated section for CV results, then separate for test set evaluation), which improves readability.
6.  **Missing Visualizations:** The task asked for `ConfusionMatrixDisplay` and `RocCurveDisplay` (or equivalent professional plots, though the generated code uses `sns.heatmap` and `plt.plot` which are acceptable). The ground truth used `ConfusionMatrixDisplay` explicitly, while the generated code used `sns.heatmap` which is fine. The output for results in print statements could be formatted more cleanly (e.g., using `f-strings` for table-like output).

To improve, the code needs to:
*   Implement `OneHotEncoder` for categorical features.
*   Use `ColumnTransformer` to properly preprocess numerical and categorical features.
*   Make the selection of the ""best model"" for tuning truly dynamic, and apply appropriate parameter grids for that model.
*   Ensure that the results of the *tuned* model are clearly presented and integrated into the final evaluation and visualizations.
*   Add feature importance for Gradient Boosting.",claude-3.5-sonnet,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import seaborn as sns
import matplotlib.pyplot as plt

# Load and preprocess data
df = pd.read_csv('diabetes.csv')
df ...",6,"The generated code attempts to address all requirements of the task, showing a good understanding of the steps involved in advanced feature engineering and dimensionality reduction. However, there are several key issues and areas where the implementation falls short of the ground truth and best practices.

**Strengths:**
*   **Structure:** The code generally follows the requested steps in a logical order (loading, engineering, scaling, PCA, selection, SMOTE, importance).
*   **Feature Engineering:** It correctly identifies and implements the requested engineered features.
*   **Basic Implementations:** It correctly uses `StandardScaler`, `MinMaxScaler`, `PCA`, `SelectKBest`, `RFE`, `SMOTE`, and `RandomForestClassifier`.
*   **Visualization:** It includes a simple visualization for feature importance.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling (Critical):**
    *   The generated code uses `df = df.fillna(df.mean())` which imputes *all* NaNs with the *mean* of their respective columns. This is problematic because `Pregnancies` and `Outcome` columns can legitimately have 0s, and imputing them would be incorrect. The ground truth correctly identifies specific columns where 0s represent missing values and then imputes them using the *median*, which is more robust to outliers than the mean. This is a significant correctness issue.
2.  **Polynomial Features Implementation (Important):**
    *   `poly_features_names = poly.get_feature_names(numeric_cols)` should be `poly.get_feature_names_out(numeric_cols)` for recent scikit-learn versions, but more critically, the line `poly_df = pd.DataFrame(poly_features[:, len(numeric_cols):], columns=poly_features_names[len(numeric_cols):])` is incorrect. It attempts to slice `poly_features` and `poly_features_names` to remove original features, but `PolynomialFeatures(include_bias=False)` already handles not generating the input features themselves, only their polynomial combinations (and cross-terms). The slicing `[:, len(numeric_cols):]` is likely introducing an error by skipping the first `len(numeric_cols)` created polynomial features, which means true polynomial terms might be missed or incorrectly labeled. The ground truth's approach of dropping the original columns first and then concatenating `df_poly` is more robust.
3.  **PCA Explanation:**
    *   While it performs PCA, it just prints `variance_ratios` and `cumulative_variance_ratio`. The task explicitly asked to ""explain variance ratios"" and to ""retain components explaining 95% of variance"" (implicitly, by setting `n_components=0.95`). The generated code doesn't set `n_components` for PCA, meaning it keeps all components.
4.  **Feature Selection Integration:**
    *   The `SelectKBest` and `RFE` steps are performed on `X_scaled_standard`, but then the code proceeds to use `X_uncorrelated` (after correlation filtering) for SMOTE and feature importance. The output of `SelectKBest` and `RFE` (`X_selected`, `X_rfe`) is not explicitly used or propagated to subsequent steps, rendering these sections less impactful in the overall pipeline. The ground truth explicitly uses `X_filtered_corr` (equivalent to `X_uncorrelated`) as the base for feature selection and the subsequent PCA, showing a more integrated workflow.
5.  **Correlation Filtering Order (Minor but impactful):**
    *   The ground truth applies correlation filtering *before* PCA and before feature selection techniques like SelectKBest and RFE, which makes sense as dimensionality reduction and selection methods benefit from a less redundant feature set. The generated code performs correlation filtering *after* PCA and some feature selection, meaning PCA was applied to potentially highly correlated features. While not a strict error, it's less optimal flow.
6.  **Readability/Comments:**
    *   The ground truth includes extensive print statements to indicate progress and explain steps, aiding readability for a ""benchmark"" evaluation. The generated code is more concise but lacks these informative print statements.
7.  **Implicit Assumptions/Code Clarity:**
    *   The generated code's polynomial feature creation for `poly_df` and combining it back into `df` is less clear and could lead to issues if the chosen `numeric_cols` overlap with engineered features that are based on these. The ground truth explicitly decides which columns to apply polynomial features to, carefully manages the `concat` operation to avoid feature duplication, and clearly comments on this.
    *   The `feature_importance` dataframe created has `X.columns` for feature names, but the `rf_model` was trained on `X_train_balanced`. If `SMOTE` or the prior steps added/removed columns, this mapping could be incorrect. The ground truth correctly uses `X_train_smote.columns` for `features`.
8.  **Risk Factor Thresholds:**
    *   The thresholds used for `Risk_Factor_Count` are slightly different from the ground truth. While both are valid, the ground truth's choices are more aligned with commonly cited medical thresholds for prediabetes/diabetes (`Glucose > 140`) and hypertension (`BloodPressure > 90`).

Overall, the generated code fulfills the basic functional requirements, but the implementation details, especially regarding missing value handling and polynomial feature integration, are less robust and could lead to incorrect results or less optimal performance compared to the ground truth. The lack of detailed output (like PCA explanation) also reduces its completeness for the specified task.",claude-3.5-sonnet,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from sklearn.preprocessing import KBinsDiscretizer
import seaborn as sns
import matplotlib.pyplot as plt

# Load and clean data
df = pd.read_csv('world-happiness-report-2021.csv')
df = df.dropna()

# Create Western Europe vs Others comparison
western_europe = df[df['Regional indicator'] == 'Western Europe']['Ladder score']
other_regions = df[df['Regional indicator'] != 'Western Europe']['Ladder score']

# T-test for Western...",6,"The generated code addresses most of the requirements by performing the requested statistical tests. However, it lacks crucial aspects of a robust solution and proper interpretation, which are key for a ""data science benchmark"" task.

Here are the specific areas for improvement:

**Cricital Missing/Improvements:**

1.  **Interpretation of Results (Major Flaw):** The prompt explicitly asks to ""Display p-values and interpret results with significance level =0.05."" The generated code only prints ""Significant"" or ""Not significant"" based on the p-value. It completely misses the actual *meaning* of the significance, such as whether Western Europe's happiness is *higher* or *lower*, or if the correlation is *positive* or *negative*. This is a critical deficiency for a statistical analysis task. The ground truth code provides excellent interpretive print statements.

2.  **Missing Values Handling (Partial):** The generated code simply uses `df.dropna()`. While this handles missing values, it drops *any* row with *any* NaN in *any* column, which might be overly aggressive if only specific columns are needed for analysis. The ground truth code intelligently handles NaNs only in `critical_columns`, which is a more nuanced and potentially less data-losing approach. The prompt specifically asked to ""handle any missing values,"" and while `dropna()` does that, it lacks the specific and robust method seen in the ground truth.

3.  **No Error Handling for File Loading:** The code assumes the CSV file exists. The ground truth code includes a `try-except FileNotFoundError` block, which is good practice for robust code.

4.  **T-test `equal_var` Parameter:** For independent t-tests (Western Europe happiness and Freedom-Generosity), it's generally safer and more robust to use `equal_var=False` (Welch's t-test) unless there's a strong reason to assume equal variances. The generated code uses the default `equal_var=True`. The ground truth correctly uses `equal_var=False`.

5.  **Lack of Descriptive Statistics:** While not explicitly asked for in every point, printing means/medians for the groups being compared (as done in the ground truth) significantly aids in the interpretation of the t-tests and Mann-Whitney U test, especially when discussing ""higher"" or ""lower"" differences.

6.  **Unnecessary Imports:** `KBinsDiscretizer`, `seaborn`, and `matplotlib.pyplot` are imported but not used, indicating some potential for code cleanliness improvement.

7.  **Order of Tests:** The prompt specified a numbered list of tests. While the generated code does all tests, it re-orders them slightly in its output and the numbering of the print statements doesn't match the prompt's numbering. This is a minor point but indicates less adherence to the specific instructions.

**Good Points:**

*   Successfully implemented all requested statistical tests (t-test, Mann-Whitney U, ANOVA, Pearson correlation, Chi-square).
*   Correctly identified the groups for each test (e.g., Western Europe vs. others, top/bottom 25% happiest, happiness categories).
*   Correctly used `pd.qcut` for creating happiness quartiles and categories, which is an efficient way to achieve the grouping.
*   Correctly calculated the median for freedom scores.
*   Clearly printed the p-values and their interpretation against alpha.",claude-3.5-sonnet,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load and preprocess data
df = pd.read_csv...",7,"The generated code addresses most of the task requirements and provides a good starting point.

**Strong Points:**
- **Data Preprocessing:** Loads data, handles missing values (though `dropna()` without specific subset might be too aggressive), and correctly extracts features and target.
- **Model Training & Evaluation:** Trains multiple regression models and correctly calculates MAE, MSE, RMSE, and R2 scores.
- **Data Splitting & Scaling:** Correctly splits data into training/testing sets and applies `StandardScaler`.
- **Hyperparameter Tuning:** Implements `GridSearchCV` for the best model, which is good.
- **Visualizations:** Includes residual plots and feature importance visualization.
- **Predictions for New Data:** Demonstrates how to make predictions for hypothetical countries.
- **Cross-validation:** Implements `cross_val_score` for all models, which was a good addition.

**Areas for Improvement:**
- **Missing Value Handling Specificity:** The ground truth code specifically handles missing values in only the target and feature columns (`df.dropna(subset=FEATURE_COLUMNS + [TARGET_COLUMN])`). The generated code uses a general `df.dropna()`, which might drop rows unnecessarily if missing values exist in other columns not relevant to the model. While it works for this dataset where the specified columns are the primary ones with NaNs, being more precise is better practice.
- **Feature Importance for All Models:** The `best_model` determination and subsequent feature importance calculation `if hasattr(best_model, 'feature_importances_')` only works for tree-based models. Linear models often have coefficients that can be interpreted as importance; a more robust solution would consider this for all applicable models or explicitly state the limitation. The ground truth correctly identifies when feature importance is not available.
- **Cross-validation Scope:** The generated code performs `cross_val_score` within the initial model training loop, which is fine, but the prompt also asks for cross-validation to assess *model stability*. The ground truth performs this separately for the *tuned* best model with explicit mean and std deviation, which is a better way to assess stability.
- **Hyperparameter Tuning for Best Model (Logic):** The code attempts to tune either 'Random Forest' or 'Gradient Boosting' based on `best_model_name`. If the `best_model_name` happens to be 'Linear Regression', 'Ridge', or 'Lasso', the `param_grid` will not be defined, leading to an error. The solution should either ensure `best_model` is always one that benefits from this type of tuning, or add an `else` clause/check. The ground truth explicitly chose one model (Gradient Boosting) for tuning from the start, making the code more robust against this.
- **Clarity of Output:** The final `results_df` printout is a bit wide and hard to read. The ground truth prints the full evaluation metrics for each model step-by-step and then a clean summary table.
- **Random State:** While `random_state=42` is used for `train_test_split` and for `RandomForestRegressor`, `GradientBoostingRegressor`, it's not used when initializing `Ridge` and `Lasso` in the generated code. While these are less affected, it's good practice for reproducibility if the model has a `random_state` parameter. The ground truth explicitly includes it.
- **Pre-calculation of `best_model_name`:** The generated code determines the `best_model_name` *after* the initial model evaluation loop, then uses `best_model_name` for hyperparameter tuning. This forces a re-lookup. The ground truth directly identifies the best model from the `model_performance` dictionary.
- **No proper error handling for file loading:** The ground truth includes a `try-except` block for `pd.read_csv`, which is good practice. The generated code assumes the file will always be present.",claude-3.5-sonnet,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mutual_info_score
import seaborn as sns
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('world-happiness-report-2021.csv...",6,"The generated code attempts to address all aspects of the task, which is a good start.

**Areas of concern/improvement:**

1.  **Handling Missing Values:** The ground truth includes explicit handling for missing values (filling with mean, dropping NaNs). The generated code completely omits this, which can lead to errors in subsequent calculations (e.g., division by zero leading to `inf`, or `NaN`s propagating) and incorrect results. `df_processed.dropna(inplace=True)` and `X.fillna(X.mean(), inplace=True)` in ground truth are crucial.
2.  **`mutual_info_score` usage:** The generated code uses `sklearn.metrics.mutual_info_score` for individual feature pairs, which is a scalar metric. The task implies `mutual_info_regression` from `sklearn.feature_selection` which is designed to score features against a target variable. The `mutual_info_score` is not suitable here as it expects 1D arrays for discrete variables or expects `n_bins` for continuous variables, and doesn't directly provide a score for each feature against the continuous target in the way `mutual_info_regression` does.
3.  **Interaction Features:** The ground truth includes `GDP_x_SocialSupport` and `GDP_x_HealthyLifeExpectancy`. The generated code creates `GDP_Social_Interaction`, `GDP_Health_Interaction`, and `Freedom_Social_Interaction`. While `Freedom_Social_Interaction` is an additional interaction, the prompt specifically asked for ""economic and social factors"", and the ones specified in the ground truth are better examples.
4.  **Polynomial Features Application:** The generated code applies polynomial features only to `X_num` and then concatenates `poly_df.iloc[:, len(num_cols):]` to the main `df`. This is problematic because `X_num` is based on the *original* dataframe, not the `df` *after* engineered features. The ground truth correctly applies polynomial features to the `df_processed` (which already contains engineered features) and uses `key_happiness_factors` which are the ones mentioned in the task for polynomial features. Also, the concatenation method `poly_df.iloc[:, len(num_cols):]]` might miss some interaction terms if `get_feature_names` returns features in a specific order that isn't just appended.
5.  **Data for Scaling/DR/Feature Selection:** The generated code reuses `num_cols` (the original numerical columns) for scaling, PCA, t-SNE, and feature selection. This means it completely ignores many of the newly engineered features and polynomial features during these crucial steps. The ground truth correctly identifies *all* numerical features after engineering steps for `X` and `y`. This is a significant functional difference.
6.  **Visualization:** While some visualizations are present, the ground truth includes more targeted and better-labeled plots (e.g., PCA/t-SNE with `hue=y` for target variable insight, and proper title/labels for feature importance). The generated t-SNE and PCA plots are bare bone.
7.  **RFE Estimator:** The ground truth uses `LinearRegression` as the estimator for RFE, which is a common and robust choice for feature selection. The generated code uses `RandomForestRegressor` for RFE. While possible, `LinearRegression` is generally preferred for RFE as a simpler, less biased estimator for feature selection, unless specified.
8.  **Correlation Filtering:** The implementation of correlation filtering in the generated code is incomplete. It identifies `high_corr_features` but doesn't *drop* them from the DataFrame, rendering the step ineffective for filtering. The ground truth correctly identifies and then shows how to drop them.
9.  **Random Forest Feature Importance:** The generated code calculates feature importance but only on `num_cols` (original numerical features), not on the full set of engineered and polynomial features (`X` in the ground truth). This limits the utility of the feature importance analysis.
10. **Code Structure and Comments:** The ground truth provides more logical sectioning and comments, making it easier to follow the requested steps.
11. **Random States:** Many processes (t-SNE, mutual info reg/RandomForest for consistency) benefit from `random_state` for reproducibility, which is more consistently applied in the ground truth.",claude-3.5-sonnet,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Load and prepare data
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])
usa_data = df[df['location'] == 'USA'].copy()
usa_data.set_index('date', inplace=True)

# Resample to weekly
weekly_cases = usa_data['new_case...",7,"The generated code addresses most of the task requirements but has some areas for improvement in terms of robustness, error handling, and adherence to best practices for time series analysis.

**Strong Points:**
*   Correctly loads, parses, and filters the data for 'USA'.
*   Correctly resamples 'new_cases' to weekly sum.
*   Performs time series decomposition using an additive model.
*   Plots the decomposed components.
*   Implements the ADF test and applies first-order differencing if the original series is non-stationary.
*   Plots ACF and PACF for the (potentially differenced) stationary series.
*   Uses `inplace=True` and `.copy()` for Pandas operations where appropriate.

**Areas for Improvement:**

1.  **Period for Decomposition:** The `period=52` for `seasonal_decompose` is hardcoded. While 52 weeks is a common assumption for yearly seasonality, the ground truth code includes a check for sufficient data length and adjusts the period if necessary (e.g., to 4 for monthly-like seasonality if data is too short). The generated code might fail or produce less meaningful results if the dataset doesn't cover at least two full periods (104 weeks) for reliable decomposition.
2.  **Decomposition Plotting:** The generated code manually creates subplots for the decomposition, which is less concise and slightly more error-prone than using `decomposition.plot()` as shown in the ground truth. The `decomposition.plot()` method is designed to handle this directly.
3.  **ADF Test Output:** While the p-value and test statistic are printed, the critical values are missing from the output. These are crucial for interpreting the ADF test results.
4.  **Error Handling/Robustness:**
    *   No check for `synthetic_covid19_data.csv` existence.
    *   No check for empty `usa_data` after filtering for 'USA'.
    *   No check for insufficient data points after resampling before decomposition or ADF test (e.g., if `weekly_cases` has less than 2-3 data points or less than `2*period` for decomposition). The ground truth includes checks for `weekly_cases.empty` and `len(weekly_cases) < 2`.
    *   No handling for `weekly_cases` potentially containing zeros or NaNs in a way that might impact decomposition or ADF test (though `.dropna()` for ADF helps).
    *   No warning or handling if differencing results in an empty series or still leaves a non-stationary series.
5.  **Docstrings/Comments:** The code is functional but lacks comments or docstrings explaining the purpose of different sections or the helper function, which would improve readability and maintainability.
6.  **Redundant Imports:** `numpy` is imported but not explicitly used. While often pulled in by Pandas, it's good practice to remove unused imports.
7.  **Plot Labels/Titles:** The plots are missing main titles for the entire figure, which can make it harder to understand the context of the plots at a glance. For example, `f'Time Series Decomposition of Weekly New Cases ({location})'`.
8.  **Output Clarification:** The ground truth clearly states the conclusion of the ADF test (stationary/non-stationary), which is helpful. The generated code only prints the p-value, leaving the interpretation to the user.

Overall, the code provides a good attempt at the problem, but lacks the robustness, user-friendliness, and some of the more advanced error-checking/best practices seen in the ground truth code.",claude-3.5-sonnet,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from scipy import stats

# Read data
df = pd.read_csv('winequality-red.csv')

# Create pH groups
df['pH_group'] = pd.cut(df['pH'], 
                       bins=[-np.inf, 3.2, 3.4, np.inf],
                       labels=['Low pH', 'Medium pH', 'High pH'])

# Calculate mean quality per group
group_means = df.groupby('pH_group')['quality'].mean()
group_std = df.groupby('pH_group')['quality'].std()
group_counts = df.gro...",7,"The generated code correctly performs the main tasks: categorizing pH groups, calculating means, performing ANOVA, and visualizing with error bars.

**Strengths:**
- Correctly uses `pd.cut` for categorization.
- Accurately calculates group means and performs ANOVA.
- Creates a relevant bar plot with error bars for visualization.
- Prints average quality and ANOVA results.

**Areas for Improvement:**
- **Confidence Interval Calculation:** The calculation `1.96 * group_std / np.sqrt(group_counts)` assumes a normal distribution and uses a z-score (1.96 for 95% CI). For smaller sample sizes (which can happen for specific pH groups), it's more statistically appropriate to use the t-distribution, which the ground truth code does (`stats.t.interval`). This is a nuance but important for statistical rigor.
- **pH Group Labels:** The ground truth code uses more descriptive labels for the pH groups, e.g., 'Low pH (< 3.2)', which is clearer than just 'Low pH'.
- **Error Bar Plotting:** The generated code plots `yerr=conf_intervals`. While this often works visually, `conf_intervals` typically represents the margin of error (half the width of the CI). For `yerr`, matplotlib expects the distance from the mean, so it should ideally be explicitly calculated as `mean - lower_bound` or `upper_bound - mean` if asymmetric, or just the margin of error if symmetric. More rigorously, the ground truth explicitly calculates `ci_lower` and `ci_upper` and then uses `mean - ci_lower` for `yerr` (assuming symmetric CIs from `stats.t.interval`).
- **Reporting Confidence Intervals:** The ground truth code explicitly prints the lower and upper bounds of the confidence intervals, which is more informative than just printing `mean  margin_of_error`.
- **Robustness:** The ground truth includes error handling for `FileNotFoundError` and checks for sufficient data (`count > 1`) before calculating CIs, making it more robust. It also handles the case where ANOVA might not be performable if there aren't enough groups with data.
- **Categorical Order:** The ground truth explicitly sets the order of categorical variables, ensuring consistent plotting and handling, which the generated code implicitly relies on the order of bins.

Overall, the generated code provides a functional solution, but with some statistical and robustness improvements, it could be made more precise and user-friendly.",claude-3.5-sonnet,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder

# Load data
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

# Select features and...",8,"The generated code correctly addresses most of the requirements. It loads the data, parses the date, selects features, handles missing values, performs one-hot encoding, scales numerical features, splits the data, trains both Linear Regression and Random Forest models, and evaluates them with the specified metrics.

Strong points:
- Correctly uses `pd.to_datetime` for date parsing.
- Correctly identifies numerical features for median imputation.
- Properly uses `OneHotEncoder` with `drop='first'` and correctly integrates the encoded features back into the DataFrame.
- Correctly scales features using `StandardScaler`.
- Correctly splits data and trains both models with specified parameters.
- Provides clear output for evaluation metrics.

Areas for improvement:
- **Scaling before splitting (`X_scaled = scaler.fit_transform(X)` followed by `train_test_split(X_scaled, y, ...)`)**: This is a data leakage issue. The scaler should be fitted only on the training data (`fit_transform`) and then used to transform the test data (`transform`). The ground truth code correctly performs scaling *after* the train-test split, which is the correct approach to prevent data leakage.
- The `try-except FileNotFoundError` block for robust file loading is missing, which is a good practice as shown in the ground truth.
- While the chaining `X[numeric_features] = X[numeric_features].fillna(X[numeric_features].median())` might work, it's generally safer (and often more robust against `SettingWithCopyWarning`) to use `.loc` for assignment or iterate through columns. The ground truth handles this by first creating a copy of the selected dataframe and then iterating for imputation.
- The initial `X = df[features]` and `y = df[target]` should ideally be performed on a `.copy()` of the original DataFrame to avoid potential `SettingWithCopyWarning` downstream, especially after modifications like `fillna`. The ground truth does this.",claude-3.5-sonnet,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
import statsmodels.api as sm
from statsmodels.stats.multicomp import pairwise_tukeyhsd

# Load and prepare data
df = pd.read_csv('synthetic_covid19_data.csv')

# Median imputation
cols_to_impute = ['reproduction_rate', 'stringency_index', 'median_age', 
                  'total_deaths', 'population', 'new_cases']
for col in cols_to_impute:
    df[col] = df[col].fillna(df[col].median())

# Test 1: ANOVA for reproduction rate...",7,"The generated code correctly performs the three requested statistical tests (ANOVA, Pearson correlation, t-test) and states the null hypothesis and conclusion based on the p-value.

However, there are several areas for improvement:

1.  **Missing Value Imputation**: The imputation for `total_deaths` and `population` specifically needs to handle potential `NaN` values *before* calculating the `death_rate` and `threshold`. If there are `NaN` values in these columns, `latest_by_country['total_deaths'] / latest_by_country['population']` will produce `NaN` which will then skew the `quantile` calculation or lead to `NaN`s in the `high_death_rate_country` assignment. The ground truth explicitly handles this by dropping NaNs for the `death_rate_ratio`.

2.  **Robustness in T-test**: The `t-test` for `median_age` assumes equal variances (`equal_var=True` by default in `scipy.stats.ttest_ind`). For robustness, especially when group sizes might differ greatly or variances are unknown, it's generally safer to use Welch's t-test by setting `equal_var=False`, which the ground truth does.

3.  **Data Filtering for ANOVA**: The ANOVA test for `reproduction_rate` across continents does not filter out `NaN` values from the `reproduction_rate` column *before* passing them to `f_oneway`. While `f_oneway` can handle `NaN`s by ignoring them, it's good practice to explicitly handle them or ensure the data passed is clean. Also, it adds a warning for insufficient data, which the generated code lacks.

4.  **Clarity of Output**: While the output is functional, it can be slightly improved for clarity, like including the alpha value explicitly in the conclusion and mentioning the specific p-value threshold used for rejection. The ground truth also adds more informative print statements describing each step.

5.  **Missing Imports**: `statsmodels.api` and `pairwise_tukeyhsd` are imported but not used.

6.  **Edge Case Handling**: The code doesn't handle edge cases like very few data points for correlation or t-test (e.g., if one group is empty), which could lead to errors or less robust results. The ground truth explicitly checks for sufficient data before performing tests.

Overall, it's a good attempt, but lacks some of the robustness, error handling, and attention to detail that make the ground truth more production-ready and reliable for analytical tasks.",claude-3.5-sonnet,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('clash_royale_cards.csv')

# Select numerical columns
num_cols = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
num_df = df[num_cols]

# Calculate correlation matrix
corr_matrix = num_df.corr()

# Create heatmap
plt.figure(figsize=(10,8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix of Clash Royale Card Metrics'...",10,"The generated code successfully addresses all requirements of the task. It properly loads the CSV, selects the specified numerical columns, calculates the pairwise correlation matrix, and visualizes it as a heatmap using seaborn and matplotlib. The imports are correct, and the structure is clean and efficient for the given task. It's concise and directly implements the requested steps. The visualization parameters `annot`, `cmap`, and `center` are well-chosen for a correlation heatmap.",claude-3.5-sonnet,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

df = pd.read_csv('clash_royale_cards.csv')

rarity_stats = df.groupby('rarity').agg({
    'elixirCost': 'mean',
    'Win Rate': 'mean', 
    'Usage': 'mean'
}).round(2)

print(rarity_stats)
```",9,"The generated code correctly solves the task. It properly loads the CSV, groups the data by 'rarity', and calculates the average 'elixirCost', 'Win Rate', and 'Usage'. The use of `.agg()` is an excellent and common way to perform multiple aggregations.

The only minor point is the `.round(2)` which was not explicitly requested or present in the ground truth. While it makes the output cleaner, it's an additional operation not strictly required by the prompt's core task. Removing it would make the code a perfect match for the specific requirements. Otherwise, it's very concise and effective.",claude-3.5-sonnet,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Load data
df = pd.read_csv('clash_royale_cards.csv')

# Create binary target column
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)

# Select features
features = ['elixirCost', 'Usage', 'rarity']
X = df[features].copy()

# One-hot encode rarity
onehot = OneHotEncoder(sparse=False)
rarity_encoded = onehot.fit_transform(X[['rarity']])
rarity_cols = onehot...",9,"The generated code correctly implements all specified requirements: loading data, creating the target column, selecting features, one-hot encoding `rarity`, and splitting the data. The imports are correct and the structure is logical.

The only minor area for improvement is the one-hot encoding approach. While using `OneHotEncoder` from `sklearn.preprocessing` is functionally correct and often preferred for consistency in a broader ML pipeline, `pd.get_dummies` is generally simpler and more straightforward for single-dataframe transformations, especially when dealing with the entire original dataframe. The `sklearn` approach, as implemented, requires careful handling of column names and concatenation, which adds a bit of complexity compared to `get_dummies`. However, this is a stylistic/efficiency choice rather than an error in correctness. The generated code also implicitly assumes the index of `X` and `rarity_encoded` will align during `pd.concat`, which is typically true if `X` is directly derived from `df`.",claude-3.5-sonnet,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load data
df = pd.read_csv('iris.csv')

# Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = df['Species']

# Encode target variable
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Split data
X_train, X_test, y_...",9,"The generated code successfully completes all specified tasks: loading data, defining X and y, encoding the target, splitting the data with the correct parameters, training a KNN classifier with `n_neighbors=3`, and reporting the accuracy. The imports are correct and the code structure is logical.

The only minor difference from the ground truth is the omission of `stratify=y_encoded` in the `train_test_split` function. While not explicitly mentioned in the prompt, stratifying the split for classification tasks, especially with potentially imbalanced classes, is a good practice and often leads to more reliable model evaluations. This is a best practice point rather than a direct failure to meet the prompt's explicit requirements. Another minor point is the `try-except` block for `FileNotFoundError` in the ground truth, which is good for robustness but also not explicitly required by the prompt. Overall, the generated code is highly functional and correct for the given task.",claude-3.5-sonnet,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy import stats

# Load and filter data
df = pd.read_csv('iris.csv')
versicolor = df[df['Species'] == 'Iris-versicolor']['PetalLengthCm']
virginica = df[df['Species'] == 'Iris-virginica']['PetalLengthCm']

# Null Hypothesis (H0): No difference in mean petal length between species
# Alternative Hypothesis (Ha): Mean petal lengths are different between species

# Perform t-test
t_stat, p_value = stats.ttest_ind(versicolor, virginica)

# Print results
print(f""t...",7,"The generated code correctly performs the core task of loading data, filtering, performing a t-test, and interpreting the results. It includes the necessary imports and uses appropriate functions. However, it falls short on completeness and clarity regarding the hypotheses, which were explicitly requested as separate points in the task description.

Specific points for improvement:
- **Hypotheses Statement (Task Requirement 4):** The null and alternative hypotheses are stated as comments, not explicitly printed as required. The ground truth prints clear, verbose statements for H0 and Ha, which is better.
- **Clarity of Interpretation (Task Requirement 6):** While the interpretation is correct, it's slightly less verbose and formal than the ground truth. The ground truth clearly states what rejecting/failing to reject the null hypothesis implies in the context of the problem, referencing the 'PetalLengthCm' and 'Iris-versicolor' vs 'Iris-virginica'.
- **Error Handling:** The generated code lacks error handling for `FileNotFoundError` if 'iris.csv' is missing or if the filtered groups are empty, which was present in the ground truth for robustness. While not explicitly asked for, it's a good practice for production-ready code.

Overall, it's a functional solution but could be improved to meet all explicit requirements (especially statement of hypotheses) and general robustness/clarity standards.",claude-3.5-sonnet,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('iris.csv')

# Set figure size
plt.figure(figsize=(12,8))

# Create subplots
plt.subplot(2,2,1)
sns.boxplot(x='Species', y='SepalLengthCm', data=df)
plt.title('Sepal Length by Species')

plt.subplot(2,2,2)
sns.boxplot(x='Species', y='SepalWidthCm', data=df)
plt.title('Sepal Width by Species')

plt.subplot(2,2,3)
sns.boxplot(x='Species', y='PetalLengthCm', data=df)
plt.title('Petal Le...",9,"The generated code correctly loads the dataset and generates four box plots, each grouped by `Species`, for the specified numerical features, with appropriate titles. It successfully addresses all requirements of the task.

**Strong points:**
*   **Correctness:** The plots are exactly as requested: box plots for each numerical feature, grouped by `Species`, with titles.
*   **Efficiency:** Using `plt.subplot` is a good way to organize multiple plots in a single figure, which can be more convenient for viewing than separate figure windows.
*   **Conciseness:** The code is straightforward and to the point.

**Minor areas for improvement (not critical for this task, but good practice):**
*   **Error Handling:** The ground truth includes `try-except` blocks for file loading, which is good practice for robust applications. The generated code assumes the file exists. For a benchmark where the data is guaranteed to exist, this is less critical.
*   **Loop vs. Hardcoding:** The ground truth uses a loop to iterate through features, making the code more scalable if the number of features changes. The generated code hardcodes each subplot call. While perfectly fine for exactly four features, a loop would be more flexible for general-purpose code.
*   **Individual Plot Customization:** By putting all plots in one figure and calling `plt.tight_layout()` and `plt.show()` once at the end, individual plot elements (like `plt.xlabel`, `plt.ylabel`, `plt.grid` from the ground truth) are not set independently for each plot within the loop. For this specific task, basic titles suffice, but further customization would require separate `plt.show()` calls or more granular subplot management.",claude-3.5-sonnet,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
import numpy as np

# Load data
df = pd.read_csv('heart.csv')

# Select features and target
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']
X = df[features]
y = df['HeartDisease']

# One-hot encode categorical variables
categorical_features = ['Sex', 'ChestPainType']
onehot = OneHotEn...",7,"The generated code correctly performs most of the required steps: loading data, selecting features, one-hot encoding, splitting data, training a Logistic Regression model, and reporting accuracy.

However, there is a subtle but important issue in the one-hot encoding step. Using `drop='first'` in `OneHotEncoder` can introduce multicollinearity for certain models if not handled carefully, especially when combined with numerical features. While it often works, for a general solution adhering strictly to standard practice (and implicitly, what `ColumnTransformer` defaults to), `handle_unknown='ignore'` is usually preferred if one doesn't explicitly want to drop a category.

More critically, when `pd.concat` is used to combine `X_numeric` and `X_encoded`, if `X` (and thus `X_numeric` and `X_encoded`) has a non-default index (e.g., if rows were dropped or reordered), the concatenation might misalign. `ColumnTransformer` handles this implicitly by working on the original DataFrame and preserving index alignment. In this specific case, since the original DataFrame `df` likely has a default integer index, it might not be an issue, but it's a less robust pattern than `ColumnTransformer` for preprocessing.

The code also uses `model.score(X_test, y_test)` which is perfectly valid for accuracy, but the ground truth explicitly uses `accuracy_score` from `sklearn.metrics` which is good practice for explicitness, although not a functional difference here. The `max_iter` for `LogisticRegression` was not set, which might lead to a `ConvergenceWarning` for some datasets, though it still often converges.

Overall, it's a good attempt, but the manual one-hot encoding and concatenation could be more robustly implemented using `ColumnTransformer` as demonstrated in the ground truth.",claude-3.5-sonnet,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

# Load data
df = pd.read_csv('heart.csv')

# Create groups
male_chol = df[df['Sex'] == 'M']['Cholesterol']
female_chol = df[df['Sex'] == 'F']['Cholesterol']

# Perform t-test
t_stat, p_val = stats.ttest_ind(male_chol, female_chol)

# Print hypotheses
print(""Null Hypothesis (H0): There is no difference in mean cholesterol levels between males and females"")
print(""Alternative Hypothesis (Ha): There is a difference in mean cholesterol levels be...",8,"The generated code correctly performs an independent two-sample t-test as requested, successfully loads the data, creates the necessary groups, states the hypotheses, prints the t-statistic and p-value, and interprets the results based on the given alpha. The imports are correct and the output format is clear.

Areas for improvement:
1.  **Robustness**: The generated code does not include error handling for file not found, missing columns, or insufficient data in groups. The ground truth includes these checks, making it more robust for real-world scenarios.
2.  **`equal_var` parameter**: The generated code uses the default `equal_var=True` for `stats.ttest_ind` (Student's t-test), which assumes equal variances. The ground truth explicitly uses `equal_var=False` (Welch's t-test), which is generally more robust as it does not assume equal variances. While not explicitly stated in the prompt, using Welch's t-test is often preferred unless variance equality is confirmed.
3.  **Encapsulation**: The ground truth encapsulates the logic within a function, making the code reusable and cleaner. The generated code is a script, which is acceptable for a direct task but less modular. However, the prompt only asked for ""the Python code without explanations,"" so a functional approach was not explicitly mandated.

Overall, the generated code fulfills the core requirements of the task effectively. The mentioned improvements would make it more production-ready and statistically sound.",claude-3.5-sonnet,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Read data
df = pd.read_csv('winequality-red.csv', delimiter=';')

# Function to detect and handle outliers
def handle_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column]
    
    df_clean = df.copy()
    df_clean[column] = df_clean[colu...",8,"The generated code correctly addresses most of the requirements.

**Strong points:**
- Correctly identifies outliers using the IQR method.
- Correctly calculates the number of outliers.
- Correctly performs winsorization by capping values at the 1.5*IQR bounds using `clip()`, which is an efficient way to do this.
- Compares mean and standard deviation before and after treatment for both specified columns.
- Uses proper imports (pandas, numpy).
- Encapsulates outlier handling logic in a function, which improves code organization and reusability.

**Areas for improvement:**
- **Delimiter issue:** The winequality-red.csv dataset typically uses a semicolon (`;`) as a delimiter, but the task description implies it's a standard CSV (comma-separated). The generated code correctly uses `delimiter=';'`, which is good if the dataset indeed uses a semicolon. However, the ground truth assumes a comma (default for `pd.read_csv`). For the purpose of this evaluation, we'll assume the task implies a standard comma-separated CSV unless otherwise specified, making the `delimiter=';'` potentially incorrect depending on the actual file. *Self-correction: The ground truth implicitly assumes comma due to not specifying delimiter, but many versions of this dataset use semicolon. We should consider the user's explicit use of `delimiter=';'` as a possible correction or adaptation to a common variant of the dataset.* Given the ambiguity, it's safer to consider it a minor deviation rather than a critical error.
- **Reporting vs. Treatment:** While the code prints the number of outliers and caps them, the winsorization is done sequentially within the loop by modifying `df_processed` for one column and then passing that *already partially processed* `df_processed` to the next column's `handle_outliers` function. This is correct as it creates a single `df_processed` where both columns are treated. However, the phrasing ""Create a new dataset version where these identified outliers are capped"" might suggest creating this new version *after* identifying all 1.5*IQR bounds for all columns, rather than applying the treatment column by column within the loop for a single cumulative `df_processed`. The current approach is functionally correct for the task, but the wording in the prompt could be interpreted slightly differently.
- **Readability of output:** The print statements in the generated code are concise, but the ground truth provides a bit more context and clearer separation (e.g., ""Original Data:"", ""Winsorized Data:""). The generated code's output is still understandable.
- **Completeness check:** The prompt mentions ""assume the CSV file is in the current directory."" The generated code doesn't include error handling for `FileNotFoundError` like the ground truth, which is a good practice for robust code but not explicitly mandated by the prompt's core requirements. However, it's a minor point.

Overall, the solution is robust and effectively solves the problem. The use of `clip()` is very efficient for winsorization.",claude-3.5-sonnet,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('heart.csv')

# Create grouped counts
grouped_data = df.groupby(['ChestPainType', 'HeartDisease']).size().unstack()

# Set up the plot
fig, ax = plt.subplots(figsize=(10, 6))

# Create positions for bars
x = np.arange(len(grouped_data.index))
width = 0.35

# Plot bars
ax.bar(x - width/2, grouped_data[0], width, label='No Heart Disease', color='lightblue')
ax.bar(x + width/2, grouped_...",6,"The generated code correctly loads the data and attempts to create a grouped bar chart. It successfully plots two bars for each `ChestPainType` representing `HeartDisease` counts. It includes a title, labeled axes, and a legend. However, there are significant issues that prevent it from being a fully correct and robust solution:

1.  **Missing `numpy` import**: The code uses `np.arange` without importing `numpy`, which will lead to a `NameError`. This is a critical omission.
2.  **Order of `ChestPainType`**: The `grouped_data.index` will sort the `ChestPainType` alphabetically by default (ASY, ATA, NAP, TA) instead of the desired order (TA, ATA, NAP, ASY) as implied by the task description and typically preferred for this dataset. This affects the correctness of the visualization if the specific order matters.
3.  **Encapsulation/Modularity**: The code is not encapsulated in a function, which is good practice for reusable code and for handling potential file path issues.
4.  **Error Handling**: There's no error handling for `FileNotFoundError` or other issues during data loading.
5.  **Hardcoded Column Values**: The `grouped_data[0]` and `grouped_data[1]` rely on the `HeartDisease` values being exactly 0 and 1. While this is true for the `heart.csv` dataset, it makes the code less robust if those values were different (e.g., True/False).
6.  **Readability/Simplicity with Seaborn**: While the `matplotlib` approach is valid, seaborn's `countplot` with `hue` functionality is significantly more concise and idiomatic for this specific task, as demonstrated in the ground truth. The manual bar plotting with `numpy` and `width` calculations is more complex than necessary for this problem given the available tools.

To improve, the developer should:
*   Add `import numpy as np`.
*   Address the `ChestPainType` ordering issue, possibly by explicitly setting categorical order.
*   Encapsulate the logic in a function and add error handling for file operations.
*   Consider using `seaborn.countplot` for a more straightforward implementation as it directly handles grouping and counting, resulting in cleaner code.",claude-3.5-sonnet,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd
import numpy as np

df = pd.read_csv('spotify_2023.csv')

# Clean shazam charts column
df['in_shazam_charts'] = pd.to_numeric(df['in_shazam_charts'].str.replace(',',''), errors='coerce')
print(f""Number of NaN values in in_shazam_charts: {df['in_shazam_charts'].isna().sum()}"")

# Group and calculate averages
grouped = df.groupby(['key','mode'])[['danceability_%','valence_%']].mean()
print(""\nAverages by key and mode:"")
print(grouped)
```",9,"The generated code correctly addresses all parts of the task. It loads the CSV, accurately cleans `in_shazam_charts` by removing commas and converting to numeric with error coercion, and correctly reports the NaN count. Furthermore, it groups the data by `key` and `mode` and calculates the required average `danceability_%` and `valence_%`. The imports are correct, and the code structure is logical.

The only minor point for improvement, which doesn't detract from correctness, is the missing `try-except` block for `FileNotFoundError` when loading the CSV, which was present in the ground truth for robustness. However, the prompt specifically asked for ""only the Python code without explanations"" and to assume the file is in the current directory, which the generated code adheres to. Therefore, the implementation is robust for the given constraints.",claude-3.5-sonnet,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Load and prepare data
df = pd.read_csv('spotify_2023.csv')
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams', 'in_spotify_playlists', 'energy_%'])

# Select features and target
X = df[['in_spotify_playlists', 'energy_%']]
y = df['streams']

# Split data
X_train,...",9,"The generated code correctly implements all specified requirements: loading the dataset, ensuring 'streams' is numeric and handling NaNs (implicitly for predictors by including them in `dropna` subset), selecting features and target correctly, splitting the data, training a Linear Regression model, and reporting R2 and MSE. The code is concise and well-structured, following good practices for data science tasks.

The only minor point of difference from the ground truth is the lack of explicit error handling for `FileNotFoundError` during CSV loading and the exclusion of print statements that provide progress updates (e.g., dataset loaded, shapes of X and y, sizes of train/test sets, rows dropped). While these print statements are useful for debugging and user feedback in a real application, they were not explicitly mandated by the prompt to *print* these intermediate steps, only to *perform* them. The prompt asked for ""only the Python code without explanations,"" which the generated code largely adheres to in its output.

Therefore, the code is highly effective and correct for the given task.",claude-3.5-sonnet,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd
from datetime import datetime

df = pd.read_csv('spotify_2023.csv')

df['release_date'] = pd.to_datetime(
    df[['released_year', 'released_month', 'released_day']].astype(str).agg('-'.join, axis=1),
    format='%Y-%m-%d',
    errors='coerce'
)

end_date = pd.to_datetime('2023-12-31')
df['days_since_release'] = (end_date - df['release_date']).dt.days

print(df[['track_name', 'release_date', 'days_since_release']].head())
```",9,"The generated code is highly correct and concise. It successfully loads the dataset, creates the `release_date` column by combining the year, month, and day, handling errors gracefully with `errors='coerce'`. It correctly calculates `days_since_release` as the difference between `2023-12-31` and `release_date`, and displays the required columns for the first 5 records.

Strong points:
- Correctly uses `pd.to_datetime` with `errors='coerce'` to handle invalid dates, fulfilling the requirement.
- Efficiently combines year, month, and day using `astype(str).agg('-'.join, axis=1)`.
- Correctly calculates `days_since_release` using `dt.days`.
- Follows all display requirements.

Areas for minor improvement (not critical enough to significantly reduce the score):
- The `days_since_release` column type is inferred. While `dt.days` returns an integer, if `release_date` was `NaT`, `days_since_release` would be `NaN`, which is a float. For strict adherence to ""an integer"" (though `NaN` is technically float), explicitly converting `days_since_release` to `pd.Int64Dtype()` or `df.astype('Int64')` would ensure it's a nullable integer type. The current implementation is generally acceptable given the context.
- No explicit error handling for `pd.read_csv` (e.g., `FileNotFoundError`), which was present in the ground truth for robustness, but not strictly asked for in the prompt's main requirements.
- The `from datetime import datetime` import is not strictly necessary as `pd.to_datetime` handles everything.",claude-3.5-sonnet,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

df = pd.read_csv('youtube_trendings.csv')

df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',',''), errors='coerce').fillna(0)
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',',''), errors='coerce').fillna(0)
df['Category'] = df['Category'].fillna('Unknown')

grouped = df.groupby('Category').agg({
    'Video views': 'sum',
    'Likes': 'sum'
})

print(grouped)
```",9,"The generated code correctly addresses all requirements of the task. It loads the CSV, performs the specified cleaning on 'Video views' and 'Likes' columns (removing commas, converting to numeric, filling NaNs with 0), fills missing 'Category' values with 'Unknown', and then correctly groups by 'Category' to sum the cleaned 'Video views' and 'Likes'. The use of `agg` is concise and effective.

A minor improvement, though not strictly required by the prompt's output format, would be to include error handling for `FileNotFoundError` as seen in the ground truth, making the script more robust. Also, explicitly calling `print` for the final result is good, but the ground truth adds a descriptive header, which is a nice touch for readability. However, in terms of core functionality and meeting the task requirements, the generated code is excellent.",claude-3.5-sonnet,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats

# Load data
df = pd.read_csv('youtube_trendings.csv')

# Clean Likes column
df['Likes'] = df['Likes'].str.replace(',', '').apply(pd.to_numeric, errors='coerce').fillna(0)

# Fill missing Category values
df['Category'] = df['Category'].fillna('Unknown')

# Create groups
music_likes = df[df['Category'] == 'Music']['Likes']
sports_likes = df[df['Category'] == 'Sports']['Likes']

# Perform t-test
t_stat, p_value = stats.ttest_ind(music_likes, sp...",8,"The generated code correctly addresses most of the requirements.

**Strong Points:**
- **Correct Imports:** Uses `pandas` and `scipy.stats` as required.
- **Data Loading:** Loads the CSV successfully.
- **Likes Cleaning:** The chain `.str.replace(',', '').apply(pd.to_numeric, errors='coerce').fillna(0)` is concise and correctly handles comma removal, conversion to numeric (with errors coerced to NaN), and filling NaNs with 0. This is a very efficient way to perform the cleaning.
- **Category Filling:** Correctly fills missing 'Category' values with 'Unknown'.
- **Group Creation:** Successfully creates `music_likes` and `sports_likes` subsets.
- **T-test Execution:** Performs the `ttest_ind` correctly.
- **Hypotheses Statement:** States the null and alternative hypotheses clearly.
- **Result Printing:** Prints the t-statistic and p-value.
- **Interpretation:** Correctly interprets the p-value against the alpha of 0.05.

**Areas for Improvement (Why not a 10):**
- **Robust File Loading:** The ground truth includes a `try-except FileNotFoundError` block for robust file loading, which is good practice for user-facing scripts. The generated code assumes the file exists.
- **New Column for Cleaned Likes:** The generated code overwrites the original 'Likes' column. While this technically works, creating a *new* column (e.g., 'Likes_cleaned' as in the ground truth) is often better practice to preserve original data for debugging or other analyses. The prompt didn't explicitly forbid overwriting, but it's a code quality point.
- **`equal_var` in `ttest_ind`:** The ground truth used `equal_var=False` for Welch's t-test, which is generally safer when assuming unequal variances is prudent (a common scenario unless you have strong reasons to assume equality). The generated code uses the default `equal_var=True`, which assumes equal population variances. While not an explicit requirement in the prompt, it's a statistical nuance that a very ""expert"" solution might consider.
- **Lack of Print Statements for Cleaning Steps:** The ground truth includes print statements to indicate successful completion of each step (loading, cleaning, filling). This provides better feedback to the user running the script, which is helpful in benchmarks and real-world applications.
- **Clarity of Hypotheses:** While stated, the hypotheses could be slightly more formal/specific (e.g., mentioning ""mean cleaned 'Likes'"" and comparing ""_music"" to ""_sports"" as in the ground truth).

Overall, the core logic is sound and solves the problem effectively and efficiently. The points for improvement are mostly about robustness, best practices, and statistical rigor that would elevate it to a perfect score.",claude-3.5-sonnet,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('youtube_trendings.csv')

# Clean likes and dislikes columns
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',',''), errors='coerce').fillna(0)
df['Dislikes'] = pd.to_numeric(df['Dislikes'].str.replace(',',''), errors='coerce').fillna(0)

# Create new columns
df['TotalEngagement'] = df['Likes'] + df['Dislikes']
df['LikeDislikeRatio'] = df['Likes'] / (df['Dislikes'] + 1)

# Display results
print(df[['Video', 'Tota...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, correctly cleans the `Likes` and `Dislikes` columns by removing commas, converting to numeric, and filling NaNs with 0. It then accurately creates the `TotalEngagement` and `LikeDislikeRatio` columns as specified, including the +1 to avoid division by zero. Finally, it displays the required columns.

The code is concise and efficient, leveraging pandas' vectorized operations effectively for cleaning and calculation. The imports are proper.

The only minor area for improvement, preventing a perfect 10, is the lack of explicit error handling for file not found or other loading issues, which was present in the ground truth. While not strictly required by the prompt's explicit points, robust data science code typically includes such checks. Additionally, the ground truth explicitly converted columns to `float` after cleaning, reinforcing the numeric type, although `pd.to_numeric` often handles this implicitly.",claude-3.5-sonnet,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

df = pd.read_csv('mobiles_2025.csv')

df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''), errors='coerce')
df['Screen Size'] = pd.to_numeric(df['Screen Size'].str.replace(' inches', ''), errors='coerce')

grouped = df.groupby(['Company Name', 'Launched Year']).agg({
    'Launched Price (USA)': 'mean',
    'Screen Size': 'mean'
}).round(2)

print(grouped)
```",9,"The generated code successfully completes all aspects of the task, including loading the CSV, cleaning both specified columns by removing specific strings and converting to numeric with error handling, grouping by the correct columns, and calculating the required averages. It efficiently chains the string replacements before conversion.

The only minor area for improvement is that the column names for the aggregated data remain 'Launched Price (USA)' and 'Screen Size', whereas the ground truth renames them to explicitly indicate they are averages and cleaned values, which can improve clarity. However, the task only specified calculating the averages within the groups, not renaming the aggregated columns. Additionally, the ground truth includes more robust error handling for file loading, which is good practice but not explicitly requested as part of the core data manipulation task. The generated code is concise and effective.",claude-3.5-sonnet,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

def extract_first_mp(camera_str):
    if pd.isna(camera_str):
        return 0
    match = re.search(r'\d+', str(camera_str))
    return int(match.group()) if match else 0

df = pd.read_csv('mobiles_2025.csv')

df['Cleaned Front Camera'] = df['Front Camera'].apply(extract_first_mp)
df['Cleaned Back Camera'] = df['Back Camera'].apply(extract_first_mp)
df['Total Camera MP'] = df['Cleaned Front Camera'] + df['Cleaned Back Camera']

print(df[['Model Name', 'C...",9,"The generated code successfully completes almost all aspects of the task.

**Correctness:**
*   It correctly loads the CSV.
*   The `extract_first_mp` function accurately extracts the first sequence of digits, handles NaN values, and returns 0 if no digits are found, matching the ground truth's logic for camera cleaning.
*   It correctly creates the `Cleaned Front Camera` and `Cleaned Back Camera` columns.
*   It accurately calculates `Total Camera MP`.
*   It displays the specified columns for the first 10 rows.

**Code Quality:**
*   Proper imports (`pandas`, `re`) are included.
*   The function `extract_first_mp` is well-defined and concise.
*   The code is generally clean and readable.

**Completeness:**
*   The primary task requirements are met.
*   The only minor aspects missing compared to the ground truth are:
    *   Error handling for `FileNotFoundError` when loading the CSV.
    *   Informative `print` statements throughout the execution (e.g., ""Dataset loaded successfully"").
    *   The use of `.to_string()` for potentially better console formatting of the output, although `print(df.head(10))` provides a perfectly acceptable default pandas output.

These missing elements are more about robustness and user experience rather than core functionality, which is why the score is very high.",claude-3.5-sonnet,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load data
df = pd.read_csv('mobiles_2025.csv')

# Clean columns
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
df['Screen Size'] = pd.to_numeric(df['Screen Size'].str.replace...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, performs the specified cleaning operations on the required columns, handles conversion errors by coercing to NaN and then dropping rows, defines features and target, splits the data, trains a Linear Regression model, and finally evaluates it by reporting R-squared and MAE. The imports are correct, and the code structure is clear and logical.

One minor area for improvement, which is not critical but good practice, would be to include error handling for `pd.read_csv` (e.g., a `try-except` block for `FileNotFoundError`). Also, explicitly converting columns to `astype(str)` before string operations, as in the ground truth, can prevent potential issues if a column contains mixed data types, although `str.replace` usually handles this gracefully by operating on string representations. However, for this task, the generated code is highly effective and accurate.",claude-3.5-sonnet,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
import math

# Read data
df = pd.read_csv('titanic.csv')

# 1. Chi-square test for Pclass vs Survival
contingency = pd.crosstab(df['Pclass'], df['Survived'])
chi2, p_chi2, dof, expected = stats.chi2_contingency(contingency)
cramers_v = np.sqrt(chi2/(len(df) * (min(contingency.shape)-1)))

# 2. T-test for Age between survivors and non-survivors
survivors_age = df[df['Survived']==1]['Age'].dropna()
nonsurvivors_age = df[df['S...",7,"The generated code generally performs the requested statistical tests and calculates effect sizes. However, there are several areas for improvement, particularly regarding robustness, interpretation clarity, and a minor error in one effect size calculation.

**Strong Points:**
*   Correctly implements Chi-square, T-test, and ANOVA.
*   Attempts to calculate and display effect sizes (Cramer's V, Cohen's d, Eta-squared).
*   Correctly uses `pd.crosstab` for the chi-square contingency table.
*   Uses `dropna()` for age in the t-test, which is good.
*   Includes a post-hoc test for ANOVA, which is a good practice if the ANOVA is significant (though not explicitly requested, it adds value).

**Areas for Improvement/Corrections:**

1.  **Missing Error Handling for File Loading:** The generated code directly reads the CSV without `try-except` for `FileNotFoundError`. The ground truth includes this important robustness feature.
2.  **Explicit Alpha Level Definition and Use:** The generated code implicitly uses `0.05` in the print statements. The ground truth defines `alpha` as a variable and uses it consistently, which is better practice for maintainability and clarity.
3.  **Incomplete Interpretation:** The generated code only prints ""Significant at =0.05: True/False"". The ground truth provides a more detailed, verbal conclusion for each test, stating whether the null hypothesis is rejected or not, and what that implies in the context of the variables. This is crucial for interpreting the results.
4.  **Cohen's d Calculation:** The `cohens_d` calculation in the generated code uses `(survivors_age.var() + nonsurvivors_age.var()) / 2`, which is a simplified formula for cases where sample sizes are equal or variances are assumed equal. The ground truth uses the `pooled_std` formula, which is more generally correct for independent samples t-test, especially when sample sizes differ (which they do here). While conceptually close, the ground truth's calculation is more robust.
5.  **Eta-squared Calculation:** The calculation for `eta_squared` in the generated code `((df['Fare'].std()**2) * (len(df['Fare'])-1))` in the denominator is incorrect for SS_total. SS_total should be the sum of squared deviations from the grand mean of all data points, not based on the entire column's variance and count this way. The ground truth correctly calculates `ss_total = np.sum((all_fares - grand_mean_fare)**2)`, which is the standard formula. This is a significant error in the effect size calculation.
6.  **Missing Assumptions Check (Levene's Test):** The ground truth includes Levene's test before t-test and ANOVA to check for homogeneity of variances, and then uses `equal_var` parameter in `ttest_ind` accordingly. This demonstrates a better understanding of the assumptions underlying these parametric tests and how to handle them. The generated code skips this important step.
7.  **No Comments/Explanation:** The ground truth includes extensive comments explaining each step, the hypotheses, and the formulas for effect sizes. This significantly improves readability and understanding, which is important for data science code that performs complex analyses.
8.  **Output Formatting:** While the generated code prints the results, the ground truth provides more structured and labeled output (e.g., using dividers like `---` and clear headers for each test).

To reach a higher score, the generated code needs to fix the Eta-squared calculation, provide more detailed interpretations, handle assumptions more rigorously (Levene's test), and improve overall code clarity and robustness (file loading, comments).",claude-3.5-sonnet,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 

# Load and process data
df = pd.read_csv('imdb_top_250.csv')
yearly_stats = df.groupby('Year').agg({
    'Rating': 'mean',
    'Runtime': 'mean'
}).reset_index()

# Create plot
plt.figure(figsize=(12,6))
sns.set_style(""whitegrid"")
plt.plot(yearly_stats['Year'], yearly_stats['Rating'], marker='o')

plt.title('Average IMDB Rating by Year', fontsize=14, pad=15)
plt.xlabel('Year', fontsize=12)
plt.ylabel('Average R...",7,"The generated code correctly loads the dataset, performs the required grouping and aggregation, and generates a line plot as requested. It uses well-named variables and includes necessary imports.

However, it falls short in robust data cleaning. The `imdb_top_250.csv` dataset often has `Year` values like "" (2000)"" and `Runtime` values like ""142 min"". The generated code directly tries to group by `Year` and calculate the mean of `Runtime`, which would likely fail or produce incorrect results due to non-numeric data in these columns. This missing data preparation step is crucial for the code to run successfully on the actual dataset. The ground truth code includes robust cleaning for both `Year` and `Runtime`.

Areas for improvement:
1.  **Data Cleaning:** Add robust data cleaning for `Year` (e.g., extracting numeric year from ""(2000)"") and `Runtime` (e.g., removing "" min"" and converting to integer). The current code implicitly assumes these columns are clean.
2.  **Error Handling:** Although not explicitly requested in the prompt, adding basic error handling (e.g., for `FileNotFoundError`) would make the code more robust, as seen in the ground truth.",claude-3.5-sonnet,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

# Load dataset
df = pd.read_csv('imdb_top_250.csv')

# Create two groups
pre_1990 = df[df['Year'] < 1990]['Rating']
post_1990 = df[df['Year'] >= 1990]['Rating']

# Perform t-test
t_stat, p_value = stats.ttest_ind(pre_1990, post_1990)

# Print results
print(""Null Hypothesis (H0): There is no difference in mean ratings between movies released before 1990 and from 1990 onwards"")
print(""Alternative Hypothesis (H1): There is a difference in mean ...",7,"The generated code correctly performs the t-test and interprets the results. It loads the dataset, creates the two groups, and applies the `ttest_ind` function. The hypotheses are stated, and the t-statistic and p-value are printed, followed by a correct interpretation based on alpha = 0.05.

However, there are a few areas for improvement where the ground truth code is more robust:

1.  **Error Handling for File Loading:** The generated code lacks a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness.
2.  **Data Type Conversion and Error Handling:** The ground truth code explicitly converts `Year` and `Rating` columns to numeric types using `pd.to_numeric` with `errors='coerce'` and then drops NaNs. This makes the code more robust against potential non-numeric entries in the CSV, which the generated code doesn't handle, and could lead to errors if the columns aren't already in the correct numeric format.
3.  **Explicit Statement of Alpha:** While the generated code correctly uses `0.05` for the comparison, it doesn't explicitly declare `alpha = 0.05` as a variable and print it, as requested in requirement 6, which helps in clarity and making the significance level easily modifiable.
4.  **Assumptions Check (Equal Variance):** The ground truth comments on the `equal_var` parameter in `ttest_ind`. While not strictly required by the prompt, it demonstrates a deeper understanding of statistical tests. The generated code defaults to `equal_var=True`.
5.  **Output Clarity:** The output of the generated code could be slightly more formatted and descriptive, similar to the ground truth (e.g., adding headers like ""--- Hypotheses ---"", ""--- T-Test Results ---"", ""--- Interpretation ---"").

Overall, it's a good attempt that covers the core requirements, but lacks some of the robustness and detailed output formatting of the ideal solution.",claude-3.5-sonnet,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

df = pd.read_csv('imdb_top_250.csv')

df['Decade'] = (df['Year'] // 10 * 10).astype(str) + 's'

print(df[['Title', 'Year', 'Decade']].head(10))
```",9,"The generated code correctly solves the task by loading the CSV, calculating the `Decade` column, and printing the specified columns for the first 10 entries. The logic for creating the `Decade` column is concise and accurate.

Strengths:
- **Correctness**: Successfully loads the data, creates the `Decade` column using the described logic, and displays the required output.
- **Conciseness**: The solution is very direct and to the point.
- **Efficiency**: Uses vectorized pandas operations for `Decade` creation.

Areas for minor improvement (why it's not a perfect 10):
- **Error Handling**: The generated code lacks explicit error handling (e.g., `try-except` for `FileNotFoundError` or missing columns), which is present in the ground truth and is good practice for robust code.
- **Readability/Structure**: While concise, wrapping the core logic in a function (like `perform_feature_engineering` in the ground truth) and a `main` function improves modularity and reusability, especially for larger scripts.
- **Self-contained `DATASET_PATH`**: The ground truth defines `DATASET_PATH` as a constant, which makes it easier to modify the path.
- **Copying DataFrame**: The ground truth uses `df.copy()` before feature engineering, which is a good practice if `df` might be used in its original state elsewhere, preventing unintended side effects. For this specific task, it's not strictly necessary as `df` isn't reused, but it's a defensive programming habit.

Overall, it's a very good and functional solution for the given task.",claude-3.5-sonnet,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Load data
df = pd.read_csv('penguins.csv')

# Impute numeric columns with mean
numeric_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numeric_cols:
    df[col] = df[col].fillna(df[col].mean())

# Impute sex with mode
df['sex'] = df['sex'].fillna(df['sex'].mode()[0])

# Group by species and sex, calculate mean body mass
result = df.groupby(['species', 'sex'])['body_mass_g'].mean()
print(result)
```",10,"The generated code is correct, complete, and of good quality. It successfully loads the dataset, correctly imputes missing numerical values with the mean and missing categorical values in 'sex' with the mode, and then groups the data by 'species' and 'sex' to calculate the average 'body_mass_g'. All requirements of the task are met. The code is concise and efficient.",claude-3.5-sonnet,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Load and prepare data
df = pd.read_csv('penguins.csv')
df = df[df['species'].isin(['Adelie', 'Gentoo'])]
df = df.dropna(subset=['flipper_length_mm', 'species'])

# Split data into two groups
adelie = df[df['species'] == 'Adelie']['flipper_length_mm']
gentoo = df[df['species'] == 'Gentoo']['flipper_length_mm']

# Perform t-test
t_stat, p_val = stats.ttest_ind(adelie, gentoo)

# Print results
print(""Null Hypothesis (H0): There is no differen...",8,"The generated code correctly performs all the core steps required by the task: loading, filtering, NA handling, performing the t-test, stating hypotheses, and interpreting the results.

Strong points:
- Correctly loads the data.
- Correctly filters for 'Adelie' and 'Gentoo' species.
- Correctly handles NAs in the specified columns.
- Correctly performs the independent two-sample t-test using `scipy.stats.ttest_ind`.
- Correctly states the null and alternative hypotheses.
- Correctly prints the t-statistic and p-value.
- Correctly interprets the p-value against the alpha of 0.05.

Areas for improvement (to achieve a higher score, closer to the ground truth):
- **Hardcoded alpha:** The significance level (0.05) is hardcoded in the interpretation step (`p_val < 0.05`). It's better practice to define it as a variable (e.g., `ALPHA = 0.05`) at the beginning for easier modification and readability, as shown in the ground truth.
- **Robustness (File handling):** The ground truth includes error handling for `FileNotFoundError` when loading the CSV, which makes the code more robust. The generated code assumes the file will always be present.
- **`equal_var` parameter in `ttest_ind`:** The `ttest_ind` function defaults to `equal_var=True`. While the problem description doesn't explicitly state whether to assume equal variances, in real-world scenarios, it's often safer to use `equal_var=False` (Welch's t-test) unless there's a specific reason to assume equal variances. The ground truth explicitly addresses this point and chooses `equal_var=False` for robustness.
- **Clarity of Hypothesis:** While the hypotheses are stated, the ground truth provides a slightly more formal and precise statement using statistical notation (e.g., ""_Adelie = _Gentoo"").
- **Explicitness of output:** The ground truth includes more descriptive print statements for each step (e.g., showing dataset shapes after filtering/NA removal), which aids in debugging and understanding the data transformation process, though not strictly required by the prompt.",claude-3.5-sonnet,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression

# Load data
df = pd.read_csv('penguins.csv')

# Select features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]
y = df['species']

# Drop rows with missing values
mask = y.notna() & X.notna().all(axis=1)
X = X[mask]
y = y[mask]

# Encode target variable
le = LabelEncode...",9,"The generated code correctly implements all specified requirements of the task. It loads the dataset, selects the correct features and target, handles missing values (though the `dropna` method is generally more idiomatic for this), encodes the target variable, splits the data with the correct parameters, trains a Logistic Regression model with `max_iter=1000`, and reports the accuracy.

Strengths:
- All required libraries are imported.
- Correct features and target are selected.
- Missing values are handled for both features and target as specified.
- `LabelEncoder` is used correctly for the target.
- Data splitting is done with the correct `test_size` and `random_state`.
- `LogisticRegression` model is initialized with `max_iter=1000`.
- Accuracy is calculated and printed.

Areas for minor improvement:
- The missing value handling with `mask = y.notna() & X.notna().all(axis=1)` is functional but less idiomatic than using `df.dropna(subset=...)` after selecting the columns of interest, which would cleanly drop rows where *any* of the specified columns have NA. While it works here because the columns are directly used, for more complex scenarios, `dropna` on the original DataFrame or a subset is clearer.
- The task explicitly requested reporting the accuracy score using `sklearn.metrics.accuracy_score`. While `model.score()` achieves the same numerical result for classification models, adhering strictly to the request would mean using `accuracy_score(y_test, y_pred)`. This is a very minor point, as the result is identical.",claude-3.5-sonnet,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load and clean data
df = pd.read_csv('penguins.csv')
df = df.dropna(subset=['body_mass_g', 'species'])

# Calculate average body mass by species
avg_mass = df.groupby('species')['body_mass_g'].mean()

# Create bar plot
plt.figure(figsize=(10,6))
sns.barplot(x=avg_mass.index, y=avg_mass.values)
plt.title('Average Body Mass by Penguin Species', fontsize=14)
plt.xlabel('Species', fontsize=12)
plt.ylabel('Body Mas...",9,"The generated code correctly performs all the required tasks: loading the data, dropping NA values in the specified columns, calculating the average body mass per species, and generating a properly titled and labeled bar plot.

**Strong points:**
*   Correctly uses pandas for data loading and manipulation.
*   Correctly handles NA values as specified.
*   Accurately calculates the average body mass per species.
*   Generates a clear and well-labeled `seaborn` bar plot with matplotlib adjustments.
*   The code is concise and readable.

**Minor areas for improvement (to achieve a perfect 10, similar to the ground truth):**
*   **Error Handling:** The ground truth includes `try-except` block for `FileNotFoundError`, which makes the code more robust. The generated code lacks this.
*   **Variable Naming:** While `avg_mass` is acceptable, using more descriptive names for the grouped series (e.g., `average_body_mass_per_species`) can sometimes improve clarity for larger scripts.
*   **Output:** The ground truth includes print statements to show the shape of the dataframe, initial NA counts, and the calculated averages, which can be useful for debugging and understanding data transformations. The generated code doesn't provide these informational outputs.
*   **Plot Customization:** The ground truth adds a grid and uses a `palette` in seaborn, which are nice-to-have but not strictly required. The ground truth also explicitly uses f-strings to construct dynamic titles and labels based on variable names, which is a good practice for maintainability.

Overall, it's a very good solution that fulfills the prompt's core requirements effectively.",claude-3.5-sonnet,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('penguins.csv')

# Drop rows with NA in specified columns
df = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 
                      'flipper_length_mm', 'body_mass_g'])

# Create new culmen_area feature
df['culmen_area'] = df['culmen_length_mm'] * df['culmen_depth_mm']

# Calculate correlation matrix for selected columns
cols = ['culmen_length_mm', 'culmen_depth_mm', 'f...",9,"The generated code correctly performs all the required steps: loading the data, dropping NA values in the specified columns, creating the `culmen_area` feature, calculating the correlation matrix for the specified columns, and visualizing it as an annotated heatmap with the correct title. The imports are proper, and the code is structured logically.

The only minor point where the ground truth is slightly more robust is the explicit error handling for `FileNotFoundError` and checking if the DataFrame becomes empty after dropping NAs. While not strictly required by the prompt's output format (assuming CSV is in the directory and data exists), it's good practice for production-level code. However, the generated code directly addresses all specified task requirements effectively and efficiently. It's a clean and correct solution.",claude-3.5-sonnet,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder

# Load the data
df = pd.read_csv('penguins.csv')

# Select features and target
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
target = 'species'
X = df[features]
y = df[target]

# Impute numerical features with mean
numeric_features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numeric_features:
    X...",8,"The generated code correctly addresses most aspects of the task, demonstrating a good understanding of data preprocessing in Python.

**Strong Points:**
*   **Correct Feature Selection:** The specified features and target variable are correctly identified and separated.
*   **Accurate Numerical Imputation:** NA values in numerical columns are correctly imputed with their respective means.
*   **Correct 'sex' Processing:** NA values in 'sex' are imputed with 'Unknown', and the mapping to 0, 1, and -1 is correctly applied.
*   **One-Hot Encoding:** The 'island' column is correctly one-hot encoded using `OneHotEncoder`, which is a robust way to handle categorical features. The integration back into the DataFrame is also handled well.
*   **Output Requirements Met:** The shape of the resulting feature matrix and the first 5 rows are printed as requested.

**Areas for Improvement (Why not a 10):**
*   **`SettingWithCopyWarning`:** The imputation of numerical features (`X[col] = X[col].fillna(X[col].mean())`) and the 'sex' column (`X['sex'] = X['sex'].fillna('Unknown')`, `X['sex'] = X['sex'].map(sex_mapping)`) inside the `X = df[features]` slice can potentially trigger `SettingWithCopyWarning`. While it often works, it's best practice to explicitly work on a copy of the slice to avoid this warning and ensure modifications are made on a distinct DataFrame. The ground truth code uses `.copy()` for this reason.
*   **Robustness of `OneHotEncoder` Integration:** When using `OneHotEncoder` and then `pd.concat`, it's important to ensure the indices align correctly. In this case, since `X` is directly from `df` and `island_df` is created from `X[['island']]`, the indices will implicitly align. However, if any rows were dropped or reordered in `X` before encoding, this could lead to issues. Using `pd.get_dummies` directly on `X` is often more robust as it handles the replacement and column dropping automatically.
*   **Missing Error Handling:** The code does not include any error handling for `FileNotFoundError` when loading the CSV, which was present in the ground truth for robustness.
*   **Redundant Import:** `numpy` is imported but not explicitly used. While not a critical issue, it's good practice to only import necessary libraries.

Overall, it's a solid solution that effectively performs the required data preprocessing steps. The minor issues relate to best practices and robustness rather than core incorrectness.",claude-3.5-sonnet,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

# Create DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002'...",9,"The generated code correctly performs all the required tasks: it creates the DataFrame, calculates total spending per customer, identifies and prints the top 3 customers by spending, and calculates the total quantity sold per category. The code is concise and uses appropriate pandas functions.

One minor area for improvement is in the printing of the top 3 customers. The ground truth code formats the output for the top 3 customers to be more human-readable (e.g., ""CustomerID: CUST001, Total Spending: $3000.00""). The generated code simply prints the head of the series, which is correct but less user-friendly in its presentation compared to the explicit formatting shown in the ground truth. Also, the ground truth explicitly iterates and prints each top customer, which can be seen as an improvement for clarity in presentation. However, the core logic is sound.",claude-3.5-sonnet,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_c...",7,"The generated code successfully tackles most of the requirements but falls short in a few key areas, preventing it from being a top-tier solution.

**Strong Points:**
*   **Comprehensive Model Evaluation:** The code correctly calculates and prints all required metrics (accuracy, precision, recall, F1-score, ROC-AUC).
*   **Visualization:** It includes correct implementations for displaying the confusion matrix and ROC curve.
*   **Cross-Validation:** It performs cross-validation and reports the mean and standard deviation of the scores.
*   **Feature Coefficients:** It correctly displays feature coefficients.
*   **Basic Preprocessing:** It handles missing values for 'Age', 'Embarked', and 'Fare' (though 'Fare' is not explicitly missing in the default `titanic.csv` dataset, it's a harmless addition).

**Areas for Improvement / Missing Features:**

1.  **Preprocessing Pipeline (Major Missing Piece):** The ground truth uses `ColumnTransformer` and `Pipeline` for preprocessing, which is a much more robust and standard way to handle different transformations for different column types (numerical scaling, categorical one-hot encoding). The generated code manually preprocesses `X` before splitting the data, which is problematic for cross-validation where scaling should happen *within* each fold to prevent data leakage. The `LabelEncoder` for 'Sex' and 'Embarked' is also less suitable than `OneHotEncoder` for logistic regression, as it imposes ordinality on nominal categories.
2.  **Dropping Irrelevant Columns:** The generated code does not drop 'PassengerId', 'Name', 'Ticket', or 'Cabin'. While it performs some feature engineering on 'Name' to create 'Title', the other columns should ideally be dropped or handled explicitly if they are not used. The ground truth correctly identifies and drops these.
3.  **Feature Interpretation:** While coefficients are displayed, the interpretation provided in the ground truth is crucial for understanding the model, which is missing from the generated code.
4.  **Handling `Pclass` as Categorical:** The ground truth correctly treats `Pclass` as a categorical variable and one-hot encodes it. The generated code treats it as a numerical variable, which might not be ideal for a logistic regression model as the numerical ordering (1, 2, 3) doesn't imply a linear relationship with survival odds.
5.  **Robust Cross-Validation Scoring:** The ground truth uses `cross_validate` to obtain multiple metrics (accuracy, precision, recall, etc.) during cross-validation, providing a more complete picture of stability across different performance aspects. The generated code only uses `cross_val_score` which typically returns only the 'accuracy' by default (or one specified metric), making the CV evaluation less comprehensive.
6.  **`sklearn` `Pipeline` for Model Training:** The ground truth beautifully integrates preprocessing and model training into a single `Pipeline`, simplifying training, prediction, and cross-validation, and preventing data leakage. The generated code performs these steps separately.
7.  **Reproducibility (Minor):** The ground truth includes `np.random.seed(42)` which is good practice for full reproducibility.

Overall, the generated code performs the core tasks and evaluation but lacks the robust, production-ready preprocessing pipeline and thoroughness seen in the ground truth.",claude-3.5-sonnet,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

# Create DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']), 
        'Ticker': ['TECHCO']*8,
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
        'Close': [152.0, 151.0, 153.5, 155.0, 154....",10,"The generated code is excellent. It correctly imports pandas, creates the DataFrame exactly as specified, sets the 'Date' column as the index, and accurately calculates both the 3-day Simple Moving Average for 'Close' prices and the percentage change for 'Volume'. Both calculations correctly handle the initial NaN values as required. The final output also includes the requested DataFrame with the new columns. The code is concise, well-structured, and directly addresses all aspects of the prompt.",claude-3.5-sonnet,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

# Create DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]
}
df = pd.DataFrame...",9,"The generated code correctly performs all the required tasks:
1. It successfully creates the DataFrame.
2. It correctly calculates and adds the 'Score_Improvement' column.
3. It accurately calculates and prints the average 'Score_Improvement' per subject.
4. It correctly identifies and prints the StudentIDs of students meeting the final score and attendance criteria.

The code is concise and efficient. The major difference from the ground truth is the lack of explicit print statements with descriptive headers for each step, and the use of `tolist()` for the final output, which is functionally correct but might not be as clear as iterating and printing each ID separately for some presentation contexts. However, `tolist()` is a perfectly valid and often preferred way to get the output from a Series. Overall, it's a very good solution. The added `print` statements in the ground truth are primarily for readability in a ""demonstration"" context, not strictly required for correctness.",claude-3.5-sonnet,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

tech_high_performers = df[(df['Department'] == 'Tech') & ...",9,"The generated code correctly defines the DataFrame, filters it based on the specified conditions (""Tech"" department and ""PerformanceScore"" greater than 3), and accurately calculates the average ""ProjectsCompleted"" for the filtered subset. The code is concise and directly addresses all the requirements of the task.

One minor area for improvement, which is why it didn't get a perfect 10, is the lack of a check for an empty filtered DataFrame. While in this specific dataset, there will always be matching entries, in a more general scenario, if `tech_high_performers` happened to be empty, `tech_high_performers['ProjectsCompleted'].mean()` would return `NaN`. The ground truth code handles this edge case gracefully by checking `if not filtered_df.empty:`. However, for the given task and dataset, the generated code produces the correct numerical output. Otherwise, it's excellent.",claude-3.5-sonnet,LLM_judge_config
M53,medium,Data Manipulation,,"import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

desktop_conversion = df[df['DeviceType'] == 'Desktop']['Converted'].mean() * 100
mobile_conversion = df[df['DeviceType'] == 'Mobile']['Co...",9,"The generated code correctly defines the DataFrame and accurately calculates and prints the conversion rates for both Desktop and Mobile users, addressing all parts of the task. The code is concise and uses direct pandas operations effectively.

The primary reason it doesn't get a perfect 10 is the lack of explicit handling for edge cases where `df[df['DeviceType'] == 'Desktop']` or `df[df['DeviceType'] == 'Mobile']` might result in an empty DataFrame. In such scenarios, `.mean()` on an empty Series of booleans would produce `NaN`. While for the given dataset this isn't an issue, robust production code should ideally account for this, as demonstrated in the ground truth with `if not desktop_users_df.empty:`. This is a minor point for this specific problem but important for general robustness.",claude-3.5-sonnet,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""], 
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

avg_aqi = df.groupby('SensorID')['AirQualityIndex'].mean()
print('Average Air ...",9,"The generated code correctly defines the DataFrame and accurately calculates the average ""AirQualityIndex"" for each ""SensorID"". The code is concise and uses the correct pandas grouping and aggregation methods.

The only minor area for improvement is in the printing of the results. While `print(avg_aqi)` does display the sensor ID and its average AQI, the ground truth code used a loop (`for sensor_id, avg_aqi in average_aqi_per_sensor.items(): print(f"" Sensor {sensor_id}: {avg_aqi:.2f}"")`) to format the output more clearly and precisely (e.g., formatting to two decimal places and explicitly mentioning ""Sensor""). This aligns slightly better with the ""clearly indicate which average belongs to which sensor"" part of the prompt. However, the generated code's output is still understandable and correct.",claude-3.5-sonnet,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

df['EngagementScore'] = df['Likes'] + (df['Shares'] * 2) + (df['Comments'] * 3)
instagram_df = d...",9,"The generated code correctly defines the DataFrame, calculates the ""EngagementScore"", filters for Instagram posts, and accurately calculates their average ""EngagementScore"". All requirements of the task are met. The code is concise and directly addresses the problem.

The only minor improvements could be in the output formatting (e.g., f-string with formatting for the average score, as in the ground truth, to match potential benchmark expectations for specific output format) and the addition of a `try-except` block or `if not instagram_df.empty:` check for robustness in case there are no Instagram posts, although for this specific dataset and task, it's not strictly necessary as Instagram posts are guaranteed. However, since the task specifically asks for a print statement, and the generated code simply prints the raw float, it aligns slightly less with a well-formatted output one might expect in a production scenario. Given the simple printing instruction, printing the raw output from `mean()` is technically correct.",claude-3.5-sonnet,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",10,"The generated code accurately and efficiently addresses all parts of the task. It correctly defines the DataFrame, calculates the ""LengthOfStay"" column, filters for ""Flu"" diagnosis, and calculates and prints the average ""LengthOfStay"" for these patients. The code is concise and demonstrates good understanding of pandas operations. All requirements from the prompt are met precisely.",claude-3.5-sonnet,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}
df = pd.DataFrame(data)

scifi_df = df[df['Genre'] ==...",9,"The generated code correctly performs all requested operations: it defines the DataFrame, filters for 'Sci-Fi' movies, and calculates the average rating. The code is concise and directly addresses the problem.

The only minor improvements could be:
1. **Formatting the output:** The ground truth code formats the output to two decimal places and provides a more descriptive print statement. The generated code simply prints the raw float result.
2. **Handling empty results:** While not strictly necessary for this specific dataset given that Sci-Fi movies are present, robust code might include a check for an empty filtered DataFrame before attempting to calculate the mean, to prevent potential errors if the filter yielded no results.

However, for the core requirements of the task, the code is fully functional and correct.",claude-3.5-sonnet,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""], 
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

df = pd.DataFrame(data)

toyota_avg_mileage = df[df['Brand'] == 'Toyota']['Mileage'].mean()
...",9,"The generated code correctly performs all requested operations: it defines the DataFrame, filters for 'Toyota' brand cars, and accurately calculates their average mileage. The code is concise and efficient.

However, the output format for the average mileage is slightly less user-friendly than the ground truth. While `print(toyota_avg_mileage)` is functionally correct, including a descriptive string with the output (as in the ground truth `print(f""The average Mileage for Toyota cars is: {average_mileage_toyota:.2f}"")`) improves readability and user experience. Also, the ground truth includes a robust check for an empty DataFrame, which this code omits, though it's not strictly necessary for this specific dataset. These are minor improvements, hence a score of 9.",claude-3.5-sonnet,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None], 
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 8...",9,"The generated code correctly defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat, addressing all the core requirements of the task. The code is concise and functionally correct.

The only minor area for improvement is in the output formatting. The ground truth code provides a more user-friendly printed output with a descriptive message and formatting (`:.2f`). While the generated code's output correctly gives the numerical answer, it lacks this descriptive context seen in the ground truth. This is a minor stylistic point, not a correctness issue.",claude-3.5-sonnet,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Read data
df = pd.read_csv('titanic.csv')

# Create age groups
df['AgeGroup'] = pd.cut(df['Age'], bins=[0,12,18,35,50,100], labels=['Child','Teen','Young Adult','Adult','Senior'])

# Figure 1: Multi-panel survival rates
fig, axes = plt.subplots(1,3, figsize=(15,5))

sns.barplot(data=df, x='Pclass', y='Surviv...",6,"The generated code attempts to address all parts of the task, which is a good start.

**Strengths:**
*   It correctly identifies and uses the required libraries (pandas, matplotlib, seaborn, plotly).
*   It creates a multi-panel figure, heatmap, violin plots, and an interactive scatter plot as requested.
*   The interactive scatter plot uses Plotly Express, which is suitable.
*   It attempts a dashboard visualization using Plotly subplots.

**Areas for Improvement / Missing Aspects:**
*   **Data Preprocessing and Robustness:** The ground truth includes essential data preprocessing steps (filling missing 'Age' and 'Fare', creating descriptive categorical labels for 'Survived', 'Pclass', and 'AgeGroup'). The generated code only creates 'AgeGroup' and does not handle missing values or proper categorical mapping for better visualization labels. This significantly impacts the quality and readability of the plots.
*   **Visualization Quality & Storytelling:**
    *   **Multi-panel figure (Task 1):** The generated plots for survival rates use raw `Pclass` numbers instead of descriptive labels (e.g., '1st Class'), making them less intuitive. The ground truth preprocesses these into understandable categories.
    *   **Heatmap (Task 2):** The `Sex` and `Pclass` labels are numerical/raw, whereas the ground truth uses descriptive labels, improving readability. The chosen colormap (`RdYlGn`) for survival rates is less intuitive for 'good' (green) vs 'bad' (red) survival than a sequential colormap like 'YlGnBu' in the ground truth, which better represents a continuous variable of survival rate.
    *   **Violin Plots (Task 3):** The current violin plot does not clearly separate survivors vs. non-survivors as effectively as the `hue='Survived'` and `split=True` method in the ground truth for Seaborn, or separate traces in Plotly for better comparison. The generated Plotly dashboard part does try to split.
    *   **Interactive Scatter Plot (Task 4):** While it uses `color='Pclass'`, it lacks the `symbol='Survived'` feature present in the ground truth, which adds another dimension of information (survival status per point). The `size='Survived'` makes points either size 0 or 1, which isn't very informative; `Survived` should be used for color/symbol, not size.
*   **Dashboard (Task 5):**
    *   **Complexity and Readability:** The dashboard attempt with `make_subplots` and `go.Bar` for the first panel is overly complex and less effective than simply using `px.bar` or `sns.barplot` across different subplots. The `go.Bar` in `make_subplots` combines multiple bar charts into a single column, which is not what a multi-panel figure typically implies and makes it hard to compare individual demographics.
    *   **Redundancy:** The dashboard duplicates some information and doesn't integrate new, unique insights as effectively as the ground truth's combined plot, which adds density plots and text analysis.
    *   **Storytelling:** The ground truth explicitly adds a text box for ""Key Insights,"" which is a direct request in the ""storytelling"" part of the prompt. The generated code does not include this crucial element.
    *   The dashboard's violin plot part is more fragmented than the direct `sns.violinplot` from the ground truth.
*   **""Only provide Python code without explanations"":** The generated code included plot titles and variable names like `fig, axes`, `figure 1`, `figure 2`, `figure 3`, `figure 4`, and `figure 5`, followed by `plt.savefig('titanic_analysis.png')` which captures all static plots *after* they've been displayed. This does not strictly adhere to the prompt.

Overall, the code provides a functional base but lacks the sophistication, robust preprocessing, and detailed storytelling elements found in the ground truth. The visualizations are basic and could be significantly improved for clarity and insights.",claude-3.5-sonnet,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.2...",9,"The generated code correctly performs all the required tasks: it defines the DataFrame as specified, filters it for the ""Dystopian"" genre, and accurately calculates and prints the average ""Stock"" for those books. The code is concise and efficient.

The only minor area for improvement, which leads to a slight deduction from a perfect score, is the lack of explicit output formatting (e.g., using an f-string with `.2f` for the average stock, as seen in the ground truth for better readability) and the absence of an empty DataFrame check before calculating the mean. While in this specific case, Dystopian books exist, a more robust solution would account for scenarios where the filtered DataFrame might be empty, which could lead to `NaN` or unexpected behavior depending on further operations. However, for the given task and data, the current implementation is perfectly functional and correct.",claude-3.5-sonnet,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Load data
df = pd.read_csv('water_potability.csv')

# T-tests for each parameter
features = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']
print(""\nT-Test Results:"")
print(""-""*50)

for feature in features:
    potable = df[df['Potability']==1][feature].dropna()
    non_potable = df[df['Potability']=...",7,"The generated code correctly implements the core statistical tests requested (t-tests, chi-square, Shapiro-Wilk) and calculates Cohen's d. It also includes basic interpretation.

However, there are several areas where it falls short of the ground truth and the task requirements:

1.  **Missing Value Handling:** The generated code uses `.dropna()` within the loops for each test. This means it's dropping rows with NaNs *per column* for each test, rather than performing a consistent imputation strategy beforehand. The ground truth correctly imputes missing values *once* for the entire dataset using the mean, which is a more robust and common approach for this type of statistical analysis, ensuring all subsequent tests operate on a consistent, complete dataset. The absence of a dedicated pre-processing step for missing values is a significant oversight for robust analysis.
2.  **Order of Tests:** The task explicitly asks for: 1) T-tests, 2) Chi-square, 3) Normality, 4) Effect sizes. The generated code performs T-tests, then Chi-square, then Normality tests, which is slightly out of order from the prompt's numbering (normality tests should precede t-tests as they inform assumptions).
3.  **Cohen's d Calculation:** The formula for Cohen's d in the generated code uses a simplified pooled standard deviation `np.sqrt((np.std(potable)**2 + np.std(non_potable)**2) / 2)`. This is only correct if the sample sizes are equal. The ground truth uses the more accurate weighted pooled standard deviation formula `np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))`, which is crucial for unequal sample sizes, commonly found in real data. It also lacks interpretation of Cohen's d magnitude (small, medium, large effects).
4.  **Assumptions for T-test:** The generated code uses `ttest_ind` without `equal_var=False`. By default, `ttest_ind` assumes equal variances (`equal_var=True`). The ground truth wisely defaults to `equal_var=False` (Welch's t-test), which is more robust when variances are unequal, a safer assumption in real-world scenarios unless proven otherwise.
5.  **Robustness of Normality Test:** The Shapiro-Wilk test is recommended for smaller sample sizes. While the generated code applies it, it doesn't include any checks or warnings for larger sample sizes (e.g., >5000), where Shapiro-Wilk might be computationally expensive or less appropriate, as shown in the ground truth.
6.  **Redundant Imports:** `warnings` is imported but not explicitly required by the prompt, though `filterwarnings` is used, which is fine, but not essential for core functionality.
7.  **Clarity and Interpretation:** While it provides ""Yes/No"" for significance, the ground truth offers more verbose and clear interpretations for each test (e.g., rejecting/failing to reject null hypothesis, what that means for the data).
8.  **Output Structure:** The ground truth clearly sectioned the output with headers, making it easier to follow. The generated code also uses headers but with less descriptive introductions to the sections.
9.  **Completeness:** The generated code includes ""Summary Statistics"" and ""Correlation with Potability,"" which were not explicitly asked for in the prompt. While potentially useful, they go beyond the specified requirements. The prompt also explicitly asked for effect sizes *for significant differences*, which implies calculating them only when the p-value is below alpha, and then interpreting them. The generated code calculates Cohen's d regardless of significance and doesn't provide a textual interpretation of its magnitude.",claude-3.5-sonnet,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import roc_...",6,"The generated code addresses most of the task requirements but has several shortcomings that prevent it from being a strong solution.

**Strengths:**
*   **Correct Libraries:** Uses appropriate libraries (pandas, sklearn, matplotlib, seaborn).
*   **Basic Data Preprocessing:** Handles missing values and scales features, though the imputation method is less robust than the ground truth.
*   **Multiple Models:** Implements Logistic Regression, Random Forest, and SVM.
*   **Evaluation Metrics:** Calculates and displays accuracy, precision, recall, F1-score, and ROC-AUC.
*   **Basic Visualizations:** Generates ROC curves for each model and a confusion matrix for the best model.
*   **Feature Importance:** Attempts to show feature importance for Random Forest and Logistic Regression.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Imputation:**
    *   **Ground Truth:** Uses `SimpleImputer(strategy='median')` and applies it *after* train-test split to prevent data leakage (fitting imputer only on training data).
    *   **Generated:** Uses `df = df.fillna(df.mean())` on the *entire* dataset *before* the train-test split. This is a form of data leakage as information from the test set is used to impute training data, potentially leading to an over-optimistic performance estimate.

2.  **Train-Test Split:**
    *   **Ground Truth:** Uses `stratify=y` to ensure the class distribution of the target variable is maintained in both training and testing sets, which is crucial for imbalanced datasets common in binary classification.
    *   **Generated:** Does not use `stratify=y`.

3.  **Cross-Validation Implementation:**
    *   **Ground Truth:** Employs a `Pipeline` within `cross_val_score` to ensure the entire preprocessing flow (imputation, scaling) is correctly applied within each CV fold, preventing data leakage during cross-validation. It also performs stratified k-fold.
    *   **Generated:** Performs `cross_val_score` directly on `X_train_scaled` and `y_train`. This means that the cross-validation folds will use data that was scaled using statistics from the *entire* original training set, not just the current fold's training portion. While scaling happened *before* this CV step, the *imputation* of the original `df` was done on the whole dataset, which is a significant leak. For a truly robust CV, preprocessing steps that learn from data (like imputation and scaling) should be part of a pipeline *inside* the CV loop.
    *   The generated code also only calculates default accuracy for CV results and stores it as 'CV Mean' and 'CV Std', rather than performing CV for all required metrics (precision, recall, f1, roc-auc) as the ground truth does.

4.  **ROC Curve Display:**
    *   **Ground Truth:** Uses `RocCurveDisplay.from_estimator` which is the modern and cleaner way to display ROC curves, often handling plotting on a shared axis more elegantly. It also plots all ROC curves on a single graph for direct comparison.
    *   **Generated:** Uses the older `roc_curve` function and manual plotting (`plt.plot`). It plots ROC curves in separate subplots rather than combined, which makes direct visual comparison harder.

5.  **Confusion Matrix Display:**
    *   **Ground Truth:** Plots confusion matrices for *each* model.
    *   **Generated:** Plots confusion matrix only for the *best* model.

6.  **Feature Importance for Best Model:**
    *   **Ground Truth:** Explicitly identifies the best model first (based on a robust metric like F1-score from cross-validation) and then displays feature importance for *that specific* model. It also correctly handles SVM not providing direct feature importance.
    *   **Generated:** The best model selection is based on the regular test set F1-score, which might not be as robust as a CV-based selection. While it handles RF and LR, it lacks the specific handling/message for SVM.

7.  **Code Structure and Readability:**
    *   The ground truth code has clear sections for each task (1. Load Data, 2. Data Preprocessing, etc.), making it easy to follow. The generated code is more monolithic.
    *   The ground truth includes error handling for file loading and verbose print statements that track progress and data shapes, which is good practice.

In summary, the generated code produces results and visualizations but misses critical best practices for robust machine learning evaluation, particularly regarding data leakage in preprocessing and cross-validation, and is less comprehensive in its metric evaluation and visualizations compared to the prompt's requirements and the ground truth.",claude-3.5-sonnet,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer

# Load and preprocess data
df = pd.read_csv('water_potability.csv')
imputer = SimpleImputer(strategy='mean')
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Cre...",7,"The generated code addresses most aspects of the task, demonstrating a good understanding of feature engineering, correlation analysis, and feature selection.

**Strengths:**
- **Correctness:** The code generally works and produces outputs as intended. Missing value handling, basic feature engineering, correlation calculation, and feature selection using SelectKBest and RFE are correctly implemented.
- **Imports:** All necessary imports are present.
- **Structure:** The code is logically structured, moving from data loading to feature engineering, correlation, feature selection, and then visualizations, and comparison.
- **Visualizations:** Generates a correlation heatmap, feature importance plot, and pairwise scatter plots as requested.
- **Feature Engineering:** Implements basic ratio, interaction, and polynomial (via squaring) features.

**Areas for Improvement:**
- **Completeness of Feature Engineering:** The prompt specifically asked for ""new meaningful features such as ratios between chemical parameters (e.g., pH/Chloramines, Hardness/Solids), interaction terms, and polynomial features."" The generated code only creates very few ratio/interaction terms (e.g., `Conductivity_Organic_Ratio` is not a chemical parameter ratio specific to the dataset's context, and only two interaction terms are created). It also only creates squared terms for pH and Hardness, not a general `PolynomialFeatures` transformation that would also include cross-product terms (`x*y`). The ground truth provides more comprehensive examples and usage of `PolynomialFeatures`.
- **Handling of Division by Zero in Ratios:** The generated code directly divides without handling potential division by zero (e.g., `df_imputed['ph']/df_imputed['Chloramines']`). While less likely for the given dataset, it's good practice to add a small epsilon to the denominator to prevent `inf` or `NaN` values, especially for potentially sparse denominators. The ground truth demonstrates this.
- **Top Correlated Features Identification:** The problem asks to ""Identify the top 5 features most correlated with water potability"". The generated code correctly identifies them. However, it doesn't print this specific list explicitly for the user's information or comparison early on, instead only mentioning them in the final comparison print statement. The ground truth explicitly prints this list after discovery.
- **Redundant Visualization for Comparison:** The last two heatmap visualizations for comparing feature selection methods are a bit complex and less intuitive than a simple printout of overlaps and unique features, which the ground truth provided. While creative, they don't add much clarity compared to direct set comparisons.
- **Error Handling:** The ground truth includes basic file not found error handling, which is a good practice for robustness. The generated code assumes the file exists.
- **Clarity in Polynomial Features:** The prompt asked for ""polynomial features"" which typically implies `PolynomialFeatures` from `sklearn.preprocessing` for quadratic and interaction terms, not just manual squaring of two features. The ground truth correctly uses `PolynomialFeatures`.",claude-3.5-sonnet,LLM_judge_config
