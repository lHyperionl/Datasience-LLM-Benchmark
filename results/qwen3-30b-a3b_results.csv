task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
from pandas.api.types import is_numeric_dtype

df = pd.read_csv('winequality-red.csv')

print(""1. Columns and their data types:"")
for col in df.columns:
    dtype = df[col].dtype
    if is_numeric_dtype(df[col]):
        cat_type = 'Numerical'
    else:
        cat_type = 'Categorical or other'
    print(f""{col}: {dtype} ({cat_type})"")

print(""\n2. Descriptive statistics for numerical columns:"")
print(df.describe())

print(f""\n3. Total rows: {len(df)}, Total columns...",6,"The generated code correctly loads the dataset, identifies data types, and provides descriptive statistics and dataset dimensions. However, it has some shortcomings compared to the ground truth.

**Strengths:**
*   **Correct loading:** `pd.read_csv` is used correctly.
*   **Basic data type identification:** It distinguishes between numerical and non-numerical types.
*   **Descriptive statistics:** `df.describe()` is used, which is good for numerical columns.
*   **Dimensions:** Correctly reports total rows and columns.

**Areas for Improvement:**
*   **Robustness:** The ground truth includes error handling for `FileNotFoundError`, which is a good practice for real-world scenarios. The generated code lacks this.
*   **Clarity in Data Type Identification:** The generated code labels non-numerical as ""Categorical or other"". The ground truth explicitly distinguishes `object` or `categorical` dtypes as ""Categorical/Ordinal"" and uses `pd.api.types` more comprehensively. While `winequality-red.csv` typically contains only numerical columns, a more robust solution would handle truly mixed datasets better.
*   **Descriptive Statistics Scope:** `df.describe()` calculates descriptive statistics for *all* numerical columns, which fulfills that requirement. However, the ground truth explicitly filters and reports only for numerical columns, which is slightly more precise if the user only wanted numerical ones (though `describe()` handles this by default).
*   **Categorical/Ordinal Identification Logic:** This is the weakest point.
    *   The generated code's logic for identifying categorical/ordinal columns is problematic. For non-numeric columns, it just prints the column name as ""categorical or ordinal.""
    *   For numerical columns, it checks `df[col].nunique() < 10`. While this can *suggest* a column might be ordinal (like 'quality' which has few unique values), it's a heuristic and not a definitive identification of a Pandas categorical/ordinal dtype. The ground truth correctly points out that in `winequality-red.csv`, all columns *are* numerical, but 'quality' is *often treated* as ordinal, which provides better context and nuance. The generated code effectively identifies all columns as categorical/ordinal if their `nunique()` is less than 10, which is misleading for truly numerical features with few unique values.
*   **Output Formatting:** The output formatting is less structured and clear than the ground truth, especially for the data type and categorical/ordinal identifications.

**Minor Issue:**
*   The ground truth uses `pd.api.types.is_numeric_dtype(dtype)` which is slightly more direct than `is_numeric_dtype(df[col])` in the loop, but both work. The generated code imports `is_numeric_dtype` directly, which is fine.

Overall, it gets the core tasks done but could be more robust, nuanced, and provide more accurate insights, especially regarding the classification of categorical/ordinal columns beyond just a heuristic or checking only for non-numeric types.",qwen3-30b-a3b,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"import pandas as pd

df = pd.read_csv('video_games_sales.csv')
print(df.head(10))
print(df.shape)
print(df.dtypes)
print(df.sort_values(by='Global_Sales', ascending=False).head(10))
print(""Unique Platforms:"", df['Platform'].unique())
print(""Unique Genres:"", df['Genre'].unique())
print(df[['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales']].describe())
print(""Missing values:\n"", df.isnull().sum())",6,"The generated code correctly performs most of the requested EDA tasks, demonstrating a good understanding of pandas functionalities. It successfully loads the data, displays the head, shape, top games by sales, unique platforms/genres, summary statistics for sales, and missing values.

However, several areas could be improved to meet the standards of the ground truth and address all requirements robustly:

1.  **Output Formatting and Readability:** The ground truth meticulously labels each section with clear print statements and separators (e.g., `--- 1. Loading Dataset...`, `=""*80""`). The generated code simply prints outputs one after another, making it hard to discern which output corresponds to which task. The task specifically asked for well-structured output.
2.  **Error Handling:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is crucial for robust code. The generated code lacks this.
3.  **Specific Column Display for Top Games:** The ground truth specifically selects `[['Name', 'Platform', 'Genre', 'Publisher', 'Global_Sales']]` for displaying top games, which is more relevant and readable than printing all columns (as the generated code does).
4.  **Column Names and Data Types:** The ground truth uses `df.info()`, which provides a much more comprehensive and readable output of column names, non-null counts, and data types than `df.dtypes` which was used in the generated code.
5.  **Output for Missing Values:** The task requested to ""Identify any missing values"". While `df.isnull().sum()` is correct, the ground truth explicitly filters to show *only* columns with missing values and provides a more descriptive message (`""Total missing values per column shown above.""` or `""No missing values found...""`). The generated code outputs all columns, even those without missing values, which can be less efficient to read for large datasets.
6.  **Redundancy Check for Columns:** The ground truth includes checks like `if 'Global_Sales' in df.columns:` before attempting operations on those columns, which adds robustness against potential variations in the dataset. The generated code assumes all columns will always exist.
7.  **Clarity of Instructions:** The prompt asks to ""Provide only the Python code without explanations."" While the generated code adheres to this, the ground truth's use of comments aligns more with standard coding practices and internal documentation, even if they aren't ""explanations"" printed to the console.

Overall, the generated code functions correctly but lacks the polish, robustness, and detailed adherence to presentation requirements present in the ground truth solution.",qwen3-30b-a3b,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd

df = pd.read_csv('video_games_sales.csv')
df['Year'] = df['Year'].fillna(df['Year'].median())
df = df.dropna(subset=['Publisher'])
df['Years_Since_Release'] = 2023 - df['Year']
df = df.drop_duplicates(subset=['Name', 'Platform'])
df = df[df['Global_Sales'] >= 0.1]
print(df.shape)
print(df.head(5))
```",6,"The generated code addresses most of the steps correctly and concisely.

**Strengths:**
*   Correctly loads the dataset.
*   Handles missing 'Year' values with the median.
*   Removes rows with missing 'Publisher' information.
*   Creates the 'Years_Since_Release' column with the correct calculation.
*   Removes duplicate games based on 'Name' and 'Platform'.
*   Filters out games with Global_Sales less than 0.1.
*   Displays the final shape and head.

**Areas for Improvement/Missing Aspects:**
*   **Robustness for 'Year' column:** The ground truth includes `pd.to_numeric(df['Year'], errors='coerce')` which is crucial for handling non-numeric data in the 'Year' column before calculating the median or performing subtractions. The generated code assumes 'Year' is already numeric, which might lead to errors if the dataset contains non-numeric entries (e.g., ""N/A"", ""20XX"").
*   **Robustness for 'Global_Sales' column:** Similar to 'Year', `df['Global_Sales'] = pd.to_numeric(df['Global_Sales'], errors='coerce')` and subsequent dropping of NaNs is good practice. The generated code assumes 'Global_Sales' is numeric, which could cause issues if it's not.
*   **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions, which is good practice for real-world scripts. The generated code lacks this.
*   **Variable Names:** While not strictly incorrect, the ground truth uses `DATASET_PATH` and `CURRENT_YEAR` as constants, which improves readability and maintainability.
*   **Information Display:** The ground truth provides print statements at each step, showing the shape changes, which is helpful for debugging and understanding the cleaning process. The generated code only shows the final shape and head.
*   **Non-negative `Years_Since_Release`:** The ground truth included `df['Years_Since_Release'] = df['Years_Since_Release'].apply(lambda x: max(0, x))` to handle cases where `CURRENT_YEAR - df['Year']` might result in a negative value (e.g., if a game's release year is in the future relative to `CURRENT_YEAR`). This is a minor but good detail.",qwen3-30b-a3b,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('video_games_sales.csv')

# Bar chart for top 10 platforms by global sales
platform_sales = df.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False).head(10)
plt.figure(figsize=(10, 6))
plt.bar(platform_sales.index, platform_sales.values)
plt.xlabel('Platform')
plt.ylabel('Total Global Sales (in millions)')
plt.title('Top 10 Gaming Platforms by Global Sales')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show...",7,"The generated code creates all the required visualizations and generally follows the instructions. However, there are several areas for improvement to reach a higher score:

1.  **Missing Data Cleaning/Preprocessing**: The ground truth code includes crucial preprocessing steps for `Year_of_Release` and sales columns to ensure they are numeric and handle potential errors (`coerce`). The generated code assumes clean data, which might lead to errors if the dataset isn't perfectly clean (e.g., if 'Year_of_Release' has non-numeric entries). This is particularly important for the 'Year' column in the line plot, which the generated code uses directly from the DataFrame without handling potential missing or non-numeric values. The `df['Year']` might not exist or be clean if `df['Year_of_Release']` as per the ground truth is the intended column.
2.  **Robustness**: The ground truth includes a `try-except` block for file loading, making the code more robust against `FileNotFoundError`. The generated code lacks this.
3.  **Visualization Refinements**:
    *   **Pie Chart**: The ground truth code handles `main_genres` and `other_count` to make the pie chart more readable by grouping small categories, which is good practice for pie charts with many categories. The generated code simply plots all genres, which could be cluttered if there are many.
    *   **Histogram**: The ground truth limits the x-axis range of the histogram to a percentile (`0.99`) to better visualize the bulk of the data, as sales data is often skewed. The generated code uses a default range, which might not be optimal for skewed data.
    *   **Scatter Plot**: The ground truth uses `alpha` and `s` parameters for better visual representation, especially when there are many overlapping points.
    *   **Year Column**: The generated code uses `df['Year']` for the line plot. The dataset usually has `Year_of_Release`. If `df['Year']` doesn't exist, it will crash. If it's intended to be `Year_of_Release`, it still needs the conversion to numeric.
    *   **Labels/Titles**: While titles and labels are present, some fine-tuning like ""Total Global Sales (in millions)"" could be ""Total Global Sales (Millions)"" as `in millions` isn't standard in chart labels.
    *   **Gridlines**: The ground truth adds gridlines to most plots, which enhances readability.
4.  **No `Year_of_Release` cleaning**: The line plot relies on `df['Year']` which is likely `df['Year_of_Release']`. Without conversion to numeric and handling NaNs, this plot might fail or produce incorrect results if the column contains non-numeric entries or NaNs. The ground truth explicitly cleans this column.",qwen3-30b-a3b,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"import pandas as pd

df = pd.read_csv('diabetes.csv')

print(""First 10 rows:"")
print(df.head(10))

print(""\nDataset shape:"", df.shape)

print(""\nColumn names and data types:"")
print(df.columns)
print(df.dtypes)

print(""\nPercentage of diabetic vs non-diabetic patients:"")
print(df['Outcome'].value_counts(normalize=True) * 100)

print(""\nMissing values count per column:"")
print(df.isnull().sum())

print(""\nSummary statistics for numerical columns:"")
print(df.describe())

print(""\nColumns with zero...",7,"The generated code correctly addresses most of the requirements.

**Strong Points:**
- Correctly loads the dataset and displays the first 10 rows.
- Accurately reports the dataset shape.
- Correctly lists column names and data types (though separately).
- Correctly calculates the percentage of diabetic vs non-diabetic patients.
- Correctly displays missing values count for each column.
- Correctly shows summary statistics for numerical columns.

**Areas for Improvement:**
- **Incomplete Data Types Display:** The ground truth used `df.info()` which provides a much more comprehensive view of data types, non-null counts, and memory usage in one go. The generated code prints `df.columns` and `df.dtypes` separately, which is less efficient and comprehensive.
- **Improved Zero Value Identification:** The generated code's check for zero values `(df == 0).any()` identifies *any* column possessing *at least one* zero value. This is not quite what the prompt asked for. The prompt's implied need was to identify columns where zero might represent *missing data*, specifically `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, and `BMI`. The generated code does not distinguish between legitimate zeros (like 'Pregnancies' or 'Outcome') and potentially missing data zeros. The ground truth correctly identifies and counts the number of zero values only for columns where zero might indicate missing data, making it more robust and domain-aware.
- **Lack of Error Handling:** The ground truth includes `try-except` blocks for file loading, which is good practice for robust code. The generated code lacks this.
- **Formatting and Readability:** While functional, the output formatting in the ground truth with clear section headers (`--- 1. ... ---`) and separators (`===`) significantly improves readability and user experience. The generated code has simpler print statements.",qwen3-30b-a3b,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

df = pd.read_csv('diabetes.csv')

columns_to_check = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_to_check] = df[columns_to_check].replace(0, np.nan)

df.fillna(df.median(), inplace=True)

df.drop_duplicates(inplace=True)

df['BMI_Category'] = pd.cut(df['BMI'], 
                            bins=[0, 18.5, 24.9, 29.9, 100], 
                            labels=['Underweight', 'Norm...",7,"The generated code addresses most requirements but has a few areas for improvement to be complete and robust.

**Strengths:**
- Correctly loads the dataset.
- Efficiently replaces zero values with NaN in the specified columns using vectorized operations.
- Correctly handles duplicate rows.
- Accurately creates the 'BMI_Category' column with appropriate bins and labels.
- Applies StandardScaler correctly to numerical features.
- Displays the shape and head of the cleaned dataset.

**Areas for Improvement:**
1.  **Missing Value Imputation:** `df.fillna(df.median(), inplace=True)` will fill *all* NaN values in the DataFrame with the median of *their respective columns*. While this works for the columns where zeros were replaced by NaNs, it's less precise than the ground truth which specifically fills *only* the `cols_to_replace_zero` columns with their medians. If there were other missing values (NaNs) in other columns not part of the initial zero replacement, this approach would implicitly fill them too, which might not always be desired behavior based on the task description (which focused on specific columns).
2.  **Robustness in Column Handling:** The generated code assumes all specified columns (`Glucose`, `BloodPressure`, etc.) exist and doesn't include error handling (e.g., `try-except` for `FileNotFoundError`) or warnings if a column is missing, unlike the ground truth.
3.  **BMI Bin Definition:** The `np.inf` in the ground truth is more robust for the upper bound of 'Obese' (>=30) than `100` in the generated code, as a BMI value of, for instance, 101 would not be categorized. While unlikely for BMI, `np.inf` is the semantically correct choice here.
4.  **Target Variable Scaling:** The generated code scales *all* numerical columns, including the 'Outcome' column if it's numerical. Typically, the target variable ('Outcome') is not scaled in a supervised learning context, especially when it's a binary classification target (0 or 1). The ground truth correctly excludes 'Outcome' from scaling.
5.  **Informative Output:** The generated code lacks informative print statements regarding the steps performed (e.g., ""Dataset loaded successfully"", ""Removed X duplicate rows"", ""Created BMI_Category column"", ""Scaled numerical features""), which are crucial for understanding the data cleaning process. It only prints the final shape and head, without comparison to the original shape.
6.  **Readability/Structure:** While functional, the ground truth code is more structured with comments for each step, which enhances readability and maintainability for a complex data cleaning task.",qwen3-30b-a3b,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('diabetes.csv')

# Correlation heatmap
plt.figure(figsize=(10,8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Histograms
numerical_cols = df.select_dtypes(include=['number']).columns
fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15,15))
axes = axes.flatten()
for i, col in enumerate(numerical_cols):
    sns.histplot(df[col], ax=axes[i], kde=True)...",7,"The generated code addresses most of the requirements and produces the specified visualizations.

**Strengths:**
*   **Correctness:** All specified plots (heatmap, histograms, box plots, pie chart, scatter plots, pair plot) are generated.
*   **Imports:** Proper libraries (pandas, matplotlib, seaborn) are imported.
*   **Loading Data:** Correctly loads the `diabetes.csv` file.
*   **Heatmap:** Correctly generates a correlation heatmap with annotations.
*   **Histograms:** Generates histograms for all numerical columns with proper subplots.
*   **Box Plots:** Correctly generates box plots comparing features by outcome.
*   **Pie Chart:** Creates a pie chart for outcome distribution.
*   **Scatter Plots:** Correctly generates the two requested scatter plots (BMI vs Glucose, Age vs Pregnancies) colored by Outcome.
*   **Pair Plot:** Correctly generates a pair plot for specified features, with hue.

**Areas for Improvement:**
*   **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.
*   **Numerical Column Identification:** While `df.select_dtypes(include=['number']).columns` works, explicitly defining `numerical_cols` and excluding 'Outcome' (as `Outcome` is the target variable and often treated separately for some plots) is slightly more robust and clearer, as seen in the ground truth. Specifically, for correlation, it's typically done on features, not including the target.
*   **Box Plot Loop:** The box plot section creates a new figure for *each* box plot, leading to many individual windows instead of a subplot grid like histograms. The ground truth uses a subplot approach for better organization and fewer plot windows.
*   **Pie Chart Labels:** The pie chart uses `outcome_counts.index` as labels implicitly. The ground truth provides more descriptive labels ('Non-Diabetic (0)', 'Diabetic (1)') which is more user-friendly. It also adds `explode` and `shadow` for better visual appeal.
*   **Scatter Plot Titles & Labels:** While titles and basic labels are present, the ground truth's `legend` labels are more descriptive (`Non-Diabetic`, `Diabetic`) than the default '0' and '1'.
*   **Pair Plot Title:** The pair plot lacks a `suptitle` to clearly describe the overall plot, unlike the ground truth.
*   **Consistency in `plt.show()`:** For the scatter plots, each one is shown individually. It would be better to combine them into a single figure with subplots, as done in the ground truth, for a more cohesive presentation.
*   **Aesthetics:** The ground truth includes `sns.set_style(""whitegrid"")` for consistent and clean aesthetics, which the generated code omits. It also uses `plt.tight_layout()` more consistently which helps prevent overlap.
*   **Clarity in Plots:**
    *   For histograms and box plots, `axes[i].set_xlabel('')` and `axes[i].set_ylabel('')` remove labels, which makes the plot less informative. It's better to keep relevant labels.
    *   `fmt="".2f""` and `linewidths=.5` in the heatmap improve readability.
    *   Explicitly setting `s` and `alpha` for scatter plots can improve visual quality.",qwen3-30b-a3b,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

df = pd.read_csv('world-happiness-report-2021.csv')

print(""First 10 rows:"")
print(df.head(10))

print(""\nDataset shape:"", df.shape)

print(""\nColumn names and data types:"")
print(df.dtypes)

print(""\nUnique Regional Indicators:"")
print(df['Regional indicator'].unique())

print(""\nCountries per region:"")
print(df['Regional indicator'].value_counts())

print(""\nTop 10 happiest countries:"")
print(df.sort_values('Ladder score', ascending=False).head(10)[['Country name...",7,"The generated code correctly performs most of the requested tasks. It loads the data, displays the head, shape, column data types, unique regional indicators, countries per region, top 10 happiest countries, and missing values.

However, there are several areas for improvement:
- **Error Handling:** The generated code lacks error handling for `FileNotFoundError` when reading the CSV, which is crucial for robustness. The ground truth includes a `try-except` block for this.
- **Output Clarity & Readability:** The ground truth code uses print statements with clear headings and separators (`=""*70""`) to delineate each task's output, making the overall output much more readable and organized. The generated code's output is a bit jumbled.
- **Column Name Consistency & Robustness:** In Task 6, the generated code directly tries to access 'GDP per capita'. The ground truth code correctly identifies `Logged GDP per capita` as the column, and also includes a check for existing columns, making it more robust against minor variations in column names.
- **Completeness (Task 3):** For displaying column names and data types, the `df.info()` method (used in ground truth) is generally preferred as it also provides a summary of non-null counts, which is very useful for initial EDA. `df.dtypes` is sufficient but less complete.
- **Task 4 (Regional Indicators):** The request was to display unique regional indicators AND the count of countries per region. The generated code separates these into two print statements without specific labels for each, potentially causing confusion. The ground truth performs this in one logical block, with a robustness check for the column's existence.",qwen3-30b-a3b,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder

df = pd.read_csv('world-happiness-report-2021.csv')

numerical_cols = df.select_dtypes(include=[np.number]).columns
categorical_cols = df.select_dtypes(exclude=[np.number]).columns

for col in numerical_cols:
    df[col].fillna(df[col].mean(), inplace=True)

for col in categorical_cols:
    df[col].fillna(df[col].mode()[0], inplace=True)

df['Happiness_Category'] = pd.cut(df['Ladder score'], bi...",7,"The generated code addresses most of the requirements but has a few inaccuracies and omissions that prevent it from scoring higher.

**Strengths:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Missing Value Handling:** Correctly identifies and fills missing numerical values with the mean and categorical values with the mode.
*   **Outlier Removal:** Implements the IQR method correctly for 'Ladder score'.
*   **Standardization:** Applies `StandardScaler` to numerical columns correctly.
*   **Encoding:** Uses `LabelEncoder` for 'Regional indicator' as requested.
*   **Display Output:** Prints the shape and head of the cleaned dataframe.

**Areas for Improvement / Issues:**

1.  **File Not Found Handling:** The generated code lacks error handling for `FileNotFoundError` when loading the dataset, which is a good practice for robustness.
2.  **`Happiness_Category` Logic:**
    *   The prompt specifies ""High: >6.5, Medium: 4-6.5, Low: <4"".
    *   The generated code uses `pd.cut` with `bins=[-np.inf, 4, 6.5, np.inf]`. This will result in:
        *   `Low`: anything `<= 4` (correct for `<4`, but includes 4)
        *   `Medium`: `(4, 6.5]` (i.e., `>4` and `<=6.5`)
        *   `High`: `(6.5, inf]` (i.e., `>6.5`)
    *   The interpretation of `Medium` (specifically including 6.5 and excluding 4) is slightly off from the prompt's `4-6.5` (which implies `4 <= x <= 6.5`). The ground truth uses `np.select` which allows for more precise conditional logic. While `pd.cut` can be made to work, the current implementation is not precisely aligned with the prompt's inclusive/exclusive ranges for 'Medium'.
3.  **`GDP_Category` Column Name:** The generated code uses `'Log GDP per capita'` for `GDP_Category` creation, whereas the actual column name in the `world-happiness-report-2021.csv` dataset is `'Logged GDP per capita'`. This would lead to a `KeyError`.
4.  **`Well_being_Score` Column Name:** Similar to the GDP issue, the generated code uses `'Freedom'` instead of `'Freedom to make life choices'` for `Well_being_Score` calculation. This would also result in a `KeyError`.
5.  **Re-selection of Numerical Columns for Scaling:** The code re-selects `numerical_cols` *after* creating new categorical features. While `pd.select_dtypes(include=[np.number])` would naturally exclude the newly created 'Happiness\_Category', 'GDP\_Category' correctly, it's good practice to be explicit if certain numerical columns should *not* be scaled (though in this case, all numerical columns are suitable). However, it's redundant to re-identify `numerical_cols` here as the previous `numerical_cols` definition would have captured the original numerical ones which are the base for scaling. The `Well_being_Score` which was added should be included and is by this re-selection which is positive.
6.  **Minor Feedback Clarity:** The print statements for loading and intermediate steps (like in the ground truth) provide better feedback during execution for a user.

The issues with column names for GDP and Well-being scores are critical as they would cause the code to crash, hence impacting correctness significantly. The `Happiness_Category` logic, while functional, doesn't perfectly match the prompt's specified ranges.",qwen3-30b-a3b,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import numpy as np

df = pd.read_csv('world-happiness-report-2021.csv')

# World Map Visualization
fig = px.choropleth(df, locations='Country name', locationmode='country names', color='Ladder score', color_continuous_scale='Viridis')
fig.update_layout(title='World Happiness Map (2021)', geo=dict(showframe=False, showcoastlines=False))
fig.show()

# Ho...",7,"The generated code addresses most of the requirements and produces the specified visualizations. However, there are several areas for improvement, especially regarding the radar chart's implementation and general code robustness.

**Strengths:**
*   **Correctness of Visualizations:** The code successfully generates a world map, bar chart, heatmap, scatter plots, and box plots as requested.
*   **Use of Libraries:** It correctly utilizes Plotly, Seaborn, and Matplotlib for different visualization types.
*   **Basic Formatting:** Titles and labels are generally present for most charts.

**Areas for Improvement:**

1.  **Radar Chart Implementation:**
    *   **Normalization:** The radar chart in the generated code uses raw values (`values = top_countries[categories].values`). This is problematic because the scales of the different features (e.g., 'Log GDP per capita' vs 'Freedom to make life choices' which are probabilities/scores) are vastly different. This makes the radar chart hard to interpret and visually misleading. Features *must* be normalized (e.g., using `MinMaxScaler`) to a common scale (e.g., 0-1) for a meaningful radar chart. This is a critical omission for this visualization type.
    *   **Categories:** The categories include 'Ladder score' itself, which is redundant if the chart is meant to show *factors* contributing to happiness. The ground truth excludes 'Ladder score' from the radar features and centers the analysis on the contributing factors.
    *   **Labels/Ticks:** The `ax.set_yticklabels([])` removes the y-axis labels, making it harder to gauge the magnitude of the normalized values. The ground truth includes these.

2.  **Dataset Column Names:** The generated code directly uses the original column names like `Log GDP per capita`, `Life expectancy`, and `Ladder score`. The ground truth renames these for better readability and consistency (`Logged GDP per capita` is `GDP per Capita`, `Healthy life expectancy` is `Healthy Life Expectancy`, `Ladder score` is `Happiness Score`). While the code still runs, renaming improves code clarity and plot labels.

3.  **Consistency in Titles/Labels:** While titles and labels are present, they could be more detailed (e.g., including the year ""2021"" as in the ground truth for better context).

4.  **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.

5.  **Scatter Plot Legends:** The scatter plots would benefit from a `hue` parameter (e.g., by `Regional indicator`) and a legend to provide more insights, similar to the ground truth.

6.  **Readability/Aesthetics:**
    *   Setting `sns.set_style` and `plt.rcParams` (as in the ground truth) upfront can improve the overall aesthetic consistency of Matplotlib/Seaborn plots.
    *   `plt.tight_layout()` is used in some places but not universally (e.g., for initial bar chart and box plots) where it could help prevent labels from overlapping.

7.  **Plotly Map Layout:** The `fig.update_layout` in the generated code is a bit sparse compared to the ground truth which adds `projection_type`.

To achieve a higher score, the radar chart normalization is the most crucial fix, followed by improving naming consistency, adding error handling, and refining plot aesthetics/information.",qwen3-30b-a3b,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"import pandas as pd

df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
print(df.head(10))
print(df.shape)
print(df.dtypes)
print(df.groupby('continent')[['total_cases', 'total_deaths']].sum())
latest_per_location = df.sort_values('date').drop_duplicates('location', keep='last')
top_locations = latest_per_location.sort_values('total_cases', ascending=False).head(5)
print(top_locations[['location', 'total_cases']])
print(df[['new_cases', 'new_deaths']].describe())
print(df.isnul...",7,"The generated code addresses most of the requirements but lacks robustness, proper output formatting, and some details specified in the task for completeness.

**Strengths:**
- Correctly loads the data and parses the 'date' column.
- Identifies the first 10 rows, shape, column data types, continental sums, top locations by total cases, summary statistics for new cases/deaths, and missing values.
- Uses appropriate pandas functions for most operations.

**Areas for Improvement:**
- **Output Clarity and Labels:** The ground truth meticulously labels each section of the output using `print` statements (e.g., ""--- First 10 rows of the dataset ---""). The generated code simply prints the outputs without any descriptive headers, making it harder to discern which output corresponds to which task requirement.
- **Error Handling:** The ground truth includes `try-except` blocks for file loading, which is good practice. The generated code lacks any error handling.
- **Specific Output Formats:**
    - For column names and their data types, the ground truth uses `df.info()`, which provides a comprehensive overview including non-null counts. The generated code uses `df.dtypes`, which is less informative.
    - For top locations, the ground truth explicitly displays 'date' column alongside 'location' and 'total_cases' to show the latest date consideration. The generated code only shows 'location' and 'total_cases'.
    - For summary statistics, while `describe()` works, the request specifically asked for mean, median, min, max, std. `describe()` provides these but also includes count, 25%, 50%, and 75% quartiles, which wasn't explicitly asked for. While not incorrect, it's not as precise as the `agg` method used in the ground truth for selected statistics.
- **Redundancy/Clarity:** No specific concern here, but the ground truth uses more explicit variable names (e.g., `latest_data_per_location` vs `latest_per_location`).
- **Robustness for Numeric Conversions:** The ground truth explicitly uses `pd.to_numeric` with `errors='coerce'` and `dropna` before calculating statistics and identifying top locations to ensure that non-numeric values (which can occur in real-world data) don't break the calculations. The generated code assumes numeric columns, which might lead to errors if data quality isn't perfect.
- **Completeness Check before Grouping:** The ground truth checks if required columns exist before attempting operations like grouping or calculating statistics, which makes the code more robust against missing columns. The generated code assumes column existence.

To achieve a higher score, the generated code should incorporate better output formatting, error handling, and more robust data type handling, aligning more closely with the explicit requirements and best practices demonstrated in the ground truth.",qwen3-30b-a3b,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('winequality-red.csv')

mean_quality = df['quality'].mean()
print(f'Average Quality: {mean_quality}')

alcohol_by_quality = df.groupby('quality')['alcohol'].mean()

alcohol_by_quality.plot(kind='bar', xlabel='Quality', ylabel='Average Alcohol Content', title='Average Alcohol Content by Wine Quality')
plt.show()",7,"The generated code correctly performs the first two requested tasks: calculating the mean quality and grouping by quality to find the average alcohol content. It also creates a bar chart as requested. However, it uses `df.plot()` directly, which is less flexible for customization compared to using `matplotlib.pyplot` or `seaborn` directly with `sns.barplot`. The visualization is functional but lacks the aesthetic refinements and additional labels that would be expected in a more complete solution (e.g., specific tick formatting, data labels on bars, better handling of figure size for readability). It also lacks the `try-except` block for file loading, which is good practice. While it addresses the core requirements, there's room for improvement in visualization quality and robustness.",qwen3-30b-a3b,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

df['hosp_patients'].fillna(0, inplace=True)
df['icu_patients'].fillna(0, inplace=True)

numeric_cols = df.select_dtypes(include=['number']).columns
for col in numeric_cols:
    if col not in ['hosp_patients', 'icu_patients']:
        df[col].fillna(df[col].mean(), inplace=True)

df['mortality_rate'] = np.where(df['total_cases'] != 0, (df['total_deaths'] / df['total_cases']) * 1...",7,"The generated code addresses most of the task requirements but has some areas for improvement.

**Strengths:**
- Correctly loads data and parses the 'date' column.
- Correctly fills missing values for 'hosp_patients' and 'icu_patients' with 0.
- Uses `np.where` for 'mortality_rate' and 'vaccination_rate_per_capita' to handle division by zero, which is an efficient approach.
- Correctly filters data for the year 2021.
- Correctly displays the shape and head of the DataFrame.

**Areas for Improvement (leading to a score of 7 instead of higher):**
1.  **Missing Value Handling for Other Numerical Columns:** The code attempts to fill missing values for *all* other numerical columns with their mean. While this approach is broadly correct for numerical columns, the task specifically asked for `positive_rate` and `reproduction_rate` to be filled with their mean. The current implementation would also apply this to columns like `total_cases`, `total_deaths`, `population`, etc., which might not always be the desired behavior for *all* numerical columns (e.g., if a daily count column had a NaN, filling it with the mean of all daily counts might not be appropriate without further context). The ground truth was more precise by explicitly targeting `positive_rate` and `reproduction_rate`.
2.  **Robustness of Mean Imputation:** The generated code's mean imputation `df[col].fillna(df[col].mean(), inplace=True)` assumes that all columns included by `select_dtypes(include=['number'])` are appropriate for mean imputation without prior type conversion. While Pandas `read_csv` often infers types, if a column intended to be numeric contains non-numeric strings, it might become `object` dtype, skipping the imputation, or if `pd.to_numeric` (as seen in ground truth) is needed, it's missing. The ground truth explicitly converted `positive_rate` and `reproduction_rate` to numeric with `errors='coerce'` before calculating the mean, making it more robust.
3.  **Handling of NaN/Inf for Mortality/Vaccination Rate:** While `np.where` is good for division by zero (resulting in `inf`), it doesn't explicitly handle cases where `total_deaths` or `people_fully_vaccinated` themselves are NaN, which would propagate to the rate calculation as NaN. The ground truth explicitly used `fillna(0)` after the calculation to catch these propagated NaNs as well. For this specific task, `np.where` is largely sufficient if the numerator columns are well-behaved, but being explicit about `fillna(0)` is generally safer.
4.  **No `try-except` for file loading:** The ground truth included a `try-except` block for `FileNotFoundError`, which is good practice for robust code. The generated code lacks this.
5.  **Output Clarity:** The print statements for shape and head are slightly less descriptive than the ground truth. The ground truth clarifies that the shape is for ""records from 2021"" and specifies that ""new columns"" are included in the head.",qwen3-30b-a3b,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

# Time series line plot for India's new cases
df_india = df[df['location'] == 'India'].sort_values('date')
plt.figure(figsize=(10, 5))
plt.plot(df_india['date'], df_india['new_cases'], marker='o', linestyle='-')
plt.title('New Cases in India Over Time')
plt.xlabel('Date')
plt.ylabel('New Cases')
plt.xticks(rotation=45)
plt.ti...",6,"The generated code correctly addresses most of the plotting requirements. However, there are several areas where it falls short compared to the ground truth, particularly in data preparation for specific plots and the quality of the visualizations.

**Strengths:**
*   Correctly loads the data and parses the 'date' column.
*   Successfully generates the five requested plot types.
*   Includes proper titles and labels for all charts.
*   Uses `matplotlib.pyplot` and `seaborn` as requested.

**Weaknesses and Areas for Improvement:**

1.  **Time Series Plot (India New Cases):**
    *   Uses `plt.plot` directly with `marker='o'`, which is less idiomatic for time series with Seaborn. The ground truth uses `sns.lineplot` which is generally preferred for its aesthetics and flexibility with DataFrames.
    *   There's no `grid(True)` or `sns.set_theme()` applied, making the plot less visually appealing than the ground truth's output.

2.  **Bar Chart (Total Deaths by Continent):**
    *   **Major Logical Flaw:** The generated code calculates `df.groupby('continent')['total_deaths'].sum()`. This sums *all* `total_deaths` entries for each continent, including daily/repeated entries for the same country. The prompt explicitly asks for the *sum of 'total_deaths' for each 'continent'*, which typically implies the latest `total_deaths` value *per country* summed up by continent to avoid gross overcounting. The ground truth correctly addresses this by taking `drop_duplicates(subset='location', keep='last')` before grouping and summing. This is a critical error in data interpretation for the visualization.

3.  **Scatter Plot (GDP per Capita vs. Total Vaccinations):**
    *   **Minor Logical Flaw:** `df_latest = df.loc[df.groupby('location')['date'].idxmax()]` correctly gets the latest entry *per location*. However, it does not handle `NaN` values for `gdp_per_capita` or `total_vaccinations` explicitly. The ground truth uses `dropna()` on these specific columns which is good practice to ensure cleaner plots.
    *   The prompt implies using the latest entry for *each location* which is handled, but the ground truth also adds `hue='continent'` and `size='population'` for richer insights, which was not explicitly asked but is a good enhancement for such a plot. The generated code also doesn't apply log scales to the axes, which are often beneficial for these types of skewed variables as shown in the ground truth.

4.  **Histogram (Reproduction Rate):**
    *   Uses `kde=False`. While not strictly wrong, enabling KDE provides a smoother estimate of the distribution and is often more informative, as done in the ground truth.
    *   Doesn't explicitly handle `NaN` values before plotting, although `seaborn.histplot` usually handles this gracefully by dropping them internally. Explicit `dropna()` is safer and clearer.

5.  **Box Plot (Stringency Index by Continent):**
    *   Similar to the histogram, it doesn't explicitly handle `NaN` values for `stringency_index` or `continent`, which could lead to issues if there are NaNs in `continent`.

6.  **Code Quality/Robustness:**
    *   Lacks error handling for `pd.read_csv`, which the ground truth includes using a `try-except` block. This makes the code less robust for real-world scenarios.
    *   No `sns.set_theme()` or other styling applied, resulting in more basic-looking plots compared to the ground truth's more professional appearance.

The primary reason for a score of 6 is the significant logical error in calculating ""Total Deaths by Continent"", which would present highly misleading data. The lack of robust error handling and less refined plotting choices also contribute to the lower score.",qwen3-30b-a3b,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"import pandas as pd

df = pd.read_csv('clash_royale_cards.csv')

print(""Column names and data types:"")
print(df.columns)
print(df.dtypes)

numerical_cols = ['elixirCost', 'Win Rate', 'Usage']
for col in numerical_cols:
    print(f""\nStatistics for {col}:"")
    print(f""Mean: {df[col].mean()}"")
    print(f""Median: {df[col].median()}"")
    print(f""Standard Deviation: {df[col].std()}"")
    print(f""Min: {df[col].min()}"")
    print(f""Max: {df[col].max()}"")

print(f""\nTotal number of cards: {len(df)}"")",8,"The generated code correctly addresses most of the task requirements: it loads the CSV, lists column names and data types, calculates descriptive statistics for the specified numerical columns using a loop, and reports the total number of rows.

Strengths:
- Correctly uses pandas to load the data.
- Accurately identifies and prints column names and data types.
- Calculates all required descriptive statistics (mean, median, std, min, max) for the specified columns.
- Correctly reports the total number of rows.
- Uses f-strings for clear output.

Areas for Improvement:
- **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and general exceptions during file loading, which is a good practice for real-world applications. The generated code lacks this.
- **Output Formatting for Statistics:** While the statistics are correct, the ground truth uses `df.agg()` which provides a more condensed and readable table format for multiple statistics across multiple columns, rather than printing each statistic individually in a loop. This is a stylistic choice but can be more efficient and presentable for larger sets of columns/statistics.
- **Clarity of Output Labels:** The generated code's output for column names and data types is correct but lacks explicit labels like ""Column Names:"" and ""Data Types of Each Column:"" which improves readability, as seen in the ground truth.",qwen3-30b-a3b,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"import pandas as pd

df = pd.read_csv('clash_royale_cards.csv')
filtered = df[df['elixirCost'] <= 3]
result = filtered[['Card', 'Win Rate']].sort_values(by='Win Rate', ascending=False)
print(result)",9,"The generated code correctly implements all the requirements of the task: it loads the CSV, filters by `elixirCost` <= 3, selects the `Card` and `Win Rate` columns, and sorts the result by `Win Rate` in descending order. The code is concise and efficient. The only minor improvement could be adding error handling for `FileNotFoundError` when reading the CSV, which the ground truth code includes for robustness in a production environment. However, given the prompt's simplicity and focus on the core data manipulation, its current form is very good.",qwen3-30b-a3b,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('clash_royale_cards.csv')
plt.hist(df['elixirCost'])
plt.title('Distribution of Elixir Cost')
plt.xlabel('Elixir Cost')
plt.ylabel('Frequency')
plt.show()",6,"The generated code correctly loads the CSV, plots a histogram of 'elixirCost', and includes a title and labeled axes, fulfilling the core requirements of the task. However, it lacks robustness compared to the ground truth. It does not include error handling for `FileNotFoundError` or other exceptions during file loading, nor does it check for the existence of the `elixirCost` column. The use of `matplotlib.pyplot.hist` directly is functional but less visually refined than using `seaborn.histplot` as in the ground truth, especially for discrete data like elixir cost which benefits from carefully chosen bins. The ground truth also includes better visualization practices like setting figure size, clear binning for discrete values, and adding a grid, which are absent in the generated code. While it completes the basic task, the lack of robustness and less refined plotting indicate areas for significant improvement.",qwen3-30b-a3b,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"import pandas as pd

df = pd.read_csv('iris.csv')

print(""Columns and data types:"")
print(df.columns)
print(df.dtypes)

numerical_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

print(""\nDescriptive statistics for numerical columns:"")
for col in numerical_cols:
    print(f""\n{col}:"")
    print(f""Mean: {df[col].mean()}"")
    print(f""Median: {df[col].median()}"")
    print(f""Standard Deviation: {df[col].std()}"")
    print(f""Min: {df[col].min()}"")
    print(f""Max: {df[col]...",7,"The generated code correctly addresses most of the requirements: loading the CSV, identifying column data types, calculating descriptive statistics for the specified numerical columns, and reporting unique species counts.

**Strong points:**
- Correctly uses pandas to load the data.
- Accurately prints column names and data types.
- Calculates and prints mean, median, standard deviation, min, and max for the specified columns.
- Correctly reports the number of unique species and their counts.

**Areas for improvement to achieve a higher score:**
- **Robustness:** The generated code lacks error handling for `FileNotFoundError` when loading `iris.csv`. The ground truth code includes a `try-except` block which is crucial for real-world applications.
- **Output formatting for descriptive statistics:** While the statistics are correct, the generated code prints them individually for each column. The ground truth, by using `describe()` or `agg()`, provides a more concise and readable table-like output for multiple columns, which is generally preferred for presenting descriptive statistics of several columns at once.
- **Completeness of descriptive statistics:** The prompt asked for ""basic descriptive statistics"", and while the generated code provides several key ones (mean, median, std, min, max), `describe()` would also include percentiles (25%, 50%, 75%) and count, which are often considered basic descriptive statistics and are provided by the ground truth's approach (even if the ground truth explicitly selected only the requested ones, `describe()` is a common way to get them). The generated code's loop makes it slightly less efficient and harder to read than a single call to `df[numerical_cols].describe()` or `df[numerical_cols].agg(...)`.
- **Clarity of output:** While the output is functional, it can be improved with clearer section headings and separators, similar to the ground truth, to make it more user-friendly. For instance, clearly labeling ""Column Names and Data Types"" versus ""Descriptive Statistics"".",qwen3-30b-a3b,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"import pandas as pd

df = pd.read_csv('iris.csv')
filtered = df[df['Species'] == 'Iris-setosa']
print(filtered[['SepalLengthCm', 'SepalWidthCm']])",9,"The generated code correctly solves the task by loading the CSV, filtering for 'Iris-setosa' species, and selecting the specified columns. It demonstrates good understanding of pandas operations. Proper imports are included.

The main reason for not giving a perfect 10 is the lack of error handling (like `FileNotFoundError` or `KeyError` for missing columns), which is present in the ground truth and is good practice for robust code, especially when dealing with external files. While the prompt did not explicitly ask for error handling, it's a significant aspect of ""completeness"" and ""code quality"" for real-world scenarios. However, for the core functional requirements of the task, the code is spot on and concise.",qwen3-30b-a3b,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('iris.csv')
plt.scatter(df['SepalLengthCm'], df['SepalWidthCm'], c=df['Species'].astype('category').cat.codes, cmap='tab10')
plt.title('Sepal Length vs Sepal Width')
plt.xlabel('SepalLengthCm')
plt.ylabel('SepalWidthCm')
plt.show()",6,"The generated code correctly loads the data, creates a scatter plot with the specified axes, and includes a title and labels. However, it fails to correctly color the points by species as requested. Instead of mapping species names to distinct colors in a readable way, it assigns numerical codes to species and uses a colormap (`cmap='tab10'`). While technically coloring, this approach does not provide a legend linking colors to species, making the 'coloring by species' requirement functionally unmet from a visualization perspective. A better approach would involve iterating through unique species and plotting each with a distinct color, then adding a legend (as done in the ground truth). This is a significant functional shortcoming for a visualization task. Otherwise, the code is concise and correctly uses the specified libraries.",qwen3-30b-a3b,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"import pandas as pd

df = pd.read_csv('heart.csv')

print(""Columns and their data types:"")
print(df.dtypes)

numerical_columns = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']

for col in numerical_columns:
    print(f""\n{col} statistics:"")
    print(f""Mean: {df[col].mean()}"")
    print(f""Median: {df[col].median()}"")
    print(f""Standard Deviation: {df[col].std()}"")
    print(f""Min: {df[col].min()}"")
    print(f""Max: {df[col].max()}"")

print(""\nHeartDisease counts:"")
print(df['HeartDisease'].value...",8,"The generated code correctly loads the CSV, prints column data types, calculates the required descriptive statistics for the specified numerical columns, and reports value counts for 'HeartDisease'.

**Strong points:**
*   Correctly uses `pandas` for all operations.
*   Clearly distinguishes between different parts of the task (though implicitly).
*   The statistical calculations are accurate and target the correct columns.
*   The `value_counts()` for `HeartDisease` is correctly implemented.

**Areas for improvement (to reach a 9 or 10):**
*   **Column Name Listing:** The task explicitly asked to ""List all column names and identify the data type of each column."" The generated code only prints `df.dtypes`, which identifies the data type but doesn't explicitly list column names separately in the way the ground truth does (`for col in df.columns:`). While `df.dtypes` implicitly shows column names, a separate printout was requested.
*   **Error Handling:** The code lacks error handling for `FileNotFoundError` or if expected columns (`Age`, `RestingBP`, `Cholesterol`, `MaxHR`, `HeartDisease`) are missing. The ground truth includes robust error handling, which is crucial for real-world applications.
*   **Clarity/Formatting:** While the output for statistics is correct, the ground truth's use of `df.describe()` and `rename(index={'50%': 'median'})` provides a more concise and well-formatted table output for statistics, rather than individual print statements for each statistic and column, which can become verbose for many columns. The ground truth also explicitly prints interpretations for `HeartDisease` values (0 = No Heart Disease, 1 = Heart Disease), which is helpful.
*   **Print Statements/Labels:** The print statements could be slightly more descriptive to match the prompt's output format, e.g., ""--- Column Names and Data Types ---"" vs. ""Columns and their data types:"".

Overall, it's a very good attempt that fulfills most of the requirements. The missing explicit column listing and error handling are the primary reasons it didn't get a perfect score.",qwen3-30b-a3b,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"import pandas as pd

df = pd.read_csv('heart.csv')
filtered = df[(df['Age'] > 50) & (df['HeartDisease'] == 1)]
result = filtered[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]
print(result)",9,"The generated code correctly addresses all the requirements of the task. It properly imports pandas, loads the CSV, filters the data based on `Age` > 50 and `HeartDisease` == 1, selects the specified columns (`Age`, `Sex`, `ChestPainType`, `Cholesterol`), and prints the result. The code is concise and directly implements the logic.

The only minor points for improvement, not critical errors, could be:
- The ground truth code wraps the logic in a function and includes error handling (e.g., `FileNotFoundError`, `KeyError`), which makes it more robust for production-level use.
- The ground truth also adds comments for clarity and explanation, which is good practice.

However, for a direct solution to the stated task, the generated code is highly effective and correct.",qwen3-30b-a3b,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"import pandas as pd

df = pd.read_csv('winequality-red.csv')
filtered = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]
max_quality_index = filtered['quality'].idxmax()
print(df.loc[max_quality_index])",7,"The generated code correctly performs the filtering and identifies the wine with the highest quality based on the specified criteria. The core logic for tasks 1 and 2 is sound. However, it falls short on completeness for task 3 and general robustness.

**Strengths:**
*   Correctly applies compound filtering conditions `(df['alcohol'] > 12) & (df['pH'] < 3.3)`.
*   Correctly uses `idxmax()` to find the index of the highest quality wine in the *filtered* subset.

**Areas for Improvement:**
*   **Task 3 (Display all features):** The line `print(df.loc[max_quality_index])` will print the row from the *original* DataFrame (`df`) using an index (`max_quality_index`) that was obtained from the *filtered* DataFrame (`filtered`). This is a critical logical error. If the `filtered` DataFrame has fewer rows than the original, `max_quality_index` might refer to an incorrect row in the `df` or an `IndexError` could occur if the index isn't directly present in the `df` (although `idxmax` returns the original index, this is still a less robust way). The correct approach is to use `filtered.loc[max_quality_index]` to ensure you're displaying the wine from the subset you just analyzed.
*   **Robustness/Error Handling:** There's no error handling for the file not being found or for the possibility that no wines match the filtering criteria (i.e., `filtered` DataFrame is empty). If `filtered` is empty, `idxmax()` will raise an error.
*   **Clarity:** The code is very concise, which is good, but without comments or a function wrapper, it's less readable and reusable than the ground truth.
*   **Completeness (Implicit):** The prompt implied a more complete script (proper imports were mentioned, and ""display all features"" suggests the final output should be clearly presented). Simply printing a pandas Series might be okay, but the ground truth's `to_string()` or a more formatted output is often desired for ""display all features"".

To score higher, the generated code needs to fix the logical error in displaying the features and add basic error handling, especially for the empty filtered dataset case.",qwen3-30b-a3b,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('heart.csv')

plt.hist(df['Age'], edgecolor='black')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

sex_counts = df['Sex'].value_counts()
plt.bar(sex_counts.index, sex_counts.values, color=['blue', 'orange'])
plt.title('Distribution of Sex')
plt.xlabel('Sex')
plt.ylabel('Count')
plt.show()",7,"The generated code correctly loads the CSV and generates a histogram for `Age` and a bar chart for `Sex`, with titles and labels, thus fulfilling the core requirements. This is a good effort.

However, there are several areas for improvement to meet the full ""expert"" standard outlined in the ground truth:
1.  **Missing Seaborn:** The ground truth specified using `matplotlib` or `seaborn` and the ground truth code used `seaborn` for enhanced aesthetics (`sns.histplot`, `sns.countplot`), which the generated code does not. The `matplotlib.pyplot.hist` function is more basic.
2.  **Sex Labeling:** The `Sex` column in `heart.csv` typically uses numerical values (0 and 1, often for Female and Male respectively). The generated code plots these numerical values directly on the x-axis for the bar chart, which is less readable and informative than mapping them to descriptive labels like 'Male' and 'Female' as done in the ground truth.
3.  **Aesthetics and Readability:** The ground truth code includes more advanced plotting aesthetics and readability enhancements such as `kde=True` for the histogram, `bins='auto'`, specific `figsize`, `fontsize` for titles and labels, `plt.grid`, `plt.tight_layout`, and `palette` for the bar chart. While not strictly required by the prompt, these align with ""expert"" level code for data visualization.
4.  **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and `KeyError`, which is crucial for production-ready code or real-world data science tasks. The generated code lacks this.
5.  **`Sex` Column Assumption:** The ground truth code explicitly states the assumption about 'Sex' column values (0 for Female, 1 for Male) and handles the mapping. This clarity is good practice.

While the generated code technically accomplishes the plotting tasks, it falls short on the ""expert"" level of detail, robustness, and visual appeal demonstrated by the ground truth.",qwen3-30b-a3b,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"import pandas as pd

df = pd.read_csv('spotify_2023.csv')

print(""Columns and their data types:"")
print(df.dtypes)

df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
print(""Number of NaNs in 'streams' column:"", df['streams'].isna().sum())

numerical_columns = ['streams', 'bpm', 'danceability_%', 'valence_%', 'energy_%']
stats = df[numerical_columns].agg(['mean', 'median', 'std', 'min', 'max'])
print(""Descriptive statistics for numerical columns:"")
print(stats)

unique_artists = df['a...",8,"The generated code correctly addresses most of the task requirements.

**Strong points:**
- Correctly loads the CSV.
- Correctly identifies and prints column data types (though `df.dtypes` is less verbose than `df.info()`, it still fulfills the requirement).
- Converts 'streams' to numeric and reports NaNs introduced.
- Calculates correct descriptive statistics for the specified numerical columns using `agg`, which is clean and efficient.
- Accurately counts and reports unique artists.

**Areas for improvement to achieve a higher score:**
- **Error Handling for File Loading:** The ground truth includes `try-except` for `FileNotFoundError`. The generated code does not, which could lead to a crash if the file isn't present.
- **Reporting NaNs in 'streams':** While it reports the count, it doesn't show sample rows with NaNs as the ground truth does, which can be helpful for debugging or understanding the data quality issue.
- **Clarity of Output:** The ground truth uses more descriptive print statements and clear section headers, which improves readability for someone evaluating the output. For example, ""Columns and their data types:"" is good, but ""--- Column Names and Data Types ---"" with `df.info()` provides more detail (non-null counts).
- **Flexibility/Robustness for numerical columns:** The ground truth adds a check `if existing_numerical_cols:` and filters columns, making it more robust if one of the specified columns were missing from the dataset. While unlikely for this specific dataset, it's a good practice.

Overall, it's a very good attempt that fulfills the core requirements. The improvements would primarily focus on robustness and user experience of the output.",qwen3-30b-a3b,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"import pandas as pd

df = pd.read_csv('spotify_2023.csv')
filtered = df[df['released_year'] == 2023]
selected = filtered[['track_name', 'artist(s)_name', 'streams']]
selected['streams'] = pd.to_numeric(selected['streams'], errors='coerce')
sorted_df = selected.sort_values(by='streams', ascending=False)
print(sorted_df)",9,"The generated code correctly addresses all requirements of the task: it loads the CSV, filters for 2023, selects the specified columns, handles `streams` as numeric, and sorts the data in descending order. The code is concise and directly implements the logic.

One minor point for improvement, though not critical for this task, is the order of operations for `pd.to_numeric`. The generated code applies `to_numeric` after filtering and selecting columns, specifically on the `selected` DataFrame. This works fine here because `streams` is a selected column. However, it's often safer and generally better practice to apply `pd.to_numeric` to the `streams` column on the *original* DataFrame (`df`) or the *already filtered* DataFrame (`filtered`) *before* selecting the columns. This ensures that the column's type is consistently handled from an earlier stage in the pipeline and avoids potential `SettingWithCopyWarning` if pandas determines that `selected` is a view rather than a true copy (though in this case pandas usually handles it well). The ground truth code performs this step earlier, which is a good practice. Nevertheless, for the given task and dataset, the generated code produces the correct output.",qwen3-30b-a3b,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('spotify_2023.csv')

plt.hist(df['bpm'])
plt.title('Distribution of BPM')
plt.xlabel('BPM')
plt.ylabel('Frequency')
plt.show()

plt.scatter(df['danceability_%'], df['energy_%'])
plt.title('Relationship between Danceability and Energy')
plt.xlabel('Danceability (%)')
plt.ylabel('Energy (%)')
plt.show()",8,"The generated code correctly performs all requested tasks: loading the CSV, generating a histogram for `bpm`, and a scatter plot for `danceability_%` vs `energy_%`. Both plots have titles and appropriately labeled axes. The imports are correct.

Areas for minor improvement:
1.  **Separate Figures:** While the code shows two distinct plots due to `plt.show()` being called after each, best practice for generating multiple plots in Matplotlib when they are independent is to explicitly create new figures for each using `plt.figure()`. Without it, subsequent plot calls might try to draw on the same figure if `plt.show()` were not called prematurely. For this specific task, `plt.show()` after each plot makes it work as intended, but in a script where you might want to show all plots at once at the end, explicitly creating figures is crucial.
2.  **`plt.show()` Placement:** Calling `plt.show()` after each plot displays them sequentially. Often, it's desirable to generate all plots and then call `plt.show()` once at the very end to display them all together in separate windows or as part of a single output. For a benchmark, this difference in display sequence is minor, but it's a common convention.
3.  **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and `KeyError` (for missing columns). While not explicitly requested in the prompt, robust production-ready code would include such checks. For a benchmark evaluating a direct task, its omission is acceptable, but it's a difference.",qwen3-30b-a3b,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"import pandas as pd

df = pd.read_csv('youtube_trendings.csv')

print(""Column names and data types:"")
print(df.dtypes)

columns_to_process = ['Video views', 'Likes', 'Dislikes']
for col in columns_to_process:
    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce').fillna(0)

print(""\nDescriptive statistics:"")
print(df[['Video views', 'Likes', 'Dislikes']].agg(['mean', 'median', 'std', 'min', 'max']))

df['Category'].fillna('Unknown', inplace=True)
print(""\nNumber o...",9,"The generated code successfully completes almost all aspects of the task.

**Strong points:**
- Correctly loads the CSV.
- Accurately identifies and cleans the 'Video views', 'Likes', and 'Dislikes' columns by removing commas, converting to numeric, and filling NaNs with 0 in a concise manner.
- Correctly calculates and reports descriptive statistics for the cleaned numeric columns.
- Correctly fills missing 'Category' values with 'Unknown'.
- Correctly reports the count of unique values in the 'Category' column.
- Uses `inplace=True` for `fillna` which is efficient.

**Areas for minor improvement:**
- **Step 1 (Column names and inferred data types):** While `print(df.dtypes)` shows the data types, the prompt asked to ""List all column names and their inferred data types."" `df.info()` as used in the ground truth is generally preferred for this as it also shows non-null counts and memory usage, which is more comprehensive. The generated code only shows types and doesn't explicitly list column names separated from types in a user-friendly way, though they are implicitly present.
- **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this. While not explicitly asked for, it's a good habit in production-ready code.

Overall, the code is very effective and addresses the core requirements precisely. The minor points are about adhering to the exact phrasing of ""list all column names and their inferred data types"" and adding robustness, not about functional correctness of the data manipulation.",qwen3-30b-a3b,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"import pandas as pd

df = pd.read_csv('youtube_trendings.csv')
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)
filtered_df = df[df['Video views'] > 50_000_000]
result = filtered_df[['Video', 'Video views']].sort_values(by='Video views', ascending=False)
print(result)",9,"The generated code is concise and correctly solves the task. It successfully loads the data, cleans the 'Video views' column by removing commas, converting to numeric, and filling NaNs with 0. It then filters the data based on the view count, selects the required columns, and sorts them in descending order.

**Strong points:**
*   **Conciseness:** The cleaning and filtering operations are chained efficiently.
*   **Correctness:** All requirements of the task are met accurately.
*   **Readability:** Despite its conciseness, the code remains readable.

**Minor areas for improvement (not affecting score significantly):**
*   The ground truth included a `try-except` block for `FileNotFoundError`, which is good practice for robust code. The generated code assumes the file exists. While the task didn't explicitly ask for error handling, for a real-world scenario, it's beneficial.
*   The task description implicitly asks to ""display"" the columns, and `print(result)` achieves this. The ground truth also explicitly uses `print()`.",qwen3-30b-a3b,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('youtube_trendings.csv')

df['Video views'] = df['Video views'].astype(str).str.replace(',', '')
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce').fillna(0)

df['Likes'] = df['Likes'].astype(str).str.replace(',', '')
df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce').fillna(0)

plt.scatter(df['Video views'], df['Likes'])
plt.title('Video Views vs. Likes')
plt.xlabel('Video Views (cleaned)')...",9,"The generated code correctly addresses all the requirements of the task. It properly loads the CSV, cleans the 'Likes' and 'Video views' columns by removing commas, converting them to numeric, and filling NaNs with 0 using `errors='coerce'` and `fillna(0)`. Finally, it generates a scatter plot with the specified title and axis labels. The imports are correct, and the code structure is straightforward and functional.

One minor improvement is that the ground truth code includes error handling for file not found and general exceptions when loading the CSV, and also checks if columns exist before processing/plotting, which makes it more robust. However, for a direct solution to the given task, the generated code is perfectly fine and complete.",qwen3-30b-a3b,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"import pandas as pd

df = pd.read_csv('mobiles_2025.csv')

df['Mobile Weight'] = pd.to_numeric(df['Mobile Weight'].str.replace('g', ''), errors='coerce')
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''), errors='coerce')

print(df.dtypes)

stats = df[['Mo...",9,"The generated code correctly performs all the requested cleaning operations and statistical calculations. It successfully loads the data, cleans the specified columns by removing specific strings and converting to numeric (handling errors with `errors='coerce'`), prints the data types, calculates descriptive statistics for the cleaned columns, and reports the count of unique company names.

**Strong Points:**
- All cleaning steps are correctly implemented.
- `errors='coerce'` is used for robust numeric conversion.
- All required outputs (data types, descriptive stats, unique count) are produced.
- Concise and uses appropriate pandas functions.

**Areas for Improvement (minor):**
- Does not explicitly import `numpy`, though `pd.to_numeric` implicitly uses it, it's good practice for numerical operations.
- The `describe()` method for the statistics prints more than just mean, median, std, min, max (it also includes count and 25%/75% quantiles). The prompt specifically asked for mean, median, std, min, max. While `describe()` includes these, filtering for just those rows: `df[columns].describe().loc[['mean', '50%', 'std', 'min', 'max']]` as in the ground truth, would be more precise to the prompt's request for *basic* descriptive statistics. However, `describe()` is generally accepted for ""basic descriptive statistics"", so this is a minor nuance.
- Output formatting could be more explicit, e.g., adding headers like ""Column Names and Data Types:"" or ""Descriptive Statistics:"". The ground truth offers more verbose and user-friendly output messages.
- The `str.strip()` method is included in the ground truth for removing leading/trailing whitespace after string replacement, which is a good defensive programming practice, although it might not be strictly necessary for this specific dataset if the data is clean.",qwen3-30b-a3b,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd

df = pd.read_csv('mobiles_2025.csv')

df['Launched Price (India)'] = df['Launched Price (India)'].str.replace('INR ', '', regex=False).str.replace(',', '', regex=False)
df['Launched Price (India)'] = pd.to_numeric(df['Launched Price (India)'], errors='coerce')

df['RAM'] = df['RAM'].str.replace('GB', '', regex=False)
df['RAM'] = pd.to_numeric(df['RAM'], errors='coerce')

filtered_df = df[(df['Company Name'] == 'Apple') & (df['Launched Price (India)'] < 100000)]

re...",9,"The generated code correctly performs all the required steps: loading the CSV, cleaning and converting the 'Launched Price (India)' and 'RAM' columns (handling errors with `coerce`), filtering the dataset, selecting the specified columns, and sorting the final result. The code is concise and directly addresses the problem.

The only minor area for improvement (and why it's not a perfect 10) is the lack of explicit error handling (`try-except`) for file loading. While the task didn't explicitly ask for it, robust production-ready code would include it. However, given the prompt just to ""Provide only the Python code without explanations,"" and assuming the file exists, it fulfills the core requirements exceptionally well. Another tiny point is that for `df['Launched Price (India)'].str.replace('INR ', '', regex=False).str.replace(',', '', regex=False)`, if the column contains non-string types, this chained operation might fail. The ground truth's `astype(str)` handles this more robustly, but for typical CSV data, it's often not an issue.",qwen3-30b-a3b,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('mobiles_2025.csv')

df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')

sns.scatterplot(data=df, x='RAM', y='Battery Capacity', hue='Company Name')
plt.title('Battery Capacity vs RAM by Company')
plt.xlabel('RAM (GB)')
plt.ylabel('Battery Capacity (mAh)')
plt.show()
``...",9,"The generated code correctly addresses all aspects of the task. It loads the CSV, cleans both 'RAM' and 'Battery Capacity' columns by removing units and converting to numeric, handling errors by coercing to NaN as required. It then creates the specified scatter plot, colors points by 'Company Name', and includes appropriate titles and labels. The imports are correct and standard.

Minor points:
- The generated code modifies the original 'RAM' and 'Battery Capacity' columns directly, which is acceptable for this task but often better practice to create new columns (`RAM_cleaned`, etc.) to preserve original data. The ground truth demonstrates this better practice.
- The generated code is more concise than the ground truth, which can be a positive, but also leaves out the `try-except` block for file loading, which is good practice for robustness.
- The generated code doesn't include additional plotting enhancements like `figsize`, `s` (marker size), `alpha`, `grid`, or `tight_layout` that the ground truth provides for better visualization, but these were not explicitly requested as requirements to be ""included"".

Overall, it's a very good solution that successfully fulfills the task requirements.",qwen3-30b-a3b,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"import pandas as pd

df = pd.read_csv('titanic.csv')
print(df.head())
print(df.shape)
print(df.dtypes)
print(df['Survived'].mean() * 100)
print(df.isnull().sum())",7,"The generated code correctly performs most of the required tasks. It loads the dataset, displays the first 5 rows, shows the shape, calculates the survival rate, and counts missing values. The use of `df.dtypes` for data types is concise and correct.

However, there are a few areas for improvement:
1.  **Readability/Clarity**: The output lacks descriptive print statements, making it harder for a user to understand what each printed block of output represents. The ground truth explicitly labels each section (e.g., ""1. First 5 rows of the dataset:"").
2.  **Robustness**: The generated code does not include error handling for `FileNotFoundError` when loading the CSV, which could cause the script to crash if the file is not present.
3.  **Completeness (Data Types)**: While `df.dtypes` lists data types, `df.info()` from the ground truth provides a more comprehensive summary, including non-null counts and memory usage, which is often more useful in initial EDA. The prompt specifically asked for ""List all column names and their data types,"" and `df.dtypes` fulfills this, but `df.info()` goes a step further in completeness for EDA.
4.  **Formatting of Survival Rate**: The survival rate is calculated correctly but is not formatted to two decimal places as shown in the ground truth, which is a minor aesthetic point but contributes to professional output.

Overall, it's a functional solution but could benefit from better user experience and robustness.",qwen3-30b-a3b,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"import pandas as pd

df = pd.read_csv('imdb_top_250.csv')

print(""Column names and data types:"")
print(df.dtypes)

print(""\nDescriptive statistics for Year, Rating, and Runtime:"")
for column in ['Year', 'Rating', 'Runtime']:
    print(f""\n{column}:"")
    print(f""Mean: {df[column].mean()}"")
    print(f""Median: {df[column].median()}"")
    print(f""Standard Deviation: {df[column].std()}"")
    print(f""Min: {df[column].min()}"")
    print(f""Max: {df[column].max()}"")

print(f""\nTotal number of movies: {...",6,"The generated code correctly addresses most of the task requirements but lacks robustness and optimal pandas idiomatic usage.

**Correctness:**
- Loads the CSV properly.
- Lists column names and data types (though `df.dtypes` is less informative than `df.info()`).
- Calculates mean, median, std, min, max for the specified columns.
- Reports the total number of movies.

**Code Quality & Completeness:**
- **Robustness (Critical Missing):** The code assumes 'Runtime' is already numeric. The `imdb_top_250.csv` dataset typically has 'Runtime' as a string (e.g., '142 min'), which needs parsing before numerical operations. This is a significant omission, as the code will raise an error when trying to calculate statistics on a non-numeric 'Runtime' column. The ground truth code handles this preprocessing with `pd.to_numeric` and `str.replace`.
- **Error Handling:** No `try-except` block for `FileNotFoundError`, which is important for production-ready code.
- **Reporting Statistics:** While correct, iterating through columns and printing statistics individually is less efficient and less readable than using pandas' `describe()` method, which performs all these calculations for multiple columns simultaneously and presents them in a clean DataFrame format.
- **Data Type Listing:** `df.dtypes` is a valid way to list data types, but `df.info()` as used in the ground truth provides more comprehensive information, including non-null counts, which is often useful for initial data exploration.
- **Imports:** Imports `pandas` which is correct, but doesn't handle `sys` for graceful exit on file error like the ground truth.

To improve, the code should include preprocessing for the 'Runtime' column, add error handling for file loading, and utilize `df.describe()` for a more concise and pandas-idiomatic way of reporting descriptive statistics.",qwen3-30b-a3b,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"import pandas as pd

df = pd.read_csv('imdb_top_250.csv')
filtered = df[df['Rating'] >= 9.0]
result = filtered[['Title', 'Rating']].sort_values(by='Rating', ascending=False)
print(result)",8,"The generated code correctly performs all the required operations: loading the CSV, filtering by rating, selecting specific columns, and sorting. It uses appropriate pandas methods efficiently. The primary reason it doesn't receive a perfect score is the lack of robust error handling (e.g., `FileNotFoundError`, `KeyError`) which is present in the ground truth and is good practice for real-world scenarios. While the prompt implied a straightforward execution (""Provide only the Python code without explanations""), robust error handling is a key aspect of ""code quality"" for a production-ready script. It also doesn't define the `DATASET_PATH` as a constant, which can improve readability and maintainability.",qwen3-30b-a3b,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('imdb_top_250.csv')

plt.hist(df['Rating'])
plt.title('Distribution of Rating')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

plt.scatter(df['Runtime'], df['Rating'])
plt.title('Rating vs Runtime')
plt.xlabel('Runtime')
plt.ylabel('Rating')
plt.show()",6,"The generated code correctly addresses the core requirements of creating a histogram for 'Rating' and a scatter plot for 'Rating' vs 'Runtime', including titles and labels for both. This demonstrates a basic understanding of the task.

However, there are significant areas for improvement regarding robustness, data handling, and plot quality:

1.  **Missing Error Handling (FileNotFoundError):** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which the generated code lacks. This makes the generated code less robust.
2.  **Missing `Runtime` Column Preprocessing:** The `Runtime` column often comes as a string (e.g., '120 min') and needs to be converted to a numeric type for plotting. The generated code directly attempts to plot `df['Runtime']`, which would likely fail or produce incorrect results if the column is not already numeric. The ground truth explicitly handles this conversion and error cases.
3.  **No Check for Column Existence:** The ground truth checks if 'Rating' and 'Runtime' columns exist before attempting to plot. The generated code assumes their presence, which could lead to `KeyError` if the CSV structure changes.
4.  **Limited Visualization Enhancement:** While `matplotlib.pyplot` is used, the ground truth leverages `seaborn` (`sns.histplot`, `sns.scatterplot`) for aesthetically more pleasing and informative plots (e.g., KDE in the histogram, `alpha` for scatter points, better default styling). The generated plots are functional but basic.
5.  **No `tight_layout` or Figure Sizing:** The ground truth includes `plt.figure(figsize=...)` and `plt.tight_layout()`, which are good practices for plot readability and avoiding overlapping elements.

For a higher score, the code needs to incorporate robust data loading, more sophisticated data preprocessing, and better plotting practices for clarity and robustness.",qwen3-30b-a3b,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"import pandas as pd

df = pd.read_csv('penguins.csv')

# 1. Report missing values per column
print(""Missing values per column:"")
print(df.isnull().sum())

# 2. Drop rows with missing values in the specified numerical columns
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_clean = df.dropna(subset=numerical_cols)

# Calculate and report descriptive statistics for the numerical columns
print(""\nDescriptive statistics for numerical columns:"")
print(df...",7,"The generated code correctly performs all the required operations: loading the dataset, reporting NA counts, dropping NAs for specific columns and calculating descriptive statistics, and counting unique values for specified columns. However, it uses `describe()` for descriptive statistics, which provides more metrics than strictly requested (mean, median, std, min, max). While `describe()` includes these, it also adds count and 25/50/75 percentiles, making it less precise to the prompt's request. Additionally, the ground truth includes error handling for `FileNotFoundError` and provides more explicit print statements for clarity. The generated code also renames the dataframe to `df_clean`, which is a good practice, but not explicitly requested. For a perfect score, it should have used `.agg()` for precise descriptive statistics, and improved output formatting and error handling.",qwen3-30b-a3b,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"import pandas as pd

df = pd.read_csv('penguins.csv')
filtered = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')]
selected = filtered[['culmen_length_mm', 'culmen_depth_mm']]
cleaned = selected.dropna()
print(cleaned.head(5))",10,"The generated code correctly implements all the requirements of the task. It properly loads the CSV, filters the DataFrame based on `species` and `island`, selects the specified columns, drops rows with missing values in the selected columns, and displays the first 5 rows. The code is concise and efficient. The use of `dropna()` without a `subset` argument is correct in this context because only the two target columns are present in the `selected` DataFrame, so `dropna()` will implicitly operate on those two columns anyway. The error handling for `FileNotFoundError` present in the ground truth is good practice for robust applications but not strictly required by the prompt, which asks for the Python code assuming the CSV is in the current directory and focuses on data manipulation steps. Therefore, its absence does not penalize the score given the constraints.",qwen3-30b-a3b,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('penguins.csv')
df_clean = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])

sns.scatterplot(data=df_clean, x='culmen_length_mm', y='culmen_depth_mm', hue='species')
plt.title('Culmen Length vs Depth by Species')
plt.xlabel('Culmen Length (mm)')
plt.ylabel('Culmen Depth (mm)')
plt.show()",9,"The generated code correctly loads the dataset, filters for missing values in the specified columns, creates a scatter plot of `culmen_length_mm` vs `culmen_depth_mm` colored by `species`, and includes a title and axis labels. All requirements from the prompt are met.

The only minor area for improvement, preventing a perfect 10, is the lack of a `try-except` block for file loading, which makes the code less robust to `FileNotFoundError`. Additionally, wrapping the plotting logic within a function and using an `if __name__ == '__main__':` block (as seen in the ground truth) is generally better practice for reusability and script organization, though not strictly required by the prompt's explicit requirements. However, for a direct solution to the task, it perfectly fulfills the criteria.",qwen3-30b-a3b,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

df = pd.read_csv('penguins.csv')

# Fill NA values in the specified columns with their respective means
df['culmen_length_mm'].fillna(df['culmen_length_mm'].mean(), inplace=True)
df['culmen_depth_mm'].fillna(df['culmen_depth_mm'].mean(), inplace=True)

# Calculate and print statistics for culmen_length_mm
mean_length = df['culmen_length_mm'].mean()
median_length = df['culmen_length_mm'].median()
std_length = df['culmen_length_mm'].std()

# Calculate and print stati...",8,"The generated code correctly performs all the specified tasks: loading the dataset, filling NA values with the mean for the specified columns, calculating and printing the mean, median, and standard deviation for these columns, and printing the total number of rows and columns.

**Strong Points:**
- **Correctness:** All calculations and data manipulations are accurate.
- **Conciseness:** The code is more concise than the ground truth while still achieving the requirements.
- **Clarity:** It's easy to read and understand what each section of the code does.

**Areas for Improvement (to reach a 10):**
- **Error Handling:** The ground truth includes error handling for `FileNotFoundError` when reading the CSV, which is a good practice for robust code. The generated code lacks this.
- **Generality/Robustness:** The ground truth uses a loop and a list `NUMERICAL_COLUMNS` to process the numerical columns. This approach is more general and robust. If more columns needed similar processing, the generated code would require repetitive lines, whereas the ground truth could simply extend the list.
- **Output Formatting:** While the generated code prints the required statistics, the output format is less structured and descriptive compared to the ground truth's use of headings and clearer labels for each section (e.g., ""--- NA Imputation Summary ---"", ""--- Statistics for Cleaned Numerical Columns ---""). For a data science task, presenting results clearly is important.
- **Warning for Missing Columns:** The ground truth checks if a column exists before attempting to process it, providing a warning if it doesn't. This makes the code more robust against potential typos or changes in the source data.",qwen3-30b-a3b,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('penguins.csv')
df = df.dropna(subset=['flipper_length_mm'])

plt.hist(df['flipper_length_mm'])
plt.title('Distribution of Flipper Length (mm)')
plt.xlabel('Flipper Length (mm)')
plt.ylabel('Frequency')
plt.show()

sns.countplot(x='island', data=df)
plt.title('Penguin Counts by Island')
plt.xlabel('Island')
plt.ylabel('Count')
plt.show()
```",8,"The generated code correctly addresses all requirements of the task. It loads the dataset, handles NA values in `flipper_length_mm`, creates a histogram with the specified title and x-axis label, and generates a bar chart for island counts with appropriate titles and labels.

**Strong Points:**
- Correctly loads the data.
- Correctly drops NA values in `flipper_length_mm`.
- Generates both required plots (histogram and bar chart).
- Labels the axes and titles correctly for both plots.
- Uses `matplotlib.pyplot` for the histogram and `seaborn` for the countplot, which is a common and effective combination.

**Areas for Improvement (minor):**
- **Robustness for file loading:** The ground truth includes `try-except` blocks for `FileNotFoundError`, which makes the code more robust. The generated code assumes the file exists.
- **Histogram y-axis label:** The generated code uses `plt.ylabel('Frequency')` for the histogram, while the ground truth uses `plt.ylabel('Count')`. While ""Frequency"" is not strictly incorrect often for histograms, ""Count"" is generally more explicit when plotting raw counts per bin, especially when `density=False` (which is the default). The problem description did not specify `Frequency` or `Count`.
- **Plot aesthetics/customization:** The generated code does not include additional aesthetic enhancements (like `figsize`, `kde`, `bins`, `set_theme`, grid lines, `tight_layout`) that are present in the ground truth for improved readability and presentation. While these are not explicit requirements, they are good practices for visualization.
- **Clarity on `plt.show()` placement:** For the histogram, it's typical to put `plt.show()` after all manipulations for that specific plot are done. Here, the `plt.show()` for the histogram is well-placed before the `sns.countplot`, clearly separating the two visualizations.

Overall, the generated code is functional and meets the core requirements effectively. The minor improvements are more about robustness and aesthetic best practices rather than core correctness.",qwen3-30b-a3b,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the data to include only penguins from the 'Torgersen' island
torgersen_penguins = df[df['island'] == 'Torgersen']

# Count and print the number of penguins belonging to each species
print(torgersen_penguins['species'].value_counts())
```",8,"The generated code correctly implements all the core requirements of the task: it loads the data, filters it for 'Torgersen' island, and counts the species within that subset. The code is concise and directly addresses the problem.

However, it lacks robustness compared to the ground truth. Specifically:
- **Error Handling**: It doesn't include `try-except` blocks for `FileNotFoundError` when loading the CSV, which is crucial for real-world applications.
- **Column Existence Checks**: It assumes the 'island' and 'species' columns exist. If these columns were missing, the code would raise a `KeyError` instead of gracefully handling or informing the user.
- **Informative Output**: While it prints the `value_counts()` output, it lacks the descriptive print statements that clarify what's being displayed or how many records were found after filtering.
- **Edge Case (No Torgersen Penguins)**: If there were no 'Torgersen' penguins, the code would still execute `value_counts()` on an empty DataFrame, which is not an error but the ground truth handles this scenario more explicitly.

These are considerations for production-ready code, but for a benchmark task, the generated code fulfills the primary computational requirements.",qwen3-30b-a3b,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)
print(df.shape)
print(df.dtypes)
```",9,"The generated code correctly defines the DataFrame, prints its shape, and lists the data types of all columns. It completely addresses all the requirements of the task. The code is concise and functional.

One minor area for improvement, though not strictly required by the prompt's output format, would be to add descriptive print statements (e.g., ""Shape of the DataFrame:"", ""Data types of each column:"") before `print(df.shape)` and `print(df.dtypes)` similar to the ground truth. This enhances readability and makes the output more user-friendly, especially in a professional context. However, given the prompt only asked for ""Provide only Python code"" and fulfilled the core requirements, this is a very minor point.",qwen3-30b-a3b,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('titanic.csv')

# 1. Survivor Count
plt.figure()
df['Survived'].value_counts().plot(kind='bar')
plt.title('Survivor Count')
plt.xlabel('Survived')
plt.ylabel('Count')
plt.show()

# 2. Passenger Class Distribution
plt.figure()
df['Pclass'].value_counts().plot(kind='bar')
plt.title('Passenger Class Distribution')
plt.xlabel('Passenger Class')
plt.ylabel('Count')
plt.show()

# 3. Age Distribution
plt.figure()
df['Age'].dropna().h...",7,"The generated code correctly creates four distinct plots with appropriate titles and labels, addressing the core requirements. It successfully visualizes survival count, passenger class distribution, age distribution, and gender count. The imports are correct and the structure is clear.

However, there are a few areas for improvement:
1.  **Handling Missing Age Values**: The generated code uses `df['Age'].dropna().hist()`. While this prevents errors, it *discards* all rows with missing age values, which can lead to a loss of data and potentially misrepresent the age distribution of the entire dataset. The ground truth code handles this by imputing missing age values (using the median), which is a more robust approach for this type of visualization.
2.  **Readability/Clarity for 'Survived' plot**: The x-axis labels for the 'Survivor Count' plot are 0 and 1, which are less intuitive than ""Non-survivor"" and ""Survivor"". Mapping these to descriptive labels, as shown in the ground truth, would improve clarity.
3.  **Aesthetics and Robustness**: While not explicitly requested, the ground truth code includes better aesthetics (e.g., specific colors, `xticks(rotation=0)`, `plt.grid`, `plt.tight_layout`, `figsize`) and robustness (e.g., `try-except` for file loading, `sort_index` for Pclass), which are good practices for production-ready code. These are minor for this specific task but are good habits. The generated code is functional but basic in presentation.",qwen3-30b-a3b,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'], 
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'], 
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89], 
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93], 
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]}

df = pd.DataFrame(data)

# 1) Calculate and print the average Midterm_Score
average_...",10,"The generated code is entirely correct and complete. It effectively creates the DataFrame, calculates the average midterm and final scores, and accurately identifies the subject with the highest average final score. The code uses `pandas` correctly and efficiently for each of the requested operations. The formatting of the output for scores is also consistent with common practices.",qwen3-30b-a3b,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
    'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
    'Volume': [1000000, 1200000, 1100000, 1300...",9,"The generated code correctly creates the DataFrame, sets 'Date' as the index, calculates the 'Daily_Range', and identifies the date with the highest 'Close' price using `idxmax()`. It also prints the updated DataFrame as required. All core functionalities are implemented correctly.

The only minor difference from the ground truth is the printing of the highest close date. The ground truth uses an f-string and `strftime` to format the date output for better readability, while the generated code simply prints the `Timestamp` object. This is a minor stylistic difference and does not impact functionality, but slightly reduces the readability of that specific output. Overall, it's a very good solution that successfully addresses all requirements.",qwen3-30b-a3b,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST001']
}

df = pd...",9,"The generated code correctly performs all the requested tasks: it creates the DataFrame, calculates the 'Total_Revenue' column, identifies the best-selling product by quantity, and prints the first 5 rows of the DataFrame. The code is concise and uses appropriate pandas functions.

One minor point of improvement is the printing of the best-selling product. The ground truth code formats the output with an f-string to make it more descriptive (""The product sold the most in terms of Quantity is: Laptop""). The generated code just prints the product name directly, which is functional but less user-friendly. However, this doesn't detract from the correctness or core functionality.",qwen3-30b-a3b,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"import pandas as pd
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)
print(df['SessionDurationMinutes'].mean())
print(df['DeviceType'].value_counts())",8,"The generated code correctly performs all requested tasks: it defines the DataFrame, calculates the average session duration, and counts users by device type. It uses the appropriate pandas functions (`.mean()` and `.value_counts()`) for these operations.

However, the output formatting is minimal compared to the ground truth. While not explicitly asked for, good practice in data analysis often involves clear print statements to label results for readability. The ground truth provides more descriptive print statements for the average session duration and a separator for clarity. The generated code simply prints the raw results.

To achieve a higher score, the generated code could enhance the output readability with descriptive print statements, similar to the ground truth.",qwen3-30b-a3b,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

print(df['TemperatureCelsius'].max())
print(df['HumidityPercent'].min())
```",9,"The generated code correctly defines the DataFrame and accurately finds the maximum temperature and minimum humidity. It correctly uses `pd.DataFrame` to create the DataFrame and the `.max()` and `.min()` methods on the respective columns. The code is concise and functional.

However, the output format is slightly less readable than the ground truth. While it prints the correct values, it doesn't provide the descriptive text (e.g., ""Maximum TemperatureCelsius recorded:""). This makes the output less user-friendly, which is a minor quality improvement point, but not a functional incorrectness. It still meets the core requirement of finding and printing the values.",qwen3-30b-a3b,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

print(df['Likes'].sum())
print(df.groupby('PostType')['Shares'].mean().idxmax())
```",9,"The generated code correctly defines the DataFrame, calculates the total number of likes, and identifies the `PostType` with the highest average number of shares. The logic is sound and produces the correct output.

**Strengths:**
- **Correctness:** All three requirements of the task are met accurately.
- **Conciseness:** The code is very compact and uses pandas functionalities efficiently.
- **Readability (for experienced users):** For someone familiar with pandas, the expressions `df['Likes'].sum()` and `df.groupby('PostType')['Shares'].mean().idxmax()` are clear.

**Areas for Improvement:**
- **Output Clarity:** While the code prints the correct values, it lacks descriptive print statements. The ground truth uses f-strings (`print(f""Total number of Likes across all posts: {total_likes}"")`) which makes the output much more user-friendly and understandable without looking at the code. This is particularly important for benchmark evaluation where the output needs to be immediately interpretable.
- **Variable Assignment (optional but good practice):** Assigning the results to descriptive variable names (e.g., `total_likes`) before printing can sometimes improve readability for complex calculations, though for these simple sums/maxes, it's less critical.

Overall, it's a strong solution that effectively solves the problem with good use of pandas. The only minor drawback is the lack of descriptive output.",qwen3-30b-a3b,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",9,"The generated code is highly correct and complete, addressing all parts of the task. It correctly defines the DataFrame, calculates the average age, and counts patients by gender. The code quality is good, with proper imports and clear variable names.

The only minor area for improvement, which is purely aesthetic and doesn't affect functionality, is the formatting of the print statements. The ground truth example used f-strings for more controlled output formatting (e.g., `:.2f` for floats) and more descriptive print messages. While the generated code's print statements are functional, they could be slightly more refined for user readability, especially the average age output. However, this is a very minor point and does not detract significantly from the overall correctness or completeness.",qwen3-30b-a3b,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}
df = pd.DataFrame(data)
print(df.loc[df['Rating'].idxmax(), 'Title'])
print(df['Year'].mean())
```",10,"The generated code is concise and correct. It successfully defines the DataFrame, identifies the title of the movie with the highest rating using `idxmax()`, and calculates the average year using `.mean()`. The code is clean, efficient, and directly addresses all requirements of the task.",qwen3-30b-a3b,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

print(len(df))
print(df['PriceUSD'].mean())",9,"The generated code correctly performs all the required tasks: defining the DataFrame, printing the total number of cars, and calculating the average price.

**Strong points:**
*   Correctly defines the DataFrame as specified.
*   Uses `len(df)` to get the total number of cars, which is a perfectly valid and common way to get the number of rows.
*   Correctly uses `df['PriceUSD'].mean()` to calculate the average price.
*   Prints the results as required.

**Areas for minor improvement (to reach a perfect 10):**
*   The output of the printed statements is terse. The ground truth code provides more descriptive print statements (e.g., ""Total number of cars in the dataset: X"" vs. just ""X""). While not strictly a requirement, a more user-friendly output is generally preferred.
*   The average price is printed without currency formatting, unlike the ground truth which formats it to two decimal places with a dollar sign. This is a minor detail but improves readability for financial data.

Overall, the generated code is very good and directly addresses the problem statement's core requirements.",qwen3-30b-a3b,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}
df = pd.DataFrame(data)

print(df.loc[df['HP'].idxmax(), 'Name'])
print(df.loc[df...",9,"The generated code correctly defines the DataFrame and accurately identifies and prints the names of the Pokemon with the highest HP and lowest Defense. It is concise and directly addresses all requirements of the task.

The improvements from the Ground Truth are minor and more about explicit variable assignment and comments rather than functional correctness. The generated code directly incorporates `idxmax()` and `idxmin()` within the print statements, eliminating the need for intermediate variables. While the ground truth uses intermediate variables for clarity (`idx_highest_hp`, `pokemon_highest_hp`), the generated code's approach is more compact and equally readable for this specific task. The only reason it's not a perfect 10 is the lack of explicit, user-friendly messages for the print statements, as seen in the ground truth ""The Pokemon with the highest HP is: {pokemon_highest_hp}"". This makes the output less informative for an end-user.",qwen3-30b-a3b,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"import pandas as pd

df = pd.read_csv('titanic.csv')
df.isnull().sum()
df['Age'].fillna(df['Age'].median(), inplace=True)
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)
df.drop(columns=['Cabin'], inplace=True)
df['Title'] = df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())
print(df.shape)
print(df.head(5))",7,"The generated code correctly performs most of the required data cleaning steps. It handles missing Age and Embarked values, drops the Cabin column, extracts a 'Title' from the 'Name' column, and displays the shape and head of the cleaned dataset.

However, there are a few areas for improvement:
1.  **Missing Value Identification (Requirement 1):** The code calculates `df.isnull().sum()` but does not explicitly *display* the columns with missing values as requested by the prompt (""identify columns with missing values""). The ground truth prints this information. This is a minor omission for a quick benchmark, but for real-world scenarios, it's a critical diagnostic step.
2.  **Title Extraction Robustness (Requirement 5):** The regex-based title extraction in the ground truth is generally more robust than `x.split(',')[1].split('.')[0].strip()`. While the `split` method works for most common names, it can fail for edge cases where the format might differ slightly or if a name contains a period in a different context. The ground truth also includes standardization of less common titles, which, while not strictly required by the prompt, makes the `Title` column more useful and is a common practice in data science preprocessing. The generated code also doesn't explicitly `import re` which would be needed for the regex approach.
3.  **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is a good practice not present in the generated code.

Overall, it's a good effort that gets the core logic right but could be improved in terms of robustness, completeness in displaying intermediate steps, and error handling.",qwen3-30b-a3b,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}
df = pd.DataFrame(data)

print(df.loc[df['P...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately identifies the title of the book with the highest price, and calculates the total stock. The code is concise and directly addresses all requirements of the task. It utilizes efficient pandas operations (`idxmax()` and `.sum()`) which are standard and appropriate for these operations. The solution is complete and demonstrates a good understanding of fundamental pandas operations.",qwen3-30b-a3b,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"import pandas as pd

df = pd.read_csv('water_potability.csv')
print(df.head(10))
print(df.shape)
print(df.dtypes)
print(df['Potability'].value_counts(normalize=True)*100)
print(df.isnull().sum())
print(df.describe())",6,"The generated code correctly performs the core data analysis tasks as requested.

**Strengths:**
*   Correctly loads the dataset.
*   Displays the first 10 rows.
*   Shows the shape of the dataset.
*   Calculates potability percentages.
*   Identifies missing values.
*   Shows summary statistics.

**Areas for Improvement:**
*   **Readability/Clarity:** The output is very condensed. The task specifically asked for various components to be displayed separately (`--- 1. Load the dataset...`, `--- 2. Show the shape...`, etc.) to improve clarity and readability, which the generated code lacks.
*   **Column Names and Data Types:** While `df.dtypes` provides the data types, the request was for ""all column names and their data types."" The `df.info()` method (used in the ground truth) typically provides a more comprehensive overview, including non-null counts, which is often more useful in EDA than just dtypes.
*   **Missing Value Display:** While `df.isnull().sum()` is correct, it prints all columns, even those with no missing values. The ground truth code focuses on showing only columns with missing values and explicitly stating ""No missing values found"" if applicable, which is a better practice for quickly identifying problematic columns.
*   **Robustness:** There's no error handling for `FileNotFoundError` or other potential issues when loading the CSV, which is crucial for real-world applications.
*   **Formatting:** The generated code lacks explicit print statements or labels for each section, making the output less organized and harder to follow compared to the ground truth. The ground truth used clear headers (`--- 1. ...`, `--- 2. ...`) and separators (`=`*50) to delineate sections, which was part of the implied expectation for structured output.",qwen3-30b-a3b,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"import pandas as pd

df = pd.read_csv('water_potability.csv')

# Step 1: Identify columns with missing values
before_missing = df.isnull().sum()
print(""Before cleaning missing values:"")
print(before_missing)

# Step 2: Fill missing values using mean for numerical columns
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())

# Step 3: Check for and remove duplicates
duplicates = df.duplicated().sum()
prin...",6,"The generated code addresses most of the requirements but has several shortcomings that prevent it from being excellent.

**Strengths:**
*   Correctly loads the data.
*   Handles missing values by filling with the mean for numerical columns.
*   Correctly identifies and removes duplicate rows.
*   Attempts to check for reasonable ranges by removing negative values.
*   Displays the first 5 rows of the cleaned dataset.

**Weaknesses and Areas for Improvement:**

1.  **Robustness on File Loading:** The generated code lacks a `try-except` block for `FileNotFoundError` which is present in the ground truth. This makes it less robust for real-world scenarios.
2.  **Before/After Statistics Clarity:** The summary statistics are less informative than the ground truth.
    *   It prints `before_missing` but doesn't explicitly state the missing values *after* the imputation step, only after the *entire* cleaning process.
    *   The `before_shape` and `after_shape` are flawed. `before_shape` is taken *after* duplicates are removed and some rows are potentially dropped due to range checks, not truly the ""before"" shape. `after_shape` is then taken *after* an unnecessary `df.dropna()` at the end, which might remove rows that were perfectly valid but had missing values in other columns (which should have been handled by the mean imputation). This makes the ""before and after"" comparison inaccurate and confusing.
    *   The `after_missing` calculation is also affected by the final `df.dropna()`, making the output misleading. The goal is to show that *after* filling missing values, there are no more missing values. The ground truth correctly checks `df.isnull().sum()` *after* imputation to verify this.
3.  **Range Validation Specificity:** While checking for non-negative values is a good start, the ground truth's range validation for 'ph' (0-14) is more specific and accurate for that particular parameter. The generated code applies the `df[col] >= 0` filter uniformly to all numerical columns, which is sufficient for most but not ideal for `ph`.
4.  **Implicit Information:** The ground truth code explicitly prints messages for each step (e.g., ""Filling missing values:"", ""Removed X duplicate rows."", ""No numerical columns with missing values found""), which greatly enhances readability and understanding of what the code is doing. The generated code is more silent.
5.  **Unnecessary `df.dropna()`:** The line `df = df.dropna()` at the very end before calculating `after_shape` is problematic. If the mean imputation was successful, there shouldn't be any missing values by that point (unless `potability` or other categorical columns had NaNs, which weren't targeted by `fillna(mean)`). This line could inadvertently drop rows if, for example, the 'Potability' column had missing values (though typically it wouldn't for this dataset). It makes the `after_shape` and `after_missing` statistics inaccurate for the purpose of demonstrating the outcome of *mean imputation*.",qwen3-30b-a3b,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('water_potability.csv')

# Bar chart for potable vs non-potable distribution
plt.figure()
sns.countplot(data=df, x='Potability')
plt.title('Distribution of Potable vs Non-Potable Water')
plt.xlabel('Potability')
plt.ylabel('Count')
plt.show()

# Histograms for pH, Hardness, and Chloramines
plt.figure()
sns.histplot(df['pH'], kde=False)
plt.title('Distribution of pH')
plt.xlabel('pH')
plt.ylabel(...",7,"The generated code correctly addresses the core requirements of the task by generating the four specified types of visualizations. The titles and labels are present, and the correct libraries are imported.

However, there are several areas where the code could be improved to match the quality and completeness of the ground truth:

1.  **Missing Value Handling:** The ground truth explicitly mentions and handles missing values for plotting (e.g., `dropna()` for histograms). The generated code implicitly relies on `seaborn`'s default handling, which is usually okay, but explicit handling (like `.dropna()`) is better practice and more robust, especially for histograms where `kde=True` might struggle with NaNs. While `seaborn`'s plotting functions generally ignore `NaN`s, explicitly calling `.dropna()` can sometimes prevent unexpected behavior or warnings, especially if `kde=True` is used.
2.  **Visualization Formatting and Readability:**
    *   **Multiple `plt.figure()` for Histograms:** The ground truth groups the histograms for pH, Hardness, and Chloramines into a single figure with subplots, which is much more efficient and visually cohesive than creating three separate figures for each histogram, as the generated code does. This is a significant drawback for presentation.
    *   **Titles & Labels:** While titles and labels are present, the ground truth provides more descriptive labels (e.g., ""Potability (0: Not Potable, 1: Potable)"", units like ""mg/L"") and uses `fontsize` for better readability.
    *   **Styling:** The ground truth sets `sns.set_style(""whitegrid"")`, which provides a consistent, clean look. It also uses specific `palette` and `color` arguments for better visual differentiation, and adds `grid` lines for better readability. The generated code lacks these aesthetic enhancements.
    *   **Boxplot Labels:** The ground truth explicitly sets x-tick labels for the boxplot to 'Not Potable' and 'Potable', improving clarity.
    *   **Heatmap Formatting:** The ground truth includes `fmt="".2f""` for the heatmap to format the annotation values, and (`plt.xticks(rotation=45, ha='right')` and `plt.yticks(rotation=0)`) to ensure labels don't overlap, which is crucial for larger heatmaps.
    *   **`plt.tight_layout()`:** This is used consistently in the ground truth to prevent plot titles and labels from overlapping, which is missing in the generated code.
3.  **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, making the code more robust.
4.  **Overall Polish:** The ground truth demonstrates best practices for data visualization by providing more detailed and thoughtful labels, consistent styling, and efficient use of subplots.

In summary, the generated code produces the required plots but lacks the refinement, best practices, and detailed formatting that make the ground truth code production-ready and highly readable from a data science perspective. The decision to create separate figures for each histogram significantly detracts from the solution's quality.",qwen3-30b-a3b,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression

df = pd.read_csv('winequality-red.csv')

# Create new features
df['alcohol_volatile_ratio'] = df['alcohol'] / (df['volatile acidity'] + 1e-5)
df['citric_ph_interaction'] = df['citric acid'] * df['pH']
df['residual_sugar_squared'] = df['residual sugar'] ** 2

X = df.drop('quality', axis=1)
y = df['quality']

# Recursive Feature Elimination...",6,"The generated code correctly performs feature engineering with three new features and applies two different feature selection techniques (RFE and SelectKBest). It correctly identifies the selected features.

However, it has several areas for improvement to meet all requirements and enhance code quality:

1.  **Meaningful Feature Engineering:** While three features are created, their 'meaningfulness' and explicit justification are not discussed in the code (as per the prompt: ""meaningful features...e.g. ratios, interactions, polynomial features""). The ground truth provides detailed comments explaining the rationale behind each engineered feature, which was implicitly part of defining ""meaningful.""

2.  **Comparison and Contrast:** The prompt explicitly asks to ""Compare and contrast the feature sets selected by each method."" The generated code simply prints the selected features from each method but does not perform an explicit comparison (e.g., finding common features, unique features, or discussing why differences might arise), nor does it ""contrast"" them as the ground truth does by explaining the nature of filter vs. wrapper methods.

3.  **Displaying Importance Scores/Rankings:** The generated code extracts `importances_rfe`, but `importances_rfe[rfe.get_support()]` will only yield the importances for the *final* selected features *from the estimator on the full feature set before RFE*. To get the correct importance scores *for the selected features after RFE*, the estimator needs to be refit on only those selected features, as shown in the ground truth. The `scores_kbest` are printed correctly, but they are not presented in a meaningful, sorted table format with feature names, as requested by ""display final selected features with their importance scores or rankings.""

4.  **Robustness and Best Practices:**
    *   No handling for potential `FileNotFoundError` for the CSV.
    *   No handling for `NaN` or `inf` values that could be introduced by division in feature engineering.
    *   Feature scaling (`StandardScaler`) is generally good practice before many ML tasks, especially RFE with certain estimators, even if not strictly required for RandomForest or `f_regression`. The ground truth includes this, showing a more complete pipeline.
    *   The prompt asked for ""proper imports."" While the necessary ones are there, placing all imports at the top is standard.
    *   The output formatting is very minimal; making the output more readable and explicit (e.g., using `pd.DataFrame` to display results for better readability and comparison) would improve the solution significantly.",qwen3-30b-a3b,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import cross_val_score, cross_validate, KFold, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sk...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being a high-quality solution.

**Positive Aspects:**
*   **Dataset Loading & Preprocessing:** Handles loading, identifies `Global_Sales` as the target, and sets up a preprocessor with `SimpleImputer` and `OneHotEncoder`.
*   **Multiple Ensemble Methods:** Implements Random Forest, Gradient Boosting, XGBoost, and AdaBoost.
*   **Cross-Validation:** Uses `KFold` and `cross_validate` for model evaluation.
*   **Voting and Stacking Ensembles:** Correctly implements both using `VotingRegressor` and `StackingRegressor` (though stacking uses `LinearRegression` as meta-learner, which is good).
*   **Evaluation Metrics:** Calculates MAE, MSE, and R.
*   **Hyperparameter Tuning:** Uses `GridSearchCV` for tuning, which is good.
*   **Feature Importance Plot:** Attempts to create a feature importance plot.
*   **Residual Plot:** Attempts to create a residual plot.

**Areas for Improvement / Missing / Incorrect Implementations:**

1.  **Robust Preprocessing:**
    *   The generated code's preprocessor applies to all numerical/categorical columns found initially. However, it doesn't handle the `Name` and `Rank` columns (which should be dropped as per the ground truth) or the `User_Score` conversion (from 'tbd' to NaN and then numeric). This is a significant data preprocessing oversight.
    *   The ground truth explicitly defined numerical and categorical features after dropping irrelevant columns and handling 'User_Score' conversion, ensuring the preprocessor only acts on relevant columns.

2.  **Explicit Target & Feature Separation:** The generated code directly uses `df.drop('Global_Sales', axis=1)` and `df['Global_Sales']` repeatedly within `cross_validate` calls without explicitly defining `X` and `y` upfront. While it works, it's less clean and can be less efficient than a single `X, y = ...` definition.

3.  **RMSE Calculation:** The task explicitly asks for RMSE, but the generated code only calculates MAE, MSE, and R.

4.  **Hyperparameter Tuning Logic:**
    *   The `param_grid` for `GridSearchCV` is hardcoded for `XGBoost`. The requirement was to tune the *best individual model*. The generated code determines the best model (`best_model_name`) but then universally applies the XGBoost `param_grid` without checking what `best_model_name` actually is. This means if `RandomForest` or `GradientBoosting` was the best, it would try to tune them with XGBoost-specific parameters, which would cause an error.
    *   The `GridSearchCV` in the generated code is performed on the `best_model` pipeline directly, which is correct in principle, but the `param_grid` issue is critical.

5.  **Feature Importance Plot Issues:**
    *   `preprocessor.transformers_[0][1].get_feature_names_out(numerical_cols)` and `preprocessor.transformers_[1][1].get_feature_names_out(categorical_cols)`: This method of getting feature names is fragile. It assumes `numerical_cols` are always the first transformer and `categorical_cols` the second, and that `numerical_cols` themselves are the initial column names, not the transformed ones. The ground truth's approach of fitting the `ColumnTransformer` and then using `get_feature_names_out` on the `OneHotEncoder` within the transformer is more robust for obtaining processed feature names.
    *   The graph `sns.barplot(x=importances, y=features)` will likely lead to an unreadable plot if there are many features due to automatic labeling and dense x-axis. The ground truth uses `nlargest(20)` for readability.
    *   The feature importance is only shown for the `best_tuned_model`. The task requested feature importance plots for *ensemble models*, implying more than just one. While `VotingRegressor` and `StackingRegressor` don't directly expose `feature_importances_`, the ground truth at least explained this limitation and showed for the best individual model that did.

6.  **Residual Plot Issues:**
    *   The residual plot uses `best_tuned_model.predict(X)` where `X` is the *original* dataframe without preprocessing. This will cause an error because `best_tuned_model` is a `Pipeline` that expects the raw `X` to preprocess it first. The ground truth first trains the final model on the data (or a split) and then passes `X_test` (or the whole `X` *through the pipeline*) for prediction.
    *   Only one residual plot (vs. predicted) is created, missing the distribution of residuals often paired with it.

7.  **Reproducibility & Robustness:**
    *   Missing `np.random.seed(42)` at the beginning (though `random_state=42` is used in models, the overall setup often benefits from a global seed).
    *   Error handling for `csv` loading is missing.
    *   No print statements for progress or step completion, making it harder to follow.

8.  **Overall Structure & Completeness:** The code runs through steps, but the logical flow and detailed implementation of each requirement are not as complete or robust as the ground truth. Specifically, the ""compare multiple ensemble methods"" part is just printing scores, not providing a consolidated summary as the ground truth does. ""Analyze prediction errors"" is limited to one plot with a bug.",qwen3-30b-a3b,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error
import pymc3 as pm
import theano.tensor as tt
import rup...",5,"The generated code attempts to address most of the requirements but falls short in correctness, completeness, and robustness, particularly for a benchmark designed to test ""advanced"" statistical modeling.

1.  **Correctness:**
    *   **Data Loading & Aggregation:** The generated code assumes 'Year' column in the CSV, but the dataset likely uses 'Year_of_Release'. This leads to a `KeyError`. Even if it were 'Year', no cleaning for missing/non-numeric values is performed, which is critical for real-world datasets.
    *   **ARIMA Model:** `statsmodels.tsa.arima.model.ARIMA` requires the `endog` (time series data) to be passed directly, not a DataFrame with an index. Also, the `yearly_sales` DataFrame's index `Year` is currently an integer, not a datetime index, which is standard practice for time series models. `pmdarima.auto_arima` (used in ground truth) is widely preferred for automatically determining ARIMA orders, which is a more advanced approach than a fixed (1,1,1) order.
    *   **Seasonal Decomposition:** `seasonal_decompose` also expects a time series with a proper frequency-set datetime index. Its usage on integer index `ts` will likely fail or produce nonsensical results. For yearly data, ""seasonal"" decomposition typically isn't applicable in the traditional sense unless a multi-year cycle `period` is specified.
    *   **Multivariate Regression:**
        *   The interaction terms are applied to *all* features after `get_dummies`, including the dummy variables themselves, leading to a massive and likely multicollinear feature set (`X_poly.shape[1]` will be huge). This is not typical for `PolynomialFeatures` interaction terms and indicates a misunderstanding of how interaction terms are properly built or selected.
        *   It then builds `model_ml = LinearRegression().fit(X_poly, y)` which is a dense regression with potentially thousands or tens of thousands of features, making it computationally expensive and unlikely to be stable or interpretable.
        *   No filtering of top categories for categorical features (Genre, Platform, Publisher) is done before one-hot encoding, leading to an explosion of columns and potential memory/instability issues.
    *   **Bayesian Regression:**
        *   `pymc3` is used, but it's applied to the `X_poly` from the problematic multivariate regression, inheriting the same issues regarding feature explosion.
        *   The prior for `beta` is `shape=X_poly.shape[1]`, which couples all coefficients together. While technically valid, it highlights the issue of very high dimensionality when applied to all-feature interaction terms.
        *   It attempts `pm.sample` but with no progress logging or checks, and it's highly likely to fail on typical hardware given the feature explosion.
    *   **Change Point Detection:**
        *   `ruptures` is correctly identified, but the `pen` (penalty) value is arbitrarily set to 10. `ruptures` algorithms often require careful tuning of this parameter, or an adaptive approach. The ground truth uses `2 * np.log(n_samples_signal)` as a heuristic, which is a more principled approach than a fixed value.
    *   **Survival Analysis:**
        *   This is critically incorrect. ""Game longevity"" based on `yearly_sales['Year']` and `(yearly_sales['Global_Sales'] > 1000).astype(int)` totally misinterprets survival analysis concepts. Survival analysis needs a *start* event, an *end* event (or censoring), and the time duration between them for *each individual entity* (game, in this case). The generated code performs survival analysis on the aggregated yearly sales data, not on individual games, and arbitrarily defines an ""event"" based on whether total sales exceed 1000, which is meaningless for longevity modeling. This is a severe conceptual error.
        *   `lifelines.utils.survival_table_from_events` is imported but not used.
    *   **Confidence and Prediction Intervals:**
        *   For `model_ml`, it attempts `np.std(y_pred) * 1.96` as prediction intervals, which is incorrect. This is merely a scaled standard deviation of predictions, not a proper prediction interval that accounts for uncertainty in parameters and residuals around new predictions. SKLearn models do not directly provide PIs; dedicated methods or bootstrapping are usually required.
        *   For Bayesian regression, it correctly uses `pm.sample_posterior_predictive`, but the `pred_intervals_bayesian` calculation `np.percentile(posterior_pred['obs'], [2.5, 97.5], axis=0)` is for the *training data's* posterior predictions, not for *new* data points as typically expected for prediction intervals.

2.  **Code Quality:**
    *   **Imports:** Mostly correct in terms of libraries, but some are unused (e.g., `theano.tensor`, `survival_table_from_events`).
    *   **Structure:** Linear, no functions, which makes it hard to manage and debug. Error handling (e.g., `try-except` for file loading) is missing.
    *   **Readability:** Variable names are okay. Lack of comments makes it hard to follow complex parts.

3.  **Completeness:**
    *   **ARIMA:** Only a basic fixed-order ARIMA, no `auto_arima` or model selection.
    *   **Seasonal Decomposition:** Does not handle the `period` parameter or the fact that yearly data typically doesn't have ""seasonality"" in the monthly/quarterly sense.
    *   **Multivariate Regression:** No confidence intervals for coefficients or proper prediction intervals.
    *   **Polynomial Regression:** No confidence/prediction intervals (only approximation for sklearn).
    *   **Bayesian Regression:** Lack of post-sampling analysis like `az.summary` and proper PI/CI for new data.
    *   **Change Point Detection:** No plot or proper identification of years.
    *   **Survival Analysis:** Fundamentally incorrect implementation (as detailed above). Missing CoxPHFitter summary or specific predictions.
    *   **Confidence/Prediction Intervals for ALL models:** Markedly incomplete and/or incorrect for most models.
    *   **Validation (Time Series Cross-Validation):** The implementation `for train_idx, test_idx in tscv.split(X_poly):` uses `X_poly` which is `df_regression.drop(columns=['Global_Sales'])` transformed, and applies `LinearRegression`, `Ridge`, `Lasso` to this. This is for the *regression* task, not for ARIMA. The `y` in the loop `y[train_idx]` is Series, not the time series `ts` for ARIMA. This implies a lack of distinction between different models and their appropriate validation methods. The `print(""Ridge Score:"")` is a basic score, not RMSE, and doesn't provide enough detail for validation.
    *   **Visualizations:** Only `decomposition.plot()` and `kmf.plot()` are present; no plots for ARIMA forecasts, change points, or Cox survival prediction.
    *   **No explanation:** While the prompt asked for ""only Python code without explanations,"" the missing comments and context within the code make it highly difficult to understand the intent or potential issues, especially given the complexity of the task.",qwen3-30b-a3b,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture
from sklearn.manifold import TSNE
from umap import UMAP
from sklearn.metrics import silhouette_score, adjusted_rand_score, homogeneity_score, completeness_score, silhouette_samples
from sklearn.decomposition import PCA
from scipy.spatial.distance import cdist
from scipy.st...",6,"The generated code attempts to address many aspects of the prompt but falls short in several key areas, leading to a moderately functional but incomplete and at times incorrect solution.

**Strong Points:**
- Correctly loads data and includes feature engineering.
- Implements multiple clustering algorithms (K-Means, DBSCAN, Agglomerative, GMM).
- Includes dimensionality reduction (t-SNE, UMAP) and basic visualization.
- Attempts to implement optimal cluster selection and consensus clustering.
- Includes anomaly detection and association rule mining.
- Tries to build a recommendation system and includes validation metrics.

**Areas for Improvement / Issues:**

1.  **Preprocessing & Feature Engineering:**
    *   `df.dropna()` is too aggressive. The ground truth carefully imputes or drops specific columns, especially `Year_of_Release`, `Global_Sales`, `NA_Sales`, etc., to preserve data. `df.dropna()` will drop rows that might have missing sales values but perfectly valid other fields, leading to significant data loss.
    *   `Regional_Sales_Distribution` calculation: The generated code creates new columns `Regional_Sales_Distribution_NA`, etc., which is good, but `df['Regional_Sales_Distribution']` itself is a Series of Pandas Series (from `apply(lambda x: x / x.sum(), axis=1)`), which would be unusable. The ground truth correctly stores proportions as separate columns.
    *   `Market_Position`: Uses `Total_Sales` for ranking, which is derived from regional sales. It ranks by `min` method. The ground truth uses `Global_Sales` and `dense` method, which is more common for ranking without gaps.
    *   `Publisher_Portfolio_Size`: Calculates `value_counts()` on 'Publisher' itself, which gives the number of *games* for each publisher. The prompt asked for `Publisher_Portfolio_Size` (presumably unique games), which the ground truth does by `df.groupby('Publisher')['Name'].nunique()`.

2.  **Clustering Algorithms:**
    *   `DBSCAN` with fixed `eps=0.5, min_samples=5`. These are critical parameters that should ideally be tuned or dynamically determined (e.g., using a k-distance plot), as in the ground truth. A fixed value leads to poor performance or failure if the data density doesn't match these values. The ground truth includes a basic method to estimate `eps`.
    *   `GaussianMixture` and `AgglomerativeClustering` don't require `n_clusters` to be passed during `fit` for prediction in the way it's done. `AgglomerativeClustering` needs it upon initialization. The `GaussianMixture` `predict` method assigns cluster labels *after* fitting. The generated code `GMM(n_components=n_clusters).fit(X)` creates the model, but `gmm.predict(X_scaled)` is not explicitly called and stored. (It appears it intends to use `gmm` variable later, but `gmm` is overridden in the loop.)

3.  **Optimal Cluster Selection:**
    *   Only `KMeans` is used for silhouette analysis for optimal clusters, whereas the task requires multiple algorithms. The ground truth performs silhouette analysis for K-Means, GMM, and Hierarchical.
    *   No implementation of Elbow method or Gap statistic as requested.

4.  **Consensus Clustering:**
    *   The implementation of `consensus_clustering` is fundamentally flawed. It sums the *labels* from different algorithms (`consensus += cluster`). Summing cluster labels (e.g., 0+1+2+3=6) is not a valid way to find consensus. Consensus clustering typically involves either building a co-occurrence matrix (as in the ground truth) or using techniques like M3C, where agreement on pairs of points is measured.
    *   It then applies `SpectralClustering` which requires a similarity matrix, but it's applied on sum of labels, which is arbitrary.
    *   DBSCAN noise points (`-1`) are not handled, which would distort any label-based consensus.
    *   The `n_clusters` for SpectralClustering is hardcoded to 2, which contradicts the goal of finding an ""optimal K"".

5.  **Cluster Profiling & Statistical Significance:**
    *   `clusterprofiles` function just returns mean/std. It doesn't perform statistical significance testing (ANOVA, Chi-squared) as explicitly requested in the prompt and done by the ground truth.
    *   The profiling uses `X_scaled` (scaled data), while profiling should ideally use the *original, unscaled features* to provide interpretable insights (e.g., ""Cluster 1 has high NA_Sales""). The ground truth correctly uses `df_original` for profiling.

6.  **Anomaly Detection:**
    *   Uses `IsolationForest` correctly. However, it doesn't display or interpret the anomalies beyond just calling the function. The ground truth shows the top anomalous games.

7.  **Association Rule Mining:**
    *   Only uses 'Genre' and 'Publisher'. The prompt and ground truth also involve the `Consensus_Cluster` itself in ARM, which is a powerful way to understand patterns within and across clusters.
    *   The `df_bin` creation is too restrictive (uses `get_dummies` directly on categorical columns), not allowing for rules like `Genre=Action AND Cluster=1`.
    *   Does not perform as comprehensively as the ground truth which explicitly prepares data for ARM including the cluster labels.

8.  **Recommendation System:**
    *   The `recommend_games` function stores `df['Title']` in `cluster_games`, but `Title` is not a column in the dataset (`Name` is).
    *   It stores `df['Title']` as `cluster_games`, but doesn't actually implement a recommendation logic given a specific game (e.g., 'If I like Game A, what else should I recommend?'). The ground truth provides an example of this.

9.  **Validation Metrics:**
    *   Only includes Silhouette and ARI (Adjusted Rand Index). ARI requires ground truth labels, which are not available in an unsupervised clustering task, making it an inappropriate metric here. The ground truth uses other internal metrics like Davies-Bouldin and Calinski-Harabasz indices, which do not require ground truth.
    *   External validation metrics (if ground truth were available) were missed.

10. **Code Structure and Imports:**
    *   `GaussianMixture` is imported twice.
    *   `UMAP` is imported as `UMAP` but used as `umap`. Error handling for file not found is missing.

Overall, the generated code provides a template, but many sections are either simplified, lack proper implementation for the given prompt, or contain conceptual errors, especially in consensus clustering and validation. The preprocessing is also less robust than required.",qwen3-30b-a3b,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.ensemble import VotingClassifier, BaggingClassifier, StackingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.m...",6,"The generated code attempts to address most of the requirements but falls short in implementation details, making it significantly less robust and comprehensive than the ground truth.

**Correctness:**
*   **Feature Engineering:** The generated code only adds two basic polynomial features (`BMI_squared`, `Age_BMI`), falling short of ""comprehensive feature engineering"" and missing imputation of zeros as NaNs. The ground truth performs both polynomial features and custom interactions after proper imputation.
*   **Bayesian Optimization:** The `objective` function for Optuna in the generated code is general (`objective(trial, model_type)`) but then `optimize_model` calls `cross_val_score(model, X, y, cv=5, scoring='accuracy')` on the entire dataset `X, y` (or `X_train, y_train` in the main block), which is not typical practice for hyperparameter tuning (usually done on a validation set or inner CV fold). The ground truth correctly uses a train-validation split for Optuna tuning.
*   **Nested Cross-Validation:** The `nested_cv` function in the generated code itself performs hyperparameter optimization using the `optimize_model` function inside each outer fold. This is a correct approach for true nested CV for hyperparameter tuning. However, the models built using these `optimize_model` calls are very basic (e.g., `VotingClassifier` combines them *after* their individual optimization, but then only the `VotingClassifier`'s accuracy is stored, not the individual models' performances).
*   **Missing Models/Specifics:**
    *   No explicit Bagging with *different base estimators* as requested. Only one `BaggingClassifier` with `DecisionTreeClassifier` is shown.
    *   No hard voting for `VotingClassifier` explicitly shown in the `nested_cv` function, even though it's briefly mentioned in the overall main block.
    *   No explicit AdaBoost or Gradient Boosting in the `nested_cv` optimization loop, though they are optimized separately in `objective` and run in the main block.
    *   The ""dynamic ensemble"" implementation is overly simplistic. It trains models on the training data and then assigns weights based on `model.score(X, y)` on the *same training data*, which is prone to overfitting and not a robust dynamic weighting strategy. The ground truth uses validation set performance from an inner fold to assign weights.
*   **CatBoost for Categorical Features:** The ground truth specifically calls out `cat_features=None` for `CatBoost` because the data is numerical *after preprocessing*. The generated code uses it directly, but the point of the requirement was its *ability* to handle categorical features. The dataset is entirely numerical.
*   **SHAP Values:** The generated code computes `shap_values` on `X_train`, not `X_test`, which is generally fine but evaluation is usually on test data. Also `shap_values` for tree models return a list/array for multi-output models, the ground truth correctly handles `shap_values[sample_idx,:]` for single instance and the `explainer.expected_value` correctly `explainer.expected_value[1]` for binary classification. The generated code also calls `shap.summary_plot(shap_values, X_train)` twice.
*   **Model Calibration:** The generated code only uses `CalibratedClassifierCV` with `method='sigmoid'`, a single method, not ""model calibration and reliability analysis"" as fully implemented by ground truth (which showed both sigmoid and isotonic calibration and plotted reliability curves).
*   **Ensemble Diversity Analysis & Model Combination:** The diversity analysis is a simple correlation matrix (`np.corrcoef(predictions)` which might be an issue as `predictions` are likely class predictions [0s and 1s] not probabilities, making `np.corrcoef` less informative for probability correlations). The ground truth specifically correlates OOF *probabilities* which is more meaningful. The ground truth also explicitly shows a meta-learner on OOF predictions, which the generated code hints at but doesn't fully implement or evaluate as a separate strategy.

**Code Quality:**
*   **Imports:** Mostly correct, but `itertools` and `time` are imported but unused. `MinMaxScaler` is imported but not used.
*   **Structure:** The use of global `X` and `y` without explicit passing `df` throughout `load_and_preprocess_data` function in ground truth is better. The generated code has `X`, `y`, `X_scaled` as global, then functions use `X`, `y` without being clear if it's scaled or original.
*   **Readability:** The `optimize_model` function and `objective` function are a bit convoluted in how they handle `model_type` as part of the `objective` function and then wrap it.
*   **Redundancy:** The `bayesian_optimization` function in the main block is almost identical to the `optimize_model` function, specifically for XGBoost. This creates unnecessary duplication.

**Completeness:**
*   **Comprehensive Feature Engineering:** Missing imputation, and detailed feature creation (e.g., polynomial, custom ratios).
*   **Advanced Ensemble Methods:** Missing specific different base estimators for Bagging (e.g., Bagging with SVC or LR), and the detailed breakdown of all boosting methods (`AdaBoost`, `GradientBoosting`, `XGBoost`, `LightGBM`, `CatBoost`) in the `nested_cv` loop is not clear. The `CatBoost` usage also doesn't highlight its categorical feature handling as part of the solution for the task.
*   **Nested Cross-Validation:** While the structure is present, the inner workings (e.g., what metrics are stored, how models are selected) are less comprehensive than the ground truth. Only accuracy is stored, not AUC, precision, recall, F1 as in the ground truth.
*   **Custom Ensemble with Dynamic Weight Assignment:** The implementation provided is very rudimentary and likely overfitting.
*   **Model Calibration and Reliability Analysis:** Missing reliability curves and Brier score analysis after calibration. Only one calibration method (`sigmoid`).
*   **Ensemble Diversity Analysis and Model Combination Strategies:** The diversity analysis method is questionable (correlating class predictions). The ground truth's meta-learner over OOF probabilities is a more advanced strategy.

Overall, the generated code attempts to cover the points, but the depth and correctness of the implementations are lacking compared to the ground truth. It provides a skeletal framework but misses many crucial details and best practices for robust data science solutions.",qwen3-30b-a3b,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"import pandas as pd
import numpy as np
import pymc3 as pm
import theano.tensor as tt
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import mixedlm

# 1) Load and preprocess dataset
df = pd.read_csv('diabetes.csv')
print(""M...",4,"The generated code attempts to address all requirements but falls significantly short in implementation details, correctness, and completeness for several key aspects of advanced statistical modeling and Bayesian analysis.

**Primary Reasons for Failure & Critical Missing/Incorrect Parts:**

1.  **Correctness:**
    *   **Hierarchical Bayesian Model (2):** The implementation is highly generic and incorrect for a hierarchical model. It attempts to define `beta` with `group_mean` and `group_sd` but doesn't actually use a grouping variable from the `df_imputed` dataframe. The `shape` argument for `beta` is also incorrect as `len(df_imputed.columns)-1` makes it a single vector, not group-specific parameters. There's no definition of what constitutes a ""group"" in the data.
    *   **Mixed-Effects Model (3):** Similar to the hierarchical model, it assumes a non-existent 'group' column (`pd.factorize(df_imputed['group'])`). The `shape` for `beta` is also incorrect for a mixed-effects setup. This makes the mixed-effects model non-functional as written.
    *   **Bayesian Logistic Regression with Informative Priors (4):** The `mu` calculation `beta_age * df_imputed['age'] + beta_bmi * df_imputed['bmi'] + beta_bp * df_imputed['bp']` assumes `age`, `bmi`, `bp` are lowercase and directly proportional, and it only includes three features, neglecting others in the dataset. This is a partial and hardcoded implementation that doesn't generalize.
    *   **Survival Analysis (6):** The simulation of 'time' and 'event' is trivial and does not reflect a realistic time-to-event setup (e.g., higher risk leading to shorter times). The PyMC3 survival model uses a `Weibull` likelihood, but the `event` observation `pm.Bernoulli('event', p=tt.switch(df_imputed['event'], 1, 0), observed=df_imputed['event'])` is fundamentally incorrect for right-censored survival data. `pm.Weibull` can handle censored data through its `observed` parameter, but linking it via a separate Bernoulli for `event` is not how it's done.
    *   **Gaussian Processes (7):** The input `X` for the GP is `df_imputed[['age', 'bmi']].values`. GPs for 2D inputs are possible, but the prompt asks for ""non-parametric modeling of relationships between *variables*"" which typically implies a function of one or a few inputs predicting another variable, not necessarily predicting the `Outcome` directly using a binary likelihood for the `f` output of the GP. The choice of `pm.Bernoulli('y', p=pm.math.sigmoid(f), observed=df_imputed['diabetes'])` means it's trying to do GP classification, which is a valid use case but not explicitly what ""modeling relationships between variables"" implies in the context of the example (usually continuous relationships).
    *   **Causal Inference (8):** Only implements Propensity Score Matching (PSM), and even then, it defines a random 'treatment' column, which invalidates the purpose of PSM (to control for confounders given an *actual* treatment). It completely misses Instrumental Variables.
    *   **Model Comparison (9) & Posterior Predictive Checks (10):** These are only shown for the `logistic_model` (which is itself incomplete) and not across different models. `pm.waic` and `pm.loo` are used, but no comprehensive comparison (like `az.compare`) is performed.

2.  **Completeness:**
    *   **Comprehensive Missing Value Handling:** While `SimpleImputer` is used, the ground truth explicitly identifies common zero-values that act as missing data in `diabetes.csv` and handles them before imputation, which is a crucial step missed here.
    *   **Mixed-effects models:** The model setup is incorrect because the grouping variable is missing.
    *   **Survival Analysis:** Simulation of time-to-event is very simplistic and doesn't reflect the complexity of real-world survival data or how `lifelines` (used in GT) would handle it.
    *   **Causal Inference:** Completely misses Instrumental Variables. The PSM is implemented trivially using a random treatment.
    *   **Gaussian Processes:** While GP is attempted, its application context (which variables, continuous vs. classification) is unclear or simplified.
    *   **Model Comparison:** Only compares one model's WAIC/LOO, not comparing multiple *distinct* models as requested.
    *   **Credible Intervals:** Not explicitly generated or plotted.
    *   **Data Preprocessing:** No standardization of features, which is crucial for MCMC convergence and GP performance.

3.  **Code Quality:**
    *   **Structure:** The code is structured section-wise according to the prompt, which is good.
    *   **Imports:** Missing `arviz` and `StandardScaler`, `lifelines`, `NearestNeighbors`. `theano.tensor` is imported but `arviz`'s `az.from_pymc3` and `az.compare`, for example, are needed for proper model comparison and analysis.
    *   **Clarity/Readability:** Uses placeholder column names like `df_imputed.iloc[:, :-1]` for features, which makes the code less readable compared to using actual column names.
    *   **Assumptions:** Makes broad assumptions about column names (`df_imputed['group']`, `df_imputed['age']`, `df_imputed['bmi']`, `df_imputed['bp']`, `df_imputed['diabetes']`) that are not present in the original dataset or derived. This leads to non-functional code.

In summary, while the code structure follows the requested tasks, the actual implementations are often either non-functional due to missing data structures (e.g., `group` column), logically incorrect for the statistical method (e.g., survival analysis, hierarchical models), or overly simplified/partial to fully meet the ""advanced"" requirement.",qwen3-30b-a3b,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering, OPTICS
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from umap import UMAP
from sklearn.metrics import silhouette_score, adjusted_rand_score, rand_score
from sklearn.cluster import Agglomer...",5,"The generated code attempts to address many aspects of the prompt but falls short in several key areas, leading to a score of 5.

**Correctness:**
*   **Data Preprocessing:** The imputation `df.fillna(df.median(), inplace=True)` is not appropriate for the Pima Indians Diabetes dataset, where '0' values in columns like Glucose or BloodPressure represent missing data, not actual zero measurements. The ground truth correctly identifies and imputes these specific '0' values.
*   **Metabolic Syndrome Indicators:** The generated code uses `waist`, `bp`, `glucose`, `hdl`, `trig`, which are not all present in the `diabetes.csv` dataset. This will cause an error or lead to incorrect feature engineering. The Ground Truth correctly identifies and uses existing features.
*   **Clustering Parameter Tuning:** The generated code hardcodes `n_clusters=3` for KMeans, GMM, Agglomerative, and Spectral Clustering. The prompt explicitly requires ""Determine optimal clustering parameters using silhouette analysis, gap statistic, and elbow method with statistical validation,"" which is missing for most algorithms. The helper function `evaluate_clusters` only has placeholders for 'gap' and 'elbow' methods.
*   **DBSCAN/OPTICS Parameterization:** `eps` and `min_samples` for DBSCAN and OPTICS are hardcoded without any method for optimal determination, which is crucial for these algorithms.
*   **Consensus Clustering:** The generated code imports `sklearn.cluster.consensus` (which doesn't exist in `scikit-learn` as a direct module for consensus clustering, likely confusing it with external libraries or conceptual consensus). The implementation provided uses a placeholder `ConsensusCluster` which is not a standard `sklearn` class. The ground truth provides a robust implementation of consensus clustering based on a co-occurrence matrix.
*   **Phenotype Analysis:** The `phenotype_results` function iterates based on `np.unique(labels)` but the `X` it receives is the *scaled* data. When `analyze_clusters` tries to `np.mean(cluster_data[:, i])` and compare it to `X[labels != cluster][:, i]`, it's operating on scaled values, not original, clinically interpretable features. The `p_value` calculation using `ttest_ind` on scaled data against the rest of the dataset is also not ideal for cluster characterization; ANOVA/Kruskal-Wallis testing of original features across all clusters is more appropriate, as shown in the ground truth.
*   **Time-Series Clustering:** The generated code provides only a comment saying ""Assuming time-series data exists..."" without further conceptual discussion or acknowledging the dataset's nature. The ground truth explicitly states the dataset is cross-sectional and provides a detailed conceptual outline for how time-series clustering *would* be done if the data were temporal.
*   **Missing Features:** The generated code completely misses the detailed feature engineering steps from the ground truth related to `High_Glucose`, `High_BP`, `High_BMI`, and `Metabolic_Syndrome_Score`. It attempts a metabolic syndrome calculation but uses non-existent columns.
*   **`GridSearchCV`:** The import of `GridSearchCV` and its presence without use suggests incomplete development or thought.

**Code Quality:**
*   **Imports:** Several imports are present but not used (e.g., `PCA`, `adjusted_rand_score`, `rand_score`, `pairwise_distances_argmin_min`, `silhouette_samples`, `GridSearchCV`). Conversely, a critical import like `SimpleImputer` or `LabelEncoder` (not strictly needed but useful for general cases) is missing.
*   **Clarity:** Lack of comments for complex sections and placeholders (`# Placeholder for gap statistic implementation`) reduce clarity.
*   **Hardcoding:** Excessive hardcoding of `n_clusters=3` without explanation or validation goes against the prompt's requirements.
*   **Error Handling:** No robust error handling (e.g., `try-except` for file loading or failed clustering attempts).

**Completeness:**
*   **Feature Engineering:** Very weak on metabolic syndrome indicators, using non-existent columns.
*   **Optimal Parameter Determination:** Fails to properly implement the elbow method, gap statistic, or systematic silhouette analysis for all algorithms.
*   **Consensus Clustering:** The implementation is incorrect and uses a non-standard module.
*   **Patient Phenotype Profiles:** While it attempts this, the execution is problematic (scaled features, inappropriate statistical test).
*   **Association Rule Mining:** The example shows how to get transactions from `metabolic_syndrome` and `Outcome`, but it comments out the actual generation (`transactions = ...`) and the discretization of continuous features is entirely missing, making it unusable in its current form. The ground truth provides a full and functional implementation with proper binning.
*   **Clinical Interpretation:** Completely missing. The prompt explicitly asks for ""clinical interpretation"" for risk stratification.

In summary, the generated code has a conceptual understanding of the tasks but fails significantly in execution due to incorrect assumptions about the dataset, missing implementations for parameter tuning, incorrect consensus clustering, and flawed or incomplete implementations for phenotyping and association rule mining, along with a complete absence of clinical interpretation.",qwen3-30b-a3b,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import re
import optuna
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import VotingRegressor, BaggingRegressor, StackingRegressor
from sklearn.linear_model im...",5,"The generated code attempts to address many requirements but falls short significantly in correctness, completeness, and adherence to best practices for advanced machine learning tasks.

**Critical Failures:**

1.  **Data Preprocessing and Pipelines:**
    *   `df.dropna()` is used without consideration for imputation strategies, which was a specific requirement for comprehensive preprocessing.
    *   `X_processed = preprocessor.fit_transform(X)`: The preprocessor is fit on the entire dataset `X` immediately. This is a severe data leakage issue. The preprocessor should be part of a `Pipeline` that is fit *within* cross-validation folds or on training data only. The ground truth correctly integrates the preprocessor into pipelines before model training.
    *   `X` `['Country name', 'Ladder']`: The dropping of 'Ladder' as a column contradicts the `y = df['Ladder']` target definition. The target variable is named 'Ladder score' in the dataset, not 'Ladder'. This will cause a `KeyError`.
    *   The PolynomialFeatures are applied indiscriminately to all numerical features, which may lead to high dimensionality and overfitting without careful consideration.
    *   Missing `eval_metric` for `XGBRegressor` when not using `use_label_encoder=False` which is deprecated.
    *   `CalibratedClassifierCV` is used for regression calibration, which is fundamentally incorrect. The task specifically asks for regression calibration.
    *   `StratifiedKFold` is used, which is for classification, not regression. `KFold` should be used.

2.  **Ensemble Methods Implementation:**
    *   **Voting Regressor:** The `voting='hard'` parameter is deprecated and has no effect for `VotingRegressor` in recent scikit-learn versions; for regression, `voting` only accepts 'soft' which implies averaging. There's no distinct ""hard"" vs ""soft"" for regression as there is for classification vote counting. The ground truth correctly notes this and demonstrates simple vs. weighted averaging.
    *   **Bagging:** `BaggingRegressor` is instantiated directly with base models without being integrated into a pipeline that includes preprocessing.
    *   Many models (`hard_voter`, `soft_voter`, `bagging_models`, `boosting_models`, `stacking_model`) are defined but never explicitly trained or evaluated with `fit()` or `predict()` calls outside of the generic `models` dictionary. The ground truth explicitly fits and prints base scores for each ensemble type.

3.  **Hyperparameter Tuning (Optuna):**
    *   The `objective` function trains on the *entire* `X_processed` dataset (`cross_val_score(model, X_processed, y)`) rather than fitting models within a pipeline on appropriately preprocessed `X` and `y`. This again leads to data leakage if `X_processed` contains information from the test set or if the preprocessing was done on the entire dataset. The ground truth correctly passes `X` and `y` to the cross_val_score within the objective, relying on the model pipeline to handle preprocessing.
    *   The `model_class` parameter for `objective` is passed but not used for constructing the model's pipeline within the function, implying that `X_processed` might not be correctly preprocessed for the model if it's outside the `objective`'s `model_class` scope.

4.  **Nested Cross-Validation:**
    *   The `nested_cv` function performs an outer loop, but the `model.fit(X_train, y_train)` call happens *before* any inner loop or hyperparameter tuning. A true nested CV involves applying the *entire* model selection and tuning process (like the Optuna search) *inside* each outer fold, using only the training data of that fold. The current implementation does not perform proper nested cross-validation. The ground truth provides a clear explanation and example of proper nested CV.
    *   It operates on `X[train_idx]` and `y[train_idx]` directly, assuming `X` is already numerically processed, which is incorrect if `X` still contains raw categorical or untransformed numerical features.

5.  **Custom Ensemble:**
    *   The `dynamic_weight_ensemble` fits models on the *entire* dataset (`model.fit(X, y)`) both for calculating scores and generating predictions, leading to severe overfitting of weights and an unrealistic performance estimate. OOF predictions are essential for this, which the ground truth correctly implements.

6.  **Blending:**
    *   The `blend_models` function trains base models directly on `X_train` and `y_train` without including the necessary preprocessing pipeline. This means models within the blend will fail if `X_train` is raw data.

7.  **Interpretation (SHAP, LIME):**
    *   **SHAP:** Uses `models['XGB']`, which is a raw `XGBRegressor` instance, not a pipeline. This will fail if `X_processed` is not directly compatible with the raw regressor (e.g., if feature names are lost or if scaling is expected). It will also fail if `X_processed` is a sparse matrix due to `PolynomialFeatures`.
    *   **LIME:** The setup for LIME is incorrect. `explainer = lime.lime_tabular.LimeTabularExplainer(X_processed, mode=""regression"")` will fail as `X_processed` is likely a sparse matrix due to `OneHotEncoder` and `PolynomialFeatures`, and LIME expects a dense representation or a specific handling of sparse data not set up here. The `predict_fn` is also missing. The ground truth provides a `predict_fn_for_lime` that handles the pipeline correctly.

8.  **Model Calibration & Uncertainty Quantification:**
    *   `CalibratedClassifierCV` is used, which is for classification probability calibration, not regression. For regression, model calibration usually refers to issues like homoscedasticity of residuals or specific methods like Isotonic Regression applied to residuals, or techniques for prediction intervals like Quantile Regression, which is missing from the generated code but correctly implemented in the ground truth.

9.  **Ensemble Diversity Analysis:**
    *   The `diversity_metrics` calculates errors based on `model.predict(X)` on the *entire* dataset. Similar to the custom ensemble, this is data leakage. OOF predictions are crucial for this. The ground truth's approach using OOF correlations is correct.

**Minor Issues / Missing Elements:**

*   Lack of imports for `CalibratedRegressorCV`, `minimize` from `scipy.optimize`, `sns`, `matplotlib.pyplot` for all plotting needs.
*   The ground truth explicitly defined `TARGET` and `DROP_COLS` to simplify code and avoid hardcoding column names.
*   The `df.dropna()` at the start will remove all rows with any missing values, potentially losing a lot of data, instead of handling specific imputation strategies.
*   The `PolynomialFeatures` without feature selection or regularization can introduce significant issues.
*   No explicit model calibration plots or uncertainty quantification as distinct components (like quantile regression from ground truth).
*   No multi-level stacking implementation. Blending is rudimentary.
*   No model combination optimization as requested (finding optimal weights).

Overall, while the generated code imports necessary libraries and outlines functions for different components, the underlying implementation details for data handling, pipeline integration, cross-validation, and specific advanced techniques are fundamentally flawed, leading to incorrect results or runtime errors.",qwen3-30b-a3b,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import arviz as az
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from statsmodels.formula.api import ols
from statsmodels.stats.outliers_influence import variance_inflation_factor
from...",4,"The generated code attempts to address all parts of the task but falls significantly short in implementation quality and correctness for several advanced statistical modeling and causal inference techniques.

Here's a breakdown of the issues:

**Correctness & Completeness:**

*   **1) Data Preprocessing:**
    *   Missing value handling is basic (`SimpleImputer` only), lacking the comprehensive `ColumnTransformer` and `OneHotEncoder` for categorical variables as suggested by ""comprehensive missing value handling."" Also, the example directly uses the `df` without proper column selection, renaming, or handling of `Ladder_score` specifically.
    *   No renaming of columns for easier access (`Ladder` instead of `Ladder_score`, etc.) which might cause issues with provided variable names later.
*   **2) Hierarchical Bayesian Models:**
    *   The implementation is highly simplified and likely incorrect. The `Ladder` variable is used, but the `mu_global` and `sigma_global` priors are too vague (`mu=0, sigma=10`). Crucially, `happiness = pm.Normal('happiness', mu=mu_region[region_idx], sigma=1, observed=df['Ladder'])` assumes a fixed `sigma=1` for the likelihood and does not include any fixed effects/predictors, which is a core part of a hierarchical model for happiness prediction. The prompt explicitly asks for modeling happiness *with regional random effects*, implying fixed effects too.
    *   It defines `Ladder` and `Regional indicator` from the `df` directly, but the column names are `Ladder_score` and `Regional_indicator` in the dataset. This implies direct column access without proper preprocessing and renaming.
*   **3) Structural Equation Models (SEM):**
    *   The `semopy` library is used, but the specified `Ladder` field is not part of the `sem_model` definition. The variables like `Economy`, `Family`, `Life.Ladder`, `Trust` are not present in the dataset's column names. The provided SEM model is generic and doesn't map to the `world-happiness-report-2021.csv` features at all (e.g., `Economy` and `Family` are not columns). This makes the SEM part fundamentally incorrect and non-functional for the given dataset.
*   **4) Causal Inference (IV and PSM):**
    *   **IV:** The `iv_regression` function is a manual implementation of 2SLS. The choice of `Freedom` as an IV for `Logged GDP per capita` is highly questionable from a causal inference perspective (it's very unlikely to satisfy the exclusion restriction). The result `gdp_iv` only stores coefficients, not confidence intervals or full results. The original prompt also mentioned ""simulate time component"" for DiD, implying the need for more careful simulation for IV if a natural IV isn't available.
    *   **PSM:** The PSM implementation is incomplete. It calculates propensity scores but then directly performs `train_test_split` on `X` and `y` for PS estimation, which is unusual. The matching step is completely missing (e.g., `NearestNeighbors` or similar to find matches), and the ATE calculation is not shown. It only computes `df['ps']` but does not use it to create matched groups or estimate the effect.
*   **5) Bayesian Regression with Informative Priors:**
    *   This section is implemented but incorrectly references dataset columns (`Ladder`, `Trust`) that aren't consistently named or handled in the general preprocessing. The `mu` calculation also assumes they are numeric and correctly named.
*   **6) Gaussian Processes:**
    *   The GP model is very basic. `X=df[['Logged GDP per capita']]` should ideally be scaled like in the Ground Truth for better performance, and `pm.gp.GP` is an old class. `pm.gp.Latent` or `pm.gp.Marginal` are typically used. The GP model definition `gp_model = pm.gp.GP(...)` followed by another `with pm.Model() as gp_model:` is confusing and likely means the first line is inert. It uses `X=df[['Logged GDP per capita']]` again without proper preprocessing or scaling.
*   **7) Mediation Analysis:**
    *   The `mediation_analysis` function uses `ols` (OLS regression within `statsmodels`). While this can be part of mediation analysis, it's not a Bayesian approach using PyMC3, which was implied by the context of other PyMC3 models. It also doesn't perform the decomposition into total, direct, and indirect effects with credibility intervals from posteriors, which is a key Bayesian advantage.
*   **8) Difference-in-Differences (DiD):**
    *   The simulation of `time`, `treatment`, and `post` is entirely random (`np.random.randint`). This is very primitive and doesn't simulate any meaningful policy or pre/post structure correctly. A proper simulation would involve creating a panel dataset with a 'pre' and 'post' period and assigning a treatment based on some criteria related to the data, not randomly. The `did_model` using `time * treatment + post * treatment` only considers these random interactions, making the ""policy impact assessment"" meaningless.
*   **9) Bayesian Model Comparison:**
    *   It uses `az.compare` which is correct, but the models being compared (the basic hierarchical and basic Bayesian linear one) are flawed themselves, diminishing the value of the comparison.
*   **10) Credible Intervals, PPC, Sensitivity:**
    *   `az.plot_posterior` and `az.plot_forest` are basic visualization calls.
    *   PPC is *not* implemented explicitly (`pm.sample_posterior_predictive` is missing).
    *   Sensitivity analysis is represented by `posterior_checks` which is just plotting, not a true sensitivity analysis by varying priors or data and observing impact on posteriors.

**Code Quality:**

*   **Imports:** Includes unnecessary imports like `ARIMA`, `variance_inflation_factor`, `KNeighborsRegressor`, `itertools`, `semopy` (though `semopy` is used, its application is incorrect). Also, `pm.distributions` and `theano.tensor` are generally not directly imported or used in typical PyMC3 model definitions.
*   **Structure:** The structure is fragmented. For example, preprocessing is done globally but then individual sections use raw `df` columns without accounting for the preprocessing.
*   **Readability:** Variable names are sometimes inconsistent with original column names, and several functions are defined but not used effectively.
*   **Reproducibility:** No `np.random.seed` or `pm.set_data_buffer_size`, which were explicitly part of the ground truth for reproducibility and performance.
*   **PyMC3 usage:** There are some older PyMC3 patterns or less optimal ways of defining models (e.g., `sigma=1` for likelihood, general structure of GP).

In summary, while the code structure matches the requirements by having sections for each point, the implementations are often superficial, incorrect, or rely on incorrect assumptions about the dataset and statistical methods. It lacks the depth, robustness, and correctness expected from an ""expert"" evaluation of advanced techniques.",qwen3-30b-a3b,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,,,Error,Failed to generate code,qwen3-30b-a3b,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX
import pmdarima as pm
from sklearn.metrics import mean_squared_error
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])
df = df[df['location'] == 'Germany']
df = df.set_index('date')
df = df.sort_index()
df['new_cases'] = df['new_cases'].fillna(method='ffill')
df['stringency_index'] = df['stringency_index'].fillna(method='ffill')
df['p...",7,"The generated code correctly implements the core requirements of the task: data loading, filtering, index setting, SARIMA modeling with exogenous variables, hyperparameter selection using `auto_arima`, training, forecasting, and RMSE evaluation, along with plotting.

**Areas for Improvement (to achieve a higher score):**

1.  **Missing Value Handling:**
    *   The generated code uses `ffill` for missing values, which is acceptable. However, it only applies it once. The ground truth demonstrates a more robust approach by including `bfill` and `interpolate` as safeguards, especially for initial NaNs or gaps. The generated code also applies `fillna` to each column separately, which can be less efficient than applying it to a selected `data` DataFrame, as done in the ground truth.
    *   It doesn't include the suggested `bfill` for cases where `ffill` might leave NaNs at the beginning of the series.
    *   It doesn't handle the edge case where `ffill` might leave NaNs if the initial values are all missing (though less likely for 'Germany' in this dataset).

2.  **Error Handling and Robustness:**
    *   The generated code lacks robust error handling, such as `try-except` blocks for file loading (`FileNotFoundError`), checking if there's enough data for the test set, or ensuring the DataFrame isn't empty after data manipulation. The ground truth includes these checks, making the code more resilient.

3.  **Code Clarity and Variable Naming:**
    *   While functional, variable names like `df` being reused after filtering (`df = df[df['location'] == 'Germany']`) is okay, but creating a `germany_df` like in the ground truth can sometimes improve clarity, especially in larger scripts.
    *   The print statements for progress and final RMSE result are more user-friendly in the ground truth.

4.  **Forecast Handling for Negative Values:**
    *   The problem statement implies 'new_cases' which cannot be negative. The ground truth explicitly handles potential negative forecasts by setting them to 0, which is a good practice for real-world applications of epidemiological data. The generated code does not include this.

5.  **SARIMAX Model Instantiation Parameters:**
    *   The ground truth explicitly sets `enforce_stationarity=False` and `enforce_invertibility=False` in the `SARIMAX` constructor. While `auto_arima` might implicitly handle differencing, it's good practice to be explicit, especially if `d` or `D` are greater than 0, to avoid potential convergence issues. This detail is missing in the generated code.

Overall, it's a good attempt that covers the main technical aspects, but could be improved significantly by adding more robust error handling, detailed missing value imputation, and better output feedback, which are crucial for a production-ready solution.",qwen3-30b-a3b,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import itertools

df = pd.read_csv('winequality-red.csv')

df['quality_group'] = pd.cut(df['quality'], bins=[0,4,6,10], labels=['low', 'medium', 'high'])

features = [col for col in df.columns if col not in ['quality', 'quality_group']]

significant_features = []

for feature in features:
    stat, p = stats.shapiro(df[feature])
    norma...",7,"The generated code addresses most of the requirements but has several areas for improvement in correctness and robustness.

**Strengths:**
*   Correctly categorizes wine quality into low, medium, and high.
*   Attempts to use Shapiro-Wilk for normality testing to decide between ANOVA and Kruskal-Wallis.
*   Performs post-hoc tests (Tukey HSD and pairwise Mann-Whitney U with Bonferroni correction).
*   Generates box plots for significant features.
*   Identifies and prints significant features.

**Areas for Improvement (leading to a score of 7):**

1.  **Normality Test Application (Critical):** The generated code checks for normality using `stats.shapiro(df[feature])` which tests the entire feature column for normality. However, for deciding between ANOVA and Kruskal-Wallis, you need to check normality *within each group* (low, medium, high quality). ANOVA assumes normality *of residuals*, which often translates to checking normality within each group, especially for small group sizes. The current approach is a significant flaw in the statistical methodology.
2.  **Homogeneity of Variance:** The generated code completely omits checking for homogeneity of variance (e.g., using Levene's test) before performing ANOVA. ANOVA also assumes equal variances across groups. If variances are unequal, even with normal data, Kruskal-Wallis or Welch's ANOVA (if available and preferred) would be more appropriate.
3.  **Robustness in Post-hoc Tukey:** The way Tukey HSD is set up by creating a new `tukey_df` is less efficient and slightly unconventional but works. However, it's susceptible to issues if any group is empty for a given feature (though `pairwise_tukeyhsd` might handle it, explicitly ensuring group data presence is good practice).
4.  **Bonferroni Correction Implementation:** While the code uses Bonferroni correction (`adjusted_p = p * len(list(itertools.combinations(group_names, 2)))`), it applies it to `p` directly before printing. The `statsmodels.stats.multitest.multipletests` function is a more robust and standard way to apply multiple comparison corrections, allowing for various methods beyond Bonferroni (like Holm, Sidak, etc.) and handling the case where group sizes are zero or very small which `mannwhitneyu` might struggle with. This is not a critical error but an improvement in best practice.
5.  **Output Clarity and Completeness:** The output is somewhat interwoven with the loop, making it less structured than the ground truth. A clear summary at the end, listing all significant features, their primary test, and post-hoc results (or just the significant pairs) would be more informative.
6.  **Error Handling:** There's no error handling for file not found, which is a minor but good practice for robustness.
7.  **Plot Customization:** The plots are basic. Adding titles, labels, and potentially custom palettes or figure sizes would improve readability and presentation.
8.  **Docstrings/Comments:** Lacks sufficient comments to explain complex statistical decisions or code blocks.",qwen3-30b-a3b,LLM_judge_config
H20,hard,Classification,,,Error,Failed to generate code,qwen3-30b-a3b,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf

# Load data
df = pd.read_csv('synthetic_covid19_data.csv')

# Parse date
df['date'] = pd.to_datetime(df['date'])

# Calculate vaccination percentage
df['vaccination_percentage'] = df['people_fully_vaccinated'] / df['population']

# Handle missing values
for col in ['reproduction_rate', 'stringency_index', 'vaccination_percentage']:
    df[col] = df.groupby('location')[col].transfo...",6,"The generated code attempts to address most parts of the task, but falls short in several key areas, particularly in data preprocessing, handling of random effects in `statsmodels`, and the detail/correctness of the output.

**Strengths:**
*   Correctly uses `statsmodels.formula.api.mixedlm` for mixed-effects modeling.
*   Includes the specified fixed effects and interaction term in the formula.
*   Attempts to extract and print fixed effects and random effects variance components.
*   Includes a basic discussion of model assumptions.

**Weaknesses and Areas for Improvement:**

1.  **Data Preprocessing and Imputation (Major Issue):**
    *   **`vaccination_percentage` Calculation:** The calculation `df['vaccination_percentage'] = df['people_fully_vaccinated'] / df['population']` is done *before* any imputation for `people_fully_vaccinated` or `population`. If `population` or `people_fully_vaccinated` has NaNs, `vaccination_percentage` will become NaN, and if `population` is 0, it will result in `inf` or NaNs. The ground truth first imputes these base columns and then calculates the percentage, and also caps it at 100%.
    *   **Missing Value Handling:** The imputation strategy `df[col] = df.groupby('location')[col].transform(lambda x: x.fillna(x.median()))` is too simplistic.
        *   It uses `x.median()` which calculates the *median of the group that includes NaNs*, meaning the median might be skewed or even NaN itself if the group is entirely NaN. A better approach (like in GT) would be to calculate the median *of the non-NaN values* within the group, or better yet, use interpolation for time-series data.
        *   It doesn't account for NaNs that might remain if a whole group is NaN (e.g., if a `location` has no `reproduction_rate` data). The ground truth provides a more robust multi-step imputation (interpolate, then global median fallback).
        *   It doesn't handle the crucial `population` and `people_fully_vaccinated` NaNs first, which are fundamental to `vaccination_percentage`.
    *   **Data Sorting for Interpolation:** The ground truth sorts data by `location` and `date` before imputation to ensure time-series interpolation (if chosen) is done correctly. The generated code skips this.

2.  **Random Effects Specification (Major Issue):**
    *   The task explicitly requires ""random intercepts for 'location' nested within 'continent'"".
    *   The generated code uses `re_formula='1 | continent/location'`. While this syntax *looks* like nested random effects, `statsmodels.mixedlm`'s `re_formula` parameter is actually for specifying random slopes (e.g., `re_formula='x'`). For specifying the *grouping* variable for random effects, you use the `groups` parameter.
    *   The *correct* way to achieve ""random intercepts for 'location' nested within 'continent'"" in `statsmodels.mixedlm` is to create a unique identifier for each `(continent, location)` combination and pass that to the `groups` parameter, with `re_formula=""1""`. The generated code incorrectly uses `groups=df['location']` and tries to specify nesting within `re_formula`, which `statsmodels` doesn't interpret as intended for the `groups` parameter. This results in random intercepts only for 'location', and the `/` in `re_formula` is likely misinterpreted or ignored, potentially causing errors or incorrect model specification. The GT correctly creates `continent_location_id`.

3.  **Output and Interpretation:**
    *   The task asks for interpretation of fixed effects, which is missing in the generated code's output.
    *   The task asks for specific reporting of variance components, not just the `cov_re` matrix directly. The ground truth extracts the scalar variance for the random intercept which is more interpretable.
    *   The general prompt for interpretation (significance and direction) is entirely omitted.

4.  **Error Handling/Robustness:**
    *   No `try-except` block for file loading, which is good practice.
    *   No warnings suppression for `statsmodels` convergence issues, which are common.
    *   No handling for potential remaining NaNs after imputation (e.g., `dropna` for the critical columns).

5.  **Completeness of Assumptions Discussion:** The discussion of assumptions is very brief and only provides the name and how to check, without fully explaining the assumption itself. The GT is more thorough.",qwen3-30b-a3b,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

df = pd.read_csv('clash_royale_cards.csv')
X = df[['elixirCost', 'Win Rate', 'Usage']]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

wcss = []
for k in range(2, 8):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

plt.plot(range(2, 8), wc...",7,"The generated code correctly performs the core steps required by the task: loading data, selecting features, standardizing, demonstrating the elbow method (though implicitly choosing K), applying K-Means, performing PCA, and visualizing the results.

However, there are several areas for improvement:
- **Error Handling**: The generated code does not include any error handling for `FileNotFoundError` when loading the CSV, nor does it check for the existence of required columns. This makes the code less robust.
- **K-selection justification**: While the elbow method plot is generated, the code doesn't explicitly state the chosen K and why, or how the plot helps in that decision. The ground truth provides a print statement for this.
- **Reproducibility**: `n_init` parameter is not set in `KMeans` which can lead to warnings in newer scikit-learn versions and less reproducible results.
- **Clarity and Verbosity**: The ground truth includes helpful print statements to show the intermediate steps, such as the shape of the dataset, selected features, and a summary of standardization, which significantly aids readability and understanding for a benchmark task. The ground truth also adds more descriptive titles and labels to plots, enhancing their interpretability.
- **Plot Customization**: While the basic scatter plot is produced, the ground truth's plot is more informative with explained variance percentages in the labels and a more descriptive title, fitting the expectation of an expert solution.
- **Completeness**: The task asks to ""Provide only the Python code without explanations,"" but implied in an ""expert code evaluator"" context, a good solution should still be robust and self-documenting to a reasonable extent (e.g., through robust error handling, clear variable names, and informative plot labels). The ground truth exceeds in these areas.

Overall, the generated code gets the job done but lacks the robustness, clarity, and expert-level detail of the ground truth.",qwen3-30b-a3b,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error

df = pd.read_csv('clash_royale_cards.csv')

X = df[['elixirCost', 'Usage', 'rarity']]
y = df['Win Rate']

X = pd.get_dummies(X, columns=['rarity'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor()
model.fit(X_train, y_train)

y_pred = model.pr...",8,"The generated code correctly addresses the core requirements of the task. It loads the data, selects the specified features and target, performs one-hot encoding on the 'rarity' column using `pd.get_dummies`, splits the data appropriately, trains a `RandomForestRegressor`, and evaluates it using R-squared and MSE. The imports are correct and sufficient.

**Strong points:**
- All key steps of the task are performed correctly.
- The use of `pd.get_dummies` is appropriate for one-hot encoding.
- The train-test split is done with the correct `test_size` and `random_state`.
- Evaluation metrics are calculated and printed.

**Areas for improvement to achieve a higher score:**
- **Missing `random_state` for `RandomForestRegressor`:** While not strictly required by the prompt, setting `random_state` for the model (e.g., `RandomForestRegressor(random_state=42)`) is good practice for reproducibility of the model training itself, similar to how it's used in `train_test_split`.
- **Error Handling/Robustness:** The ground truth code includes basic error handling for file not found and missing columns, and also handles potential `NaN` values, which makes it more robust for real-world scenarios. The generated code assumes the CSV exists and is clean. Although the prompt didn't explicitly ask for error handling, it's a good practice and elevates code quality.
- **`drop_first=True` for `get_dummies`:** While `pd.get_dummies` without `drop_first=True` still performs one-hot encoding correctly, adding `drop_first=True` (as in the ground truth) is a common practice to avoid multicollinearity. For this specific problem, it might not significantly impact the performance of `RandomForestRegressor`, but it's a good detail for a linear model or broader best practice.
- **Clarity of Output:** The print statements for R2 and MSE could be slightly more descriptive (e.g., ""R-squared (R2) Score:"", ""Mean Squared Error (MSE):"") for better readability, similar to the ground truth.",qwen3-30b-a3b,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"import pandas as pd
import scipy.stats as stats

df = pd.read_csv('clash_royale_cards.csv')
unique_rarities = df['rarity'].unique()
groups = [group['Win Rate'].values for name, group in df.groupby('rarity')]
f_stat, p_val = stats.f_oneway(*groups)
print(f""F-statistic: {f_stat}, p-value: {p_val}"")
if p_val < 0.05:
    print(""Reject the null hypothesis."")
else:
    print(""Fail to reject the null hypothesis."")",6,"The generated code correctly performs an ANOVA test and outputs the F-statistic and p-value, along with a basic interpretation. However, it falls short on several requirements and lacks robustness compared to the ground truth.

**Strengths:**
*   Correctly loads the dataset and uses `scipy.stats.f_oneway` for ANOVA.
*   Correctly groups data by 'rarity' for the ANOVA test.
*   Provides a basic interpretation of the p-value.

**Areas for Improvement to achieve a higher score:**
*   **Missing Hypotheses:** The task explicitly asked to ""State the null and alternative hypotheses for the test."" The generated code completely omits this.
*   **Missing Specific Output Formats:** The task requested ""Print the F-statistic and the p-value from the ANOVA test"" and for the interpretation, to ""state whether to reject the null hypothesis."" While it prints these, it does not include the significance level (`alpha`) or the more descriptive conclusions present in the ground truth.
*   **Error Handling and Data Robustness:** The ground truth includes robust error handling for `FileNotFoundError`, checks for empty dataframes after cleaning, ensures 'Win Rate' is numeric, handles `NaN` values, and checks if there are enough rarity categories or sufficient observations per group for a valid ANOVA. The generated code lacks all these crucial checks, which could lead to crashes or incorrect results with imperfect datasets.
*   **Clarity of Interpretation:** The interpretation is very brief (""Reject the null hypothesis."" or ""Fail to reject the null hypothesis.""). The ground truth provides a more complete and insightful conclusion for each case.
*   **Code Structure:** While functional, the code is very condensed. Adding comments or breaking down the steps (as in the ground truth) would improve readability and align better with the task's explicit steps (e.g., ""1) Load the dataset. 2) Identify..."").",qwen3-30b-a3b,LLM_judge_config
H25,hard,Classification,iris.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
df = pd.read_csv('iris.csv')

# Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = df['Sp...",9,"The generated code is highly correct and addresses almost all requirements of the task. It successfully loads the data, defines features and target, encodes the target, splits data, trains the specified models, calculates and prints the required metrics (accuracy, precision, recall, F1-score with macro average), and identifies the best model based on F1-score.

**Strong points:**
*   **Correctness:** All stipulated steps are correctly implemented.
*   **Clarity:** The code is well-structured and easy to understand.
*   **Completeness:** Meets nearly all functional requirements.
*   **Correct Model Parameters:** The `random_state` and `n_estimators` for Random Forest and `kernel='linear'` for SVC are correctly applied.
*   **Macro Averaging:** Correctly uses `average='macro'` for precision, recall, and F1-score.

**Area for minor improvement:**
*   **Logistic Regression `max_iter`:** The ground truth code included `max_iter=200` for Logistic Regression to prevent potential `ConvergenceWarning`. While the generated code might run without issues on this specific dataset, for robustness, it's good practice to set `max_iter` explicitly for Logistic Regression when training on larger or more complex datasets where the default might not be sufficient. However, for this dataset, it doesn't cause a critical failure.
*   **`stratify` in `train_test_split`:** The ground truth used `stratify=y_encoded` which is generally better practice for classification tasks, especially with imbalanced datasets, to ensure that the proportion of each class is roughly the same in both the training and testing sets. The generated code did not include `stratify`, which is a minor oversight but not a critical error for the iris dataset which is relatively balanced.

Overall, the generated code is excellent and fully functional for the given task.",qwen3-30b-a3b,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

df = pd.read_csv('iris.csv')
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
kmeans.fit(X_scaled)
labels = kmeans.labels_

pca = PCA(n_components=2)
X...",9,"The generated code successfully completes almost all aspects of the task. It correctly loads the data, selects and standardizes features, applies K-Means with specified parameters, performs PCA, and generates both required scatter plots.

Strengths:
- Correctly loads data.
- Correctly selects the specified features.
- Correctly standardizes features using `StandardScaler`.
- Correctly applies K-Means with `n_clusters=3`, `n_init='auto'`, and `random_state=42`.
- Correctly applies PCA to 2 components.
- Generates two scatter plots with correct data (`X_pca`) and appropriate coloring (K-Means labels and true species labels).
- Includes all necessary imports.

Areas for minor improvement:
- **Plot Customization:** The generated plots are functional but lack some of the detailed customization present in the ground truth, such as `figsize`, `alpha`, `xlabel`, `ylabel`, `colorbar`, and `grid`. While the prompt didn't explicitly ask for these specific customizations, good practice in data visualization often includes them. However, they are not critical to the core requirements of ""creating a scatter plot... coloring the points... Add a title.""
- **K-Means `fit_predict` vs `fit` then `labels_`**: The generated code uses `kmeans.fit(X_scaled)` and then `labels = kmeans.labels_`, which is functionally equivalent to `kmeans.fit_predict(X_scaled)` as used in the ground truth. Both are correct.
- **Error Handling**: The ground truth includes `try-except` for `FileNotFoundError`, which is good practice. The generated code omits this, but the prompt didn't explicitly require error handling.

Overall, the code is very good and largely fulfills the requirements. The minor points are mostly about visual aesthetics and robustness rather than core functional correctness.",qwen3-30b-a3b,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

df = pd.read_csv('iris.csv')
df['PetalArea'] = df['PetalLengthCm'] * df['PetalWidthCm']
df['SepalArea'] = df['SepalLengthCm'] * df['SepalWidthCm']

X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'PetalArea', 'SepalArea']]
y = df['Species']

le = LabelEncoder()
y = le.fit_transform(y)

m...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong points:**
*   **Correct Feature Engineering:** Accurately creates `PetalArea` and `SepalArea` and includes them in the feature set `X`.
*   **Correct Model and Parameters:** Uses `GradientBoostingClassifier` with `n_estimators=100` and `random_state=42`.
*   **Correct Cross-validation:** Performs 5-fold cross-validation using `cross_val_score`.
*   **Correct Target Encoding:** Correctly uses `LabelEncoder` for the `Species` column.
*   **Proper Imports:** Includes all necessary libraries.

**Areas for minor improvement:**
*   **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. While not explicitly asked for, it's good practice for robust code. The generated code assumes the file exists.
*   **Output Clarity:** The ground truth prints the individual fold scores and formats the mean accuracy nicely. The generated code only prints the mean, which is acceptable but less informative.
*   **Variable Names:** Using `y` directly for the encoded target (overwriting the original `y`) is common but `y_encoded` (as in ground truth) makes it slightly clearer that the target has been transformed. This is a minor stylistic point.

Overall, it's a very solid solution that successfully completes the core requirements of the task. The minor points listed are good practices rather than critical failures in functionality for this specific problem.",qwen3-30b-a3b,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

df = pd.read_csv('heart.csv')

X = df[['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG', 'MaxHR', 'ExerciseAngina', 'Ol...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly identifies `X` and `y` based on the specified columns.
*   Correctly identifies numerical and categorical features for preprocessing.
*   Uses `ColumnTransformer` with `StandardScaler` for numerical features and `OneHotEncoder` for categorical features, including `handle_unknown='ignore'`.
*   Splits data into training and testing sets with the correct ratio (80/20) and `random_state=42`.
*   Trains a `RandomForestClassifier` with specified `n_estimators` and `random_state`.
*   Evaluates the model using accuracy, macro-averaged precision, recall, F1-score, and ROC-AUC.
*   Includes all necessary imports.

**Areas for minor improvement:**
*   **Pipeline usage:** While the code's approach of `preprocessor.fit_transform(X)` before `train_test_split` then training the model without a pipeline is functional, using a `Pipeline` as shown in the ground truth is generally preferred for robustness and to prevent potential data leakage (though the current approach ensures the preprocessor is fitted on the full dataset before splitting, so true leakage wouldn't occur in the same way as fitting on `X_train` and transforming `X_test` separately without the `Pipeline`). The prompt doesn't explicitly *require* `Pipeline`, but it's a best practice in `sklearn`.
*   **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for resilience. The generated code lacks this.
*   **Stratification:** The ground truth uses `stratify=y` in `train_test_split`. While not explicitly stated in the prompt, stratifying the split by the target variable (`y`) is good practice for classification tasks, especially with imbalanced datasets, to ensure that both training and testing sets have a similar proportion of target classes. Without it, the split might randomly create sets with different class distributions, potentially affecting model training and evaluation.

Overall, the generated code accurately fulfills the core requirements and demonstrates a strong understanding of the task. The minor areas for improvement are best practices rather than critical errors for this specific problem statement.",qwen3-30b-a3b,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

df = pd.read_csv('heart.csv')
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']
X = df[features]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

k_range = range(2, 6)
wcss = []
silhouette_scores = []

for k in k_range:
    kmeans = KMeans(...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
*   **Correctness:**
    *   Loads the dataset successfully.
    *   Correctly selects and standardizes the specified numerical features.
    *   Correctly implements the elbow method (WCSS) and silhouette score calculation for K from 2 to 5.
    *   Applies K-Means clustering with `n_init='auto'` and `random_state=42`.
    *   Performs PCA to reduce dimensionality to 2 components.
    *   Visualizes the clusters using PCA components with proper coloring.
    *   Chooses `best_k` based on silhouette score, which is a valid (though not explicitly required for final selection based on 'visual inspection') method.

*   **Code Quality:**
    *   Proper imports are included.
    *   The code is well-structured and easy to follow.

*   **Completeness:**
    *   All steps outlined in the task are addressed.

**Areas for improvement (minor):**
*   **Prompting for K:** The prompt asks to ""use the elbow method (plotting Within-Cluster Sum of Squares - WCSS) and silhouette scores to justify your choice of K."" The generated code correctly plots both, but then programmatically selects `best_k` based *solely* on the maximum silhouette score. While this is a valid approach, the ground truth code explicitly states `chosen_k = 3` *after* showing both plots, implying a human decision based on visual interpretation, which the prompt hinted at (""to justify your choice""). The ground truth code's added `print` statements explaining the typical visual decision for K=3 would be helpful for clarity in a real-world scenario.
*   **Plotting:** The ground truth code combines the WCSS and Silhouette plots into a single figure with subplots, which makes comparison easier. The generated code produces two separate plots, which is acceptable but slightly less refined for presentation.
*   **Error Handling:** The ground truth code includes basic error handling for `FileNotFoundError` when loading the CSV, which is good practice. The generated code lacks this.
*   **Print Statements/Feedback:** The ground truth provides helpful `print` statements throughout to indicate progress and confirm actions (e.g., ""Dataset loaded successfully"", ""Features standardized""). The generated code is silent in this regard.",qwen3-30b-a3b,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.impute import SimpleImputer

df = pd.read_csv('winequality-red.csv')
df['target'] = df['quality'].apply(lambda x: 1 if x >= 7 else 0)
X = df.drop(['quality', 'target'], axis=1)
y = df['target']

X_train, X_te...",9,"The generated code correctly addresses all requirements of the task. It properly creates the binary target, handles missing values (though the ground truth's handling with `isnull().sum().sum()` and median for specific columns is slightly more robust and informative than `SimpleImputer` for this dataset where missing values are rare and often categorical), splits the data, trains a logistic regression model, evaluates performance, and reports feature importance.

Strong points:
- All required libraries are imported.
- Binary target creation is correct.
- Data splitting is done with an 80/20 ratio and `random_state`.
- Feature scaling using `StandardScaler` is correct.
- Logistic Regression model is trained and `max_iter` is set.
- All requested evaluation metrics (accuracy, F1-score, classification report) are calculated and printed.
- Feature importance based on `model.coef_` is correctly extracted and sorted.

Areas for minor improvement (not critical enough to significantly impact the score):
- The `SimpleImputer(strategy='mean')` is used, which is generally fine, but for numerical features (like those in this dataset), `median` can sometimes be more robust to outliers than `mean`. The Ground Truth also includes a check if there are any missing values before imputation, which is good practice for datasets where missing values might not always be present.
- The `stratify=y` argument in `train_test_split` was missing. While the data might be balanced enough here, it's good practice for binary classification to ensure the proportion of classes is maintained in both train and test sets.
- The output of feature importance is just the DataFrame. The ground truth adds more context/interpretation of what positive/negative coefficients mean, which is helpful for business users.
- Doesn't include error handling for `pd.read_csv` if the file is not found.",qwen3-30b-a3b,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

df = pd.read_csv('heart.csv')

plt.figure(figsize=(8, 6))
sns.boxplot(x='HeartDisease', y='Oldpeak', data=df)
plt.title('Distribution of Oldpeak by Heart Disease Status')
plt.xlabel('Heart Disease (0 = No, 1 = Yes)')
plt.ylabel('Oldpeak')
plt.show()

X = df[['Oldpeak']]
y = df['HeartDisease']
model = LogisticRegression()
model.fit(X, y)
coef ...",6,"The generated code correctly performs some of the required tasks but misses several key instructions and lacks robustness and detailed interpretation.

**Strengths:**
*   Correctly loads the dataset.
*   Successfully generates the box plot as requested.
*   Performs logistic regression and calculates the coefficient and odds ratio correctly.
*   Discretizes the `Oldpeak` column into three bins using `pd.qcut` as requested.
*   Calculates the proportion of `HeartDisease`=1 within each bin.

**Areas for Improvement / Missing Elements:**

1.  **File Handling and Error Checking (Completeness, Code Quality):** The ground truth includes a `try-except` block for `FileNotFoundError` and checks for missing values. The generated code assumes the file exists and is clean, which is not robust for a real-world scenario.
2.  **Box Plot Labels (Correctness/Completeness):** The box plot in the generated code lacks descriptive x-axis labels (e.g., 'No Heart Disease', 'Heart Disease') as specified by the prompt. It also misses grid lines.
3.  **Interpretation of Odds Ratio (Completeness):** The prompt explicitly asks to ""interpret its meaning in the context of odds ratios."" The generated code prints the coefficient and odds ratio but provides no textual interpretation (e.g., ""For every one-unit increase in Oldpeak, the odds of having Heart Disease are multiplied by...""). This is a significant omission.
4.  **Displaying Bin Proportions (Completeness/Clarity):** While the proportions are calculated, the output format is slightly less readable than the ground truth. The ground truth also explicitly shows the distribution of the bins themselves which is a good sanity check.
5.  **Visualization of Bin Proportions (Completeness):** The task asked to ""Calculate and display the proportion..."". While printing is a display, the ground truth includes an *optional* but very useful bar plot visualization, which is missing from the generated code and would significantly enhance the ""display"" aspect.
6.  **Readability/Comments (Code Quality):** The generated code is very concise, but could benefit from comments or more elaborate print statements to explain each step, similar to the ground truth. The ground truth is much more verbose and user-friendly in its output.
7.  **Robustness of `qcut` (Code Quality):** The ground truth includes a `try-except` block for `pd.qcut` to handle cases where there might be duplicate values preventing unique quantiles, and provides a fallback using `pd.cut`. This adds significant robustness, which is missing in the generated code.

Overall, the generated code executes the core computational steps but fails to meet the detailed requirements for output, interpretation, and robustness.",qwen3-30b-a3b,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

df = pd.read_csv('spotify_2023.csv')

# Convert 'streams' to numeric and drop NaNs
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams'])

# Create binary target variable 'is_popular'
threshold = df['streams'].quantile(0.75)
df['is_p...",9,"The generated code correctly implements all specified steps for the task. It loads the data, correctly handles the `streams` column (numeric conversion and dropping NaNs), creates the `is_popular` target variable using the 75th percentile, selects the specified features, performs one-hot encoding for `key` and `mode`, splits the data, trains a Logistic Regression model with `max_iter=1000`, and evaluates it using F1-score (macro), ROC AUC, and displays the confusion matrix. All required imports are present.

Minor points for improvement (but not critical enough to lower the current score significantly):
- The `train_test_split` does not include `stratify=y`. While not explicitly stated in the prompt, stratifying the target variable during the split is good practice for classification tasks, especially with imbalanced classes, to ensure similar class distributions in training and testing sets.
- The output for evaluation metrics is less verbose compared to the ground truth, but it still provides the required information. Adding descriptive print statements for each step (like in the ground truth) would improve readability and reproducibility verification.
- The code doesn't include error handling for file loading, which is a good practice as shown in the ground truth.",qwen3-30b-a3b,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('spotify_2023.csv')

# Select audio features and drop rows with NaNs
features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%', 'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm']
selected = df[features].dropna()

# Save original data before scaling
original_data ...",9,"The generated code successfully completes almost all aspects of the task.

**Strong Points:**
*   **Correctness:** It correctly loads the data, selects the specified audio features, handles NaNs, standardizes the data, applies K-Means with the correct parameters, performs PCA, generates the scatter plot with cluster coloring, and calculates/displays the mean values of the original features per cluster.
*   **Conciseness:** The code is quite concise and efficient.
*   **Imports:** All necessary imports are included.
*   **Parameter Usage:** `n_clusters=4`, `random_state=42`, and `n_init='auto'` for K-Means, and `n_components=2`, `random_state=42` for PCA are correctly used.

**Areas for Minor Improvement:**
*   **Error Handling for File:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. While not explicitly asked for, robust code would include this, making the generated code slightly less robust than the ground truth in a real-world scenario.
*   **Clarity of DataFrame Operations:** The ground truth stores the original (unstandardized) data in `df_original_for_analysis` *after* dropping NaNs but *before* scaling, and then meticulously adds the `cluster` column to this specific DataFrame for mean calculation. The generated code uses `selected.copy()` as `original_data` which is then used for grouping. Both approaches work because `selected` itself was the result of `dropna()`. However, the explicit `df_original_for_analysis` in the ground truth might be slightly clearer in intent (to preserve unstandardized data for *that specific purpose*). This is a very minor point.
*   **Print Statements for NaN Handling:** The ground truth includes print statements showing how many rows were dropped due to NaNs. While not a strict requirement, this adds useful debugging and informational output which is good practice for data cleaning steps.
*   **Plot Labels/Titles:** The ground truth adds more detailed labels (e.g., 'Principal Component 1' vs. 'PC1', 'Principal Component 2' vs. 'PC2') and a slightly more descriptive title for the plot. While the generated code's labels are acceptable, more descriptive ones are generally better.

Overall, the generated code is highly functional and correctly addresses the core requirements of the task. The few missing elements are primarily related to robustness, verbosity, and detailed output/plotting, which are good practices but not explicit requirements.",qwen3-30b-a3b,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('spotify_2023.csv')

df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams'])
df = df[(df['streams'] > 10_000_000) & (df['released_year'].between(2010, 2023))]

grouped = df.groupby('released_year')[['danceability_%', 'energy_%', 'valence_%']].mean()

for feature in ['danceability_%', 'energy_%', 'valence_%']:
    plt.figure()
    plt.plot(grouped.index, grouped[feature])
    p...",8,"The generated code correctly addresses the core requirements of the task: loading data, converting streams, filtering, grouping, calculating yearly averages, and plotting.

**Strong Points:**
*   **Correctness:** All calculations and filtering conditions are applied correctly.
*   **Conciseness:** The filtering steps are combined efficiently.
*   **Clarity:** The code is easy to understand.
*   **Plotting:** It generates individual plots for each feature, which is acceptable per the prompt, though the ground truth provided a multi-subplot figure.

**Areas for Improvement:**
*   **Plotting Presentation:** While functional, the plotting could be improved for better presentation.
    *   Using `plt.subplots` to create a single figure with multiple subplots (as in the ground truth) would be more organized and allow for easier comparison between trends, rather than showing three separate modal windows or plots that require closing/navigating.
    *   Adding `plt.tight_layout()` would prevent potential overlaps in titles/labels if using subplots.
    *   Including markers on the plot lines (e.g., `marker='o'`) can enhance readability.
    *   Setting `plt.xticks` to rotate labels might be helpful if there are many years.
*   **Error Handling:** The ground truth includes `try-except` for file loading, which is good practice for robust code. The generated code lacks this.
*   **Intermediate Output/Debugging Notes:** The ground truth includes print statements to show intermediate `df.shape` or `isnull().sum()`, which are helpful for understanding data transformation and debugging. These are not strictly part of the task's output but are valuable in a real-world scenario.
*   **No Exit on Empty Data:** If the filtering results in an empty DataFrame, the code would proceed to `groupby` and plotting, potentially leading to errors or empty plots without a clear message. The ground truth checks for this.

Overall, it's a very good attempt that fulfills the primary requirements. The improvements would primarily focus on robustness and presentation for a more professional and user-friendly solution.",qwen3-30b-a3b,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, r2_score

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean and convert numeric columns
for col in ['Likes', 'Dislikes', 'Video views']:
    df[col] = df[col].str.replace(',', '').astype(float).fillna(0)

# Process 'published' as numeric year
df['published'] = pd.to_datetime(df['published'], error...",9,"The generated code is almost entirely correct and addresses most requirements successfully.

Strong points:
- Correctly loads the dataset.
- Accurately cleans `Likes`, `Dislikes`, and `Video views` by removing commas, converting to numeric, and filling NaNs with 0. The use of `.astype(float).fillna(0)` then implicitly converts to numeric (float), which is fine before `LinearRegression` as it handles float types.
- Correctly treats the `published` column by extracting the year, handling potential errors, and filling NaNs with 0.
- Correctly fills missing `Category` values with 'Unknown' and applies one-hot encoding using `pd.get_dummies`.
- Correctly defines `X` using the specified features (`Dislikes`, `Video views`, `published` year, and OHE `Category`) and `y` as `Likes`.
- Correctly splits the data 80/20 with `random_state=42`.
- Correctly trains a Linear Regression model.
- Correctly evaluates and reports R-squared and MAE.
- Includes all necessary imports.

Areas for minor improvement (preventing a perfect 10):
- The `youtube_trendings.csv` file loading doesn't include error handling for `FileNotFoundError`. While the prompt didn't explicitly ask for it, robust code usually includes this. The ground truth includes it, which is good practice.
- The `category_dummies` are joined to `X` without explicitly indicating a prefix or managing potential column name collisions if 'Unknown' or other categories have column names that clash with existing features. `pd.get_dummies` by default uses the column name as a prefix, but then joining needs to be careful if there are multiple columns with similar names. For this specific dataset and problem, it's unlikely to be an issue, but conceptually the `X.join(category_dummies)` might be less robust than `pd.concat`. The ground truth uses `pd.concat` with explicit prefixes which is generally safer for complex feature sets. In this context, `pd.get_dummies(df['Category'])` without a prefix might create a column `Unknown` if 'Unknown' is a category, but it's unlikely to conflict with `Dislikes`, `Video views`, or `published`.

Overall, the code is highly functional and accurate based on the prompt's requirements.",qwen3-30b-a3b,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('youtube_trendings.csv')

df['Video views'] = df['Video views'].str.replace(',', '').astype(float)
df['Video views'] = df['Video views'].fillna(0)

df['published'] = df['published'].astype(int)

grouped = df.groupby('published')

avg_views = grouped['Video views'].mean()
unique_videos = grouped['Video'].nunique()

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))

ax1.plot(avg_views.index, avg_views.values, marke...",6,"The generated code correctly performs most of the required tasks:
1.  **Loads the dataset**: Yes, `pd.read_csv('youtube_trendings.csv')`.
2.  **Cleans `Video views`**: Yes, it removes commas, converts to float, and fills NaNs with 0.
3.  **Treats `published` year as numeric**: Yes, it attempts to convert it to int. This is where a potential issue lies (see below).
4.  **Groups by `published` year**: Yes.
5.  **Calculates metrics**: Correctly calculates mean `Video views` and unique `Video` titles.
6.  **Creates subplots**: Yes, two line plots as required.
7.  **Titles and labels**: Includes appropriate titles and axis labels.
8.  **Imports**: Correctly imports pandas and matplotlib.pyplot.

However, there are a few areas for improvement and a critical oversight based on the typical `youtube_trendings.csv` dataset, which usually has a `publishedAt` column (a timestamp string) rather than a simple `published` integer column representing just the year.

**Specific areas for improvement:**

*   **Handling `published` column**: The prompt specified ""ensure the `published` column (year) is treated as a numeric or integer type."" The ground truth code assumes `publishedAt` and extracts the year from it. The generated code assumes a column named `published` already exists and attempts `df['published'].astype(int)`. If the actual dataset uses `publishedAt` (a common format for YouTube data), this line would fail because `publishedAt` is a string (datetime) and cannot be directly converted to `int` without first extracting the year. This is a significant assumption/error given the context of a typical YouTube dataset. If the dataset *did* have a pre-extracted `published` year column, this part would be correct, but it's an important distinction often found in the `youtube_trendings.csv` filename.
*   **Robustness**: The generated code doesn't include any error handling for file not found or issues during data type conversion (e.g., `pd.to_numeric` with `errors='coerce'` as seen in the ground truth is more robust). While not explicitly asked for, good practice in an expert scenario would involve it.
*   **Implicit vs. Explicit Column Naming**: The prompt stated ""Ensure the `published` column (year) is treated as a numeric or integer type."" The ground truth correctly infers this as extracting the year from `publishedAt`. The generated code assumes a column already named `published` which stores only the year. This distinction is subtle but important for real-world datasets.

The major deduction is for the `published` column assumption which is likely to cause a runtime error with standard `youtube_trendings.csv` files. If it worked in a specific test environment where such a column pre-existed, the score would be higher. Assuming typical datasets, this is a flaw.",qwen3-30b-a3b,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"import pandas as pd
import re
from collections import Counter
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

df = pd.read_csv('youtube_trendings.csv')

df['title_word_count'] = df['Video'].str.split().str.len()

df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)

correlation = df['title_word_count'].corr(df['Video views'], method='pearson')
print(f""Pearson correlation: {correlation}"")

df['published'] = pd.to_datetime(...",8,"The generated code successfully completes most of the tasks.

**Strengths:**
- Correctly loads the data.
- Accurately calculates `title_word_count`.
- Cleans the 'Video views' column effectively by removing commas, converting to numeric, and filling NaNs.
- Correctly calculates and prints the Pearson correlation.
- Filters the dataset for the specified years (2023-2024).
- Processes titles by converting to lowercase, removing punctuation, and filtering stop words for the common word analysis.
- Correctly finds and prints the 5 most common words.
- Includes all necessary imports.

**Areas for Improvement:**
- **Robustness in correlation output:** The ground truth includes a descriptive print statement for the correlation and formatting (`.4f`), which is good practice for user understanding. The generated code just prints ""Pearson correlation: {correlation}"".
- **Error Handling:** The ground truth includes `try-except` blocks for file loading and `nltk` download, making it more robust. The generated code assumes the file exists and the NLTK download will always succeed.
- **`title_word_count` and `Video views` type handling:** The ground truth explicitly uses `.astype(str)` for `Video` and `Video views` before string operations, which makes the code more robust against potential non-string entries in those columns. While `.str.split()` and `.str.replace()` will often handle this by coercing to string implicitly, explicit conversion is safer.
- **Word cleaning (Numbers/Empty Strings):** The ground truth's word processing step `word.isalpha()` in `[word for word in title.split() if word not in stop_words and word.isalpha()]` provides a slightly cleaner list of words by excluding numbers or empty strings that might result from `re.sub` and `split()`. The generated code doesn't explicitly exclude non-alphabetic words from its `words_list`.
- **Output clarity for common words:** The ground truth formats the output for common words with more detail (e.g., ""'word': X occurrences""), which is clearer than just ""word: count"".

Overall, it's a solid solution that addresses the core requirements. The improvements mentioned mainly pertain to robustness, clarity, and minor edge cases in data cleaning, which are often characteristics of ""expert"" level code.",qwen3-30b-a3b,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score

# Load data
df = pd.read_csv('mobiles_2025.csv')

# Clean 'Launched Price (USA)'
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '', regex=False).str.replace(',', '', regex=False)
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'], erro...",6,"The generated code addresses most of the tasks but has several issues that prevent it from being a strong solution.

**Correctness Issues:**
1.  **Target Creation (`is_expensive_usa`)**: The generated code sets `is_expensive_usa` using `(df['Launched Price (USA)'] > 1000).astype(int)`. This implicitly treats `NaN` values in `Launched Price (USA)` after cleaning as `False` (0) before the `dropna`. The requirement was specifically to drop rows where `is_expensive_usa` is NaN, which means *only* NaN *after* the `> 1000` condition (implying the original price was unparseable). The ground truth correctly handles this by creating `0` when price is not NaN and `<=1000`, `1` when price is not NaN and `>1000`, and `NaN` *only* if the original `Launched Price (USA)` was indeed not parseable. The generated code effectively converts such parsing NaNs to 0s, which is not what was requested.
2.  **`Launched Year` as Feature**: The task explicitly asks for `Launched Year` to be included in `X`. The generated code one-hot encodes `Company Name` and `Processor`, then drops those *original* columns. After that, it simply defines `X` as `df.drop(columns=['is_expensive_usa'])`. This implicitly includes `Launched Year` if it exists in the DataFrame and is not dropped, which is fine, but it doesn't explicitly guarantee its inclusion or handle potential issues if it were to be dropped or renamed (though in this case it works because `Launched Year` is not subject to custom cleaning functions or OHE).
3.  **`Front Camera` and `Back Camera` Cleaning**: The `extract_mp` function in the generated code uses `re.search(r'(\d+)', text)` which *only* extracts integer values. The ground truth `re.search(r'(\d+(\.\d+)?)\s*MP', ...)` is much more robust, specifically looking for MP values (e.g., ""12.5 MP"") including decimals, and defaults to 0 if ""MP"" isn't found. The generated code's `extract_mp` would just find the first number in the string, which might not be an MP value, and wouldn't handle decimals.
4.  **Dropping NaNs in Features**: The code does `df = df.dropna()`. This is too broad. It will drop rows for *any* NaN across the entire DataFrame, including potentially in columns not specified as features or before all feature cleaning and definition of `X` and `y` are finalized. The requirement was to ""Drop rows with any NaNs in features or target"" *after* all transformations, which the ground truth does correctly by concatenating `X` and `y` and then dropping NaNs. This might lead to different datasets and thus different model performance.
5.  **`dropna` order**: The generated code does `df = df.dropna(subset=['is_expensive_usa'])` early, but this can lead to issues if the 'Launched Price (USA)' cleaning produced NaNs that then propagated to `is_expensive_usa` and were then dropped. The ground truth carefully manages this, dropping NaNs *only* for the target specifically, and then handling feature NaNs later.

**Code Quality/Completeness Issues:**
1.  **Missing Error Handling for CSV**: The generated code does not include `try-except` for `pd.read_csv`, which is good practice and was shown in the ground truth.
2.  **Verbose Cleaning**: The `clean_numeric_column` function is good, but it's called multiple times explicitly. The ground truth opted for separate, clear lines for each cleaning, which makes it easier to trace specific column transformations.
3.  **No `stratify` in `train_test_split`**: The ground truth correctly uses `stratify=y` to maintain the class distribution in train and test sets, which is crucial for imbalanced classification tasks. The generated code omits this.
4.  **No `n_jobs` in RandomForestClassifier**: The ground truth uses `n_jobs=-1` to speed up training, which is a good practical optimization.
5.  **Lack of Intermediate `print` statements**: The `print` statements in the ground truth help in tracing the execution and understanding data shape changes, which is beneficial for debugging and understanding the pipeline.

Fixing these issues, particularly the target creation, camera cleaning, and NaN handling, would significantly improve the solution.",qwen3-30b-a3b,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"import pandas as pd
import numpy as np

def clean_price(value):
    if pd.isna(value):
        return np.nan
    for symbol in ['PKR', 'INR', 'CNY', 'USD', 'AED']:
        value = value.replace(symbol, '')
    value = value.replace(',', '')
    value = value.strip()
    try:
        return float(value)
    except:
        return np.nan

df = pd.read_csv('mobiles_2025.csv')

price_columns = [
    'Launched Price (Pakistan)',
    'Launched Price (India)',
    'Launched Price (China)',
    'Launche...",7,"The generated code correctly addresses most of the task requirements. It successfully loads the dataset, cleans the specified price columns, calculates the average global price, and identifies the top 5 models.

However, there are a few areas for improvement:
1.  **Robustness of `clean_price` function**: The `clean_price` function assumes the input `value` is already a string when `value.replace()` is called. If the column contains non-string types (e.g., numbers, or mixed types), `replace` might fail. The ground truth explicitly converts `value` to `str(price_str)` before processing.
2.  **Handling of `skipna` in `mean()`**: The generated code calculates the `mean` without explicitly stating `skipna=True`. While `pd.DataFrame.mean` defaults to `skipna=True`, making it explicit improves readability and ensures correctness even if the default behavior were to change or if `np.nanmean` (which also defaults to `skipna=True`) were used. The problem statement explicitly mentioned: ""This average should only consider countries for which price data is available (not NaN) for that specific model,"" which `skipna=True` directly addresses.
3.  **Error Handling for File Not Found**: The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code lacks this.
4.  **Assumed Column Existence**: The generated code directly applies the `clean_price` function to columns without checking if they exist (`if col in df.columns:`). While unlikely for this specific problem, in real-world scenarios, this can lead to KeyErrors if a column name is misspelled or missing.
5.  **Display Format**: While the top 5 models are correctly identified, the instruction asked to ""Display the `Model Name` and their calculated `Average Global Price`...sorted by `Average Global Price` descending. Include proper imports..."" The ground truth uses `print(""Top 5 Mobile Phone Models by Average Global Price:"")\nprint(top_5_models_display)` which is a more explicit and user-friendly display. The generated code just `print(top_models)` which is correct but less structured. It also lacks a clear print statement explaining the output.

Overall, it's a good attempt that gets the core logic right, but could benefit from more robust error handling and explicit adherence to best practices for data cleaning and manipulation.",qwen3-30b-a3b,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('mobiles_2025.csv')

df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''), errors='coerce')

df_clean = df.dropna(subset=['RAM', 'Battery Capacity', 'Launched Price (USA)'])

apple_df = df_...",8,"The generated code correctly addresses most aspects of the task. It loads the dataset, cleans the specified columns (`RAM`, `Battery Capacity`, `Launched Price (USA)`), handles conversion errors by coercing to NaN and then dropping rows, filters for Apple products, and groups by `Launched Year`. It then generates box plots for each year using subplots, with appropriate titles and y-axis labels.

**Strong points:**
*   **Correct Cleaning:** The cleaning steps for `RAM`, `Battery Capacity`, and `Launched Price (USA)` are mostly correct, including handling `NaN` values via `errors='coerce'` and then `dropna`.
*   **Filtering and Grouping:** Correctly filters for 'Apple' and then groups by `Launched Year`.
*   **Box Plots:** Generates box plots for the required metrics per year.
*   **Subplots and Titles/Labels:** Uses subplots and sets appropriate titles and y-axis labels for each plot.
*   **Conciseness:** The code is quite concise while still being readable.

**Areas for improvement:**
*   **Robust Cleaning (minor):** While `str.replace` works for the given strings, the ground truth explicitly adds `.str.strip()` to remove leading/trailing whitespace which is good practice. Also, `RAM` and `Battery Capacity` could benefit from `astype(str)` before `str.replace` to prevent errors if the column contains non-string types. Although `pd.to_numeric` would handle some of this, making the string operations robust is better.
*   **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` which is good practice for robust code, though not explicitly asked for in the prompt's requirements for *this* benchmark.
*   **Missing 'Launched Year' Cleaning:** The `Launched Year` column should also be cleaned to ensure it's numeric and possibly an integer, and rows with NaN in this column should be dropped (as seen in the ground truth). The current code implicitly assumes `Launched Year` is clean enough for `groupby`.
*   **Plotting Method:** Using `group['RAM'].plot(kind='box', ax=ax1)` is functional, but `seaborn.boxplot` often provides a more aesthetically pleasing and statistically informative default plot, and was implicitly suggested by the prompt (""Use subplots for clarity (e.g., one row of three subplots, or three separate figures)""). However, the prompt also says ""matplotlib.pyplot OR seaborn"", so `df.plot(kind='box')` is acceptable. The ground truth leverages `seaborn` which is a common practice for data science visualization.
*   **Redundancy in `df_clean`:** The `dropna` assignment to `df_clean` creates a copy unnecessarily. The original `df` could have been modified in-place: `df.dropna(subset=['RAM', ...], inplace=True)`. This is a minor efficiency point.

Overall, it's a solid solution that meets the core requirements of the prompt effectively. The points for improvement are mostly about robustness, best practices, and minor aesthetic choices, rather than critical functional failures.",qwen3-30b-a3b,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold, learning_curve, validation_curve, cross_val_score, cross_val_predict, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.pipeline import Pipeline
from sklear...",5,"The generated code attempts to address many of the requirements but falls short in several critical areas, particularly concerning advanced preprocessing, feature engineering implementation, and the comprehensive nature of the evaluation and interpretability sections.

**Critical Issues and Missing Components:**

1.  **Imputation Strategy:**
    *   **Ground Truth (GT):** Implements `IterativeImputer` (Multiple Imputation by Chained Equations) for numerical features, which is an advanced dynamic imputation strategy.
    *   **Generated:** Uses `SimpleImputer(strategy='median')` for numerical features and `SimpleImputer(strategy='most_frequent')` for categorical in the preprocessing pipeline. This is a basic imputation, not ""multiple imputation strategies"" as requested. Also, initially, `df['Age'].fillna(df['Age'].median())` and `df['Embarked'].fillna(df['Embarked'].mode()[0])` are performed directly on the DataFrame *before* the pipeline, which implies `SimpleImputer` in the pipeline might be redundant for these specific columns or less effective than intended for generalizing.

2.  **Advanced Feature Engineering:**
    *   **GT:** Implements `FeatureEngineerTransformer` (a custom transformer) for Name (Title extraction, standardization, Is_Married, Name_Length), Cabin (Deck extraction), FamilySize, IsAlone, and Fare_Per_Person. Crucially, GT uses `PolynomialFeatures` *within* the numerical pipeline.
    *   **Generated:** Performs Title, FamilySize, IsAlone, and FarePerPerson directly on the `df` *before* defining `X` and `y` and setting up the pipeline. This means these features are static and not part of a reproducible, pipeline-controlled preprocessing step. `PolynomialFeatures` is imported but never used in the pipeline. Text mining from names is rudimentary (simple regex for title, no ""sentiment"" or more complex extraction). `df['Cabin'].fillna('Unknown')` is done, but no actual ""Deck"" extraction.

3.  **Feature Selection:**
    *   **GT:** Implements `SelectKBest`, `RFE`, and `SelectFromModel`, applies them to the *processed* data, and then demonstrates their impact by training a model on the reduced feature sets.
    *   **Generated:** Imports `SelectKBest`, `f_classif`, `RFE` but does not use them at all. This requirement is completely missed.

4.  **Hyperparameter Tuning:**
    *   **GT:** Uses `RandomizedSearchCV` for all models with well-defined, somewhat extensive parameter grids. It wraps the model within a pipeline (preprocessor + classifier), ensuring the search space is correctly applied.
    *   **Generated:** Uses `RandomizedSearchCV` but the search space for some models is quite limited (`n_iter=5`). More importantly, `X_preprocessed` is created by fitting the `preprocessor` on `X` *before* the `RandomizedSearchCV`. This implies features are preprocessed once for the entire dataset (or `X` and `y` are the original), not dynamically within the pipeline for each fold, which is incorrect for a robust setup where preprocessing parameters might depend on the training fold. The GT defines a full pipeline including the preprocessor *within* the `RandomizedSearchCV` to ensure proper cross-validation.

5.  **Ensemble Models:**
    *   **GT:** Stacking uses `final_estimator=LogisticRegression`, Voting uses `voting='soft'` and all best-tuned base models.
    *   **Generated:** Stacking also uses `LogisticRegression` but for Voting and Stacking classifiers, only `rf`, `gb`, `xgb` are used as estimators. The prompt asks for combining ""the best models,"" implying all best-performing base models should be considered. `MLPClassifier` and `SVC` are excluded from the ensembles.

6.  **Extensive Model Evaluation:**
    *   **GT:** Performs Stratified K-Fold Cross-Validation for *all* base and ensemble models using `cross_val_score`, plots CV results (boxplot), plots Learning Curves (for Stacking), plots Validation Curves (for RandomForest), and performs Statistical Significance Tests (Paired t-test) to compare models.
    *   **Generated:** Performs `cross_val_score` for all models but does not plot the overall CV results. Learning curve and validation curve functions are defined but not called for any specific model/param in the main script body. Statistical significance tests are present but the implementation appears to be attempting to compare all pairs rather than comparing all models against a designated best one, which is more typical.

7.  **Model Interpretability (SHAP/LIME):**
    *   **GT:** Implements SHAP (`TreeExplainer` for XGBoost on *processed* data, summary plots, and a single instance explanation) and LIME (`LimeTabularExplainer` for StackingClassifier, requiring a proper prediction function integrating the full pipeline).
    *   **Generated:** Imports SHAP. Uses `shap.DeepExplainer` with `XGBClassifier` and `X_preprocessed`. `DeepExplainer` is generally for deep learning models, while `TreeExplainer` is more appropriate and efficient for tree-based models like XGBoost. More critically, LIME is imported but not used at all.

8.  **Code Structure and Reproducibility:**
    *   The direct modification of `df` (`df['Age'].fillna`, `df['Title'] = ...`, `df.drop(...)`, `pd.get_dummies`) before `X` and `y` are defined makes the pipeline less robust. The GT code uses a custom transformer and a `ColumnTransformer` to put *all* preprocessing within a controlled pipeline, which is the best practice for production and reproducibility.

**Areas for Improvement (to achieve a higher score):**

*   **Integrate all preprocessing steps within the `Pipeline` and `ColumnTransformer`**. This includes feature engineering (`Title`, `FamilySize`, `IsAlone`, `FarePerPerson`, `Deck` and `PolynomialFeatures`) and imputation.
*   **Implement PolynomialFeatures** as requested.
*   **Implement `IterativeImputer`** for advanced missing value handling.
*   **Execute Feature Selection steps** (SelectKBest, RFE, SelectFromModel) and demonstrate their impact by training and evaluating models with selected features.
*   **Call the `plot_learning_curve` and `plot_validation_curve` functions** for appropriate models/parameters.
*   **Implement LIME** to satisfy the model interpretability requirement completely.
*   Ensure that the `RandomizedSearchCV` is done on a pipeline that includes the preprocessor to avoid data leakage during hyperparameter tuning.",qwen3-30b-a3b,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Define features and target
X = df[['Year', 'Runtime']]
y = df['Rating']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Ma...",7,"The generated code correctly addresses the core requirements of loading data, defining features and target, splitting data, training a Linear Regression model, and evaluating it using R-squared and MAE. It includes all necessary imports and follows a logical flow.

However, it lacks essential data preprocessing steps that would make it robust in a real-world scenario. Specifically, it doesn't handle potential issues with the 'Runtime' column (e.g., 'X min' format) or missing/non-numeric values in 'Year', 'Runtime', or 'Rating'. The ground truth code includes crucial data cleaning and error handling (like checking for file existence and column presence), which are important for production-ready code. Without these, the code might fail or produce incorrect results if the CSV data isn't perfectly clean.

To achieve a higher score, the generated code should incorporate robust data preprocessing, especially for the 'Runtime' column, and handle potential missing values.",qwen3-30b-a3b,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

df = pd.read_csv('imdb_top_250.csv')
features = df[['Rating', 'Runtime']]
original = features.copy()

scaler = StandardScaler()
scaled = scaler.fit_transform(original)

kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
kmeans.fit(scaled)
labels = kmeans.labels_

plt.scatter(original['Runtime'], original['Rating'], c=labels, cmap='viridis')
plt.title...",6,"The generated code correctly loads the data, selects features, standardizes them, applies K-Means with the specified parameters, and generates a scatter plot. It also calculates and prints the mean rating and runtime per cluster.

However, there are several areas for improvement:

1.  **Robustness/Data Cleaning:** The ground truth includes essential data cleaning steps for 'Runtime' and 'Rating' columns (handling ' min' in Runtime, coercing to numeric, and dropping NaNs). This is crucial for real-world data and makes the K-Means applicable. The generated code assumes perfect numeric data, which might not be the case for `imdb_top_250.csv`.
2.  **Scatter Plot Labels:** The scatter plot in the generated code lacks a proper color bar and detailed color differentiation, making it harder to interpret which color corresponds to which cluster label compared to the ground truth which uses a `colorbar`.
3.  **Use of `fit_predict` vs. `fit` and `labels_`:** While the generated code's approach (`kmeans.fit(scaled)` followed by `labels = kmeans.labels_`) is functionally correct, directly using `kmeans.fit_predict(scaled_features)` is often more concise when you only need the labels for the fitted data, as shown in the ground truth.
4.  **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` and missing columns. The generated code does not.
5.  **Direct Feature Assignment for Clustering:** The ground truth adds the `cluster_label` directly back to the DataFrame (`df['cluster_label'] = kmeans.fit_predict(scaled_features)`). This is a good practice as it keeps the cluster assignment linked to the original data, simplifying subsequent analysis (like groupby for cluster means). The generated code calculates `labels` separately and then uses `cluster_data = original[labels == i]` to iterate, which is slightly less efficient/clean for calculations.
6.  **Minor Plotting Details:** The ground truth includes `figsize` and `grid` for better plot presentation.",qwen3-30b-a3b,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter

nltk.download('punkt')
nltk.download('stopwords')

df = pd.read_csv('imdb_top_250.csv')

df['title_length'] = df['Title'].str.len()

correlation = df['title_length'].corr(df['Rating'], method='pearson')
print(""Pearson correlation:"", correlation)

processed_titles = []
for title in df['Title']:
    text = title.lower()
    text = re.sub(r'[^a-z0-9\s]...",6,"The generated code correctly performs text preprocessing, stop word removal, and finds the most common words, which are significant parts of the task. However, there are several shortcomings that prevent a higher score.

**Positive Aspects:**
*   Correctly loads the dataset.
*   Correctly creates the `title_length` feature.
*   Performs tokenization and stop word removal effectively.
*   Correctly uses `collections.Counter` to find the most common words.
*   Handles NLTK downloads correctly.

**Areas for Improvement/Deficiencies:**
1.  **Correlation Calculation (Major Issue):** The prompt explicitly asks for ""Pearson correlation coefficient between `title_length` and the `Rating` column"". The generated code calculates `df['title_length'].corr(df['Rating'], method='pearson')` without ensuring the `Rating` column is numeric or handling potential non-numeric values (e.g., 'Not Rated' or 'N/A' strings which are common in real-world datasets). The Ground Truth explicitly converts `Rating` to numeric and handles errors, and then drops NaNs for accurate correlation calculation. This is a critical omitted step for robust data analysis.
2.  **Missing Output Requirement:** The correlation printing format `Pearson correlation between title_length and Rating: {correlation:.4f}` was specifically requested, but the generated code just prints `Pearson correlation:`.
3.  **Efficiency of Preprocessing:** The preprocessing loop iterates row by row and repeatedly initializes `stop_words = set(stopwords.words('english'))` inside the loop. This is inefficient; `stop_words` should be defined once outside the loop.
4.  **Pandas Vectorization vs. Loop:** While the row-by-row iteration for preprocessing technically works, it's not idiomatic Pandas. The Ground Truth demonstrates a more efficient and cleaner way to apply transformations on DataFrame columns using `.apply()` with vectorized operations (e.g., `df['Title'].str.lower()`, `df['processed_title'].apply(word_tokenize)`), which is generally preferred for performance and readability in Pandas.
5.  **Output Format for Common Words:** The prompt requested printing ""frequencies"" and the format for common words: `'{word}': {count}`. The generated code prints `word: freq`, which is close but not exact.

The most critical missing piece is the robust handling of the `Rating` column for correlation calculation as it directly impacts the correctness of a required output.",qwen3-30b-a3b,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

df = pd.read_csv('penguins.csv')

X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']]
y = df['species']

numeric_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm...",7,"The generated code correctly addresses most of the requirements but has a critical flaw in handling the `island` imputation and subsequent dropping of rows.

**Strengths:**
*   **Correct Imports and Structure:** Proper imports are used, and the code follows a logical flow from data loading to model evaluation.
*   **Correct Feature and Target Definition:** `X` and `y` are correctly defined.
*   **Correct Numerical Imputation:** `SimpleImputer(strategy='median')` is correctly used for numerical features.
*   **Correct Categorical Imputation:** `SimpleImputer(strategy='most_frequent')` is correctly used for categorical features.
*   **Correct One-Hot Encoding:** `OneHotEncoder` is used correctly.
*   **Correct Scaling:** `StandardScaler` is used correctly for numerical features.
*   **Correct Label Encoding:** `LabelEncoder` is used for the target variable.
*   **Correct Model Training and Evaluation:** `RandomForestClassifier` and `cross_val_score` are used as specified.

**Weaknesses and Areas for Improvement:**
*   **Incorrect Handling of `island` NA Dropping:** The requirement states: ""if a row still has NA in `island` after mode imputation, e.g. if all were NA, it can be dropped, or a placeholder like 'Unknown' can be used if mode is NA itself. For this task, if mode imputation results in NA for island, drop those rows"".
    *   The generated code creates `X_cat` from `X[categorical_cols]` *before* `np.hstack`. This means `X_cat` is an array that still has NAs if the original `island` column had NAs.
    *   `X_combined = pd.DataFrame(np.hstack([X_num, X_cat]), columns=numeric_cols + categorical_cols)`: When `np.hstack` is performed, if `X_cat` contained `NaN`s, these `NaN`s (which are float `NaN`s) are coerced into string 'nan' if the column becomes an object type, or they might remain `NaN`s if the array is numeric like for `X_num`. In the case of categorical strings, `SimpleImputer` would fill them correctly with the mode *in `X_cat`*.
    *   However, the `X_combined = X_combined.dropna(subset=['island'])` line *after* imputation and combination is problematic. If `SimpleImputer` filled the `island` NAs, then this `dropna` would *not* drop any rows related to `island` NAs. Furthermore, the mode imputation performed on `X_cat` using `SimpleImputer(strategy='most_frequent')` on the *original* dataframe's `island` column *before* stacking should have filled the NAs. The subsequent `dropna` on `island` is redundant if the imputation was successful, or incorrect if it was meant to catch cases where the *mode itself was NA* (which `SimpleImputer` handles by returning the mode, even if there was only one unique non-NA value) or if the column was entirely NA (which `SimpleImputer` would typically return `np.nan` if the column was numeric, but the behavior for fully NA string columns can be tricky without `fill_value`).
    *   The `SimpleImputer` for `most_frequent` correctly handles the case where the input column itself is all `NaN`. In that case, `most_frequent` would return `NaN`. If the *mode itself* is `NaN`, `SimpleImputer` will impute `NaN` values with `NaN`. The ground truth explicitly handles this edge case for `island` where the entire column might be NA. The generated code `SimpleImputer(strategy='most_frequent')` would likely impute `NaN` for fully NA `island` columns, and then `X_combined = X_combined.dropna(subset=['island']) ` would correctly drop them. However, it's less explicit and might miss nuances for other scenarios.

*   **Handling `y` alignment after dropping rows:**
    *   `X_combined = X_combined.dropna(subset=['island'])` will drop rows *only from `X_combined`*.
    *   `y = y[X_combined.index]` attempts to align `y` but `y` is still the original `y` that might contain NA for species (although not explicitly required to check for species NA, it's good practice as in GT). If `X_combined.index` loses rows compared to `y`'s original index, care must be taken to ensure `y` corresponds to the correct `X` rows. This is better handled by preserving the `DataFrame` structure or using `df.dropna` carefully. The ground truth addresses this by adjusting `y` using `y = y.loc[X.index]` after `X.dropna`.

*   **Clarity on `np.hstack` and `pd.DataFrame`:** While technically correct, transforming back and forth between NumPy arrays and Pandas DataFrames, especially when numeric and categorical columns have different data types, can introduce potential issues with data type handling or column order if not meticulously managed. The ground truth keeps operations more within the Pandas ecosystem where possible, which often makes column tracking easier.

The primary reason for a score of 7 is the less robust handling of the `island` NA dropping requirement, and the potential for `y` and `X` misalignment subsequent to dropping rows. While it might pass for a typical dataset, the explicit ""if mode imputation results in NA for island, drop those rows"" implies a more robust check needed for the edge case of all NAs in 'island', which the ground truth handles explicitly.",qwen3-30b-a3b,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

df = pd.read_csv('penguins.csv')
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df_clean = df.dropna(subset=features).copy()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_clean[features])

kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
kmeans.f...",8,"The generated code correctly performs most of the required steps: loading data, selecting features, handling NAs, standardizing, K-Means clustering, PCA, and generating two scatter plots.

However, there are a few areas for improvement:
1.  **Plotting**: While the plots are generated, they lack the visual polish and clarity that `seaborn` (used in the ground truth) provides, specifically in terms of `hue` mapping for categorical labels and proper legend display. The generated code uses `c` for coloring which maps directly to numerical values and doesn't automatically generate a legend for categories. For the `species` plot, converting to `cat.codes` is a workaround but loses the semantic labels in the legend.
2.  **Robustness (File Not Found)**: The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice for robust code. The generated code assumes the file exists.
3.  **Readability/Structure**: The ground truth explicitly states which task each section of code addresses with comments, which improves readability and maintainability.

Overall, the core functionality is present and correct, but the presentation/plotting aspects could be enhanced, and some minor robustness/readability improvements could be made.",qwen3-30b-a3b,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

df = pd.read_csv('penguins.csv')

numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
imputer_num = SimpleImputer(strategy='mean')
df[numerical_cols] = imputer_num.fit_transform(df[numerical_cols])

imputer_sex ...",7,"The generated code correctly addresses most of the requirements but has a few areas for improvement, particularly regarding the plotting and the handling of `SimpleImputer` output.

**Strong Points:**
*   **Correct Data Loading:** It loads the `penguins.csv` file appropriately.
*   **Correct Imputation Strategies:** It uses `SimpleImputer(strategy='mean')` for numerical columns and `SimpleImputer(strategy='most_frequent')` for 'sex', which aligns with the requirements. It also correctly drops NA rows for 'island'.
*   **Correct Feature Engineering:** It creates `X` by combining numerical features and one-hot encoded `island` and `sex`.
*   **Correct Target Encoding:** It uses `LabelEncoder` for the 'species' target.
*   **Correct Model Training:** It trains a `GradientBoostingClassifier` with the specified parameters.
*   **Correct Feature Importance Extraction and Printing:** It extracts and prints feature importances associated with their respective names, sorted descending, meeting the requirement.

**Areas for Improvement:**
*   **`SimpleImputer` Output Type and `pd.concat`:** When `SimpleImputer` is used on a DataFrame (e.g., `df[numerical_cols] = imputer_num.fit_transform(df[numerical_cols])`), `fit_transform` returns a NumPy array. While this usually works for subsequent numerical operations, it can drop column names if not handled carefully, potentially leading to issues or less readable code in some contexts. The ground truth avoided this by using `fillna` directly on the DataFrame columns. For the numerical columns, the column names are implicitly carried forward by virtue of `df[numerical_cols]` being assigned, but it's a subtle point.
*   **Plotting Method:** The plotting section uses `sns.FacetGrid` and `g.map(sns.boxplot, ""sex"", ""flipper_length_mm"")`. While this does produce box plots per species, it groups them by species as columns rather than grouping `flipper_length_mm` by `sex` *within* each species plot on a single figure, as implied by the ""For each `species`, create box plots to visualize the distribution of `flipper_length_mm` (using imputed data) grouped by the imputed `sex` column."" requirement and demonstrated by the ground truth's `sns.boxplot(data=df_processed, x='species', y='flipper_length_mm', hue='sex')`. The ground truth's approach provides a clearer direct comparison. The generated code also sets titles using `{col_name}` which works, but the overall presentation is less direct for the specific visualization request. It also explicitly said ""Ensure plots have titles"", and while `set_titles` works, it doesn't give a grand title to the entire visualization like `plt.title` does.
*   **Clarity and Readability:** The ground truth breaks down the process more explicitly with comments, which is beneficial for understanding. The generated code is more concise but could benefit from more structured variable naming (e.g., `model` vs. `gb_classifier`) and intermediate steps.
*   **Robustness (`FileNotFoundError`):** The ground truth includes a `try-except` block for `FileNotFoundError` which is good practice for data loading. The generated code does not.",qwen3-30b-a3b,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer

# Load the dataset
df = pd.read_csv('penguins.csv')

# Handle numerical columns using KNN Imputation
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
X_num = df[numerical_cols]
imputer = KNNImputer(n_neighbors=3)
X_imputed = imputer.fit_transform(X_num)
df_imputed_num = pd.DataFrame(X_imputed, columns=numerical_cols, index=df.index)
df.update(df_imputed_num)

# Handle categorical colu...",9,"The generated code successfully completes the task. It loads the data, correctly applies KNN imputation to the specified numerical columns using `n_neighbors=3`, and imputes NA values in the 'sex' column with its mode. Finally, it prints the remaining NA values, which should be zero for the imputed columns, verifying the operations.

**Strong Points:**
*   **Correctness:** All imputation methods are applied as specified and correctly implemented.
*   **Clarity:** The code is well-structured and easy to understand.
*   **Efficiency of KNN Imputation:** The use of `df.update(df_imputed_num)` is an efficient way to merge the imputed numerical data back into the original DataFrame without reassigning the whole DataFrame slices which could lead to `SettingWithCopyWarning` in more complex scenarios.

**Areas for Minor Improvement:**
*   **Error Handling (File Not Found):** The generated code doesn't include error handling for `FileNotFoundError` if 'penguins.csv' is not present, which the ground truth gracefully handles. This is a common practice in robust data scripts.
*   **Verification Output:** While `df.isna().sum()` provides the overall NA count, explicitly mentioning ""Number of remaining NA values..."" as done in the ground truth makes the output clearer for verification.
*   **Comments:** A few more comments describing each step would further enhance readability, similar to the ground truth.
*   **Redundancy check for mode imputation:** The ground truth checks if `df[categorical_col_for_mode].mode()` is empty before accessing `[0]`. While unlikely for the `sex` column in a typical dataset, it's a good defensive programming practice if the column could hypothetically be entirely NA.

Overall, the generated code provides a very good and correct solution to the problem. The minor points are about robustness and output clarity rather than fundamental correctness.",qwen3-30b-a3b,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
import scipy.stats as stats

penguins = pd.read_csv('penguins.csv')
filtered = penguins[penguins['species'].isin(['Adelie', 'Gentoo'])].dropna(subset=['culmen_length_mm'])
adelie = filtered[filtered['species'] == 'Adelie']['culmen_length_mm']
gentoo = filtered[filtered['species'] == 'Gentoo']['culmen_length_mm']
t_stat, p_value = stats.ttest_ind(adelie, gentoo)
print(f't-statistic: {t_stat}, p-value: {p_value}')
print(""Reject the null hypothesis."" if p_value < 0.05 ...",8,"The generated code correctly performs all the required steps: loading the data, filtering species, handling NAs (though the chaining order is slightly less robust than the ground truth in terms of debugging), performing the t-test, and printing the results with interpretation.

**Strong points:**
*   **Concise and correct logic:** It gets straight to the point and accurately implements the statistical test and data manipulation steps.
*   **Correct output:** It prints the t-statistic, p-value, and the conclusion about the null hypothesis as required.
*   **Effective use of pandas:** The filtering and `dropna` chaining is efficient.

**Areas for improvement (to reach a 9 or 10):**
*   **Error Handling:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth. This makes it less robust for real-world scenarios.
*   **Clarity and Readability:** While concise, the ground truth's use of a function, comments, and intermediate print statements for data shapes and NA handling steps enhances readability and debugging, especially for someone unfamiliar with the dataset or task.
*   **Docstrings/Comments:** The generated code has no comments or docstrings explaining the purpose of the code or specific steps, unlike the ground truth.
*   **Intermediate Checks:** The ground truth includes checks for `adelie_culmen.empty` or `gentoo_culmen.empty` after filtering and NA handling, which is a good practice to prevent errors if one of the groups ends up with no data.
*   **Explicit `equal_var`:** While `ttest_ind` defaults to `True` for `equal_var`, explicitly setting it (as in the ground truth) clarifies the assumption being made for the t-test, which is important for statistical rigor.
*   **Formatting of output:** While it prints the correct values, the ground truth format is slightly more detailed and user-friendly (e.g., f-strings for precision, separate lines for different parts of the output).

Overall, it's a very good attempt that fulfills the core requirements, but it could benefit from more robust error handling and improved readability/explainability for a production-ready or collaborative environment.",qwen3-30b-a3b,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load the dataset
df = pd.read_csv('penguins.csv')

# Define features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper...",7,"The generated code correctly addresses most of the requirements but has a significant issue in the `ColumnTransformer` for categorical features, specifically how it chains `ColumnTransformer` within another `ColumnTransformer` and handles the `sex` and `island` encoding separately instead of a single `Pipeline`.

**Correctness:**
- Data loading: Correct.
- Feature/Target definition: Correct.
- Preprocessing:
    - Numerical imputation (median): Correctly implemented using `SimpleImputer` in a `Pipeline`.
    - `sex` imputation (mode) and One-Hot Encoding: This is where the main issue lies. The `categorical_transformer` is itself a `ColumnTransformer` which contains pipelines for `sex` and `island`. While `island` gets a simple `OneHotEncoder`, `sex` gets an imputer *and* an encoder. The issue here is conceptual and practical: `ColumnTransformer` expects a list of (name, transformer, columns) tuples. If `categorical_transformer` is applied to `categorical_cols` which contains *both* `island` and `sex`, the inner `ColumnTransformer` should be configured differently. As written, `categorical_transformer` is set up to handle `sex` and `island` independently within itself, but then `preprocessor` tries to apply this single `categorical_transformer` to `categorical_cols` which contains both. This creates a mishandling of how `island` and `sex` are picked up by the inner and outer transformers, leading to incorrect transformation or errors when running. The `ground truth` correctly uses a `Pipeline` for categorical features (impute then encode) and then applies this single `pipeline` to both `island` and `sex` via the `ColumnTransformer`.
    - `species` label encoding: Correctly applied to `y_train` and `y_test` separately, but the `ground truth` applies it before the split to ensure consistency across the entire target, which is generally safer. However, for a simple classification task, this approach might still work functionally if all classes are present in both splits.
- Data splitting: Correct, including stratification and `random_state`.
- Model training: Correct.
- Prediction: Correct.
- Evaluation: Correct, using `accuracy_score` and `classification_report`.
- Missing `class_names` for `classification_report`: The generated code misses passing `target_names=le.classes_` to `classification_report`, which makes the report less readable. The `ground truth` handles this correctly by capturing the class names from the label encoder.

**Code Quality:**
- Imports: All necessary imports are present.
- Structure: Clear and follows a logical flow (load, define, split, preprocess, model, evaluate).
- Readability: Generally good, but the nested `ColumnTransformer` for categorical features adds unnecessary complexity and introduces the bug.

**Completeness:**
- The code attempts to meet all requirements. The primary shortfall is the incorrect setup of the categorical preprocessor which makes it not fully complete the task as intended (i.e., applying the specified imputation for sex and OHE for both island and sex in a robust way). The missing `target_names` in the classification report also makes the output less complete than desired.

**Areas for Improvement:**
1.  **Categorical Preprocessing Refinement:**
    *   The `categorical_transformer` should be a `Pipeline` that first imputes with `most_frequent` (for *both* `island` and `sex` if they both could have missing values, or specific rules if needed) and then uses `OneHotEncoder`.
    *   Revised `categorical_transformer` structure:

        ```python
        categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')), # Applies to both island and sex
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ])

        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numerical_transformer, numerical_cols),
                ('cat', categorical_transformer, categorical_cols) # This applies the combined pipeline to both columns
            ])
        ```
    *   The current separate handling of `sex` imputation and `island` encoding within a `ColumnTransformer` that is then applied to `categorical_cols` is flawed for handling multiple columns that share preprocessing needs.

2.  **`classification_report` Readability:** Pass `target_names=le.classes_` to `classification_report` for clearer output, similar to the ground truth. This requires getting `le.classes_` after `le.fit_transform`.

3.  **Label Encoding Order:** While functional, applying `LabelEncoder` to `y_train` and `y_test` separately can introduce edge cases if `y_test` contains classes not seen in `y_train` (though `stratify` helps mitigate this). The ground truth's approach of encoding the full `y` before splitting is generally more robust for consistent label mapping.",qwen3-30b-a3b,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

corr_years = df['PerformanceScore'].corr(df['YearsExperie...",9,"The generated code correctly defines the DataFrame, calculates the specified Pearson correlation coefficients, prints them, and correctly identifies the feature with the stronger absolute correlation using `abs()`. This fulfills all the requirements of the task.

**Strong points:**
*   Correct DataFrame creation.
*   Accurate calculation of Pearson correlation coefficients using the `.corr()` method.
*   Correct printing of the correlation values.
*   Correct logic for comparing the absolute correlations and identifying the stronger one.

**Areas for minor improvement (to reach a perfect 10):**
*   **Clarity in Printing:** The generated code prints only the raw correlation values. The ground truth adds descriptive text using f-strings (e.g., ""Pearson correlation between PerformanceScore and YearsExperience: ..."") which makes the output much more readable and informative.
*   **Precision in Output:** The ground truth formats the correlation values to 4 decimal places (`:.4f`), which is good practice for numerical results like correlations.
*   **Handling Equal Absolute Correlations:** The `else` clause in the generated code's conditional statement assumes that if `abs(corr_years)` is not greater than `abs(corr_training)`, then `abs(corr_training)` *must* be greater than `abs(corr_years)`. It doesn't explicitly handle the edge case where the absolute correlations are exactly equal. While statistically rare, a robust solution would include an `elif abs(corr_training) > abs(corr_years)` and an `else` for equality, as shown in the ground truth.
*   **Specificity in `corr` method:** While `pearson` is the default for `.corr()`, explicitly specifying `method='pearson'` as in the ground truth can improve code readability and remove ambiguity, especially if other correlation methods might be considered in a broader context.

Overall, it's a very good solution that successfully achieves the core objectives. The suggested improvements are minor and relate more to output formatting, robustness for edge cases, and explicit parameter usage.",qwen3-30b-a3b,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test
from sklearn.linear_model import LogisticRegression
from sklearn.utils import resample
import statsmodels.api as sm
import scipy.stats as stats
import pymc3 as pm
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score

# Load and...",4,"The generated code attempts to address many of the prompt's requirements but falls short significantly in correctness, completeness, and statistical rigor, particularly in the interpretation and application of time-to-event data.

1.  **Fundamental Misinterpretation of Time-to-Event (Kaplan-Meier & CoxPH):** The most critical flaw is using `Age` as the `duration_col` for both Kaplan-Meier and CoxPH models, and `Survived` as the `event_col`. This is fundamentally incorrect for the Titanic dataset given the task's stated definition: ""voyage as time-to-event data (with survival as event indicator)."" In a Titanic context, `Age` is a covariate, not the duration of the voyage. The duration for all passengers on the Titanic's maiden voyage is essentially a fixed, short period (the duration of the voyage until the sinking or rescue). The event (`E`) is dying/not dying on *that specific voyage*. The Ground Truth correctly assigns a constant `T=1` and `E = 1 - Survived` to reflect this. The generated code's use of `Age` as duration makes the survival analysis statistically meaningless in the context of the Titanic voyage itself. `lifelines` expects `duration_col` to represent the time until an event or censoring, and `event_col` to indicate if the event occurred. Using `Age` here implies that people are being followed *from birth* until they either reach their current age (if survived) or die (if not survived), which is not the survival analysis relevant to the Titanic disaster.

2.  **Kaplan-Meier Specifics:**
    *   The prompt asked for KM analysis ""treating the voyage as time-to-event data."" The generated code plots `Age` as the time axis, which goes against this specific instruction and the nature of the Titanic problem.
    *   It doesn't segment the KM curve by meaningful factors like Sex or Pclass, which is a common and important part of exploratory survival analysis.

3.  **Cox Proportional Hazards Model:**
    *   Again, the `duration_col='Age'` is a major conceptual error.
    *   The model assumes `Age` has a direct hazard relationship whereas it's likely a covariate.
    *   The formula `formula = 'Survived ~ Age + Sex + Fare + Pclass_2 + Pclass_3 + Age*Sex'` is an attempt at interaction effects, but the `fit` method of `CoxPHFitter` doesn't directly take a formula like that within the `fit` arguments for typical Python formula parsing; it expects the dataframe to already contain transformed columns if needed. `lifelines` does have a `formula` argument but it's typically used in the `CoxPHFitter` constructor or for more complex model definitions, not directly in this `fit` call for simple interaction. The Ground Truth correctly creates the interaction term `Age_x_Sex` as a new column in the dataframe.

4.  **Propensity Score Matching:**
    *   The treatment definition `y = df['Pclass_1']` is problematic. `Pclass_1` is a one-hot encoded column (0 or 1). This is a treatment variable. However, `df[['Age', 'Fare', 'Sex', 'Pclass_2', 'Pclass_3']]` are used as covariates `X`. This setup defines the propensity score for *being in Pclass 1* given other factors, which is okay, but it doesn't clearly articulate the desired ""treatment effect (e.g., passenger class impact)"" as a comparison between two specific classes, like `Pclass 1` vs `Pclass 3` as the Ground Truth does.
    *   Crucially, it calculates propensity scores but **does not perform any matching** (e.g., k-nearest neighbors). It just stops after computing the scores. This is a critical omission for propensity score matching.
    *   It doesn't analyze the treatment effect or check balance after matching (because no matching is done).

5.  **Bootstrap Resampling:**
    *   The function `bootstrap_cox_model` attempts bootstrapping, but `model.params_.values` might directly return the raw coefficients. It computes a 95% CI from these, which is good.
    *   However, the conceptual error of using `Age` as `duration_col` persists, making the bootstrapped CIs statistically flawed in context.

6.  **Bayesian Analysis with PyMC3:**
    *   The Bayesian model uses a `Weibull survival likelihood`, which is completely inappropriate for the Titanic voyage context where the ""duration"" is fixed and the event is binary (survived/died). This implies durations are continuous and follow a Weibull distribution, which is not the case. A Bayesian Logistic Regression for the `Survived` outcome would be more appropriate (as in the Ground Truth).
    *   The linear predictor `linear_pred = pm.math.dot(df, beta)` directly multiplies `df` (the entire dataframe) with `beta` (a vector of coefficients). This is highly unorthodox and likely incorrect as `df` contains a mix of features and the target, and `beta` should correspond to features *only*. The Ground Truth correctly separates `X_bayesian` (features) and `y_bayesian` (target).
    *   The `observed` variable `pm.DensityDist('observed', ...)` attempts to define likelihood but the `Weibull.logp` is designed for continuous time-to-event data where `value` is the time, not a discrete event, and `linear_pred` affects the rate/scale, not directly added to logp in this manner. This part of the code is highly problematic and likely results in a model that doesn't sample correctly or provides nonsensical results. The prompt specifically mentioned ""probabilistic modeling of survival factors,"" which would imply a standard logistic regression for the binary outcome, not a continuous survival model.
    *   It samples but doesn't interpret `pm.summary` or `az.plot_trace/posterior` in a meaningful way beyond printing the summary.

7.  **Advanced Statistical Tests and Model Diagnostics:**
    *   `cph.check_assumptions(df)` is called, but with `Age` as `duration_col`, the test results would be misleading in the context of the prompt.
    *   Multivariate analysis and interaction effects (beyond the basic Cox model) are mentioned but not fully developed.
    *   No general statistical tests like `ttest_ind` or `chi2_contingency` (as in Ground Truth for PSM balance) are performed outside of survival analysis.

8.  **Model Validation and Sensitivity Analysis:**
    *   The sensitivity analysis attempts to vary an `Age Threshold` and define a new `AgeGroup` variable. While it's a form of sensitivity, it's not the requested ""validation of model assumptions"" or ""sensitivity analysis"" in the broader sense for survival models or imputation strategies/prior choices as outlined in the prompt and performed in the Ground Truth. It applies a fixed model structure with a new feature, rather than examining robustness to initial data processing choices or model assumptions.
    *   No imputation strategy sensitivity, or Bayesian prior sensitivity (as in Ground Truth) is discussed.

**Code Quality:**
*   Imports are present but some necessary ones (like `SimpleImputer`, `StandardScaler`, `NearestNeighbors` for PSM, `arviz` etc. used in GT) are missing.
*   Data preprocessing is minimal, missing `FamilySize` and `IsAlone` feature engineering.
*   Uses `LabelEncoder` for `Sex` which is fine but `map` is also common.
*   Code structure is okay, but the statistical misinterpretations override any structural benefits.

In summary, the generated code demonstrates a general understanding of the tools (`lifelines`, `pymc3`) but critically misapplies them to the given problem statement, especially regarding the 'time-to-event' definition and the nature of the Titanic dataset. The Bayesian section is particularly flawed. It misses key analytical steps for PSM and comprehensive sensitivity analysis.",qwen3-30b-a3b,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 7...",9,"The generated code is largely correct and addresses most of the task requirements.

**Strong Points:**
- Correctly initializes the DataFrame.
- Accurately calculates and prints the Pearson correlation coefficient.
- Generates a scatter plot with the correct axes, coloring by subject, and includes a title and legend.
- Correctly uses `pd.cut` to assign `Overall_Grade` based on `Final_Score`. This is a very concise and idiomatic pandas way to handle binning.
- Correctly prints the count of students in each grade category.

**Areas for Minor Improvement:**
- **Plot Customization:** The plot is functional but lacks some of the aesthetic enhancements and readability improvements present in the ground truth, such as explicit figure sizing (`plt.figure(figsize=...)`), grid (`plt.grid`), marker size (`s`), alpha, and `tight_layout()`. While not strictly required by the prompt, these improve plot quality significantly.
- **Grade Binning Precision:** The `pd.cut` bins `[0, 70, 80, 90, 100]` with `labels=['D', 'C', 'B', 'A']` implies a `left=False` behavior (exclusive on the left, inclusive on the right by default for `cut`). This means a score of 70 would fall into 'C', 80 into 'B', and 90 into 'A'. The prompt specifies ""C if 70 <= Final_Score < 80"", ""B if 80 <= Final_Score < 90"", ""A if Final_Score >= 90"". The default `right=True` behavior of `pd.cut` means `(70, 80]` for C, `(80, 90]` for B, `(90, 100]` for A. This is a subtle difference from the exact inclusive/exclusive ranges specified in the prompt for `C` and `B` and the ""A if Final_Score >= 90"". For instance, a score of exactly 90 would be 'A' in the ground truth, but would be 'B' in the generated code because `pd.cut` with `right=True` lumps 90 into the `(80, 90]` bin (B). To perfectly match the ground truth's logic, `np.select` is more precise for discrete ranges. However, `pd.cut` is a valid approach if the binning logic is adjusted (e.g., `bins=[69.9, 79.9, 89.9, 100]`) or by setting `right=False` and adjusting bin boundaries/labels. For example, `bins=[-1, 69.9, 79.9, 89.9, 100]` and `labels=['D', 'C', 'B', 'A']` with `right=False` would achieve it more accurately, or adjusting the `labels` with `right=True` and slightly modified bins.
- **Clarity of Output:** The ground truth added print statements for each step, which improves the readability of the output when the code is run. While not mandatory, it makes the code's execution flow clearer.

Overall, a very good attempt that demonstrates strong understanding of pandas and matplotlib/seaborn. The minor discrepancies are subtle logical ones or minor aesthetic choices.",qwen3-30b-a3b,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']),
    'Ticker': ['TECHCO']*10,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, ...",10,"The generated code is excellent. It correctly implements all the required steps:
1.  **DataFrame Creation and Indexing**: The DataFrame is created correctly, and 'Date' is set as the index as specified.
2.  **Daily Percentage Return**: The formula for `Daily_Return` is accurate, using `pct_change()` effectively.
3.  **Simple Moving Averages**: Both the 3-day and 5-day SMAs are calculated correctly using the `rolling().mean()` method.
4.  **Plot Generation**: The plot includes the 'Close' price, 'SMA_3Day', and 'SMA_5Day' on the same chart. It has a correct title, legend, and labeled axes. The use of `plt.xticks(rotation=45)` and `plt.tight_layout()` enhances readability.
5.  **Bullish Crossover Identification**: The logic for identifying bullish crossovers (`(df['SMA_3Day'] > df['SMA_5Day']) & (df['SMA_3Day'].shift(1) < df['SMA_5Day'].shift(1))`) is precise and correctly captures the condition where the shorter SMA crosses *above* the longer SMA from below. The dates are then correctly printed.

The code is clean, well-structured, and directly addresses all parts of the prompt with correct and efficient pandas and matplotlib operations.",qwen3-30b-a3b,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

data = {'OrderID': [101, 102, 103, 104, 105, 106, 107], 'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 'Quantity': [1, 2, 1, 1, 3, 1, 1], 'Price_USD': [1200, 25, 75, 300, 50, 25, 1150], 'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST001']}
df...",9,"The generated code correctly addresses all parts of the task:
1.  **DataFrame Creation**: Successfully creates the DataFrame as specified.
2.  **Total Sales Revenue by Product**: Correctly calculates the total sales revenue per product and prepares it for visualization.
3.  **Horizontal Bar Chart**: Generates a correct horizontal bar chart with appropriate labels and title.
4.  **Average Order Value**: Accurately calculates the average order value using total revenue and unique order IDs.
5.  **Repeat Customers**: Correctly identifies customers with more than one purchase and displays their IDs and purchase counts.

**Strong Points**:
*   All calculations are correct.
*   The plot is generated and is accurate.
*   The code is concise and efficient.
*   Uses `pandas` and `matplotlib` effectively.

**Areas for Improvement (minor)**:
*   **Plot aesthetics**: The generated plot is functional but could benefit from minor aesthetic improvements for better readability, such as `plt.tight_layout()` for adjusting plot parameters for a tight layout, adding gridlines, or setting a figure size, similar to the ground truth.
*   **Output Formatting**: The print statements for average order value and repeat customers are functional but could be slightly more descriptive or formatted (e.g., using f-strings for currency formatting or clearer headers for lists) to match the polish of the ground truth.
*   **Comments**: While the code is relatively straightforward, adding comments for each section (e.g., ""Calculate total sales revenue"", ""Generate bar chart"") would improve readability for more complex tasks, though it's less critical for this specific task.

Overall, it's a very good solution that fully satisfies the task requirements. The minor improvements are mostly about presentation and robustness rather than core functionality.",qwen3-30b-a3b,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd
import numpy as np

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

df['DeviceType_Numeric'] = df['DeviceType'].map({'Desktop': 0, 'Mobile': 1})
df['PagesPerMinute'] = np.where...",9,"The generated code correctly defines the DataFrame, creates the `DeviceType_Numeric` feature, and selects the correct final features. It also correctly handles the division by zero for `PagesPerMinute` using `np.where`, which is an efficient way to achieve the desired outcome. The use of `np.where` directly addresses the potential division by zero for *all* cases (0/0, x/0 for x>0 or x<0) by setting the result to 0 when the denominator is 0.

One minor point that could be considered for an even higher score (though not explicitly problematic for the given data) is that if `PagesViewed` were also 0 when `SessionDurationMinutes` was 0, `np.where` handles this by setting `PagesPerMinute` to 0. The ground truth code's approach of `replace([np.inf, -np.inf], np.nan).fillna(0)` is robust but slightly more verbose to address the `0/0` case which results in `NaN`, and `x/0` cases which result in `inf`. Both approaches effectively handle the problem as specified.

The code is concise and efficient. It imports necessary libraries and follows a logical flow for data preparation.",qwen3-30b-a3b,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26, 45, 28]
}
df = pd.DataFrame(data)
...",9,"The generated code correctly defines the DataFrame, calculates the mean and standard deviation for temperature, identifies outliers using the specified 1.5 standard deviation threshold, and prints the required details for each outlier. The approach of using the absolute Z-score for outlier detection `abs((df['TemperatureCelsius'] - mean_temp) / std_temp) > 1.5` is mathematically equivalent and equally valid to checking against upper and lower bounds. The code is concise and directly addresses all parts of the prompt.

One minor area for improvement, though not strictly required by the prompt's output format, would be to convert the 'Timestamp' column to datetime objects using `pd.to_datetime(df['Timestamp'])`. While the current string format works for printing, converting it to datetime is good practice for time-series data and might lead to more robust solutions in broader contexts (e.g., if date/time operations were needed). The current code does not format the timestamp in the output for consistency with the ground truth, but this specific formatting wasn't a strict requirement in the prompt.",qwen3-30b-a3b,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [10, 5, 25, 15, 8, 30, 9,...",9,"The generated code correctly performs all the required tasks. It accurately defines the DataFrame, calculates the `EngagementScore`, groups by Platform and PostType to find the average EngagementScore, and then correctly identifies the `PostType` with the highest average `EngagementScore` for each `Platform`. The final print statement is also correct and clearly formatted.

The only minor area for improvement that could bring it to a perfect 10 is the formatting of the `Highest Average` printout, which does not specify decimal places as the ground truth did, but this is a minor stylistic choice and doesn't affect correctness. The use of `idxmax()` is an efficient and Pythonic way to achieve the desired result, which is commendable. Additionally, while the ground truth added comments for clarity, the generated code is self-explanatory enough for this task.",qwen3-30b-a3b,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu"", ""Injury"", ...",9,"The generated code correctly defines the DataFrame, calculates 'LengthOfStay', computes the average length of stay and patient count per diagnosis, and identifies the diagnosis with the longest average stay. All functional requirements from the task are met.

**Strong points:**
- Correctly uses `pd.to_datetime` for date columns.
- Accurately calculates `LengthOfStay` using `dt.days`.
- Correctly uses `groupby().agg()` to calculate both average `LengthOfStay` and `PatientCount`.
- Correctly identifies the diagnosis with the longest average `LengthOfStay` using `idxmax()`.
- The output format for the grouped data is clear.

**Areas for minor improvement:**
- The prompt asked to ""print this information (Diagnosis, Average LengthOfStay, Patient Count)"" which implies a clear, formatted output for each diagnosis. While `print(grouped.to_string(index=False))` does print the table, it lacks the specific formatting (e.g., ""days"" suffix, specific decimal precision if needed) seen in the ground truth's `for` loop print statements, which might be considered slightly less explicit in presenting the required ""information"". However, it's a valid and concise way to display the data.
- The ground truth includes `print` statements to indicate the ""steps"" being performed, which is good for readability and debugging, but not strictly a functional requirement for the task itself. The generated code is more concise by omitting these.

Overall, the generated code produces the correct results and is highly efficient and readable. The minor difference in output formatting for the summary table is negligible.",qwen3-30b-a3b,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",9,"The generated code correctly performs all the required steps. It accurately defines the DataFrame, calculates the ""WeightedRating"" using the specified formula, adds it as a new column, and correctly identifies and prints the title and weighted rating of the movie with the highest weighted rating.

The code is concise and efficient, leveraging pandas' vectorized operations for calculating the weighted rating.

The only minor area for improvement (and why it's not a perfect 10) is the lack of explicit error handling for `sum_votes` being zero, although given the specific dataset where `sum_votes` will never be zero, this is not a critical flaw. Also, the output format for the final print statement was `""{max_row['Title']} {max_row['WeightedRating']}""` which is slightly different from the ground truth's more descriptive output and formatting, but it still fulfills the requirement of printing the ""Title"" and ""WeightedRating"". This is a very minor aesthetic point.",qwen3-30b-a3b,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 10000, 8000]
}
df = pd...",9,"The generated code correctly defines the DataFrame, calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"", and presents the result in a new DataFrame, fulfilling all task requirements. The use of `.groupby().mean().reset_index()` is concise and efficient. The only minor difference from the ground truth is the column naming in the final grouped DataFrame; the generated code keeps the original column names (""PriceUSD"", ""Mileage"") while the ground truth specifically renames them to ""Average_PriceUSD"" and ""Average_Mileage"" for clarity, which was demonstrated using `agg()`. This is a stylistic choice and both are functionally correct for the task's stated requirements. The ground truth also includes an extra print statement for clarity, which is a good practice, but not strictly a functional requirement.",qwen3-30b-a3b,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63, 83],
    ""De...",10,"The generated code is entirely correct and complete. It accurately defines the DataFrame, calculates the ""TotalStats"" column, identifies and prints the name of the Pokemon with the highest ""TotalStats"", and calculates and prints the average ""Speed"" for each ""Type1"". The code is concise and directly addresses all requirements of the task. No improvements are necessary for functionality or adherence to the prompt.",qwen3-30b-a3b,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, GaussianMixture
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from umap import UMAP
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from scipy.cluster import hierarchy
from scipy.stats import chi...",6,"The generated code attempts to address most of the requirements but falls short in several areas, particularly concerning the depth and correctness of implementation for advanced features, comprehensive validation, and visualization.

**Pros:**
*   Includes most required libraries.
*   Performs some basic feature engineering (FamilySize, IsAlone, Title, FarePerPerson).
*   Applies multiple clustering algorithms (K-Means, Agglomerative, GMM).
*   Performs dimensionality reduction (PCA, t-SNE, UMAP).
*   Attempts cluster analysis and external validation.
*   Includes some visualizations.

**Cons/Areas for Improvement:**

1.  **Feature Engineering (Missing Advanced Aspects):**
    *   **Social Network Analysis:** The ground truth includes `TicketGroupSize` and a more robust `FarePerPerson` based on ticket groups, which is a stronger indicator than `FamilySize`. The generated code's `TicketShare` is similar but not identical in intent or robustness. The ground truth also considers `NameLength`, `TicketPrefix`, `CabinDeck`, and `HasCabin`, along with interaction features like `Age_Class`, `Fare_Class`, `FarePerPerson_Class`, which are completely missing in the generated code. The generated code also `LabelEncodes` `Title` directly without intelligent grouping of rare titles, which can be problematic for clustering.
    *   **Text-based Features:** Completely misses `NameLength` and `TicketPrefix` as described in the ground truth.

2.  **Preprocessing & Scaling (Incomplete):**
    *   The generated code uses `LabelEncoder` for 'Sex', 'Embarked', and 'Pclass' *before* `StandardScaler`. `LabelEncoder` imposes an arbitrary ordinality that is inappropriate for nominal categorical features for distance-based clustering algorithms. One-Hot Encoding (as in the ground truth) is necessary. The ground truth uses `ColumnTransformer` and `Pipeline` for a more robust and correct preprocessing flow, which is a standard and best practice. The generated code manually encodes and scales.
    *   Missing imputation strategy for `Fare` (ground truth handles this).

3.  **Clustering Algorithms (Partial):**
    *   DBSCAN is included in the imports but not actually used in the clustering analysis or visualization loop where other algorithms (`KMeans`, `Agglomerative`, `GMM`) are applied.
    *   The `GaussianMixture` implementation in the generated code does not explicitly retrieve labels (it returns `best_model`, which is the GMM fitted object). While GMM supports `predict`, the `cluster_analysis` function's structure isn't tailored to GMM's probabilistic nature as well as it is for centroid-based models.

4.  **Optimal Number of Clusters / Validation Metrics (Severely Lacking):**
    *   The generated code *only* uses Silhouette Score to determine the ""best"" number of clusters and doesn't plot the scores or examine other metrics like Elbow Method (Inertia), Calinski-Harabasz, or Davies-Bouldin. The task explicitly asks for ""multiple validation metrics"" and a thorough determination of optimal clusters.
    *   There is no ""gap statistic"" implementation as requested.

5.  **Comprehensive Visualizations (Incomplete and Misleading):**
    *   The visualization loop `for i, (labels, name, reduced) in enumerate(zip([kmeans_labels, agglo_labels, gmm_labels], [""KMeans"", ""Agglomerative"", ""GMM""], [pca, tsne, umap])):` is problematic. It incorrectly assumes that `pca`, `tsne`, and `umap` should be used with *all* cluster labels (KMeans, Agglomerative, GMM). The intent should be to show *each* clustering result on *each* dimensionality reduction. The current code shows KMeans on PCA's reduction, Agglomerative on t-SNE's, and GMM on UMAP's, which is not a comprehensive comparison.
    *   The `sns.histplot(df['Survived'], ax=axes[i, 1])` part is static and redundant; it plots the *overall* survival distribution in every row, not a survival distribution *per cluster* or segmented by the current clustering. This is a significant misinterpretation of the visualization requirement ""showing cluster distributions, survival rates"".
    *   Missing key visualizations requested: survival rates per cluster, feature importance/distribution for each segment (e.g., violin plots for Age, Fare, count plots for Pclass, Sex), and heatmap of scaled feature means.

6.  **Cluster Stability Analysis (Insufficient):**
    *   The implementation uses `KMeans(n_clusters=3, random_state=42)` for stability analysis, hardcoding `n_clusters=3` without explanation or linkage to the `optimal_k` derived from validation metrics.
    *   The output is a `plt.imshow(consensus...)` which is visually a matrix, but it doesn't calculate or report actual stability metrics like Adjusted Rand Index (ARI) or Jaccard, which are standard for this task. It just shows the raw cluster assignments across runs, which is hard to interpret quantitatively.

7.  **Consensus Clustering (Missing):** The task explicitly asks for ""consensus clustering"" which is not implemented at all.

8.  **External Validation (Basic but Lacks Depth):**
    *   The `chi2_contingency` is a good start, but the ground truth `survival_by_cluster` analysis provides a more direct and interpretable view of survival rates within each cluster, which is ultimately what ""external criteria and survival outcomes"" implies for this dataset. The visualization of survival rates per cluster is also missing from the external validation section of the generated code.

Overall, while the code structure is clean and uses appropriate libraries, the depth of implementation for advanced concepts like comprehensive feature engineering, rigorous cluster validation, and detailed cluster analysis/visualization is limited. Many requirements are only superficially met or completely missed.",qwen3-30b-a3b,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dystopian""],
    ""Stock"": [12, 8, 15,...",9,"The generated code correctly performs all the required tasks: defining the DataFrame, counting books and calculating average price per author, and identifying the author with the most books in stock. The use of `groupby().agg()` is efficient and appropriate for the aggregations.

Areas for minor improvement:
1.  **Output Formatting for Author Statistics**: While `print(author_stats)` correctly displays the information, the ground truth's use of a loop and f-strings (`print(f""Author: {row['Author']:<20} | Books: {row['count_of_books']:<5} | Avg Price: ${row['average_price']:.2f}"")`) provides a more user-friendly and formatted output, which was implied by ""Print this information, showing Author, Count of Books, and Average Price."" The generated code outputs a raw DataFrame, which is less ""user-friendly"" for presentation.
2.  **Explicitly Stating Stock Count**: The last print statement `print(f""Author with most books in stock: {max_stock_author}"")` correctly identifies the author but doesn't explicitly state *how many* books that author has in stock, which the ground truth does (`Total stock for this author: {max_stock_value} units`). This would make the output more complete.

Overall, the code is highly correct and functional, adhering to the requirements. The minor points are related to presentation and completeness of the final output, not functional correctness.",qwen3-30b-a3b,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, learning_curve, validation_curve, GridSearchCV
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm...",6,"The generated code attempts to address many aspects of the prompt but falls short in implementing several ""advanced"" features and exhibits some structural and conceptual issues.

**Strengths:**
*   **Basic Structure:** It correctly sets up data loading, train-test split, and a basic preprocessing pipeline.
*   **Base Models & GridSearchCV:** It correctly defines and optimizes multiple base models using `GridSearchCV`.
*   **Ensemble Methods (Basic):** It implements `VotingClassifier` and `StackingClassifier` correctly.
*   **Evaluation (Basic):** It performs cross-validation and plots learning curves, and includes basic model comparison.
*   **Interpretability (SHAP):** It includes a SHAP implementation.
*   **Calibration (Basic):** It attempts `CalibratedClassifierCV`.

**Weaknesses and Areas for Improvement:**

1.  **Preprocessing (Completeness & Advanced):**
    *   **Multiple Imputation:** The prompt explicitly asked for ""multiple imputation strategies"" (plural). The generated code uses `SimpleImputer(strategy='mean')` which is a single, basic imputation method. The ground truth uses `IterativeImputer` (MICE), which is more advanced.
    *   **Outlier Detection and Treatment:** This was a key requirement (""outlier detection and treatment""). The generated code completely misses this. The ground truth includes a custom `OutlierCapper`.
    *   **Advanced Feature Engineering:** While `PolynomialFeatures` is included, the prompt also mentioned ""interaction features."" The current `PolynomialFeatures(degree=2, include_bias=False)` does include interaction terms inherently, but the prompt's wording implied a more explicit and potentially diverse approach (e.g., beyond just polynomial). The preprocessing setup is also not fully integrated into the `best_models` loop in a way that allows easy access to processed feature names for SHAP.

2.  **Model Optimization (Advanced):**
    *   **Bayesian Optimization:** The prompt listed ""GridSearchCV or Bayesian optimization."" The generated code only uses `GridSearchCV`. While not a complete failure, it's a missed opportunity to show advanced optimization.
    *   **Pipeline during Gridsearch:** In the generated code, `GridSearchCV` for `svm` and `mlp` (and other models) is run on an entire pipeline: `pipeline = Pipeline([('preprocessor', preprocessor), ('classifier', model)])`. However, the `plot_validation_curve` then tries to use `param_name='classifier__n_estimators'`. Not all models have `n_estimators`. This part of the code would likely fail or produce meaningless plots for `SVM` or `MLP`. The ground truth does GridSearchCV on just the classifier and then uses `X_train_processed_df` which is already preprocessed. This is a subtle but important difference in how the pipeline is integrated.

3.  **Ensemble Models (Completeness):**
    *   **Blending:** The prompt specifically requested ""blending"" technique. The generated code does not include any blending implementation. The ground truth provides a custom blending implementation.

4.  **Advanced Evaluation (Completeness & Correctness):**
    *   **Stratified K-Fold:** Used in `GridSearchCV` and `cross_val_score`, which is good.
    *   **Learning Curves/Validation Curves:** Implemented, but the validation curve plotting for N_estimators parameter for all models will fail. The `plot_validation_curve` function is also not flexible enough to properly handle different parameter types (e.g., `hidden_layer_sizes` for MLP).
    *   **Bootstrap Sampling:** Implemented, but the confidence interval calculation is less robust. It samples `X_train` and `y_train` but then evaluates on `X_test`, which is not standard bootstrap (bootstrap is usually on the test set itself for CI). The score calculation `model.score(X_test, y_test)` is generic and often uses accuracy, while ROC AUC is a more common metric for CI in classification.
    *   **Brier Score:** The prompt requested `Brier Score` (and `probability calibration plots`), but the generated code doesn't calculate or plot it, only ROC curves.
    *   **Confidence Intervals:** The generated code calculates them during bootstrap, but doesn't integrate them into the final dashboard visualization.

5.  **Model Interpretability (Correctness & Breadth):**
    *   **LIME:** The prompt asked for ""SHAP or LIME."" Only SHAP is implemented.
    *   **SHAP Implementation:** The SHAP `explainer` uses `X_train.values` and expects `X_test.values` for `shap_values`. This is problematic because `X_train` and `X_test` are raw dataframes, not the processed ones. SHAP needs the processed data that the models were trained on. This will lead to an error or incorrect results. The ground truth converts processed arrays back to dataframes (`X_train_processed_df`, `X_test_processed_df`) specifically for SHAP compatibility and passing correct feature names.
    *   **Feature Importance:** While SHAP provides feature importance implicitly, the prompt requested ""feature importance analysis,"" which could be more explicitly linked to other metrics or a dashboard.

6.  **Statistical Significance Testing:**
    *   **Paired t-tests:** Implemented. However, the `t_test` function re-runs cross-validation for each comparison, which is computationally inefficient. Good practice is to store the CV scores once for each model (as the ground truth does) and then perform paired t-tests on those stored scores.

7.  **Model Comparison Dashboard (Completeness):**
    *   **Prediction Confidence Intervals:** Not integrated into the dashboard.
    *   **Comprehensive Metrics:** While accuracy is shown, other metrics like precision, recall, F1-score, ROC AUC, and Brier score are not explicitly shown in the final dashboard (only accuracy for CI). The ground truth provides a `DataFrame` with all metrics.
    *   **Visualization:** The visual dashboard is limited to a single bar plot for accuracy. The ground truth provides a more comprehensive dashboard with multiple metric plots and ROC curves.

8.  **Model Calibration (Completeness):**
    *   **Probability Calibration Plots:** The plot generated is labeled ""Calibration Curve"" but it's an ROC curve, not a reliability diagram. The prompt specifically asked for ""probability calibration plots"" which typically refers to reliability diagrams (fraction of positives vs. mean predicted value). The ground truth implements this correctly.

Overall, the generated code has good intentions but consistently either misses specific requirements (e.g., multiple imputation, outlier treatment, blending, LIME, proper calibration plots, full dashboard CI integration) or implements them incorrectly (e.g., SHAP data input, validation curve parameter logic, bootstrap on train then test). The level of ""advanced"" implementation for several components isn't met.",qwen3-30b-a3b,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from sklearn.impute import KNNImputer, IterativeImputer
from sklearn.linear_model import LogisticRegression
from pygam import LogisticGAM
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.neighbors import LocalOutlierFactor
from sklearn.model_selection import train_test...",4,"The generated code attempts to address many of the requirements but falls short in implementation details, correctness, and completeness for an ""expert"" level task.

**Primary Reasons for Failure:**

*   **Missing Data Imputation Comparison:** The code calculates MSE for imputation but only between `original.dropna()` and `imputed.dropna()`. This is fundamentally incorrect as `original.dropna()` discards rows with missing values, making the comparison meaningless and non-informative about imputation effectiveness. A proper comparison would be on the imputed values themselves or
    their impact on down-stream models. It also incorrectly uses `SimpleImputer` without importing it, leading to a runtime error. It states it implements MICE but then uses `IterativeImputer` from `scikit-learn` which, while MICE-like, isn't explicitly `MICE` and its effectiveness isn't truly compared against KNN. The task asked for *multiple* imputation techniques and *comparison* of their effectiveness, which wasn't robustly done.
*   **Multivariate Statistical Models (GAM):** The `pygam` LogisticGAM fit itself is technically present, but there's no evaluation or interpretation beyond the fit. The task specified advanced statistical models, and merely fitting is insufficient.
*   **Bayesian Logistic Regression (PyMC3):** The PyMC3 implementation is critically flawed. It tries to use `pm.math.dot(X, beta)` where `X` is the entire DataFrame from the start, not a preprocessed numerical array. This will fail. It lacks proper data preparation (scaling, splitting into train/test) for PyMC3 within that section, and there's no posterior predictive checks or performance evaluation (ROC AUC, classification report) which are essential for a good Bayesian model. The `trace` is sampled but never used for prediction or detailed analysis.
*   **Hypothesis Testing:**
    *   **MANOVA:** The formula for MANOVA directly uses column names which might not be robust if `data` is not yet imputed. The ground truth separates features/target properly.
    *   **Permutation Tests:** It uses `permutation_test_score` from `sklearn.model_selection` which is a function that gives a p-value for a specific score (e.g., accuracy) but this is not the general permutation testing for *any* hypothesis (like mean difference) asked for, which requires manual implementation as done in the ground truth. It's a very limited application of permutation testing.
    *   **Bootstrap Confidence Intervals:** Completely missing. This was a specific requirement.
*   **Robust Statistical Methods:** It only uses `RANSACRegressor` which is for regression. `Potability` is a binary classification target. This is not a robust method applicable to the classification problem at hand, or robustly demonstrating handling outliers/non-normal distributions for the *classification* task. The ground truth correctly chose RLM for a continuous variable to demonstrate the concept.
*   **Model Diagnostics:**
    *   **Residual Analysis:** For logistic regression, `probplot` on `resid_dev` (which are deviance residuals) is not standard for assessing normality in the same way as linear regression residuals. While Q-Q plots are mentioned, their application here is misleading if interpreted for normality. Proper residual plots (like residuals vs. fitted values) for GLMs are different.
    *   **Leverage vs. Cook's Distance:** It plots Cook's distance vs. Leverage, but doesn't interpret, identify influential points, or discuss their implications. `influence_plot` from `statsmodels` provides a more direct way to visualize these.
    *   **Goodness-of-Fit Tests:** No explicit goodness-of-fit tests for logistic regression (e.g., Hosmer-Lemeshow or Deviance tests beyond just showing the summary).
    *   **Diagnostic Dashboard:** The dashboard visualizations are very generic (`lineplot` of fitted values and residuals without context). They don't form a coherent ""diagnostic dashboard"" for logistic regression.
*   **Completeness:** Several key requirements were either fully missed or implemented poorly:
    *   Multiple missing data imputation *comparison* of effectiveness.
    *   Bootstrap confidence intervals.
    *   Robust statistical methods for *classification* or demonstrating how they apply to the dataset in a relevant way.
    *   Advanced statistical visualizations are partly present but lack the depth and specific examples (e.g., leverage plots directly from statsmodels).
*   **Code Quality/Structure:**
    *   Missing `SimpleImputer` import, causing an error.
    *   Lack of clear separation between steps, less print statements to guide execution compared to ground truth.
    *   Re-fitting models on `X, y` directly after imputation without a consistent split (train/test) or scaling, which is done only partly for some sections. Data preprocessing (scaling) is not consistently applied across all models.
    *   No error handling for `pd.read_csv`.
    *   Inconsistent variable naming (`data` then `X, y`).

The generated code provides a superficial attempt at most requirements but lacks the robustness, correctness, and statistical depth expected from an ""expert"" solution. The critical errors in imputation comparison, Bayesian modeling implementation, and applicability of robust methods for the given task, along with missing key techniques, make it a low-scoring solution.",qwen3-30b-a3b,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, GaussianMixture, SpectralClustering
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.ense...",6,"The generated code attempts to address most of the requirements but falls short in several areas, particularly regarding the 'advanced' aspects of the prompt and the robustness of the implementations.

**Strengths:**
*   **Basic Structure:** It follows a logical flow of data loading, preprocessing, feature engineering, clustering, dimensionality reduction, and visualization.
*   **Multiple Algorithms:** It includes K-means, Agglomerative, DBSCAN, Gaussian Mixture, and Spectral Clustering algorithms.
*   **Dimensionality Reduction:** It uses PCA, t-SNE, and UMAP for visualization.
*   **Cluster Validation Metrics:** It attempts to use Silhouette, Calinski-Harabasz, and Davies-Bouldin scores for optimal cluster determination.
*   **Anomaly Detection:** It includes Isolation Forest for anomaly detection.
*   **Visualization:** It uses Plotly for interactive plots.

**Weaknesses and Areas for Improvement:**

1.  **Feature Engineering (Major Weakness):**
    *   **Lack of Dataset-Specific Features:** The generated code attempts to engineer features like `pH_balance`, `contamination_risk`, `sulfate_balance`, and `alkalinity_ratio`. However, it only checks for columns 'Iron', 'Lead', 'Arsenic', and 'Alkalinity' which are *not present* in the `water_potability.csv` dataset. This results in these features not being created, and the feature engineering step effectively does nothing beyond dropping the 'Potability' column.
    *   **No ""Advanced"" Features:** The prompt explicitly asks for ""advanced features including water quality indices, chemical balance ratios, contamination risk scores, and temporal patterns if applicable."" The ground truth provides detailed, if simplified, implementations for `pH_Deviation`, `Hardness_Quality_Score`, `Contamination_Risk_Score`, various chemical ratios, and an `WQI_Overall`. The generated code's attempt at feature engineering is largely absent due to missing columns and lacks the depth requested.

2.  **Optimal Number of Clusters (Incomplete/Inaccurate):**
    *   **Visualization Missing:** While metrics are calculated, there are no plots (like elbow method plot, silhouette score plot) to visually aid in determining the optimal K. The ground truth provides these plots, which are crucial for interpreting the results of these validation techniques.
    *   **Gap Statistic/Elbow Method Not Implemented:** The prompt specifically asks for ""elbow method"" and ""gap statistic."" The elbow method is a visual technique, which without a plot, is not properly addressed. Gap statistic is entirely missing.

3.  **Cluster Stability Analysis (Very Limited):**
    *   **Bootstrap Implementation Flawed:** The `bootstrap_stability` function, while named appropriately, returns a co-occurrence matrix. However, it does not *use* this matrix to perform any actual stability analysis (e.g., using hierarchical clustering on the co-occurrence matrix to see stable groups, as shown in the ground truth's consensus clustering). It only computes the matrix and then stops.
    *   **Consensus Clustering Missing:** The prompt asks for consensus clustering, which is a key part of stability analysis. The generated code computes a co-occurrence matrix but does not proceed to use it for consensus clustering (e.g., building a dendrogram or using a consensus clustering algorithm).

4.  **Cluster Profiling:**
    *   **Limited Depth:** The `analyze_clusters` function only calculates the mean of features per cluster. While a good start, the prompt asks for ""chemical composition patterns, contamination risks, and potability rates,"" suggesting a more interpretative analysis beyond just raw means. The ground truth explicitly calculates a `Potability_Rate` and has the necessary engineered features to discuss ""contamination risks"" directly.

5.  **External Validation:**
    *   **Basic Potability Check:** It checks for potability rates per cluster, which is good. However, it lacks a conceptual discussion on how domain knowledge (water quality standards) would be used to validate, as the ground truth includes.

6.  **DBSCAN Parameter Tuning:**
    *   The DBSCAN parameters (`eps=0.5`, `min_samples=5`) are hardcoded without any attempt at estimation or justification (e.g., using a k-distance graph). The ground truth intelligently suggests how `eps` could be estimated and plots the K-distance graph.

7.  **Minor Issues:**
    *   **Redundant Imports:** `DBSCAN`, `AgglomerativeClustering`, and `SpectralClustering` are imported multiple times.

To improve, the generated code needs to:
*   Implement the missing feature engineering correctly using the available columns and aligning with ""advanced features.""
*   Provide visualizations for optimal K determination.
*   Fully implement and demonstrate cluster stability analysis (e.g., showing a consensus dendrogram).
*   Add more explicit profiling of contamination risks and integrate the (missing) engineered features into the analysis.
*   Address the DBSCAN parameter tuning.",qwen3-30b-a3b,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression

df = pd.read_csv('winequality-red.csv')

correlations = df.corr()['quality'].drop('quality')
top_positive = correlations.sort_values(ascending=False).head(3)
top_negative = correlations.sort_values(ascending=True).head(3)

print(""Top 3 positively correlated features:"")
print(top_positive)
print(""\nTop 3 negatively correlated features:"")
print(top_negative)

plt.figure(figs...",9,"The generated code successfully addresses all core requirements of the task. It correctly calculates correlations, identifies top positive and negative features, generates a correlation heatmap, and creates scatter plots with linear regression lines for the most correlated features.

Strong points:
- Correctly uses pandas for data loading and correlation calculation.
- Accurately identifies top positive and negative correlations.
- Generates a clear correlation heatmap.
- Implements linear regression for scatter plots correctly using `sklearn.linear_model.LinearRegression`.
- Plots regression lines on the scatter plots.
- Includes all necessary imports.

Areas for minor improvement (to reach a perfect 10):
- **Error Handling:** The code does not include error handling for file not found or other potential issues when loading the CSV, which is present in the ground truth.
- **Plot Customization:** While functional, the plots could benefit from more detailed titles, axis labels, and potentially legends to match the ground truth's level of polish (e.g., displaying R-squared on the regression line).
- **Redundant Model Instantiation:** `LinearRegression()` is instantiated twice (once for each scatter plot). While not a major issue, it could be instantiated once if desired, though separate instantiation is also fine for clarity.
- **Assumed `data` argument in `sns.scatterplot`:** The generated code uses `sns.scatterplot(x=top_positive_feature, y='quality', data=df)`. While this works, `x` and `y` being string names of columns in the `data` DataFrame is the intended use. The ground truth explicitly passes `df[column_name]` which is also valid but less idiomatic for seaborn. Both are correct but the generated code's approach is often preferred with seaborn.

Overall, it's a very good solution that fulfills the prompt's requirements effectively.",qwen3-30b-a3b,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load and clean data
df = pd.read_csv('video_games_sales.csv')
df.dropna(inplace=True)

# 1. Nintendo vs Others
nintendo_sales = df[df['Publisher'] == 'Nintendo']['Global_Sales']
others_sales = df[df['Publisher'] != 'Nintendo']['Global_Sales']
t_stat, p_val_nintendo = stats.ttest_ind(nintendo_sales, others_sales)
print(f""1. Nintendo vs Others p-value: {p_val_nintendo}"")

# 2. Action vs Sports
action_sales = df[df['Genre']...",4,"The generated code attempts to address the task but fails significantly in correctness, completeness, and robustness.

**Primary Reasons for Low Score:**

1.  **Missing Interpretation of Results:** The prompt explicitly asks to ""Display p-values and interpret results with significance level =0.05"". The generated code only prints p-values and completely lacks any interpretation, which is a core requirement of hypothesis testing. This is a critical omission.
2.  **Inadequate Data Cleaning:**
    *   `df.dropna(inplace=True)` at the beginning drops *all* rows with *any* missing values. This is too aggressive and can remove a significant portion of the data, especially if columns not relevant to the analysis have NaNs (e.g., `Critic_Score`, `User_Score`, `Rating`). The ground truth code drops NaNs more strategically, only from columns essential for the specific tests (`Global_Sales`, `Year_of_Release`, `Publisher`, `Genre`, `Platform`).
    *   The `Year_of_Release` column is often read as float due to NaNs, and needs explicit conversion to `int` after dropping NaNs, which the generated code misses, potentially causing issues for comparisons like `df['Year'] < 2000`.
    *   The generated code uses `df['Year']` which is not a column in the dataset (it's `Year_of_Release`). This will lead to a `KeyError`.
3.  **No Error Handling for File Not Found:** The ground truth code includes a `try-except` block for `FileNotFoundError`, which is good practice for robust code. The generated code lacks this.
4.  **Implicit Assumptions/Lack of Robustness:**
    *   The t-tests (`ttest_ind`) by default assume equal variances (`equal_var=True`). In many real-world scenarios, this assumption is violated. The ground truth correctly uses `equal_var=False` (Welch's t-test), which is more robust.
    *   For the first t-test (Nintendo vs. Others), the prompt specifically asks to test if Nintendo has *significantly higher* sales, implying a one-sided test. The generated code uses the default two-sided test.
    *   There are no checks for sufficient data in groups before performing statistical tests (e.g., `len(group) < 2`), which could lead to errors if groups are empty or too small.
5.  **Lack of Structure and Comments:** While not explicitly asked for extensive comments, the ground truth uses clear print statements and sections to explain what each part of the code is doing, which is much better for readability and understanding the flow of a multi-part task.

**Overall, the generated code provides a skeleton of the statistical tests but critically fails to meet the requirements for interpretation, robust data handling, and thorough problem adherence.**",qwen3-30b-a3b,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv('video_games_sales.csv')

# Handle missing values
categorical_cols = ['Platform', 'Genre', 'Publisher']
for col in categorical_cols:
    df[col].fillna('Unknown', inplace=True)

numerical_c...",6,"The generated code addresses most of the requirements but has several significant deficiencies, particularly in handling missing values and feature selection, which impact its correctness and robustness.

**Deficiencies:**

1.  **Missing Value Handling (Critical):** The generated code attempts to fill missing categorical values with 'Unknown' and numerical values with the median. This is generally a valid strategy. However, the ground truth code specifically drops rows where `Global_Sales` is NaN, as it's directly used for the target variable. More importantly, the generated code *does not* drop `NaN` values for `Year`, `Publisher`, `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales` *before* using them. `df['Year']` will still contain `NaNs` after median imputation, preventing `2023 - df['Year']` from working correctly with potential non-integer values (depending on the median of `Year`). The ground truth specifically converts `Year` to `int` after dropping NaNs to ensure this. This is a critical logical flaw.
2.  **Feature Selection (Major):** The generated code uses `X = df.drop('High_Sales', axis=1)`. This means *all* original columns, including `Name`, `Platform`, `Genre`, `Publisher`, `Developer`, `Rating`, `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`, `Global_Sales`, `Year` (and the newly created features), are included as features. Many of these (like `Name`, `Developer`, `Rating`, and importantly `Global_Sales` itself, which is the direct source of the target variable `High_Sales`) should be excluded from `X`. Including `Global_Sales` directly makes the model's prediction trivial and unrealistic (data leakage), as it's directly correlated with the target variable. The ground truth explicitly defines a list of relevant `features` to avoid this.
3.  **Target Variable Location:** The prompt states to create `High_Sales` based on `Global_Sales > 1.0`. The generated code does this correctly.
4.  **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` for the CSV, which is good practice. The generated code lacks this.
5.  **Readability/Completeness of Output:** While the evaluation metrics are printed, the formatting of feature importance is just a loop, not as cleanly presented or sorted as in the ground truth (which uses a Pandas Series and sorts it). The confusion matrix lacks labels.
6.  **`random_state` for `train_test_split`:** The generated code correctly uses `random_state`.
7.  **`stratify` for `train_test_split`:** The ground truth includes `stratify=y` in `train_test_split`, which is important for maintaining the class distribution in imbalanced datasets, especially for binary classification. The generated code misses this.

**Positive Aspects:**

*   Correctly uses `LabelEncoder` for categorical features.
*   Correctly creates `Total_Regional_Sales` and `Years_Since_Release`.
*   Correctly uses `RandomForestClassifier` with 100 estimators.
*   Correctly calculates and prints accuracy, precision, recall, F1-score, and confusion matrix.
*   Displays feature importance (though not ideally formatted).
*   Correct imports.",qwen3-30b-a3b,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures, StandardScaler

df = pd.read_csv('video_games_sales.csv')

df.fillna({
    'NA_Sales': 0,
    'EU_Sales': 0,
    'JP_Sales': 0,
    'Other_Sales': 0,
    'Global_Sales': 0,
    'Publisher': 'Unknown',
    'Genre': 'Unknown'
}, inplace=True)

df['Sales_Ratio_NA_EU'] = df['NA_Sales'] / (df['EU_Sales'].replace(0, 1))...",7,"The generated code addresses most of the requirements but has significant areas for improvement, particularly regarding missing values handling, PCA implementation, and interaction features.

**Strong Points:**
*   Correctly loads the dataset.
*   Successfully implements most of the requested engineered features: `Sales_Ratio_NA_EU`, `Publisher_Avg_Sales`, `Genre_Market_Share`, `Platform_Popularity`, and `Sales_Momentum`.
*   Performs correlation analysis and correctly visualizes it with a heatmap.
*   Applies polynomial features (degree 2) to sales columns and concatenates them.
*   Attempts PCA and calculates explained variance.
*   Includes outlier handling using the IQR method for numerical columns.

**Areas for Improvement / Missing or Incorrect aspects:**

1.  **Missing Value Handling (Critical):**
    *   The ground truth explicitly handles missing values for `Year` (median), `Publisher` (mode), and `Genre` (mode) using `SimpleImputer`. The generated code uses `fillna` with `0` for sales and `Unknown` for `Publisher`/`Genre`. This is less robust and can severely impact numerical calculations (e.g., `Year` being 0, or sales being 0 where they were just missing). The task specifically mentioned ""handle missing values"".
    *   The `Year` column, in particular, is highly problematic if filled with `NaN` by `fillna` as it's crucial for `Sales_Momentum` and `Year_Genre` interaction. The generated code implicitly assumes no NaNs for `Year` after `fillna` for other columns, which might not be true.

2.  **PCA Implementation (Partial Correctness):**
    *   The `X` for PCA is constructed by dropping specific columns (`Year`, `Name`, `Platform`, `Publisher`, `Genre`). However, it should only include numerical columns. The `df.select_dtypes(include=np.number).columns` correctly identifies numerical features, but then `X` incorrectly tries to drop non-existent columns (like `Name`, `Platform`, `Publisher`, `Genre` from a `numerical_features` list).
    *   More importantly, the PCA results (the `principal_components` themselves) are calculated but **not used or added back to the DataFrame**. The ground truth explicitly shows transforming the data to principal components and creating a new DataFrame for them to demonstrate the reduction. The generated code only prints explained variance, which is a start, but not the full dimensionality reduction analysis expected.

3.  **Interaction Features (`Year_Genre`) (Incorrect Implementation):**
    *   The task asks for ""interaction features between Year and Genre"". The ground truth correctly implements this by one-hot encoding `Genre` and then multiplying `Year` by each genre-specific dummy variable. This creates meaningful numerical interaction terms.
    *   The generated code creates `df['Year_Genre'] = df['Year'].astype(str) + '_' + df['Genre']`. This creates a *string* concatenation, which is a categorical feature, not a numerical interaction feature as typically understood in feature engineering for modeling (e.g., `Year * Is_Genre_Sports`). This is a significant misinterpretation of ""interaction features.""

4.  **Implicit Assumptions/Potential Errors:**
    *   The `EU_Sales.replace(0, 1)` in `Sales_Ratio_NA_EU` is a decent attempt to avoid division by zero, but adding a small epsilon (as in the ground truth) is more standard and reflects a slight variation in the denominator rather than setting it rigidly to 1 when 0.
    *   The `col` loop for outlier handling might be applied to new polynomial features or interaction features that might not be suitable for IQR capping, or if they are, they need to be handled carefuly. The ground truth specifies target columns for outlier handling.

5.  **Output and Explanation:** The ground truth includes `print` statements and `df.info()` calls at various stages, which are helpful for understanding the data processing pipeline and debugging. The generated code is concise but lacks this helpful output for demonstrating the impact of each step.",qwen3-30b-a3b,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"import pandas as pd
import numpy as np
from scipy import stats

df = pd.read_csv('diabetes.csv')

cols_to_clean = df.columns.drop('Outcome')
df[cols_to_clean] = df[cols_to_clean].replace(0, np.nan)
df[cols_to_clean] = df[cols_to_clean].fillna(df[cols_to_clean].median())

diabetic_glucose = df[df['Outcome'] == 1]['Glucose']
non_diabetic_glucose = df[df['Outcome'] == 0]['Glucose']
t_stat, t_p = stats.ttest_ind(diabetic_glucose, non_diabetic_glucose)

diabetic_bmi = df[df['Outcome'] == 1]['BMI']
no...",6,"The generated code addresses most of the required statistical tests and data preparation steps, but it loses points on several critical details and the lack of proper interpretation.

**Positives:**
*   Correctly implements loading the dataset, general zero-value cleaning, and median imputation.
*   Successfully performs t-test, Mann-Whitney U test, Chi-square test, Pearson correlation, and ANOVA.
*   Correctly extracts and prints p-values.
*   Provides a simple interpretation based on the p-value and alpha.

**Areas for Improvement / Missing Aspects:**

1.  **Data Cleaning - Biological Implausibility:** The generated code replaces zeros with NaN and then imputes *all* columns except 'Outcome' (`cols_to_clean = df.columns.drop('Outcome')`). This is incorrect for 'Pregnancies' where 0 is a valid and meaningful value. The ground truth explicitly handles this by only impacting `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, and `BMI`. This oversight in the generated code potentially alters the 'Pregnancies' distribution and subsequently the Pearson correlation result.
2.  **Age Group Binning:** The `pd.cut` for `AgeGroup` has a slight discrepancy with the task prompt: `<30` (young) and `30-50` (middle), `>50` (older). The generated code uses `bins=[0, 30, 50, 100]` which means `(0, 30]`, `(30, 50]`, `(50, 100]`. The ground truth uses `bins=[0, 29, 50, np.inf]`, which more accurately reflects `<30` as `(0, 29]` and `30-50` as `(29, 50]`, and `>50` as `(50, inf)`. This detail can change bin membership for boundary values.
3.  **BMI Category Binning:** Similarly, the `pd.cut` for `BMICategory` differs. The generated code uses `bins=[0, 18.5, 24.9, 29.9, 100]`. The ground truth uses `[0, 18.5, 25, 30, np.inf]` with `right=False`, which correctly maps to `>=18.5 and <25` for normal, etc. The generated code's normal range is `(18.5, 24.9]`, missing values like 24.95 or 25.0 up to 29.9. This will likely lead to different categorizations.
4.  **T-test `equal_var` parameter:** The generated code uses the default `equal_var=True` for `stats.ttest_ind`. For comparing groups where variances might differ, `equal_var=False` (Welch's t-test) is often a safer and more robust choice, as used in the ground truth. This is a subtle but important statistical consideration.
5.  **Interpretation:** While the generated code prints ""Significant"" or ""Not significant"", it lacks the *detailed interpretation* provided in the ground truth. For example, for the t-test, the ground truth specifies ""diabetic patients have significantly higher glucose levels,"" which indicates the direction of the difference. ForPearson correlation, it specifies ""positive"" or ""negative"" correlation. For chi-square, it explains an ""association."" For ANOVA, it explains ""difference across categories."" This level of detail in interpretation was a key requirement.
6.  **Error Handling/Robustness:** The ground truth includes basic error handling for file not found, which is good practice. The generated code assumes the file exists.
7.  **Informative Output:** The generated code only prints p-values and a simple significance statement. The ground truth provides more context, such as means/medians for groups, and the test statistics (t-stat, U-stat, Chi2-stat, F-stat), which are valuable for understanding the test results beyond just the p-value.
8.  ** (Alpha) Definition:** The ground truth explicitly defines `ALPHA = 0.05` at the beginning, promoting better readability and easy modification of the significance level. The generated code hardcodes `0.05` in the print statements.",qwen3-30b-a3b,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, classification_report
fr...",6,"The generated code addresses most of the task requirements but has several areas for improvement in correctness, completeness, and code quality.

**Strengths:**
*   Successfully loads data and handles zero imputation.
*   Creates the three requested new features (`Glucose_BMI_ratio`, `Age_group`, `Insulin_level`).
*   Performs one-hot encoding for categorical features.
*   Splits data into training and testing sets.
*   Trains and evaluates multiple classification models (Logistic Regression, Random Forest, SVM, Gradient Boosting).
*   Calculates various evaluation metrics (accuracy, precision, recall, F1, ROC AUC).
*   Generates confusion matrices and ROC curves.
*   Performs hyperparameter tuning using GridSearchCV.
*   Displays feature importance for tree-based models.

**Areas for Improvement (leading to a score of 6):**

1.  **Missing Data Preprocessing (Scaling):** The code performs one-hot encoding, but it completely omits numerical feature scaling (e.g., using `StandardScaler`) before model training and evaluation. This is crucial for models sensitive to feature scales like Logistic Regression and SVM. The ground truth code correctly uses a `ColumnTransformer` for this.
2.  **Inconsistent Preprocessing for Tuning:** The code assumes all preprocessing (dummies) happens before the train/test split. However, if any preprocessing steps (like scaling) were added, they would need to be applied correctly to both train and test sets *after* splitting, or using a `Pipeline` for robustness. The current implementation directly passes the original `X_train` and `X_test` (with only `dummies` applied) to models.
3.  **Redundant Model Fitting/Evaluation Loop:** The code has two separate loops after model definition: one for `cross_val_score` and another where models are fit and `results` are populated. This is inefficient. Ideally, models should be fit once for cross-validation within `cross_val_score`, and then fit once more on the full training data before evaluating on the test set. The `models` dictionary currently stores unfitted models, leading to repeated fitting for evaluation.
4.  **Hyperparameter Tuning Target:** The problem explicitly asked to perform hyperparameter tuning for the *best model*. The generated code identifies the ""best model"" based on `cross_val_scores` (accuracy), but then uses a fixed set of `param_grid` based on the *name* of that best model, which means it will only tune *one* type of model. The ground truth code explicitly picked Random Forest for tuning, which is a more defined approach given the broadness of ""best model.""
5.  **Feature Importance Plotting:** The feature importance plot uses the entire `X.columns`, which is correct for tree models if the data was not scaled and transformed through a process that changes feature names. However, if scaling and other transformations were in place, the `X.columns` would not map correctly to the features of the fitted models. The `all_feature_names` approach in the ground truth is more robust. Also, plotting *all* features (`feature_importance_df`) without a `head()` or threshold can result in very dense, unreadable plots if there are many features.
6.  **Minor Imports/Libraries:** Includes `PCA` in imports but doesn't use it. This is minor but contributes to code quality.
7.  **Plotting Repetition:** Individual plots for confusion matrices and ROC curves are generated inside a loop, causing multiple plot windows to appear sequentially. A better approach, as in the ground truth, is to use subplots to display all confusion matrices in one figure and all ROC curves in another combined figure for easier comparison.",qwen3-30b-a3b,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('diabetes.csv')
if df.isnull().sum().sum() > 0:
    df = df.fillna(df.median())

df['Metabolic_Score'] = df['...",6,"The generated code attempts to address all parts of the task, but it falls short in several key areas of correctness and completeness.

**Areas of Weakness:**

1.  **Missing Value Handling:** The prompt explicitly states to ""handle missing values"" and the ground truth identifies 0s as missing for specific columns (`Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`). The generated code only checks for `df.isnull().sum().sum() > 0` which *won't* catch these 0s, and then applies `df.fillna(df.median())` which is insufficient for the specified ""0 as missing"" scenario. This is a critical failure in data preprocessing.
2.  **Polynomial Features Application:** The generated code applies polynomial features to `X` (original features) and then concatenates the results `X_poly = pd.concat([X, poly_df], axis=1)`. This means the original features are *duplicated* when their polynomial counterparts (`x`, `x^2`, `xy`) are also present, which can inflate feature space unnecessarily and cause issues with collinearity. The ground truth correctly drops the original columns after generating polynomial features for them.
3.  **Feature Scaling Input:** The generated code scales `X_poly` *before* correlation filtering or any other feature selection, which is generally fine. However, it then uses `X_std_df` (which is `X_poly` scaled) for PCA, SelectKBest, RFE, and correlation filtering. When printing feature importance, it uses `X_res.columns` which comes *after* SMOTE, but the RFE and SelectKBest are performed on `X_std_df`.
4.  **PCA:** The prompt asks to ""explain variance ratios"", and while `explained_variance` is printed, there's no mention of how many components explain a certain percentage of variance (e.g., 95%) or how many components were ultimately kept for further steps. The generated code performs PCA but doesn't actually use `X_pca` for subsequent steps, which means PCA was performed but not integrated into the feature pipeline.
5.  **SMOTE Integration:** SMOTE is applied, but the `X_res` and `y_res` from SMOTE are only used for training the Random Forest for feature importance visualization. The previous feature selection steps (SelectKBest, RFE) were performed on `X_std_df` or `X_corr_df` *before* SMOTE, which is generally acceptable *for feature selection*, but the task implies a progressive refinement.
6.  **Readability/Structure:** The code is functional but lacks comments, print statements, or clear sectioning as present in the ground truth, making it harder to follow the pipeline's stages. No robust error handling for file loading.
7.  **Feature Importance Visualization:** While it plots feature importance, the y-axis labels (`X_res.columns`) can become very long and unreadable, especially with polynomial features. The ground truth limits to top N features and uses `sns.barplot` with `x` and `y` specified for better readability.

**Areas of Strength:**

*   Correctly imports necessary libraries.
*   Attempts to create all specified engineered features.
*   Applies polynomial features.
*   Performs both StandardScaler and MinMaxScaler.
*   Applies PCA (though not fully integrated).
*   Correctly uses SelectKBest and RFE methods.
*   Implements correlation-based feature filtering.
*   Applies SMOTE.
*   Evaluates and attempts to visualize feature importance using Random Forest.

Overall, the code has the right components but struggles with the nuanced implementation details of integrating them into a robust and correct pipeline. The failure to correctly handle missing values (0s interpreted as NaN) is a significant flaw.",qwen3-30b-a3b,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"import pandas as pd
import numpy as np
from scipy import stats

df = pd.read_csv('world-happiness-report-2021.csv')
df.dropna(inplace=True)

# 2) T-test: Western European countries vs others
western_europe = df[df['Region'] == 'Western Europe']
others = df[df['Region'] != 'Western Europe']
t_stat, p_val = stats.ttest_ind(western_europe['Happiness Score'], others['Happiness Score'])
print(f""T-test p-value: {p_val}"")

# 3) Mann-Whitney U test: GDP per capita between top 25% and bottom 25% happines...",5,"The generated code attempts to address all parts of the task but falls short on several critical aspects, primarily correctness (due to incorrect column names and assumed columns) and completeness (missing interpretations and proper error handling).

Here's a breakdown of the issues:

1.  **Incorrect Column Names:** The most significant issue is the use of incorrect column names.
    *   `df['Region']` is used instead of `df['Regional indicator']`. This will cause `KeyError` exceptions.
    *   `df['Happiness Score']` is used instead of `df['Ladder score']`. This will also cause `KeyError` exceptions.
    *   `df['Log GDP per capita']` is used instead of `df['Logged GDP per capita']`. Another `KeyError`.
    These errors prevent the code from running successfully.

2.  **Missing Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice. The generated code lacks this.

3.  **Incomplete Missing Value Handling:** The generated code uses `df.dropna(inplace=True)`, which drops *all* rows with *any* NaN. While this handles missing values, the ground truth is more precise by specifying `critical_columns` for dropping NaNs, ensuring that only rows relevant to the analysis with missing data are removed, potentially preserving more data if NaNs exist in irrelevant columns. The ground truth also explicitly prints how many rows were dropped, which is good for transparency.

4.  **Missing Interpretations and -level Comparisons:** The task explicitly asks to ""Display p-values and interpret results with significance level =0.05"". The generated code only prints the p-values without any interpretation (e.g., ""Reject/Fail to reject null hypothesis"") or comparison against the `alpha` level. This is a critical omission for a statistical analysis task.

5.  **Hardcoding `alpha`:** The `alpha = 0.05` should be explicitly defined and used in if-else statements for interpretation, as in the ground truth.

6.  **T-test `equal_var` parameter:** For `ttest_ind`, the `equal_var=False` (Welch's t-test) is generally safer as it doesn't assume equal variances, which is often not true in real-world data. The generated code uses the default `equal_var=True`.

7.  **Clarity and Structure:** The generated code is more concise but lacks the print statements that elaborate on *what* test is being performed, the means/medians, and the clear separation of each test within the output, as seen in the ground truth. This makes the output less informative.

8.  **Assumptions in Chi-square:** For the chi-square test, the `pd.qcut` is used to create happiness categories. While valid, the definition of ""Low"", ""Medium"", ""High"" in the ground truth for quantiles 1/3 and 2/3 is more explicit and generally preferred for clearer group definition than `labels=False` then mapping.

Overall, the generated code demonstrates knowledge of the statistical tests but fails to implement them correctly due to column name issues and drastically lacks the required interpretation and detail requested in the prompt.",qwen3-30b-a3b,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn....",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being a fully correct and complete solution.

**Strengths:**
*   **Loads dataset correctly.**
*   **Splits data into train/test sets.**
*   **Trains multiple regression models.**
*   **Evaluates models using MAE, MSE, RMSE, and R score.**
*   **Performs hyperparameter tuning (though with issues).**
*   **Attempts residual plots and feature importance.**
*   **Implements cross-validation.**
*   **Attempts prediction for new data.**

**Weaknesses and Areas for Improvement:**

1.  **Preprocessing - Feature Column Names (Critical Error):** The generated code attempts to define features using `'GDP per capita'`, `'Freedom'`, etc., but the dataset uses `'Logged GDP per capita'`, `'Freedom to make life choices'`. This will cause a `KeyError` or fail to load data correctly, making the entire model training process invalid. The ground truth correctly identifies and uses the actual column names from the dataset. This is a fundamental error.
2.  **Preprocessing - Categorical Encoding:** The code unnecessarily performs one-hot encoding on all categorical columns (`Country name`, `Regional indicator`) for regression. While this is valid preprocessing in some contexts, for *this specific task*, the prompt only asks for numerical features as inputs for the model, so these encoded features would not be used. The ground truth explicitly states that no encoding is needed for the *specified features*.
3.  **Missing Value Handling:** The code uses `df.dropna()`, which is a blunt instrument. While it might work for this specific dataset if only a few rows are missing in the *relevant* columns, a more robust approach (as shown in the ground truth) is to drop rows only if the target or specified feature columns have missing values, or to use imputation if missing values are widespread and relevant. The ground truth explicitly checks for missing values in the *relevant* columns `FEATURE_COLUMNS + [TARGET_COLUMN]`.
4.  **Hyperparameter Tuning Logic:** The `if/elif/else` for `param_grid` is problematic.
    *   It only defines `param_grid` for Random Forest and Gradient Boosting. If a linear model (e.g., Ridge or Lasso) happens to be the ""best model"" initially (which is less likely given the models, but possible given the `results` sorting), `param_grid` will be empty, and `GridSearchCV` will run without any tuning, essentially just training the default model.
    *   The prompt asked for tuning the *best model*, but also specifically mentioned ""Gradient Boosting Regressor"" in the example output, implying a strong candidate. The generated code's approach is to tune *whichever* model came out best initially, which is a reasonable interpretation, but the empty `param_grid` for non-forest models is a flaw.
    *   The ground truth picks a strong candidate (GBR) for tuning, but also notes that RF could be chosen if better, providing a more robust overall solution.
5.  **Visualization - Residual Plots & Feature Importance Loop:** The generated code loops through *all* models for residual plots and feature importance. While this isn't strictly ""wrong,"" the prompt asked for ""residual plots and feature importance visualizations"" (plural), implying for *the* model(s) deemed most important (e.g., the best tuned model). The ground truth focuses on the best tuned model, which is a more practical and effective approach. Running all plots for all models might generate too many outputs unnecessarily.
6.  **Feature Importance for Linear Models:** The `hasattr(model, 'feature_importances_')` check is good, but it still tries to plot for all models. For linear models, coefficients are feature importance, not `feature_importances_`. The ground truth correctly `if hasattr` and notes specifically for the best model.
7.  **Cross-Validation:** The cross-validation for evaluation should ideally be done on the *best tuned model*, or at least clearly indicate what `model` is being used in the loop. The generated code runs CV for *all* initial models, which is redundant after identifying a ""best model"" and tuning it. The ground truth performs CV specifically for the best tuned model to assess its stability.
8.  **Prediction for New Countries:** The `new_data` DataFrame has column names that do not match the actual feature names from the dataset (e.g., `'GDP per capita'` instead of `'Logged GDP per capita'`). This would cause a `KeyError` or incorrect prediction. This directly relates to the first critical error about column names.
9.  **Imports and Pipelines:** While not strictly required by the prompt, the ground truth uses `ColumnTransformer` and `Pipeline` for more robust preprocessing and model training, especially if scaling or more complex categorical encoding was involved. The generated code lacks this more advanced (and often necessary) structuring.

Overall, the core logic is present, but the missing value handling, incorrect feature column names, and less precise parameter tuning/visualization logic significantly reduce its correctness and robustness.",qwen3-30b-a3b,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_regression, RFE, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_spli...",6,"The generated code attempts to address all parts of the task, but it has several critical issues that make it incomplete and incorrect in parts.

**Correctness:**
*   **Feature Engineering:**
    *   `Happiness_Efficiency`: Uses `Log GDP per capita` instead of `Logged GDP per capita` which is the actual column name. This will cause an error unless fixed.
    *   `Social_Wellness_Index`: Uses `Healthy life expectancy at birth` which is not a column in the provided dataset schema for 2021 report, it should be `Healthy life expectancy`.
    *   `Governance_Score`: Correctly uses `Freedom to make life choices` and `Perceptions of corruption`.
    *   `Regional_Happiness_Rank` and `GDP_vs_Regional_Average`: Logic seems correct, but `Regional_GDP_Mean` is not dropped as in the Ground Truth, leaving an unnecessary temporary column.
*   **Polynomial Features:**
    *   It operates on `X` which is a subset of the dataframe, but then concatenates `poly_df` to `df` without proper index management (assuming `X` and `df` have aligned indices initially, which they should).
    *   Does not handle non-numeric columns or potential `NaN` values before applying polynomial features, which can lead to errors.
*   **Feature Scaling:**
    *   It scales the entire `df` including non-numeric columns (`Country name`, `Regional indicator`) and the target variable (`Ladder score`), which is incorrect. Scaling should only be applied to numerical features used for modeling.
    *   The results of `scaled_minmax` and `scaled_robust` are computed but never used, leading to redundant calculations.
*   **Dimensionality Reduction (PCA & t-SNE):**
    *   Again, it uses `scaled_std` which contains non-numeric columns and the target variable, leading to incorrect results or errors.
    *   No visualization for PCA and t-SNE is performed, which was explicitly requested.
    *   No handling of `perplexity` for `t-SNE` based on dataset size, which is important.
*   **Feature Selection:**
    *   `SelectKBest`: The `X` used for feature selection is created by dropping `Ladder score`, `Country name`, `Regional indicator`. This is a better approach than using the whole `df` for scaling, but still doesn't account for other non-numeric columns or potential NaNs.
    *   `RFE`: Uses `RandomForestRegressor` directly without specifying `n_features_to_select` in the constructor based on a fixed value, although this is done for SelectKBest. `n_features_to_select=5` is provided, so this is correct.
*   **Correlation-based Filtering:**
    *   The logic for identifying highly correlated features is correct, but `X` is modified in place, which might affect subsequent steps if not handled carefully (though here it's fine as `X` is redefined before mutual info).
*   **Mutual Information Scoring:**
    *   `mi_scores` are calculated but not associated with feature names or displayed, making their output useless.
*   **Interaction Features:**
    *   Uses `Log GDP per capita` and `Healthy life expectancy at birth` which are incorrect column names again.
*   **Feature Importance:**
    *   Visualization is attempted, but `y=X.columns` in `sns.barplot` is incorrect, it should be `sns.barplot(x=importances, y=X.columns)`. The bar plot will likely fail or produce an empty plot. The `X` used here might be problematic if previous steps failed silently or modified it in an unexpected way.

**Code Quality:**
*   **Imports:** Mostly good.
*   **Structure:** The execution flow is generally logical, but the non-sequential numbering from the prompt is ignored (e.g., interaction features appear late in the prompt but early in the code).
*   **Readability:** Variable names are okay. Lack of comments makes it harder to understand the intent behind certain choices (e.g., why full `df` is scaled).
*   **Error Handling:** No robust error handling (e.g., `try-except` for file loading).
*   **Modularity:** Not very modular; variables like `X` and `df` are modified globally throughout the script, making it hard to debug or reuse parts.

**Completeness:**
*   Missing the critical step of handling missing values (NaNs).
*   Missing visualizations for PCA and t-SNE.
*   Missing explicit display of mutual information scores.
*   The `PolynomialFeatures` are applied to selected columns, not ""key happiness factors"" as generally defined in the prompt.
*   The requested different column name `Logged GDP per capita` often causes issues, the ground truth explicitly handles this.

In summary, the code makes a good attempt at the requirements but falls short on the details, specifically data preprocessing (missing values, correct column names), proper application of scaling/dimensionality reduction to only numerical features, and completeness in visualizations and output display for certain steps. The most significant issues are the incorrect column names and applying scaling/dimensionality reduction to the entire dataframe including non-numeric columns.",qwen3-30b-a3b,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])
usa_df = df[df['location'] == 'USA']
usa_df.set_index('date', inplace=True)

weekly_cases = usa_df['new_cases'].resample('W').sum()

decomposition = seasonal_decompose(weekly_cases, model='additi...",6,"The generated code addresses most of the core requirements of the task. It loads data, filters, resamples, performs decomposition, and conducts the ADF test with differencing, followed by ACF/PACF plots.

However, there are several areas for improvement:

1.  **Robustness/Error Handling**: The code lacks any error handling for file not found, missing columns, or empty dataframes after filtering, which are critical in real-world data science applications.
2.  **Decomposition Period**: The `seasonal_decompose` function is called without specifying the `period` argument. For weekly data, a period of 52 (weeks in a year) is typically appropriate for COVID-19 data. Without a specified period, `statsmodels` will try to infer it, which might not be accurate or it might default to 1, leading to a less meaningful decomposition.
3.  **Plotting**:
    *   The decomposition plot doesn't have a specific title or adjusted layout, making it less informative than it could be.
    *   The ACF/PACF plots automatically determine the number of lags. While often acceptable, explicitly setting a reasonable `lags` value (e.g., `lags=40` or `lags=50` for weekly data to show correlation over a year) or making it dynamic based on data length would be better.
4.  **Clarity for Stationarity**: The print statements for the ADF test are minimal. They should clearly state the p-value result against the significance level (e.g., 0.05) and explicitly conclude whether the series is stationary or not, both for the original and differenced series.
5.  **Output Presentation**: The overall output (plots, print statements) is functional but could be more verbose and clearly labeled for better understanding, as seen in the ground truth.
6.  **Warnings**: The ground truth code includes robust warning suppression for matplotlib/statsmodels, which often produce noisy warnings, enhancing user experience. The generated code lacks this.
7.  **`inplace=True` for `set_index`**: While `inplace=True` generally works, it's often considered good practice to avoid it in pandas chained operations or for better readability, opting for `usa_df = usa_df.set_index('date')`. This is a minor point, but worth noting.

In summary, the code provides a functional solution but lacks the robustness, detail, and user-friendliness expected from expert-level code for a benchmark task.",qwen3-30b-a3b,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
import numpy as np

df = pd.read_csv('winequality-red.csv')

df['pH_group'] = pd.cut(df['pH'], bins=[0, 3.2, 3.4, 10], labels=['Low', 'Medium', 'High'])

grouped = df.groupby('pH_group')['quality'].mean().reset_index()
print(grouped)

low_qual = df[df['pH_group'] == 'Low']['quality']
med_qual = df[df['pH_group'] == 'Medium']['quality']
high_qual = df[df['pH_group'] == 'High']['quality']

f_stat, p_value = stats.f_one...",6,"The generated code partially addresses the requirements but has significant shortcomings, particularly in the ANOVA and visualization sections.

**Strengths:**
*   **Correct Imports:** All necessary libraries are imported.
*   **pH Categorization:** Correctly categorizes pH groups, although the labels are simplified (""Low"", ""Medium"", ""High"") compared to the ground truth's more descriptive ones.
*   **Mean Calculation:** Correctly calculates the mean quality for each group.
*   **ANOVA Test:** Performs the ANOVA test, though it doesn't print a conclusion based on the p-value, which is a missed opportunity for interpreting the results.

**Weaknesses and Areas for Improvement:**
*   **Incomplete pH Labeling:** The `pd.cut` labels are ""Low"", ""Medium"", ""High"", which are less descriptive than the ground truth's labels and don't match the task's specified descriptions (e.g., ""Low pH (< 3.2)""). This affects interpretability.
*   **No Reporting of Averages:** The code calculates `grouped`, but it only `print(grouped)` without any descriptive text, which is less user-friendly than the ground truth's structured output.
*   **ANOVA Interpretation Missing:** While the p-value is printed, there's no interpretation or conclusion drawn from it, which is crucial for a statistical test.
*   **Incorrect Confidence Interval Calculation:** The error bars are calculated using `sem * 1.96`. This assumes a normal distribution and a very large sample size for the z-score of 1.96 to be directly applicable for a 95% CI. For smaller sample sizes (which individual pH groups might have), a t-distribution should be used, and the degrees of freedom should be considered, as done in the ground truth (`stats.t.interval`). This is a critical statistical error for displaying confidence intervals.
*   **Visualization Issues:**
    *   **Error Bar Type:** The error bars represent `sem * 1.96` (approximating a Z-interval), not the 95% confidence intervals based on the t-distribution as required for statistical rigor, especially with varying sample sizes.
    *   **No Y-axis Limit:** The plot doesn't set a `ylim`, which can make comparison difficult if the quality scale isn't immediately obvious.
    *   **No Value Labels on Bars:** The bars don't display the average quality values directly on them, reducing readability compared to the ground truth.
    *   **Basic Plotting:** The plot is very basic in terms of aesthetics (colors, grid, specific font sizes), which is acceptable for a minimal solution but less polished than the ground truth.
*   **Lack of Error Handling:** Doesn't include `try-except` for file loading or checks for empty groups before ANOVA, which could lead to crashes in real-world scenarios.",qwen3-30b-a3b,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
import numpy as np

data = pd.read_csv('synthetic_covid19_data.csv')
data['date'] = pd.to_datetime(data['date'])

selected_features =...",6,"The generated code addresses most of the requirements but has several issues that affect its correctness and adherence to best practices.

**Issues:**
1.  **Missing Value Handling for Features:** The `SimpleImputer` is applied to `X` (numerical features) *before* the 'continent' column is integrated. This means that if any of the specified `selected_features` (numerical ones) have missing values, they are imputed correctly. However, the imputation for the `target` variable is done separately, which is fine. The main issue here is the order of operations for `continent`.
2.  **Order of Scaling and Splitting:** The ground truth correctly splits the data *before* scaling numerical features. The generated code scales `X_combined` (which includes all features after imputation and encoding) *before* `train_test_split`. This is a significant data leakage issue. The `StandardScaler` learns the mean and standard deviation from the *entire* dataset (including the test set), which is then applied to the training data. This contaminates the training process with information from the test set, leading to an over-optimistic evaluation of the model's performance on unseen data.
3.  **One-Hot Encoding `drop_first=True`:** The ground truth uses `drop_first=True` for `OneHotEncoder` (implicitly via `pd.get_dummies`), which is a good practice to avoid multicollinearity. The generated code does not specify this, which can lead to redundant features. While not strictly ""wrong"" for model training, it's a best practice for feature engineering.
4.  **Handling of `continent` in Features:** The `selected_features` list initially excludes 'continent'. The 'continent' column is handled separately for one-hot encoding *after* the initial imputation of `selected_features`. This means the `continent` column itself was not part of the initial `X` that went through the `imputer_X`. While `continent` is categorical and wouldn't be imputed by a numerical imputer, it's generally cleaner to keep all features that need preprocessing together, and then apply specific preprocessing steps (like imputation for numerical, encoding for categorical). The ground truth correctly includes 'continent' in `features` from the start and then handles its categorical nature specifically.
5.  **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError`, which is good practice for robustness. The generated code lacks this.

**Areas for Improvement:**
*   **Correct Order of Preprocessing:** Split data into train/test sets *before* scaling.
*   **Integrated Feature Handling:** Ensure all features (numerical and categorical) are considered when setting up the initial `X` and `y` and then apply appropriate transformations sequentially (e.g., impute numerical, one-hot encode categorical, then scale).
*   **Robustness:** Add error handling (e.g., `try-except` for file loading).
*   **Best Practices:** Consider `drop_first=True` for one-hot encoding.",qwen3-30b-a3b,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"import pandas as pd
import scipy.stats as stats
import statsmodels.api as sm

df = pd.read_csv('synthetic_covid19_data.csv')

# 1) Handle missing values with median imputation
columns_to_impute = ['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']
for col in columns_to_impute:
    df[col].fillna(df[col].median(), inplace=True)

# 2) ANOVA for mean 'reproduction_rate' across continents (Asia, Europe, North America)
df_anova = df[df['continent'].isin(...",7,"The generated code correctly performs the requested hypothesis tests. It loads the data, handles missing values, and implements ANOVA, Pearson correlation, and a t-test as specified.

However, there are several areas for improvement:

1.  **Missing Imports**: `numpy` is implicitly needed for `df_latest['death_rate_ratio'].quantile(0.75)` to correctly handle potential NaNs (though `pandas` handles this gracefully too), and general numerical operations. `statsmodels` is imported but not used.
2.  **Robustness in Imputation**: The initial imputation does not print median values or confirm successful imputation, which can be helpful for debugging.
3.  **ANOVA Grouping Robustness**: The ANOVA grouping `[group['reproduction_rate'].values for name, group in df_anova.groupby('continent')]` could fail if a continent has no data, leading to an empty array for `f_oneway`. The ground truth includes checks for this.
4.  **T-test Robustness**:
    *   The creation of `df_latest` assumes `df.sort_values('date').groupby('country').tail(1)` will accurately get the *latest* entry. While `tail(1)` often works for sorted data, `df.groupby('country').last()` after sorting by date (as in the ground truth) is more explicit and robust for getting the *last* valid entry.
    *   The `death_rate_ratio` calculation `df_latest['total_deaths'] / df_latest['population']` might produce `inf` or `NaN` if `population` is 0 or NaN. The generated code does not explicitly handle these cases, which could lead to `quantile` or subsequent operations failing or producing unexpected results.
    *   The t-test groups `high_death` and `low_death` do not explicitly drop NaNs in `median_age` within those groups, although the initial imputation should have handled it. However, if any `median_age` values were `NaN` *after* imputation (e.g., if a column didn't exist or imputation method wasn't robust), this could cause issues.
    *   The `equal_var=False` is a good choice for the t-test, implying Welch's t-test which is generally more robust.
5.  **Output Clarity**: While the p-values and rejection statements are present, the null hypotheses are not explicitly stated for each test before the result, as required by the prompt's `For each test, state the null hypothesis...`. The current output `Null hypothesis: Mean reproduction rate is equal across Asia, Europe, North America. Reject null at =0.05?` is a single line, not a separate statement.
6.  **Missing `ALPHA` Constant**: Using a constant for the significance level (`ALPHA = 0.05`) makes the code more readable and maintainable.
7.  **General Prompt Adherence**: The prompt asks for ""only the Python code without explanations"". The generated code includes print statements that explain, which is acceptable but could be more concise if strictly adhering to ""only code"".

Overall, the core statistical tests are implemented correctly, but the code lacks some robustness checks and adherence to output formatting details presented in the ground truth.",qwen3-30b-a3b,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('clash_royale_cards.csv')
selected = df[['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']]
corr_matrix = selected.corr()
sns.heatmap(corr_matrix)
plt.show()
```",7,"The generated code correctly performs all the core requirements: loading the CSV, selecting the specified numerical columns, calculating the correlation matrix, and visualizing it with a heatmap. This demonstrates a good understanding of the task.

However, there are several areas for improvement that prevent it from being a perfect solution, especially when compared to the robustness and best practices of the ground truth:

1.  **Readability/Best Practices for Visualization:** The `sns.heatmap` call is basic. While it works, it lacks parameters for better visualization like `annot=True`, `cmap`, and `fmt` which are crucial for making the heatmap easily interpretable (showing values, better color representation).
2.  **Error Handling:** The code has no error handling for common issues like `FileNotFoundError` if the CSV doesn't exist, or `KeyError` if a specified column is misspelled or missing from the dataset. The ground truth includes robust `try-except` blocks.
3.  **Code Structure/Modularity:** The generated code is a simple script. The ground truth encapsulates the logic in a function (`analyze_clash_royale_data`) and uses a `if __name__ == ""__main__""` block, which is much better practice for reusability, testing, and preventing global variable pollution if this were part of a larger application.
4.  **Clarity/Comments:** While the task asked for code only, in a real-world scenario, adding comments explaining the steps would enhance clarity. The ground truth also uses descriptive variable names (`DATASET_PATH`, `NUMERICAL_COLUMNS`).
5.  **Informative Output:** The ground truth includes print statements to confirm loading, show selected data, and the correlation matrix, which is helpful for debugging and understanding the code's execution.

Overall, it's a functional solution that gets the job done, but it could be significantly improved in terms of robustness, best practices, and user experience for the visualization.",qwen3-30b-a3b,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"import pandas as pd

df = pd.read_csv('clash_royale_cards.csv')
grouped = df.groupby('rarity')[['elixirCost', 'Win Rate', 'Usage']].mean()
print(grouped)",9,"The generated code correctly solves the task by performing all required operations: loading the CSV, grouping by `rarity`, calculating the mean for specified columns, and printing the result. It uses proper imports and is concise. The only minor point of distinction from the ground truth is the lack of comments and the explicit definition of `file_path`, which while not strictly necessary for functionality, contributes to readability and maintainability in more complex scenarios. However, for this specific task, the code is highly effective and correct.",qwen3-30b-a3b,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

df = pd.read_csv('clash_royale_cards.csv')
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)
X = df[['elixirCost', 'Usage', 'rarity']]
y = df['is_high_win_rate']

preprocessor = ColumnTransformer(
    transformers=[
        ('rarity', OneHotEncoder(), ['rarity'])
    ],
    remainder='passthrough'
)

X_encoded =...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, creates the target column, selects features, applies one-hot encoding, and splits the data. The use of `ColumnTransformer` with `OneHotEncoder` is a robust and modern approach for preprocessing, which is excellent.

One minor point that prevents a perfect 10:
- The `try-except` block for `FileNotFoundError` is missing. While not strictly part of the data preparation logic, it's good practice for robust script execution when dealing with file I/O, as shown in the ground truth.
- The `X_encoded` variable is created as a NumPy array by `fit_transform` and thus loses column names. While this is acceptable for `sklearn` models, providing the user with features as a pandas DataFrame (e.g., by creating column names post-encoding) can be beneficial for interpretability, especially during exploration or debugging. However, for a classification task, a NumPy array is perfectly fine.

Overall, it's a very high-quality solution that demonstrates good understanding of data preprocessing pipelines.",qwen3-30b-a3b,LLM_judge_config
M25,medium,Classification,iris.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

df = pd.read_csv('iris.csv')
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = df['Species']
le = LabelEncoder()
y_encoded = le.fit_transform(y)
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)
knn = KNeighb...",9,"The generated code correctly addresses all requirements outlined in the task. It loads the data, defines features and target, encodes the target, splits the data, trains the KNN classifier, makes predictions, and reports the accuracy. The imports are correct and sufficient.

Minor areas for improvement include:
- The ground truth includes `stratify=y_encoded` in `train_test_split`, which is good practice for classification tasks with potentially imbalanced classes to ensure representative splits. While not strictly required by the prompt, it significantly improves the robustness of the evaluation. The generated code omits this.
- The ground truth includes error handling for `FileNotFoundError` for `iris.csv`. While not explicitly requested, it's a good practice for robust code.
- The print statement for accuracy is less descriptive than the ground truth's formatted string, which identifies what the reported number represents.

Overall, it's a very good solution that successfully accomplishes the core task.",qwen3-30b-a3b,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
import scipy.stats as stats

df = pd.read_csv('iris.csv')
versicolor = df[df['Species'] == 'Iris-versicolor']
virginica = df[df['Species'] == 'Iris-virginica']
petal_length_vers = versicolor['PetalLengthCm']
petal_length_virg = virginica['PetalLengthCm']
t_stat, p_value = stats.ttest_ind(petal_length_vers, petal_length_virg)
print(""Null hypothesis: There is no significant difference in mean PetalLengthCm between Iris-versicolor and Iris-virginica."")
print(""Alternati...",7,"The generated code correctly loads the dataset, filters the data for the specified species, performs the t-test, and interprets the result based on the p-value and alpha. It hits most of the key requirements.

However, there are a few areas for improvement:
- **Hypotheses Clarity:** The hypotheses are stated, but not as formally or explicitly as in the ground truth. Specifically, the ground truth clarifies the null hypothesis as `(_versicolor = _virginica)` and the alternative as `(_versicolor  _virginica)`, which is a good practice for academic rigor.
- **Output Formatting:** The output of the t-statistic and p-value is somewhat basic. The ground truth provides clearer labels and formatting (e.g., using f-strings with precision) for better readability.
- **Completeness on Interpretation:** While the code correctly states ""Reject"" or ""Fail to reject,"" it doesn't explicitly state the ""Conclusion"" as requested in the task (e.g., ""There is a statistically significant difference...""). This is a critical part of interpreting the statistical outcome.
- **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` and empty groups, which makes the code more robust. The generated code lacks this. While not explicitly asked for robust error handling, it's a good practice and present in the ground truth for comparison.

Overall, it's a functional solution but could benefit from more detailed output, more formal hypothesis statements, and a more comprehensive conclusion statement as requested by the task.",qwen3-30b-a3b,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('iris.csv')

features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

for feature in features:
    plt.figure()
    sns.boxplot(x='Species', y=feature, data=df)
    plt.title(f'Box plot of {feature}')
    plt.show()
```",8,"The generated code correctly addresses the core requirements of loading the dataset, creating 4 box plots for the specified features grouped by species, and adding titles. The imports are proper, and the structure is clean and concise.

Areas for improvement (to reach a score of 9 or 10):
- **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions, which is good practice for robust code. The generated code lacks this.
- **Plot Customization/Aesthetics:** While the generated plots are functional, the ground truth includes more aesthetic improvements like `figsize`, `xlabel`, `ylabel`, `grid`, and `tight_layout`, which make the plots more readable and professional. The task asked for ""appropriate titles"" and implied well-formed plots, which the ground truth delivers more comprehensively.
- **Configuration (Minor):** The ground truth uses constants for `DATASET_PATH`, `NUMERICAL_FEATURES`, and `GROUPING_COLUMN`, which makes the code more configurable and readable, especially for larger projects. This is a stylistic choice but generally considered good practice.",qwen3-30b-a3b,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

df = pd.read_csv('heart.csv')

features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']
X = df[features]
y = df['HeartDisease']

X = pd.get_dummies(X, columns=['Sex', 'ChestPainType'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

model = LogisticRegr...",9,"The generated code correctly implements all specified steps: loading data, selecting features, performing one-hot encoding for the specified categorical features, splitting data with the correct test size and random state, training a Logistic Regression model, and reporting the accuracy. The use of `pd.get_dummies` is an acceptable and concise way to handle one-hot encoding for this problem.

Areas for minor improvement:
- **Missing `OneHotEncoder` and `ColumnTransformer` imports from `sklearn.preprocessing` and `sklearn.compose` respectively**: While `pd.get_dummies` achieves the task, the problem statement specifically mentioned `sklearn.preprocessing` for one-hot encoding, hinting towards `OneHotEncoder` and `ColumnTransformer` (or a manual `fit_transform`). The ground truth uses these for a more robust and scalable pipeline approach.
- **No error handling for file not found**: The ground truth included a `try-except` block for `FileNotFoundError`, which is good practice.
- **Lack of Pipeline**: The ground truth uses `Pipeline` which is best practice for combining preprocessing and modeling steps, ensuring consistent application of transformations across train and test sets and preventing data leakage.
- **`max_iter` for `LogisticRegression`**: The ground truth explicitly set `max_iter=1000` for `LogisticRegression` to prevent potential convergence warnings, which is a good practice for robustness, although not strictly necessary for all datasets.
- **Formatted print output**: The ground truth provided a more user-friendly formatted output for the accuracy score.

Overall, the core functionality is excellent and solves the problem effectively. The minor points are about best practices and adherence to potential implicit requirements from the prompt's `sklearn` module hints.",qwen3-30b-a3b,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"import pandas as pd
import scipy.stats as stats

df = pd.read_csv('heart.csv')
male_cholesterol = df[df['Sex'] == 'M']['Cholesterol']
female_cholesterol = df[df['Sex'] == 'F']['Cholesterol']
t_stat, p_val = stats.ttest_ind(male_cholesterol, female_cholesterol)
print(""Null hypothesis: There is no significant difference in mean cholesterol levels between male and female patients."")
print(""Alternative hypothesis: There is a significant difference in mean cholesterol levels between male and female p...",7,"The generated code correctly performs the core task of loading data, separating by sex, and conducting an independent two-sample t-test on cholesterol levels. It also correctly states the hypotheses, prints the t-statistic and p-value, and interprets the result based on the significance level.

However, there are several areas for improvement:
- **Robustness:** The code assumes the file `heart.csv` exists and contains the `Sex` and `Cholesterol` columns. The ground truth includes error handling for `FileNotFoundError` and missing columns, which is crucial for real-world scenarios.
- **Handling Missing Values:** The generated code does not explicitly handle missing values (`.dropna()`) in the `Cholesterol` column for either group, which could lead to errors if NaNs are present in the dataset and `ttest_ind` does not implicitly handle them in the way intended. While `ttest_ind` *does* typically handle NaNs by ignoring them by default, explicit handling is good practice and reduces ambiguity.
- **`equal_var` parameter in `ttest_ind`:** The generated code uses the default `equal_var=True` for `stats.ttest_ind`, which assumes equal population variances. The ground truth explicitly uses `equal_var=False` (Welch's t-test), which is generally more robust when variance equality is not known or tested. While not strictly ""incorrect"" if the assumption holds, specifying `equal_var=False` often leads to a more defensively programmed solution for t-tests unless equal variances are proven.
- **Code Structure and Reusability:** The generated code is a script, while the ground truth wraps the logic in a function, making it more reusable and testable. Using a function also allows for parameters like `file_path` and `alpha` to be easily modified.
- **Output Clarity:** While the output is functional, the ground truth's output is more detailed and user-friendly, clearly labeling each section (Hypotheses, T-test Results, Interpretation).
- **Completeness of Interpretation:** The interpretation ""Reject the null hypothesis"" is correct, but adding a concluding sentence about what that rejection means (e.g., ""There is a statistically significant difference..."") would make it more complete, as done in the ground truth.",qwen3-30b-a3b,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"import pandas as pd
import numpy as np

df = pd.read_csv('winequality-red.csv')

def find_outliers(column):
    Q1 = column.quantile(0.25)
    Q3 = column.quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return lower, upper, ((column < lower) | (column > upper)).sum()

for col in ['volatile acidity', 'residual sugar']:
    lower, upper, count = find_outliers(df[col])
    print(f""Outliers in {col}: {count}"")

df_winsorized = df.copy()

for col in ['volati...",8,"The generated code correctly addresses the core requirements of the task: identifying outliers using IQR, reporting their count, performing winsorization, and comparing statistics before and after treatment.

**Strong points:**
- Correctly identifies outliers and calculates IQR bounds.
- Effectively uses `np.clip` for winsorization, which is a concise and appropriate method.
- Clearly states the original vs. winsorized means and standard deviations.
- Uses functions (`find_outliers`) for better organization, although it recalculates IQR internally.

**Areas for improvement:**
- **Redundant IQR calculation:** The `find_outliers` function calculates `lower` and `upper` bounds, but then these are recalculated in a separate loop for winsorization. The bounds should ideally be calculated once and then reused or passed.
- **Reporting clarity:** The reporting for outliers and the comparison of statistics is more concise in the generated code than the ground truth. While not strictly incorrect, the ground truth provides more detailed output (e.g., Q1, Q3, IQR, bounds, individual changes) which can be more informative for someone analyzing outlier treatment.
- **Error handling:** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code lacks this.
- **Output formatting:** The output for statistics comparison could be slightly more structured or descriptive.

Overall, it's a solid solution that completes the core task effectively and efficiently. The improvements are mainly around robustness, reusability of calculated values, and more detailed output as present in the ground truth.",qwen3-30b-a3b,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('heart.csv')
sns.countplot(x='ChestPainType', hue='HeartDisease', data=df)
plt.title('Heart Disease Count by Chest Pain Type')
plt.xlabel('Chest Pain Type')
plt.ylabel('Count')
plt.legend(title='Heart Disease')
plt.show()
```",8,"The generated code correctly loads the dataset and creates a grouped bar chart as requested. It uses `seaborn.countplot` effectively to show the counts of `HeartDisease` (0 and 1) for each `ChestPainType`, which aligns with the requirements. It also includes a title, axis labels, and a legend.

Strong points:
- Correctly uses `pd.read_csv` to load data.
- Correctly uses `sns.countplot` for creating the grouped bar chart.
- Includes all required plot elements: title, x-label, y-label, and legend.

Areas for improvement to achieve a higher score (9 or 10):
- **Robustness:** The code lacks error handling for `FileNotFoundError` or other issues during CSV loading, which is present in the ground truth.
- **Clarity of Legend:** The `HeartDisease` legend shows 0 and 1. While correct, mapping these to more descriptive labels like 'No Heart Disease' and 'Heart Disease' (as done in the ground truth) would improve readability.
- **Function Encapsulation:** The ground truth encapsulates the plotting logic within a function, making it reusable and easier to manage, whereas the generated code is a script. While not explicitly asked for, it's good practice for benchmark solutions.
- **Categorical Order:** The ground truth explicitly orders `ChestPainType` categories, ensuring consistent plotting order (`TA`, `ATA`, `NAP`, `ASY`). While `countplot` might often sort them alphabetically or by appearance, explicit ordering is more robust.
- **Plot size/layout:** No `plt.figure(figsize=...)` or `plt.tight_layout()` was used, which could sometimes lead to cramped plots, although it worked fine for this specific case.",qwen3-30b-a3b,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"import pandas as pd

df = pd.read_csv('spotify_2023.csv')

df['in_shazam_charts'] = df['in_shazam_charts'].str.replace(',', '')
df['in_shazam_charts'] = pd.to_numeric(df['in_shazam_charts'], errors='coerce')
nan_count = df['in_shazam_charts'].isna().sum()
print(f""Number of NaNs in 'in_shazam_charts': {nan_count}"")

grouped = df.groupby(['key', 'mode'])
result = grouped[['danceability_%', 'valence_%']].mean()
print(result)",9,"The generated code correctly performs all requested operations: loading the CSV, cleaning and converting the `in_shazam_charts` column (including reporting NaNs), grouping by `key` and `mode`, and calculating the average `danceability_%` and `valence_%`.

**Strong Points:**
- Correct use of `pd.read_csv` for loading data.
- Accurate cleaning of `in_shazam_charts` by removing commas and coercing to numeric, with proper error handling (`errors='coerce'`).
- Correctly identifies and prints the count of NaNs after coercion.
- Precisely groups data by `key` and `mode` and calculates the mean of the specified columns.
- The output format for the final result is clean and readable.

**Minor Area for Improvement:**
- While the `str.replace(',', '')` works for numeric conversion, adding `.astype(str)` before `.str.replace()` (e.g., `df['in_shazam_charts'].astype(str).str.replace(',', '')`) would make it more robust against potential non-string types in the column, preventing a potential AttributeError if the column were already numeric or mixed with non-string, non-numeric values that don't have a `.str` accessor. However, for a CSV where numeric values are read as objects (strings), the current approach often works.
- The prompt asked to ""display"" the averages. While printing `result` does display it, adding a descriptive print statement before it (like the ground truth did) would enhance user readability. This is a very minor point and doesn't affect correctness.",qwen3-30b-a3b,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

df = pd.read_csv('spotify_2023.csv')
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['in_spotify_playlists', 'energy_%', 'streams'])
X = df[['in_spotify_playlists', 'energy_%']]
y = df['streams']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, ran...",9,"The generated code correctly addresses almost all requirements of the task. It successfully loads the dataset, handles the `streams` column to ensure it's numeric and drops NaNs from relevant columns, selects the correct features and target, splits the data, trains a Linear Regression model, and reports the R-squared and MSE. The imports are proper and the code structure is clean and concise.

The only minor improvements compared to the ground truth are:
1.  **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code assumes the file will always be present, which is acceptable for a benchmark setting but less robust in a production environment.
2.  **Output Formatting and Messaging:** The ground truth includes more descriptive print statements (e.g., confirming dataset loaded, reporting dropped rows, showing shape of X and y, confirming model training, and detailed output for evaluation metrics). The generated code's output is more terse (`R-squared: {r2}`, `MSE: {mse}`). While the core values are reported, the additional context from the ground truth is helpful.

These points are minor and don't detract significantly from the core functionality and correctness of the solution for the given task.",qwen3-30b-a3b,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

df = pd.read_csv('spotify_2023.csv')
df['release_date'] = pd.to_datetime(df[['released_year', 'released_month', 'released_day']], errors='coerce')
fixed_date = pd.to_datetime('2023-12-31')
df['days_since_release'] = (fixed_date - df['release_date']).dt.days
print(df[['track_name', 'release_date', 'days_since_release']].head(5))
```",9,"The generated code correctly performs all the requested tasks: loading the dataset, combining the date components into a single datetime column named `release_date` (crucially handling errors with `errors='coerce'`), calculating `days_since_release` as the difference in days from '2023-12-31', and displaying the requested columns for the first 5 tracks. The solution is concise and efficient.

One minor point for improvement, although not strictly a requirement given the prompt's context, is that `df['days_since_release']` will be a float column if there are `NaT` values in `release_date` due to `dt.days` returning `NaN`. The ground truth explicitly casts it to `Int64` to maintain an integer type while allowing for NaNs, which is technically more robust if the requirement was strictly for an integer *type* rather than just integer *values* where non-NaN. However, `dt.days` on a Timedelta series essentially gives integer days, so the result is correct for valid dates. Given the prompt's simplicity, this is a very strong solution.",qwen3-30b-a3b,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"import pandas as pd

df = pd.read_csv('youtube_trendings.csv')
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)
df['Category'] = df['Category'].fillna('Unknown')
grouped = df.groupby('Category').sum()
print(grouped)",7,"The generated code correctly performs the core data cleaning and aggregation tasks as requested. It successfully loads the CSV, cleans ""Video views"" and ""Likes"" by removing commas, converting to numeric, and filling NaNs with 0. It also correctly fills missing ""Category"" values with 'Unknown' and then groups by ""Category"" to calculate the sum of the cleaned numeric columns.

However, there are a few areas for improvement:
1.  **Implicit Conversion**: The `Video views` and `Likes` cleaning lines implicitly assume that `df['Video views']` and `df['Likes']` are already string-like types. While `str.replace` usually handles this okay on Series, explicitly converting to string with `.astype(str)` before `.str.replace` is more robust, especially if a column might contain mixed types or pure numeric at source (though less common for these types of columns in CSVs).
2.  **Display Specific Columns**: The task specifically asked to ""calculate and display the sum of cleaned `Video views` and the sum of cleaned `Likes`."" The generated code uses `grouped = df.groupby('Category').sum()` which will sum all numeric columns, not just `Video views` and `Likes`. While `Likes` and `Video views` are the only *relevant* numeric columns after cleaning, it's better practice or more robust to explicitly select the columns to sum, e.g., `df.groupby('Category')[['Video views', 'Likes']].sum()`.
3.  **No Error Handling**: The ground truth includes robust error handling for `FileNotFoundError` and checks for the existence of columns before attempting operations on them. The generated code assumes the file exists and all specified columns are present, which can lead to runtime errors if the dataset format deviates. While the prompt didn't explicitly ask for error handling, it's a critical aspect of ""expert"" or ""production-ready"" code. For a benchmark, this level of robustness is often desired.
4.  **Clarity of Output**: The prompt asked to ""display the sum of cleaned Video views and the sum of cleaned Likes"". While `print(grouped)` does show this, adding a descriptive header (like in the ground truth) would make the output more user-friendly.

Overall, it's a good attempt that gets the core logic right but could benefit from more robust error handling and precise column selection for aggregation.",qwen3-30b-a3b,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

df = pd.read_csv('youtube_trendings.csv')

df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)
df['Category'] = df['Category'].fillna('Unknown')

music_likes = df[df['Category'] == 'Music']['Likes']
sports_likes = df[df['Category'] == 'Sports']['Likes']

t_stat, p_val = ttest_ind(music_likes, sports_likes)

print(""Null Hypothesis: The mean number of likes for Music and Sports categories are equa...",7,"The generated code correctly performs the core task of cleaning the `Likes` column, filling missing `Category` values, creating the groups, and performing the independent two-sample t-test. It also correctly states the hypotheses, prints the statistics, and interprets the result based on alpha.

However, it falls short in a few areas compared to the requirements and robustness of the ground truth:

1.  **Readability/Clarity of Cleaning:** While functional, the `Likes` cleaning `pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)` is a single line, which is less explicit about the step-by-step cleaning process (remove commas, then convert, then fill NaNs) compared to the ground truth. The ground truth also explicitly handles `astype(str)` for `Likes` before `.str.replace`, which is safer if the column isn't guaranteed to be string-like initially, although in `youtube_trendings.csv` it often is.
2.  **Robustness (File Not Found):** The generated code doesn't include error handling for `FileNotFoundError` when loading the CSV, which is a good practice for user-facing code and was present in the ground truth.
3.  **Completeness of Output/Interpretation:** The interpretation of the p-value is terse (""Reject the null hypothesis."" vs. ""Fail to reject the null hypothesis.""). The ground truth provides a more detailed conclusion linking back to the problem statement (e.g., ""There is a statistically significant difference...""). The ground truth also prints the t-statistic and p-value with formatting (`.4f`), which is a minor but good detail.
4.  **Implicit `equal_var`:** The `ttest_ind` call in the generated code uses the default `equal_var=True`, assuming equal variances. While this might be acceptable depending on the data, the ground truth explicitly uses `equal_var=False` (Welch's t-test), which is often a safer and more robust choice as it does not assume equal population variances and was specifically mentioned in the ground truth requirements (though not explicitly in the prompt's steps, it was part of the provided ground truth as context). Not specifying `equal_var=False` is a subtle difference that can affect results.
5.  **Intermediate Checks:** The ground truth includes checks for `len(music_likes) < 2` or `len(sports_likes) < 2`, which is crucial for robust statistical analysis to ensure that a t-test can actually be performed. This is missing in the generated code.

Overall, it's a good attempt that gets the core logic right but could benefit from more robustness, detailed output, and adherence to best practices in statistical testing.",qwen3-30b-a3b,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"import pandas as pd

df = pd.read_csv('youtube_trendings.csv')

df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)
df['Dislikes'] = pd.to_numeric(df['Dislikes'].str.replace(',', ''), errors='coerce').fillna(0)

df['TotalEngagement'] = df['Likes'] + df['Dislikes']
df['LikeDislikeRatio'] = df['Likes'] / (df['Dislikes'] + 1)

print(df[['Video', 'TotalEngagement', 'LikeDislikeRatio']])",8,"The generated code correctly performs all the required steps: loading the data, cleaning 'Likes' and 'Dislikes' by removing commas, converting to numeric, and filling NaNs with 0 (all in a concise line for each column), creating `TotalEngagement`, creating `LikeDislikeRatio`, and displaying the specified columns.

**Strengths:**
*   **Correctness:** All calculations and data manipulations are accurate according to the prompt.
*   **Conciseness:** The cleaning steps for `Likes` and `Dislikes` are very efficient and well-grouped, showcasing good use of pandas chaining.
*   **Completeness:** All five requirements from the prompt are met.

**Areas for Improvement (minor):**
*   **Error Handling for File Loading:** The ground truth includes robust error handling for `FileNotFoundError` when loading the dataset, which is good practice for real-world applications. The generated code assumes the file exists. While the prompt didn't *explicitly* require error handling, it's a mark of higher code quality.
*   **Type Conversion for Cleaning:** The ground truth explicitly uses `astype(str)` before `str.replace(',', '')` for `Likes` and `Dislikes`. While `pd.to_numeric` often handles this implicitly when `errors='coerce'` is used, being explicit about converting to string first makes the intention clearer and can prevent issues if the column contained mixed types that weren't immediately coercible to string but might have been to a number indirectly. In this specific case, `pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce')` works because `str.replace` implicitly attempts string conversion if the Series isn't already string-like. The ground truth's `df['Likes'].astype(str).str.replace(',', '')` is slightly more resilient.
*   **Comments/Readability:** While the code is short and clear enough, adding comments for each step (similar to the ground truth) would enhance readability for more complex tasks. However, for this task, the code's self-explanatory nature is largely sufficient.

Overall, it's a very solid solution that gets the job done effectively and efficiently. The missing error handling is the primary reason it's not a 9 or 10, as robust file loading is generally critical for data science scripts.",qwen3-30b-a3b,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"import pandas as pd

df = pd.read_csv('mobiles_2025.csv')

df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '', regex=False).str.replace(',', '', regex=False)
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'], errors='coerce')

df['Screen Size'] = df['Screen Size'].str.replace(' inches', '', regex=False)
df['Screen Size'] = pd.to_numeric(df['Screen Size'], errors='coerce')

grouped = df.groupby(['Company Name', 'Launched Year']).mean()

print(groupe...",8,"The generated code correctly performs all the core tasks: loading the CSV, cleaning both specified columns (handling errors via `errors='coerce'`), grouping by the correct columns, and calculating the mean of the cleaned columns. It is concise and efficient.

Areas for improvement (preventing a higher score):
1.  **Readability/Explicit Naming**: The cleaned columns overwrite the original columns (`Launched Price (USA)` and `Screen Size`). While this works, creating new columns like `Cleaned Price (USD)` and `Cleaned Screen Size (inches)` (as in the ground truth) improves readability and maintains the original data if needed for other operations.
2.  **Error Handling for File Loading**: The code assumes the file `mobiles_2025.csv` will always be found. The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions, which is good practice for robust code.
3.  **Intermediate Output/Feedback**: The ground truth provides print statements for successful loading, the number of NaNs introduced, and the final aggregated data, which gives the user more feedback during execution. The generated code only prints the final `grouped` DataFrame.

Overall, it's a very good, functional solution for the task. The minor improvements are related to robustness and user experience rather than core correctness.",qwen3-30b-a3b,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

def clean_camera(text):
    match = re.search(r'\d+', text)
    return int(match.group()) if match else 0

df = pd.read_csv('mobiles_2025.csv')
df['Cleaned Front Camera'] = df['Front Camera'].apply(clean_camera)
df['Cleaned Back Camera'] = df['Back Camera'].apply(clean_camera)
df['Total Camera MP'] = df['Cleaned Front Camera'] + df['Cleaned Back Camera']
print(df[['Model Name', 'Cleaned Front Camera', 'Cleaned Back Camera', 'Total Camera MP']].head(10))
`...",8,"The generated code correctly implements the core logic required by the task: loading the CSV, cleaning camera specifications by extracting the first digit sequence, creating the 'Total Camera MP' feature, and displaying the specified columns for the first 10 rows. The `clean_camera` function is concise and effective.

Areas for improvement to reach a perfect score:
1.  **Robustness of `clean_camera`:** The function does not explicitly handle `NaN` or non-string inputs from the DataFrame. While `re.search` on `NaN` often just returns `None`, an explicit check like `if pd.isna(text): return 0` (as in the ground truth) or converting to string (`str(text)`) before regex matching would make it more robust for varied data types in the input columns.
2.  **Error Handling for CSV Load:** The generated code lacks a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for real-world applications.
3.  **Readability/Comments:** While the code is short, adding comments, especially for the `clean_camera` function and different steps, would improve its readability and maintainability.
4.  **Display Formatting:** Using `.to_string()` for `print` when displaying the DataFrame (as in the ground truth) often provides better console formatting for larger DataFrames, preventing truncation. For 10 rows, it's less critical, but it's a good practice.",qwen3-30b-a3b,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

df = pd.read_csv('mobiles_2025.csv')

df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
df['Screen Size'] = pd.to_numeric(df['Screen Size'].str.replace(' inches', ''), errors='coerce')
df['...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, cleans the specified columns by removing units and converting them to numeric types (handling errors by coercing to NaN and then dropping rows), defines features and target, splits the data, trains a Linear Regression model, and evaluates it using R-squared and MAE. The imports are correct and standard practices are followed.

Strong points:
- All required libraries are imported.
- Column cleaning is done correctly using `str.replace` and `pd.to_numeric` with `errors='coerce'`.
- NaN values are handled by dropping rows after cleaning specific columns.
- Feature and target variables are correctly defined.
- Data splitting, model training, and evaluation are performed as specified.
- The output format for R-squared and MAE is clear.

Minor areas for improvement (not critical for current task, but good practice):
- The `try-except` block for `pd.read_csv` present in the ground truth code is missing. While not explicitly asked for, it makes the code more robust against missing files.
- The output messages could be slightly more descriptive (e.g., ""Model Evaluation Results:"" as in the ground truth).
- `str.strip()` after `str.replace` can sometimes be useful if there might be leading/trailing spaces left from the original string before conversion, though for these specific patterns, it's less critical.",qwen3-30b-a3b,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"import pandas as pd
import scipy.stats as stats
import numpy as np

df = pd.read_csv('titanic.csv')

# 1. Chi-square test
contingency_table = pd.crosstab(df['Pclass'], df['Survived'])
chi2, p_chi2, dof, expected = stats.chi2_contingency(contingency_table)
n = len(df)
k = min(contingency_table.shape)
cramers_v = np.sqrt(chi2 / (n * (k - 1)))

# 2. T-test
survived_age = df[df['Survived'] == 1]['Age'].dropna()
not_survived_age = df[df['Survived'] == 0]['Age'].dropna()
t_stat, p_t = stats.ttest_ind(...",7,"The generated code correctly performs the three requested statistical tests (Chi-square, T-test, ANOVA) and calculates their respective effect sizes. It also correctly interprets the results based on the p-value and a defined alpha level.

However, there are several areas for improvement that prevent it from achieving a higher score:

1.  **Robustness and Error Handling (Missing):** The generated code lacks any error handling for file not found (e.g., `try-except` for `pd.read_csv`). It assumes the CSV file will always be present and correctly formatted. The ground truth includes this.
2.  **Missing Value Handling Consistency:** While `.dropna()` is used for Age and Fare, it doesn't handle cases where an entire group might become empty after dropping NAs, which could lead to errors in subsequent calculations (e.g., `mean()` or `std()` on empty series). The ground truth code checks for empty groups before performing calculations.
3.  **Statistical Assumptions (Missing):** For the T-test and ANOVA, critical assumptions (like homogeneity of variances) are not tested (e.g., Levene's test). The generated code doesn't adjust the T-test for unequal variances (e.g., `equal_var=False` equivalent to Welch's t-test), which is a best practice. The ground truth includes Levene's test and adapts the t-test accordingly.
4.  **Clarity of Output:** While the output correctly states p-values and conclusions, it lacks some of the descriptive statistics (like mean ages/fares per group) that aid in interpretation and are present in the ground truth.
5.  **Cramer's V Calculation:** The formula used for Cramer's V `np.sqrt(chi2 / (n * (k - 1)))` is correct for a 2xK or Kx2 table (where k is the number of rows or columns, whichever is smaller). However, the general formula for any size RxC table is `np.sqrt(chi2 / (n * min(R-1, C-1)))`. The ground truth uses the more general and robust `min(contingency_table.shape) - 1`. While it might yield the same result for a 2x3 table, it's less robust.
6.  **Eta-squared calculation:** The calculation of `ss_between` and `ss_within` for eta-squared is correct, but `(ss_between + ss_within)` is generally `ss_total`. It's equivalent, but `etas_squared = ss_between / ss_total` is clearer.
7.  **Interpretation Detail:** The conclusions are very concise. The ground truth provides slightly more detailed interpretations, e.g., mentioning that post-hoc tests are needed for ANOVA.
8.  **Redundant Imports:** While `scipy.stats` is imported as `stats`, `chi2_contingency`, `ttest_ind`, and `f_oneway` are still called as `stats.chi2_contingency` etc., which is fine. The ground truth does `from scipy.stats import ...` which is also fine. No major issue here.

Overall, the code is functional and addresses the core requirements. However, the lack of robustness, assumption checks, and slightly less detailed output makes it less complete and statistically rigorous than the ground truth.",qwen3-30b-a3b,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('imdb_top_250.csv')
grouped = df.groupby('Year').mean()
plt.plot(grouped.index, grouped['Rating'])
plt.title('Average Rating Over Time')
plt.xlabel('Year')
plt.ylabel('Average Rating')
plt.show()",6,"The generated code correctly loads the CSV, groups by `Year`, calculates the mean of `Rating` (and implicitly `Runtime`, though not explicitly referenced for `Runtime`), and plots the average `Rating` over `Year` with a title and labels. However, there are several areas for improvement.

**Areas for Improvement:**
1.  **Implicit Calculation for `Runtime` and `Rating`:** The line `grouped = df.groupby('Year').mean()` calculates the mean for all numeric columns, including `Rating` and `Runtime`. While `Rating` is used for plotting, the requirement to *explicitly* calculate and potentially store ""average `Runtime`"" per year is addressed implicitly, but not clearly reflected in the `grouped` variable's usage afterwards for `Runtime`. It would be better to use `.agg()` to explicitly select and name the aggregated columns as per the ground truth's `Average_Rating` and `Average_Runtime`.
2.  **Robustness (Data Cleaning):** The generated code assumes 'Year', 'Rating', and 'Runtime' columns are perfectly clean and numeric. The ground truth code includes crucial data cleaning steps (e.g., handling non-numeric 'Year' entries like ""(2000)"", cleaning 'Runtime' to remove ' min' and convert to int, and using `pd.to_numeric` with `errors='coerce'` for 'Rating'). This lack of robustness makes the generated code fragile if the input CSV is not perfectly formatted. This is a significant omission for a real-world data science task.
3.  **Completeness (Runtime Plotting):** Although the task stated ""For each year, calculate the average `Rating` and the average `Runtime`"", the generated code only plots the `Rating`. While it *does* calculate the average `Runtime` implicitly, it doesn't *use* it or mention it. The prompt asked to calculate both, and while only `Rating` was required for plotting, a more complete solution might have explicitly shown that both were calculated, or at least used `.agg()` for clarity as mentioned in point 1.
4.  **Error Handling:** There is no error handling (e.g., for `FileNotFoundError`) which the ground truth code includes.
5.  **Function Encapsulation:** The ground truth code encapsulates the logic within a function, which is good practice for reusability and modularity. The generated code is a global script.",qwen3-30b-a3b,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"import pandas as pd
import scipy.stats as stats

df = pd.read_csv('imdb_top_250.csv')

group1 = df[df['Year'] < 1990]
group2 = df[df['Year'] >= 1990]

print(""Null hypothesis: There is no significant difference in the mean rating between movies released before 1990 and those released in 1990 or later."")
print(""Alternative hypothesis: There is a significant difference in the mean rating between the two groups."")

t_stat, p_val = stats.ttest_ind(group1['Rating'], group2['Rating'])

print(""T-statist...",8,"The generated code correctly performs all the required steps: loading the data, creating the two groups, stating the hypotheses, performing the t-test, printing the t-statistic and p-value, and interpreting the result based on the alpha level.

Strong points:
- Correctly uses pandas to load data and filter groups.
- Correctly uses `scipy.stats.ttest_ind` for the independent two-sample t-test.
- Clearly states the null and alternative hypotheses.
- The interpretation of the p-value against the alpha level is correct.

Areas for improvement (preventing a higher score):
- **Robustness/Error Handling:** The ground truth code includes error handling for `FileNotFoundError` and checks for sufficient data in groups, which is a good practice for real-world scripts. The generated code assumes the file exists and always has enough data.
- **Data Type Conversion:** The ground truth handles potential `Year` and `Rating` column data type issues (e.g., if they are not numeric from the CSV, or contain errors). The generated code assumes they are clean numeric types. While `imdb_top_250.csv` typically has clean data, robust code would explicitly convert and handle errors (`pd.to_numeric`, `dropna`).
- **Hypothesis Wording:** While the hypotheses are stated correctly, they could be slightly more formal and precise, as seen in the ground truth (e.g., specifying ""mean 'Rating'""). However, for the core requirement, it's acceptable.
- **Output Clarity:** The ground truth provides more descriptive print statements (e.g., ""--- Hypotheses ---"", ""--- T-Test Results ---""). This improves readability.

Overall, it's a very good attempt that meets the core functional requirements. The missing robustness and explicit data type handling are common oversights in simpler benchmark answers but are crucial for production-ready code.",qwen3-30b-a3b,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"import pandas as pd

df = pd.read_csv('imdb_top_250.csv')
df['Decade'] = df['Year'].astype(int).apply(lambda x: f""{x - x % 10}s"")
print(df[['Title', 'Year', 'Decade']].head(10))",9,"The generated code correctly performs all the requested actions: it loads the CSV, creates the `Decade` column accurately using a lambda function, and displays the specified columns for the first 10 entries. The `lambda x: f""{x - x % 10}s""` approach is a concise and effective way to calculate the decade.

The only minor area for improvement is that it lacks robust error handling for file not found or missing columns, which the ground truth includes. However, for a direct task solution without explicit instructions for error handling, it's very good. The display format `print(df[['Title', 'Year', 'Decade']].head(10))` is also correct and aligns with the request. The ground truth's use of `to_string(index=False)` provides a slightly cleaner output by removing the DataFrame index, but this is a minor formatting detail.",qwen3-30b-a3b,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"import pandas as pd

df = pd.read_csv('penguins.csv')

numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numerical_cols:
    df[col] = df[col].fillna(df[col].mean())

df['sex'] = df['sex'].fillna(df['sex'].mode()[0])

grouped = df.groupby(['species', 'sex'])['body_mass_g'].mean().reset_index()
print(grouped)",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, imputes numerical columns with their means and the 'sex' column with its mode, groups the data by `species` and `sex`, and calculates the average `body_mass_g`.

Minor points for improvement (not critical enough to reduce the score significantly for this task, but good practice):
1.  **Error Handling**: The ground truth code includes basic error handling for `FileNotFoundError` and `KeyError`, which is good practice for robust scripts. The generated code lacks this, which could be beneficial in a production environment.
2.  **Explicit Display**: The task asks to ""display the average `body_mass_g`"", and while `print(grouped)` does display it, `print(average_body_mass_by_group)` in the ground truth, which implicitly would print a Series or DataFrame without the `reset_index()`, is often more direct for a simple aggregated result. `reset_index()` transforms the grouped Series back into a DataFrame, which is fine, but not strictly necessary for just displaying the aggregated mean. For this specific task, either is acceptable for display.

Overall, the core logic is perfectly implemented and efficient.",qwen3-30b-a3b,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"import pandas as pd
import scipy.stats as stats

df = pd.read_csv('penguins.csv')
df_filtered = df[df['species'].isin(['Adelie', 'Gentoo'])]
df_filtered = df_filtered.dropna(subset=['flipper_length_mm'])
adelie = df_filtered[df_filtered['species'] == 'Adelie']['flipper_length_mm']
gentoo = df_filtered[df_filtered['species'] == 'Gentoo']['flipper_length_mm']
t_stat, p_value = stats.ttest_ind(adelie, gentoo)
print(""Null hypothesis: The mean flipper length of Adelie and Gentoo penguins are equal."")...",8,"The generated code correctly performs the core task of loading data, filtering, performing a t-test, and interpreting the results.

**Strong Points:**
*   Correctly loads the CSV and filters for 'Adelie' and 'Gentoo' species.
*   Correctly removes NA values for `flipper_length_mm` (though it missed `species` for NA removal, as per the explicit ground truth requirement, it's less critical as `species` should not be NA after filtering).
*   Correctly separates data for the two groups.
*   Correctly performs the independent two-sample t-test using `scipy.stats.ttest_ind`.
*   States the null and alternative hypotheses clearly.
*   Prints the t-statistic and p-value.
*   Correctly interprets the p-value against `alpha = 0.05` to decide whether to reject the null hypothesis.

**Areas for Improvement (to reach a perfect score):**
*   **NA Handling for 'species' column:** The prompt explicitly stated to remove NAs where `flipper_length_mm` *or `species`* is missing. The generated code only `dropna` on `flipper_length_mm`. While redundant for 'species' after filtering, following the prompt strictly is important for a perfect score.
*   **Error Handling:** The ground truth code includes robust error handling for `FileNotFoundError` when loading the dataset and also checks if there's enough data in each group for the t-test. The generated code lacks these checks, which could lead to errors if the file is missing or data is insufficient.
*   **Explicitness on `equal_var`:** The `ttest_ind` function has an `equal_var` parameter which defaults to `True`. While often acceptable, explicitly setting it (e.g., to `False` for Welch's t-test which is more robust when variances are unknown/unequal, as done in the ground truth) is good practice. The problem did not specify, so default isn't wrong, but explicit choice shows deeper understanding.
*   **Clarity and Structure:** The ground truth code used comments and print statements to clearly delineate each step, which improves readability and understanding of the code's flow, aligning with a ""benchmark"" style solution. The generated code is more concise but less pedagogical in its presentation.",qwen3-30b-a3b,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

df = pd.read_csv('penguins.csv')
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]
y = df['species']
df_clean = df.dropna(subset=['species', 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])
X = df_clean[['culmen_l...",9,"The generated code successfully accomplishes all the core requirements of the task: loading data, selecting features and target, handling NAs (though with a slight inefficiency), encoding the target, splitting the data, training a Logistic Regression model with `max_iter=1000`, and reporting the accuracy.

Strengths:
- **Correctness**: All functional requirements are met; the code produces the correct output.
- **Completeness**: All steps outlined in the task are addressed.
- **Proper Imports**: Necessary libraries are correctly imported.
- **Logistic Regression Configuration**: `max_iter=1000` is correctly set.
- **Data Split**: Correctly uses `test_size=0.2` and `random_state=42`.

Areas for minor improvement:
- **Redundancy in feature/target assignment**: The lines `X = df[['culmen_length_mm', ...]]` and `y = df['species']` before `df_clean` are not strictly necessary as `X` and `y` are then re-assigned from `df_clean`. While not an error, it adds unnecessary operations. The features and target should be extracted *after* cleaning the DataFrame.
- **Implicit Column Check**: The code assumes all specified columns exist. The ground truth includes a check for missing columns, which adds robustness.
- **No Error Handling for File Not Found**: The code would crash if `penguins.csv` was not present, unlike the ground truth which includes a `try-except` block.
- **Readability/Clarity**: The ground truth is more explicit about each step with comments and print statements, which is good practice for more complex scripts or for explainability. The generated code is concise but lacks these informative outputs.

Overall, it's a very good solution that directly addresses the prompt. The minor points are more about best practices and robustness rather than functional correctness for this specific task.",qwen3-30b-a3b,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('penguins.csv')
df = df.dropna(subset=['body_mass_g', 'species'])
sns.barplot(x='species', y='body_mass_g', data=df)
plt.title('Average Body Mass by Species')
plt.xlabel('Species')
plt.ylabel('Average Body Mass (g)')
plt.show()
```",6,"The generated code correctly performs the data loading and NA handling for `body_mass_g` and `species`. It also generates a bar plot with a title and labeled axes.

However, it fails to correctly calculate the average `body_mass_g` per `species` before plotting. The current code plots the `body_mass_g` directly from the cleaned DataFrame, which results in a bar plot where each bar represents the mean of *all* `body_mass_g` values within that species, not the average of body mass for each species as explicitly requested. The `sns.barplot` function does perform an aggregate (mean by default) when mapping a quantitative variable (`body_mass_g`) to a categorical variable (`species`), but it's crucial to explicitly perform the `groupby().mean()` step beforehand to ensure the data for plotting is exactly what's required by the task: ""For each `species`, calculate the average `body_mass_g`."" While `seaborn.barplot` internally calculates the mean, the task explicitly asks for the calculation step *before* plotting, implying that the aggregated data should be used as the basis for the plot, or at least that the calculation is a distinct logical step that should be represented.

To improve, the code should explicitly perform the `groupby('species')['body_mass_g'].mean()` aggregation and then plot this aggregated data. This aligns better with the task's steps and makes the code more robust and explicit about the data transformation. Additionally, using f-strings for titles and labels, as shown in the ground truth, makes the code more dynamic and maintainable.",qwen3-30b-a3b,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Drop rows with missing values in specified columns
df = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])

# Create new feature 'culmen_area'
df['culmen_area'] = df['culmen_length_mm'] * df['culmen_depth_mm']

# Calculate Pearson correlation matrix
correlation_matrix = df[['culmen_length_mm', 'culmen_depth_mm', 'fl...",9,"The generated code correctly addresses all requirements of the task. It loads the data, correctly handles NA values in the specified columns by dropping rows, creates the new `culmen_area` feature, calculates the Pearson correlation matrix for the required columns, and visualizes it as an annotated heatmap with the correct title. The use of pandas, seaborn, and matplotlib is appropriate.

The code is concise and directly implements the requested steps.

One minor point for potential improvement, not critical for correctness but good practice, could be adding a `try-except` block for `pd.read_csv` to handle `FileNotFoundError` gracefully, as seen in the ground truth. Also, adding `plt.tight_layout()` and rotating x-axis labels (as in GT) can improve readability for complex heatmaps, though for this specific matrix, it's not strictly necessary. However, these are minor aesthetic/robustness points and do not detract from the core functionality required by the prompt.",qwen3-30b-a3b,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd

df = pd.read_csv('penguins.csv')

features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
X = df[features]

# Impute numerical features with mean
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numerical_cols:
    X[col] = X[col].fillna(X[col].mean())

# Impute and map 'sex' column
X['sex'] = X['sex'].fillna('Unknown')
sex_mapping = {'MALE': 0, 'FEMALE': 1, 'Unkn...",8,"The generated code successfully accomplishes all the specified tasks. It correctly loads the data, selects features, imputes numerical NaNs with the mean, handles the 'sex' column as required (imputes 'Unknown', maps to integers), and applies one-hot encoding to 'island'. It also prints the shape and head of the processed feature matrix.

Strengths:
- **Correctness:** All transformation steps (imputation, mapping, one-hot encoding) are correctly implemented according to the problem description.
- **Conciseness:** The code is quite concise while still being readable.
- **Completeness:** It addresses all requirements outlined in the prompt.

Areas for minor improvement:
- **Target Variable:** The prompt explicitly states the goal is to predict `species` and implies the processed features are for a classification task. While the prompt asks to print the shape and head of the *feature matrix*, it's good practice to also explicitly define and optionally show the target variable (`y`) for a classification task, even if not explicitly printed. The generated code effectively creates the `X` (feature matrix) but doesn't explicitly separate or handle `species` as `y`.
- **SettingWithCopyWarning:** For the lines `X[col] = X[col].fillna(X[col].mean())` and `X['sex'] = X['sex'].fillna('Unknown')`, there's a potential for a `SettingWithCopyWarning` if `X` is a slice of `df`. Using `df[features].copy()` initially would prevent this. While it works in this case (often pandas handles it internally for direct assignments on a filtered DataFrame), it's good practice for robustness.
- **Error Handling:** The ground truth includes basic file not found error handling, which is a good robustness measure for real-world scenarios. The generated code assumes the file exists.
- **Clarity/Comments:** While generally clear, adding a few more comments to explain each major step (e.g., ""Impute numerical features with mean"" is good) could improve readability for larger, more complex scripts.",qwen3-30b-a3b,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

data = {'OrderID': [101, 102, 103, 104, 105, 106, 107], 'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 'Quantity': [1, 2, 1, 1, 3, 1, 1], 'Price_USD': [1200, 25, 75, 300, 50, 25, 1150], 'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', 'CUST001']}

df = pd.DataFrame(data)

# Calcul...",9,"The generated code correctly performs all the required tasks: creating the DataFrame, calculating total spending per customer, identifying and printing the top 3 customers, and calculating and printing the total quantity sold per category. The code is clean and uses appropriate pandas functionalities.

Minor points for improvement (not affecting correctness):
1. The print statements for the top 3 customers and category quantities could be a bit clearer, e.g., explicitly stating the currency or using a more formatted output like `print(f""CustomerID: {row['CustomerID']}, Total Spending: ${row['Total']:.2f}"")` for consistency with professional output.
2. The ground truth includes more descriptive print statements to clearly delineate each step, which can be useful for debugging or presentation. The generated code is more concise, which can also be a positive depending on the context.",qwen3-30b-a3b,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score

df = pd.read_csv('titanic.csv')

# Handle missing values
df['Age'].fillna(df['Age']....",6,"The generated code addresses most of the task requirements but has several areas for improvement, especially concerning data preprocessing, model evaluation, and overall code structure.

**Strengths:**
*   Correctly loads the data.
*   Handles missing values for 'Age' and 'Embarked'.
*   Performs categorical encoding using `pd.get_dummies`.
*   Includes feature scaling using `StandardScaler`.
*   Splits data into training and testing sets.
*   Trains a Logistic Regression model.
*   Evaluates the model using accuracy, precision, recall, F1-score, and ROC-AUC.
*   Displays feature coefficients.
*   Creates a confusion matrix and ROC curve visualization.
*   Performs cross-validation.

**Areas for Improvement/Weaknesses:**

1.  **Preprocessing Pipeline and ColumnTransform:** The generated code performs preprocessing steps sequentially (missing values, one-hot encoding, scaling). This is less robust and harder to manage than using `ColumnTransformer` and `Pipeline` as shown in the ground truth. The current approach also means that `StandardScaler` is applied to one-hot encoded features which are binary (0/1), which is generally not recommended or necessary for binary categorical features and can lead to less interpretable coefficients. The `original_columns` for coefficients will also be different after `pd.get_dummies` is applied to the full DataFrame before splitting. A `ColumnTransformer` and `Pipeline` properly separate numerical and categorical transformations.

2.  **`Pclass` Handling:** The ground truth code correctly identifies `Pclass` as a categorical variable and one-hot encodes it. The generated code implicitly treats `Pclass` as a numerical feature by not including it in `pd.get_dummies` and then scaling it with `StandardScaler`. This is a common mistake and often leads to suboptimal model performance, as the numerical value of `Pclass` (1, 2, 3) doesn't imply a linear relationship in the same way as continuous features.

3.  **Feature Engineering:** The task explicitly asks for ""feature scaling"" and ""encoding categorical variables,"" which the ground truth extends to ""feature engineering"" by creating `FamilySize`. The generated code does not include any feature engineering (e.g., `FamilySize`) which is a common and impactful step in Titanic survival prediction.

4.  **`ConfusionMatrixDisplay` and `RocCurveDisplay`:** The generated code uses basic `matplotlib` functions to plot the confusion matrix and ROC curve. The `sklearn.metrics` module provides specialized display functions (`ConfusionMatrixDisplay`, `RocCurveDisplay`) that offer more convenient and professional-looking visualizations, as demonstrated in the ground truth.

5.  **Interpretation of Coefficients:** While coefficients are displayed, there's no textual interpretation provided as required by the prompt (""their interpretation"").

6.  **Cross-Validation Detail:** The cross-validation (CV) only shows the scores for accuracy (default for `LogisticRegression` with `cross_val_score`). The ground truth shows how to get CV scores for multiple metrics using `cross_validate`, which is more comprehensive. Also, it doesn't print the mean and standard deviation for each metric, just the raw scores and the mean of the accuracy.

7.  **Reproducibility (Random Seed):** The ground truth sets `np.random.seed(42)` and `random_state=42` in `LogisticRegression`, which is good practice for reproducibility. The generated code only sets `random_state` for `train_test_split` and `LogisticRegression` has `max_iter` but no `random_state`.

8.  **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV. The generated code does not.",qwen3-30b-a3b,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']), 'Ticker': ['TECHCO']*8, 'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0], 'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5], 'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0], 'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0, 157.0, 154.0], 'Volume': [1000000, 1200000, 110000...",10,"The generated code is correct and effectively solves all aspects of the task. It properly creates the DataFrame, sets the 'Date' as the index, calculates the 3-day Simple Moving Average for 'Close' prices (handling NaNs for initial days correctly), and calculates the percentage change in 'Volume' (also handling the initial NaN). The necessary import is present, and the output is the requested DataFrame. The code is concise and follows best practices for pandas operations.",qwen3-30b-a3b,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'], 
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'], 
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91], 
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89], 
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]}

df = pd.DataFrame(dat...",9,"The generated code correctly performs all the required operations: creating the DataFrame, calculating the 'Score_Improvement' column, grouping by 'Subject' to calculate the average 'Score_Improvement', and filtering for students meeting the specified criteria. The imports are correct, and the code is structured logically.

The only minor point of improvement is in the printing of the final student IDs. The ground truth code iterates through the series and prints each `student_id` on a new line, which is slightly more readable for a list of IDs than the default series printout. However, the output is still correct and contains the required information. The ground truth also includes additional `print` statements with headings which were not explicitly asked for but help with clarity when running the script. The generated code focuses purely on the requested output without extra descriptive prints, which is perfectly acceptable given the prompt.",qwen3-30b-a3b,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

filtered_df = df[(df['Department'] == 'Tech') & (df['Perf...",9,"The generated code correctly defines the DataFrame, filters it, and calculates the average number of `ProjectsCompleted` for the filtered group. It produces the correct numerical output as required by the task.

**Strong points:**
*   **Correctness:** The code is logically and syntactically correct and achieves the specified task.
*   **Conciseness:** It performs the required operations efficiently and without unnecessary complexity.
*   **Adherence to requirements:** All filtering and calculation steps are correctly implemented.

**Areas for improvement (minor):**
*   **Output formatting:** The ground truth code provided a more descriptive print statement (e.g., ""The average number of 'ProjectsCompleted' for Tech department employees with PerformanceScore > 3 is: 17.50""), which is generally good practice for user-facing output. The generated code only prints the number.
*   **Edge case handling:** The ground truth code included a check for an empty `filtered_df` to handle cases where no employees match the criteria, which is a robust programming practice. While not strictly required by the prompt's output specification (which only asked for the calculation and print), it's a good touch for production-ready code.

Overall, the generated code is highly functional and correct, with only minor areas for enhancement in terms of output verbosity and robustness.",qwen3-30b-a3b,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

desktop_conversion_rate = df[df['DeviceType'] == 'Desktop']['Converted'].mean() * 100
mobile_conversion_rate = df[df['DeviceTyp...",9,"The generated code correctly defines the DataFrame, calculates the conversion rates for Desktop and Mobile users, and prints them clearly labeled. It adheres to all the requirements of the task.

**Strong points:**
*   **Correctness:** The calculations for conversion rates are accurate. Filtering by `DeviceType` and then applying `.mean()` on the `Converted` column for boolean values is a concise and correct way to get the conversion rate.
*   **Conciseness:** The solution is very compact and efficient, performing the filtering and calculation in single lines.
*   **Clarity:** The output is well-labeled, as requested.

**Areas for minor improvement (why it's not a 10):**
*   **Robustness/Edge Cases:** The generated code does not explicitly handle potential edge cases where a `DeviceType` (e.g., 'Desktop' or 'Mobile') might not exist in the `DeviceType` column. In such a scenario, `df[df['DeviceType'] == 'Desktop']` would return an empty DataFrame, and `empty_df['Converted'].mean()` would result in `NaN`. While `NaN` formatted with `:.2f` will print as ""nan%"", it's often better practice to explicitly handle these cases (e.g., by checking if the filtered DataFrame is empty and assigning 0 or printing a warning if no data for that device type exists), as shown in the ground truth. For this specific task, given the provided sample data, this isn't an issue, but it's a consideration for robust real-world code.

Overall, it's an excellent solution that directly addresses the problem statement effectively. The one point deduction is for not considering the empty-dataframe edge case, which is a good practice for more robust code.",qwen3-30b-a3b,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

averages = df.groupby('SensorID')['AirQualityIndex'].mean()

for sensor, avg in...",10,"The generated code is excellent. It correctly defines the DataFrame as specified, and then accurately calculates the average ""AirQualityIndex"" for each unique ""SensorID"" using `groupby()` and `mean()`. The output clearly indicates which average belongs to which sensor, fulfilling all requirements of the task. The code is clean, concise, and uses standard pandas operations efficiently. There are no unnecessary imports or redundant lines of code. It perfectly matches the required output format.",qwen3-30b-a3b,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

df['EngagementScore'] = df['Likes'] + df['Shares'] * 2 + df['Comments'] * 3
instagram_...",10,"The generated code is entirely correct and effectively addresses all aspects of the task. It correctly defines the DataFrame, calculates the ""EngagementScore"" feature according to the specified formula, filters the DataFrame for ""Instagram"" posts, and accurately computes their average ""EngagementScore"". The code is concise, well-structured, and uses standard pandas operations efficiently. The output format is also appropriate for the task (printing just the calculated average).",qwen3-30b-a3b,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",10,"The generated code correctly defines the DataFrame, calculates the ""LengthOfStay"" column, filters for ""Flu"" diagnosis, and accurately calculates and prints the average ""LengthOfStay"" for these patients. All requirements of the task are met with concise and correct pandas operations. The code quality is excellent, with proper imports and logical flow. While the ground truth code includes helpful print statements for debugging/verification, the generated code directly addresses the core computational requirements, which is perfectly acceptable for a benchmark task.",qwen3-30b-a3b,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}
df = pd.DataFrame(data)

sci_fi_movies = df[df['Genre...",8,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates their average rating, fulfilling the core requirements of the task. The code is concise and directly addresses the problem.

Areas for improvement to reach a perfect score:
1.  **Output Formatting:** The ground truth code formats the output using an f-string with two decimal places (`.2f`), making the output more user-friendly and professional (`The average rating for Sci-Fi movies is: 8.70`). The generated code simply prints the raw float value (`8.7`). While functionally correct, better formatting enhances readability.
2.  **Edge Case Handling:** The ground truth code includes a check for `if sci_fi_movies_df.empty:` to handle cases where no Sci-Fi movies might be found. While not strictly necessary for *this specific dataset* (as Sci-Fi movies are present), robust code often considers such possibilities, especially in a production environment where data might change, causing `mean()` on an empty Series to return `NaN`. For this specific benchmark scenario, where the data is fixed and known to have Sci-Fi entries, omitting this check is not a critical error, but it's a good practice distinction.",qwen3-30b-a3b,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

toyota_df = df[df['Brand'] == 'Toyota']
average_mileage = toyota_df['...",9,"The generated code correctly defines the DataFrame, filters for Toyota cars, and accurately calculates their average mileage. All core requirements of the task are met. The code is concise and efficient.

One minor suggestion for improvement, though not critical for correctness in this specific instance, would be to format the output print statement for better readability, similar to how it is done in the ground truth (e.g., `print(f""The average Mileage for Toyota cars is: {average_mileage:.2f}"")`). This enhances user experience. Also, the ground truth includes a robust check for an empty DataFrame before calculating the mean, which is good practice for production code (though not strictly necessary for this specific dataset given the problem constraints). Neither of these minor points detracts significantly from the correctness or quality of the solution for the given task.",qwen3-30b-a3b,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 80...",9,"The generated code correctly performs all the required tasks: it defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat. The code is concise and efficient.

The only minor area for improvement that prevents a perfect 10 is the formatting of the print output. The ground truth provides a more descriptive and user-friendly print statement with f-string formatting and floating-point precision (`average_fire_attack:.2f`), which is good practice for presenting numerical results. Additionally, the ground truth includes an `if not fire_pokemon_df.empty:` check, which is a good robustness measure, although for the given data, it's not strictly necessary as Fire-type Pokemon are present. However, the core logic is perfectly sound and achieves the task goals.",qwen3-30b-a3b,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

df = pd.read_csv('titanic.csv')
df['age'].fillna(df['age'].median(), inplace=True)
df['age_group'] = pd.cut(df['age'], bins=[0, 18, 35, 50, 100], labels=['0-18', '19-35', '36-50', '51+'])

fig, axes = plt.subplots(3, 1, figsize=(10, 15))
sns.barplot(x='pclass', y='survived', data=df, ax=axes[0])
axes[0].set_title('Survival Rate by Class')
sns.barplot(x='sex', y='survived', data=df, ax=axes[1])
...",7,"The generated code addresses most of the requirements but lacks some sophistication and attention to detail present in the ground truth.

**Strengths:**
*   Correctly uses `pandas`, `matplotlib.pyplot`, `seaborn`, and `plotly.express`.
*   Successfully creates a multi-panel figure for survival rates by different demographic groups.
*   Generates a correct heatmap for survival rates across class and gender combinations.
*   Creates a violin plot comparing age distributions by survival status and gender.
*   Builds an interactive scatter plot with `plotly.express`.
*   Attempts a dashboard-style figure combining multiple visualizations using `gridspec`.

**Areas for Improvement:**
1.  **Data Preprocessing and Robustness:**
    *   The generated code uses `df['age'].fillna(df['age'].median(), inplace=True)` which is good but the ground truth also handles `Fare` (which is good practice for robustness) and correctly capitalizes column names (`Age`, `Fare`) which is how they appear in `titanic.csv`. The generated code assumes lowercase column names, which is a potential source of error.
    *   The `AgeGroup` bins and labels could be more descriptive and standard (e.g., 'Child', 'Teenager', 'Young Adult', 'Adult', 'Senior') as in the ground truth, rather than just ranges.
    *   The `Survived_Cat` and `Pclass_Cat` mappings in the ground truth enhance clarity and ensure proper categorical ordering for plotting, which is missing in the generated code.

2.  **Visualization Quality and Storytelling:**
    *   **Visual Clarity:** The ground truth code pays more attention to details like `set_ylim(0, 1)` for survival rate plots for consistency, `sharey=True` in multi-panel plots to optimize space and readability, and explicit `plt.rcParams` for better aesthetics.
    *   **Interactive Scatter Plot:** While the plotly scatter plot is created, the ground truth's use of `symbol='Survived_Str'` and `hover_name='Name'` adds significantly more storytelling value by differentiating survivors visually and providing per-passenger detail on hover. The generated code only uses `hover_data=['survived']`.
    *   **Dashboard Sophistication:**
        *   While the generated code uses `gridspec`, its dashboard is essentially a re-arrangement of the individual plots without significant new insights or combining of information (e.g., KDE/histograms for distributions like age/fare like the ground truth).
        *   The ground truth's dashboard adds a dedicated section for ""Key Insights"" and annotations, which is crucial for storytelling and goes beyond just plotting by interpreting the results directly within the dashboard. The generated code's last subplot workaround saying ""Interactive Plot Disabled"" is a significant weakness, as the task explicitly asks for an interactive plot. An interactive plot *cannot* be embedded directly into a static Matplotlib figure in the way it seems to imply, so it's better to show it separately or note its interactive nature.

3.  **Code Style and Best Practices:**
    *   Using lowercased column names (`pclass`, `sex`, `age`, `survived`) instead of the correct `Pclass`, `Sex`, `Age`, `Survived` from the CSV is a common mistake and should be corrected directly in the `df.read_csv` or consistently mapped.
    *   The ground truth includes `try-except` for file loading, which makes the code more robust.
    *   The overall structure of the ground truth code, with clear sections (`Configuration`, `Data Preprocessing`, then numbered sections for each plot requirement), enhances readability and maintainability.

The generated code provides functional plots, but it doesn't achieve the ""advanced data visualization and storytelling"" and ""comprehensive dashboard-style figure combining multiple visualizations with proper annotations and insights"" level as effectively as the ground truth. It misses opportunities for deeper visual differentiation, more detailed interactivity, and narrative explanations.",qwen3-30b-a3b,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, ...",9,"The generated code correctly performs all the required tasks: it defines the DataFrame, filters it for the ""Dystopian"" genre, and accurately calculates and prints the average ""Stock"" for those books. It is concise and directly addresses the problem.

The only minor area for improvement compared to the ground truth is the lack of explicit print statements showing the filtered DataFrame or a more descriptive output for the average stock (e.g., using an f-string to explain what the printed number represents, as done in the ground truth). Also, the absence of an `if not dystopian_books_df.empty:` check, while not strictly necessary for this specific dataset since Dystopian books are present, would make the code more robust for general cases where the filtered DataFrame might be empty. However, given the prompt's constraints and the provided data, this is not a correctness issue. The code is highly effective and fulfills the task accurately.",qwen3-30b-a3b,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"import pandas as pd
import scipy.stats as stats
import numpy as np

df = pd.read_csv('water_potability.csv')

# T-tests for each numerical feature
numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
numerical_cols.remove('Potability')

for col in numerical_cols:
    group1 = df[df['Potability'] == 1][col]
    group2 = df[df['Potability'] == 0][col]
    t_stat, p_val = stats.ttest_ind(group1, group2)
    print(f""T-test for {col}: t-stat={t_stat:.4f}, p-value={p_val:.4f}"")
    ...",7,"The generated code addresses most of the task requirements but has several shortcomings regarding correctness, robustness, and completeness.

**Strengths:**
*   Correctly imports necessary libraries.
*   Performs t-tests, chi-square goodness-of-fit, and Shapiro-Wilk tests as requested.
*   Attempts Cohen's d calculation.

**Areas for Improvement:**

1.  **Missing Value Handling:** The most critical omission is the lack of any missing value handling. The `water_potability.csv` dataset is known to have missing values, and without imputation or dropping, statistical tests performed on columns with NaNs will either fail or silently drop observations, leading to incorrect results. The ground truth correctly handles this by imputing with the mean.
2.  **Order of Operations and Robustness:**
    *   **Normality Tests:** The task asks to apply normality tests on key numerical features after loading data. The generated code places it after t-tests, which is not ideal. Also, Shapiro-Wilk has sample size limitations (typically up to 5000), which the ground truth gracefully handles.
    *   **T-test `equal_var`:** The `ttest_ind` in `scipy.stats` defaults to `equal_var=True`. For most real-world data, especially when comparing different groups, `equal_var=False` (Welch's t-test) is more robust as it does not assume equal variances, which is often violated. The ground truth uses `equal_var=False`.
    *   **T-test and Cohen's d `dropna()`:** While not explicitly missing value handling for the whole dataframe, the individual `group1` and `group2` extractions `df[df['Potability'] == 1][col]` will result in NaNs if the original column has them. `ttest_ind` will ignore NaNs by default, but it's cleaner to explicitly handle them or impute beforehand. The calculation of `pooled_std` and `cohens_d` would fail if `std1` or `std2` are NaN (due to fewer than 2 non-NaN values) or if `n1` or `n2` are too small (leading to division by zero `n1+n2-2`). The ground truth adds checks for this.
3.  **Interpretation of Results:** The generated code only prints p-values and test statistics. It lacks an explicit interpretation statement based on the significance level (e.g., ""Reject null hypothesis, significant difference"" or ""Fail to reject null hypothesis, no significant difference""), which was explicitly requested (""Display p-values, test statistics, and interpretation of results for each test"").
4.  **Cohen's d Calculation for non-significant results:** While correct to only interpret Cohen's d for significant results, the code calculates it only for p < 0.05. The ground truth code prints a note about not interpreting it rather than not calculating it, providing a clearer output. The Cohen's d calculation also lacks a check for division by zero if `pooled_std` becomes zero (e.g., if all values in both groups are identical).
5.  **Print Statements and Clarity:** The ground truth has much clearer and structured print statements, indicating which test is being performed, explicit significance levels, and a summary. The generated code is sparse in this regard.
6.  **Task Requirements Check:**
    *   ""Compare mean values of each water quality parameter (pH, Hardness, Solids, etc.) between potable and non-potable water samples"" - **Partial**, t-test done, but no explicit interpretation.
    *   ""Perform chi-square goodness-of-fit test to check if potability follows expected distribution"" - **Yes**, done.
    *   ""Apply normality tests (Shapiro-Wilk) on key numerical features"" - **Yes**, done.
    *   ""Calculate effect sizes (Cohen's d) for significant differences"" - **Yes**, but the implementation could be more robust.
    *   ""Display p-values, test statistics, and interpretation of results for each test with appropriate significance level ( = 0.05)."" - **Partial**, interpretation is missing.

Overall, the generated code performs the core statistical tests but lacks the robustness, comprehensive error handling (especially for missing values), and detailed interpretation required for a high-quality solution in a data science context.",qwen3-30b-a3b,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ro...",7,"The generated code addresses most of the requirements but has several areas for improvement, particularly regarding best practices in data preprocessing, cross-validation, and visualization detail.

**Strengths:**
*   **Correctness of Core Logic:** The code generally implements the requested steps: data loading, train-test split, imputation, scaling, model training, evaluation with multiple metrics, confusion matrices, ROC curves, cross-validation, and selection of the best model.
*   **Metric Calculation:** All required metrics (accuracy, precision, recall, F1-score, ROC-AUC) are calculated.
*   **Multiple Models:** Logistic Regression, Random Forest, and SVM are included.
*   **Feature Importance:** Attempts to show feature importance for suitable models.
*   **Proper Imports and Structure:** The code is reasonably well-structured and uses appropriate imports.

**Areas for Improvement:**

1.  **Data Leakage in Imputation/Scaling:** The most critical issue is that `SimpleImputer` and `StandardScaler` are fitted *before* the train-test split in the provided ground truth code extract. This is incorrect. The prompt states ""Perform data preprocessing including handling missing values, feature scaling..."". The ground truth code actually does the split *before* imputation and scaling, which is correct (fitting on trained data, then transforming both train and test). The generated code implements the train-test split *before* imputer and scaler *fit_transform* on `X_train` and *transform* on `X_test`, which is correct and avoids data leakage. However, the ground truth code *comment* implies (This will be done after train-test split to prevent data leakage), but then does it on `X_train` and `X_test` in the next block. So, the ground truth code is correct regarding this. The generated code also implements this correctly. My bad in initial assessment of the ground truth code for this specific point.

2.  **Missing Value Strategy:** The generated code uses `strategy='mean'` for `SimpleImputer`. While valid, `median` is often preferred for skewed distributions to avoid outliers heavily influencing the imputed values, especially for numerical features. The ground truth uses `median`, which is a good choice.

3.  **Cross-Validation Preprocessing:** The generated code performs cross-validation *only* on `X_train_scaled`. For a truly robust cross-validation, the imputation and scaling steps should be part of a `Pipeline` within the `cross_val_score` function, so that the preprocessing steps are refit on each fold's training data and applied to its validation data. This prevents data leakage across CV folds. The generated code performs CV on already scaled data, which pre-processes the entire `X_train` first, leading to a lesser but still present form of data leakage in the CV process. The ground truth code *correctly* identifies this and implements a `Pipeline` for CV.

4.  **ROC Curve Visualization Details:** The ROC curve plotting in the generated code combines all curves on one plot, which is good. However, it relies on `plt.plot(fpr, tpr, ...)` directly. `sklearn.metrics.RocCurveDisplay.from_estimator` (or `plot_roc_curve` in older versions) is a cleaner and more standard way to do this, as shown in the ground truth, which also automatically handles labels and plotting.

5.  **Confusion Matrix Visualization:** The generated code only prints the confusion matrix as a raw NumPy array. The task explicitly asks to ""Create confusion matrices,"" implying a visual representation. The ground truth code uses `seaborn.heatmap` for a clear visualization, which is much better.

6.  **SVM Feature Importance:** The generated code correctly notes that SVM doesn't directly provide feature importance but simply prints a generic message. The ground truth provides a more detailed explanation.

7.  **Best Model Selection Metric:** The generated code selects the best model based on ROC-AUC from the test set results. While ROC-AUC is a good metric, selecting the best model based on robust cross-validation results (e.g., CV F1-score mean) is generally a more stable and reliable approach, especially when dealing with potentially imbalanced datasets or for generalization performance. The ground truth code correctly uses CV F1-Score for best model selection.

8.  **Missing Imports/Warning Suppression:** The generated code doesn't import `seaborn` or `ConfusionMatrixDisplay`, and doesn't suppress warnings, which are minor but contribute to polish.

9.  **Error Handling:** The ground truth includes basic error handling for file not found, which is a good practice.

10. **Stratified Split:** The ground truth uses `stratify=y` for the train-test split, which is crucial for maintaining class distribution, especially with imbalanced datasets. The generated code misses this.",qwen3-30b-a3b,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split

df = pd.read_csv('water_potability.csv')

# Create new features
df['pH_Chloramines'] = df['pH'] / df['Chloramines']
df['Hardness_Solids'] = df['Hardness'] / df['Solids']
df['Nitrate_...",6,"The generated code addresses most of the requirements but has several significant issues that prevent it from being a strong solution.

**Strengths:**
*   It includes the necessary imports.
*   It attempts to create new features, including ratios and polynomial features.
*   It calculates and visualizes a correlation matrix.
*   It applies SelectKBest and RFE for feature selection.
*   It attempts to create feature importance plots and pairwise scatter plots.
*   It includes a comparison of selected features.

**Weaknesses and Areas for Improvement:**

1.  **Missing Data Handling:** The ground truth correctly identifies and handles missing values by imputing with the mean. The generated code completely skips this crucial step. The `water_potability.csv` dataset is known to have NaNs in 'ph', 'Sulfate', and 'Trihalomethanes'. Without handling these, subsequent operations (like feature engineering and correlation) will fail or produce incorrect results. `df['pH_Chloramines'] = df['pH'] / df['Chloramines']` will produce NaNs if `pH` or `Chloramines` have NaNs (where `pH` is `ph` in the dataset).

2.  **Feature Engineering - Meaningfulness & Scope:**
    *   **Ratios:** The selected ratio features (`pH_Chloramines`, `Hardness_Solids`, `Nitrate_Fluoride`) are very limited. The prompt asks for *meaningful* features, and the ground truth provides a more robust set of ratios and interaction terms. `Nitrate_Fluoride` ratio might not be as directly meaningful or representative as `Sulfate_Chloramines`, `Organic_carbon_Trihalomethanes`, etc., that the ground truth chose, based on domain knowledge of water quality.
    *   **Interaction Terms:** The generated code completely omits interaction terms, which were explicitly requested (""interaction terms"").
    *   **Polynomial Features:** While `PolynomialFeatures` is used, the generated code applies it to *all* features (`df.drop('Potability', axis=1)`). This can lead to an enormous number of features (e.g., 8 original features produce 44 polynomial features, but 9 original features would produce 54, etc., and with engineered features, this can explode rapidly), causing performance issues and making the model less interpretable and prone to overfitting. The ground truth wisely selects a *subset* of original features for polynomial expansion to manage dimensionality.
    *   **Division by Zero Epsilon:** The ground truth includes `epsilon` in ratio calculations to prevent division by zero, which is a good practice overlooked by the generated code.

3.  **Correlation Analysis - Top 5 Features:**
    *   The prompt asks for ""top 5 features **most correlated** with water potability"". This implies absolute correlation. The generated code uses `sort_values(ascending=False)` which will give the feature most positively correlated first, but might miss a highly negatively correlated feature. The ground truth correctly uses `.abs()` to get the magnitude of correlation.
    *   More critically, `target_corr.head(5).index.tolist()` will include 'Potability' itself because it will have a perfect correlation of 1.0. The ground truth correctly excludes 'Potability' from the list of ""top correlated features"" by slicing `[1:6]`.

4.  **Feature Selection - Scaling:** The generated code does not scale features before `SelectKBest` and `RFE`. While `RandomForestClassifier` (used in RFE) is somewhat robust to feature scaling, `SelectKBest` (especially with `f_classif`) can be affected by features with vastly different scales. Standard practice involves scaling features before applying most feature selection techniques and machine learning models.

5.  **Visualization Issues:**
    *   **Feature Importance Plot:** `model.fit(X, y)` is called for feature importance *after* `RFE` has already potentially subsetted `X`. The ground truth correctly trains a `RandomForestClassifier` on the *full scaled dataset* (`X_scaled_df`) specifically to get comprehensive feature importances across *all* engineered features. The generated code's approach will get feature importances only for the original `X` (before polynomial features are concatenated if `X` is redefined). If `X` contains all the polynomial features already, it will be very wide, and plotting all importances without sorting and selecting top N will be unreadable. The ground truth correctly sorts and picks top N.
    *   **Pairwise Scatter Plots:**
        *   `top_corr_features = top_5_features[:5]` is redundant and incorrect as `top_5_features` already contains 5 features (or 6 if 'Potability' is included due to the bug mentioned above).
        *   The ground truth also ensures that the features exist in the dataframe before plotting, which is good practice.
        *   The ground truth uses `hue='Potability'` and `diag_kind='kde'` for more informative plots, whereas the generated code simply plots without hue for the target class.

6.  **Code Structure and Comments:** The ground truth is much better structured with clear sections and comments for each task, making it more readable and maintainable. The generated code is more monolithic.

7.  **Comparison of Feature Sets:** The comparison is basic. The ground truth provides more detailed insights like features unique to each method, and a union of all selected features, which is more comprehensive.

In summary, while the generated code attempts to meet all requirements, its implementation has several critical flaws relating to data preprocessing (missing value handling), the robustness and comprehensiveness of feature engineering, the correctness of correlation analysis, the omission of a standard preprocessing step (scaling) before feature selection, and the quality/completeness of visualizations.",qwen3-30b-a3b,LLM_judge_config
