task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# List all column names and identify the data type of each column
print(""Column Names and Data Types:"")
print(df.dtypes)

# Identify numerical columns
numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns
print(""\nNumerical Columns:"")
print(numerical_columns)

# For all numerical columns, calculate and report basic descriptive statistics
print(""\nDescriptive Statistics for Numerical Co...",7,"The generated code correctly loads the dataset, identifies data types, calculates descriptive statistics for numerical columns, and reports dataset dimensions. It also identifies most columns as numerical.

However, there are a few areas for improvement where the ground truth is more comprehensive:
- **Error Handling:** The generated code lacks error handling for `FileNotFoundError` if the CSV file isn't present, which is crucial for robust code.
- **Data Type Identification Detail:** While `df.dtypes` is concise for listing types, the ground truth provides a more explicit identification (e.g., ""Numerical"", ""Categorical/Ordinal"") which is more aligned with the prompt's request to ""identify the data type of each column (numerical, categorical, etc.)"". The generated code relies solely on Pandas dtypes for identification, which is not wrong, but less explicit.
- **Descriptive Statistics Presentation:** The ground truth uses `df[numerical_columns].describe()` which provides a consolidated table for all numerical columns concisely. The generated code loops through each numerical column and prints `.describe()` individually, which is less efficient and harder to read for many numerical columns.
- **Categorical/Ordinal Identification Nuance:** For the `winequality-red.csv` dataset, all columns are technically numerical. The ground truth code correctly points out the nuance that 'quality', while numerical, is often treated as ordinal. The generated code strictly looks for `object` dtypes for categorical/ordinal, which will result in an empty list for this specific dataset, missing this nuance. The ground truth also explicitly clarifies why certain types were identified.",llama-3.3-8b-instruct,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('video_games_sales.csv')
print(""First 10 rows of the dataset:"")
print(df.head(10))

# Show the shape of the dataset
print(""\nShape of the dataset (rows and columns):"")
print(df.shape)

# List all column names and their data types
print(""\nColumn names and their data types:"")
print(df.dtypes)

# Display the top 10 games by Global_Sales
print(""\nTop 10 games by Global_Sales:"")
print(df.sort_values(by='Global_Sale...",7,"The generated code correctly performs most of the required tasks. It loads the dataset, displays the head, shape, dtypes, top 10 games, unique values for platform and genre, summary statistics for sales columns, and missing values.

However, there are several areas for improvement to meet the quality and robustness of the ground truth:
1.  **Error Handling**: The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is crucial for robust scripts.
2.  **Clarity of Output**: The output formatting is less structured and clear compared to the ground truth. The ground truth uses section headers and separators to make the output more readable, which aligns better with an ""exploratory data analysis"" report.
3.  **Completeness of `.info()`**: The ground truth uses `df.info()` for displaying column names and data types, which also includes non-null counts and memory usage, providing a more comprehensive overview than just `df.dtypes`.
4.  **Displaying Top 10 Games Columns**: While the generated code prints the whole dataframe for top 10 games, the ground truth specifically selects relevant columns (`Name`, `Platform`, `Genre`, `Publisher`, `Global_Sales`) for display, which is a better practice for readability when showing specific filtered results.
5.  **Robustness for Column Existence**: The ground truth includes checks like `if 'Global_Sales' in df.columns:` before attempting operations, which makes the code more robust against potential issues if columns are missing or misspelled in the dataset.
6.  **Missing Values Clarity**: The ground truth explicitly states ""No missing values found in the dataset"" if there are none, and also filters `missing_values[missing_values > 0]` to only show columns with actual missing values, which is more user-friendly. `df.isnull().sum()` will list all columns, even those with 0 missing values, which can be verbose for datasets with many columns.
7.  **Unnecessary Import**: `import numpy as np` is included but not used.

Overall, the generated code is functional but could benefit from improved error handling, output formatting, and robustness checks.",llama-3.3-8b-instruct,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values in the Year column by filling them with the median year
df['Year'] = df['Year'].fillna(df['Year'].median())

# Remove games with missing Publisher information
df = df.dropna(subset=['Publisher'])

# Create a new column 'Years_Since_Release' that calculates how many years have passed since each game's release year
df['Years_Since_Release'] = 2023 - df['Year']

# ...",7,"The generated code addresses most of the requirements correctly.

**Strengths:**
- Loads the dataset.
- Correctly handles missing 'Year' values with the median.
- Correctly removes rows with missing 'Publisher'.
- Creates the 'Years_Since_Release' column as requested.
- Correctly removes duplicates based on 'Name' and 'Platform'.
- Correctly filters out low-selling games.
- Displays the final shape and head.

**Areas for Improvement:**
- **Robustness in 'Year' handling:** The current code assumes 'Year' is numeric. The ground truth code explicitly converts 'Year' to numeric with `errors='coerce'` before filling NaNs, which is more robust to potential non-numeric entries in the 'Year' column. It also converts the 'Year' column back to int after filling NaNs, which is good practice.
- **Robustness in 'Global_Sales' handling:** Similar to 'Year', `Global_Sales` might contain non-numeric data. The ground truth code converts 'Global_Sales' to numeric with `errors='coerce'` and then drops NaNs, making the filtering more robust. The generated code assumes 'Global_Sales' is already numeric.
- **Defensive programming:** The ground truth includes a `try-except` block for `FileNotFoundError` and general exceptions, which is good practice for real-world data processing scripts.
- **Redundant `numpy` import:** `numpy` is imported, but not used in the generated code.

The missing robustness measures for non-numeric data in 'Year' and 'Global_Sales' are significant omissions for a data cleaning task, as they can lead to errors or incorrect results if the input CSV is not perfectly clean.",llama-3.3-8b-instruct,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Create a bar chart showing the top 10 gaming platforms by total global sales
plt.figure(figsize=(10,6))
top_10_platforms = df['Platform'].value_counts().head(10)
plt.bar(top_10_platforms.index, top_10_platforms.values)
plt.xlabel('Platform')
plt.ylabel('Global Sales')
plt.title('Top 10 Gaming Platforms by Global Sales')
plt.show()

# Generate a pie chart displaying the d...",6,"The generated code addresses most of the visualization requirements but has critical issues in data handling and specific plot implementations.

**Correctness:**
*   **Bar Chart (Top Platforms):** Incorrectly calculates the top 10 platforms. `df['Platform'].value_counts()` counts the number of games per platform, not the total global sales per platform, which was the requirement.
*   **Line Plot (Games per Year):** The line `df['Year'] = pd.to_datetime(df['Year_of_Release']).dt.year` can lead to errors if `Year_of_Release` is already numeric or contains non-date friendly values. The ground truth properly handles `Year_of_Release` as a numeric column and performs data cleaning for it.
*   **Sales Column Handling:** Does not explicitly convert sales columns to numeric, which `pd.read_csv` might not do by default, leading to potential plotting issues if they are loaded as objects. The ground truth includes robust data cleaning for this.

**Code Quality:**
*   **Data Cleaning:** Lacks any robust data cleaning, especially for `Year_of_Release` and sales columns, which is crucial for reliable plotting. The ground truth dedicates a section to this.
*   **Error Handling:** No `try-except` block for file loading, making it less robust than the ground truth.
*   **Formatting:** While titles and labels are present, the plots generally lack the fine-tuning (e.g., `xticks` rotation, `tight_layout`, `grid`) that improves readability and presentation, which the ground truth includes. `plt.show()` is called after every plot, which is good.

**Completeness:**
*   All required plot types are generated.
*   Titles and labels are included.
*   It assumes the CSV is in the current directory.

**Areas for Improvement:**
*   Implement proper data cleaning, especially for `Year_of_Release` and all sales columns, ensuring they are numeric before plotting. Coerce errors to NaN and drop NaNs where appropriate.
*   Correct the logic for the bar chart to sum `Global_Sales` for each platform, not just count games.
*   Improve plot aesthetics by adding grid lines, rotating x-axis labels if necessary, and using `plt.tight_layout()` for better spacing.
*   Add error handling for file loading.",llama-3.3-8b-instruct,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('diabetes.csv')
print(df.head(10))

# Show the shape of the dataset
print(df.shape)

# List all column names and their data types
print(df.dtypes)

# Calculate the percentage of diabetic vs non-diabetic patients
print(df['Outcome'].value_counts(normalize=True) * 100)

# Display the count of missing values for each column
print(df.isnull().sum())

# Show basic summary statistics for all numerical columns
numeric...",6,"The generated code addresses most of the requirements but lacks some crucial aspects for a complete and robust solution.

**Strong points:**
- Correctly loads the dataset and displays the first 10 rows.
- Accurately reports the shape, data types, outcome percentages, missing values, and summary statistics.
- Uses `select_dtypes` for numerical columns, which is good practice.

**Areas for improvement:**
- **Error Handling:** The ground truth includes `try-except` blocks for file loading, which is critical for robust code. The generated code will crash if `diabetes.csv` is not found.
- **Output Clarity:** While the calculations are correct, the output lacks descriptive print statements and separators found in the ground truth, making it harder for a user to understand which output corresponds to which task requirement. For example, simply printing `df.shape` without context is not user-friendly. `df.info()` from the ground truth is also better for data types as it includes non-null counts.
- **Specific Identification of Zero Values:** The prompt asks to ""Identify any columns with zero values that might represent missing data."" The generated code uses `df.eq(0).sum()`, which counts zeros for *all* columns, including 'Pregnancies' and 'Outcome' where zero is a valid value. The ground truth specifically lists `COLUMNS_TO_CHECK_FOR_ZERO_AS_MISSING` and provides a more targeted analysis, which is crucial for identifying *potential* missing data rather than just all zero counts. Also, the ground truth calculates the *percentage* of zeros, which adds valuable context.
- **No Handling of `Outcome` Column:** While correctly calculated, the code assumes the `Outcome` column exists without checking, similar to the ground truth.
- **No `numpy` usage:** Although `import numpy as np` is present, `numpy` is not utilized in the code, indicating it's an unnecessary import for this specific solution.",llama-3.3-8b-instruct,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Identify and replace columns with zero values that represent missing data
zero_columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
for col in zero_columns:
    df[col] = df[col].replace(0, pd.NA)

# Handle missing values by filling them with the median of each respective column
for col in zero_columns:
    df[col] = df[col].fillna(df[col].me...",7,"The generated code correctly addresses most of the requirements.

**Strong points:**
*   Correctly loads the dataset.
*   Identifies and replaces zero values with `pd.NA` (which is functionally similar to `np.nan` for `fillna`).
*   Fills missing values with the median of respective columns.
*   Removes duplicate rows.
*   Creates the 'BMI_Category' column, although with slightly different binning logic.
*   Applies `StandardScaler` to the specified numerical features.
*   Prints the shape and head of the cleaned dataframe.

**Areas for improvement to achieve a higher score:**
*   **Robustness in loading:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. This makes the code less robust for real-world scenarios.
*   **Missing column check:** When replacing zeros and filling NaNs, the ground truth code checks if the column actually exists in the DataFrame (`if col in df.columns:`). The generated code assumes all `zero_columns` are present, which isn't robust if the input CSV changes.
*   **BMI Categorization Bins:** The binning for `BMI_Category` is slightly off: `[0, 18.5, 25, 30, df['BMI'].max()]`. The ground truth uses `[0, 18.5, 24.9, 29.9, np.inf]`. The generated code's normal category `(18.5-25)` is `[18.5, 25)` which incorrectly includes `24.9` and `25.0` in 'Normal' based on the problem statement's `18.5-24.9`. It also misinterprets `Obese (>=30)`. Using `df['BMI'].max()` as the upper bound for the last bin is generally less robust than `np.inf` because it depends on the maximum value in the *current* dataset, whereas `np.inf` correctly covers all values `> = 30`.
*   **Numerical Features for Scaling:** The generated code hardcodes the `numerical_cols` for scaling, which is a subset of all numerical columns and excludes 'Outcome'. The ground truth dynamically identifies all numerical columns and then explicitly removes 'Outcome', which is more flexible if the dataset structure changes (e.g., more numerical features are added). It's also important to note that the problem statement *doesn't* explicitly state to exclude the 'Outcome' column from scaling, but it's a common practice. The ground truth explicitly states this exclusion.
*   **Clarity of output:** While the output (shape and head) is sufficient, the ground truth provides clear labels (""Cleaned dataset shape:"", ""First 5 rows..."") and also shows the original shape, which is helpful for comparison.
*   **`pd.NA` vs `np.nan`:** While `pd.NA` works and is conceptually similar, `np.nan` is more widely used and understood for representing missing numerical data in Pandas. Since the task is primarily numerical data, `np.nan` is a more conventional choice.",llama-3.3-8b-instruct,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Create a correlation heatmap
plt.figure(figsize=(10,8))
sns.heatmap(df.select_dtypes(include=['int64', 'float64']).corr(), annot=True, cmap='coolwarm', square=True)
plt.title('Correlation Heatmap')
plt.show()

# Generate histograms for all numerical features
fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(20,10))
sns.histplot(df['Glucose'], ax=axs[0,0])
s...",7,"The generated code successfully addresses most of the requirements and produces the requested visualizations.

Strengths:
- **Correctness:** All plots are generally generated correctly, displaying the intended relationships and distributions.
- **Imports:** Correct libraries are imported.
- **Plot Types:** All required plot types (heatmap, histograms, box plots, pie chart, scatter plots, pair plot) are present.
- **Scatter and Pair Plots:** These are well-implemented with `hue='Outcome'`, which is a strong point.

Areas for Improvement:
- **Readability and Maintainability (Histograms & Box Plots):** The repetition of `sns.histplot(df['Feature'], ax=axs[row,col])` for every single feature is not ideal. The ground truth code uses a loop for this, which is much more concise, scalable, and less error-prone. This applies to both histograms and box plots.
- **Pie Chart Requirement:** The prompt specifically asked for a *pie chart* for the distribution of diabetes outcomes. The generated code uses `sns.countplot`, which is a bar chart. While it shows the distribution, it doesn't fulfill the ""pie chart"" requirement.
- **Titles and Labels:** While main titles are present for some plots, many subplots (histograms, box plots) lack specific titles or clear `xlabel`/`ylabel` for each feature. The ground truth demonstrates much better labeling.
- **Error Handling/Loading:** The generated code lacks the `try-except` block for file loading, which is good practice for robustness.
- **Numerical Column Identification:** The ground truth programmatically identifies numerical columns, making it more robust to dataset schema changes. The generated code hardcodes column names for histograms and box plots, which is less flexible.
- **Missing Specificity:** For the heatmap, `df.select_dtypes(include=['int64', 'float64'])` is good, but the Ground Truth explicitly excludes 'Outcome' when creating the `numerical_cols` list for correlation, which is often desired (to see correlations *between features*, not with the target). The generated code's approach includes 'Outcome' in the correlation, which might or might not be intended.
- **Plot Aesthetics:** While functional, the aesthetics (e.g., `set_style`, specific color palettes, `tight_layout` application) could be improved to match the quality of the ground truth. `tight_layout()` is applied in most places but could be more consistently used after each `plt.show()`.",llama-3.3-8b-instruct,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Display the first 10 rows
print(""First 10 rows:"")
print(df.head(10))

# Show the shape of the dataset
print(""\nShape of the dataset:"")
print(df.shape)

# List all column names and their data types
print(""\nColumn names and data types:"")
print(df.dtypes)

# Display the unique regional indicators and count of countries per region
print(""\nUnique regional indicators and count ...",7,"The generated code addresses most of the requirements but has several issues that prevent it from scoring higher.

**Strengths:**
- Correctly loads the dataset.
- Successfully displays the first 10 rows, shape, top 10 happiest countries, summary statistics, and missing values.
- Uses `pandas.head()`, `df.shape`, `df.sort_values().head()`, `df.describe()`, and `df.isnull().sum()` effectively.

**Areas for Improvement:**
- **Robustness (Missing File Handling):** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is crucial for robust code. The generated code lacks this, meaning it would crash if the file isn't present.
- **Column Name Mismatch (Task 4 & 5):**
    - Task 4 asks for 'Regional indicator' but the generated code uses `'Region'`. The dataset provided (world-happiness-report-2021.csv) actually has 'Regional indicator', so this will fail.
    - Task 5 asks for 'Country name' but the generated code uses `'Country'`. The dataset provided (world-happiness-report-2021.csv) actually has 'Country name', so this will also fail.
- **Column Name Mismatch (Task 6):** Task 6 asks for 'GDP per capita', but the dataset has 'Logged GDP per capita'. The generated code uses 'GDP per capita', which would result in a KeyError or exclude that column from summary statistics if not handled. The ground truth correctly uses 'Logged GDP per capita' or filters existing columns.
- **Data Type Display (Task 3):** The prompt asks for column names and *their data types*. The generated code uses `df.dtypes`, which is correct for showing data types but `df.info()` (used in the ground truth) provides a more comprehensive summary including non-null counts, which is often preferred for EDA and implicitly covers data types. While `df.dtypes` is technically correct, `df.info()` is better for the stated task.
- **Clarity and Separation:** The ground truth includes clear print statements and separators (`=""*70`) which improve readability and make it easier to distinguish between the outputs of different tasks. The generated code's print statements are less distinct.
- **Unnecessary Import:** `import numpy as np` is included but not used.",llama-3.3-8b-instruct,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Handle missing values
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].fillna(df[col].mode()[0])
    else:
        df[col] = df[col].fillna(df[col].mean())

# Create new derived features
df['Happiness_Category'] = pd.cut(df['Ladder score'], bins=...",6,"The generated code addresses most of the requirements but has several inaccuracies and areas for improvement, preventing it from achieving a higher score.

**Positive Aspects:**
- **Loading Data:** Correctly loads the CSV file.
- **Missing Values:** The approach to fill missing values is generally correct (mean for numerical, mode for categorical).
- **Derived Features:** All three derived features are attempted. `Well_being_Score` is correctly calculated. `Happiness_Category` uses `pd.cut` which is a valid approach (though the bins need to be adjusted for exact match with requirements).
- **Outlier Removal:** Correctly implements the IQR method for removing outliers.
- **Standardization:** Correctly uses `StandardScaler` for numerical features.
- **Encoding:** Correctly uses `LabelEncoder` for 'Regional indicator'.
- **Display Output:** Correctly prints the shape and head of the DataFrame.

**Areas for Improvement/Correction:**

1.  **Missing Value Handling (Categorical Mode):** The original code uses `df[col].mode()[0]` which is more robust for cases where `mode()` might return multiple modes. The generated code uses `df[col].mode()[0]` which is also correct. However, the iteration for missing values could be improved by separating numerical and categorical columns first, as done in the ground truth, for clearer logic.
2.  **`Happiness_Category` Logic:**
    *   Requirement: High: >6.5, Medium: 4-6.5, Low: <4.
    *   Generated Code: `bins=[0, 4, 6.5, np.inf], labels=['Low', 'Medium', 'High']`. This leads to:
        *   (0, 4] -> Low (correct for <4 if upper bound is exclusive, but typically `pd.cut` right-inclusive by default).
        *   (4, 6.5] -> Medium (should be 4-6.5 inclusive).
        *   (6.5, inf) -> High (should be >6.5).
    *   The use of `np.select` in the ground truth for this specific condition set is more precise and directly maps to the requirements' inclusive/exclusive boundaries. `pd.cut` by default includes the right boundary unless `right=False` is specified. Even then, getting the exact `4-6.5` range inclusive for both ends and `>6.5` and `<4` using `cut` can be tricky without careful bin definition (`[0, 4), [4, 6.5], (6.5, np.inf)` doesn't perfectly match).
3.  **`GDP_Category` Feature:**
    *   Requirement: Based on quartiles of `Logged GDP per capita`.
    *   Generated Code: Uses `df['GDP per capita']` which is a different column than `Logged GDP per capita`. This is a significant error as it uses the wrong column.
    *   Labels are 'Low', 'Medium', 'High', 'Very High'. The requirement was just 'Q1', 'Q2', 'Q3', 'Q4'. While functional, it doesn't match the specified labels.
4.  **`Well_being_Score` Calculation:** The generated code correctly calculates the average of the three features.
5.  **Robustness (File Not Found):** The generated code does not include error handling for `FileNotFoundError` when loading the dataset, which is a good practice for robustness in real-world scenarios, as shown in the ground truth.
6.  **Clarity/Readability:** While functional, the ground truth's use of comments and explicit identification of numerical/categorical columns before filling missing values makes the code more readable and maintainable.
7.  **`numerical_features` for Standardization:** The code correctly selects numerical features based on `dtype`. This includes the newly created numerical features (`Well_being_Score`, etc.), which is good.

Overall, the code performs most steps, but the inaccuracies in `GDP_Category` and subtle differences in `Happiness_Category` implementation, along with the lack of error handling, reduce its score.",llama-3.3-8b-instruct,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from plotly.offline import plot

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Create a world map visualization
fig = go.Figure(data=go.Choropleth(
    locations=df['Country'].astype(lambda x: x.strip()).index,
    z = df['Ladder Score'].astype(float),
    locationmode = 'country names',
    colorscale = 'Reds',
    colorbar_title = ...",6,"The generated code attempts to address all requirements of the task, but it has several issues regarding correctness, completeness, and adherence to best practices, especially concerning data handling and specific plot types.

**Correctness Issues:**
*   **Column Names:** The biggest issue is that the generated code assumes column names like 'Country', 'Region', 'GDP per Capita', 'Freedom to Make Life Choices', etc., which are not the actual column names in the `world-happiness-report-2021.csv` dataset. The ground truth code correctly renames the columns (`df.rename`) to match the desired labels for plotting, or directly uses the correct original names where appropriate. This leads to `KeyError` for several plots. For example, 'Country' should be 'Country name', 'Region' should be 'Regional indicator'. 'Ladder Score' should be 'Ladder score'.
*   **Plotly Map:**
    *   `locations=df['Country'].astype(lambda x: x.strip()).index` is incorrect. It should be `locations=df['Country name']` if the column existed, or `locations=df['Country name']` after renaming. The `.index` method will likely pass integer indices, not country names, to `locations`.
    *   `z = df['Ladder Score'].astype(float)` has the same `KeyError` issue for 'Ladder Score'.
    *   The map uses `plotly.graph_objects` directly instead of `plotly.express` which is simpler for choropleth maps.
    *   `plot(fig, output_file='world_map.html')` saves the map to an HTML file instead of displaying it in the notebook/environment directly using `fig.show()`, which is generally preferred for interactive exploration unless explicitly asked for file output.
*   **Bar Chart:** `orient='h'` for `sns.barplot` is generally used when `x` and `y` are flipped for a horizontal bar chart. Here, `x` is `index` (regions) and `y` is `values` (happiness scores), so this would create a vertical bar chart. For a horizontal bar chart with the current `x` and `y` assignments, it should be `x=avg_happiness_by_region.values, y=avg_happiness_by_region.index`.
*   **Scatter Plots:** The scatter plots use `plt.scatter` directly, which is less rich in features than `sns.scatterplot` (e.g., no automatic hue for regions). The task specifically mentions using seaborn for some plots.
*   **Radar Chart:**
    *   The calculation of `angles` in the radar chart is off. It defines `angles` as `np.radians(i) for i in range(0, 360, 72)` but then recalculates `angles_rad` inside the loop. The `x` and `y` calculation `values * np.cos(angles_rad)` and `values * np.sin(angles_rad)` implies multiplying a list (values) by a numpy array, which is not what's intended for plotting coordinates. Each value should correspond to a specific angle, not broadcast across all angles.
    *   More importantly, radar charts require data to be on a comparable scale (normalized). The generated code doesn't perform any normalization, which means larger absolute values of factors will dominate the chart, potentially misrepresenting the ""profile"". For instance, ""GDP per Capita"" might have a much higher raw value range than ""Generosity,"" making comparisons meaningless.
    *   The `radius=1.2` for `plt.xticks` is an argument for setting the radius of the tick labels, not for the radial extent of the plot. The plot itself needs proper `set_theta_zero_location` and `set_theta_direction` for typical radar charts, and the `plt.show()` after `plt.plot()` inside the loop means only the last country will be plotted if not handled carefully (though `plt.show()` is outside the loop here). The plotting of `x` and `y` as `np.concatenate((y, [y[0]])), np.concatenate((y, [y[0]]))` for a polar plot is also suspicious; it should be `plt.plot(angles_to_plot, values_to_plot)`.

**Completeness (Missing elements/details):**
*   **Error Handling:** No `try-except` block for file loading, which is good practice.
*   **Plot Formatting:** While titles and labels are present, additional formatting like `plt.tight_layout()`, rotating x-tick labels for readability, setting legend outside for scatter plots, and potentially adjusting plot sizes (though some `figsize` are set) are missing in some places compared to the ground truth.
*   **Imports:** `sklearn.preprocessing.MinMaxScaler` and `math` were needed for a robust radar chart and general utility, but were not imported.

**Code Quality:**
*   **Redundant/Incorrect Imports:** `plotly.offline import plot` is generally not needed if `fig.show()` is used. `plotly_express` is typically preferred for high-level plotting.
*   **Column Renaming:** The ground truth correctly identifies that the raw column names are not user-friendly for direct plotting and renames them. The generated code skips this crucial step.

Overall, the code attempts to fulfill the requirements but fails significantly due to incorrect column names derived from the dataset and a flawed implementation of the radar chart without normalization. The use of basic `plt.scatter` instead of `sns.scatterplot` also misses an opportunity for better visualization.",llama-3.3-8b-instruct,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd

# Load the dataset and parse the 'date' column as datetime objects
def load_dataset():
    df = pd.read_csv('synthetic_covid19_data.csv')
    df['date'] = pd.to_datetime(df['date'])
    return df

# Perform exploratory data analysis
def exploratory_data_analysis(df):
    # Display the first 10 rows
    print(""First 10 rows:"")
    print(df.head(10))
    
    # Show the shape of the dataset (rows and columns)
    print(""\nShape of the dataset:"")
    print(df.shape)
 ...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being production-ready or fully correct for a data science benchmark.

**Correctness Issues:**

1.  **Date Parsing (Minor):** While `pd.to_datetime(df['date'])` works, `parse_dates=['date']` during `read_csv` is generally more efficient for large datasets as it's optimized at the C level. However, the generated code's approach is functional.
2.  **`df.dtypes` vs. `df.info()`:** The requirement asked for ""List all column names and their data types."" `df.dtypes` lists types. `df.info()` (used in ground truth) additionally provides non-null counts and memory usage, which is often preferred in EDA for a more comprehensive summary. The generated code `print(df.dtypes)` is technically correct for the exact wording, but less comprehensive.
3.  **Handling Missing Data in `continent` (Major):** The generated code's `df.groupby('continent')[['total_cases', 'total_deaths']].sum()` will create a 'NaN' group if there are missing values in the 'continent' column. The ground truth explicitly handles this by dropping NaNs in 'continent' before grouping (`df.dropna(subset=['continent'])`). This is a common and important consideration in real-world data.
4.  **Top 5 Locations Logic (Critical Error):** This is the most significant flaw.
    *   The requirement is ""Identify and display the top 5 'location' entries with the highest 'total_cases' based on the **latest date for each location**.""
    *   The generated code uses `latest_date_df.groupby('location')['total_cases'].sum()`. This line sums all `total_cases` for each location *after* filtering for the latest date for each location. Since `latest_date_df` already contains only one entry per location (the latest one), summing `total_cases` for each `location` in this dataframe effectively does nothing, it just returns the `total_cases` from that single latest entry. The subsequent `sort_values(ascending=False).head(5)` would then correctly give the top 5 locations based on these *latest* `total_cases` values.
    *   However, the *output format* of `print(latest_date_df.groupby('location')['total_cases'].sum().sort_values(ascending=False).head(5))` only prints the `total_cases` value and the `location` as the index. It does **not** display the 'date' associated with that `total_cases` value, which is crucial for verifying ""based on the latest date"". The ground truth correctly displays `[['location', 'date', 'total_cases']]`.
    *   Also, the generated code sorts by `total_cases` sum, which is okay, but `describe()` doesn't show the max specifically, `sum()` does, but the `describe()` for `total_cases` and `total_deaths` isn't requested, it's just 'new_cases' and 'new_deaths'.
5.  **Summary Statistics (`.describe()`):** The requirement is to calculate ""mean, median, min, max, std"". While `.describe()` does provide mean, min, max, and std, it also provides count, 25/50/75 percentiles, and doesn't explicitly give median (it's the 50% percentile, but `median()` is a direct function). The ground truth used `.agg()` for explicit calculation of these specific statistics, which is better.
6.  **Error Handling:** The generated code lacks any error handling for `FileNotFoundError` or other potential issues when loading the CSV, which is present in the ground truth and crucial for robust code.

**Code Quality and Completeness:**

*   **Function Structure:** The use of functions (`load_dataset`, `exploratory_data_analysis`, `main`) is good for modularity.
*   **Prints and Headings:** All required print statements and headings are present.
*   **Imports:** `pandas` is correctly imported.
*   **Assumptions:** The assumption about the CSV file being in the current directory is correctly handled.

Overall, the generated code makes a good attempt but fails on crucial logical and output requirements for the top 5 locations, handles missing data less robustly, and uses less precise methods for summary statistics and initial data loading compared to the ground truth.",llama-3.3-8b-instruct,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
wine_data = pd.read_csv('winequality-red.csv')

# Calculate the average 'quality' of the red wines
avg_quality = wine_data['quality'].mean()
print(f""Average quality: {avg_quality}"")

# Group the wines by 'quality' and calculate the average 'alcohol' content
avg_alcohol_by_quality = wine_data.groupby('quality')['alcohol'].mean()
print(""Average alcohol by quality:"")
print(avg_alcohol_by_quality)...",8,"The generated code correctly addresses all three main requirements of the task: calculating the mean quality, grouping by quality to find average alcohol, and creating a bar chart. Imports are correct, and the code structure is logical.

**Strengths:**
- Successfully calculates a and prints the average quality.
- Correctly groups by quality and computes the average alcohol content for each group.
- Generates a bar chart visualizing the relationship between quality and average alcohol.
- Uses appropriate libraries (pandas, matplotlib, seaborn).

**Areas for Improvement (to reach a perfect 10):**
- **Error Handling:** The ground truth includes `try-except FileNotFoundError` for loading the CSV, which is good practice for robust code. The generated code lacks this.
- **Readability/Formatting of Output:** The average quality is printed, but the prompt asks for reporting, implying slightly more descriptive printing or formatting like the Ground Truth's `:.2f`. The grouped alcohol output could also benefit from being introduced more clearly.
- **Plot Enhancements:** While the plot is functional, the Ground Truth adds more polish:
    - Setting the `palette` in `sns.barplot` for better aesthetics.
    - Adding `plt.xticks` and `plt.yticks` for font size control.
    - Adding data labels on top of the bars for precise values.
    - Using `plt.tight_layout()` for better plot spacing.
    - More professional titles and axis labels.
- **Variable Naming in Plotting:** The use of `avg_alcohol_by_quality.index` for `x` and `avg_alcohol_by_quality.values` for `y` is functional but less explicit than providing a DataFrame to `sns.barplot` with named columns (which requires `reset_index()` on the grouped Series first, as done in the ground truth). This is a minor point but `sns.barplot` generally prefers DataFrame inputs.",llama-3.3-8b-instruct,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
data = pd.read_csv('synthetic_covid19_data.csv')

# Parse the 'date' column as datetime objects
data['date'] = pd.to_datetime(data['date'])

# Handle missing values
data['hosp_patients'] = data['hosp_patients'].fillna(0)
data['icu_patients'] = data['icu_patients'].fillna(0)
for col in ['positive_rate', 'reproduction_rate']:
    data[col] = data[col].fillna(data[col].mean())

# Create new columns
data['mortality_rate'] = (data['...",8,"The generated code addresses most of the requirements correctly.

**Strengths:**
*   Loads the dataset and parses the 'date' column as datetime objects.
*   Correctly handles missing values for 'hosp_patients' and 'icu_patients' by filling with 0.
*   Correctly fills missing values for 'positive_rate' and 'reproduction_rate' with their respective means.
*   Calculates 'mortality_rate' correctly and handles `inf` values.
*   Calculates 'vaccination_rate_per_capita' correctly and handles `inf` values.
*   Filters the dataset to include only records from 2021.
*   Displays the shape and head of the cleaned dataset.
*   Uses proper imports (`pandas` and `numpy`).

**Areas for Improvement:**
*   **Completeness on Missing Value Handling for New Columns:** While `replace([np.inf, -np.inf], 0)` handles infinite values for `mortality_rate` and `vaccination_rate_per_capita`, it *doesn't* explicitly handle `NaN` values that could arise from `0/0` in `total_deaths`/`total_cases` or if `total_deaths`/`total_cases`/`people_fully_vaccinated`/`population` were already NaN before the calculation. The ground truth explicitly handles `NaN` values for these new columns using `.fillna(0)`. This could lead to `NaN`s in the `mortality_rate` or `vaccination_rate_per_capita` if the original columns had missing values.
*   **Robustness of Division by Zero:** The task specifically mentioned ""Handle potential division by zero by replacing resulting NaNs or infs with 0."" While `replace([np.inf, -np.inf], 0)` covers `inf`, `fillna(0)` after the calculation is crucial for `NaN`s from `0/0`. The generated code misses the `fillna(0)` for the newly created columns.
*   **Clarity for `fillna` on Average:** While functional, the `for` loop for `positive_rate` and `reproduction_rate` is a concise way to apply `fillna(mean)`. However, the ground truth explicitly uses `pd.to_numeric` with `errors='coerce'` before calculating the mean for these columns, which adds robustness if the columns weren't already numeric. This is a minor point as `pandas` usually infers numerical types correctly, but it's good practice.
*   **Output Formatting:** The ground truth provides more descriptive print statements for the shape and head, which is helpful in a benchmark context.",llama-3.3-8b-instruct,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset and parse the 'date' column
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

# Create a time series line plot of 'new_cases' for the location 'India' over the entire period
df_india = df[df['location'] == 'India']
plt.figure(figsize=(10,6))
sns.lineplot(x='date', y='new_cases', data=df_india)
plt.title('New Cases in India over Time')
plt.xlabel('Date')
pl...",6,"The generated code correctly addresses most of the plotting requirements for different types of visualizations (line, bar, scatter, histogram, box plot) and includes proper imports and titles/labels. However, there are several key issues that prevent it from achieving a higher score:

1.  **""Total Deaths by Continent"" Calculation Error**: The calculation `df.groupby('continent')['total_deaths'].sum().reset_index()` for the bar chart is incorrect for a cumulative metric like `total_deaths`. Summing all `total_deaths` entries per continent (which are cumulative daily totals) will result in a vastly inflated and nonsensical number. The correct approach, as demonstrated in the ground truth, is to take the *latest* `total_deaths` value *per location* within each continent and then sum those values up. This is a significant logical flaw in the data aggregation for this specific plot.

2.  **Scatter Plot Latest Entry**: The `latest_entries = df.groupby('location').max().reset_index()` for the scatter plot is not robust enough. While `max()` might work for `date` (assuming dates are sequential), using `drop_duplicates(subset='location', keep='last')` after sorting by date is a more reliable and common way to get the latest entry for each location, ensuring all columns correspond to that latest date, not just their maximum values (e.g., `total_vaccinations` might not be max on the same day `gdp_per_capita` is max if `gdp_per_capita` is constant or only updated annually). The current approach could lead to incorrect pairings of `gdp_per_capita` and `total_vaccinations` if their max values don't align with the latest date for each location.

3.  **Box Plot for Stringency Index**: The box plot for `stringency_index` by continent uses `df.groupby('continent')['stringency_index'].median().reset_index()` which then feeds a single median value per continent into `sns.boxplot`. A box plot is designed to show the *distribution* of a variable across categories, requiring the raw (or appropriately grouped) data, not just a single aggregated statistic like the median. The correct approach is to pass the raw `stringency_index` data for each continent to `sns.boxplot` as demonstrated in the ground truth (`sns.boxplot(data=stringency_data, x='continent', y='stringency_index')`). This is a fundamental misunderstanding of how a box plot visualizes data.

4.  **Lack of Error Handling**: The generated code does not include any error handling for file not found or other potential issues during data loading, which is less robust than the ground truth.

5.  **Small Details/Improvements**:
    *   No `sns.set_theme(style=""whitegrid"")` for consistent visual style.
    *   Titles and labels are basic but acceptable; the ground truth provides slightly more descriptive ones (e.g., specifying ""Daily"" cases, ""Latest Data"").
    *   Plots are not as formatted (e.g., no `grid=True`, no `tight_layout`, no `rotation` for x-ticks where appropriate).
    *   Scatter plot does not include `hue` or `size` based on continent/population, which would add valuable insight.
    *   Histogram `bins` are fixed at 20, while `kde=True` and more flexible binning could be used.
    *   No handling of `NaN` values before plotting, which can lead to issues with certain plots.",llama-3.3-8b-instruct,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# List all column names and identify the data type of each column
print(""Column Names and Data Types:"")
print(df.dtypes)
print(""\n"")

# Identify numerical columns
numerical_columns = ['elixirCost', 'Win Rate', 'Usage']

# Calculate and report basic descriptive statistics for numerical columns
print(""Descriptive Statistics for Numerical Columns:"")
for column in numerical_columns:
    if column in df.colu...",7,"The generated code correctly loads the dataset, lists column names and data types, calculates descriptive statistics for the specified numerical columns, and reports the total number of rows. It successfully addresses all core requirements.

However, there are areas for improvement primarily in robustness and presentation, which prevent a higher score:
1.  **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions during file loading, making the code more robust. The generated code lacks this, which is crucial for real-world applications where files might be missing or corrupted.
2.  **Descriptive Statistics Output:** While `df[column].describe()` provides the required statistics (mean, median, std, min, max), it also includes count, 25%, 50%, and 75% quartiles. The task specifically asked for mean, median, standard deviation, min, and max. The ground truth uses `.agg()` which allows for precise selection of only the requested statistics, leading to a cleaner and more focused output.
3.  **Clarity of Output for Data Types:** The ground truth explicitly prints ""Column Names:"" and then iterates through them, followed by ""Data Types of Each Column:"". The generated code just prints `df.dtypes` directly, which is less explicit about which information is being presented.
4.  **Redundant Newlines:** The `print(""\n"")` after printing `df.dtypes` and after each column's `describe()` output is unnecessary and adds extra blank lines, making the output slightly less compact.

Overall, it's a functional solution that meets the requirements, but could be improved in terms of robustness, output specificity, and presentation.",llama-3.3-8b-instruct,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Filter the dataset
filtered_df = df[df['elixirCost'] <= 3]

# Select and display the required columns
result_df = filtered_df[['Card', 'Win Rate']]

# Sort the resulting data by Win Rate in descending order
sorted_df = result_df.sort_values(by='Win Rate', ascending=False)

# Print the sorted dataframe
print(sorted_df)
```",9,"The generated code correctly implements all the requirements of the task: it loads the CSV, filters based on `elixirCost <= 3`, selects the 'Card' and 'Win Rate' columns, and sorts the result by 'Win Rate' in descending order. The code is concise and readable.

The only minor improvements could be the addition of error handling for `FileNotFoundError` when loading the CSV, similar to the ground truth. While not strictly required by the prompt's output format (which only asks for the code), it is good practice for robust applications. However, for a data science benchmark focusing on the core task logic, this is a very strong submission.",llama-3.3-8b-instruct,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
def load_dataset():
    try:
        dataset = pd.read_csv('clash_royale_cards.csv')
        return dataset
    except FileNotFoundError:
        print(""The file 'clash_royale_cards.csv' was not found."")
        return None

# Generate histogram
def generate_histogram(dataset):
    plt.figure(figsize=(10, 6))
    plt.hist(dataset['elixirCost'], bins=50, edgecolor='black')
    plt.title('Distribution of Elixir Cost'...",7,"The generated code successfully loads the dataset and generates a histogram of the `elixirCost` column, fulfilling the core requirements of the task. It includes proper imports for `pandas` and `matplotlib.pyplot` and correctly sets the title and axis labels.

However, there are several areas for improvement:
- **Plotting Library:** The ground truth explicitly mentioned using `matplotlib or seaborn`. While `matplotlib` is used, the ground truth opted for `seaborn` (`histplot`) which is generally preferred for its aesthetic appeal and simplified syntax, especially for data visualization tasks in data science. The generated code uses basic `plt.hist`.
- **Binning:** The `bins=50` argument in `plt.hist` is arbitrary and not ideal for `elixirCost`, which is a discrete integer value typically ranging from 1 to 10. The ground truth's `bins=range(1, 12)` is much more appropriate, ensuring each integer cost has its own bin. The current binning might make the histogram less interpretable for discrete data.
- **Error Handling:** The `load_dataset` function correctly handles `FileNotFoundError`, but it simply returns `None` and prints a message. The `main` function then checks for `None`. The ground truth includes more robust error handling (e.g., `exit()` calls) and additional checks like verifying the existence of the `elixirCost` column, making it more resilient.
- **Code Structure/Readability:** While the function-based structure is okay, the ground truth's use of comments to clearly delineate sections (e.g., ""Load the dataset"", ""Generate histogram"") enhances readability.
- **Minor Enhancements:** Aspects like `figsize`, `edgecolor`, and `xticks`/`yticks` font sizes and explicit range setting, and adding a grid (as seen in the ground truth) improve the visualization quality. The generated code also didn't import `seaborn`, which was mentioned in the prompt.",llama-3.3-8b-instruct,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# Load the iris dataset
def load_iris_dataset():
    try:
        iris_df = pd.read_csv('iris.csv')
        return iris_df
    except Exception as e:
        print(f""An error occurred: {e}"")
        return None

# Process the iris dataset
def process_iris_dataset(iris_df):
    # List all column names and identify the data type of each column
    print(""Column Names and Data Types:"")
    print(iris_df.dtypes)
    
    # Numerical columns
    numerical_cols = ['Sepal...",7,"The generated code correctly loads the dataset, identifies column data types, calculates descriptive statistics for numerical columns, and reports unique species counts. However, there are a few areas for improvement.

**Areas for Improvement:**

1.  **Descriptive Statistics Output Format:** The prompt specifically asked for ""basic descriptive statistics (mean, median, standard deviation, min, max)"". The generated code uses ` Iris_df[col].describe()`, which provides more statistics (count, 25%, 50%, 75% percentiles) than requested and prints them column by column instead of a consolidated table per numerical column, which is less efficient and harder to read for multiple columns compared to `df[numerical_columns].describe()` or `df[numerical_columns].agg(['mean', 'median', 'std', 'min', 'max'])`. The ground truth uses `agg` for explicit selection of the requested statistics, which is better.
2.  **Column Names and Data Types Output:** The prompt asks to ""List all column names and identify the data type of each column."" The generated code only prints `iris_df.dtypes`, which lists data types but doesn't explicitly list the column names in a separate, clear way as requested by ""List all column names"".
3.  **Robustness (Error Handling/Input Validation):** While there's a `try-except` for file loading, the code doesn't handle cases where the specified numerical columns or the 'Species' column might be missing from the loaded DataFrame. The ground truth code includes checks for the existence of `numerical_columns` and the `Species` column, which makes it more robust.
4.  **Clarity/Readability of Output:** The ground truth code uses print statements with clear headers (`--- Column Names and Data Types ---`, `--- Descriptive Statistics for Numerical Columns ---`) to demarcate different sections of the output, making it more readable and organized, which the generated code lacks.",llama-3.3-8b-instruct,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

# Load the iris dataset
iris_df = pd.read_csv('iris.csv')

# Filter the dataset to include only 'Iris-setosa' records
setosa_df = iris_df[iris_df['Species'] == 'Iris-setosa']

# Select and display the 'SepalLengthCm' and 'SepalWidthCm' columns
print(setosa_df[['SepalLengthCm', 'SepalWidthCm']])
```",9,"The generated code correctly solves the task by loading the CSV, filtering for 'Iris-setosa' species, and selecting/displaying the specified sepal columns. It follows good coding practices with proper imports and clear variable names. The only minor improvement would be to add error handling (e.g., `FileNotFoundError`, `KeyError`) as demonstrated in the ground truth, which makes the code more robust for real-world scenarios. However, for a basic task execution, it is highly effective.",llama-3.3-8b-instruct,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the iris dataset
iris = pd.read_csv('iris.csv')

# Create a scatter plot
plt.scatter(iris['SepalLengthCm'], iris['SepalWidthCm'], c=iris['Species'])

# Add title and labels
plt.title('Iris Dataset: Sepal Length vs Width')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')

# Show the plot
plt.show()
```",7,"The generated code correctly loads the dataset, creates a scatter plot of `SepalLengthCm` vs `SepalWidthCm`, and includes a title and axis labels. These are significant strong points. However, it fails to correctly color the points by species. The `c=iris['Species']` argument in `plt.scatter` does not automatically map string categorical values to distinct colors, resulting in an error or an incorrect plot depending on the Matplotlib version and configuration. The ground truth addresses this by iterating through unique species and plotting each subset, ensuring correct color assignment and allowing for a legend. This is a critical omission that impacts the visual representation of the data as required by the prompt. The x-axis label is also slightly different from the ground truth (`Sepal Length (cm)` vs `SepalLengthCm`). To improve the score, the coloring by species needs to be correctly implemented, potentially by mapping species to numerical values or by iterating and plotting per species, as shown in the ground truth.",llama-3.3-8b-instruct,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# List all column names and identify the data type of each column
print(""Column Names and Data Types:"")
print(df.dtypes)

# Calculate descriptive statistics for numerical columns
numerical_columns = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']
descriptive_stats = df[numerical_columns].describe()
print(""\nDescriptive Statistics for Numerical Columns:"")
print(descriptive_stats)

# Report the count of occurrences for e...",7,"The generated code correctly performs the main tasks of loading the dataset, displaying column types, calculating descriptive statistics for the specified numerical columns, and reporting value counts for `HeartDisease`. However, it lacks robustness and specific details required by the prompt.

**Strengths:**
- Correctly loads the CSV using pandas.
- Accurately prints column data types using `df.dtypes`.
- Correctly identifies and uses the specified numerical columns for descriptive statistics.
- Uses `describe()` effectively to get the statistics.
- Correctly uses `value_counts()` for the `HeartDisease` column.

**Areas for Improvement:**
- **Column Names Listing:** The prompt explicitly asked to ""List all column names and identify the data type of each column."" The generated code only prints `df.dtypes`, which shows types but doesn't explicitly *list* each column name as a separate item, as was done in the ground truth for better readability.
- **Specific Statistics for Numerical Columns:** The prompt asked for ""mean, median, standard deviation, min, max"". While `describe()` provides these (median as 50%), the ground truth extracted and presented *only* these specific statistics, making the output more focused on the prompt's request. The generated code prints the full `describe()` output which includes count, 25%, and 75% quartiles, which weren't explicitly requested.
- **Error Handling:** There is no error handling for `FileNotFoundError` or other potential issues during CSV loading, which is a good practice for robust code, especially for a data science task where files might be missing.
- **Clarity of Output:** The ground truth added more descriptive print statements and separators (e.g., ""--- Column Names and Data Types ---"") which improves the readability of the output, making it easier to parse for an expert evaluator.
- **Interpretation of HeartDisease:** The ground truth provided an interpretation of the `HeartDisease` values (0 and 1), which is helpful context.

Overall, it's a functional solution that addresses the core requirements, but could be improved in terms of output formatting, specificity of requested statistics, and robustness.",llama-3.3-8b-instruct,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

# Load the heart.csv dataset
def load_heart_dataset():
    try:
        data = pd.read_csv('heart.csv')
        return data
    except FileNotFoundError:
        print(""The file 'heart.csv' was not found in the current directory."")
        return None

# Filter the dataset and display selected columns
def filter_and_display(data):
    filtered_data = data[(data['Age'] > 50) & (data['HeartDisease'] == 1)]
    selected_columns = filtered_data[['Age', 'Sex', 'ChestPai...",9,"The generated code correctly loads the dataset, filters it according to the specified conditions (`Age` > 50 AND `HeartDisease` == 1), and selects the required columns (`Age`, `Sex`, `ChestPainType`, `Cholesterol`). It also includes proper imports and basic error handling for `FileNotFoundError`. The code is well-structured using functions.

Areas for minor improvement:
1.  The `filter_and_display` function directly prints the output. While this fulfills ""display,"" returning the DataFrame and handling the print statement in `main` or a separate `display` function would offer more flexibility and testability.
2.  The `load_heart_dataset` function does not take `file_path` as an argument, hardcoding 'heart.csv'. This reduces reusability, though the task implied 'heart.csv' in the current directory. The ground truth's approach of passing the path as an argument with a default is more robust.
3.  Error handling is present for `FileNotFoundError` but doesn't cover potential `KeyError` if columns are missing or other `pandas` specific errors, which the ground truth addresses.
These are minor points given the prompt's specific requirements.",llama-3.3-8b-instruct,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

# Load the dataset
data = pd.read_csv('winequality-red.csv', sep=';')

# Filter wines based on 'alcohol' and 'pH' values
filtered_wines = data[(data['alcohol'] > 12) & (data['pH'] < 3.3)]

# Identify the wine with the highest 'quality' score
best_wine = filtered_wines.loc[filtered_wines['quality'].idxmax()]

# Display all features of the best wine
print(best_wine)
```",8,"The generated code correctly performs all the requested tasks: filtering based on compound conditions, identifying the highest quality wine in the subset, and displaying its features. The use of `pandas` is appropriate and efficient.

Strong points:
- Correct application of boolean indexing for filtering.
- Effective use of `idxmax()` to find the highest quality wine.
- Concise and readable code.

Areas for improvement (to reach a perfect 10):
- **Robustness:** The ground truth includes error handling (`try-except` blocks for `FileNotFoundError`, `KeyError`, etc.), which makes the code more robust against common issues like a missing file or incorrect column names. The generated code lacks this.
- **Assumptions:** The ground truth explicitly defines `DATASET_PATH` and passes it as an argument to a function, demonstrating a more modular and reusable approach. The generated code hardcodes the filename directly inside the script.
- **Modularity:** Encapsulating the logic within a function (`analyze_wine_quality`) as shown in the ground truth is better practice for larger scripts and reusability.
- **CSV Separator:** The generated code uses `sep=';'`, which might be an assumption based on some versions of the dataset. While it works if that's the case, the prompt didn't specify the separator. The common default is `','`, and the ground truth doesn't specify `sep` which implicitly assumes comma. If the actual file uses semicolons, `sep=';'` is correct, but it's an extra unstated assumption. For a general solution, sticking to the default or making the separator configurable would be better.
- **Output Clarity:** While `print(best_wine)` works, the ground truth's `print(best_wine.to_string())` ensures all features are displayed without truncation, which can happen with default `print` for wide dataframes/series. It also includes more descriptive print statements for progress and results, which is helpful in an analytical script.",llama-3.3-8b-instruct,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the heart.csv dataset
df = pd.read_csv('heart.csv')

# Generate a histogram for the 'Age' column
plt.figure(figsize=(8, 6))
plt.hist(df['Age'], bins=10, edgecolor='black')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Generate a bar chart for the 'Sex' column
plt.figure(figsize=(8, 6))
df['Sex'].value_counts().plot(kind='bar')
plt.title('Distribution of Sex')
plt.xlabel('Sex')
plt.yla...",7,"The generated code correctly loads the dataset and creates both the specified plots with titles and axis labels. This demonstrates a good understanding of the core requirements.

However, there are a few areas for improvement to reach a higher score:
1.  **Plotting Library Usage**: While `matplotlib` is used, the ground truth code leverages `seaborn` for enhanced aesthetics and features (like KDE in `histplot`, `countplot` for bar charts). The generated code uses basic matplotlib calls, which are functional but less visually appealing or feature-rich than seaborn's defaults for these types of plots.
2.  **Explicit Conversion for Sex Labels**: The `Sex` column often contains numerical values (0 and 1). The ground truth intelligently maps these to 'Female' and 'Male' for better readability on the bar chart. The generated code directly plots `value_counts()`, which would display 0 and 1 on the x-axis, making the plot less immediately understandable to a layperson without prior knowledge of the dataset's encoding.
3.  **Error Handling**: The ground truth includes robust error handling for `FileNotFoundError`, `KeyError`, and general exceptions. The generated code lacks any error handling, which is crucial for production-ready code or real-world data science tasks where file paths or column names might change.
4.  **Minor Details**: The ground truth adds minor improvements like `plt.tight_layout()` and `plt.grid()`, which enhance readability and presentation.

Overall, the code is functional and addresses the primary plotting requirements, but could be improved in terms of robustness, readability, and leveraging more advanced plotting features for better visualization.",llama-3.3-8b-instruct,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load dataset: {e}"")
        return None

# Main function
def main():
    # Load the dataset
    data = load_dataset('spotify_2023.csv')
    
    if data is not None:
        # List all column names and identify the data type of each
        print(""Column Names and Data Types:"")
   ...",7,"The generated code addresses most of the requirements but has a few minor issues regarding clarity and completeness in its output and error handling:

1.  **Column Names and Data Types:** While `data.dtypes` lists the data types, the request also asked to ""List all column names and identify the data type of each"". `df.info()` from the ground truth provides a more comprehensive and readable output for both column names and their types, along with non-null counts, which is generally preferred. `data.dtypes` only shows the series of columns with their types, not a clear list of names.
2.  **Reporting NaNs in `streams`:** The generated code correctly converts `streams` and reports `data['streams'].isna().sum()`. However, the prompt asked to ""report any NaNs"" which could imply showing a sample of rows, not just the count. The ground truth provides a more detailed report including the original/new dtype and sample rows, which is better.
3.  **Descriptive Statistics:** The generated code iterates through each numerical column and prints `describe()` for each. While correct, printing `describe()` individually for each column can be verbose. The ground truth's approach of using `df[existing_numerical_cols].describe().loc[['mean', '50%', 'std', 'min', 'max']]` provides a concise and well-formatted table of *only* the requested statistics for *all* numerical columns simultaneously, which is more efficient and readable.
4.  **Error Handling for File Not Found:** The generated code has a `try-except` block for `FileNotFoundError` but doesn't exit the program gracefully if the file isn't found. It just returns `None` and then the `main` function checks `if data is not None`. The ground truth explicitly exits, preventing subsequent operations from failing.
5.  **Imports:** `import numpy as np` is present but not used.

Overall, the code is functional and largely correct, but could be improved in terms of output formatting and adherence to the nuances of the prompt's reporting requirements.",llama-3.3-8b-instruct,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Filter the dataset to include only songs where `released_year` is 2023
df_2023 = df[df['released_year'] == 2023]

# Select and display the `track_name`, `artist(s)_name`, and `streams` columns
df_2023 = df_2023[['track_name', 'artist(s)_name', 'streams']]

# Ensure the `streams` column is treated as numeric (coerce errors to NaN if necessary)
df_2023['streams'] = pd.to_numeric(df_2023['st...",9,"The generated code correctly addresses all requirements of the task. It loads the CSV, filters by year, selects the specified columns, correctly converts the 'streams' column to numeric with error coercion, and sorts the data in descending order by streams.

**Strong points:**
- Proper use of pandas for data manipulation.
- Correct filtering and column selection.
- Correct handling of the `streams` column conversion to numeric, which is crucial for sorting.
- Correct sorting order.
- Concise and readable code.

**Areas for minor improvement (not critical for this task but good practice):**
- Error handling for `pd.read_csv`: The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions, which is robust for real-world applications. The generated code assumes the file exists. While the prompt states to assume the CSV is in the current directory, adding error handling makes the code more robust.
- The prompt specified to ""display"" the columns, and `print(df_2023)` accomplishes this. The ground truth adds an extra `print` statement with a descriptive header which is a nice touch for user-friendliness, but not strictly required.
- The `import numpy as np` is not strictly necessary as `pd.to_numeric` doesn't inherently require `numpy` in this specific setup. This is a minor point and doesn't detract from correctness.",llama-3.3-8b-instruct,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Generate a histogram to visualize the distribution of the `bpm` column
plt.figure(figsize=(10,6))
plt.hist(df['bpm'], bins=20, edgecolor='black')
plt.title('Distribution of Song BPM')
plt.xlabel('BPM')
plt.ylabel('Frequency')
plt.show()

# Create a scatter plot visualizing the relationship between `danceability_%` (x-axis) and `energy_%` (y-axis)
plt.figure(figsize=(10,6))
pl...",9,"The generated code correctly addresses all requirements of the task. It successfully loads the CSV, generates a histogram for `bpm`, and creates a scatter plot for `danceability_%` vs `energy_%`. Both plots have appropriate titles and axis labels, and necessary libraries are imported.

One minor suggestion for improvement, common for these types of tasks, is to separate `plt.show()` calls if you want to display multiple plots consecutively as separate windows during execution. Placing `plt.show()` after each plot definition (as done in the generated code) will display each plot individually and then close it before showing the next one. For a more conventional display where all plots are shown at the very end, `plt.show()` is typically called only once at the end of the script after all plots have been defined. However, this is a stylistic choice and doesn't detract from the correctness of the plots themselves. Also, while `bins=20` is a reasonable choice, `bins='auto'` (as in the ground truth) often provides a more data-driven number of bins for histograms. These are minor points that do not significantly impact the correctness or quality of the solution for the given task.",llama-3.3-8b-instruct,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# List all column names and their inferred data types
print(""Column Names and Data Types:"")
print(df.dtypes)

# Clean columns: Video views, Likes, Dislikes
for col in ['Video views', 'Likes', 'Dislikes']:
    df[col] = df[col].str.replace(',', '').astype(float, errors='coerce')
    df[col] = df[col].fillna(0)

# Calculate descriptive statistics for cleaned columns
print(""\nDescriptive ...",8,"The generated code successfully completes most of the tasks.

**Strong points:**
*   Correctly loads the CSV.
*   Correctly lists column data types.
*   Successfully cleans 'Video views', 'Likes', and 'Dislikes' by removing commas, coercing to numeric, and filling NaNs with 0.
*   Calculates and prints descriptive statistics for the cleaned columns.
*   Fills missing 'Category' values with 'Unknown'.
*   Reports the count of unique 'Category' values.
*   Imports are correct.

**Areas for improvement:**
*   **Missing Error Handling for File Not Found:** The generated code lacks a `try-except` block for `FileNotFoundError` when loading the CSV, which is crucial for robust code. The ground truth includes this.
*   **Descriptive Statistics Format:** While `df[col].describe()` provides the required statistics (mean, median (as 50th percentile), std, min, max), the output format is less clear and less directly aligned with printing *each* statistic individually as requested. The ground truth prints each statistic explicitly with a clear label and formatting.
*   **Clarity in Output for `df.dtypes`:** `df.dtypes` is technically correct for showing data types, but `df.info()` as used in the ground truth provides a more comprehensive summary including non-null counts, which is often more useful for initial data inspection and for understanding potential missing values. The task asked to ""list all column names and their inferred data types"", `df.dtypes` fulfills this, but `df.info()` goes a bit beyond, which is good practice.

Overall, it's a very good attempt that meets most of the functional requirements. The minor points relate to robustness and output formatting.",llama-3.3-8b-instruct,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '').astype(float, errors='coerce').fillna(0)

# Filter the dataset
df = df[df['Video views'] > 50000000]

# Select and display the required columns
result = df[['Video', 'Video views']]

# Sort the resulting data
result = result.sort_values(by='Video views', ascending=False)

# Display the result
print(result)
```",9,"The generated code correctly addresses all requirements of the task. It loads the data, correctly cleans the `Video views` column by removing commas, converting to float (with error coercion), and filling NaNs with 0. It then filters the data based on the view count, selects the specified columns, and sorts them as required. The code is concise and efficient.

One minor point for improvement, compared to the ground truth, is the `astype(float, errors='coerce')` call. While it works for numeric types, `pd.to_numeric()` is often preferred for robustness when dealing with mixed types and `errors='coerce'` specifically, as it's designed for this conversion with error handling. In this specific case, `astype` with `errors='coerce'` achieves the same result as `pd.to_numeric` when the string conversion is done prior, but `pd.to_numeric` is generally considered the more idiomatic and robust way to perform this operation. However, given the `str.replace` call before `astype`, the `astype` method functions correctly here. The ground truth also includes a `try-except` block for file loading, which is good practice but not explicitly requested, so its absence in the generated code isn't a deduction. Overall, a very good solution.",llama-3.3-8b-instruct,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the 'Likes' and 'Video views' columns
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)

# Create a scatter plot
plt.scatter(df['Video views'], df['Likes'])

# Set the plot title and labels
plt.title('Video Views vs. Likes')
pl...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, cleans the specified columns by removing commas, converting to numeric types (with `errors='coerce'`), and filling NaNs with 0. It then correctly generates a scatter plot with the specified title and axis labels.

Strong points:
- All required libraries are imported.
- Column cleaning is concise and effective, using `str.replace(',', '')`, `pd.to_numeric(..., errors='coerce')`, and `fillna(0)` in a single line for each column.
- The scatter plot is correctly generated with the specified title and axis labels.

Areas for minor improvement (not critical for this task but good practice):
- The generated code assumes the `youtube_trendings.csv` file exists in the current directory and doesn't include error handling for `FileNotFoundError` during `pd.read_csv`. While the prompt didn't explicitly require error handling, it's good practice for robust code.
- Could add `plt.figure(figsize=...)` for better plot sizing and `plt.grid(True)` for readability, similar to the ground truth. However, the core plotting requirements are met.",llama-3.3-8b-instruct,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
def load_dataset():
    try:
        dataset = pd.read_csv('mobiles_2025.csv')
        return dataset
    except Exception as e:
        print(f""An error occurred: {e}"")

# Clean the dataset
def clean_dataset(dataset):
    dataset['Mobile Weight'] = dataset['Mobile Weight'].str.replace('g', '').str.replace('', np.nan).astype(float)
    dataset['RAM'] = dataset['RAM'].str.replace('GB', '').str.replace('', np.nan).astype(float)
 ...",4,"The generated code attempts to address the task but contains several critical errors and omissions that prevent it from correctly cleaning the data and handling all requirements.

**Critical Errors:**
1.  **Incorrect error handling during cleaning:**
    *   `str.replace('', np.nan)`: This attempts to replace empty strings with `np.nan` *before* converting to numeric. `str.replace` expects a string for the replacement argument, not a float (`np.nan`). This operation will *not* convert empty strings or non-numeric values to `NaN` in a pre-numeric conversion step.
    *   `.astype(float)`: This will raise errors if there are non-numeric strings *after* the initial `replace` steps. The task explicitly requires handling conversion errors by setting to `NaN`. The correct way to do this is `pd.to_numeric(..., errors='coerce')`, which is missing. This means the code will likely crash or produce incorrect data if there are any non-numeric entries (beyond the units removed) in the specified columns.
2.  **Incomplete Cleaning Specifications:**
    *   `Mobile Weight`: Missing `.str.strip()` which is important to remove leading/trailing whitespace after removing 'g'.
    *   `RAM`: Missing `.str.strip()`.
    *   `Battery Capacity`: Missing `.str.strip()`.
    *   `Launched Price (USA)`: Missing `.str.strip()`.
    These omissions can lead to non-numeric strings if the original values contain whitespace before/after the units.
3.  **No error handling for `Launched Price (USA)` `USD ` removal:** The ground truth code removes `USD ` *before* removing commas. The generated code also does this but doesn't specifically coerce errors like the ground truth.

**Omissions/Minor Issues:**
1.  **Missing `mean`, `median`, `std`, `min`, `max` reporting explicitly:** While `describe()` outputs these, the task specifically asked for them to be *reported* as individual statistics in the ground truth. The generated code just prints the full `describe()` output, which is less granular than requested, although it implicitly contains the values.
2.  **No handling of `FileNotFoundError`:** The `load_dataset` function has a generic `except Exception as e:`, but a specific `FileNotFoundError` handling is better practice as shown in the ground truth.
3.  **Lack of comments/structure:** The ground truth code uses comments and a structured function (`analyze_mobile_data`) with clear print statements to guide the user through the process, which is good practice for a benchmark evaluation. The generated code is much more concise but less informative.
4.  **Implicit assumption of column existence:** The code directly accesses columns like `dataset['Mobile Weight']` without checking if they exist, which could lead to `KeyError` if the CSV structure changes. The ground truth code includes checks (e.g., `if 'Company Name' in df.columns:`).

Overall, the failure to correctly implement `errors='coerce'` for numeric conversions is a fundamental flaw that makes the cleaning process fragile and incorrect for the specified task requirements.",llama-3.3-8b-instruct,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Error loading dataset: {e}"")
        return None

# Clean the dataset
def clean_dataset(data):
    try:
        data['Launched Price (India)'] = data['Launched Price (India)'].str.replace('INR ', '').str.replace(',', '').astype(float).replace([np.inf, -np.inf], np.nan)
        data['RAM'] = ...",8,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans both 'Launched Price (India)' and 'RAM' columns (including handling errors by coercing to NaN using `astype(float)` and `replace([np.inf, -np.inf], np.nan)` which is a robust way though `errors='coerce'` in `pd.to_numeric` used in ground truth is more direct for errors), filters the data as specified, selects the correct columns, and sorts the final output. The use of functions for modularity is a good practice.

Areas for minor improvement:
1.  **Error Handling for Cleaning:** The generated code uses `.astype(float).replace([np.inf, -np.inf], np.nan)` for cleaning. While this works, `pd.to_numeric(..., errors='coerce')` is generally the more idiomatic and concise way to handle conversion errors directly to `NaN` in pandas, as used in the ground truth. The current approach might fail if the string contains non-numeric characters before `astype(float)` can even process it, whereas `pd.to_numeric` is designed to handle such cases gracefully with `errors='coerce'`.
2.  **Redundancy in Error Handling:** The `.replace([np.inf, -np.inf], np.nan)` after `astype(float)` is good practice if `astype(float)` itself could produce `inf` values from extremely large numbers, but for typical string-to-number conversion tasks, `errors='coerce'` in `pd.to_numeric` often suffices and is more direct for handling non-parseable strings.
3.  **Intermediate Prints:** The ground truth includes `print` statements to show progress and details like dataframe shape, which is helpful for debugging and understanding flow, though not strictly required by the prompt's output format.
4.  **`copy()` vs. SettingWithCopyWarning:** The ground truth explicitly uses `.copy()` after filtering (`filtered_df = df[...] .copy()`) to prevent potential `SettingWithCopyWarning` if subsequent operations were to modify `filtered_df`. While not always necessary depending on the operations, it's a good defensive programming practice in pandas. The generated code's `data = clean_dataset(data)` and `filtered_data = filter_dataset(data)` style, where functions return new dataframes, generally mitigates this issue naturally in the filtering step. However, if `clean_dataset` modified `data` in-place, the warning could still occur if not careful. In this case, `data['Launched Price (India)'] = ...` modifies in-place, so the use of `.copy()` in the filtering step of `main` is good.",llama-3.3-8b-instruct,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the 'RAM' column
df['RAM'] = df['RAM'].str.replace('GB', '').str.replace(',', '').apply(pd.to_numeric, errors='coerce')

# Clean the 'Battery Capacity' column
df['Battery Capacity'] = df['Battery Capacity'].str.replace('mAh', '').str.replace(',', '').apply(pd.to_numeric, errors='coerce')

# Create a scatter plot
plt.figure(figsize=(10, 6))
for company in df['Company Nam...",7,"The generated code correctly loads the dataset, cleans the 'RAM' and 'Battery Capacity' columns by removing units and converting to numeric, handles conversion errors, and creates a scatter plot with a title and axis labels. However, it misses one key requirement: coloring the points by 'Company Name'.

The approach for coloring by company name in the generated code is manual (iterating through unique companies and plotting each group) which is less efficient and robust than using `seaborn.scatterplot` with the `hue` parameter, as shown in the ground truth. While it does produce a plot with different colors for different companies (due to `plt.scatter` assigning default colors for each new plot call), it doesn't explicitly meet the 'color the points according to the `Company Name` column' in the most direct and Pythontic way, especially if the number of companies is large or if there needs to be a clear legend mapping. Also, it imports `matplotlib.pyplot` but not `seaborn`, which is a common and often preferred library for this type of visualization.

**Areas for improvement:**
1.  **Coloring by Company Name:** Use `seaborn.scatterplot` with the `hue` argument for easier and more effective coloring by a categorical variable.
2.  **Legend:** While `plt.legend()` is called, its effectiveness is limited by the manual plotting approach. `seaborn` handles this automatically and more cleanly with `hue`.
3.  **Robustness of Cleaning:** The generated code adds `str.replace(',', '')` to the cleaning process. While possibly useful for some data, the prompt only specified removing 'GB'/'mAh' and converting to numeric. The ground truth focuses strictly on the prompt's requirements. This isn't necessarily wrong but adds an unrequested step unless comma is part of the unit.
4. **Code Quality:** Using `df['RAM'] = df['RAM'].astype(str).str.replace(...)` is slightly safer as it ensures the column is a string before calling `.str` accessor, although pandas often handles mixed types gracefully. The ground truth does this.",llama-3.3-8b-instruct,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('titanic.csv')

# Display the first 5 rows
print(df.head())

# Show the shape of the dataset
print(df.shape)

# List all column names and their data types
print(df.info())

# Calculate the survival rate
survival_rate = (df['Survived'].sum() / df.shape[0]) * 100
print(f'Survival Rate: {survival_rate:.2f}%')

# Show the count of missing values for each column
print(df.isnull().sum())
```",8,"The generated code correctly addresses almost all requirements of the task. It properly loads the dataset, displays the first 5 rows, shows the shape, lists column names and data types, calculates the survival rate, and counts missing values.

Strengths:
- Correctly uses pandas for all data manipulation tasks.
- Calculates the survival rate accurately.
- Correctly identifies missing values using `isnull().sum()`.
- The output format for the survival rate is good (formatted to two decimal places).

Areas for Improvement:
- The `df.info()` method prints directly to the console and returns `None`. Printing `print(df.info())` results in an extra `None` being printed, which is redundant. It should just be `df.info()`.
- The code lacks descriptive print statements and labels for each section of output, unlike the ground truth which clearly indicates what each output represents (e.g., ""1. First 5 rows of the dataset:""). This reduces readability for someone running the script.
- There's no error handling for `FileNotFoundError`, which is a robust practice for loading files.
- The `import numpy as np` is unnecessary as `numpy` functions are not explicitly used in the solution. While not harmful, it's good practice to only import what's needed.",llama-3.3-8b-instruct,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
def load_imdb_dataset():
    try:
        dataset = pd.read_csv('imdb_top_250.csv')
        return dataset
    except Exception as e:
        print(f""An error occurred: {e}"")

# Analyze the dataset
def analyze_dataset(dataset):
    # List all column names and their data types
    print(""Column Names and Data Types:"")
    print(dataset.dtypes)
    
    # Calculate and report basic descriptive statistics for numerical columns
    numerical_columns ...",6,"The generated code attempts to address all parts of the task but has significant issues in correctness and completeness, particularly regarding data preprocessing and the precise statistical reporting.

**Correctness & Completeness Issues:**

1.  **Runtime Preprocessing:** The most critical omission is the lack of preprocessing for the 'Runtime' column. The `imdb_top_250.csv` dataset typically has 'Runtime' as a string (e.g., '142 min'). The generated code attempts to call `.describe()` directly on this column, which will result in non-numeric descriptive statistics (e.g., count, unique, top, freq for object types) rather than the required mean, median, std, min, max. This fundamentally fails requirement 2 for 'Runtime'. The ground truth explicitly handles this conversion.
2.  **Specific Statistics:** The task explicitly asks for ""mean, median, standard deviation, min, max"". The generated code uses `dataset[column].describe()`, which, while useful, provides more statistics than requested (e.g., count, 25%, 75%) and presents the median as '50%'. The ground truth code carefully selects only the requested statistics and renames '50%' to 'median' for clarity.
3.  **Error Handling (`load_imdb_dataset`):** While it includes a `try-except`, it catches a generic `Exception` and just prints a message, then returns `None`. The task implies that the script should terminate gracefully on error (e.g., `sys.exit(1)` as in the ground truth) if the file isn't found, preventing subsequent operations on a non-existent dataset.
4.  **Assumptions about 'Year', 'Rating':** It directly calls `.describe()` on 'Year' and 'Rating' assuming they are numeric. While they usually are, robust code (like the ground truth) would confirm their numeric type, especially after preprocessing.

**Code Quality:**

1.  **Function Structure:** The use of `load_imdb_dataset` and `analyze_dataset` functions is a good modular approach.
2.  **Clarity of Output:** The output for descriptive statistics is less concise than the ground truth's tabular format. Printing each column's `describe()` output separately makes it harder to compare stats across columns.
3.  **Imports:** Only `pandas` is imported, which is sufficient for basic operations. No `sys` for graceful exit.

To improve, the code needs to explicitly convert 'Runtime' to a numeric type, filter the specific statistics requested, and provide more robust error handling for file loading.",llama-3.3-8b-instruct,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Filter the dataset
filtered_df = df[df['Rating'] >= 9.0]

# Select and display the Title and Rating columns
result_df = filtered_df[['Title', 'Rating']]
print(result_df)

# Sort the resulting data by Rating in descending order
sorted_df = result_df.sort_values(by='Rating', ascending=False)
print(sorted_df)
```",7,"The generated code correctly performs all the required data manipulation steps: loading, filtering, selecting columns, and sorting. It properly imports pandas and uses the correct methods.

However, there are a few areas for improvement:
1.  **Redundant print statements**: The code prints `result_df` before sorting, and then `sorted_df` after sorting. The task implies that only the final, sorted result should be displayed.
2.  **Lack of error handling**: The ground truth includes robust error handling for `FileNotFoundError`, `KeyError`, and general exceptions. The generated code does not include any error handling, which is crucial for production-ready code or even for robust scripts.
3.  **Clarity/Efficiency**: The sorting is done on `result_df` and assigned to `sorted_df`, which is then printed. While functionally correct, it could be slightly more concise to directly sort and print the result, or chain the operations if readability isn't compromised.

Overall, it's a good attempt that solves the core problem, but lacks the robustness and conciseness often expected in well-written Python code.",llama-3.3-8b-instruct,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""An error occurred: {e}"")

# Create a histogram for the Rating column
def create_rating_histogram(data):
    plt.hist(data['Rating'], bins=10, edgecolor='black')
    plt.title('Distribution of Movie Ratings')
    plt.xlabel('Rating')
    plt.ylabel('Frequency')
    plt.show()

# ...",7,"The generated code correctly addresses most of the requirements: loading the dataset, creating a histogram for `Rating`, and a scatter plot for `Rating` vs. `Runtime`, and including titles and axis labels.

However, there are a few areas for improvement, especially regarding robustness and detailed plot customization:

1.  **Handling `Runtime` Column:** The generated code implicitly assumes the `Runtime` column is already in a numeric format suitable for plotting. The ground truth code correctly anticipates that `Runtime` might be a string (e.g., '120 min') and includes robust preprocessing to extract numeric values and handle potential non-numeric entries. This is a common data cleaning step in real-world scenarios that the generated code misses, making it less robust to variations in the data format.
2.  **Error Handling for File Not Found:** While the `load_dataset` function uses a `try-except` block, it simply prints an error message and continues, which might lead to `None` being passed to subsequent functions and cause further errors. The ground truth code exits gracefully in case of `FileNotFoundError`.
3.  **Plot Customization and Aesthetics (Minor):** The generated code uses basic matplotlib plots. The ground truth code leverages seaborn for slightly more aesthetically pleasing plots (e.g., `histplot` with KDE, `scatterplot` with alpha) and includes additional visual enhancements like `plt.grid` and `plt.tight_layout` for better presentation. This is not explicitly required by the prompt but contributes to higher quality visualization.
4.  **No Seaborn Import/Usage:** The prompt allows for matplotlib or seaborn. The generated code only uses matplotlib, while the ground truth utilizes both for different aspects.

Overall, it's a good attempt that meets the core requirements, but lacks the robustness and some best practices for data processing and visualization that are present in the ground truth.",llama-3.3-8b-instruct,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Report the number of missing values for each column
print(""Missing values count:"")
print(df.isna().sum())

# Drop rows with missing values in numerical columns
numerical_df = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])

# Report descriptive statistics for numerical columns
print(""\nDescriptive statistics for numerical columns:"")
print(numerical_df[['culmen_lengt...",6,"The generated code correctly loads the dataset and reports missing values. It also correctly filters rows for the descriptive statistics and calculates them. However, there are several areas for improvement:

1.  **Completeness of Descriptive Statistics:** The task specifically asked for ""mean, median, standard deviation, min, max"". The `.describe()` method provides these, but it also includes `count`, `25%`, `50%`, and `75%` percentiles, which were not explicitly requested. While not strictly wrong, it's not as precise as the ground truth's `agg()` method.
2.  **Unique Value Calculation for 'species' and 'island':**
    *   The `species` unique count is calculated on `numerical_df`, which is a subset of the original DataFrame after dropping NAs in specific numerical columns. The prompt did not specify that the unique count for `species` should be on the filtered data; it implied it should be on the full dataset.
    *   The `island` unique count is correctly calculated on `df` (the original DataFrame). This inconsistency is an error. Unique counts are typically reported on the full dataset unless specified otherwise.
3.  **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` when loading the CSV, which is good practice for production-ready code. The generated code lacks this.
4.  **Clarity/Readability:** While functional, the prompts (`print` statements) are less descriptive than in the ground truth, making the output slightly less clear about what each section represents.
5.  **Robustness for Missing Columns:** The ground truth includes checks (e.g., `if 'species' in df.columns:`) before trying to access columns, which makes it more robust against variations in the dataset structure. The generated code assumes all columns will exist.

To improve, the code should:
*   Use `agg()` for descriptive statistics to precisely match the requested metrics.
*   Calculate unique values for both `species` and `island` on the *original* DataFrame (`df`) since the task didn't specify filtering these columns.
*   Consider adding basic `try-except` blocks for file loading.
*   Improve print statements for clearer output.",llama-3.3-8b-instruct,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the dataset
filtered_df = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')]

# Select the required columns
selected_df = filtered_df[['culmen_length_mm', 'culmen_depth_mm']]

# Drop rows with missing values
cleaned_df = selected_df.dropna()

# Display the first 5 rows
print(cleaned_df.head(5))
```",9,"The generated code correctly performs all the required steps: loading the data, filtering by species and island, selecting the specified columns, dropping rows with NaNs, and displaying the first 5 rows. The code is clean, concise, and follows good practices.

However, there is a minor difference in the `dropna()` call. The ground truth code uses `dropna(subset=['culmen_length_mm', 'culmen_depth_mm'])`, which explicitly drops NaNs only from those two columns. The generated code uses `dropna()` without the `subset` argument. In this specific scenario, since `selected_df` only contains `culmen_length_mm` and `culmen_depth_mm`, `dropna()` implicitly achieves the same result. If `selected_df` had other columns, the generated code's `dropna()` would drop rows with NaNs in *any* column, which would be a deviation from the requirement to drop NaNs *only* from the two specified culmen columns. While it doesn't cause an issue here due to the prior selection, specifying the `subset` is technically more robust and aligns more precisely with the stated requirement for dropping NaNs *from these two columns*.

Additionally, the generated code lacks error handling for `FileNotFoundError` or other exceptions during CSV loading, which the ground truth includes. While the prompt implied the file would be present, robust code typically includes such handling. This is a minor point, as the primary task was data manipulation.",llama-3.3-8b-instruct,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Create a scatter plot
plt.scatter(df['culmen_length_mm'], df['culmen_depth_mm'], c=df['species'].map({species: i for i, species in enumerate(df['species'].unique())}))

# Filter out missing values
plt.scatter(df.loc[(df['culmen_length_mm'].notna()) & (df['culmen_depth_mm'].notna()) & (df['species'].notna()), 'culmen_length_mm'],
            df.loc[(df['culmen_length_mm'].notna())...",6,"The generated code attempts to address all requirements but falls short in several areas, particularly correctness and efficiency.

**Correctness Issues:**
1.  **Duplicate Plotting:** The code plots the data twice. The first `plt.scatter` call plots the *entire* unfitered dataset (or rather, attempts to plot it with a numerical representation of species that might not map correctly without `unique()` being called on the filtered data, leading to incorrect color mapping). The second `plt.scatter` then plots the filtered data. This is inefficient and potentially confusing if `plt.show()` wasn't called until the end.
2.  **Incorrect Filtering Order:** The filtering for missing values is applied *after* the first plot command, meaning the first plot is done on potentially dirty data. The task explicitly states, ""The plot should only include data points for which `culmen_length_mm`, `culmen_depth_mm`, AND `species` are all non-missing (not NA)."" The filtering should happen *before* any plotting.
3.  **Color Mapping for Species:** While `enumerate(df['species'].unique())` can assign numerical values for colors, it doesn't ensure distinct or visually intuitive colors as well as a library like seaborn would, or custom colormaps in matplotlib. Moreover, if the `unique()` method is called on the *unfiltered* dataframe, and then applied to the *filtered* dataframe, there's a risk that a species present in the unfiltered data but not the filtered data (due to NA values in other columns) will still generate a color index, which might be okay, but it's less robust than mapping directly from the filtered unique species or using a categorical plotting function.
4.  **No Legend:** A critical omission is the lack of a legend for the species color mapping. Without it, the ""color the points according to the `species` column"" requirement is not fully met as the user cannot interpret what each color represents.

**Code Quality and Completeness Issues:**
1.  **No Seaborn Used:** The prompt explicitly mentioned ""Use matplotlib or seaborn for plotting."" While matplotlib is used, seaborn offers built-in functionality (like `sns.scatterplot` and its `hue` parameter) that directly handles categorical coloring and automatically generates a legend, simplifying the code and improving plot quality for this kind of task. The ground truth demonstrates this effectively.
2.  **Innefficient Filtering:** The filtering logic `df.loc[(df['culmen_length_mm'].notna()) & (df['culmen_depth_mm'].notna()) & (df['species'].notna()), ...]` is repeated for each axis. It would be much cleaner and more efficient to create a filtered dataframe first: `df_filtered = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])` and then plot against `df_filtered`.
3.  **Lack of Readability:** The long inline filtering in the `plt.scatter` call makes the code less readable compared to pre-filtering the DataFrame.

To improve, the code should first filter the DataFrame, then plot using either `matplotlib.pyplot.scatter` with a proper color mapping and legend, or preferably, use `seaborn.scatterplot` which handles many of these aspects more robustly (coloring and legend generation).",llama-3.3-8b-instruct,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd
import numpy as np

# Read the dataset
def load_penguins_dataset(file_path):
    try:
        data = pd.read_csv(file_path)
        return data
    except FileNotFoundError:
        print(""The file does not exist"")
        return None
    except pd.errors.EmptyDataError:
        print(""The file is empty"")
        return None
    except pd.errors.ParserError:
        print(""Error parsing the file"")
        return None

# Main function
def main():
    file_path = 'pen...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, fills NA values in the specified columns with their respective means, calculates and prints the mean, median, and standard deviation for these columns, and finally prints the total number of rows and columns.

Strengths:
- **Correctness:** All calculations and data manipulations are performed accurately according to the task description.
- **Modularity:** The use of `load_penguins_dataset` function separates the data loading logic, which is good practice.
- **Error Handling:** The `load_penguins_dataset` function includes basic error handling for `FileNotFoundError`, `EmptyDataError`, and `ParserError`, which is more robust than the ground truth's `try-except` block.
- **Clarity:** The print statements clearly label the output statistics.

Areas for minor improvement (leading to a score of 9 instead of 10):
- **Redundant Import:** `import numpy as np` is not used in the provided code. While harmless, best practice is to remove unused imports.
- **Magic Strings:** The column names (`'culmen_length_mm'`, `'culmen_depth_mm'`) are repeated multiple times. Defining them as a list (like `NUMERICAL_COLUMNS` in the ground truth) would make the code more maintainable and less prone to typos, especially if more columns were involved or if the column names needed to change. This would also allow for a more compact loop for printing statistics, similar to how the ground truth handles it.
- **Direct Variable Access:** The code directly accesses `data['culmen_length_mm']` and `data['culmen_depth_mm']` instead of using the `data` variable and printing values formatted to two decimal places, which makes the output slightly less readable than the ground truth.
- **Function Encapsulation:** While `load_penguins_dataset` is good, the main logic within `main()` could be further refactored, e.g., a function for performing NA imputation and another for calculating and printing statistics, to enhance modularity and readability for larger scripts. However, for this specific task, it's acceptable.",llama-3.3-8b-instruct,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except FileNotFoundError:
        print(""The file does not exist"")
        return None
    except pd.errors.EmptyDataError:
        print(""The file is empty"")
        return None
    except pd.errors.ParserError:
        print(""Error parsing the file"")
        return None

# Drop rows where `fli...",8,"The generated code successfully addresses all core requirements of the task. It loads the dataset, drops NA values for `flipper_length_mm`, and generates both the specified histogram and bar chart with appropriate titles and axis labels.

Strengths:
- **Modularity:** The code is well-organized into functions for each specific task (loading, dropping NAs, creating each plot), which improves readability and maintainability.
- **Error Handling (Loading):** The `load_dataset` function includes robust error handling for `FileNotFoundError`, `EmptyDataError`, and `ParserError`, which is a good practice.
- **Correct Plot Types:** It correctly uses `sns.histplot` for the histogram and `sns.countplot` for the bar chart.
- **Correct Labels and Titles:** All required titles and axis labels are present for both plots.

Areas for Improvement:
- **Redundant `ylabel` in `histplot`**: For `sns.histplot`, `plt.ylabel('Count')` is often redundant as 'Count' is the default and usually added by seaborn. While not incorrect, it's unnecessary.
- **Clarity in `countplot` Y-axis Label**: While `plt.ylabel('Count')` works for the `island` count plot, `Number of Penguins` or `Penguin Count` might be more descriptive, aligning better with the task's spirit of ""label the axes appropriately"".
- **`sns.set_theme`**: The ground truth uses `sns.set_theme` for styling, which is a good practice for consistent and professional-looking plots. The generated code does not include this.
- **Explicit `data=` argument in `sns.histplot` and `sns.countplot`**: While `sns.histplot(data['flipper_length_mm'], ...)` works, using `sns.histplot(data=data, x='flipper_length_mm', ...)` is generally preferred as it is more explicit and aligns better with seaborn's typical API usage, especially when dealing with multiple columns. The same applies to `sns.countplot`.
- **Plot Customization**: Parameters like `figsize` are present, but additional tweaks like `plt.tight_layout()` or `plt.grid()` could enhance plot presentation (though not strictly required by the prompt).

Overall, the generated code is highly functional and well-structured, fulfilling the task requirements effectively. The suggested improvements are primarily about minor best practices and stylistic enhancements rather than fundamental correctness.",llama-3.3-8b-instruct,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except FileNotFoundError:
        print(""The file does not exist"")
        return None
    except pd.errors.EmptyDataError:
        print(""The file is empty"")
        return None
    except pd.errors.ParserError:
        print(""Error parsing the file"")
        return None

# Filter the data to include only penguins from the 'Torgersen' island
def f...",7,"The generated code correctly addresses the core requirements of loading the dataset, filtering by 'Torgersen' island, and counting species. It demonstrates good code organization by using functions for different steps.

However, there are several areas for improvement, particularly regarding error handling and robustness:

1.  **Missing Error Handling for Empty Filtered Data:** If `filter_data` returns an empty DataFrame (i.e., no penguins on 'Torgersen' island), `count_penguin_species` will attempt to access `torgersen_data['species']` which could lead to an error or unexpected behavior if the DataFrame is empty. The ground truth explicitly checks for this `empty` case.
2.  **Missing Column Existence Checks:** Neither the `filter_data` function nor `count_penguin_species` checks if the 'island' or 'species' columns exist before attempting to access them. This makes the code vulnerable to `KeyError` if the CSV file's schema deviates from expectations. The ground truth includes these checks for robustness.
3.  **No `print()` for Data Loading Success:** While `load_dataset` handles `FileNotFoundError`, it doesn't print a confirmation message upon successful loading, unlike the ground truth which informs the user that the dataset was loaded.
4.  **No Indication of Filtered Data Size:** The ground truth prints how many penguins were found on 'Torgersen' island, which is helpful for the user to understand the interim results. The generated code lacks this.
5.  **Less Specific Error Messages:** The error messages (e.g., ""The file does not exist"") are functional but less detailed than the ground truth's messages, which explicitly name the file ('penguins.csv').
6.  **`pd.errors.ParserError` and `pd.errors.EmptyDataError`:** While good to include more specific `pandas` errors, these aren't always necessary for basic file loading and often `Exception as e` can catch a broader range of parsing issues.

Overall, the code is functional for the happy path but could be significantly more robust and user-friendly with improved error handling and more informative output.",llama-3.3-8b-instruct,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

print(""Shape of the DataFrame:"", df.shape)
print(""Data t...",10,"The generated code is excellent. It correctly defines the DataFrame exactly as specified, accurately prints its shape, and correctly lists the data types of each column. The code is concise and efficient, directly addressing all parts of the task. All necessary imports are present, and the structure is clean. There are no missing requirements or logical errors.",llama-3.3-8b-instruct,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Error: {e}"")

# Create bar chart for survivors vs non-survivors
def survivors_vs_non_survivors(data):
    survivors = data['Survived'].value_counts()
    plt.bar(survivors.index, survivors.values)
    plt.title('Survivors vs Non-Survivors')
    plt.xlabel('Outcome')
    plt.ylab...",7,"The generated code correctly identifies the need for four distinct plots and uses appropriate libraries (pandas, matplotlib). It also attempts to handle missing age values, though not optimally.

**Strengths:**
*   Correctly identifies the need for four visualizations.
*   Uses `pandas` for data loading and `matplotlib.pyplot` for plotting.
*   Attempts to modularize code with functions.
*   Adds titles and axis labels.

**Areas for Improvement:**

1.  **Handling Missing Age Values:** The `dropna()` approach for age handles missing values by removing them, which might lead to loss of data and a biased representation if a significant portion of age data is missing. The ground truth fills missing values with the median, which is a more robust approach for this task.
2.  **Displaying Plots:** Each minor function calls `plt.show()`, which results in four separate plot windows opening sequentially. The ground truth (and a better practice for multiple plots) either groups them into subplots or runs `plt.show()` once after each plot is fully configured. The current approach is functional but less streamlined for user experience.
3.  **Error Handling for File Not Found:** The generated code has a `try-except` block for `load_dataset` but simply prints an error. The ground truth takes the extra step of exiting the program, preventing subsequent errors if the DataFrame is not loaded.
4.  **Clarity of X-axis for `Survived`:** The x-axis for 'Survivors vs Non-Survivors' uses `0` and `1`. It would be more user-friendly to map these to 'Non-survivor' and 'Survivor' as done in the ground truth.
5.  **`plt.figure(figsize)`:** The generated code doesn't specify figure sizes, which can sometimes lead to cramped plots or inconsistent aesthetics.
6.  **`plt.tight_layout()` and `plt.grid()`:** These are minor omissions but contribute to higher quality visualizations as shown in the ground truth.
7.  **`matplotlib.pyplot.bar` vs. `Series.plot(kind='bar')`:** While both work, using `Series.plot(kind='bar')` directly on the `value_counts()` Series is often more concise and idiomatic in pandas for simple bar charts.",llama-3.3-8b-instruct,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'], 
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'], 
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89], 
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93], 
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]}
df = pd.DataFrame(data)

# Calculate and print the average 'Mi...",10,"The generated code is excellent. It correctly imports pandas, creates the DataFrame as specified, and accurately calculates and prints average midterm and final scores. It also correctly identifies and prints the subject with the highest average final score, including the numerical average. The code is well-structured, follows the requirements, and produces the correct output. The f-string formatting for printing scores is also correct.",llama-3.3-8b-instruct,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
    'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
    'Volume': [1000000,...",9,"The generated code is highly correct and achieves all the requirements of the task. It correctly creates the DataFrame, sets 'Date' as an index, calculates the 'Daily_Range', finds the date with the highest 'Close' price, and prints the updated DataFrame cleanly.

One minor point for improvement, which is purely stylistic, relates to how the date with the highest 'Close' price is extracted.
The ground truth uses `df['Close'].idxmax()` which returns the index directly (which is a Timestamp object in this case, thanks to `set_index('Date')`). This Timestamp object can then be formatted using `.strftime('%Y-%m-%d')` for a cleaner output format.

The generated code uses `df.loc[df['Close'].idxmax()]` which returns a pandas Series (the row itself) and then accesses `.name` to get the index. While this works and is correct, `df['Close'].idxmax()` alone is sufficient to get the Timestamp if the only goal is to print the date itself. The `max_close_date.name` will print the full timestamp including time, which is not strictly explicit in the prompt but often desirable to format. The ground truth's `date_highest_close.strftime('%Y-%m-%d')` provides a more controlled format for the date.

Despite this very minor difference in extracting and formatting the date, the logic is sound, and the output is correct. The code quality is good, with all necessary imports and clear steps.",llama-3.3-8b-instruct,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create a pandas DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107], 
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 
        'Quantity': [1, 2, 1, 1, 3, 1, 1], 
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150], 
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST0...",10,"The generated code is correct, complete, and follows good coding practices. It successfully creates the DataFrame, calculates the 'Total_Revenue' column, correctly identifies the best-selling product by quantity, and prints the first 5 rows of the updated DataFrame as requested. All imports are handled properly, and the code structure is clear and readable. The solution fully addresses all requirements of the task.",llama-3.3-8b-instruct,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate and print the average ""SessionDurationMinutes""
average_session_duration = df[""SessionDurationMinutes""].mean()
print...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the average session duration, and counts users by device type, fulfilling all requirements of the task. The code is concise, uses appropriate pandas functions (`.mean()`, `.value_counts()`), and the output formatting for the average session duration is good. The structure is clear and easy to understand. While the prompt asked for ""Provide only Python code"" and the ground truth included extensive comments, the generated code being evaluated is pure Python and follows the expected output format requirements. It fully solves the task as requested.",llama-3.3-8b-instruct,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

# Find and print the maximum ""TemperatureCelsius"" recorded across all sensors
m...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It accurately defines the DataFrame as specified, and correctly finds and prints both the maximum temperature and minimum humidity using the `max()` and `min()` pandas series methods. The code quality is good, with proper imports and a clear structure.

The only minor area for improvement that prevents a perfect 10 is the output format of the print statements. The ground truth code includes units in its output strings (""°C"" for temperature and ""%"" for humidity), which the generated code omits. While not explicitly asked for, adding units makes the output more user-friendly and informative for data science contexts. This is a very minor aesthetic point and does not affect the correctness of the calculations.",llama-3.3-8b-instruct,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# Print the total number of ""Likes"" across all posts
total_likes = df[""Likes""].sum()
print(f""Total number of likes: {total_likes}...",9,"The generated code correctly defines the DataFrame, calculates the total number of likes, and identifies the PostType with the highest average number of shares.

**Strong points:**
*   **Correctness:** All three main requirements of the task (DataFrame creation, total likes, highest average shares) are met accurately.
*   **Conciseness:** The code is efficient and directly addresses the problem without unnecessary complexity.
*   **Readability:** Variable names are descriptive, and the code flow is easy to follow.

**Areas for minor improvement:**
*   **Output String Exactness:** The prompt asks for ""Total number of Likes across all posts"" and the generated code prints ""Total number of likes: {total_likes}"". While functionally correct, the output string is slightly different. Similarly, for the second print statement, the requested output format was ""PostType with the highest average number of Shares: {post_type_highest_avg_shares}"", whereas the generated code includes the average value, which wasn't strictly asked for (though it's good additional info). Adhering to the exact wording in output strings is often a minor detail but can be critical in automated evaluations.
*   **Redundancy in final print:** The generated code calculates `highest_average_shares` but only `post_type_with_highest_average_shares` was requested in the output format. While `highest_average_shares` is useful for verification, including it in the print statement when not explicitly asked for could be seen as slightly deviating from the strict output requirement.

Overall, the code is very good and fully functional for the given task. The minor points are about adhering to exact output format requirements rather than functional correctness.",llama-3.3-8b-instruct,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the average age, and successfully counts patients by gender, fulfilling all requirements of the task. The code is clean, well-structured, and directly addresses the problem statement. The use of an f-string for printing the average age with two decimal places is good practice, and the `value_counts()` method is the most appropriate way to count occurrences for categorical data. The output formats are slightly different from the ground truth but are perfectly acceptable and clear.",llama-3.3-8b-instruct,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}
df = pd.DataFrame(data)

# Print the ""Title"" of the movie with the highest ""Rating""
max_rating_movie = df....",10,"The generated code is excellent. It correctly defines the DataFrame, accurately identifies and prints the title of the movie with the highest rating using the efficient `idxmax()` method, and precisely calculates and prints the average year of all movies. The code is clean, concise, and directly addresses all requirements of the task. It demonstrates a solid understanding of pandas operations. The use of `idxmax()` for finding the index of the maximum value is a very good and idiomatic pandas approach. The formatting for the average year is also correctly applied.",llama-3.3-8b-instruct,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

# Print the total number of cars in the dataset
print(""Total number o...",10,"The generated code correctly defines the DataFrame, accurately calculates and prints the total number of cars, and correctly calculates and prints the average ""PriceUSD"". It addresses all requirements of the task. The imports are proper, and the code structure is clean and readable. The variable names are clear, and the overall solution is concise and effective. While the ground truth used f-strings for more precise formatting, the generated code directly provides the correct numerical output, which fully satisfies the prompt's requirements.",llama-3.3-8b-instruct,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}
df = pd.DataFrame(data)

# Print the name of the Pokemon with the highest ""HP""
ma...",10,"The generated code is excellent. It correctly defines the DataFrame as specified in the task. It then accurately identifies and prints the name of the Pokemon with the highest ""HP"" and the lowest ""Defense"" using appropriate pandas functions (`idxmax()` and `idxmin()`). The code is clean, concise, and directly addresses all requirements of the task. The variable naming is clear, and the f-strings for printing are effective.",llama-3.3-8b-instruct,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('titanic.csv')

# Identify columns with missing values
print(""Columns with missing values: "", df.columns[df.isnull().any()])

# Fill missing Age values with the median age
df['Age'] = df['Age'].fillna(df['Age'].median())

# Fill missing Embarked values with the most frequent port
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode().iloc[0])

# Drop the Cabin column
df = df.drop('Cabin', axis=1)

# Creat...",9,"The generated code correctly addresses all requirements of the task. It loads the data, correctly handles missing `Age` and `Embarked` values, drops the `Cabin` column, and extracts titles using a regular expression. It also correctly displays the shape and head of the cleaned dataframe.

Strengths:
- Correctly identifies and fills missing values for `Age` (median) and `Embarked` (mode).
- Properly drops the `Cabin` column.
- Extracts titles efficiently using `str.extract` with a clear regex pattern.
- Adheres to the display requirements.
- Uses proper imports.

Areas for minor improvement (not critical for the given task, but good practice):
- The Ground Truth handles the `FileNotFoundError` for `titanic.csv`, which is robust. The generated code assumes the file exists.
- The Ground Truth includes a more comprehensive title standardization (e.g., replacing 'Mlle' with 'Miss'), which can be beneficial for downstream analysis but was not explicitly asked for in the prompt. The generated code's title extraction is precise enough for the given prompt.
- The print statement for ""Columns with missing values"" might show columns before cleaning specific ones, which could be confusing. The prompt asked to ""identify columns with missing values"" without specifying how to display them.

Overall, the generated code is highly effective and meets the task requirements well.",llama-3.3-8b-instruct,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}
df = pd.DataFrame(data)

# Print the ""Titl...",10,"The generated code is excellent. It correctly defines the DataFrame as specified, precisely identifies the title of the book with the highest price using `idxmax()`, and accurately calculates the total number of books in stock using `sum()`. The code is clean, concise, and directly addresses all parts of the task. The variable names are clear, and the f-string formatting for output is appropriate. There are no missing imports or structural issues.",llama-3.3-8b-instruct,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Error loading dataset: {e}"")
        return None

# Perform exploratory data analysis
def exploratory_data_analysis(data):
    # Display the first 10 rows
    print(data.head(10))
    
    # Show the shape of the dataset
    print(data.shape)
    
    # List all column names and their data t...",6,"The generated code attempts to address all requirements but falls short in several areas regarding correctness and robustness.

**Strengths:**
*   Properly uses pandas for data loading and manipulation.
*   Includes a function for loading the dataset, which is good practice.
*   Attempts to display all requested EDA components.

**Weaknesses and Areas for Improvement:**

1.  **Potability Calculation Error:** The most significant error is in calculating potability percentages.
    *   `data[data['Potability'] == 'Yes']` incorrectly assumes the 'Potability' column contains string 'Yes' or 'No'. In the `water_potability.csv` dataset, the 'Potability' column contains numerical values (1 for potable, 0 for non-potable). This will result in 0 potable counts and incorrect percentages.
    *   The `potable_count/len(data):.2%` format is good, but the underlying calculation is wrong.
2.  **Output Clarity and Formatting:**
    *   The ground truth code uses `print` statements to clearly delineate each section (e.g., ""--- 1. First 10 rows of the dataset ---""). The generated code simply prints the output without any descriptive headers, making it harder to follow which output corresponds to which task.
    *   `print(data.info())` is incorrect; `data.info()` prints directly and returns `None`, so `print(data.info())` will print the info summary followed by `None`. It should simply be `data.info()`.
3.  **Robustness (Error Handling/Assumptions):**
    *   It assumes the 'Potability' column always exists. The ground truth includes a check (`if 'Potability' in df.columns:`), which is more robust.
    *   The `load_dataset` function catches `Exception`, which is broad. Specific `FileNotFoundError` is better for user feedback.
4.  **Completeness/Precision:**
    *   For missing values, `data.isnull().sum()` is correct, but displaying `missing_values[missing_values > 0]` (as in ground truth) is often more informative as it only shows columns with actual missing data, especially in large datasets. The ground truth also explicitly states ""No missing values found"" if none exist.
    *   While `data.select_dtypes(include=['int64', 'float64']).columns` to select numerical columns is correct, `df.describe()` automatically handles this, making the explicit selection redundant unless specific columns were to be excluded.
    *   The `numpy` import is not used.
    *   The task explicitly asked to display the first 10 rows, the generated code only calls `data.head(10)` which correctly displays it but without a heading.

To achieve a higher score, the generated code needs to correct the potability calculation, improve output formatting and descriptions, and add more robust error handling and checks.",llama-3.3-8b-instruct,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Identify columns with missing values
missing_values = df.isnull().sum()
print(""Missing Values Before Cleaning:"")
print(missing_values)

# Fill missing values using mean for numerical columns
numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns
df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].mean())

# Check for and remove any duplicate ...",5,"The generated code attempts to address the requirements but falls short in several critical aspects.

**Correctness & Completeness Issues:**

1.  **Missing Value Reporting (Before):** The generated code calculates `missing_values` but then prints `df.isnull().sum().sum()` for ""Missing Values Count Before"" *after* imputation. This means the ""before"" count is actually the ""after"" count, making the ""before/after"" comparison for missing values incorrect and misleading. The ground truth correctly captures and displays missing values *before* imputation.
2.  **Range Verification:** The requested range verification (`4. Verify that all values are within reasonable ranges for water quality parameters`) is not implemented correctly. The generated code simply prints the min/max values for each numerical column. It does *not* verify against *reasonable ranges* (e.g., pH 0-14, other parameters >= 0) nor does it *remove* rows that fall outside these ranges as specified implicitly by ""Verify that all values are within reasonable ranges"". The ground truth code correctly defines and applies these physical range filters. This is a significant omission.
3.  **Before/After Comparison (Shape):** Similar to missing values, the ""Dataset Shape Before"" is printed *after* operations like imputation and duplicate removal, making it inaccurate. The ground truth correctly captures the initial shape.
4.  **Error Handling:** The generated code lacks basic error handling for file not found, which is present in the ground truth and is good practice.
5.  **Output Clarity:** The ground truth code provides more descriptive print statements, clearly delineating each step of the cleaning process and the results of that step (e.g., how many duplicates were removed, how many rows removed due to range issues). The generated code's output is less informative.

**Minor Quality Issues:**

*   While `df.drop_duplicates()` is correct for removing duplicates, the ground truth's approach of showing *how many* duplicates were removed offers better transparency.
*   The `print(""\nDuplicate Rows Removed"")` statement is very generic and doesn't confirm if any duplicates were actually found and removed, unlike the ground truth.

Overall, the core logic for missing value imputation and duplicate removal is present, but the crucial aspects of before/after reporting accuracy and the critical range verification step are either incorrect or completely missing, significantly impacting the code's ability to fulfill the task requirements thoroughly.",llama-3.3-8b-instruct,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('water_potability.csv')

# 1. Bar chart for potable vs non-potable water
plt.figure(figsize=(8,6))
plt.bar(df['Potable'], [1 if x == 'TRUE' else 0 for x in df['Potable']], edgecolor='black')
plt.title('Distribution of Potable vs Non-Potable Water Samples')
plt.xlabel('Potable')
plt.ylabel('Count')
plt.show()

# 2. Histograms for pH, Hardness, and Chloramines
plt.figure(figsize...",6,"The generated code attempts to address all parts of the task but has several inaccuracies and areas for improvement, particularly regarding the dataset's structure and visualization choices.

**Strengths:**
*   **Imports:** Correctly imports `pandas`, `matplotlib.pyplot`, and `seaborn`.
*   **Structure:** Organizes the code into sections for each visualization.
*   **Basic Visualization Types:** Correctly chooses bar chart, histograms, boxplot, and heatmap for the respective tasks.

**Weaknesses and Areas for Improvement:**

1.  **Bar Chart (Potability Distribution):**
    *   **Incorrect column usage:** The ground truth shows `Potability` is a column with numeric values (0 and 1). The generated code assumes `df['Potable']` exists and then tries to convert its values (e.g., `[1 if x == 'TRUE' else 0 for x in df['Potable']]`), which is a fundamental misunderstanding of the dataset's structure as implied by the task description and ground truth (where 'Potability' is the column name). This will likely lead to an error or an incorrect plot if such a column doesn't exist or has different values.
    *   **Incorrect plotting method:** Using `plt.bar` directly with a custom list for heights is less robust for categorical counts than `sns.countplot`, which is designed for this exact purpose and correctly handles the counting.
    *   **Missing labels/clarity:** The x-axis labels are not clearly defined for 'Potable' (e.g., what does 0 and 1 represent?).

2.  **Histograms (pH, Hardness, Chloramines):**
    *   **Column Names:** Uses 'PH' instead of 'ph' (case sensitivity matters for column names).
    *   **Missing Value Handling:** While `sns.histplot` automatically handles NaNs, the ground truth explicitly uses `.dropna()` to make this explicit and for clarity, and also uses `fig, axes` for better subplot organization. The generated code uses `plt.subplot` repeatedly, which is functional but less idiomatic for this task when creating multiple subplots than `plt.subplots`.
    *   **Titles/Labels:** Though present, they could be more descriptive (e.g., ""Distribution of pH Levels"" vs. ""Distribution of pH"").

3.  **Boxplot (pH vs. Potability):**
    *   **Column Names:** Again, uses 'PH' instead of 'ph' and `Potable` instead of `Potability`. This is a critical error given the actual dataset column names.
    *   **Missing x-axis clarification:** Similar to the bar chart, the x-axis for 'Potable' should have clear labels (e.g., 'Not Potable', 'Potable') as shown in the ground truth.

4.  **Correlation Heatmap:**
    *   **Feature Selection:** `df.select_dtypes(include=['int64', 'float64'])` is a good general approach, but the ground truth simply uses `df.corr()` which by default works on numerical columns. For this specific dataset, both approaches would likely yield similar results.
    *   **Missing Context:** The title ""Correlation Heatmap"" is generic; ""Correlation Heatmap of Water Potability Features"" is more specific and useful.
    *   **Label Readability:** The ground truth includes `plt.xticks(rotation=45, ha='right')` and `plt.yticks(rotation=0)` for improved readability of labels, which is a good practice for heatmaps with many features. This is missing in the generated code.

5.  **Missing Value Handling (General):** The task explicitly asks to ""handle missing values in plots"". While `seaborn` plotting functions often gracefully handle NaNs by ignoring them (as noted in the ground truth for `histplot` and `boxplot`), the generated code doesn't explicitly demonstrate awareness or specific handling (like `dropna()` in the ground truth for histograms, or specific notes in the code).

6.  **Readability/Best Practices:** The ground truth sets `sns.set_style(""whitegrid"")` for consistent styling and includes error handling for `FileNotFoundError`, which are good production practices that are missing in the generated code. `plt.tight_layout()` is used more consistently and effectively in the ground truth, preventing overlapping elements.",llama-3.3-8b-instruct,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.preprocessing import StandardScaler
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Create new features
df['alcohol_to_sugar_ratio'] = df['alcohol'] / df['residual_sugar']
df['total_acidity_to_ph_ratio'] = df['total_acidity'] / df['pH']
df['citric_acid_to_density_ratio'] = df['citric_acid'] / df['densi...",5,"The generated code attempts to address the prompt but falls short in several critical aspects, leading to significant correctness and completeness issues.

**Correctness Issues:**

*   **Feature Engineering - Incorrect Column Names:** The engineered features `alcohol_to_sugar_ratio` and `total_acidity_to_ph_ratio` use column names `residual_sugar` and `total_acidity` respectively. The `winequality-red.csv` dataset does *not* contain a `residual_sugar` column (it's `residual sugar`) nor a `total_acidity` column (this would be `fixed acidity` or a sum of acidities). This will cause `KeyError` or produce incorrect features.
*   **Feature Engineering - Meaningfulness:** The feature `citric_acid_to_density_ratio` is less intuitively meaningful in the context of wine quality compared to, say, acidity ratios or interactions derived from primary components. The ground truth provides more contextually relevant engineered features.
*   **Scaling Incorrectly:** `df_std = scaler.fit_transform(df.drop('quality', axis=1))` scales the original features *before* engineered features are added back into the DataFrame for feature selection. The engineered features are then implicitly excluded from the `df_std` DataFrame used for RFE and SelectKBest. This is a subtle but critical logical error in the data preparation pipeline. The engineered features must be part of the `X` (features) that are scaled.
*   **Feature Name Retrieval after Scaling:** `df.columns[1:][rfe.support_]` and `df.columns[1:][ skb.get_support()]` attempt to get column names. `df.columns[1:]` includes `'fixed acidity'`, `'volatile acidity'`, etc., and the target `'quality'` which is dropped *before* scaling, so the indices will be off when trying to map back to the scaled `df_std` (which is a numpy array without column names). The target `quality` is also being included in `df.columns[1:]` which is incorrect.
*   **RFE Feature Importances:** The line `rf importances = rf.fit(df_std, df['quality']).feature_importances_` tries to get feature importances from the *original* `rf` estimator fit on *all* scaled features, not just the features selected by RFE. To get meaningful importances for RFE, one should train the RandomForest on *only the features selected by RFE* and then extract importances from *that* refit model, as shown in the ground truth. The current approach will include importances for features *not* selected by RFE.
*   **SelectKBest Scores Loop:** `skb_scores = skb.scores_` correctly gets all scores. However, the loop `for i in range(len(skb_features)): print(f""Feature: {skb_features[i]}, Score: {skb_scores[i]}"")` will lead to an `IndexError` because `skb_scores` contains scores for *all* original features, not just the selected ones. To get scores for only selected features, one needs to filter `skb_scores_` using `skb.get_support()`, similar to the ground truth.

**Completeness Issues:**

*   **Comparison and Contrast:** The comparison is very minimal: it only prints the feature names. It completely lacks the detailed comparison and contrast analysis, which was a specific requirement (""Compare and contrast the feature sets selected by each method""). The ground truth provides a textual explanation of the differences between RFE and SelectKBest and why their results might differ.
*   **Displaying Importance/Rankings:** While it attempts to print importances/scores, the output format and the correctness of the scores obtained (especially for RFE) are problematic. The ground truth presents this clearly in a DataFrame.
*   **Error Handling/Robustness:** The code lacks any error handling for file not found, or for potential `NaN`/`inf` values introduced by division in feature engineering. While not explicitly asked for in the prompt, it significantly improves code quality and robustness.
*   **Clarity of Output:** The output for feature importances and scores is not well-formatted or clearly attributed to which method it belongs compared to the ground truth.

**Code Quality Issues:**

*   **Variable Naming:** `df_std` is misleading as it implies a DataFrame but it's a NumPy array.
*   **Consistency in Feature Selection:** While `rfe.support_` and `skb.get_support()` correctly give boolean masks, the subsequent indexing like `df.columns[1:][...]` is prone to errors, especially given the scaling issue.
*   **No comments:** Lack of comments makes it harder to understand the intent behind certain operations.

Overall, the generated code demonstrates a basic understanding of the methods but fails to correctly implement key steps, especially in data preparation, feature selection analysis subsequent steps, and output formatting.",llama-3.3-8b-instruct,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, XGBRegressor, AdaBoostRegressor
from sklearn.model_selection import KFold, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load and preprocess the dataset
df = pd.read_csv('video_games_sales.csv')
df = df.dropna()
categorical_cols = df.select_d...",7,"The generated code addresses most of the requirements but has significant issues with data preprocessing, model evaluation consistency, and general robustness.

**Strong Points:**
*   **Attempts to cover all requirements:** It includes individual ensemble models, voting, stacking, hyperparameter tuning, feature importance, and residual plots.
*   **Basic structure:** Uses a loop for evaluating individual models and prints performance metrics.
*   **Correct use of `KFold` for cross-validation.**
*   **Correct use of `VotingRegressor` and `StackingRegressor`.**

**Areas for Improvement / Missing/Incorrect Aspects:**

1.  **Data Preprocessing (Major Issue):**
    *   **Missing Value Handling:** The code uses `df.dropna()`. This is a very aggressive approach and can lead to significant data loss, especially if 'User_Score' (which is textual 'tbd' sometimes) is present. The ground truth uses `SimpleImputer` and converts 'User_Score' to numeric first.
    *   **Categorical Encoding:** `pd.get_dummies` is used, which is simpler but less robust than `OneHotEncoder` within a `ColumnTransformer` (as in ground truth). `get_dummies` directly applied to the full DataFrame also complicates feature importance mapping and pipeline building later.
    *   **No `ColumnTransformer` or `Pipeline` for Preprocessing:** The generated code applies preprocessing directly to `X` before model fitting. This is problematic for cross-validation where preprocessing steps (like imputation or encoding) should ideally be learned *only* on the training data of each fold to prevent data leakage. The ground truth correctly uses `Pipeline` and `ColumnTransformer` to integrate preprocessing into the model fitting process. This is a critical omission for robust ML workflows.
    *   **Handling of 'User_Score' specifically:** The ground truth specifically handles `User_Score` converting 'tbd' to NaN before imputation, which is not done here.

2.  **Model Evaluation Consistency (Minor Issue):**
    *   The evaluation metrics are printed within the cross-validation loop for individual models, but then there's a separate section (Task 8) where models are refit on the *entire* dataset (`X`, `y`) and evaluated. This leads to evaluation results that might be overly optimistic (due to training and testing on the same data) and inconsistent with the cross-validation results. The goal of cross-validation is to provide a more reliable estimate of generalization performance. Task 8 should summarize the CV results.

3.  **Hyperparameter Tuning:**
    *   **Model Selection:** It hardcodes `GradientBoostingRegressor` for tuning without dynamically determining the 'best individual model' first, which was a requirement.
    *   **Pipeline Integration:** Tuning is done on `X` and `y` directly, not within a pipeline that includes preprocessing. This means the best parameters found might not be optimal if preprocessing steps were also included in the search space or if data leakage from preprocessing occurred.

4.  **Feature Importance:**
    *   **Applicability:** `model.feature_importances_` is not universally available for all models (e.g., `AdaBoostRegressor` doesn't directly have it unless its base estimator is tree-based, and it's not present for `VotingRegressor` or `StackingRegressor` as a whole). The code blindly assumes it's available for all models in the `models` dictionary.
    *   **Feature Names:** When `get_dummies` is used, the feature names become explicit. However, the visualization uses `range(len(model.feature_importances_))` for x-axis ticks, which doesn't show the actual feature names, making the plots less interpretable. The ground truth explicitly gets feature names after one-hot encoding.
    *   **Re-fitting Models:** Models are re-fit again on the entire `X, y` just for feature importance plotting, which is redundant given they were already fitted for CV, and could be done more efficiently.

5.  **Residual Plots:**
    *   Similar to evaluation, models are fit on the *entire* dataset `X, y` for residual plotting, which can give an overly optimistic view of error distribution (especially for models that might overfit). Ideally, residuals are analyzed on out-of-sample predictions (e.g., from a test set or cross-validation predictions).
    *   The plotting logic for residuals is basic; the ground truth provides a more standard residual vs. predicted plot and a histogram of residuals.

6.  **Readability/Robustness:**
    *   No error handling for file loading.
    *   Missing `np.random.seed` for full reproducibility.
    *   The variable `ilan` in the voting ensemble loop seems like a typo (`y ilan.iloc[val_index]`).

To improve, the generated code needs to prioritize sound ML practices, particularly regarding data leakage prevention and robust preprocessing within a `Pipeline`. It also needs to dynamically choose the best model for hyperparameter tuning and improve the interpretability of visualizations.",llama-3.3-8b-instruct,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import TimeSeriesSplit
from pymc3 import Model, Normal, traceplot
from scipy.stats import linregress

# Load dataset
df = pd.read_csv('video_games_sales.csv')

# Aggregate sales data by year
df_grouped = df.groupby('Year')['...",1,"The generated code attempts to address some requirements but largely fails due to fundamental errors, incorrect implementations, and significant omissions.

**Key Failures and Missing Critical Components:**

1.  **Data Loading and Aggregation (Incorrect):**
    *   The `Year` column does not exist in `video_games_sales.csv`. It's typically `Year_of_Release`. Using `df_grouped = df.groupby('Year')['Global_Sales'].sum().reset_index()` will fail instantly.
    *   It doesn't handle missing `Year_of_Release` values or convert them to numeric, which is crucial for time series analysis.

2.  **ARIMA Modeling (Fundamental Errors):**
    *   `statsmodels.tsa.arima.model.ARIMA` requires the `endog` (dependent variable) and `exog` (exogenous variables, if any). It does not take a DataFrame directly for `endog`.
    *   Crucially, `statsmodels.tsa.arima.model.ARIMA` needs a `DateTimeIndex` for time series analysis. The generated code does not convert the aggregated `Year` column to a DateTimeIndex, which will lead to errors or incorrect behavior for time series models.
    *   It hardcodes the ARIMA order `(5,1,0)` without any `auto_arima` or model selection, which is a major omission for an ""expert"" solution.
    *   `arima_results.forecast(steps=5)` will likely fail without a proper time series index setup.
    *   The confidence interval plotting `arima_forecast.std()` is incorrect and will not provide proper confidence intervals from `arima_results.conf_int()`.

3.  **Seasonal Decomposition (Incomplete/Potentially Incorrect):**
    *   While `seasonal_decompose` is called, it's applied to yearly data, where meaningful ""seasonality"" is typically monthly or quarterly. For yearly data, `period` needs to be specified for long-term cycles, which is not done (leaving it at default or failing if `period` is not auto-deduced properly).

4.  **Multivariate Regression (Critically Flawed):**
    *   `X = df[['Year', 'Genre', 'Platform', 'Publisher']]` and then `model.fit(X,y)` will fail because `LinearRegression` cannot handle categorical string data (`'Genre'`, `'Platform'`, `'Publisher'`). One-hot encoding or similar preprocessing is completely missing.
    *   Interaction terms are a specific requirement but are entirely absent.

5.  **Polynomial Regression with Regularization (Critically Flawed):**
    *   `X_poly = pd.concat([X, X**2], axis=1)` is fundamentally incorrect for creating polynomial features for `X` containing categorical columns like `Genre`, `Platform`, `Publisher`. You can't square categorical strings. It also doesn't handle the 'Year' properly for polynomial features.
    *   Scaling for regularization is missed, which is essential.

6.  **Bayesian Regression (PyMC3 - Critically Flawed/Incomplete):**
    *   `from pymc3 import Model, Normal, traceplot` is a good start, but the model definition `with Model() as model: ...` is a generic template, not a specific model for the given data (`Global_Sales` based on predictors).
    *   `mu = Normal('mu', mu=0, sd=1)` and `sigma = Normal('sigma', mu=0, sd=1)` are defined, but how they relate to the predictors (`Year`, `Genre`, etc.) for `mu` is not defined. The linear model `mu = intercept + beta*X` structure is missing.
    *   `trace = traceplot(pymc3_model)` calls a non-existent `pymc3_model` variable. It should be `pm3.sample` first, then `az.plot_trace(trace)`.
    *   No handling of categorical variables for PyMC3.
    *   Crucially, **no confidence intervals or prediction intervals** from the Bayesian model (e.g., `pm3.sample_posterior_predictive`) are extracted or used.

7.  **Change Point Detection (Incorrect Library/Implementation):**
    *   `fromOutdoor_change import changepoint` is an incorrect import. The standard library for this is `ruptures`.
    *   Even if that were correct, the usage `cp = changepoint(df_grouped['Global_Sales'])` is likely insufficient and lacks parameters for model type and penalty.

8.  **Survival Analysis (Critically Flawed/Misunderstood):**
    *   The task explicitly asks for `game longevity` (years a game remains profitable) and implies Kaplan-Meier and Cox proportional hazards.
    *   The generated code uses `scipy.stats.expon` which is for general exponential distributions, not for fitting survival models to `Time` and `Event` data. This fundamentally misunderstands survival analysis.
    *   The approach `exponential_dist = expon.pdf(df_grouped['Year'], scale=1)` and `game_longevity = expon.rvs(scale=1, size=len(df_grouped))` are entirely unrelated to modeling actual game longevity from the dataset.
    *   There is no attempt to define `Longevity_Years` or an `Observed_Event` column, which are central to survival analysis. It completely misses libraries like `lifelines`.

9.  **Confidence and Prediction Intervals (Incomplete/Incorrect):**
    *   `arima_ci = arima_results.conf_int()` is good, but is not fully utilized or correctly plotted.
    *   `poly_ci = ridge_model.predict(X_poly)` is simply a prediction, *not* a confidence or prediction interval. Sklearn models do not directly provide these, and an approximation or bootstrapping method is needed (as shown in the ground truth).
    *   Confidence/prediction intervals for multivariate regression, Bayesian regression, and survival analysis are either missing or fundamentally incorrect.

10. **Time Series Cross-Validation (Incomplete/Incorrect):**
    *   Uses `TimeSeriesSplit` but applies it to `df_grouped` which is yearly sales. The internal `model.fit(X_train, y_train)` uses `LinearRegression` which expects `X_train` to be features, not just a time series dataframe.
    *   It doesn't perform TS-CV for all models (ARIMA is notably missing from this section), only a generic regression model. The polynomial and regularized models are not properly evaluated with TS-CV.
    *   No appropriate metrics (e.g., RMSE) are calculated or printed.

**Overall:**

The code demonstrates a superficial understanding of the requested tasks. It imports some relevant libraries but then uses them incorrectly or incompletely, often replacing complex statistical modeling with trivial or erroneous lines of code. Many critical data preprocessing steps (e.g., handling missing values, converting data types, one-hot encoding) are entirely overlooked. The most advanced requirements like Bayesian regression, change point detection, and survival analysis are either implemented with the wrong tools or in a trivial, non-functional manner.",llama-3.3-8b-instruct,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE, UMAP
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.decomposition import PCA
from Orange.classification import AssociationRuleMining
from orange.featureiligence import Featureiligence
from orange.r...",3,"The generated code attempts to address the task requirements but largely fails due to fundamental errors, incorrect assumptions, and a reliance on non-standard or incorrectly used libraries.

**Primary Reasons for Failure:**

*   **Incorrect Feature Engineering:**
    *   `df['Regional_Sales_Distribution'] = df['Europe_Sales'] / df['Global_Sales']`: The task specifies ""proportion of sales across regions"" (`NA_Sales_Prop`, `EU_Sales_Prop`, etc.). The generated code only creates one, and `Europe_Sales` is not a column in the dataset (it should be `EU_Sales`). This also doesn't handle division by zero.
    *   `df['Market_Position'] = df['Genre'].map(df['Genre'].value_counts()).rank(method='min', ascending=False)`: This calculates the rank of the *count* of each genre, not the rank of games *within* their genre based on sales, which was the requirement (`rank within genre by Global_Sales`).
    *   `df['Publisher_Portfolio_Size'] = df['Publisher'].map(df['Publisher'].value_counts()).rank(method='min', ascending=False)`: Similar to market position, this seems to rank publishers by their count, not the number of *unique* games they've published.

*   **Incorrect Data Preprocessing/Scaling:**
    *   The `StandardScaler` is applied directly to the entire `df` including non-numerical columns, `Name`, `Genre`, `Platform`, `Publisher`, which will cause errors or meaningless scaling. Numerical and categorical features need separate handling (e.g., OneHotEncoding for categorical).
    *   Missing `dropna` for essential columns like `Name`, `Genre`, `Platform`, `Publisher`, `Year_of_Release`, `Global_Sales`.

*   **Incorrect Library Usage / Non-Standard Libraries:**
    *   `from orange.classification import AssociationRuleMining`, `from orange.featureiligence import Featureiligence`, `from orange.recommendation import Recommender`: These `orange` library imports are completely incorrect and do not exist in standard Orange3 distributions this way. This makes the association rule mining and recommendation system parts non-functional. The task implicitly expects `mlxtend` for ARM and `sklearn` or custom code for recommendations. Even if Orange was intended, the classes/modules are wrong.
    *   `UMAP` is imported from `sklearn.manifold`, which is incorrect. It should be `import umap.umap_ as umap`.

*   **Flawed Optimal Cluster Determination:**
    *   Only uses silhouette score for K-Means and only for K up to 10. The task required elbow method and gap statistic, and evaluation across multiple algorithms.
    *   `optimal_clusters = np.argmax(silhouette_scores) + 2`: This is correct for finding the *index* but the variable `optimal_clusters` is then not used to re-run any clustering algorithms.

*   **Critically Flawed Consensus Clustering:**
    *   The generated consensus clustering is completely wrong. It tries to zip together `kmeans_labels`, `dbscan_labels`, etc., and then tries to form `consensus_labels` by creating lists of tuples, which is not how consensus clustering works. Consensus clustering typically involves a co-occurrence matrix or ensemble methods to find stable clusters.

*   **Incorrect Cluster Profiling and Statistical Significance:**
    *   `cluster_df = df[consensus_labels[cluster]]`: This line attempts to slice the DataFrame `df` using the `consensus_labels`, which are lists of cluster assignments derived incorrectly. This will lead to `KeyError` or `IndexingError`.
    *   `ttest_ind(cluster_df[column], other_cluster_df[column]).pvalue`: The use of `ttest_ind` across every pair of clusters for every column is not the standard way to perform statistical significance testing across *multiple* clusters (ANOVA/Chi-squared is). Also, it operates on the wrongly extracted `cluster_df`.

*   **Flawed Association Rule Mining & Recommendation System:**
    *   As noted, relies on non-existent `orange` modules. `arm.fit(df)` and `recommender.fit(df, kmeans_labels)` are placeholder calls that would not work even if the Orange modules were correctly imported, as they lack the necessary data structure conversions (e.g., transactional data for ARM, user-item matrix for recommendation).

*   **Incorrect Anomaly Detection:**
    *   `OneClassSVM` is imported instead of `IsolationForest`, which was specified in the requirements.

*   **Incomplete and Incorrect Validation:**
    *   `algorithm.labels_` for `DBSCAN`, `GMM`, and `AgglomerativeClustering` is not always directly available after `fit` (for GMM, it's `predict`).
    *   `external_metrics['accuracy'] = accuracy_score(df['Genre'], kmeans.labels_)`: This is a fundamental misunderstanding of external validation. `df['Genre']` is ground truth *feature*, not ground truth *cluster assignment*. Clustering is unsupervised learning; you don't typically have ground truth cluster labels (unless it's a known dataset with labels). Accuracy is used for classification, not for comparing obtained clusters to a feature.

**Summary:**
The code demonstrates an attempt to use various data science techniques but fails at most steps due to a lack of understanding of data preprocessing for clustering, the specific algorithms' requirements, correct library usage, and the underlying statistical concepts for validation and profiling. The reliance on non-existent `orange` library calls prevents key sections of the task from being fulfilled at all.",llama-3.3-8b-instruct,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.datasets import load_diabetes
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.linear_model import LogisticRegression
from...",3,"The generated code attempts to address many aspects of the prompt but fails significantly in correctness, completeness, and code quality for an expert-level task.

*   **Correctness (Major Issues):**
    *   **Feature Engineering:** The `pd.get_dummies(X, columns=['Gender', 'Ethnicity'])` line assumes 'Gender' and 'Ethnicity' columns exist, but the `diabetes.csv` dataset typically contains only numerical features like `Pregnancies`, `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`, `DiabetesPedigreeFunction`, `Age`, and `Outcome`. This will cause an error or incorrect processing.
    *   **PCA usage:** Applying `PCA` indiscriminately without a clear rationale for dimensionality reduction and then losing interpretability for SHAP is not a good practice for this problem.
    *   **Missing Value Handling:** The prompt explicitly asked for comprehensive feature engineering, including handling 0s as NaNs in specific columns, which is completely missing.
    *   **Ensemble Implementations:**
        *   `voting_ensemble`: `SVC` generally needs `probability=True` for soft voting, which is not set.
        *   `bagging_ensemble`: The `base_estimator` for `BaggingClassifier` is incorrectly set to `estimators[0]` (Logistic Regression) without any rationale. It should be a single estimator like `DecisionTreeClassifier` or `LogisticRegression` and it's missing other base estimators like `SVC`.
        *   `boosting_ensemble`: This function just returns a list of estimators. It doesn't *implement* a boosting ensemble. These are individual models.
        *   `stacking_ensemble`: The `estimators` list in `StackingClassifier` should be a list of (name, estimator) tuples, not just estimators. This will likely cause an error. Also, `cv` parameter is crucial for proper stacking to avoid data leakage, and it's missing.
    *   **Bayesian Optimization:** The `objective` function for Optuna (`objective(trial)`) returns `n_estimators, learning_rate` directly instead of a metric (like AUC or accuracy) to be optimized. This is fundamentally incorrect for optimization.
    *   **Nested Cross-Validation:** The code defines `kfold` but never uses it for actual nested cross-validation (outer and inner loops, splitting data, and evaluating models across folds). The `scores = []` line is created but never populated.
    *   **Custom Ensemble:** The `custom_ensemble` function uses hardcoded `weights = [0.2, 0.3, 0.5]` and trains models on the entire `X, y` (full dataset) which is highly prone to overfitting and data leakage. The prompt asked for ""dynamic weight assignment"".
    *   **SHAP Implementation:** `shap.Explainer(model)` typically requires a model and a background dataset or a prediction function. The `shap_values` function's usage might be too simplistic or incorrect for certain models. The visualization part is entirely missing.
    *   **Model Calibration:** `calibration_curve_plot` function returns `fractions, mates` but doesn't actually plot anything as requested.
    *   **Ensemble Diversity/Combination:** `ensemble_diversity` just returns individual model scores, not a diversity measure. `model_combination` just adds probabilities, which is a very naive average, not a sophisticated ""combination strategy."" The requested correlation matrix analysis is completely missing.
    *   **CatBoost:** The prompt explicitly asks for `CatBoost for handling categorical features`, which is typically used for datasets with actual categorical features, and it needs proper configuration (e.g., `cat_features` parameter). The current `diabetes.csv` doesn't have categorical features directly; if `Gender` and `Ethnicity` were added, CatBoost would need to be informed. The generic `catboost_classifier = CatBoostClassifier(n_estimators=100)` is not sufficient for this- if `Gender` and `Ethnicity` were synthesized, CatBoost needs to know.

*   **Completeness (Significant Gaps):**
    *   **Comprehensive Feature Engineering:** Missing polynomial features, interaction terms, handling 0s as NaNs, and general custom features.
    *   **Advanced Ensemble Methods:** While it lists boosting algorithms, it doesn't *implement* them as ensembles (e.g., training and demonstrating their performance). Bagging implementations are basic.
    *   **Nested Cross-Validation:** Only initiates `KFold` but doesn't perform the actual nested CV loop for unbiased evaluation.
    *   **Custom Ensemble:** Lacks dynamic weight assignment (e.g., optimizing weights based on validation performance or using meta-models for weighting).
    *   **Model Interpretation (SHAP):** Only `shap_values` is called; no actual plotting (summary, dependence, force plots) or in-depth analysis.
    *   **Model Calibration:** Only computes `calibration_curve` outputs; no visualization or Brier score calculation.
    *   **Ensemble Diversity Analysis/Combination Strategies:** Missing correlation matrix, true diversity metrics, and more sophisticated combination methods beyond simple averages.

*   **Code Quality:**
    *   **Imports:** Includes `load_diabetes` which is for a different dataset. Imports `trial, trunc_normal` from `optuna` but doesn't use `trunc_normal`.
    *   **Structure:** While functions are defined, the `main` function simply calls them and returns their outputs without demonstrating actual model training, prediction, or evaluation results for comparison, which is essential for a benchmark. No `fit`, `predict`, `evaluate` calls are performed on the returned models after setting up the functions in `main`.
    *   **Readability/Comments:** Minimal comments for complex operations.
    *   **Reproducibility:** Missing `random_state` in many estimators where it's crucial (e.g., `BaggingClassifier`, `AdaBoostClassifier`, `GradientBoostingClassifier`, `LGBMClassifier`, `CatBoostClassifier`, `LogisticRegression`, `SVC`, `DecisionTreeClassifier`). While `KFold` has `random_state`, the models trained within loops would not be reproducible.

Overall, the code provides a scaffold of some requested components but largely fails to correctly implement them or integrate them into a coherent solution for model training, evaluation, and analysis. It's a collection of isolated, often incomplete or incorrect, function definitions.",llama-3.3-8b-instruct,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import arviz as az
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from scipy.stats import norm
import matplotlib.pyplot as plt

# Load and preprocess the dataset
def load_data(file_name):
    data = pd.read_csv(file_name)
    data = data.dropna()  # remove rows with missing values
    data = data.apply(lambda x: x.astype(str).str.lower())  # convert to lower case
    return ...",1,"The generated code attempts to address the prompt but fundamentally misunderstands almost all advanced statistical modeling concepts requested. It provides highly generic, incorrect, or incomplete implementations for each section, making it effectively useless for the given task.

Here are the critical issues:

1.  **Data Loading and Preprocessing:**
    *   `load_data` applies `dropna()` which is a catastrophic error given that the task explicitly asks for ""comprehensive missing value handling"" for known 0-value NaNs in this specific dataset. The ground truth correctly identifies and handles these.
    *   `data.apply(lambda x: x.astype(str).str.lower())` is applied to numerical columns, effectively converting all numerical data to strings and then to lowercase, which is meaningless and makes all subsequent numerical operations (like `pm.math.dot(X, beta)`) fail or produce nonsensical results. The ground truth correctly uses numerical imputation and scaling.

2.  **Hierarchical Bayesian Models:**
    *   The `hierarchical_bayes` function implements a basic logistic regression (`pm.math.inv_logit(mu)`) with a `beta` for each feature and a global `alpha`. It does *not* implement a hierarchical model. A hierarchical model would involve group-level varying effects, which the prompt specifically asked for (e.g., using `Age_Group` as in the ground truth). The current implementation is indistinguishable from the `bayesian_logistic_regression` function in its structure.

3.  **Mixed-Effects Models:**
    *   The `mixed_effects_model` function assumes an 'ID' column exists, which is not present in the `diabetes.csv` dataset and requires explicit creation or simulation, as done in the ground truth for `Age_Group`. The `alpha = pm.Normal('alpha', mu=0, sigma=1, shape=(len(id),))` is incorrect; it creates an `alpha` for *every row* (where `len(id)` is the number of rows), not for unique IDs, and assigns `alpha[id]` which will fail because `id` is a Series, not an index. This is not a correct mixed-effects model.

4.  **Bayesian Logistic Regression:**
    *   The `bayesian_logistic_regression` function is essentially a duplication of the `hierarchical_bayes` function's core logic (minus the `alpha` intercept in `mu`, which is a common mistake). It doesn't demonstrate ""informative priors based on medical knowledge"" as requested; it uses simple `mu=0, sigma=1` priors for all betas.

5.  **MCMC Sampling:**
    *   While MCMC is used (`pm.sample`), the specific requirements are ""anterior inference."" The function `mcmc_sampling` again just duplicates a basic logistic regression. All models implicitly use MCMC, so a separate function solely for this with the same model structure is redundant and unnecessary as per the prompt's intent.

6.  **Survival Analysis:**
    *   The `survival_analysis` function expects `Time` column, which is not in the dataset. The prompt explicitly stated to ""simulate time component"" for survival analysis.
    *   The likelihood `obs = pm.Weibull('obs', alpha=..., scale=..., observed=time)` is a generic Weibull distribution, not a Cox Proportional Hazards model or any other common survival model which would incorporate covariates dynamically. The ground truth correctly uses `lifelines.CoxPHFitter`.

7.  **Gaussian Processes:**
    *   The `gaussian_processes` function attempts to define a GP (`pm.GP('gp', pm.probabilistic.Deterministic, X)`), but this is not how PyMC3 GPs are typically structured or used. It then seems to just layer a GP on top of a logistic regression, which doesn't make sense for ""non-parametric modeling of relationships between variables"" like Glucose vs. Age, as demonstrated in the ground truth. The GP is not used in the likelihood calculation, rendering it ineffective.

8.  **Causal Inference:**
    *   The `causal_inference` function is fundamentally flawed. It includes `ps = pm.Bernoulli('ps', p=pm.math.inv_logit(pm.math.dot(X, beta)))` and `iv = pm.Normal('iv', mu=0, sigma=1)`, which are just new probabilistic variables within the existing logistic regression model. These do *not* implement propensity score matching or instrumental variables. PSM requires estimating propensity scores and then performing matching (or weighting), typically outside PyMC3. IV methods require a specific model structure (e.g., 2SLS) that is not represented here. The ground truth uses `sklearn.linear_model.LogisticRegression` for propensity estimate and `sklearn.neighbors.NearestNeighbors` for matching for PSM, and `statsmodels` for 2SLS.

9.  **Bayesian Model Comparison:**
    *   `bayesian_model_comparison` creates two identical copy-pasted logistic regression models (`model1` and `model2`). Comparing identical models using WAIC/LOO is pointless and does not fulfill the requirement of ""Bayesian model comparison using WAIC and LOO cross-validation"" between *different* relevant models (e.g., hierarchical vs. non-hierarchical).

10. **Credible Intervals and Posterior Predictive Checks:**
    *   `credible_intervals` only calls `az.plot_hdi(trace, var_names='beta')`, showing just one type of visualization and not performing posterior predictive checks. The `plot_hdi` function is also part of ArviZ, not directly related to the model definition inside PyMC3. VPC (posterior predictive check) requires `pm.sample_posterior_predictive` and comparison of simulated versus observed data.

In summary, the generated code demonstrates a very limited understanding of the requested advanced statistical modeling techniques and the specific implementation details required by PyMC3. It is largely a superficial attempt to match keywords from the prompt with generic PyMC3 model structures without correctly implementing the underlying statistical concepts. It is not functional for the given task.",llama-3.3-8b-instruct,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture, SpectralClustering, OPTICS
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.model_selection import train_test_split
from umap import UMAP
from keras.layers import Input, Dense
from keras.m...",3,"The generated code attempts to address many of the requirements but fails significantly in correctness, completeness, and various aspects of code quality, making it largely non-functional for the specified task.

**Major Issues leading to a low score:**

1.  **Fundamental Data Loading and Preprocessing Errors:**
    *   The prompt specifies using `diabetes.csv`, which is a common dataset (Pima Indians Diabetes) with specific column names (Pregnancies, Glucose, BloodPressure, etc.) and characteristics (e.g., zeros representing missing values). The generated code assumes `age`, `bmi`, `hba1c` columns directly, which are not present in the standard `diabetes.csv`. This means the code immediately breaks or operates on incorrect data.
    *   `df.dropna()` is used, which is highly inappropriate for `diabetes.csv` as many 0s are sentinel values for missing data, not actual zeros for medical measurements (e.g., Glucose of 0). The ground truth correctly identifies and imputes these.
    *   Feature engineering (`age_group`, `metabolic_syndrome`, `hba1c_category`) is based on non-existent columns, leading to errors. Even if the columns existed, the approach to metabolic syndrome is overly simplistic and doesn't align with the ground truth's ""advanced feature engineering"" for metabolic syndrome.

2.  **Incorrect Data Preparation for Analysis:**
    *   `train_test_split` is used (`X_train, X_test`, `y_train, y_test`). While common in supervised learning, for *unsupervised* clustering and segmentation, splitting the data into training/testing sets is typically not done this way. All available data for the patients is usually used for clustering. Even when it is used, the `X_train_std` and `X_test_std` are then used interchangeably for clustering fitting and evaluation, which is incorrect. `autoencoder.fit(X_train_std, X_train_std, validation_data=(X_test_std, X_test_std))` is reasonable for training the AE, but then `tsne_embedding = TSNE(n_components=2).fit_transform(X_train_std)` immediately uses `X_train_std` again, demonstrating confusion about the split's purpose here.
    *   Label encoding for `age_group` and `hba1c_category` and one-hot encoding after feature creation is fine in principle, but the base features are wrong.

3.  **Dimensionality Reduction Issues:**
    *   The autoencoder architecture is very basic, but more critically, it computes embeddings (`Dense(2, activation='relu')` in the *output* layer) without exposing the encoder part separately. The ground truth correctly uses `Model(inputs=input_layer, outputs=encoder_output)` to get the actual embeddings. The given code just trains the autoencoder but doesn't actually extract the latent space representation (`X_ae`) for visualization or further use explicitly.

4.  **Clustering and Parameter Determination:**
    *   It attempts to use Silhouette, Calinski-Harabasz, and Davies-Bouldin scores for `KMeans`, but then defaults to `n_clusters=3` for *all* other algorithms (`DBSCAN`, `Hierarchical`, `GMM`, `Spectral`). This defeats the purpose of ""optimal clustering parameters and statistical validation"" as it doesn't determine them for DBSCAN (`eps`, `min_samples`), Hierarchical, GMM (BIC/AIC), Spectral, or OPTICS. The ground truth specifically tackles this for each algorithm using appropriate metrics.
    *   The `DBSCAN` parameters (`eps=0.5, min_samples=10`) are hardcoded, not determined optimally. The `OPTICS` algorithm is used but no attempt is made to extract clusters or interpret its output for optimal parameters.
    *   There is no ""gap statistic"" implementation, though it's mentioned in the problem description. The generated code uses Calinski-Harabasz and Davies-Bouldin, which are different internal validation metrics.

5.  **Missing or Severely Flawed Implementations for Key Requirements:**
    *   **Consensus Clustering:** The implementation `consensus_clusters = [] for algorithm in [kmeans, dbscan, hierarchical, gmm, spectral, optics]: algorithm.fit(X_train_std); consensus_clusters.append(algorithm.labels_)` merely collects the labels from *individual* clustering runs. It does *not* perform consensus clustering (e.g., building a co-occurrence matrix and re-clustering it) as described and implemented in the ground truth. This is a critical failure.
    *   **Patient Phenotype Profiles:** The `if feature != 'outcome' and feature != 'cluster':` loop attempts to perform t-tests but `cluster_df = df[df['cluster'] == cluster]` implies a `cluster` column already exists in `df` which it doesn't. The clusters are assigned to `algorithm.labels_` but not added back to the original `df`. This section will fail. Even if it worked, it's a very simplistic comparison (just t-test for each feature) compared to detailed profiles with mean/std and visualizations.
    *   **Anomaly Detection:** `OneClassSVM` is used, but there's no visualization or integration of the anomaly labels back into the main DataFrame for further use as required.
    *   **Time-Series Clustering:** The code just runs `KMeans` on `X_train_std` again, labelling it `kmeans_ts`. This is a complete misinterpretation of ""time-series clustering"" for which the dataset is cross-sectional. The ground truth correctly acknowledges this and provides a conceptual explanation. The generated code's attempt is nonsensical in context.
    *   **Association Rule Mining:** `frequent_itemsets = apriori(X_train_std, min_support=0.1, use_colnames=True)` is conceptually wrong. `apriori` expects a boolean DataFrame (or one-hot encoded categorical data), not standardized numerical features (`X_train_std`). This will likely fail or yield meaningless results. The ground truth correctly performs discretization before ARM.
    *   **Patient Risk Stratification:** Similar to phenotyping, it assumes a `cluster` column exists (`df['cluster'] == cluster`). The `risk_strata` list just stores the mean outcome for each cluster, which is a start, but the requirement for a ""system with clinical interpretation"" is a deep failure.

6.  **Code Quality / Readability:**
    *   Imports are okay but some (like `PCA`) are imported but not used.
    *   Lack of comments and clear structure for complex steps.
    *   No error handling for file loading or other potential issues.
    *   Plotting is absent for dimensionality reduction or cluster profiles, which were implicitly required for visualization.

In summary, the generated code demonstrates a superficial understanding of the task requirements and specific data domain. Many components are implemented incorrectly, leading to non-functional code or code that produces invalid results. The critical errors in data handling, feature engineering, and core algorithm applications (especially consensus clustering and ARM) lead to a very low score.",llama-3.3-8b-instruct,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import (
    VotingRegressor,
    BaggingRegressor,
    AdaBoostRegressor,
    GradientBoostingRegressor,
    XGBoostRegressor,
    LGBMRegressor,
    CatBoostRegressor
)
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error
from optuna import trial, Troll
from hyperopt import hp, fmin, tpe
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
import shap
from ...",2,"The generated code attempts to address several parts of the prompt but fails significantly in correctness, completeness, and code quality.

1.  **Correctness:**
    *   **Data Preprocessing/Feature Engineering:** The approach to preprocessing is fundamentally flawed. It drops 'Happiness Rank' from X and sets it as y, but then tries to one-hot encode 'Country', which is not a numerical feature directly. It also drops 'Country name' in the ground truth but uses 'Country' for OHE, implying an inconsistent understanding of the dataset's structure. The feature engineering (log transforms) is applied without handling potential zero or negative values. The `XGBoostRegressor`, `LGBMRegressor`, and `CatBoostRegressor` imports are incorrect (should be from `xgboost`, `lightgbm`, `catboost` packages, not `sklearn.ensemble`).
    *   **Ensemble Methods:** The `boosting_regressor` function incorrectly uses `VotingRegressor` to combine different boosting models, which isn't how boosting generally works. Ensemble models are defined but never trained or evaluated properly in the main script flow; they are just defined as functions.
    *   **Stacking:** The `stacking_ensemble` function simply returns a list of estimators, which is not how `StackingRegressor` works in `sklearn`. It completely misses the meta-learner concept and the `StackingRegressor` class.
    *   **Bayesian Optimization:** The `optuna` usage is completely wrong. `optuna.trial.Trial()` is not how you create a `Trial` object (it's managed by the `study.optimize` method). `hyperopt` is imported but not used at all. The optimization function doesn't return an objective value for Optuna to minimize/maximize.
    *   **Nested Cross-Validation:** It correctly initializes `KFold` but then just loops through folds without actually training or evaluating models within the folds, making the section purely ceremonial.
    *   **Custom Ensemble:** The custom ensemble dynamically assigns *fixed* weights, not *dynamic* weights based on performance as requested. It also just returns the models and weights, not a functional ensemble.
    *   **Blending/Multi-level Stacking:** These sections are just model definitions, lacking any actual implementation of the blending or multi-level stacking logic.
    *   **Model Interpretation (SHAP/LIME):** The SHAP explainer initialization is incorrect (`shap.Explainer` without a `model` attribute might work for some models, but usually needs a trained model and data). The LIME explainer initialization is incorrect for regression (`class_names=[str(y_test.mean())]`).
    *   **Model Calibration:** `CalibratedClassifierCV` is used, which is for classification, not regression.
    *   **Diversity Analysis/Combination Optimization:** The definitions are vague and poorly implemented. The `optimize_ensemble` function's `trial.integers` and `trial.cat` are not standard Optuna APIs.

2.  **Code Quality:**
    *   **Imports:** Several imports are incorrect or missing (`XGBoostRegressor`, `LGBMRegressor`, `CatBoostRegressor` should be from their respective libraries, not `sklearn.ensemble`). `optuna.trial`, `Troll` (`from optuna import trial, Troll`) is completely wrong.
    *   **Structure:** The code defines many functions but then just calls them without using their return values for further processing (e.g., training, prediction, evaluation). The overall flow is disjointed.
    *   **Assumptions/Missing Code:** It makes significant assumptions that many standard `sklearn` models can be instantiated directly with `XGBoostRegressor()` without importing from `xgboost`, etc. It also lacks clear training and evaluation steps for most of the defined ensembles.

3.  **Completeness:**
    *   The code largely provides function definitions rather than actual implementations of the task requirements. Many crucial steps (like fitting models, making predictions, calculating metrics, actual hyperparameter optimization, robust cross-validation loops, comprehensive SHAP/LIME interpretations, proper calibration for regression, dynamic weight assignment based on performance) are either missing or incorrectly implemented.
    *   The prompt asked for comprehensive model interpretation using SHAP and LIME, specifically including plots and more detailed usage, which is missing.
    *   Uncertainty quantification is completely absent.
    *   Model combination optimization using mathematical approaches (like minimizing RMSE with constraints) is missing.

In summary, the code demonstrates a superficial understanding of the requirements without implementing the core logic and correct usage of the specified libraries and techniques. It feels like a skeleton with many placeholders and incorrect API calls, making it non-functional for the given task.",llama-3.3-8b-instruct,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import statsmodels.api as sm
from statsmodels.formula.api import glm
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import matplotlib.pyplot as plt
import seaborn as sns

# Load and pre...",2,"The generated code attempts to address all parts of the task but fundamentally fails in its implementation, leading to incorrect or non-functional solutions for most requirements. Missing critical imports, using incorrect libraries/functions for complex statistical tasks, and making gross errors in methodology are prevalent.

**Here's a breakdown of the critical issues:**

1.  **Missing Imports:** Many functions and classes used (`az`, `GLM`, `GMModReg`, `mediation`, `pm.waic`, `pm.loo`, `pm.bayes_factor`, `plot_posterior`) are either not imported or are incorrectly referenced/do not exist as direct functions in the PyMC3 top-level API or the specified `med` or `pyMC3_implementations` modules. This makes large portions of the code outright fail to run.
2.  **Preprocessing:**
    *   `df = df.dropna()` and `df = df.fillna(df.mean())` are contradictory. If you drop NaNs, filling them with the mean is redundant and indicates a lack of understanding of data handling.
    *   The `ColumnTransformer` usage for `X` in the SEM section is completely wrong. `StandardScaler` is applied directly to `X` (which is already a NumPy array), then `ColumnTransformer` is created with a tuple `(('num', StandardScaler(), X.columns))` where `X.columns` would fail as `X` is already a NumPy array. This will cause an error.
3.  **Hierarchical Bayesian Models:**
    *   It assumes `Region` and `Happiness` columns exist directly. The problem states ""model happiness with regional random effects"", which implies `Ladder_score` for happiness and `Regional_indicator` for regions. The code uses `df['Region']` and `df['Happiness']`, which are not directly available in `world-happiness-report-2021.csv` after column renaming (e.g., `Regional_indicator`, `Ladder_score`).
    *   `regional_effects = pm.Normal('regional_effects', mu=0, sigma=sigma, shape=len(df['Region'].unique()))` correctly captures the idea of regional effects but then `regional_effects[df['Region']]` will fail because `df['Region']` (if it existed) is a string and needs to be mapped to integer IDs. The `mu + regional_effects[df['Region']]` line is fundamentally incorrect for indexing random effects.
    *   No informative priors as requested in the prompt.
4.  **Structural Equation Models (SEM):**
    *   The code uses `sm.GLM` as a stand-in for SEM, which is incorrect. SEM involves modeling latent variables and causal pathways, typically done with specialized packages (e.g., `lavaan` in R, `semopy` or `statsmodels` for simpler confirmed factor analysis/path analysis, but not `GLM` for the full scope). The prompt specifically asked for SEM to analyze ""causal pathways"", which `GLM` does not do.
    *   The `ColumnTransformer` part is applied to `X` *after* it's already scaled by `StandardScaler()`, which is redundant and likely wrong.
5.  **Causal Inference (IV and PSM):**
    *   **Instrumental Variables (IV):** `sm.GMModReg` is not a standard Two-Stage Least Squares (2SLS) IV implementation. `sm.OLS` with predicted values from the first stage is the typical way, as shown in the ground truth. Also, `gdp`, `happiness`, `instrument` are directly used as `y`, `X`, `Z` which is not how `GMModReg` works. The choice of `Freedom to Make Life Choices` as an instrument for `GDP` is highly questionable in terms of IV assumptions (exogeneity, relevance).
    *   **Propensity Score Matching (PSM):** `ps_model = Pipeline([('scale', StandardScaler()), ('nn', NearestNeighbors(n_neighbors=1))])` and `ps_model.fit(gdp.values.reshape(-1, 1))` simply scales GDP and fits a nearest neighbor model. This is *not* how PSM works. PSM requires estimating propensity scores (probability of treatment) using a logistic regression, then matching treated to control units based on those scores. A key part of PSM (identifying treatment/control groups) is entirely missing.
6.  **Bayesian Regression with Informative Priors:** While a simple Bayesian regression is implemented, the priors chosen (`mu=0, sigma=1`) are *not* informative, directly contradicting the prompt's requirement for ""informative priors based on happiness research literature"".
7.  **Gaussian Processes (GP):**
    *   The `sklearn.gaussian_process` module is used, but the prompt requested PyMC3/PyMC4. While functional, it deviates significantly from the explicit PyMC3 requirement for Bayesian modeling.
    *   No plotting or interpretation of the GP, which is crucial for non-parametric models.
8.  **Mediation Analysis:**
    *   `from med ｜cure import mediation` is a syntax error (`｜`) and seems to be a placeholder or an incorrect import. `mediation` is not a standard Python package for mediation analysis that would automatically take `X, y, instrument` and calculate mediation. Mediation analysis typically involves separate regression models/simulations, as demonstrated in the ground truth. The `med Conclusion = mediation(...)` line will fail.
9.  **Difference-in-Differences (DiD):**
    *   The simulation of the time component is completely flawed: `df['Year'] = df['Country'].apply(lambda x: int(x.splitckt_split_[0]))` assumes `Country` name contains year information which is not true for this dataset. This will cause an error.
    *   The PyMC3 model for DiD is incorrect. A DiD model typically involves `outcome ~ treated * post + controls`, which is a linear regression; the PyMC3 model here is a simple regression of happiness on a (non-existent) year. It doesn't capture the DiD logic.
10. **Bayesian Model Comparison:**
    *   `pm.waic`, `pm.loo`, `pm.bayes_factor` are deprecated or not directly available as top-level functions in PyMC3. `arviz` is the correct library for these computations (`az.waic`, `az.loo`, `az.compare`). ArviZ is not imported either. This section will fail.
11. **Credible Intervals, PPC, Sensitivity Analysis:**
    *   `from pyMC3_implementations import plot_posterior` and `import plot_posterior` are redundant and `pyMC3_implementations` is not a standard module. `plot_posterior.plot(trace)` will fail. ArviZ (`az.plot_posterior`, `az.plot_ppc`) is the correct tool for these.
    *   No sensitivity analysis is actually performed, merely commented upon.

Overall, the generated code demonstrates a very limited understanding of the requested advanced statistical methods and how to implement them correctly in PyMC3, ArviZ, and other scientific Python libraries. It contains numerous syntax errors, logical flaws, and uses incorrect methodologies for nearly every specific task.",llama-3.3-8b-instruct,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture, SpectralClustering, HDBSCAN
from sklearn.decomposition import TruncatedSVD
from sklearn.manifold import TSNE, MDS
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import silhouette_score, calc_v_measure_score, calinski_harabasz_score
from sklearn.neural_netwo...",4,"The generated code attempts to address many aspects of the prompt but falls significantly short in implementation quality, correctness, and completeness compared to the advanced requirements.

**Correctness & Completeness Issues:**

*   **1. Loading and Preprocessing:**
    *   `df['happiness_profile'] = df['Happiness Rank'] / df['Population']` and `df['regional_characteristic'] = df['Region']`: These lines are problematic. `Happiness Rank` and `Population` are not in the `world-happiness-report-2021.csv` directly. The actual file uses 'Ladder score', 'Logged GDP per capita', etc. 'Region' is also not a direct column, it's 'Regional indicator'. This indicates a fundamental misunderstanding of the dataset structure.
    *   `df.dropna()`: This is a very crude imputation method. The prompt asked for ""advanced feature engineering including happiness profiles and regional characteristics"" and the ground truth used median/mode imputation, advanced feature creation (interaction terms), and one-hot encoding. The generated code's feature engineering is minimal and incorrect based on the dataset.
    *   Scaling: It scales `df.drop('Country', axis=1)` which includes non-numerical columns not suitable for scaling unless properly handled (e.g., one-hot encoding).

*   **2. Multiple Clustering Algorithms:**
    *   `HDBSCAN` is imported but then used as `hdbscan = HDBSCAN(min_samples=10)` when the module is `hdbscan.HDBSCAN`. This is a minor syntax error, but indicative of lack of testing.
    *   Optimal parameter determination is extremely simplistic and often incorrect (e.g., `kmeans.n_clusters = kmeans.elbow_value_` would not work without `yellowbrick`'s visualizer actually running and setting that attribute). The prompt asked for more robust methods like silhouette analysis, gap statistic, and stability analysis for *optimal parameter determination* for *all* algorithms (where applicable).

*   **3. Advanced Dimensionality Reduction:**
    *   `UMAP` is imported but not used.
    *   `AutoEncoder` is imported as `from sklearn.neural_network import AutoEncoder` which is incorrect. `AutoEncoder` is not a standard `sklearn` module. It needs to be built with a deep learning framework like Keras/TensorFlow, as shown in the ground truth. The line `ae = AutoEncoder())(input_shape=(df.shape[1],))` is a syntax error and not a valid AutoEncoder implementation.
    *   `MDS` (Multidimensional Scaling) is imported but not used. `Manifold learning` was mentioned as a category, and `MDS` is one technique, but the prompt specifically asked for t-SNE, UMAP, autoencoders, and manifold learning (specifically Isomap in GT for a concrete example).

*   **4. Optimal Clustering Parameters:**
    *   The `yellowbrick` library is used for K-Elbow and Silhouette visualizers, which are good for *visualizing*. However, the code then manually calculates silhouette scores and gap scores, but the `gap_score` function is not standard `sklearn` (requires custom implementation or specific library like `gaps`).
    *   Stability analysis with `adjusted_rand_score` on `df_scaled` vs. `labels` is fundamentally flawed. `adjusted_rand_score` measures agreement between two clusterings/label sets. The current usage is incorrect; it should compare `labels` from one run to `labels` from a perturbed run or a different initialization of the *same algorithm*.

*   **5. Consensus Clustering:**
    *   The implementation `consensus_labels = mode(labels)[0][0]` is a gross misinterpretation of consensus clustering. This line simply takes the `mode` of the *first* K-Means run's labels for the *first* sample, not a consensus across *multiple algorithms* or multiple runs. True consensus clustering involves a co-association matrix or similar aggregation of results from different algorithms/runs.

*   **6. Detailed Country Profiles:**
    *   The loop `for cluster in range(5):` assumes exactly 5 clusters, which might not be the case and doesn't adapt to the (incorrectly) determined `consensus_labels`.
    *   The plotting `plt.bar(countries, happiness)` attempts to plot happiness scores of individual countries for each cluster on a bar chart and tries to show statistical significance between each country's happiness score which is not how country profiling is typically done (it should compare *feature distributions/means across clusters*).
    *   Statistical significance testing: `ttest_ind(happiness[i], happiness[j])` is comparing two *single values* (the happiness scores of two countries), not two groups of data. This is statistically nonsensical. It should compare the *distribution* of a feature (e.g., GDP per capita) in one cluster against another, or all clusters using ANOVA, as the ground truth does. Effect size analysis is completely missing.

*   **7. Anomaly Detection:**
    *   Only One-Class SVM is used, not multiple methods like Isolation Forest and LOF as requested.

*   **8. Network Analysis:**
    *   `if df['Happiness Score'].iloc[i] > df['Happiness Score'].iloc[j]:` is a very arbitrary and non-standard way to define relationships for network analysis based on ""happiness factors."" It does not use ""happiness factors"" (the features) to determine similarity, but merely a hierarchical relationship based on the single happiness score. The prompt asked for relationships based on *happiness factors* (features), implying statistical similarity.

*   **9. Association Rule Mining:**
    *   `frequent_itemsets = apriori(df[['Happiness Score', 'Family', 'Health Life Expectancy']], min_support=0.1)`: `Happiness Score`, `Family`, and `Health Life Expectancy` are numerical and need to be *discretized (binned)* before applying `apriori`. This step is crucial and missing, making the ARM potentially ineffective or incorrect. `Family` and `Health Life Expectancy` also do not exist as column names in the provided dataset variant.

*   **10. Country Recommendation System & Policy Insights:**
    *   Recommendation system: `df_scaled[0]` for `NearestNeighbors` implies using only the first row's data for finding neighbors, not a target country. It's not a generic system.
    *   Policy insights: The insights provided are hardcoded qualitative statements, not derived directly from the analysis. The prompt asked for insights *based on clustering results* (e.g., comparing cluster profiles), which would lead to dynamic, data-driven insights. ""Geopolitical validation"" is mentioned but not implemented.

**Code Quality Issues:**

*   **Imports:** Many imports are present (`TruncatedSVD`, `MDS`, `train_test_split`, `calc_v_measure_score`, `calinski_harabasz_score`, `scipy`, `statsmodels.api`, `Birch`) but remain unused or are used incorrectly. `UMAP` is imported but not used. `AutoEncoder` import is incorrect.
*   **Structure:** The code is a monolithic script without functions or classes, making it difficult to read, debug, and reuse. The ground truth provides a class-based structure, which is much better for a complex multi-step analysis.
*   **Variable Names:** `kmeans elbows`, `kmeans.elbow_value_` are unconventional and likely incorrect usage even with `yellowbrick`.
*   **Readability:** No comments explaining complex parts or decisions.
*   **Non-existent libraries/functions:** `AutoEncoder` from `sklearn.neural_network` and `gap_score` from `sklearn.metrics` do not exist.
*   **Hardcoding:** Assumes 5 clusters in several places, which is not adaptive.

In summary, while the generated code attempts to include many of the requested components, its implementation is largely incorrect, incomplete, and filled with errors or statistical misunderstandings, making it dysfunctional for the complex task. It lacks the robustness, accuracy, and advanced techniques demonstrated in the ground truth.",llama-3.3-8b-instruct,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import statsmodels.tsa.statespace.sarimax
import pmdarima
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Load data and parse 'date'
data = pd.read_csv('synthetic_covid19_data.csv')
data['date'] = pd.to_datetime(data['date'])
data = data[data['location'] == 'Germany']
data.set_index('date', inplace=True)

# Set target and exogenous variables
y = data['new_cases']
exog = [data['stringency_index'], data['people_fully_vaccinated']]

# ...",4,"The generated code attempts to address the task but contains several critical errors and omissions that prevent it from functioning correctly or fully meeting the requirements.

**Correctness Issues:**
1.  **Exogenous Variable Handling:** The most significant error is how exogenous variables (`exog`) are handled.
    *   `exog = [data['stringency_index'], data['people_fully_vaccinated']]` creates a list of Series, not a DataFrame, which is expected by `pmdarima.auto_arima` and `model_fit.predict`.
    *   Later, `exogenous=train[exog[0]]` and `exogenous=test[exog[0]]` attempt to slice `train` and `test` DataFrames using a pandas Series (`data['stringency_index']`), which will fail or produce incorrect results. `auto_arima` and `predict` expect a DataFrame for `exogenous` parameter, or at least a numpy array.
2.  **Missing Value Imputation:** `y.fillna(y.mean(), inplace=True)` and `exog[i].fillna(exog[i].mean(), inplace=True)` are problematic. While `y.mean()` might work for `new_cases`, `exog[i].mean()` is applied to a Series within the `exog` list, which only fills NaNs in *that specific Series across the entire dataset*, not within `train` or `test` sets independently. This also doesn't align with the requirement to handle missing values *before* splitting the data, or consistently. The ground truth's ffill/bfill/interpolate is a more robust approach for time series.
3.  **Data Splitting:** `train, test = data[0:train_size], data[train_size:]` splits the *entire DataFrame* `data`. The `y` and `exog` variables are created *before* the split, meaning `y` and `exog` are global variables not aligned with the `train` and `test` sub-DataFrames, leading to `auto_arima` using the full `y` and improperly handled `exog` (as explained above). The `auto_arima` function should be called with `y_train` and `X_train`, and `predict` with `X_test`.
4.  **`model_fit.predict` Arguments:** The arguments for `predict` are incorrect. `forecast, conf_int = model_fit.predict(...)` extracts `conf_int` incorrectly if `return_conf_int=True`. `predict` returns a tuple if `return_conf_int` is true, usually `(forecast_series, conf_interval_df)`. Also, the `exogenous` parameter is again incorrectly specified.

**Code Quality and Completeness Issues:**
1.  **Imports:** While necessary imports are present, `statsmodels.tsa.statespace.sarimax` is imported but never explicitly used (only pmdarima's SARIMAX is implicitly used). `numpy` is not imported, but often useful for RMSE calculation.
2.  **Missing Value Handling Completeness:** The requirement for ""e.g., forward fill or interpolation"" was not fully met by just filling with mean, and the handling method was applied incorrectly.
3.  **Detailed Output/Steps:** The ground truth includes more verbose print statements for progress and error handling (e.g., FileNotFoundError, insufficient data for test set), which is good practice.
4.  **Plotting:** The plotting is basic. It doesn't include the training data as context, which is useful for visualizing the overall time series. The confidence interval plotting is also not aligned with how `pmdarima` returns `conf_int`. `plt.fill_between(range(len(test)), conf_int[:, 0], conf_int[:, 1], ...)` assumes a numerical index for the test period, but `test` might have a datetime index. The `plt.plot(forecast)` also plots against a numerical index. This would lead to visual misalignment.

In summary, the core logic for SARIMA modeling (data splitting, exogenous variable handling, and model fitting/prediction) is fundamentally flawed in its implementation, leading to incorrect or non-functional code.",llama-3.3-8b-instruct,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
from scipy.stats import normaltest, kruskal
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from statsmodels.stats.multicomp import Multicompare

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Define quality groups
df['quality_group'] = pd.cut(df['quality'], bins=[0, 4, 5, 6, 8], labels=['low', 'medium', ...",4,"The generated code has significant issues in correctness, completeness, and code quality.

**Correctness:**
1.  **Normality Test Misapplication**: The `normaltest` (D'Agostino and Pearson's) is applied to *one* group (`group1`) to decide between ANOVA and Kruskal-Wallis. This is fundamentally incorrect. Normality needs to be assessed for all groups. Even if it were applied to all, testing normality on individual groups for ANOVA decision isn't the most robust approach, and the ground truth correctly uses Shapiro-Wilk for individual groups and then Levene's for homogeneity of variance.
2.  **ANOVA Implementation**: The `statsmodels.formula.api.ols` and `anova_lm` approach is correct for ANOVA but it's only chosen if `normaltest(group1).pvalue <= 0.05`, meaning if `group1` is *not* normal, which is the opposite of when you'd use ANOVA. This logic is inverted.
3.  **Kruskal-Wallis Implementation**: `kruskal` is only chosen if `normaltest(group1).pvalue > 0.05`, meaning if `group1` *is* normal. Again, this is inverted. It's supposed to be used when normality assumptions are violated.
4.  **Post-hoc Test Selection**: The selection for post-hoc tests also suffers from the same inverted normality logic. It attempts to use `Multicompare` (Dunn's test) if `group1` is normal (when one would use Tukey HSD) and Tukey HSD if `group1` is not normal (when one would use Dunn's).
5.  **Multicompare (Dunn's Test)**: The `Multicompare` class from `statsmodels.stats.multicomp` is not the standard way to perform Dunn's test directly. It's a general multi-comparison framework. The ground truth's approach using pairwise Mann-Whitney U with Bonferroni correction is a common and acceptable proxy for Dunn's. The generated code's use of `Multicompare(..., method='dunn')` is unconventional and likely incorrect without specific `statsmodels` setup for Dunn's, which is not clearly documented for direct `method='dunn'` use in this manner for post-hoc after Kruskal-Wallis.
6.  **Box Plot Logic**: The `if normaltest(...)` condition for plotting box plots doesn't make sense. Box plots are suitable regardless of normality and should be plotted for all significant features. Furthermore, it duplicates plotting code within the `if/else`.
7.  **Summary Logic**: The summary re-runs all statistical tests in a separate loop, which is inefficient and unnecessary. The results should have been stored from the first loop.
8.  **Quality Grouping**: `pd.cut(df['quality'], bins=[0, 4, 5, 6, 8], labels=['low', 'medium', 'high'])`. The bins [0, 4, 5, 6, 8] are not consistent with the task description: 4 is 'low', 5-6 is 'medium', >= 7 is 'high'. The bins should be `[0, 4.5, 6.5, 10]` to correctly categorize `score <= 4`, `5 <= score <= 6`, and `score >= 7`. For example, `pd.cut` with `right=True` (default) means `(4, 5]` so a quality of 5 would go into 'low' if the bin is `[0,4,5,6,8]`. This is highly problematic for categorization.

**Code Quality:**
1.  **Readability/Structure**: The code is repetitive, especially regarding the statistical testing and plotting loops. Storing results in dictionaries (as in the ground truth) would make it much cleaner and more efficient.
2.  **Variable Naming**: `H` and `p` are reused for different test statistics (F-statistic for ANOVA vs. H-statistic for Kruskal-Wallis) which can be confusing.
3.  **Error Handling**: No error handling (e.g., `FileNotFoundError` for the CSV, or cases where groups might be too small for tests).
4.  **Magic Numbers**: `0.05` (alpha) is hardcoded repeatedly instead of being defined as a constant.

**Completeness:**
1.  **Homogeneity of Variance**: The code completely ignores testing for homogeneity of variance (Levene's test), which is a critical assumption for ANOVA.
2.  **Post-hoc for Kruskal-Wallis**: The ground truth correctly identifies pairwise Mann-Whitney U with Bonferroni correction as the typical approach. The `Multicompare` usage is problematic and doesn't explicitly show corrected p-values or which pairs are significant.
3.  **Visualization Detail**: While box plots are created, they lack titles, axis labels, and consistency in aesthetics. Saving figures or arranging them in a grid is also often desired in such tasks.
4.  **Summary Detail**: The summary of significant features just lists them without providing the p-values from the primary tests or the detailed post-hoc results for each, which is less informative than the ground truth.
5.  **No `stats.shapiro`**: The generated code uses `normaltest`, but the task specifically mentions ""normality assumptions"" which implies the need for a formal test like Shapiro-Wilk (as used in the ground truth) which is common for smaller sample sizes.

Overall, the core logic for selecting statistical tests and their corresponding post-hoc analysis is flawed due to inverted conditions and potentially incorrect post-hoc implementation for non-parametric tests. The code also lacks robustness, clarity, and completeness in its output and error handling.",llama-3.3-8b-instruct,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, f1_score
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
import shap

# Load data
data = pd.read_csv('synthetic_covid19_data.csv')
data['date'] = pd.to_datetime(data['date'...",4,"The generated code attempts to address all parts of the task, but crucial errors and omissions in implementation lead to an incorrect and incomplete solution.

**Major Issues:**

1.  **Incorrect Target Variable Definition:** The most critical flaw is in the definition of `high_daily_death_increase`. The prompt clearly states: ""A 'high_daily_death_increase' is defined as a day where 'new_deaths' for a 'location' is in the top 10% of its own historical 'new_deaths' (excluding days with zero new deaths).""
    *   The generated code calculates a single `np.percentile` across _all_ `new_deaths` for a given `df` (which is the entire dataset) and assigns it to 'percentile' column for each row. This is fundamentally wrong as it doesn't group by `location` and doesn't consider only `new_deaths > 0` for percentile calculation.
    *   The `calculate_percentile` function calculates the 90th percentile across the *entire dataframe* for `new_deaths`, not for each `location` independently. Furthermore, it incorrectly assigns `0` if `new_deaths` is `0`, which doesn't reflect the requested percentile logic.
    *   The condition for `high_daily_death_increase` (`data['new_deaths'] > data['percentile']`) is also flawed because the `percentile` column itself is incorrect.

2.  **Missing `min_periods` for Rolling Features:** The `rolling(7).mean()` and `rolling(7).std()` functions by default will produce `NaN` values for the first 6 periods. The prompt asked to handle missing values from feature engineering. While `fillna(0)` is used later, it's not ideal for rolling features that should ideally start calculating from `min_periods=1` or `min_periods` set appropriately to avoid excessive NaNs at the beginning of each location's time series. The ground truth correctly uses `min_periods=1`.

3.  **Incorrect Missing Value Handling for Feature Engineering:** `data.fillna(0, inplace=True)` is a blanket fillna. While it satisfies the prompt's loose ""fill with 0 or backfill"" for FE NaNs, it applies to *all* columns, which may not be appropriate for static demographic features (e.g., `median_age`, `gdp_per_capita`), where mean/median imputation might be more suitable. More importantly, it doesn't consider the ""backfill"" part of the prompt, and doesn't differentiate how NaNs from FE should be handled specifically (e.g., ffill then 0, as in ground truth).

4.  **Incorrect `people_fully_vaccinated` handling:** The prompt specifically requested `'people_fully_vaccinated' / 'population'` as a feature. The generated code uses `people_fully_vaccinated` directly without normalizing by population.

5.  **Incorrect SHAP Implementation:**
    *   `shap.Explorer` is not a standard SHAP explainer class for Tree models; it should be `shap.TreeExplainer`.
    *   The `shap_values` are calculated on `X_test`, but `explainer.expected_value` and `X_test` likely refer to the raw (untransformed) `X_test` data, which will not align with the model's expected input after `ColumnTransformer`. SHAP needs the features to be in the same format as they were fed into the model after preprocessing.
    *   The `explainer` is initialized with the `_pipeline`, but the `_pipeline` itself is not the `best_estimator_` obtained from `GridSearchCV`.

6.  **Suboptimal Data Splitting and Preprocessing Pipeline:**
    *   The code performs a `train_test_split` *before* defining the `ColumnTransformer` and fitting it. A time-based split was requested, but a standard random `train_test_split` is used, which is incorrect for time-series data.
    *   The ColumnTransformer is defined in a `_pipeline` within `GridSearchCV`, but the scaling and encoding are also performed *manually* (`scaler.fit_transform`, `encoder.fit_transform`) *before* the `GridSearchCV` loop. This leads to data leakage (scaling/encoding fitted on the entire dataset) and incorrect preprocessing within the pipeline. The manual `X` and `y` construction before `train_test_split` means the `ColumnTransformer` (if it were correctly implemented) would be applied to already preprocessed data, leading to errors.
    *   The `GridSearchCV` is given `X_train` and `y_train` that are already manually preprocessed, but then the `_pipeline` attempts to preprocess them again inside the `GridSearchCV`. This is a serious logic error.

7.  **Minor Issues / Omissions:**
    *   No proper imports for `matplotlib.pyplot` (used for SHAP plot).
    *   `use_label_encoder=False` and `eval_metric='logloss'` for XGBoost are good practices that were missed.
    *   No `classification_report` for evaluation.
    *   The `grid_search` parameters (`n_estimators`, `learning_rate`) are quite small/large for the `GridSearchCV` example, especially learning rate [0.1, 0.5, 1]. This might not yield optimal performance.
    *   No explicit handling for `np.inf` values that could arise from division by zero in `people_fully_vaccinated_ratio` (though this feature was not correctly implemented anyway).
    *   Lack of comments and print statements to track progress, making it harder to debug.

Overall, while the code attempts to follow the high-level steps, the core implementation details for crucial steps like target variable creation, feature engineering, preprocessing, and SHAP are significantly flawed. It would not produce a correct or reliable model.",llama-3.3-8b-instruct,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import mixed

# Load data
data = pd.read_csv('synthetic_covid19_data.csv')

# Parse 'date'
data['date'] = pd.to_datetime(data['date'])

# Calculate 'vaccination_percentage'
data['vaccination_percentage'] = data['people_fully_vaccinated'] / data['population']

# Handle missing values
data['reproduction_rate'] = data.groupby('location')['reproduction_rate'].ffill().bfill()
data['stringency_in...",3,"The generated code attempts to address the task but falls short in several critical areas, leading to incorrect or incomplete execution.

**Correctness (2/10):**
*   **Missing Value Handling:** The imputation strategy is deeply flawed. `ffill().bfill()` is applied across all locations for each column, which can incorrectly fill values from different locations. More importantly, it is applied *before* `vaccination_percentage` is calculated, meaning `people_fully_vaccinated` and `population` are not properly handled before division, leading to potential NaNs or incorrect values. The prompt specifically asked for ""interpolation or median imputation grouped by location,"" which `ffill().bfill()` doesn't fully represent for interpolation, especially when linear or median imputation might be more appropriate. It also doesn't account for remaining NaNs after this simple imputation.
*   **Vaccination Percentage Calculation:** It directly calculates `vaccination_percentage` without handling `population` values that might be zero or `NaN`, which would lead to `inf` or `NaN` values that aren't handled at all. It also doesn't cap it at 100%.
*   **Mixed Model Definition:** The `re_formula='~1|continent/location'` syntax for nested random effects is not directly supported by statsmodels `mixedlm` in the way intended for deeply nested random effects. `statsmodels.formula.api.mixedlm` (which seems to be implied by `mixed`) expects grouping as a single column. The `continent/location` syntax is typically for `lme4` in R, or `patsy` formula interactions, but not directly for defining nested random effects in `mixedlm`'s `groups` argument. The prompt asked for ""random intercepts for 'location' nested within 'continent',"" which requires creating a combined ID. This will likely lead to an error or an incorrect model specification.
*   **Results Access:** The `results.feather.from_formula` attribute is completely incorrect and appears to be hallucinated. This will cause an `AttributeError` and the code will crash trying to report results. Statsmodels results are typically accessed via `results.summary()`, `results.fe_params`, `results.pvalues`, `results.cov_re`, etc.
*   **Interpretation:** The attempt at interpretation literally prints the incorrect attribute access, so no actual interpretation is provided.

**Code Quality (3/10):**
*   **Imports:** `statsmodels.api` is imported, but `mixed` is typically from `statsmodels.formula.api`. The code uses `from statsmodels.formula.api import mixed`, which is good.
*   **Variable Names:** `data` is a fine variable name.
*   **Structure:** The structure roughly follows the steps outlined but the implementation details are poor.
*   **Error Handling:** No error handling for file loading or model fitting.

**Completeness (4/10):**
*   **Data Loading & Parsing:** Partially complete.
*   **Vaccination Percentage Calculation:** Partially complete, misses capping and robust handling of `population`.
*   **Missing Value Handling:** Incomplete and incorrect. Doesn't effectively handle all missing values as required by the prompt's suggestions (interpolation, median imputation, group-wise).
*   **Mixed Effects Model:** The formula for fixed effects is correct, but the random effects specification is incorrect for `statsmodels`.
*   **Reporting Fixed Effects:** Attempts to report, but uses non-existent attributes, failing completely. It doesn't report standard errors or p-values explicitly beyond the summary, which would be expected.
*   **Interpretation:** While a section is present, it fails due to incorrect result access.
*   **Reporting Variance Components:** `results.var_params` will likely not give the desired variance component for the random intercepts in the way `cov_re` does.
*   **Model Assumptions Discussion:** This section is very brief and generic, not fulfilling the ""briefly describe how they could be checked (plotting residuals, Q-Q plots, etc. - no need to implement checks)"" part as comprehensively as the ground truth.

Overall, the generated code demonstrates a superficial understanding of the requirements but fails significantly in the implementation details, particularly around missing value handling, model specification for mixed effects in `statsmodels`, and correctly accessing model results. It would not execute successfully to produce the required output.",llama-3.3-8b-instruct,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select features for clustering
features = df[['elixirCost', 'Win Rate', 'Usage']]

# Standardize features
scaler = StandardScaler()
standardized_features = scaler.fit_transform(features)

# Determine appropriate number of clusters (K)
wcss = []
for k...",7,"The generated code correctly addresses most of the requirements but has several areas for improvement to achieve a higher score.

**Correctness:**
*   **Feature Selection:** `features = df[['elixirCost', 'Win Rate', 'Usage']]` is correct for selecting the specified columns.
*   **Standardization:** `StandardScaler` is used correctly.
*   **Elbow Method:** The WCSS calculation and plotting are correctly implemented.
*   **K-Means Application:** K-Means is applied and `fit_predict` is used correctly.
*   **PCA:** PCA is applied correctly to reduce dimensionality.
*   **Visualization:** The scatter plot uses the correct components and colors by cluster labels.

**Code Quality & Completeness (Areas for Improvement):**

1.  **Missing `n_init` in KMeans:** In newer versions of scikit-learn (1.1+), not specifying `n_init` (or setting it to 'auto') for `KMeans` raises a warning. The ground truth explicitly sets `n_init=10` which is good practice for reproducibility and to suppress the warning. The generated code implicitly uses the default which might lead to warnings or less stable results.
2.  **Explicit K Choice:** The ground truth code visually presents the elbow plot and then explicitly chooses a `k` (e.g., `chosen_k = 3`) with a comment explaining the rationale, which is better than hardcoding `k = 5` without context for the user viewing the plot. While the task says ""Determine an appropriate number of clusters (K)... You can use the elbow method (plotting WCSS),"" the ground truth adds more context about `chosen_k` after plotting.
3.  **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and missing columns, which is crucial for real-world scenarios. The generated code lacks this.
4.  **Noisy Plot Output:** The generated code produces two `plt.show()` calls back-to-back without closing the first plot or combining them, which can be less user-friendly. Although the task only asks for one plot of the final clusters, showing the elbow plot and then the final clusters typically requires careful handling of `plt.show()` or `plt.close()`. The ground truth uses `plt.show()` for the elbow plot, then `plt.show()` for the scatter plot, which is acceptable but could be improved.
5.  **Readability/Comments:** While not explicitly requested, the ground truth has more descriptive print statements and comments explaining each step, making the code much easier to understand and debug. For example, it prints dataset shape, head, feature selection confirmation, etc., which is very helpful for validation.
6.  **Minor Plot Improvements:** The ground truth adds more detailed axis labels to the PCA plot (including explained variance), a color bar, and a more descriptive title, which enhances the interpretability of the visualization. It also uses `pca_df` which is a more structured way to handle the PCA results than direct array indexing `pca_features[:, 0]`.

Overall, the generated code produces the desired output plots and computations. However, it's less robust, less user-friendly, and less aligned with best practices for production-ready data science code compared to the ground truth.",llama-3.3-8b-instruct,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select features and target
X = df[['elixirCost', 'Usage', 'rarity']]
y = df['Win Rate']

# Preprocess the 'rarity' column using one-hot encoding
...",7,"The generated code correctly addresses most of the requirements. It loads the data, selects the correct features and target, performs one-hot encoding, splits the data, trains a RandomForestRegressor model, and evaluates it using R2 and MSE.

However, there are a few areas for improvement:
1.  **OneHotEncoding with `pd.concat`**: The `OneHotEncoder` usage and subsequent `pd.concat` is a bit clumsy. Specifically, `pd.DataFrame(X_rarity.toarray())` by default creates columns with integer names (0, 1, 2, ...), which might overlap with existing numeric column names if any, and makes the resulting DataFrame less readable. It also doesn't handle the `drop_first=True` argument directly for multicollinearity as easily as `pd.get_dummies`. While it works, `pd.get_dummies` is generally preferred for its simplicity and direct integration with DataFrames when dealing with categorical columns in Pandas.
2.  **Missing `random_state` for `RandomForestRegressor`**: The `RandomForestRegressor` lacks `random_state=42`. While the `train_test_split` has it, not setting it for the model means its training process is not reproducible across runs, which is a good practice for benchmark scenarios.
3.  **No handling of non-numeric data/missing values**: The prompt implicitly assumes clean data or that the user handles it. The ground truth code includes basic error handling for `FileNotFoundError` and checks for missing columns, as well as a warning for dropping rows with missing values which is more robust for a general solution. The generated code assumes the CSV is perfectly clean and has all columns.
4.  **Clarity of Output**: The printed output for R2 score and MSE is less descriptive than the ground truth, which includes headers and formatting. While minor, it impacts user experience for interpreting the results.

The commented-out Linear Regression part is good as it shows consideration for alternative models, aligning with the prompt's `(e.g., Linear Regression or RandomForestRegressor)` flexibility.",llama-3.3-8b-instruct,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy.stats import f_oneway

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load dataset: {e}"")

# Perform ANOVA test
def perform_anova_test(data):
    # Identify unique categories in the 'rarity' column
    rarities = data['rarity'].unique()
    
    # State the null and alternative hypotheses
    print(""Null Hypothesis: The mean Win Rate is...",6,"The generated code correctly implements the core ANOVA test and interpretation, and largely follows the prompt's requirements. However, it lacks robust error handling and data preprocessing steps that are crucial for real-world data analysis, such as handling `FileNotFoundError`, non-numeric 'Win Rate' values, or `NaN` values in 'Win Rate' or 'rarity'. The code also doesn't explicitly handle the case where a rarity group might have too few observations (less than 2) for ANOVA, which could lead to issues. The prompt asked for a direct script without functions and a more specific structure for hypothesis statements. Encapsulating the logic within functions is good practice but deviates from the direct script request. The specific hypotheses stated are slightly less precise than the ground truth's.",llama-3.3-8b-instruct,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
df = pd.read_csv('iris.csv')

# Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
...",6,"The generated code correctly addresses most of the task requirements, including loading data, defining features and target, encoding the target, splitting data, and training the specified models. It also calculates and prints accuracy, precision, recall, and F1-score.

However, there are a few critical omissions and minor discrepancies that prevent a higher score:
1.  **SVC kernel:** The generated code uses `SVC(random_state=42)` which defaults to an RBF kernel. The task explicitly asked for ""SVC with a linear kernel"". This is a significant deviation in model specification.
2.  **Logistic Regression `max_iter`**: While the ground truth code used `max_iter=200` for Logistic Regression (to prevent potential convergence warnings, although not explicitly asked for), the generated code uses default parameters, which might sometimes lead to convergence warnings in more complex datasets (though less likely with Iris). This is a minor point, but worth noting for robustness.
3.  **Best Model Selection Logic:** The most significant flaw is how the ""best model"" is identified.
    *   The task explicitly states: ""Print which model performed best based on the highest F1-score (macro average).""
    *   The generated code uses `max(classifiers, key=lambda x: classifiers[x].score(X_test, y_test))`. `classifier.score()` by default returns the *accuracy* for classification models. Therefore, it identifies the best model based on accuracy, not F1-score, which directly violates the requirement.
    *   Also, the code doesn't store the F1 scores explicitly for comparison, making the ""best model"" logic convoluted and incorrect as implemented for the task's criteria.

Points to improve:
*   Ensure SVC uses `kernel='linear'`.
*   Store the calculated F1-scores for each model in a dictionary or similar structure.
*   Modify the best model selection logic to find the model with the maximum F1-score (macro average) from the stored values.",llama-3.3-8b-instruct,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('iris.csv')

# Select the features
features = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Standardize these features
scaler = StandardScaler()
standardized_features = scaler.fit_transform(features)

# A...",8,"The generated code correctly performs all the core steps required by the task: loading data, selecting and standardizing features, applying K-Means, performing PCA, and generating two scatter plots.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly selects the specified features.
*   Correctly uses `StandardScaler`.
*   Applies `KMeans` with the specified `n_clusters`, `n_init='auto'`, and `random_state`.
*   Applies `PCA` to 2 components.
*   Generates two scatter plots with correct data (`X_pca`, `kmeans_labels`, `true_species_encoded`).
*   Adds titles to both plots.
*   Includes all necessary imports.

**Areas for improvement (prevented a perfect score):**

*   **Plot Enhancements (Minor):** The generated code's plots are functional but lack colorbars and individual point labels (as in the ground truth) which enhance interpretability, especially for the true species plot where distinct species names would be beneficial.
*   **Feature Selection Method:** While `features = df[['SepalLengthCm', ...]]` works correctly for selection, `features = ['SepalLengthCm', ...]` and then `X = df[features]` is often preferred as it cleanly separates the list of feature names from the DataFrame indexing. However, this is a stylistic choice and not a functional error.
*   **Error Handling (Minor):** The ground truth includes a `try-except` block for `FileNotFoundError` which is good practice for robust code. The generated code does not include it, which is not a P0 issue for this specific task but a general good-to-have.",llama-3.3-8b-instruct,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# Load the dataset
df = pd.read_csv('iris.csv')

# Create two new features
df['PetalArea'] = df['PetalLengthCm'] * df['PetalWidthCm']
df['SepalArea'] = df['SepalLengthCm'] * df['SepalWidthCm']

# Define features X
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'PetalArea', 'SepalArea']]...",9,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, creates the new features 'PetalArea' and 'SepalArea' correctly, and defines `X` to include both original and new features as specified. The target `y` is also correctly defined and encoded using `LabelEncoder`. The Gradient Boosting Classifier is initialized with the correct parameters, and 5-fold cross-validation is performed. The mean cross-validated accuracy score is reported.

Minor points for improvement:
1.  **Error handling for file loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading `iris.csv`, which is good practice. The generated code assumes the file will always be present. While not strictly requested to include error handling, it significantly improves robustness.
2.  **Output format:** The ground truth prints the f-string with higher precision for the mean accuracy and also shows the individual fold scores, which provides more context. The generated code prints a simpler message. This is a minor aesthetic difference rather than a functional one.

Overall, the core logic is perfectly sound and meets the functional requirements.",llama-3.3-8b-instruct,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load the dataset
df = pd.read_csv('heart.csv')

# Define features and target
X = df[['Age', 'Sex', 'ChestPainType',...",7,"The generated code largely fulfills the requirements of the task.

**Strong points:**
*   Correctly loads the dataset and defines features and target.
*   Correctly identifies and applies one-hot encoding to specified categorical features and scaling to numerical features.
*   Uses `ColumnTransformer` for preprocessing.
*   Splits data into training and testing sets with the correct proportion and `random_state`.
*   Trains a `RandomForestClassifier` with specified parameters.
*   Evaluates the model using all requested metrics (accuracy, precision, recall, F1-score, ROC-AUC) with macro averaging where appropriate.
*   Includes all necessary imports.

**Areas for Improvement:**
*   **Data Leakage in Preprocessing:** The primary issue is that the `preprocessor.fit_transform(X)` is applied to the *entire* dataset `X` *before* the train-test split (`X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)`). This leads to data leakage because the scaling and one-hot encoding transformations learn parameters (e.g., mean, standard deviation for `StandardScaler`, and categories for `OneHotEncoder`) from the entire dataset, including the test set. This can lead to an artificially inflated performance measurement. The ground truth correctly addresses this by using a `Pipeline` where `fit_transform` is only called on `X_train` and `transform` on `X_test` internally by the pipeline after the split.
*   **Missing Error Handling:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth and is good practice.
*   **`handle_unknown` in `OneHotEncoder`:** The `OneHotEncoder` by default raises an error if an unknown category is encountered during transformation. While less critical for this dataset, setting `handle_unknown='ignore'` (as in the ground truth) is more robust for real-world scenarios where test data might contain categories not seen in training.
*   **Stratification in `train_test_split`**: The `train_test_split` call does not include `stratify=y`, which is important for binary classification tasks (like HeartDisease) to ensure that both training and testing sets have a similar proportion of the target classes. This helps to prevent biased evaluation, especially with imbalanced datasets.
*   **Clarity of numerical features definition**: While `numerical_features` was correctly identified `['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']`, it's not dynamically derived like in the ground truth (`[col for col in features if col not in categorical_features]`). This isn't a problem for this specific task, but the dynamic approach is more robust to changes in feature lists.",llama-3.3-8b-instruct,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('heart.csv')

# Select numerical features
numerical_features = df[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']]

# Standardize the selected features
scaler = StandardScaler()
standardized_features = scaler.fit_transform(num...",9,"The generated code correctly performs all the required steps, including loading data, selecting and standardizing features, using both elbow and silhouette methods to help choose K, applying K-Means, performing PCA, and visualizing the clusters.

**Strong points:**
*   Correctly imports necessary libraries.
*   Accurately loads the dataset and selects the specified numerical features.
*   Properly standardizes the features using `StandardScaler`.
*   Effectively implements both WCSS (Elbow Method) and Silhouette Score calculations for K from 2 to 5.
*   Correctly plots both WCSS and Silhouette Scores, which is crucial for determining K.
*   Applies K-Means with `n_init='auto'` and `random_state=42`, as specified.
*   Successfully performs PCA to 2 components.
*   Generates a scatter plot of the principal components, colored by cluster labels, with an appropriate title and axis labels.

**Areas for minor improvement:**
*   **Choice of K:** The generated code automatically selects `best_k` based *solely* on the maximum silhouette score. While this is a valid approach, the task description specifically asked ""Use the elbow method (plotting Within-Cluster Sum of Squares - WCSS) and silhouette scores to justify your choice of K."" The ground truth code emphasizes that this choice often involves visual inspection / expert judgment from both plots rather than a purely programmatic selection, especially given the ""elbow"" can be subjective. The ground truth also explicitly stated the chosen K. The generated code's automatic selection misses the nuance of interpreting *both* plots for justification. If the prompt strictly meant ""programmatically select K"", then the generated code's approach is fine, but the wording ""justify your choice"" implies a more holistic consideration.
*   **Plotting details:** The generated plots for WCSS and Silhouette scores lack labels for K on the x-axis (`plt.xticks`) and have a somewhat simplistic line graph without markers (`marker='o', linestyle='-'`) which makes them less visually informative than the ground truth. The PCA plot could also benefit from separating points by cluster to enhance visual distinction (e.g., using different markers or more explicit colors as in the ground truth).
*   **Robustness:** The ground truth includes a `try-except` block for file loading, which is good practice for robustness. The generated code assumes the file exists.
*   **Clarity:** The ground truth code includes print statements to indicate progress and confirm data shapes, which aids in understanding the execution flow and debugging. This is not strictly required by the prompt but improves clarity.

Overall, the code is highly functional and fulfills almost all requirements. The main deficiency lies in the specific interpretation and implementation of the ""justify your choice of K"" requirement, which the ground truth handles more comprehensively by presenting both visuals and implying expert interpretation rather than a purely algorithmic choice based on one metric.",llama-3.3-8b-instruct,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.feature_selection import SelectFromModel
import numpy as np

# Load dataset
df = pd.read_csv('winequality-red.csv')

# Create binary target variable
df['target'] = df['quality'].apply(lambda x: 1 if x >= 7 else 0)

# Preprocess...",7,"The generated code correctly addresses most of the task requirements, including binary target creation, basic preprocessing, data splitting, model training, and evaluation. However, there are significant issues with how feature importance is handled.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly creates the binary target variable based on the specified condition.
*   Handles missing values (though `dropna` might not always be the best strategy, it's a valid attempt).
*   Correctly splits data into train/test sets.
*   Scales features appropriately using `StandardScaler`.
*   Trains a Logistic Regression model.
*   Evaluates performance using accuracy, F1-score, and classification report.

**Areas for Improvement:**
*   **Feature Importance (Major Issue):** The prompt specifically asks for ""most important features according to the model coefficients."" The generated code incorrectly attempts to use `SelectFromModel` with a `threshold=0.01` on the coefficients. `SelectFromModel` is typically used for feature selection by thresholding coefficients *or* feature importance scores. For a Logistic Regression model, the coefficients themselves are the direct indicators of importance. A threshold of `0.01` without explicitly stating what this threshold applies to (e.g., absolute coefficients) is arbitrary and does not directly report ""the most important features according to the model coefficients"" in a clear, ranked manner as requested. The ground truth correctly extracts coefficients, calculates their absolute values, and sorts them to show importance.
*   **Missing `numpy` import for `threshold`:** While `numpy` is imported, it's not explicitly used in the `SelectFromModel` threshold, which is typically a float. This isn't a critical error but worth noting.
*   **Print statements:** While not explicitly requested, the ground truth includes informative print statements for better user experience and debugging, especially for a benchmark. The generated code lacks these.
*   **Missing `np` alias for `numpy`** The ground truth uses `np` for `numpy`, which is a standard convention. The generated code imports `numpy` but doesn't use `np` explicitly in this snippet. (Though it's not used at all in the provided solution for actual calculations).
*   **Best practice for handling missing values:** While `dropna()` removes rows with any missing values, it can lead to significant data loss if many rows have few missing values. The ground truth demonstrates a more robust approach (imputation with median, though for this specific dataset, there are typically no missing values, it's good practice).

To improve, the feature importance section needs to be completely re-written to extract and sort coefficients, similar to the ground truth.",llama-3.3-8b-instruct,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

# Load the dataset
df = pd.read_csv('heart.csv')

# Create a box plot showing the distribution of `Oldpeak` for patients with `HeartDisease`=0 and `HeartDisease`=1
plt.figure(figsize=(8,6))
sns.boxplot(x='HeartDisease', y='Oldpeak', data=df)
plt.title('Distribution of Oldpeak by HeartDisease')
plt.show()

# Perform a logistic regression with ...",7,"The generated code addresses most of the requirements but lacks some crucial details and robustness present in the ground truth.

**Correctness:**
*   The box plot function call is correct for the requested visualization.
*   Logistic regression is performed correctly.
*   `pd.qcut` is used correctly for binning.
*   Proportion calculation for `HeartDisease`=1 within bins is correct.

**Code Quality:**
*   Imports are correct and placed at the top.
*   Variable names are clear.

**Completeness:**
*   **Missing Interpretation Clarity (Crucial):** While the generated code calculates the `Oldpeak` coefficient and the odds ratio, it only prints them. It does **not** provide the *interpretation* of the odds ratio in plain language as explicitly requested by ""interpret its meaning in the context of odds ratios."" The ground truth provides detailed conditional interpretation. This is a significant omission for a ""reporting"" task.
*   **Missing Boxplot Details:** The box plot created by the generated code is functional, but it lacks specific requirements like `xlabel`, `ylabel`, and custom `xticks` labels that were present in the ground truth and improve readability.
*   **Missing Error Handling:** It lacks the `try-except` block for `FileNotFoundError` when loading the CSV, which makes it less robust.
*   **Missing Print Statements for Clarity:** The ground truth includes more verbose print statements (`Dataset loaded successfully`, `Box plot displayed`, etc.) which improve the user experience by indicating progress and completion of steps.
*   **Missing Data Validation:** The ground truth includes checks for `NaN` values in relevant columns and handles them by dropping rows, making it more robust to real-world data issues. This is absent in the generated code.
*   **Missing Fallback for `pd.qcut`:** The ground truth includes a `try-except` block for `pd.qcut` to handle potential `ValueError` if there are too many identical values in `Oldpeak` preventing unique quantiles, providing a fallback to fixed-width binning. This robustness is missing.
*   **Missing Visualization for Bin Proportions:** The ground truth includes an optional bar plot to visualize the proportions of Heart Disease by Oldpeak bin, which enhances understanding. While ""display"" could be interpreted as just printing, a visualization is a strong addition.

Overall, the core logic is present, but the generated code is less complete, less robust, and lacks the detailed interpretation required by the prompt compared to the ground truth.",llama-3.3-8b-instruct,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

# Load data
df = pd.read_csv('spotify_2023.csv')

# Convert `streams` to numeric and drop rows with errors
df['streams'] = pd.to_numeric(df['st...",9,"The generated code correctly addresses almost all aspects of the task. It loads the data, correctly handles the `streams` column conversion and NaN values, creates the `is_popular` target variable using the 75th percentile, selects the specified features, performs one-hot encoding for `key` and `mode` (using `ColumnTransformer` and `Pipeline`, which is a good practice), splits the data, trains a Logistic Regression model, and evaluates it using F1-score (macro), ROC AUC score, and a confusion matrix.

The primary reason for not giving a perfect 10 is that when `ColumnTransformer` is used, numerical features not covered by a transformer are dropped by default unless explicitly specified with `remainder='passthrough'`. While the `features` list was modified to `features + categorical_features` to include the raw numerical features in `X`, the `preprocessor` only specifies transformations for `categorical_features`. This means the `ColumnTransformer` (by default) will drop the `danceability_%`, `energy_%`, etc., columns, leading to an incorrect set of features being passed to the Logistic Regression model.

To fix this, `remainder='passthrough'` should be added to the `ColumnTransformer` definition:
```python
preprocessor = ColumnTransformer(
    transformers=[('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_features)],
    remainder='passthrough' # This is the missing part
)
```
Also, `handle_unknown='ignore'` is good practice for `OneHotEncoder` to prevent errors during prediction if a new category appears that was not present in the training data.

Minor points:
- The ground truth code includes helpful print statements for dataset loading, dropped rows, percentile value, and class distribution, which are good for debugging and understanding the data processing steps. The generated code lacks these.
- The ground truth also explicitly sets `random_state=42` for `LogisticRegression` for full reproducibility, which is good practice. The generated code does not.",llama-3.3-8b-instruct,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('spotify_2023.csv')

# Select audio features
selected_features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%', 
                     'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm']
df_selected = df[selected_features]

# Drop rows with any NaNs in selected fe...",8,"The generated code correctly performs all the main steps required by the task: data loading, feature selection and NaN handling, standardization, K-Means clustering, PCA, visualization, and calculation of cluster means.

**Strong Points:**
- Correctly identifies and uses all specified audio features.
- Correctly handles NaNs by dropping rows.
- Applies StandardScaler, KMeans (with correct parameters), and PCA (with correct parameters).
- Generates a scatter plot of PCA components colored by cluster labels.
- Calculates and displays mean values of original features per cluster.
- All necessary imports are present.

**Areas for Improvement (to achieve a higher score):**
- **Robustness in Data Loading:** The generated code lacks a `try-except` block for `pd.read_csv`, which is good practice to handle `FileNotFoundError` if the CSV isn't present.
- **Handling of `df_selected` and `df_std` for Mean Calculation:** The method for calculating cluster means (`df_selected.iloc[dfclusters == cluster]`) is functional but relies on the implicit assumption that the indexing of `df_selected` (after `dropna`) and `df_std` (which is a NumPy array) aligns with `dfclusters`. While it works in this specific case because `dropna` preserves the order, it's less robust. The ground truth's approach of explicitly creating `df_original_for_analysis` and assigning `cluster` column to it makes the process of getting original data back for mean calculation more explicit and robust, especially if intermediate steps were to reorder data.
- **Plotting Details:** The plot lacks axis labels and a legend/color bar, which makes it less informative compared to the ground truth. The title is also quite generic.
- **Intermediate Debugging/Information:** The ground truth includes print statements about row counts before and after NaN dropping, which is helpful for debugging and understanding data loss, though not strictly required by the prompt's output.

Overall, it's a very good attempt that fulfills the core requirements. The improvements suggested are mainly about robustness, clarity, and enhanced visualization.",llama-3.3-8b-instruct,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('spotify_2023.csv')

# Convert `streams` to numeric (coerce errors to NaN)
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# Filter for songs with `streams` greater than 10,000,000 and where `released_year` is between 2010 and 2023, inclusive
df_filtered = df[(df['streams'] > 10000000) & (df['released_year'] >= 2010) & (df['released_year'] <= 2023)]

# Drop rows with NaN `streams` after con...",8,"The generated code correctly addresses the core requirements of the task. It loads the data, correctly converts the `streams` column to numeric, filters the data based on `streams` and `released_year` (though the order of filtering and dropping NaNs is slightly off but still produces the correct result for the given conditions), groups by `released_year`, calculates the required average features, and generates line plots for each feature with appropriate titles and labels.

**Strong Points:**
*   Correct use of `pd.to_numeric` with `errors='coerce'`.
*   Accurate filtering criteria applied for `streams` and `released_year`.
*   Correct grouping and aggregation for `mean` of the specified features.
*   Proper use of `matplotlib.pyplot` for creating subplots, assigning titles, and labeling axes.
*   `plt.tight_layout()` is used for better plot presentation.

**Areas for Improvement:**
*   **Order of Operations (Minor):** The ground truth drops NaNs from `streams` *before* filtering for `streams` > 10,000,000. The generated code filters first and then drops NaNs. While for this specific condition (`streams > 10M`), dropping NaNs beforehand is more robust as it removes non-numeric entries that can't be compared. In the generated code, if a `streams` value was 'abc', it would become NaN, then the filter `df['streams'] > 10000000` would simply evaluate to `False` for that row, effectively achieving the same outcome as dropping NaNs before filtering if the intent is to only consider valid numbers. However, it's generally cleaner and safer to handle non-numeric values (by dropping corresponding rows) right after conversion if those non-numeric values are not supposed to participate in numerical operations or comparisons.
*   **Robustness:** The generated code does not include error handling for `FileNotFoundError` when loading the CSV, which is a good practice as shown in the ground truth.
*   **Clarity in Filtering Step:** Combining all filtering conditions into one line, while concise, can sometimes make it harder to debug or understand the exact sequence of filtering steps compared to breaking it down. The ground truth explicitly states ""Drop rows with NaN `streams` after conversion"" and then handles that before the numerical filtering. The generated code places `dropna` after the main filtering line. This is a minor point related to code clarity and best practice for handling data cleaning stages.
*   **Plot Enhancements (Minor):** The ground truth adds `marker='o'` to plot points, `grid(True)`, and `xticks` rotation for better readability, which are nice-to-have additions for presentation. The generated code's plots are functional but could benefit from these minor aesthetic improvements.",llama-3.3-8b-instruct,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Remove commas, convert to numeric, and fill resulting NaNs with 0 for Likes, Dislikes, and Video views
df['Likes'] = df['Likes'].str.replace(',', '').astype(float).fillna(0)
df['Dislikes'] = df['D...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being fully correct and robust.

**Issues:**

1.  **`published` column handling:** The prompt asked to treat the `published` column (year) as a numeric feature. The generated code simply does `pd.to_numeric(df['published'])`. The `youtube_trendings.csv` dataset's `published` column is a datetime string (e.g., ""2024-01-01""). Directly converting it to numeric will result in `NaN` values or errors and is not extracting the year. The ground truth correctly parses it as datetime and then extracts the year.
2.  **OneHotEncoder usage:**
    *   `sparse=False` in `OneHotEncoder` is deprecated in recent scikit-learn versions (it's now `sparse_output=False`). While not a functional error for now, it's good practice to use the current API.
    *   The way `df = pd.concat([df, encoded_category_df], axis=1)` is used can lead to issues if the index of `df` is not reset after some operations (though in this specific case it might align). A safer approach involves `pd.get_dummies` directly or ensuring index alignment.
    *   More importantly, `encoder.get_feature_names_out()` should be called *after* `fit_transform` or `fit` on the *original* `Category` column that might contain NaNs that are replaced by 'Unknown'. The line `X = df[['Dislikes', 'Video views', 'published'] + encoder.get_feature_names_out()]` expects `encoder.get_feature_names_out()` to provide column names from the *final, encoded* dataframe, but the way it's used with `df` (which still contains the original non-encoded category column) is confusing. It's better to explicitly build the `X` dataframe from the transformed data. The `pd.get_dummies` approach in the ground truth is often simpler for this.
3.  **Missing Error Handling/Robustness:** The generated code lacks error handling for `FileNotFoundError` for the CSV, and does not provide warnings if expected columns are missing, unlike the ground truth.
4.  **Implicit Type Conversion for Numeric Columns:** The `astype(float)` for Likes, Dislikes, and Video views is correct for handling NaNs but `astype(int)` would be more appropriate at the end given they represent counts, which the ground truth does.
5.  **Output Format:** The prompt requested specific output format for R2 and MAE (e.g., precision), which the generated code doesn't adhere to.

To improve, the code needs to correctly parse the `published` date to extract the year, use `OneHotEncoder` or `pd.get_dummies` more robustly, and improve general robustness and output formatting.",llama-3.3-8b-instruct,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# For the `Video views` column: remove commas, convert to numeric, and fill resulting NaNs with 0
df['Video views'] = df['Video views'].str.replace(',', '').astype(float).fillna(0)

# Ensure the `published` column (year) is treated as a numeric or integer type
df['published'] = pd.to_numeric(df['published'])

# Group the data by the `published` year
grouped_df = df.groupby...",6,"The generated code correctly addresses most of the requirements but has a critical flaw related to the `published` column and a few minor issues.

**Strengths:**
- Loads the dataset correctly.
- Cleans the `Video views` column efficiently by removing commas, converting to float, and filling NaNs with 0 in a single line.
- Groups the data by year and calculates the mean of `Video views` and unique count of `Video` titles.
- Creates two subplots and plots the required trends with appropriate titles and labels.
- Uses `plt.tight_layout()` for better layout.

**Areas for Improvement / Critical Issues:**
- **Critical Issue: Handling of `published` column (Year Extraction):** The prompt explicitly states ""Ensure the `published` column (year) is treated as a numeric or integer type."" In the `youtube_trendings.csv` dataset, the year is typically part of a full date-time string in a column like `publishedAt`. The generated code assumes there is a column named `published` which directly contains just the year and tries to convert it to numeric using `pd.to_numeric(df['published'])`. This is almost certainly incorrect given the typical structure of such datasets (where a date/time string needs parsing to extract the year). This will likely lead to a `KeyError` if `published` column doesn't exist or a `ValueError` if it exists but contains non-numeric date strings. **This is the biggest drawback.**
- **Clarity in Grouping and Aggregation:** While the aggregation logic works, splitting `metrics_df = grouped_df['Video views'].mean().reset_index()` and then `metrics_df['unique_videos'] = grouped_df['Video'].nunique().reset_index()['Video']` is less concise and potentially less robust than using an `agg` function directly on the groupby object.
- **Plotting Aesthetics:** While functional, the plots could benefit from markers, differing linestyles, and potentially setting specific x-axis ticks to be years for better readability, similar to the ground truth.
- **Error Handling:** The code lacks any error handling for file not found or issues during data processing, which is generally good practice for robust scripts.
- **Function Encapsulation:** The ground truth wraps the logic in a function, which makes the code reusable and more organized. The generated code is a direct script. While not strictly disallowed by the prompt, it's a common good practice for data analysis tasks.",llama-3.3-8b-instruct,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
import nltk
from nltk.corpus import stopwords

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Create a new feature 'title_word_count'
df['title_word_count'] = df['Video'].apply(lambda x: len(x.split()))

# Clean 'Video views' column
df['Video views'] = df['Video views'].apply(lambda x: x.replace(',', ''))
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce')
df['Video views'] = df['Video view...",7,"The generated code correctly addresses most of the tasks.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly calculates `title_word_count`.
*   Effectively cleans `Video views` by removing commas, converting to numeric, and filling NaNs.
*   Calculates the Pearson correlation coefficient correctly (though uses `scipy.stats.pearsonr` instead of `df.corr()`, which is a valid alternative).
*   Correctly downloads NLTK stopwords.
*   Processes text (lowercasing, tokenization, stop word removal) and finds the 5 most common words.

**Areas for Improvement / Issues:**
*   **Minor Issue:** The initial `df['Video views'] = df['Video views'].apply(lambda x: x.replace(',', ''))` line might fail if the `Video views` column contains non-string types, leading to an `AttributeError`. The ground truth converts to string first (`.astype(str)`) which is safer.
*   **Minor Issue:** The filtering for `published` year `df['published'].dt.year` assumes the `published` column is already in datetime format. It's best practice and safer to explicitly convert it to datetime using `pd.to_datetime` before attempting to access `.dt.year`, as done in the ground truth. This is a common source of errors if the column is read as a string.
*   **Incomplete Text Cleaning:** The `re.findall(r'\b\w+\b', title.lower())` extracts words, but it doesn't explicitly remove punctuation. For example, ""title!"" would become ""title"" but ""word's"" would become ""word's"". The ground truth uses `re.sub(r'[^\w\s]', '', title)` which completely removes non-alphanumeric characters, ensuring cleaner word parsing.
*   **Missing Error Handling:** No error handling for `FileNotFoundError` when loading the CSV, which is good practice.
*   **Missing Clarity in Output:** The print statements could be more descriptive (e.g., adding text before the correlation value, formatting the list of common words more clearly).

Overall, it's a good attempt that covers most requirements, but could be made more robust and complete with minor adjustments.",llama-3.3-8b-instruct,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
from sklearnlovermia.encoder import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score

# Load data
df = pd.read_csv('mobiles_2025.csv')

# Clean Launched Price (USA)
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', '').str.replace('$', '').astype(float)
df['is_expensive_usa'] = np....",6,"The generated code attempts to address all requirements but falls short in several key areas of correctness and robustness, particularly concerning data cleaning and handling `NaN` values.

**Areas for Improvement (leading to a score of 6):**

1.  **Launched Price (USA) Cleaning and Target Creation:**
    *   **Incorrect `str.replace`:** `df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', '').str.replace('$', '').astype(float)`
        *   The original price string `USD 1,234` does not contain `$` so `str.replace('$', '')` is unnecessary.
        *   More importantly, the ground truth correctly handles `astype(str)` first, then regex replacement, then `pd.to_numeric(errors='coerce')` to gracefully handle unparseable values into `NaN`. The generated code directly uses `astype(float)` which will raise an error if any value cannot be converted, rather than handling them as `NaN`.
    *   **Target Calculation:** `np.where(df['Launched Price (USA)'] > 1000, 1, 0)`
        *   This line computes `is_expensive_usa` based on the *cleaned* `Launched Price (USA)` after the `astype(float)` conversion. If `astype(float)` fails, the code will crash before reaching this step.
        *   The ground truth explicitly creates `NaN` for `is_expensive_usa` if the source price was `NaN` (due to `errors='coerce'`), and then drops rows where `is_expensive_usa` is `NaN`. The generated code's current approach makes `is_expensive_usa` either 0 or 1, and only then drops `NaN`s, but this `NaN` would only occur if the *original* `is_expensive_usa` column (if it existed) had NaNs, not because of the cleaning of `Launched Price (USA)`. This is a subtle but critical difference in `NaN` handling related to the binary target. The task explicitly stated: ""Drop rows where `is_expensive_usa` is NaN."" This implies that `is_expensive_usa` *can* be NaN *before* dropping, which means it should be created with `NaN`s for unparseable prices, as in the ground truth.

2.  **Camera Feature Cleaning:**
    *   **Incorrect MP Extraction:** `df['Front Camera'].apply(lambda x: int(''.join(filter(str.isdigit, x))) if any(char.isdigit() for char in x) else 0)`
        *   This logic will extract *all* digits and convert them to an integer, not just the MP value. For example, `48MP` would become `48`, but `24MP (ultrawide)` or `0.3MP` would become `24` and `3` respectively. This is incorrect. The task specifically asks for ""the first numerical MP value"" and handles decimals (e.g., `0.3MP`). The ground truth's `re.search(r'(\d+(\.\d+)?)\s*MP')` is the correct and robust way to do this.
        *   It also does not handle non-string types (e.g., `NaN`) gracefully; `str.isdigit` would fail on `NaN`. The ground truth explicitly checks `pd.isna(camera_str)`.

3.  **OneHotEncoder Import and Usage:**
    *   **Incorrect Import:** `from sklearnlovermia.encoder import OneHotEncoder` -- This is a typo (`sklearn.preprocessing` vs `sklearnlovermia.encoder`). This line will cause an `ImportError`. This is a critical error.
    *   **Mismatch in `get_feature_names_out()`:** When defining `X`, the generated code uses `one_hot_encoder.get_feature_names_out()`. However, `one_hot_encoded_df` is created, and the column names from that DataFrame should be used, not directly from the encoder object's `get_feature_names_out()` if the encoder was not initially fitted with all features. This leads to brittle code. The ground truth creates `dummy` variables and then explicitly lists the generated columns with `col.startswith('Company Name_')`.

4.  **NaN Handling in X and Y:**
    *   `X = X.dropna()` and `y = y.dropna()`
        *   This approach is problematic because dropping rows independently from `X` and `y` will likely lead to misaligned indices, especially if different rows have NaNs in `X` versus `y`. The correct approach (as demonstrated in the ground truth) is to concatenate `X` and `y`, drop NaNs, and then split them back, ensuring row alignment.
        *   The task explicitly stated: ""Drop rows with any NaNs in features or target."" The generated code drops them *after* the initial `X` and `y` definition *independently*. This is less robust than the ground truth's combined `dropna`.

**Strong Points:**

*   Correct use of `pandas` for basic data loading and `str.replace`.
*   Correct use of `train_test_split`, `RandomForestClassifier`, `f1_score`, and `roc_auc_score`.
*   Adherence to the specified `random_state` and `n_estimators`.
*   Attempts to complete all cleaning steps mentioned in the prompt.",llama-3.3-8b-instruct,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Define a function to clean price columns
def clean_price(price):
    for symbol in ['PKR', 'INR', 'CNY', 'USD', 'AED']:
        price = price.replace(symbol, '')
    price = price.replace(',', '')
    try:
        return pd.to_numeric(price)
    except ValueError:
        return np.nan

# Apply the function to price columns
df['Launched Price (Pakistan)'] = df['Launched Price (Pakistan)']...",7,"The generated code correctly addresses most of the task requirements. It loads the dataset, applies the cleaning function to the specified columns, calculates the average global price, and identifies/displays the top 5 models.

However, there are a few areas for improvement:
1.  **Error Handling in `clean_price` function**: The `clean_price` function in the generated code assumes `price` is always a string and attempts to `replace` characters. If a price value is already `NaN` or a non-string type (e.g., an integer or float that wasn't correctly parsed initially), `price.replace` will raise an `AttributeError`. The ground truth handles this by first checking `pd.isna(price_str)` and also ensuring input is converted to string. This makes the `clean_price` function more robust.
2.  **Robustness for Missing Columns**: The generated code directly accesses `df['Launched Price (Pakistan)']` etc. which will throw a `KeyError` if any of these columns are not present in the CSV. The ground truth includes checks like `if col in df.columns:` which makes it more robust to variations in the input data.
3.  **`mean(axis=1)` behavior with NaNs**: While `mean(axis=1)` by default uses `skipna=True`, explicitly stating it (as in the ground truth) can make the intent clearer. The generated code implicitly relies on this default.
4.  **`pd.to_numeric` vs. `float()`**: The use of `pd.to_numeric` in the generated code is generally robust for converting to numeric types and handles non-convertible errors by returning `NaN` when `errors='coerce'` is used (which is the default in many `pd.to_numeric` calls, though not explicitly specified here). For this specific use case, returning `NaN` via the `try-except ValueError` block with `float()` (as in the ground truth) is also perfectly acceptable and achieves the same result for parsing individual strings after cleaning.
5.  **Output Sorting**: The `nlargest` method already returns the top N rows sorted by the specified column in descending order. Therefore, the `sort_values` call in the final `print` statement is redundant (`.sort_values(by='Average Global Price', ascending=False)`). The ground truth correctly sorts first and then takes the head. Both achieve the same result for top N.

Overall, the generated code performs the core task correctly for typical inputs but lacks some of the robustness and defensive programming found in the ground truth, which makes the ground truth more production-ready.",llama-3.3-8b-instruct,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('mobiles_2025.csv')

# Clean and preprocess the data
data['RAM'] = pd.to_numeric(data['RAM'].str.replace('GB', ''), errors='coerce')
data['Battery Capacity'] = pd.to_numeric(data['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
data['Launched Price (USA)'] = pd.to_numeric(data['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''), errors='coerce')

# Drop rows with miss...",4,"The generated code fails to meet several key requirements of the task, particularly regarding the plotting structure.

**Correctness:**
- **Data Loading:** Correctly loads the CSV.
- **Cleaning:** The cleaning logic for 'RAM', 'Battery Capacity', and 'Launched Price (USA)' is mostly correct (`str.replace` and `pd.to_numeric` with `errors='coerce'`). However, it misses stripping whitespace after replacement, which could lead to issues if the 'GB', 'mAh', or 'USD ' are followed by spaces. The ground truth correctly handles `str.strip()`.
- **Handling NaNs:** Correctly drops rows where the cleaned columns are NaN.
- **Filtering:** Correctly filters for 'Apple' products.
- **Plotting (Critical Failure):** This is where the code deviates significantly and fundamentally fails the plotting requirement. The task explicitly asks for ""For each `Launched Year`, generate box plots to visualize and compare the distributions of the cleaned `RAM`, cleaned `Battery Capacity`, and cleaned `Launched Price (USA)`. Use subplots for clarity (e.g., one row of three subplots, or three separate figures).""
    - The generated code attempts to put all years *onto the same box plot* for each metric, using `label=year` within `boxplot`. This does not achieve the requested ""for each Launched Year"" separate visualizations. It generates *one* figure with three subplots, and each subplot tries to overlay all years for its respective metric, which is not what was asked. The boxes will overlap and be unreadable.
    - The ground truth correctly iterates through *each year* and generates a *new figure* with three subplots for *that specific year's data*, comparing the three metrics within that year.

**Code Quality:**
- **Imports:** Correct.
- **Clarity:** The cleaning steps are clear. The plotting loop is conceptually flawed for the specified output.
- **Readability:** Generally readable.

**Completeness:**
- **Loading:** Yes.
- **Cleaning:** Mostly, but misses `str.strip()`.
- **NaN Handling:** Yes.
- **Filtering:** Yes.
- **Grouping:** Yes, it groups, but doesn't effectively use the grouping for the final plotting requirement.
- **Plotting:** Critically fails to meet the requirement for *generating box plots per year* for each metric. It attempts to aggregate all years onto single plots rather than showing year-by-year distributions.
- **Titles and Labels:** Sets generic titles and y-labels, but not dynamically per year as requested by the task (""indicating the year and metric""). The generated code's labels are fixed, whereas the task implies dynamic labels for each year's set of plots.
- **Subplots/Separate Figures:** Fails to generate separate figures or distinct subplot sets *for each year*.

The primary failure is the misinterpretation of the plotting requirement, leading to a fundamentally incorrect visual output compared to the explicit instructions.",llama-3.3-8b-instruct,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.feature_selection import SelectFromModel, RFE, mutual_info_classif
from sklearn....",4,"The generated code attempts to address many aspects of the prompt but fails significantly in execution, demonstrating a lack of understanding of several core machine learning concepts and libraries.

**Primary Reasons for Failure & Critical Missing/Incorrect Aspects:**

1.  **Fundamental Data Preprocessing Issues:**
    *   **Feature Engineering (Advanced):** The prompt explicitly asked for *advanced* feature engineering including ""polynomial features, interaction terms, text mining from names.""
        *   While `PolynomialFeatures` is imported, it's never integrated into the `ColumnTransformer` or a `Pipeline` for numerical features. It's just a standalone object `poly_features` that is not used.
        *   Text mining from names (like extracting titles, name length) is completely missing.
        *   ""Interaction terms"" were vaguely defined by `interaction_features = FeatureExtractor(['Pclass', 'Age', 'Fare'])`, but `FeatureExtractor` is undefined and doesn't exist in scikit-learn. This indicates a syntactical and conceptual error.
    *   **Missing Value Imputation (Multiple Strategies):** The prompt asked for ""multiple imputation strategies."" The code only uses `SimpleImputer(strategy='mean')`. More critically, the `ColumnTransformer` for imputation (`imputer = ColumnTransformer(...)`) is incorrectly structured and doesn't actually apply imputation to anything after `X_train`, `X_test` are split. The `imputer` object is created but never used to transform `X_train` or `X_test`. This means the models are likely trained on data with NaNs, which will cause errors or unexpected behavior. The ground truth explicitly uses `IterativeImputer` for multiple imputation.
    *   **Preprocessing Pipeline Integration:** The `ColumnTransformer` (`preprocessor`) is defined correctly for basic numerical (scaling) and categorical (one-hot encoding) columns. However, it's never actually applied to `X_train` or `X_test`. The pipelines for base models are also missing this crucial preprocessing step. This is a critical flaw, as models are trained on raw, unprocessed data.
    *   **`PassengerId`, `Name`, `Ticket`, `Cabin` Handling:** The prompt implied these should be handled (e.g., text mining from Names, extracting deck from Cabin) and potentially dropped. The generated code does not address these columns at all. `PassengerId` and `Ticket` are numeric/string but may not be useful features directly.

2.  **Hyperparameter Tuning (Integration with Preprocessing):**
    *   The `GridSearchCV` instances are fit directly on `X_train` and `y_train` *without* any preprocessing pipeline. This means the models are trained on raw, unscaled, and un-encoded data. This is fundamentally incorrect for models like SVM and Neural Networks which are sensitive to scaling, and for categorical features which need encoding.
    *   `KFold` was used for cross-validation in GridSearchCV, but `StratifiedKFold` is generally preferred for imbalanced classification tasks like survival prediction to maintain class distribution. While `KFold` is not strictly wrong, `StratifiedKFold` is a better practice here.

3.  **Ensemble Models (Incorrect Usage):**
    *   `SVC` in `VotingClassifier` often requires `probability=True` to enable soft voting, which is not set in the generated code for `SVC()`. If default hard voting was intended, it should be specified.
    *   The ensemble models are fit on the `X_train` *again*, but the underlying base models were fit on unprocessed data. The ensembles themselves still operate on unprocessed data.

4.  **Feature Selection (Incomplete and Misleading):**
    *   Only `RFE` and `SelectFromModel` are imported, but `mutual_info_classif` is imported but not used. The prompt asked for ""multiple techniques and compare their impact,"" but only `RFE` is actually *applied*.
    *   The `RFE` application (`selector.fit(X_train, y_train)`) operates on unprocessed `X_train`, which is not ideal for `RF` or `SVC` based RFE. Also, the selected features are not then used to train and evaluate models to ""compare their impact"" as requested.

5.  **Model Evaluation (Weak):**
    *   The stratified k-fold cross-validation is only demonstrated for `rf_grid.best_estimator_` and not for all models or ensembles as requested. The scoring is just `model.score()`, which defaults to accuracy and doesn't include comprehensive metrics like ROC AUC, precision, recall, F1-score which are crucial for classification tasks and explicitly asked for.
    *   Learning curves and validation curves are completely missing.
    *   Statistical significance tests are missing.

6.  **Model Interpretability (Severe Errors):**
    *   `shap.Explainer` is used, but it's initialized with `rf_grid.best_estimator_`, which is a `RandomForestClassifier` but *not* a pipeline. This means the SHAP values are calculated on the raw `X_test`, which is again incorrect as the model expects processed data.
    *   `shap.summary` plots need `show=False` for programmatic control, and `plt.show()` afterward. The code just calls `summary(shap_values, X_test)`, which would result in unhandled plots.
    *   LIME is completely missing.

7.  **Code Quality & Structure:**
    *   Imports are somewhat disorganized (e.g., `xgboost` imported twice).
    *   Inconsistent variable naming (e.g., `data` vs `X_train`).
    *   Missing imports (e.g., `OneHotEncoder` was imported but not used explicitly in the `ColumnTransformer` which needs `from sklearn.preprocessing import OneHotEncoder`).
    *   The code is structured as sequential blocks, but the critical interdependencies (e.g., preprocessing applied before tuning and evaluation) are completely broken.

In summary, the generated code outlines some desired steps but fails to properly implement the core logic for a robust machine learning pipeline, particularly around data preprocessing, model tuning integration, and comprehensive evaluation. Many parts are syntactically incorrect or conceptually flawed, leading to a non-functional or severely flawed solution.",llama-3.3-8b-instruct,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Define features X
X = df[['Year', 'Runtime']]

# Define the target y
y = df['Rating']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Linear Regres...",9,"The generated code correctly addresses all the core requirements of the task: loading data, defining features and target, splitting data (with correct test size and random state), training a Linear Regression model, and evaluating it using R-squared and MAE. The imports are proper, and the code structure is clear and follows a logical flow.

The only minor aspect where it differs from the more robust ground truth is the lack of explicit data preprocessing for the 'Runtime' column (e.g., stripping ' min' and converting to numeric) and error handling for file loading or missing columns. While the problem statement didn't explicitly ask for such robustness (it assumes the CSV file is in the current directory and implicitly assumes data types are suitable), in a real-world scenario, these data cleaning steps would be crucial. However, given the directness of the prompt, the code provides a perfectly functional and correct solution based on the explicit requirements. The print statement for evaluation combines R2 and MAE which is also fine.",llama-3.3-8b-instruct,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Select the features
X = df[['Rating', 'Runtime']]

# Standardize these features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
labels = kmeans.fit_predict(X_scaled)

# Create a scatter plot
...",6,"The generated code correctly performs the core K-Means clustering steps: loading data, selecting features, standardizing, and applying K-Means with the specified parameters. It also attempts to visualize the results and calculate cluster means.

However, there are several key areas for improvement as compared to the ground truth:

1.  **Robustness in Data Loading/Cleaning**: The generated code lacks any error handling for `FileNotFoundError` or checks for missing/incorrect columns (`Rating`, `Runtime`). More critically, it completely omits the necessary data cleaning for the 'Runtime' column (e.g., removing ' min' and converting to numeric), and handling `NaN` values, which are essential for this specific dataset and often crucial in real-world data science tasks. The ground truth explicitly handles this, which is a significant omission in the generated code.
2.  **Scatter Plot Detail**:
    *   The `c=labels` correctly colors points by cluster.
    *   The `cmap` and `s` (size) parameters are missing, leading to a less informative and aesthetically pleasing plot.
    *   A `colorbar` is not added, which is very helpful for understanding which colors correspond to which cluster labels.
    *   The axis labels omit units (`(minutes)`) which is a minor detail but good practice.
    *   The `plt.figure(figsize=...)` and `plt.grid` calls from the ground truth are missing, which can impact readability and presentation.
3.  **Cluster Mean Calculation**: While the generated code correctly calculates the mean rating and runtime per cluster, it iterates using a `for` loop. The ground truth's use of `df.groupby('cluster_label')[['Rating', 'Runtime']].mean()` is significantly more concise, idiomatic, and efficient for pandas, demonstrating a higher level of pandas proficiency.
4.  **No `cluster_label` column in DataFrame**: The generated code assigns labels to a standalone `labels` variable and then uses `X[labels == i]` to select cluster members. A more robust and common practice, as shown in the ground truth, is to assign the `labels` back to the original DataFrame as a new column (e.g., `df['cluster_label'] = labels`). This makes subsequent analysis, like the `groupby`, much cleaner and more integrated with the DataFrame structure. Its current implementation implicitly assumes that `X` (which is `df[['Rating', 'Runtime']]`) has the same index as `labels`, which is fine in this case, but less explicit.

Overall, it gets the core functional requirements done, but lacks robustness, data cleaning foresight, detailed plotting, and optimal pandas usage compared to a production-ready solution.",llama-3.3-8b-instruct,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter

# Load dataset
df = pd.read_csv('imdb_top_250.csv')

# Create new feature: title length
df['title_length'] = df['Title'].str.len()

# Calculate Pearson correlation coefficient
from scipy.stats import pearsonr
corr_coef, _ = pearsonr(df['title_length'], df['Rating'])
print(f""Pearson correlation coefficient: {corr_coef}"")

# Preprocess m...",7,"The generated code correctly addresses most of the requirements.

**Strong points:**
*   Correctly loads the dataset.
*   Accurately creates the `title_length` feature.
*   Correctly calculates and prints the Pearson correlation coefficient using `scipy.stats.pearsonr`.
*   Successfully preprocesses titles by converting to lowercase, removing punctuation, and tokenizing.
*   Effectively removes stop words.
*   Correctly finds and prints the 10 most common words with their frequencies.
*   Includes all necessary imports and handles NLTK downloads.

**Areas for improvement:**
*   **Correlation Calculation with `Rating` Column:** The generated code directly computes `pearsonr(df['title_length'], df['Rating'])`. This is problematic because the `Rating` column from `imdb_top_250.csv` is typically a string (e.g., ""8.7""). The Ground Truth correctly uses `pd.to_numeric(df['Rating'], errors='coerce')` and `dropna` to ensure the `Rating` column is numerical and to handle potential non-numeric entries, which is crucial for a valid correlation calculation. The generated code would likely raise a `TypeError` if `Rating` is not numeric, or produce incorrect results if pandas implicitly attempts coercion that fails.
*   **Punctuation Removal:** The generated code uses `title.translate(str.maketrans('', '', string.punctuation))`. While this is a standard way to remove punctuation, it specifically targets characters defined in `string.punctuation`. The Ground Truth's use of `re.sub(r'[^a-z0-9\s]', '', text)` is more robust as it removes *any* character that is not alphanumeric or whitespace, catching a broader range of ""punctuation"" or unwanted symbols, as implied by ""remove punctuation (characters that are not alphanumeric or whitespace)"".
*   **Clarity on ""Pearson correlation coefficient"" vs. ""Pearson correlation between title_length and Rating"":** The prompt asks to print ""Pearson correlation coefficient between `title_length` and the `Rating` column"". The generated code's print statement is slightly less specific than the prompt's request (missing the specific columns).

To improve, the code should explicitly convert the `Rating` column to a numeric type and handle potential non-numeric values before calculating the correlation. It could also consider using `re` for punctuation removal for robustness.",llama-3.3-8b-instruct,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('penguins.csv')

# Define features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', '...",5,"The generated code has several critical issues that prevent it from correctly solving the task.

**Correctness Issues:**

1.  **Imputation of Numerical Features:** The line `X[numerical_features] = X[numerical_features].apply(lambda x: x.fillna(x.median()))` attempts to fill NaNs using `apply` but `apply` on a DataFrame with a lambda function that returns a Series can lead to unexpected behavior or `SettingWithCopyWarning` if not handled carefully, and often doesn't modify the original DataFrame in place as intended. A direct `fillna` on columns or `apply` for each column separately is safer. More crucially, `x.median()` calculates the median for each *column* within `numerical_features`, which is correct. However, `apply` does not propagate changes to the original DataFrame `X` correctly through this assignment unless `X.copy()` was made prior and then correctly reassigned.

2.  **Imputation of Categorical Features and Dropping Rows:**
    *   `for feature in categorical_features: if X[feature]. dtype == 'object': X[feature] = X[feature].fillna(X[feature].mode()[0])` is generally correct for filling with mode.
    *   **Major Issue:** `X = X[X['island'] != 'unknown']` This line incorrectly assumes that if an `island` value was missing and subsequently imputed, it would become the string `'unknown'`. The task specified dropping rows if `island` was *still* NA after mode imputation, or if mode imputation *itself* resulted in NA (e.g., all values were NA). The generated code performs mode imputation, but then tries to filter based on a literal string 'unknown', which was never explicitly mentioned as a fill value. This will likely remove valid data or do nothing depending on what the mode actually was, and *fails to address the requirement to drop rows if imputation results in NA for 'island'*.
    *   The dropping of rows based on `X['island'] != 'unknown'` is also problematic because it happens *inside* the loop for categorical features, and for *each* categorical feature being processed, not just `island`. This logic is flawed.

3.  **Scaling of Numerical Features After ColumnTransformer:**
    *   `preprocessor = ColumnTransformer(...)` creates a preprocessor.
    *   `X = preprocessor.fit_transform(X)` transforms `X` into a NumPy array. At this point, the DataFrame structure is lost.
    *   `X[:, numerical_features] = scaler.fit_transform(X[:, numerical_features])` is incorrect because `numerical_features` are column *names*. After `ColumnTransformer`, `X` is a NumPy array, and `numerical_features` cannot be used for indexing. The numerical features will be the first columns in the transformed array, but accessing them by their original names (e.g., `X[:, 'culmen_length_mm']`) will raise an error. This line attempts to re-scale parts of `X` that may have already been scaled (if 'passthrough' was selected implicitly scaling them) or, more likely, it will fail due to incorrect indexing. The `ColumnTransformer` itself should be used to handle scaling (e.g., by including `StandardScaler` in the `num` transformer).

4.  **Target Variable Imputation:** The ground truth code correctly handles the case where `species` (the target) might have missing values, by dropping those rows. The generated code does not include this crucial step, which is good practice and often necessary in real-world scenarios.

5.  **`make_scorer` for Accuracy:** `scoring='accuracy'` is sufficient for classification tasks in `cross_val_score`. Using `make_scorer(lambda y, y_pred: (y == y_pred).mean())` is overly complicated and unnecessary, though it functionally achieves accuracy.

**Code Quality and Completeness Issues:**

*   **Pipeline Usage:** The imports `Pipeline` and `ColumnTransformer` suggest an intention to use a proper scikit-learn pipeline, which would simplify and robustly manage the sequential preprocessing steps (imputation, encoding, scaling). However, the code proceeds with manual, sequential transformations, leading to a loss of DataFrame structure and indexing issues as described above. A proper pipeline would handle passing feature names and applying transformations cleanly.
*   **Clarity on Dropping Rows for Island:** The conditional dropping of rows related to 'island' (e.g., `if X['island'].isnull().all():`) as implemented in the ground truth for edge cases is more robust than the generated code's simplified assumption.
*   The generated code does not explicitly handle the case where `X` might have `NaN` values *after* the imputation and transformation due to unintended side effects or missed edge cases.

In summary, the core logic for preprocessing (especially handling missing values in 'island' and correctly indexing after `ColumnTransformer`) is flawed, leading to a high likelihood of errors or incorrect results. The manual application of transformations prevents the benefits of a `ColumnTransformer` and `Pipeline` approach.",llama-3.3-8b-instruct,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Select the features and drop rows with missing values
selected_features = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]
df_selected = selected_features.dropna()

# Standardize the features
scaler = StandardS...",6,"The generated code correctly addresses most of the task requirements but has several critical issues that prevent it from being a fully correct solution.

**Positive Aspects:**
*   Loads the dataset.
*   Selects the specified features.
*   Applies `StandardScaler` and `KMeans` with the correct parameters (`n_clusters=3`, `n_init='auto'`, `random_state=42`).
*   Applies `PCA` to 2 components.
*   Generates two plots with titles, using PCA components.

**Areas for Improvement/Critical Issues:**

1.  **Incorrect NA Handling and `species` Alignment:**
    *   The `df_selected = selected_features.dropna()` line only drops NA where `selected_features` themselves are NA. However, it implicitly loses the row context for the `species` column.
    *   For the second plot, `df['species']` is used directly, which still contains NAs and includes rows that were dropped during `df_selected.dropna()`. This means the `c=df['species_code']` and the `pca_species` (which is derived from `df_selected` after NA removal) will have misaligned indices, leading to incorrect plotting of species labels. The ground truth correctly handles this by creating `df_selected` with `species` and dropping NAs from that combined DataFrame.
    *   The line `selected_features = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]` creates a new DataFrame, so `selected_features.dropna()` only affects that new DataFrame, not `df`, making the `species` alignment challenging.

2.  **Redundant PCA/Scaling for Second Plot:** `pca_species = pca.fit_transform(scaler.fit_transform(df_selected))` re-applies `fit_transform` which is unnecessary and slightly inefficient. The PCA components from the first `pca.fit_transform(df_standardized)` should be reused. More importantly, it re-fits the `scaler` and `pca` which is incorrect. The same `scaler` and `pca` objects derived from `fit_transform` on the *cleaned* data should be used for all transformations.

3.  **Basic Plotting:** While it generates plots, it uses `plt.scatter` directly without `seaborn` or customizing plot aesthetics (like point size, transparency, legend, grid, axis labels) which are often expected for clear data science plots. `c=` with `matplotlib.pyplot` uses default colormaps, which might not be as clear as `seaborn`'s `hue` or `palette`.

4.  **`species_code` Mapping:** The `species_map` and `df['species_code'] = df['species'].map(species_map)` approach to converting species names to numerical codes is fine, but it needs to be applied to the *filtered* `df` (or more precisely, aligned with the `pca_features`) for correct results. As implemented, it operates on the original `df` indices, leading to the misalignment issue mentioned in point 1.",llama-3.3-8b-instruct,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import label_encode
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('penguins.csv')

# Impute missing values
imputer_num = Simp...",3,"The generated code has several significant errors and omissions that prevent it from correctly solving the given task.

**Major Issues:**

1.  **Incorrect Imputation and Data Handling for `df`**:
    *   The `SimpleImputer` is used to create `X_num` and `X_cat`, but these imputed arrays are then concatenated into `X`. However, `X` is *not* used for the subsequent data processing (like one-hot encoding within the `ColumnTransformer`) or for the plotting.
    *   The original `df` DataFrame is used for plotting, but it's never actually imputed with the `SimpleImputer` results. This means the plots will use the *unimputed* data for `flipper_length_mm` and `sex`, which violates the requirement to use imputed data.
    *   The requirement to drop rows where `island` is NA is completely missed.
    *   The imputation for `sex` using `SimpleImputer(strategy='most_frequent')` is generally okay, but the code doesn't store this imputed `sex` back into `df` for plotting. It also tries to use a `sex_imputed` column in the plotting loop which doesn't exist.

2.  **Incorrect Label Encoding**:
    *   `sklearn.metrics.label_encode` does not exist. It's likely a typo for `sklearn.preprocessing.LabelEncoder`. This will cause an `AttributeError`.
    *   Even if fixed, the `LabelEncoder` should be used on the `species` column from the *imputed/processed* DataFrame, not directly on `df['species'].astype(str)` if `df` hasn't had `island` NAs dropped.

3.  **Incorrect Feature Definition for `X` in Pipeline**:
    *   The `ColumnTransformer` is defined to process `num_features` and `cat_features` from `df`. This is correct in principle, but the `ColumnTransformer` is applying transformations to the *original* `df` columns, not the pre-imputed `X_num` and `X_cat` that were created earlier. This means the `ColumnTransformer` (within the `Pipeline`) attempts to impute *again* internally, or fails if there are NAs in the initial data it receives (which there would be). The task specifies using the *four imputed numerical measurements* and *one-hot encoded imputed sex/island*. The current setup does not correctly implement this flow for `X`.
    *   The numerical features are passed as 'identity' in the `ColumnTransformer`, meaning they won't be imputed by the pipeline itself, assuming they are clean. But the task specifies they need imputation.

4.  **Incorrect Feature Importances and Names**:
    *   The `feature_names = model.named_steps['preprocessor'].get_feature_names_out()` line will get the correct names from the `OneHotEncoder` and the 'identity' numerical features within the pipeline. This part is okay *if* the data feeding into the pipeline were set up correctly.

5.  **Incorrect Plotting Logic**:
    *   The plotting loop tries to use `df['species'].unique()` for iteration, which is fine.
    *   However, `species_df[species_df['sex_imputed'] == 'Male']['flipper_length_mm']` will fail because `sex_imputed` column does not exist. The original `sex` column in `df` has NaNs and hasn't been imputed in `df` itself.
    *   The box plot is intended to show `flipper_length_mm` grouped by `sex` for each `species`. The generated code tries to do this manually with `plt.boxplot` and explicit slicing, which is much more cumbersome and error-prone than using `seaborn.boxplot` with `x`, `y`, and `hue` as in the ground truth.
    *   The `flipper_length_mm` being used for plotting is from the original `df`, which is not imputed.

6.  **Missing `train_test_split` Requirement**: The task does not ask for `train_test_split`. While not strictly an error, it's an unnecessary step that adds complexity.

**Summary of Key Failures:**

*   **Imputation Mismatch**: The imputation logic is separated from the data used by the model and the plots, leading to unhandled NaNs in the plotting data and a convoluted setup for the model input.
*   **Plotting Errors**: The plotting code is fundamentally flawed due to reliance on a non-existent column (`sex_imputed`) and not using the imputed data.
*   **Incorrect Label Encoding Import/Usage**: Will cause a direct crash.
*   **Missing Requirement**: Dropping NA `island` rows.

The overall approach to imputation and data flow is fragmented and incorrect for the specified task requirements.",llama-3.3-8b-instruct,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer
import numpy as np

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(""Failed to load dataset: "", str(e))

# Perform KNN imputation for numerical columns
def knn_imputation(df, numerical_cols, n_neighbors):
    imputer = KNNImputer(n_neighbors=n_neighbors)
    for col in numerical_cols:
        df[col] = imputer.fit_transfor...",6,"The generated code correctly addresses most of the task requirements but has a significant issue with the KNN imputation implementation.

**Correctness:**
The KNN imputation for numerical columns is incorrect. `imputer.fit_transform(df[[col]])` applies the imputer to each numerical column *individually*. KNN imputation, by its nature, should consider *all specified numerical columns together* to find the nearest neighbors based on the multidimensional space of those columns. By applying it column by column, it essentially degenerates to a simple mean imputation (or similar, depending on how `KNNImputer` handles single-feature input) and loses the core benefit of KNN which leverages relationships between features.

The categorical imputation for `sex` is correct.
The NA verification is also correct, although it prints for all columns, not just the target ones, which is acceptable.

**Code Quality:**
The code is structured well into functions, which is good practice. Imports are correct. The dataset loading has a basic error handling.

**Completeness:**
It loads the data.
It attempts KNN imputation for numerical columns but does so incorrectly.
It correctly imputes `sex` with the mode.
It prints remaining NA values for verification.

**Areas for Improvement:**
1.  **Critical Fix:** The KNN imputation needs to be applied to all numerical columns simultaneously. Instead of iterating `for col in numerical_cols: df[col] = imputer.fit_transform(df[[col]])`, it should be `df[numerical_cols] = imputer.fit_transform(df[numerical_cols])` (or similar, depending on whether it needs to be `pd.DataFrame()` or `pd.Series()` conversion). This is the biggest flaw.
2.  The error handling for `FileNotFoundError` in `load_dataset` could be more specific, similar to the ground truth, which provides a clearer message and potentially exits. Currently, it just prints a generic error.
3.  The `impute_categorical` function iterates through `categorical_cols` but the task only specified 'sex'. While general functions are good, adhering strictly to the task's single target ('sex') might be preferred for a precise solution.

The generated code is a good attempt, but the fundamental misunderstanding/misapplication of `KNNImputer` in the primary imputation step significantly impacts its correctness for the advanced cleaning task.",llama-3.3-8b-instruct,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the dataset to include only 'Adelie' and 'Gentoo' species
df_filtered = df[(df['species'] == 'Adelie') | (df['species'] == 'Gentoo')]

# Handle NA values in the `culmen_length_mm` column
df_filtered = df_filtered.dropna(subset=['culmen_length_mm'])

# Perform an independent two-sample t-test
t_stat, p_val = stats.ttest_ind(df_filtered[df_filtered['species'] == 'Adelie']['culmen_le...",8,"The generated code correctly performs all the required steps: loading data, filtering species, handling NAs, performing the t-test, printing results, and interpreting the p-value. It is concise and directly addresses the problem.

However, the code could be improved in terms of robustness and adherence to professional practices. Specifically:
1.  **Error Handling**: There is no error handling for `FileNotFoundError` when loading the CSV, which could cause the script to crash if the file is missing.
2.  **Modularity/Readability**: Encapsulating the logic within a function (as in the ground truth) improves reusability, testability, and overall code organization, especially for more complex tasks.
3.  **Clarity of Output**: While the output is correct, the ground truth provides more detailed print statements about the steps being performed (e.g., shape of filtered data, number of rows dropped), which aids in debugging and understanding the execution flow.
4.  **Implicit Assumptions**: The `ttest_ind` function by default assumes `equal_var=True`. While this is fine for a standard t-test, making it explicit (`equal_var=True`) enhances clarity, as the choice between Student's and Welch's t-test can be important depending on variance assumptions.

Overall, it's a very good attempt that fully solves the problem, but could benefit from best practices for production-ready or more robust code.",llama-3.3-8b-instruct,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('penguins.csv')

# Define features and tar...",7,"The generated code correctly implements most of the requirements.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly defines features and target.
*   Uses `ColumnTransformer` and `Pipeline` effectively for preprocessing.
*   Correctly applies median imputation for numerical features and mode imputation for categorical features (`sex` and `island`).
*   Correctly applies One-Hot Encoding for categorical features.
*   Performs a stratified `train_test_split` with the specified `random_state`.
*   Trains a `RandomForestClassifier` with the specified `n_estimators` and `random_state`.
*   Makes predictions and prints the accuracy score.

**Areas for improvement:**
*   **Label Encoding for Target:** The task explicitly asks to ""ensure `species` is label encoded if it's not already numeric."" The current code passes `y` directly to `train_test_split` and `clf.fit()`, which will likely work because `RandomForestClassifier` can sometimes handle string targets, but it doesn't explicitly perform the `LabelEncoder` step as requested. The ground truth code correctly applies `LabelEncoder` to `y_encoded`. While `sklearn` models can sometimes implicitly handle string targets, explicit label encoding is often a safer and more consistent practice, especially for classification reports.
*   **Classification Report `target_names`:** When printing the `classification_report`, it's best practice to pass `target_names` if the target was label encoded. Since the generated code doesn't explicitly label encode `y`, the report will show numeric labels (0, 1, 2) which are less readable than the actual species names. The ground truth code handles this by using the `label_encoder.classes_`.
*   **Duplicate Feature:** `body_mass_g` is listed twice in `numerical_features`. While this doesn't break the code, it's redundant and a minor quality issue.
*   **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.",llama-3.3-8b-instruct,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Calculate the Pearson correlatio...",9,"The generated code correctly defines the DataFrame, calculates the specified Pearson correlation coefficients, prints them, and correctly identifies the feature with the stronger absolute correlation with ""PerformanceScore"". The solution is concise and accurate.

Minor points for improvement:
1.  **Readability of printed output**: While the output is correct, the ground truth uses f-strings and `:.4f` formatting to present the correlation coefficients with consistent precision, which enhances readability and professionalism of the output. The generated code prints them directly, leading to variable precision (e.g., `0.9066698660232231`).
2.  **Explicit correlation method**: The `corr()` method by default calculates Pearson correlation, so `method='pearson'` is technically redundant for this specific task. However, explicitly stating it, as done in the ground truth, can improve code clarity for future readers or if the correlation method ever needs to be changed. This is a minor stylistic point.

Overall, the generated code successfully completes all aspects of the task and is very high quality.",llama-3.3-8b-instruct,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import lifelines
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.utils import concordance_index
from scipy import stats
import statsmodels.api as sm
import pymc3 as pm
import numpy as np
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('titanic.csv')

# 1. Kaplan-Meier survival analysis
kmf = KaplanMeierFitter()
kmf.fit(df['Voyage'], df['Survived'])
kmf.plot()

# 2. Cox proportional hazards regression models
cph = CoxPHFitter()
cph.f...",2,"The generated code attempts to address the prompt but largely fails to deliver on the core requirements due to significant misunderstandings of statistical methods and library usage.

Here's a breakdown of the critical issues:

**1. Kaplan-Meier Survival Analysis:**
*   **Incorrect `T` and `E` for Titanic data:** The prompt explicitly states ""treatment the voyage as time-to-event data (with survival as event indicator)"". The standard `Titanic` dataset does not have a `Voyage` column that represents time-to-event. The ground truth correctly interprets this, using a fixed `T=1` and `E` as death. The generated code uses `df['Voyage']` which doesn't exist and `df['Survived']` directly which is the event indicator, not the duration. This makes the KM plot meaningless in this context.
*   **Missing Data Imputation:** The code doesn't handle missing values for 'Age' or 'Embarked', which are present in the Titanic dataset and crucial for statistical modeling.

**2. Cox Proportional Hazards Regression Models:**
*   **Incorrect `T` and `E`:** Similar to KM, the `cph.fit(df, 'Survived', ['Pclass', 'Sex', 'Age'])` call is fundamentally flawed for Cox regression. `lifelines` expects a `duration_col` and an `event_col`. `df['Survived']` is the event, not the duration. The features should be passed as a DataFrame, not a list of column names, and a `duration_col` must be specified.
*   **Lack of Preprocessing:** Categorical features (Pclass, Sex) are used directly without proper encoding, which CoxPHFitter can handle, but it's not robust.

**3. Advanced Statistical Tests and Model Diagnostics:**
*   **Multivariate Analysis:** The `statsmodels.Logit` attempt is completely separate from survival analysis and doesn't explicitly link to the `CoxPHFitter`. It's a general multivariate analysis but not within the context of survival models as requested by the prompt.
*   **Interaction Effects:** The syntax `cph.fit(df, 'Survived', ['Pclass', 'Sex', 'Age', 'Pclass:Sex'])` will not work for interaction terms in `lifelines`. Interactions need to be explicitly created as new columns in the DataFrame (`df['Pclass:Sex'] = df['Pclass'] * df['Sex']`).
*   **Model Diagnostics:** `residuals = cph.resid_` is not how `lifelines` residuals are accessed or used for diagnostics. `cph.check_assumptions()` is a diagnostic but its use for proportional hazards relies on variation in duration, which isn't handled correctly. The `plot(qq)` also won't work directly with the output of `probplot`.

**4. Propensity Score Matching:**
*   **Conceptual Misunderstanding:** The implementation `ps = norm.cdf(df['Pclass'])` is a complete misunderstanding of propensity scores. Propensity scores are estimated probabilities of treatment assignment (e.g., Pclass=1) given covariates, typically using logistic regression, not simply a CDF of a single variable.
*   **Matching Logic:** `matched_df = pd.merge(df, ps, left_on='PassengerId', right_index=True)` and `matched_df = matched_df[matched_df['Pclass'] == 1]` don't implement any form of matching (e.g., nearest neighbor, caliper matching). This section is entirely incorrect.

**5. Bootstrap Resampling:**
*   **Incorrect `bootstrap_ci` implementation:** The custom `bootstrap_ci` function is flawed. It tries to call `estimator.summary()` as if it were a function that returns a single estimand for CI calculation, which it isn't. The `summary()` method prints information, it doesn't return the coefficients in a suitable format for collecting. Also, `cph.bootstrap_ci = bootstrap_ci` is assigning a new method, but then `ci = cph.bootstrap_ci('Survived', df)` is using string 'Survived' as an argument, which doesn't align with the function definition. Bootstrap for CoxPH would involve resampling the data and refitting the model multiple times, then extracting coefficients or C-indices from each fit.

**6. Bayesian Analysis:**
*   **PyMC3 Model Definition:** The model defined has several issues. `survival_prob = pm.math.exp(...)` is not appropriate for a binary outcome (survival). A `pm.Deterministic` with `pm.invlogit` (sigmoid) and then `pm.Bernoulli` for likelihood is typically used for logistic regression. `n=1` in `Binomial` is correct if `p` is defined as probability of success, but the linear predictor `exp` is wrong.
*   **Feature Handling:** The model tries to use `df['Sex'] == 'male'` and `df['Pclass'] == 1` directly within the PyMC3 model, which means these need to be converted to numerical arrays *before* passing them to PyMC3, which is not done. Missing values are also not handled for the Bayesian model.
*   **Lack of `arviz` and proper analysis:** PyMC3 is used, but `arviz` is not imported or used for proper inference data handling, summary, or plotting, which is standard practice.

**7. Model Validation and Sensitivity Analysis:**
*   `cph.check_assumptions()` is called, but again, the fundamental issue of the `T` (duration) column prevents proper use of this method.
*   No other sensitivity analyses (e.g., imputation sensitivity, prior sensitivity for Bayesian model) are attempted.

In summary, the code demonstrates a superficial understanding of some statistical concepts but fails significantly in their practical implementation using the specified libraries, particularly for survival analysis, propensity score matching, and Bayesian modeling. Data preprocessing and handling of missing values are also largely overlooked.",llama-3.3-8b-instruct,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# Create the DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'], 
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'], 
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77], 
        'Final_Score': [...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
1.  **Correct DataFrame Creation:** The DataFrame is created accurately with the provided data.
2.  **Correct Correlation Calculation:** The Pearson correlation coefficient is calculated and printed correctly.
3.  **Accurate Scatter Plot:** The scatter plot is generated with the correct axes ('Attendance_Rate' vs 'Final_Score'), colored by 'Subject', and includes a legend, title, and axis labels as requested. The use of seaborn's `scatterplot` is appropriate and leads to good aesthetics.
4.  **Correct Grade Assignment:** The 'Overall_Grade' column is added using `pd.cut`, which is a very efficient and correct way to categorize numerical data into bins. The grade boundaries are correctly applied for A, B, C, and D.
5.  **Correct Grade Distribution:** The count of students in each grade category is correctly calculated and printed.

**Areas for improvement (minor):**
1.  **Missing `numpy` import:** Although `pd.cut` is used for grading instead of `np.select`, if the latter approach were chosen (as in the ground truth for more explicit condition handling), `numpy` would be needed. This is a very minor point, as `pd.cut` is a perfectly valid and often preferable method here.
2.  **Formatting for Correlation:** The ground truth formats the correlation coefficient to 4 decimal places, which is a good practice for readability. The generated code prints it with full precision. This is a stylistic choice and not a correctness issue.
3.  **Plot Enhancements (Optional but good practice):** The ground truth includes `plt.tight_layout()`, `plt.grid(True)`, and places the legend outside (`bbox_to_anchor`). While not explicitly demanded in the prompt (beyond including ""clear axis labels"" which was met), these additions improve plot readability and professional appearance. The generated code produces a functional plot, but these slight enhancements would make it even better.

Overall, the solution is highly correct, uses appropriate libraries and functions, and satisfies all the core task requirements. The minor points are mostly about stylistic choices or common best practices rather than functional errors.",llama-3.3-8b-instruct,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']), 
        'Ticker': ['TECHCO']*10, 
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0], 
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0], 
        'Low': [149.5, 150.5, 1...",9,"The generated code is very good and correctly addresses almost all aspects of the task.

**Strong Points:**
- **Correct DataFrame Creation:** The DataFrame is created accurately and the 'Date' column is correctly set as the index in a concise manner.
- **Accurate Calculations:** Daily percentage return, 3-day SMA, and 5-day SMA are all calculated correctly according to the specified formulas.
- **Plot Generation:** A line plot is generated with the 'Close' price, 'SMA_3Day', and 'SMA_5Day' on the same chart, with a title, legend, and labeled axes.
- **Correct Bullish Crossover Logic:** The logic for identifying bullish crossovers (SMA_3Day crossing above SMA_5Day) is correctly implemented using boolean indexing and `shift()`.

**Areas for Minor Improvement:**
- **Plotting `df.index` vs `df['Column']` for x-axis:** While `plt.plot(df['Close'], ...)` automatically uses the DataFrame index for the x-axis when plotting a Series, explicitly using `plt.plot(df.index, df['Close'], ...)` is generally considered good practice for clarity and consistency, especially when dealing with time series. This is a minor stylistic point, but good to note. The current implementation implicitly achieves the correct x-axis.
- **Output Format for Bullish Signals:** The task asked to ""print the dates"" where the SMA_3Day crossed above the SMA_5Day. The generated code prints the full `DatetimeIndex` object, which includes the `dtype` and `name` information. While this is technically correct, the ground truth's `for date in bullish_crossovers: print(date.strftime('%Y-%m-%d'))` provides a cleaner, more user-friendly output that directly fulfills ""print the dates.""

Overall, the code is highly functional and accurate, demonstrating a strong understanding of pandas and matplotlib for financial data analysis. The issues are minor and do not detract significantly from its correctness or completeness.",llama-3.3-8b-instruct,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107], 
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 
        'Quantity': [1, 2, 1, 1, 3, 1, 1], 
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150], 
        'CustomerID': ['CUST001', 'CUST...",7,"The generated code correctly creates the DataFrame, calculates average order value, and identifies repeat customers. The plot is generated, but the calculation for product revenue is incorrect.

**Correctness:**
*   **Product Revenue Calculation (Incorrect):** The line `revenue_by_product = df.groupby('Product')['Quantity'].multiply(df.groupby('Product')['Price_USD']).sum()` is incorrect. `df.groupby('Product')['Quantity']` returns a GroupBy object, and multiplying it by another GroupBy object from `df.groupby('Product')['Price_USD']` does not perform element-wise multiplication within each group in the way intended. It leads to an error when executed locally. The correct approach is to calculate the revenue per row first (`df['Revenue'] = df['Quantity'] * df['Price_USD']`) and then group and sum this `Revenue` column.

**Code Quality:**
*   Imports are correct and placed at the top.
*   Variable names are descriptive.
*   Comments indicate sections, though not as detailed as the ground truth.

**Completeness:**
*   **Product Revenue Calculation:** As noted above, this calculation is incorrect.
*   **Plotting:** The plot is generated, but the `figsize` parameter is passed twice (once to `plt.figure` and once to `plot` method), although it functions.
*   **Output Clarity:** The output for repeat customers is good, and the average order value is clear.

To improve, the primary focus should be on fixing the `revenue_by_product` calculation to accurately reflect the total sales revenue per product. Adding comments for each step, similar to the ground truth, would also enhance readability.",llama-3.3-8b-instruct,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd
import numpy as np

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Create a new numerical feature ""DeviceType_Numeric""
df[""DeviceType_Numeric""] = df[""DeviceType""].map({""Desk...",9,"The generated code is very good and correctly addresses almost all aspects of the task.

**Strong points:**
*   **Correct DataFrame definition:** Accurately creates the initial DataFrame as specified.
*   **Correct DeviceType_Numeric mapping:** Uses `map` with the correct dictionary to create the `DeviceType_Numeric` column.
*   **Correct feature selection:** Selects the exact features required for the final DataFrame.
*   **Correct output format:** Prints the head of the resulting feature DataFrame.

**Area for minor improvement:**
*   **Division by zero handling:** The `np.where` condition `df[""PagesPerMinute""].isna() | df[""PagesPerMinute""].isinf()` is conceptually correct for handling `NaN` (e.g., from 0/0) and `Inf` (e.g., from x/0). However, a more robust and idiomatic pandas approach for handling `inf` values is often to use `replace([np.inf, -np.inf], np.nan)` first, and then `fillna(0)`. While the generated code's `np.where` *might* work for this specific dataset given the `PagesViewed` are all non-negative, for negative infinities (if they somehow arose) or a more general solution, a two-step `replace` then `fillna` is safer. For instance, `isinf()` only checks for positive and negative infinity, while `isna()` checks for `NaN`. If a result was `-np.inf`, the current `isinf()` would catch it, but if it was `np.nan` (e.g., from 0/0), `isna()` would catch it. The `np.where` effectively flattens this, which is fine. The ground truth's method ensures consistency for any `inf` becoming `NaN` before filling all `NaN`s. This is a very minor conceptual difference, not a correctness issue for this specific data.

Overall, the code is highly effective and demonstrates a strong understanding of pandas operations for data preparation.",llama-3.3-8b-instruct,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26, 45, 28]
}
df = pd.DataFrame(data)
...",9,"The generated code correctly defines the DataFrame, calculates the mean and standard deviation for 'TemperatureCelsius', identifies outliers based on the 1.5x standard deviation rule, and prints the required information for each outlier. The code is clean, concise, and directly addresses all parts of the prompt.

**Strong points:**
- Correct DataFrame creation.
- Accurate calculation of mean and standard deviation.
- Correct logic for identifying outliers using the 1.5 STD threshold (both upper and lower bounds).
- Correctly iterates through outliers and prints the specified fields (SensorID, Timestamp, TemperatureCelsius).

**Minor areas for improvement (not critical for this task, but good practice):**
- The ground truth includes `df['Timestamp'] = pd.to_datetime(df['Timestamp'])`. While not strictly required for the string comparison in printing, it's good practice for robust time-series operations.
- The ground truth includes more descriptive print statements for the mean, standard deviation, and bounds, which improves readability and understanding of the results. The generated code's optional print statement is a good start but could be more verbose.
- A message indicating if no outliers are found (like in the ground truth) would make the output more user-friendly.",llama-3.3-8b-instruct,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [10, 5, 25, 15, 8, 30, 9,...",9,"The generated code correctly defines the DataFrame, calculates the EngagementScore, and accurately identifies and prints the PostType with the highest average EngagementScore for each platform. The logic is sound and produces the correct output for all specified tasks.

The code is well-structured and uses standard pandas operations effectively. The use of a loop to iterate through unique platforms, followed by `groupby()` and `idxmax()`/`max()` for each platform, is a clear and understandable approach.

A minor improvement could be to format the `highest_avg_engagement_score` to two decimal places for better readability, similar to the ground truth. Also, while the looping approach is correct, using a more vectorized pandas operation (like `groupby().apply(lambda x: x.loc[x['EngagementScore'].idxmax()])` after a nested groupby, or a sort then drop_duplicates as in the Ground Truth) can sometimes be more performant for very large datasets, though for this size, the current approach is perfectly fine. However, given the prompt's simplicity and the correctness of the solution, these are minor points.",llama-3.3-8b-instruct,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu"", ""Injury"", ...",9,"The generated code correctly performs all the required tasks: defining the DataFrame, calculating `LengthOfStay`, computing average stay and patient count per diagnosis, and identifying the diagnosis with the longest average stay. The code is concise and directly addresses the problem statement without unnecessary verbosity.

**Strong Points:**
- **Correctness:** All calculations and identification steps are accurate.
- **Conciseness:** The use of `groupby().agg()` is efficient and idiomatic pandas for the aggregation step.
- **Clarity:** Variable names are descriptive.

**Minor Improvements for a perfect score:**
- **Output Formatting:** While the `diagnosis_info` DataFrame is printed, the prompt specifically asked to ""Print this information (Diagnosis, Average LengthOfStay, Patient Count)"". The current output `print(diagnosis_info)` is functional but doesn't explicitly format the output as a clean table with distinct columns as might be expected from ""print this information"" to a user or report. The ground truth demonstrated a more explicit formatted printout.
- **Function Encapsulation:** The ground truth code encapsulates the logic within a function, which is good practice for reusability and organization in larger scripts or applications, although not strictly required by the prompt's output format (""Provide only Python code""). This is a minor point, as the prompt did not ask for a function.
- **Docstrings/Comments:** The ground truth includes detailed comments and a docstring, which enhances readability and maintainability. While the generated code is simple enough not to heavily suffer without them, it's a general good practice in coding.",llama-3.3-8b-instruct,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",9,"The generated code correctly performs all the required tasks:
1. Defines the DataFrame as specified.
2. Calculates the ""WeightedRating"" correctly using the given formula.
3. Adds ""WeightedRating"" as a new column to the DataFrame.
4. Identifies and prints the ""Title"" and ""WeightedRating"" of the movie with the highest ""WeightedRating"".

The code is concise and directly addresses the prompt.

One minor area for improvement, though not strictly required by the prompt, would be to handle the edge case where `total_votes` could be zero, which would lead to a `ZeroDivisionError`. The ground truth code includes such a check. While this specific input doesn't trigger it, robust code often accounts for such scenarios. However, for the given problem constraints and input, the code is fully functional and correct.",llama-3.3-8b-instruct,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 10000, 8000]
}
df = pd...",9,"The generated code correctly defines the DataFrame, calculates the average price and mileage per brand, and presents this in a new grouped DataFrame. It uses an efficient `groupby().agg()` method, which is good practice. The `reset_index()` call correctly turns 'Brand' back into a column, as opposed to leaving it as the index, which is often a desired outcome for further processing or display.

Areas for minor improvement:
1.  **Output Clarity:** The ground truth code includes a descriptive print statement (`print(""DataFrame showing average PriceUSD and Mileage per Brand:"")`) before the DataFrame. While not strictly required by the prompt ""Print the resulting grouped DataFrame"", it greatly enhances the user experience by providing context to the output. The generated code just prints the DataFrame directly.
2.  **Column Naming:** The ground truth code uses named aggregations (e.g., `Average_PriceUSD`) which results in clearer column names in the output DataFrame. The generated code defaults to `PriceUSD` and `Mileage`, which might be ambiguous (`is this the original column or the average?`). While technically correct, enhancing readability of the aggregated columns is a good practice.

Overall, the code is highly functional and correct, with only minor aesthetic and clarity improvements possible.",llama-3.3-8b-instruct,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63, 83],
    ""De...",10,"The generated code is excellent. It correctly defines the DataFrame, adds the ""TotalStats"" column using a concise and idiomatic pandas approach (summing multiple columns at once), accurately identifies the Pokemon with the highest ""TotalStats"" and prints its name and the value, and correctly calculates and prints the average speed for each 'Type1'. The code is clean, readable, and directly addresses all requirements of the task.",llama-3.3-8b-instruct,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score, gap_statistic
from umap import UMAP
from umap.plot import point_cloud
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('titanic.csv')

# Engineer advanced features
df['family_members'] = df['sibsp'] + df['parch'...",3,"The generated code attempts to address the task but falls significantly short in terms of correctness, completeness, and robustness.

**Major Flaws:**

*   **Incorrect Feature Engineering:**
    *   `df[' ticket_sharing'] = np.where(df['fare'] > 0, 1, 0)` is a very simplistic and likely inaccurate proxy for ticket sharing. It doesn't identify actual shared tickets. The ground truth correctly uses `groupby('Ticket')`.
    *   `df['title'] = df['name'].apply(lambda x: x.split()[0])` will extract salutations like ""Mr."", ""Mrs."", ""Miss"", ""Dr."", etc., but it doesn't handle names with multiple parts before the title or group rare titles, leading to many low-frequency categories.
    *   Missing many advanced features specified: social network analysis (family groups beyond size, actual ticket sharing/group size), socioeconomic indicators (Age_Class, Fare_Class), and text-based features (ticket prefix, cabin deck, name length is present but simple).

*   **Preprocessing Errors:**
    *   The code directly calls `StandardScaler().fit_transform(df)`. This is a critical error because `df` contains mixed data types (numerical, categorical, object) and `NaN` values. `StandardScaler` can only process numerical data. It will fail or produce meaningless results.
    *   No handling of missing values (e.g., 'Age', 'Embarked', 'Cabin'). The ground truth handles this carefully.
    *   No one-hot encoding for categorical features (`Sex`, `Embarked`, `Pclass`, `Title`, etc.). This is a fundamental step for clustering algorithms like K-Means.

*   **Clustering Application Issues:**
    *   Clustering algorithms (KMeans, AgglomerativeClustering, DBSCAN, GaussianMixture) are initialized with `n_clusters=5` or default parameters but then are `fit_predict`ed *within the loop for optimal cluster determination* using `KMeans`. This means only K-Means is properly evaluated for optimal clusters, and the other algorithms are never properly fit to the data outside of the loop.
    *   DBSCAN parameters (`eps`, `min_samples`) are highly data-dependent and hardcoded without explanation or a method for finding them.
    *   `gap_statistic` is imported but not a standard `sklearn` function. It would lead to an import error. The ground truth defines its own approach to common validation metrics.

*   **Optimal Cluster Determination:**
    *   Only K-Means is used for determining optimal clusters. The requirement was to apply this to *multiple* clustering algorithms.
    *   The `gap_statistic` implementation is incorrect/missing.
    *   No explicit choice of `optimal_k` based on plots or analysis is made after these scores are generated.

*   **Analysis and Visualization:**
    *   The `analyze cluster characteristics and survival patterns` section relies on `df['cluster']` which is never created. It also assumes `kmeans.labels_` is globally accessible after the loop which isn't robust.
    *   The `sns.barplot(x=df.columns, y=kmeans.fit(df_scaled).transform(df_scaled))` line for ""Feature Importance"" is fundamentally incorrect. `transform` on `KMeans` returns distance to centroids, not feature importance. It's also trying to plot `df.columns` (strings) against numerical data directly from `transform`, which won't work meaningfully.
    *   The survival rate plot attempts to plot `kmeans.labels_` (numerical cluster IDs) against `df['survived'].value_counts()`, which will merge all survival counts into one bar plot per label, not survival rates *per cluster*.
    *   There's no analysis of cluster characteristics (e.g., mean/median of original features within each cluster) beyond printing `value_counts()` of survival.

*   **Stability Analysis:**
    *   While `adjusted_rand_score` is used, the loop for `kmeans_stable` is not correct. It fits `kmeans_stable` once and then compares subsequent *new* `KMeans` instances against the `kmeans_stable.labels_` result, but this is a bare minimum. Consensus clustering is missing entirely.

*   **External Validation:**
    *   `accuracy_score(actual_survival, predicted_survival)` is misused. Clustering is unsupervised; there's no ""ground truth"" for cluster labels to compare against directly to get an `accuracy_score`. You can't compare arbitrary cluster labels (0, 1, 2...) to survival outcomes (0, 1) and expect a meaningful accuracy. The ground truth correctly validates survival patterns *within* clusters, which is the proper approach for external validation in unsupervised learning.

*   **Imports:** `umap.plot` and `gap_statistic` are imported but not necessary or incorrectly used. `TSNE` and `PCA` are required by the prompt but not imported/used.

This code has fundamental logical errors in data processing and analysis interpretation, making its output unreliable and largely incorrect for the given task.",llama-3.3-8b-instruct,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dystopian""],
    ""Stock"": [...",9,"The generated code is highly correct and complete, addressing all requirements of the task.

**Strong Points:**
- **Correct DataFrame Creation:** The DataFrame `df` is correctly initialized, matching the ground truth.
- **Accurate Aggregation:** The `groupby().agg()` method is used effectively to calculate the count of books (`Title: 'count'`) and the average price (`Price: 'mean'`) per author. This is a concise and standard pandas approach.
- **Clear Column Renaming:** Renaming the aggregated columns to 'Count of Books' and 'Average Price' makes the output very readable.
- **Correct Most Stock Author Identification:** The `groupby('Author')['Stock'].sum()` followed by `idxmax()` correctly identifies the author with the highest total stock.
- **Clear Output:** The final print statements clearly present the required information for both parts of the task.

**Minor Improvements (for a perfect 10):**
- **Output Formatting for Author Statistics:** While `print(author_info)` correctly displays the DataFrame, the ground truth's use of f-strings for formatted output (e.g., `Avg Price: ${row['average_price']:.2f}`) provides a more human-readable and aesthetic presentation, especially for numerical values like currency. The generated code's output for average price just shows the default precision.
- **Separators:** The ground truth includes print separators (`--- Author Book Statistics ---`, `--- Author with Most Books in Stock ---`) to improve the readability and organization of the output when multiple pieces of information are printed. This is a minor aesthetic point.

Overall, the generated code is excellent in its functionality and adherence to the task requirements. The minor points are mostly about presentation rather than correctness or functionality.",llama-3.3-8b-instruct,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
fro...",3,"The generated code attempts to address many aspects of the prompt but fails significantly in correctness, completeness, and quality.

**Correctness:**
*   **Data Preprocessing and Outlier Treatment:** The `IsolationForest` application is incorrect. It assigns `1` for inliers and `-1` for outliers. The line `df[df[:, -1] != -1, :]` attempts to filter a NumPy array using Pandas DataFrame indexing, which causes an error. Even if corrected, it performs filtering *after* imputation, which is not ideal, and does not apply the preprocessing to the DataFrame itself. The code then uses `df` again without linking it to `df_imputed` or the outlier treatment results effectively.
*   **Feature Engineering:** `PolynomialFeatures` is applied to `df` *before* the data is fully processed (imputation, outlier capping, scaling). This is incorrect. Also, it's applied on `df_imputed` but the result is not passed to `X`. The generated code then tries to use `X=df_poly` but previous `df_imputed` is not passed into the `df_poly` correctly.
*   **Data Splitting:** `X = df_poly` is used for features, but `df['potability']` (from the *original*, unimputed, un-outlier-treated DataFrame) is used for `y`. This creates a mismatch.
*   **Hyperparameter Tuning:** While `GridSearchCV` is used, the `scoring` metric `f1_macro` might not be the most appropriate primary metric for a binary classification task where `ROC AUC` is often preferred, as implied by the ""advanced"" nature of the task and is used in the ground truth. The prompt explicitly asks for `ROC AUC` in the evaluation section, not `f1_macro` for hyperparameter tuning.
*   **Ensemble Models:**
    *   `VotingClassifier` is instantiated but neither `voting='soft'` nor `weights` are specified, which are crucial for performance.
    *   `BlendingClassifier` is explicitly described in the prompt to be a custom implementation. The generated code instead tries to instantiate a non-existent `BlendingClassifier` class without providing any implementation. This is a critical omission.
*   **Evaluation:**
    *   The `skfold` implementation for cross-validation only evaluates the `stacking_model` and stores `accuracy_score` without other required metrics (ROC AUC, precision, recall, F1, Brier). It also doesn't perform this for all models, which the prompt asks for.
    *   The learning curve plotting is flawed; `X_train_sample, X_test_sample = X_train[:n], X_test` uses `X_test` directly, not a corresponding subset of `X_train`, which is not how learning curves are typically generated.
    *   The Shapley values are calculated directly on `stacking_model`, which is not typically a tree-based model (unless its meta-learner is). The best practice for tree models is to use `shap.TreeExplainer`. The prompt's example uses a tree model for SHAP, but the generated code's `stacking_model` could be any combination.
    *   LIME implementation attempts to pass `X_train` into `LimeTabularExplainer` directly, but `X_train` is a NumPy array, it needs feature names. `X.columns` would fail as `X` is a NumPy array.
    *   Statistical significance testing `ttest_rel(scores, scores)` compares a list of scores against itself, which will always result in a p-value of 1. It does not compare different models as required.
*   **Model Calibration:** The calibration plot section uses `scipy.stats.norm.cdf` on predicted probabilities, which is not how model calibration plots (reliability diagrams) are constructed. `sklearn.calibration.calibration_curve` is the correct function.

**Completeness:**
*   **Missing Imports:** Several necessary imports like `IterativeImputer`, `CalibratedClassifierCV`, `calibration_curve`, `brier_score_loss`, `roc_auc_score`, `roc_curve`, `warnings`, `clone`, `BaseEstimator`, `TransformerMixin` (for custom outlier capper) are missing, which are crucial for the complex feature engineering and evaluation steps requested.
*   **Preprocessing:** Lacks multiple imputation strategies (only `SimpleImputer` is used), custom outlier detection/treatment (instead of just identifying and filtering), and advanced feature engineering (the `PolynomialFeatures` application is flawed).
*   **Model Optimization:** Bayesian optimization is completely missing, and `GridSearchCV` is applied without `StratifiedKFold` for all models, causing `GridSearchCV` to use default K-Fold.
*   **Ensemble Methods:** Blending is not implemented.
*   **Evaluation:**
    *   Stratified K-fold cross-validation is only used for `GridSearchCV` and a single `stacking_model` evaluation, not systematically for all models as part of a comprehensive evaluation pipeline.
    *   Validation curves are entirely missing.
    *   Bootstrap sampling for confidence intervals is missing.
    *   Comprehensive metrics (precision, recall, ROC AUC, Brier Score) are largely missing from the evaluation loops.
*   **Model Interpretability:** While `shap` and `lime` are mentioned, their correct application and visualization, particularly for `shap.summary_plot` and `shap.dependence_plot`, are not fully realized or correctly implemented given the data preparation.
*   **Statistical Significance Testing:** The implementation is incorrect.
*   **Model Comparison Dashboard:** While there are plot attempts, they are disconnected from a unified dashboard structure, and key elements like prediction confidence intervals are missing. The plots presented are not comprehensive or well-structured as a ""dashboard"".
*   **Model Calibration:** The implementation is fundamentally incorrect.

**Code Quality:**
*   **Structure:** The code is a linear script with poor modularization. Functions or classes for preprocessing, model training, evaluation, and plotting would greatly improve readability and maintainability.
*   **Clarity/Readability:** Variable names are sometimes reused or implicitly assumed (e.g., `df` vs `df_imputed` vs `df_poly`).
*   **Error Handling:** Lacks robust error handling, especially for file loading (`water_potability.csv` without `try-except`).
*   **Consistency:** Mixes NumPy arrays with Pandas DataFrames without consistent conversion, leading to indexing errors (`df[df[:, -1] != -1, :]` and `X.columns` when X is an array).
*   **Comments:** Comments are sparse and do not explain complex logic or potential issues.

In summary, the generated code demonstrates a superficial understanding of the requirements. Many advanced techniques are merely imported or mentioned but are not correctly implemented or integrated into a coherent system. The fundamental errors in data preprocessing, evaluation, and ensemble methods render the solution ineffective for the stated task.",llama-3.3-8b-instruct,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.imputation import mice
from statsmodels.imputation import iterative
from sklearn.impute import KNNImputer
import pymc3 as pm
from scipy.stats import f_oneway
from scipy.stats import permutation
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.neighbors import LocalOutlierFactor
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_c...",2,"The generated code attempts to address the prompt but fails significantly in execution, completeness, and correctness across almost all sections.

**Critical Failures:**

1.  **Imports and Dependencies:** Many imports are missing (`sklearn.experimental`, `pygam`, `pymc3.Sampling`, `arviz`, `statsmodels.multivariate.manova`, `statsmodels.robust.robust_linear_model`, `statsmodels.genmod.generalized_linear_model`, `statsmodels.genmod.families`, specific functions from `sklearn.metrics`, `sklearn.model_selection`). This leads to immediate `AttributeError`s and `ModuleNotFoundError`s. Even basic `statsmodels` functions like `mice.MICE` were deprecated and required an `import statsmodels.imputation.mice as sm_mice` and then using `sm_mice.MICE`, which is indicative of a lack of testing or understanding of library versions.
2.  **Missing Data Imputation:**
    *   The `imputation_comparison` function attempts to use `statsmodels.imputation.mice` and `statsmodels.imputation.iterative`. These modules/classes are either incorrect (`iterative` is not a class or function directly available under `statsmodels.imputation` in the way it's used), or they are used incorrectly (e.g., `imputed_mice = mice_model.fit()` returns a `MICEData` object, not an imputed DataFrame).
    *   There's no comparison of *effectiveness* of the imputation techniques, which was a specific requirement. The function just returns the fit objects.
    *   The function doesn't return DataFrames suitable for direct use in subsequent steps.
3.  **Multivariate Statistical Models:**
    *   **Logistic Regression:** `sm.Logit` is used correctly for its basic form, but the `regularization='l1'` argument is incorrect for `sm.Logit.fit()`. `statsmodels` GLM and Logit models don't directly handle regularization through a simple keyword argument like scikit-learn. Regularization needs to be implemented manually or via specific `L1` or `L2` classes if available, or specialized solvers.
    *   **GAM:** `sm.GLM(y, X, family=sm.families.Binomial())` is a Generalized *Linear* Model, not a Generalized *Additive* Model (GAM). The prompt explicitly asked for GAM, which typically implies non-linear smoothing terms (e.g., using `pygam`).
    *   **Bayesian Logistic Regression (PyMC3):** The PyMC3 model definition is a basic structure for a Bayesian logistic regression, but it's entirely incomplete. It lacks:
        *   Context management (`with pm.Model() as model:` is missing).
        *   Definition of the `X` and `y` tensors within the PyMC3 model for observed data.
        *   Sampling (`pm.sample`).
        *   Any form of prediction or evaluation of the Bayesian model.
        *   The prompt asked for `PyMC3` which is the correct library, but the implementation is only a skeleton.
4.  **Advanced Hypothesis Testing:**
    *   **MANOVA:** The code uses `f_oneway` which is for one-way ANOVA (comparing means across groups for a *single* dependent variable). The prompt explicitly asked for *Multivariate* ANOVA (MANOVA), which tests multiple dependent variables simultaneously.
    *   **Permutation Tests:** The function `permutation-stattest` is not a standard Python function or available in common libraries like `scipy.stats` for general permutation tests. This section would lead to a `SyntaxError` and `NameError`.
    *   **Bootstrap Confidence Intervals:** Completely missing this requirement.
5.  **Anomaly Detection:**
    *   The functions just return the fit objects (`iforest_fit`, `lof_fit`, `oc_svm_fit`), not the actual anomaly predictions or scores, nor is there any analysis or display of results (e.g., number of detected anomalies, score distribution).
6.  **Dimensionality Reduction:**
    *   `sm.PCA()` is not a valid constructor for PCA in `statsmodels`. `statsmodels.multivariate.pca.PCA` exists but is used differently, and `sklearn.decomposition.PCA` is more common.
    *   `sm.factor_factorize(X)` is not a valid function for Factor Analysis in `statsmodels`. `sklearn.decomposition.FactorAnalysis` is the expected implementation.
7.  **Robust Statistical Methods:**
    *   `sm.RobustLinearRegression()` is not a standard class in `statsmodels`. The correct class is `statsmodels.robust.robust_linear_model.RLM`. It's also applied to `y` and `X` where `y` is 'Potability' (binary), but RobustLinearRegression is for continuous outcomes, not classification. This part is fundamentally incorrect for the given task.
8.  **Model Diagnostics:**
    *   `sm.graphics.gqplot(y, X)` is for Generalized Quantile Plots, not standard residual plots.
    *   `sm.graphics.influence_plot(y, X)` is used with `y` and `X` directly, but it expects a fitted model object, not raw data.
    *   `sm.GLM(y, X, family=sm.families.Binomial()).fit().ll` calculates the log-likelihood (part of goodness-of-fit) but doesn't produce other diagnostic elements like residual analysis plots, influence measures, or comprehensive goodness-of-fit tests beyond the log-likelihood.
    *   **Missing key diagnostic visualizations:** Q-Q plots, leverage plots are mentioned in requirements but not implemented correctly.
9.  **Advanced Statistical Visualizations:**
    *   `sm.graphics.qqplot(y)` and `sm.graphics.leverage_plot(y, X)` again require model results, not raw arrays/DataFrames.
    *   `sm.graphics.summary2(y, X)` is used to display a model summary, not a ""diagnostic dashboard"" visualization.
    *   The code only calls `plt.show()` once at the very end, implying figures are not displayed as they are generated.
    *   Many specific visualizations requested (e.g., Q-Q plots of residuals, leverage plots with influence measures) are not correctly generated or are missing.
10. **Overall Structure and Readability:** While functions are used, the code is very condensed and lacks any form of output (print statements, plot displays with `plt.show()` after each plot) making it impossible to evaluate what's happening without manually inserting print statements and `plt.show()`. The functions often return objects that are not used or printed, reducing their utility. The use of global variables `df` inside functions is bad practice.

In summary, the code demonstrates a superficial understanding of the task requirements and specific library functionalities. Many core components are either syntactically incorrect, semantically misused, or completely missing. It's not executable as-is and would require extensive debugging and correction.",llama-3.3-8b-instruct,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, GaussianMixtureModel, SpectralClustering
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, gap_statistic
from umap import UMAP
from scipy.stats import norm
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objs as go
from plotly.subplots import make_subplots
from sk...",3,"The generated code attempts to address many parts of the prompt but fails significantly in execution, demonstrating a lack of understanding of several critical concepts and library usage.

**Correctness:**
- **Feature Engineering:** The engineered features are syntactically incorrect (`df['alkalinity')`, `df['calCIum']`, `df['magnesium']`, `faecal_coliform`, `total_coliform` columns do not exist in the dataset) or overly simplistic/nonsensical for real water quality analysis. The ground truth, while simplified, at least uses existing columns and more plausible calculations.
- **Clustering Algorithms:** `GaussianMixtureModel` is incorrect; it should be `GaussianMixture`. Calls to `fit()` on clustering algorithms are done within the loop for optimal K determination, but `kmeans.labels_` etc. are accessed immediately after, before `predict()` is called, which isn't always the correct way to get labels after `fit()`.
- **Optimal Number of Clusters:**
    - `gap_statistic` is called as if it's a scikit-learn metric, but it's not a standard function; it needs a custom implementation or a third-party library.
    - `kmeans.fit(df)` is called repeatedly inside the loop for all algorithms, which is inefficient and suggests a misunderstanding of how to use these algorithms for comparison.
    - `df` is used directly for clustering and evaluation, but it's not scaled, which is crucial for distance-based algorithms like KMeans, Hierarchical, and DBSCAN.
- **Dimensionality Reduction:** `UMAP` is imported correctly, but `t_sne = UMAP(...)` is assigned UMAP, not t-SNE. The `t_sne` module is actually `sklearn.manifold.TSNE`. The code attempts `t_sne.fit_transform(df)` directly on the unscaled `df`, which is incorrect. `FactorAnalysis` is used, but without proper scaling or consideration of its assumptions.
- **Cluster Stability Analysis:** The bootstrap resampling generates `cluster_labels` but doesn't actually perform any stability analysis (e.g., comparing ARI scores across bootstraps or generating a consensus matrix). It just stores labels from each bootstrap.
- **Visualization:** Plotly `iplot` is called without importing `plotly.offline`. The `make_subplots` attempts to plot non-existent columns (`chemical_composition`). Matplotlib plots are hardcoded for `kmeans.labels_`, `hclust.labels_`, `dbscan.labels_` based on arbitrary cluster count (5) and don't reflect the optimal K.
- **Validation:** `df['potability']` is assumed to exist in the `val_results` loop, but the task states it might be absent or should be handled if present.
- **Anomaly Detection:** The formula for `anomaly_score` (`np.sum(np.abs(sample - df.mean(axis=0)) > 2 * df.std(axis=0))`) is a very simplistic and often ineffective method for anomaly detection. It's not cluster-based as requested, but rather a global statistic.

**Code Quality:**
- **Imports:** Missing several crucial imports present in the ground truth (e.g., `SimpleImputer`, `MinMaxScaler`, `TSNE`, `NearestNeighbors`, `IsolationForest`, `cdist`, `dendrogram`, `linkage`). Includes `train_test_split` which is not used. `GaussianMixtureModel` is wrong. `gap_statistic` is missing.
- **Structure:** The code is a linear script, which is acceptable, but the logical flow is flawed due to the correctness issues mentioned above.
- **Hardcoding:** Arbitrarily sets `n_clusters=5` for initial clustering and validation, ignoring the ""optimal number of clusters"" section.
- **Readability/Maintainability:** Uses non-existent column names (`alkalinity`, `calCIum`, `magnesium`, `faecal_coliform`, `total_coliform`, `chemical_composition`) throughout, which makes the code unexecutable and clearly indicative of not understanding the specific dataset.

**Completeness:**
- **Advanced Feature Engineering:** Lacks most of the specified features (water quality indices, only `water_quality_index` is attempted; chemical balance ratios, one attempted but using wrong columns; contamination risk scores, one attempted but using wrong columns; temporal patterns not addressed at all, even with a note).
- **Multiple Clustering Algorithms:** Attempts to use the correct algorithms, but their application is flawed.
- **Optimal Number of Clusters:** Attempts to use multiple methods but `gap_statistic` is not correctly implemented. Visualizations for optimal K are not generated.
- **Dimensionality Reduction:** Attempts PCA, t-SNE (incorrectly using UMAP), Factor Analysis, but misses UMAP proper and 3D components.
- **Cluster Stability Analysis:** Only performs basic bootstrapping, no actual stability *analysis* (e.g., consensus clustering, ARI comparison).
- **Water Quality Profiles:** Not explicitly generated or analyzed. The validation section just computes mean potability.
- **Visualization Dashboard:** Generates some plots but they are incomplete, incorrectly mapped, and lack the specified radar charts, 3D plots (though 3D DR is mentioned, the plots are 2D). Interactive visualizations are limited to basic Plotly scatters.
- **External Validation/Domain Knowledge:** Not implemented.
- **Anomaly Detection:** Rudimentary anomaly detection used that is not cluster-based.

In summary, the code demonstrates a superficial understanding of the task and a lack of practical knowledge regarding the libraries and data science methodologies needed. Many critical components are either missing, incorrect, or conceptually flawed.",llama-3.3-8b-instruct,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
import numpy as np

# Load data
data = pd.read_csv('winequality-red.csv')

# Determine correlations
corr = data.corrwith(data['quality']).sort_values(ascending=False)

# List top 3 positively and negatively correlated features
print(""Top 3 positively correlated features:"")
print(corr.head(3))
print(""\nTop 3 negatively correlated features:"")
print(corr.tail(3))

# ...",7,"The generated code addresses most of the requirements but has some shortcomings in terms of correctness and completeness.

**Strengths:**
*   Correctly loads the dataset.
*   Successfully determines correlations with 'quality'.
*   Identifies and prints the top 3 positively and negatively correlated features.
*   Generates a correlation matrix heatmap.
*   Creates scatter plots for the most positive and negative features with regression lines.
*   Uses appropriate libraries (pandas, matplotlib, seaborn, sklearn).

**Areas for Improvement/Weaknesses:**
*   **Correlation Calculation for Top/Bottom Features:** The `corr = data.corrwith(data['quality']).sort_values(ascending=False)` line correctly calculates correlations with 'quality'. However, when taking `corr.head(3)` and `corr.tail(3)`, it implicitly includes 'quality' correlating with itself (which is 1.0) in the positively correlated list. The task implicitly asks for other features, not 'quality' itself. The ground truth explicitly drops `TARGET_FEATURE` from `quality_correlations` before determining top/bottom features, which is a more robust approach.
*   **Scatter Plot Titles:** The titles for the scatter plots `f'Scatter plot for {most_pos_corr_feature} and quality'` are a bit generic. Adding 'quality' to the title would be clearer (e.g., `f'Scatter plot: {most_pos_corr_feature} vs. quality'`).
*   **Scatter Plot Aesthetics/Labels:** The ground truth includes more complete plot details like `xlabel`, `ylabel`, `grid`, and `legend`, making the plots more readable and informative. The generated code's scatter plots lack these.
*   **Missing Information/Prints:** The ground truth includes print statements for loading success, shape, head, info, and descriptive statistics, which are good practice for data exploration and verification. It also shows correlation coefficients. The generated code is more concise but misses some of these helpful outputs.
*   **Error Handling:** The ground truth includes a `try-except` block for file loading, which is crucial for robust code. The generated code lacks this.
*   **Use of `scipy.stats.linregress` vs. `sklearn.linear_model.LinearRegression`:** While `sklearn` works, `scipy.stats.linregress` directly provides R-squared, p-value, etc., which are often useful for analyzing linear relationships, and is more direct for a simple linear regression task. The `sklearn` approach requires manual calculation if R-squared or other stats are needed, or further inspection of the model object. The prompt allowed `scipy/sklearn` so this isn't a critical error, but `scipy` is often preferred for simple 1D linear regression when statistical output is desired.
*   **Plot Legends:** The regression lines on the scatter plots do not have a legend, making it less clear what the red line represents without context. The ground truth adds a legend with the R-squared value.

To enhance the score, the generated code should refine the correlation listing to exclude 'quality' itself, improve plot aesthetics and information, and add error handling.",llama-3.3-8b-instruct,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, ttest_rel, mannwhitneyu
from scipy_chi2 = from scipy.stats import chi2_contingency

# Load dataset and clean missing values
def load_dataset():
    dataset = pd.read_csv('video_games_sales.csv')
    dataset = dataset.dropna()
    return dataset

# Test 1: Nintendo games vs all other publishers
def test_nintendo_sales(dataset):
    nintendo_games = dataset[dataset['Publisher'] == 'Nintendo']
    other_publishers =...",4,"The generated code attempts to address the task but suffers from several critical issues that make it largely incorrect or incomplete for the given requirements.

**Correctness Issues:**
1.  **Missing `Year_of_Release` cleaning/conversion:** The `test_release_year` function tries to use `dataset['Year']` which is incorrect. The column is `Year_of_Release`. Even if it were `Year`, the `dropna()` in `load_dataset` is too broad and doesn't explicitly convert `Year_of_Release` to integer type, which is crucial as it's often imported as float or object. This will likely cause errors or incorrect filtering for the Mann-Whitney U test.
2.  **Incorrect Chi-square import:** `from scipy_chi2 = from scipy.stats import chi2_contingency` is a syntax error and will cause the script to fail immediately. It should simply be `from scipy.stats import chi2_contingency`.
3.  **Chi-square filter missing:** The prompt specifically asks to focus ""on top 5 genres and top 5 platforms by count"" for the chi-square test. The generated code calculates `top_5_genres` and `top_5_platforms` but *never uses them* to filter the `genre_platform` contingency table. This leads to testing the association across *all* genres and platforms, not just the top 5, violating a specific instruction.
4.  **No `alternative` hypothesis for t-test & Mann-Whitney U:**
    *   For the Nintendo test, the requirement is ""significantly *higher* global sales,"" implying a one-tailed test. The `ttest_ind` call lacks the `alternative='greater'` parameter.
    *   For the Mann-Whitney U test, the requirement is ""significantly *different* sales,"" implying a two-tailed test. While `mannwhitneyu` defaults to `two-sided` for versions 1.7.0+, it's good practice to explicitly state it or at least be aware of the default behavior across scipy versions. Even if it defaults to two-sided, the core issue with ""Year"" column and the missing cleaning is a major problem.
5.  **Overly aggressive `dropna()`:** `dataset.dropna()` will drop *any* row with *any* missing value. While this partially addresses cleaning, it doesn't align with the ground truth's specific cleaning strategy which focuses on critical columns (`Global_Sales`, `Year_of_Release`, `Publisher`, `Genre`, `Platform`) and converts `Year_of_Release` to int after specific NA handling. This could lead to unnecessary data loss or failures if certain columns (like `User_Score` which often has 'tbd') are present and not explicitly handled.
6.  **No interpretation of results with significance level:** The output only prints ""Reject H0"" or ""Fail to reject H0"" without explicitly stating the p-value against the alpha (e.g., `p < 0.05` or `p >= 0.05`) as the ground truth does. The interpretation is very basic and lacks the detailed null/alternative hypothesis statements and clear conclusion the ground truth provides.

**Code Quality Issues:**
1.  **Syntax Error:** `from scipy_chi2 = from scipy.stats import chi2_contingency` is a severe syntax error that prevents execution.
2.  **Redundant `tolist()`:** Converting pandas Series to lists (`.tolist()`) before passing them to `ttest_ind` or `mannwhitneyu` is unnecessary and slightly less efficient. Pandas Series can be directly passed.
3.  **Lack of error handling:** No checks are performed before statistical tests (e.g., checking if groups have enough samples for t-test, or if the contingency table is valid for chi-square), which can lead to runtime errors or warnings with small/empty datasets.
4.  **No explicit `ALPHA` variable:** The significance level `0.05` is hardcoded multiple times instead of defined once as `ALPHA`.

**Completeness Issues:**
1.  **Missing specific cleaning for `Year_of_Release`:** As mentioned, this is a significant omission.
2.  **Incomplete interpretation:** The interpretation lacks the detailed statements of null/alternative hypotheses and the precise comparison with the alpha value, which are present in the ground truth and part of professional reporting for hypothesis tests.
3.  **No specific handling for `User_Score`:** While not directly tested, the ground truth demonstrates good practice by handling it. The `dropna()` might inadvertently remove rows based on this column if it has NaNs not handled by other tests.

Overall, the generated code has fundamental errors (syntax, incorrect column name), misses key instructions (chi-square filtering, one-tailed test), and has several quality deficiencies.",llama-3.3-8b-instruct,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Load dataset and handle missing values
df = pd.read_csv('video_games_sales.csv')
df.dropna(inplace=True)

# Create binary target variable 'High_Sales'
df['High_Sales'] = np.where(df['Global_Sale...",7,"The generated code correctly addresses most of the requirements and successfully builds and evaluates a classification model.

**Strengths:**
- Correctly loads data.
- Creates the 'High_Sales' target variable as requested.
- Applies Label Encoding to the specified categorical features.
- Creates the 'Total_Regional_Sales' and 'Years_Since_Release' features.
- Splits data into training and testing sets.
- Trains a Random Forest Classifier with 100 estimators.
- Evaluates the model using accuracy, precision, recall, F1-score, and confusion matrix.
- Displays feature importance.
- Includes all necessary imports.

**Areas for Improvement / Specific Feedback:**

1.  **Missing Value Handling:**
    *   **Ground Truth:** `df.dropna(subset=['Global_Sales'], inplace=True)` and then `df.dropna(subset=['Year', 'Publisher', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales'], inplace=True)`. This is a more robust way to handle NaNs, especially for `Global_Sales` which is critical for the target. It also handles potential `NaN` values in `Year` before converting to `int`.
    *   **Generated:** `df.dropna(inplace=True)` drops *all* rows with *any* missing values. While this might suffice for this specific dataset if NaNs are few and scattered, it's generally not the best or most robust strategy. For instance, if 'Name' had missing values, those rows would be dropped unnecessarily when they aren't used for training. More targeted dropping (or imputation) is usually preferred. The `Year` column missing values might not be handled correctly before `astype(int)` if `dropna()` alone isn't enough, which could lead to errors.

2.  **Feature Selection for X:**
    *   **Ground Truth:** Explicitly defines `features = [...]` which includes specific columns like `Platform`, `Genre`, `Publisher`, `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`, `Total_Regional_Sales`, `Years_Since_Release`. This is precise and clear.
    *   **Generated:** `X = df.drop(['High_Sales', 'Global_Sales'], axis=1)`. This approach implicitly selects all other columns. This can be problematic if the dataset contains columns that are not suitable for training (e.g., 'Name', 'Rank', or even 'Year' if `Years_Since_Release` is the desired feature, leading to multicollinearity). While it happens to work reasonably well for *this specific dataset* because most other columns are suitable, explicit feature selection (`features = [...]`) is generally better practice for clarity, control, and robustness against unexpected columns in real-world datasets. The 'Year' column was not dropped from `X` but 'Years_Since_Release' was created, which might lead to multicollinearity if 'Year' is included in `X`. The ground truth excludes 'Year' explicitly.

3.  **Readability/Comments:** The ground truth includes more comprehensive comments and print statements which aid in understanding the data processing steps, which is good for expert code evaluation but not explicitly asked for by the prompt. However, the generated code satisfies the ""only Python code without explanations"" requirement.

Overall, the generated code performs the requested tasks and yields a functional solution, but the handling of missing values and feature selection could be more robust and explicit for a production-level solution.",llama-3.3-8b-instruct,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
import seaborn as sns

# Load dataset and handle missing values
df = pd.read_csv('video_games_sales.csv')
df.fillna(df.mean(), inplace=True)

# Create engineered features
df['Sales_Ratio_NA_EU'] = df['NA_Sales'] / df['EU_Sales']
df['Publ...",3,"The generated code attempts to address several parts of the task but fails significantly in correctness and completeness, making it largely unsuitable for a production environment or thorough data analysis.

**Major issues:**

1.  **Missing Value Handling (Incorrect):** `df.fillna(df.mean(), inplace=True)` will only fill numerical columns. Categorical columns ('Publisher', 'Genre', 'Name') are not handled at all, which will lead to errors or incorrect results in subsequent steps (e.g., `groupby` for Publisher/Genre, `get_dummies`). The ground truth uses `SimpleImputer` and correctly handles both numerical and categorical missing values with appropriate strategies (median for 'Year', mode for 'Publisher'/'Genre').
2.  **Feature Engineering - Sales_Ratio_NA_EU (Potential Division by Zero):** `df['Sales_Ratio_NA_EU'] = df['NA_Sales'] / df['EU_Sales']` does not handle potential division by zero if `EU_Sales` is 0. The ground truth adds a small epsilon to prevent this.
3.  **Feature Engineering - Platform_Popularity (Incorrect):** `df['Platform_Popularity'] = df.groupby('Platform')['Game']._get_count()` is syntactically incorrect (`'Game'` column likely doesn't exist, and `_get_count()` is not a standard pandas method for this purpose). It should be `transform('count')` on an existing column like 'Name'.
4.  **Feature Engineering - Sales_Momentum (Incorrect & Incomplete):** `df['Sales_Momentum'] = df['Global_Sales'] - df['Global_Sales'].median()` calculates the overall median, not the median per year as specified in the task (`median sales for that year`).
5.  **PCA (Incorrect Application):** `pca_df = pca.fit_transform(df.select_dtypes(include='number'))` applies PCA directly to all numerical columns without proper scaling. PCA is highly sensitive to the scale of features, and `StandardScaler` should be used first (as in the ground truth).
6.  **Polynomial Features (Incorrect Application):** The `poly_pipe` is created but the resulting `poly_df` is not integrated back into the original DataFrame, nor are the original sales columns explicitly targeted. The task specifically asks to apply polynomial features to ""sales columns"".
7.  **Interaction Features (Completely Incorrect):**
    *   `df['Year_Genre'] = df['Year'] * df['Genre']` is fundamentally invalid. You cannot directly multiply a numerical 'Year' column with a categorical 'Genre' column. This would result in an error or `NaN`s depending on the data types.
    *   `df['Year_Genre_Cat'] = pd.cut(df['Year_Genre'], bins=5)` follows directly from the incorrect `Year_Genre` and further obfuscates the intent.
    *   The task asks for interaction features *between* Year and Genre, implying dummy variables for Genre multiplied by Year, similar to how the ground truth implemented it.
8.  **Outlier Handling (Incomplete & Flawed):**
    *   `Q1 = dfquantile(0.25)` and `Q3 = df_quantile(0.75)` are syntax errors (missing `.` before `quantile`).
    *   `outliers = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))` attempts to calculate outliers for *all* columns in the DataFrame simultaneously, which is unlikely to be the intended behavior, especially for categorical or ID columns. The task asks to identify and handle outliers *using IQR method*, typically applied to specific numerical columns, often by capping or removal. The generated code attempts to remove rows containing *any* outlier, which is very aggressive and can lead to significant data loss without proper consideration. The ground truth explicitly targets specific numerical columns and caps the outliers, which is a more robust approach than simply dropping rows.
9.  **Imports and Structure:** While imports are present, `Pipeline` and `train_test_split` are imported but not used, indicating a lack of focus on the core requirements. The code lacks clear separation and print statements to show progress or intermediate results, making it harder to debug, unlike the ground truth.
10. **Completeness:** Several aspects are completely missed or poorly implemented:
    *   `Publisher` column is not handled for missing values before `groupby` or `transform`.
    *   No proper visualization for PCA (cumulative explained variance plot).
    *   No explicit analysis of the impact of polynomial features.
    *   The interaction features are completely wrong.
    *   The outlier handling method is incorrect in its application and effect.",llama-3.3-8b-instruct,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, mannwhitneyu, chi2_contingency, pearsonr
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_order

# Load dataset
dataset = pd.read_csv('diabetes.csv')

# Clean zero values
dataset = dataset.replace(0, np.nan)
dataset = dataset.fillna(dataset.median())

# Test glucose levels between diabetic and non-diabetic patients
glucose_diabetic = dataset[dataset['Outcome'] == 1]['Glucose']
glu...",6,"The generated code addresses most of the statistical tests required by the task. However, there are several issues that reduce its score:

1.  **Imputation:** The median imputation is applied globally to all columns (`dataset.fillna(dataset.median())`). The task specifically states that ""Pregnancies and Outcome can legitimately be 0"" and only *certain* columns (`Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI'`) should have 0s replaced with NaN before median imputation. The generated code replaces *all* 0s in the entire DataFrame with NaN initially, which means `Pregnancies` and `Outcome` columns might incorrectly have 0s replaced with NaNs and then imputed by their medians. This is a significant deviation from the data cleaning instructions.
2.  **Missing Interpretation/P-value Display:** The prompt explicitly asks to ""Display p-values and interpret results with significance level α=0.05"". The generated code only prints the p-values without any interpretation (e.g., comparing to alpha, stating rejection/failure to reject null hypothesis, or providing a conclusion). This is a critical omission for a ""hypothesis testing and statistical analysis"" task.
3.  **ANOVA Implementation:** The ANOVA implementation is incorrect.
    *   It uses `ols('Glucose ~ C(BMI)', data=dataset).fit()` which attempts to use `BMI` as a categorical variable directly. `BMI` is a continuous variable, not a categorical one for the purpose of the ANOVA requested. The task specifically asks to test ""if glucose levels differ significantly across **BMI categories** (underweight, normal, overweight, obese)"". The generated code creates `bmi_categories` but then doesn't use them in the `ols` formula.
    *   The term `C(BMI)` in `ols` typically implies treating `BMI` itself as a categorical variable, which is inappropriate for continuous `BMI` values. Even if it were to implicitly use `bmi_categories`, the `ols` formula would need to be `Glucose ~ C(bmi_categories)`.
    *   The `anova_order` function from `statsmodels.stats.anova` is deprecated and `anova_lm` should be used instead. Even with `anova_lm`, the formula would need to specify the categorical BMI column.
    *   The output of `anova_order` (or `anova_lm`) is a full ANOVA table, but the task only requests the p-value and interpretation. The ground truth code correctly groups the data by the new categories and performs `scipy.stats.f_oneway` which is aligned with the prompt's simplicity preference for displaying p-values.
4.  **Age Group Bins:** The age group bins are `[0, 30, 50, 100]` which creates `(0, 30]`, `(30, 50]`, `(50, 100]`. The task specifies `young: <30, middle: 30-50, older: >50`. This implies `[0, 29]`, `[30, 50]`, `[51, inf]`. While functionally close, it's not exact and the ground truth handles the binning more precisely.
5.  **Imports:** While all necessary functions are imported, `ols` and `anova_order` are not the most direct or simplest way to perform the requested ANOVA, and `anova_order` is deprecated. Using `scipy.stats.f_oneway` is simpler and more aligned with the other basic statistical tests.

To improve, the code needs to:
*   Correctly implement the data cleaning strategy for zero values.
*   Add the requested interpretations for each p-value and compare them against `alpha=0.05`.
*   Fix the ANOVA implementation to correctly use the defined BMI categories and produce a p-value for interpretation, ideally using `scipy.stats.f_oneway` for consistency and directness.
*   Ensure age group binning exactly matches the prompt.",llama-3.3-8b-instruct,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, rec...",4,"The generated code has several significant issues that prevent it from correctly and completely addressing the task requirements.

**Correctness:**
*   **Missing Values Imputation:** The line `df = imputer.fit_transform(df)` replaces the entire DataFrame with a NumPy array, losing column names. Immediately after, `df = pd.DataFrame(df, columns=df.columns)` attempts to recreate the DataFrame, but `df.columns` will be lost at this point, leading to an AttributeError or incorrect column assignments if it temporarily works. The ground truth correctly iterates and imputes column by column, preserving structure. More critically, the task specified replacing zeros in *specific columns* with the median, which the generated code does not correctly implement (it treats all NaNs introduced by the imputer, not specific zeros).
*   **Feature Engineering - Age_group:** The `Age_group` bins specified (`[20, 30, 40, 50, 60]`) are insufficient and will miss values below 20 and above 60. The ground truth provides a more robust binning.
*   **Preprocessing Pipeline:** The generated code attempts to use `ColumnTransformer` *outside* of the `Pipeline` for the initial training `df.select_dtypes(...)`, but then includes `preprocessor` within a `Pipeline` for subsequent model training and evaluation. This leads to double preprocessing or incorrect transformations. The `preprocessor` should be defined once and then incorporated into a `Pipeline` with the model, processing raw `X_train` and `X_test`. In the generated code, `X` is already transformed by `preprocessor.fit_transform(df.drop('Outcome', axis=1))`, so when `Pipeline([('preprocessor', preprocessor), ('model', model)])` is used with `X_train`, it attempts to re-transform already transformed data, which is incorrect.
*   **SVC probability:** The SVC model needs `probability=True` to compute `predict_proba` for ROC-AUC, which is missing.
*   **Feature Importance Display:** The code attempts to print `model.feature_importances_` directly, but `model` here refers to the raw sklearn model *before* fitting within the pipeline. It doesn't correctly retrieve feature importances from the *fitted* model within the pipeline nor does it map them to feature names (especially after OneHotEncoding), which is a critical part of the task. The visualization is also missing.
*   **Confusion Matrix Visualization:** The generated code uses `plt.imshow` but does not include proper labels, ticks, or a color bar, making the confusion matrix uninterpretable. The ground truth uses `seaborn.heatmap` which is much better for this.
*   **`average='weighted'` for precision/recall/f1:** While not strictly incorrect, for binary classification, `average='binary'` (or default as it's typically for the positive class) is usually preferred unless there's a good reason to weight by class support. The task implies standard metrics for a binary outcome.
*   **Hyperparameter Tuning Target:** The task states ""hyperparameter tuning for the best model,"" but the generated code hardcodes `best_model = models['Gradient Boosting']` without any empirical justification from previous model performance.
*   **Imports:** Several necessary imports like `OneHotEncoder`, `seaborn`, `ConfusionMatrixDisplay`, `RocCurveDisplay` (or their direct plotting equivalents) are missing or misused.

**Code Quality:**
*   **Redundant/Incorrect Preprocessing:** As mentioned, the preprocessing logic is muddled due to applying `fit_transform` directly to `df.drop('Outcome', axis=1)` and then attempting to use the same `preprocessor` within a pipeline.
*   **Non-idiomatic Pandas:** Imputation using `df = imputer.fit_transform(df)` and then trying to recreate `df` is not good practice. Pandas' `fillna` is more robust after selecting specific columns.
*   **Plotting:** Lack of proper plot titles, labels, or clear separation between plots makes the output difficult to follow.

**Completeness:**
*   **Preprocessing Zeroes:** The instruction to replace zeros with median in *specific* columns (`Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`) is not correctly implemented. The `SimpleImputer` used globally might fill zeros (interpreted as NaNs) but it's not specific to the identified problem columns and does not handle the `replace(0, np.nan)` step prior to imputation.
*   **Hyperparameter Tuning Model Selection:** The task says ""for the best model,"" which implies selecting it based on initial performance, but the code arbitrarily picks Gradient Boosting.
*   **Feature Importance (Visualization & Details):** While the code attempts to print feature importances, it doesn't correctly handle the case (especially due to pipeline usage and OneHotEncoding) and completely omits the visualization.
*   **Comprehensive Evaluation Reporting:** The output provides basic scores but lacks the structured reporting (e.g., classification reports) that the ground truth provides, especially for the final, best model.
*   **Warning Suppression/Reproducibility:** Missing `warnings.filterwarnings` and `np.random.seed`.",llama-3.3-8b-instruct,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.correlation import correlation_matrix
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('diabetes.csv')

# Handle missing values
df.fillna(df.mean(), inplace=True)...",3,"The generated code attempts to address the task but falls short on multiple critical aspects, leading to significant correctness and completeness issues.

**Correctness Issues:**
*   **Missing Value Handling:** The ground truth meticulously identifies specific columns where '0' represents missing values and imputes them with the median. The generated code uses `df.fillna(df.mean(), inplace=True)`, which is a generic approach that might incorrectly impute 0s that are legitimate values (like 'Pregnancies') and doesn't handle the '0' as NaN requirement.
*   **Engineered Features:**
    *   `Risk_Factor_Count`: The ground truth carefully defines thresholds for 'high-risk' based on domain knowledge. The generated code simply adds `Age`, `BMI`, `Systolic_Blood_Pressure`, and `Diastolic_Blood_Pressure`. The latter two columns do not exist in the dataset, leading to immediate `KeyError`. Even if they did, a simple sum doesn't represent a ""count of high-risk factors.""
    *   `BMI_BloodPressure_interaction`: Similar to the above, it references `Systolic_Blood_Pressure` which doesn't exist.
    *   `Pedigree_Age_product`: It references `Pedigree_Function` instead of `DiabetesPedigreeFunction`, leading to `KeyError`.
*   **Polynomial Features:** The generated code applies polynomial features without considering the original data and often creates new features on top of existing ones, which is not what the question intended (applying to selected numerical columns). The ground truth correctly drops original columns to merge the new polynomial features. The generated code selects *all* numerical columns, including the target ('Outcome').
*   **Feature Scaling:** The generated code scales `df` directly, which includes the 'Outcome' target column, which should never be scaled. Also, it performs both StandardScaler and MinMaxScaler but only uses `df_std` for PCA and `df` (unscaled) for feature selection and SMOTE, making the scaling largely ineffective or misused. The ground truth correctly scales only the features (X).
*   **Correlation-based Feature Filtering:** The generated code uses `correlation_matrix` from `sklearn.correlation`, which is not a standard scikit-learn module (it's likely a typo for `corr()` method of pandas DataFrame). Even if it were, the logic `if not any(abs(corr_df.iloc[:, i]) > threshold)` would select columns where *no* value in the column's correlations (including self-correlation which is 1) is above the threshold, which is incorrect. The ground truth correctly isolates the upper triangle to avoid redundant checks and self-correlation.
*   **Feature Selection (SelectKBest and RFE):** The generated code applies these directly on `df` (which includes the target and unscaled data), causing issues. The ground truth correctly separates X and y and applies selection on scaled feature data.
*   **SMOTE:** The generated code applies SMOTE on the *entire* `df` (including target and unscaled data), which is incorrect. SMOTE should only be applied to the training features (X_train) and target (y_train), *after* splitting the data. The ground truth correctly performs this.
*   **Feature Importance Visualization:** The generated code plots `feature_names` (all columns from the original df) against `feature_importance` (from the scaled/SMOTEd data), which may not align due to feature engineering, scaling, and potential feature reduction implied by other steps. The ground truth correctly extracts feature names from the SMOTE-transformed X data.

**Completeness Issues:**
*   The generated code does not include any dataset splitting (`train_test_split`), which is crucial for a realistic machine learning pipeline, especially for SMOTE and feature importance evaluation.
*   The explanation of PCA variance ratios is very brief.
*   The task explicitly asks for an explanation of PCA variance ratios, which is minimally addressed.
*   No imports for `SimpleImputer`, `matplotlib.pyplot`, `seaborn`, `train_test_split`, `LogisticRegression`, `Counter`, `warnings`.

**Code Quality Issues:**
*   Lack of comments and print statements to trace execution, making it hard to follow.
*   Several variables are unused or overwritten without clear purpose (e.g., `df_mm`).
*   Incorrect library import (`sklearn.correlation`).
*   The code would fail to run due to `KeyError` stemming from incorrect column names for engineered features and the missing `sklearn.correlation`.

Overall, the generated code contains numerous foundational errors in data preprocessing steps, feature engineering logic, and the application of machine learning techniques, making it largely non-functional and unreliable for the given task.",llama-3.3-8b-instruct,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, mannwhitneyu, f_oneway, pearsonr, chi2_contingency
import statsmodels.api as sm

# Load dataset and handle missing values
def load_dataset():
    dataset = pd.read_csv('world-happiness-report-2021.csv')
    dataset.dropna(inplace=True)
    return dataset

# Test 1: Western European countries vs all other regions
def test_1_western_europe_vs_all(dataset):
    western_europe = dataset[dataset['Region'] == 'Western ...",2,"The generated code has significant issues that prevent it from correctly solving the given task.

**Critical Failures:**

*   **Incorrect Column Names:** The most glaring issue is the use of incorrect column names throughout the code. The prompt specifies `world-happiness-report-2021.csv`, which has columns like `Ladder score`, `Regional indicator`, `Logged GDP per capita`, `Social support`, `Freedom to make life choices`, and `Generosity`. The generated code assumes `Region`, `Happiness Rank`, `Happiness Score`, and `GDP per capita`, which are not present in the specified dataset. This makes almost every statistical test in the generated code fail or produce errors as it tries to access non-existent columns.
*   **Incomplete Data Preprocessing:** While it attempts `dropna()`, it doesn't specify `subset` for critical columns, meaning it might drop rows unnecessarily or not handle NaNs in relevant columns if they are not explicitly dropped. The ground truth wisely identifies critical columns to ensure all downstream tests work with complete data for the variables being analyzed.
*   **Incorrect Logic for Quantiles/Groups:**
    *   **Test 1 (Western Europe):** Uses `Happiness Rank` instead of `Ladder score` (happiness score) and `Region` instead of `Regional indicator`.
    *   **Test 2 (GDP per capita):** Incorrectly uses `nlargest(25, 'Happiness Rank')` and `nsmallest(25, 'Happiness Rank')`. The task asks for top/bottom 25% of happiness *scores*, not ranks, and explicitly for 25% *of the data*, not just 25 rows. This would only work if the dataset always had 100 rows, which is not the case. The ground truth correctly calculates `int(0.25 * n_countries)`.
    *   **Test 3 (ANOVA):** Again, uses `Region` and `Happiness Score` which are incorrect.
    *   **Test 5 (Chi-Square):** It expects a pre-existing `Happiness Category` column, but the task requires the code to *create* this category (High/Medium/Low based on quantiles). The generated code performs no such categorization.
*   **Missing Interpretations and Print Statements:** The task explicitly asks for an alpha level of 0.05 and `Display p-values and interpret results with significance level α=0.05`. The generated code only prints the p-value and a very generic interpretation without specifying the alpha used in the output or being as descriptive as the ground truth. It also misses printing means/medians which are crucial for interpreting the direction of the difference in t-tests and Mann-Whitney U.
*   **Fixed Alpha:** The `alpha` variable is hardcoded as `0.05` within each test function's `if` condition, rather than being defined globally as `alpha = 0.05` and used consistently, which is better practice as shown in the ground truth.
*   **Unused Import:** `statsmodels.api as sm` is imported but not used.

**Overall:**

The code demonstrates an understanding of the types of tests required, but critically fails on data handling, column mapping, and implementing the specific logic for data splitting and categorization as requested by the prompt. It's not executable as is given the provided CSV.",llama-3.3-8b-instruct,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
from sklearn.inspection import per...",6,"The generated code addresses most of the task requirements but has several critical issues that prevent it from being production-ready or fully correct.

**Correctness Issues:**
1.  **Column Names:** The most significant error is the use of incorrect column names for features. The prompt specifies ""GDP per capita"", ""Freedom"", ""Perceptions of corruption"" but the actual dataset column names are ""Logged GDP per capita"", ""Freedom to make life choices"", and ""Perceptions of corruption"". The code attempts to use the simplified names, which will lead to a `KeyError` at `X = df[['GDP per capita', ...]]` because these columns don't exist in the CSV. This makes the code fundamentally non-functional for the given dataset.
2.  **Preprocessing Categorical Variables:** The code attempts to `LabelEncode` 'Region' and 'Aggregates', but the dataset doesn't have these exact column names. The relevant categorical columns are 'Regional indicator' and 'Country name'. More importantly, none of these are specified to be features for the regression model, so encoding them is irrelevant if they are not used. The prompt's specified features are all numerical.
3.  **Missing Value Handling:** `df.dropna()` will drop *any* row with *any* missing value, which might be too aggressive if only a subset of features or the target needs clean rows. The ground truth specifically drops missing values only in the relevant target and feature columns.

**Completeness Issues:**
1.  **Hyperparameter Tuning Target:** The code identifies the ""best model"" based on R-squared but then applies a fixed `param_grid` (`n_estimators`, `max_depth`) which is suitable only for tree-based models like Random Forest or Gradient Boosting. If the ""best model"" were Linear Regression, Ridge, or Lasso, this tuning would fail or be inappropriate (though it would likely not be the best R-squared anyway). The ground truth specifically tunes a strong candidate like Gradient Boosting, which is a safer approach.
2.  **Cross-Validation:** While cross-validation is implemented, it's done manually with a `for` loop, rather than using `sklearn.model_selection.cross_val_score`, which is more concise and standard. Also, the cross-validation is applied to `X_train` and `y_train` only, rather than the full dataset (`X`, `y`) which is typically used for general model stability assessment (the ground truth uses `X`, `y`).
3.  **Visualization Libraries:** Imports `matplotlib.pyplot as plt` but not `seaborn`, which is generally preferred for nicer statistical plots.
4.  **Prediction Analysis:** The analysis of hypothetical countries is very brief and doesn't provide the level of detail or explanation found in the ground truth.

**Code Quality Issues:**
1.  **No `try-except` for file loading:** The code assumes the CSV file exists, which is less robust than the ground truth's `try-except` block.
2.  **Random State for Models:** `Ridge` and `Lasso` (and others if they have `random_state` arguments) are not initialized with `random_state`, which can lead to non-reproducible results for models that have stochastic components.
3.  **Feature Importance Plotting:** `permutation_importance` is used, which is good, but the plot itself (`plt.barh`) could use better labels or titles for clarity.

To improve, the code needs to:
*   Correctly identify and use the exact column names from the dataset.
*   Refine the data preprocessing, especially for categorical variables, ensuring only necessary encoding is applied.
*   Improve robustness (e.g., file loading).
*   Standardize the cross-validation implementation using `cross_val_score`.
*   Make the hyperparameter tuning more robust to the ""best model"" selection or explicitly choose a strong candidate for tuning.",llama-3.3-8b-instruct,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import seaborn as sns

# Load and preprocess the dat...",4,"The generated code attempts to address all requirements but suffers from significant correctness issues, particularly in data manipulation and feature engineering, which makes many subsequent steps invalid.

Here's a breakdown of the problems:

**Correctness Issues:**
*   **Column Names Mismatch:** The code assumes 'Ladder Score', 'GDP per capita', 'Region', 'Freedom', and 'Corruption' are present in the DataFrame. The ground truth (and typical `world-happiness-report-2021.csv` structure) uses 'Ladder score', 'Logged GDP per capita', 'Regional indicator', 'Freedom to make life choices', and 'Perceptions of corruption'. This fundamental mismatch breaks most of the feature engineering and subsequent steps. For instance, `df['Happiness_Efficiency'] = df['Ladder Score'] / df['GDP per capita']` will fail or produce an empty column.
*   **Polynomial Features Application:** The generated code attempts to apply polynomial features to specific columns by directly assigning results to new columns like `df[['Ladder Score squared', ...]]`. `fit_transform` on `PolynomialFeatures` returns a NumPy array. Assigning it back to a DataFrame with multiple new column names this way is incorrect and will likely lead to shape mismatches or type errors. It also hardcodes the output column names instead of dynamically generating them as `PolynomialFeatures` does.
*   **Data for Dimensionality Reduction and Feature Selection:** The generated code performs PCA, t-SNE, SelectKBest, RFE, and mutual information scoring on the *entire* `df` (which includes original columns, engineered features, and non-numeric columns like 'Country name', 'Regional indicator'), rather than specifically on a set of numerical features (like `X` in the ground truth). This is incorrect for these algorithms, as they expect numerical input, and can lead to errors or nonsensical results. No `X` and `y` separation is performed.
*   **Feature Scaling Application:** Similar to dimensionality reduction, the scaling is applied to the entire `df`, including non-numeric columns. The output `df_scaled` is then created for each scaler, but these scaled DataFrames are not used further. The original `df` is used for subsequent steps, negating the scaling.
*   **SelectKBest `f_classif`:** The problem is a regression task (predicting 'Ladder score'), but `f_classif` is used, which is for classification. The correct score function for regression is `f_regression`.
*   **RFE Input and Usage:** RFE is applied to the entire `df` instead of just numerical features (`X`).
*   **Mutual Information Scoring Input:** `mutual_info_regression` is applied to `df, df['Ladder Score']`, which is incorrect. It should be `X, y`. If `df` contains non-numeric columns, this will error.
*   **Feature Importance Visualization:** The `plt.bar(df.columns, importances)` will fail because `df.columns` will be much larger than the number of importances returned by `rf.feature_importances_` if `df` contains non-numeric columns or if a subset of features was used for RF training unintentionally. The ground truth correctly sorts and selects top features for plotting.


**Completeness/Quality Issues:**
*   **Missing Imports:** `mutual_info_regression` is imported twice (once from `sklearn.feature_selection` at the top, and again later).
*   **Error Handling:** No file not found handling for CSV.
*   **Warning Suppression:** Not included.
*   **Clarity and Structure:** The code is a linear script, but the constant re-use of `df` without proper separation of `X` and `y` for different steps makes it less robust and prone to errors. The ground truth introduces `df_processed`, `X`, and `y` which greatly improves clarity and modularity.
*   **Visualization:** While plots are attempted, they often lack proper labeling or color mapping (e.g., PCA/t-SNE plots don't map to 'Ladder Score' as the ground truth does).
*   **Drop NaNs:** `df = df.dropna()` is used without specifying columns, which might remove too many rows if only a few specific columns are relevant for NaN handling. The ground truth intelligently handles NaNs for specific columns.
*   **'GDP_vs_Regional_Average' calculation:** `df['GDP per capita'].mean()` is a global mean, not a regional average, which contradicts the task requirement.
*   **Interaction Features:** Only two interaction features are created, while the ground truth creates more.

The critical issue is the reliance on incorrect column names and the lack of proper data preparation (like `X` and `y` separation, and preparing `X` to be purely numeric before feeding into algorithms), which makes most of the advanced steps non-functional or incorrect.",llama-3.3-8b-instruct,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller

# Load the dataset and filter for 'location' == 'USA'
data = pd.read_csv('synthetic_covid19_data.csv')
data = data[data['location'] == 'USA']

# Set the 'date' column as the index for the filtered data
data['date'] = pd.to_datetime(data['date'])
data.set_index('date', inplace=True)

# Resample the 'new_cases' data...",6,"The generated code correctly implements most of the required steps, including loading and filtering data, setting the index, resampling, performing decomposition, and conducting the ADF test with differencing. It also attempts to plot the decomposed components and the ACF/PACF.

However, there are several significant areas for improvement and a few omissions:
- **Decomposition Plot:** The decomposition plot method used `plt.subplot(411)` etc., which is a basic manual plot. The `statsmodels.tsa.seasonal.seasonal_decompose` object has a `.plot()` method that generates a much cleaner and more professional plot and is simpler to use (as demonstrated in the ground truth).
- **ADF Test Logic:** The ADF test result printout is good, but the `if stationarity_test[1] > 0.05` block specifically only plots ACF/PACF *if* it was non-stationary and differenced. If the original series was already stationary, the ACF/PACF plots are completely skipped, which is a critical omission. The task explicitly states ""Plot the ACF and PACF of the (potentially differenced) stationary time series,"" implying it should always happen on the final stationary series.
- **`period` in `seasonal_decompose`:** The `period` argument was omitted in the `seasonal_decompose` function. For weekly data, a period of 52 (weeks in a year) is typically crucial for accurate seasonality identification. Without it, `seasonal_decompose` defaults to a period of 1, which will not capture yearly seasonality. This is a major functional flaw for time series analysis.
- **Robustness/Edge Cases:** The code lacks robustness checks, such as handling empty dataframes after filtering, insufficient data points for decomposition/ADF test (e.g., if there's very little data after filtering or resampling), or `NaN` values in the `new_cases` column that could cause issues with resampling or decomposition. The ground truth demonstrates better handling of these.
- **Imports:** While `numpy` is imported, it's not strictly used in the provided code. Using `from statsmodels.graphics.tsaplots import plot_acf, plot_pacf` inside the `if` block is not ideal; it should be at the top with other imports `from statsmodels.graphics.tsaplots import plot_acf, plot_pacf`.
- **Plot Titles/Labels:** Plots lack descriptive titles and axis labels for clarity, although `matplotlib` might add some defaults.
- **Hardcoded File Path:** The file path is hardcoded directly, which is acceptable for a script but a function with an argument (as in ground truth) is more reusable.

Overall, the code follows the core requirements but misses important details that enhance correctness, robustness, and presentation quality common in expert-level time series analysis.",llama-3.3-8b-instruct,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import f_oneway
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Categorize wines into pH groups
df['pH_group'] = pd.cut(df['pH'], bins=[0, 3.2, 3.4, np.inf], labels=['Low', 'Medium', 'High'])

# Calculate average quality for each pH group
avg_quality = df.groupby('pH_group')['quality'].mean()

# Perform ANOVA test
f_stat, p_value = f_oneway(df['quality'][df['pH_group'] == 'Low'], 
   ...",5,"The generated code addresses some parts of the task but falls short on several critical requirements, leading to a low score.

**Correctness:**
*   **pH Group Labels:** The generated code uses simplified labels (`Low`, `Medium`, `High`) instead of the specified detailed labels (`Low pH (< 3.2)`, `Medium pH (3.2 to 3.4)`, `High pH (> 3.4)`).
*   **Confidence Intervals/Error Bars:** This is a major failure. The task specifically asks for error bars representing 95% confidence intervals. The generated code uses `yerr=1.95/np.sqrt(avg_quality.values)`. This is a hardcoded value likely intended as a generic standard error approximation, but it's not a correct calculation for a 95% confidence interval (which requires standard error of the mean, degrees of freedom, and t-distribution, or directly using `scipy.stats.t.interval`). It also incorrectly assumes `avg_quality.values` represents `n` for `sqrt(n)`, when it represents the quality means. This completely misrepresents the error bars and is incorrect for the task's requirement.
*   **ANOVA Input:** While `f_oneway` is used, the selection of data for ANOVA (`df['quality'][df['pH_group'] == 'Low']`) is less robust than collecting non-empty groups, and it hardcodes the pH group names, making it less flexible to changes in group labels.

**Completeness:**
*   **Reporting Average Quality with CI:** The task explicitly asks to ""Calculate and report the average 'quality' for each pH group"" in a readable format. The generated code only prints the mean, not the calculated confidence intervals.
*   **ANOVA Conclusion:** The task asks to ""determine if there is a statistically significant difference"" and implies printing a conclusion based on the p-value. The generated code only prints the F-statistic and p-value, without an interpretation.
*   **Visualization Details:**
    *   The task explicitly asks for error bars representing 95% confidence intervals, which is not met.
    *   The visualization style is a simple `plt.errorbar`, which is less readable and informative than a bar chart with error bars, as demonstrated in the ground truth. The ground truth also adds quality values on top of bars, which is a nice touch for readability.
    *   Missing `plt.tight_layout()` and `plt.ylim()`.

**Code Quality:**
*   **Imports:** Missing `scipy.stats` as `stats` (only `f_oneway` is imported directly).
*   **Structure:** No clear separation of concerns (e.g., configuration, data loading, each step).
*   **Clarity/Readability:** Variable names are okay, but the lack of comments and structured sections makes it less clear.

Overall, the code performs parts of the task (loading, categorization, basic ANOVA) but fundamentally fails on the confidence interval calculation and visualization requirements, which are central to performing and presenting statistical analysis.",llama-3.3-8b-instruct,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder

# Load the data...",6,"The generated code attempts to address all requirements but has several critical issues that make it incorrect or inefficient for the given task.

**Critical Issues:**

1.  **Missing `numpy` Import:** The line `X_encoded = np.concatenate(...)` will cause a `NameError` because `numpy` is not imported. This is a fundamental error.
2.  **Incorrect Imputation of Categorical Data:** `SimpleImputer(strategy='median')` is applied to `X` which includes 'continent'. While the intent might have been to impute median for numerical columns, processing `X` as a whole array after `imputer.fit_transform(X)` (which implicitly converts `X` to a numpy array if it's a DataFrame) will attempt to impute 'continent' with a median, which is nonsensical for a categorical column.
3.  **Incorrect One-Hot Encoding Application:**
    *   `X_imputed[:, 7]` assumes 'continent' is always at index 7 after imputation, which might not hold if columns are reordered or if `SimpleImputer` alters the structure unexpectedly without explicitly handling column names.
    *   `continent_encoder.fit_transform` is applied to `X_imputed[:, 7]`, which is the *imputed* column. The imputation strategy (median) for 'continent' is wrong; it should be handled as a separate categorical feature from the start for encoding, or imputed with a 'most_frequent' strategy if missing values existed in 'continent'. The prompt requires handling *missing values in selected features and target by imputing with the median*, which implies numerical features. 'continent' was only for encoding.
4.  **Scaling and Pipelines Redundancy/Incorrectness:**
    *   `X_train_scaled = scaler.fit_transform(X_train)` and `X_test_scaled = scaler.transform(X_test)` are called *before* the pipelines.
    *   Then, the pipelines themselves also include `('scaler', StandardScaler())`. This means the data is being scaled *twice* before entering the model within the pipeline. This is incorrect and potentially leads to wrong results (double scaling).
    *   `X_train_scaled` and `X_test_scaled` are passed to `linear_regression.fit` and `random_forest_regressor.fit`. Since the pipeline itself includes a `StandardScaler`, it expects *unscaled* input. This setup is flawed; one should either scale manually *or* use the pipeline for scaling, but not both.
5.  **Accessing `columns` on NumPy Array:** `numerical_features = X_train.columns` will fail because `X_train` is a NumPy array after `train_test_split` (due to previous operations like imputation and concatenation), and NumPy arrays do not have a `.columns` attribute. This means numerical features are not correctly identified for scaling.
6.  **Less Robust Missing Value Handling for Target:** While `y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))` works, it's generally cleaner to impute `y` directly using `y.fillna(y.median(), inplace=True)` or a specific imputer defined only for `y` if `y` is still a pandas Series/DataFrame. Using `SimpleImputer` fit on the entire `y` array is acceptable but less explicit.
7.  **Parsing Date:** `df['date'] = pd.to_datetime(df['date'])` is correct for parsing, but the prompt says to parse them *when loading* the dataset, which is usually done via `parse_dates=['date']` in `pd.read_csv` for efficiency and immediate correct data type. This is a minor point, but it's a detail from the prompt.

**Improvements to achieve a higher score:**

*   **Import `numpy`:** Add `import numpy as np`.
*   **Correct Imputation and Encoding Workflow:**
    *   Keep `continent` as a separate column or process it explicitly for one-hot encoding *before* numerical imputation if needed.
    *   Use `ColumnTransformer` to handle different preprocessing steps for numerical and categorical features in a structured way (as seen in some correct pipeline practices). This allows applying `SimpleImputer(strategy='median')` only to numerical columns and `OneHotEncoder` only to categorical columns within a single preprocessing step.
    *   Alternatively, manually separate numerical and categorical columns before imputation and encoding.
*   **Correct Scaling with Pipelines:** If using pipelines, remove the manual `scaler.fit_transform` and `scaler.transform` calls outside the pipeline. Pass the *unscaled* `X_train` and `X_test` to the `fit` and `predict` methods of the `linear_regression` and `random_forest_regressor` pipeline objects.
*   **Defensive Programming:** Add a `try-except` block for `FileNotFoundError` when loading the CSV, as in the ground truth, for robustness.
*   **Clarity and Readability:** The ground truth's step-by-step comments make the code clearer (though the prompt asked for no explanations, the structure helps). Using more descriptive variable names can also help.",llama-3.3-8b-instruct,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
from scipy.stats import f_oneway, pearsonr
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
import numpy as np

# Load dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Handle missing values by median imputation
df['reproduction_rate'] = df['reproduction_rate'].fillna(df['reproduction_rate'].median())
df['stringency_index'] = df['stringency_index'].fillna(df['stringency_index'].median())
df['median_age'] = df['median_age']...",4,"The generated code attempts to address all parts of the task but has significant issues in correctness and completeness, particularly in the t-test section and the handling of the death rate ratio.

**Correctness Issues:**

*   **Missing 'total_deaths/population' calculation:** The code immediately attempts to use `df['total_deaths/population']` without ever defining or calculating this column. This will lead to a `KeyError`. The task explicitly states to calculate `total_deaths / population`.
*   **Incorrect `high_death_rate_country` logic:**
    *   The line `df['high_death_rate_country'] = df.groupby('country')['total_deaths/population'].transform(lambda x: x > x.quantile(0.75))` attempts to get quantiles *within each country's deaths/population*, rather than across all countries as specified by the task (""across all countries (use latest entry per country)"").
    *   The subsequent logic (`df_latest` and the `apply` function) is overly complicated and incorrect for determining `high_death_rate_country`. It re-attempts to find the latest data point per country but then uses it in a way that doesn't correctly assign the binary flag based on the 75th percentile of the *global* death rate ratio.
    *   The `high_death_rate_country` column should be a binary (0 or 1, or True/False) flag for each country, not a conditional assignment based on a comparison to `total_deaths_population`.
*   **T-test implementation:** The code attempts to use `statsmodels.formula.api.ols` for a t-test. While `ols` can be used for linear regression where the t-statistic of a coefficient can be interpreted, it's not the direct and appropriate way to perform an independent samples t-test between two groups on a single variable (`median_age`). `scipy.stats.ttest_ind` is the correct function for this purpose, and it was implicitly allowed by the prompt (""Use an appropriate t-test""). The current implementation will raise an error because `t_test()` expected arguments it won't receive properly from the `ols` output for a direct group comparison.
*   **ANOVA input:** The `anova_test` function is defined with a `continent` parameter but then uses hardcoded `'Asia'`, `'Europe'`, and `'North America'` lists inside. The parameter is redundant and not used. It's also trying to print the null hypothesis as `f'Null Hypothesis: Means of ""reproduction_rate"" are equal across {continent}'` which would incorrectly print ""across continents"" instead of specifying the actual continents included.
*   **Missing value imputation:** While it correctly uses median imputation, it's very verbose. A loop over the column list would be cleaner and more robust.
*   **Missing null hypothesis statements:** The prompt specifically asks to state the null hypothesis *for each test*. The generated code attempts this but includes `correlation_test()`, `anova_test('continents')`, `t_test()` function calls at the end without proper printing of the statements before the function call, which could be confusing in the output. The ground truth prints the null hypothesis then calls the function.
*   **No Failsafe for missing files**: The code directly attempts `pd.read_csv` without a `try-except` block, which could lead to a crash if the file is not found.

**Completeness Issues:**

*   The crucial calculation of `total_deaths / population` is missing, which makes the derivation of `high_death_rate_country` impossible as intended.
*   The `df['date']` column is not converted to datetime objects, which is essential for correctly determining the ""latest entry per country"" when calculating the death rate for the 75th percentile. The current approach to find the latest data is flawed because it tries to find the max of the ratio, not the ratio for the latest date.
*   The task says ""state the null hypothesis, print the p-value, and conclude..."". While it prints the p-value and conclusion, the way the null hypothesis is stated is inside the function, which might not be ideal based on the specific ""for each test, state the null hypothesis"" wording, implying it could be printed before the test execution.

**Code Quality:**

*   Imports: Uses `statsmodels` which wasn't strictly required but not harmful.
*   Structure: The use of functions for tests is good, but the global scope modifications (imputation, adding columns) could make it hard to reuse.
*   Readability: The `high_death_rate_country` derivation is convoluted and hard to follow due to the incorrect logic.

Overall, the code demonstrates an understanding of the types of tests required but fails significantly in implementing the complex logic required for the `high_death_rate_country` derivation and the subsequent t-test, leading to large correctness issues.",llama-3.3-8b-instruct,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select numerical columns
numerical_df = df[['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']]

# Calculate pairwise correlation matrix
correlation_matrix = numerical_df.corr()

# Visualize correlation matrix using heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)
plt.ti...",9,"The generated code correctly performs all the requested tasks: loading the CSV, selecting the specified numerical columns, calculating the correlation matrix, and visualizing it as a heatmap. It includes all necessary imports and the visualization is well-formatted.

Strengths:
- Correctly loads the data.
- Accurately selects the specified numerical columns.
- Correctly calculates the pairwise correlation matrix using `.corr()`.
- Successfully generates a heatmap with `seaborn.heatmap`, including annotations and a suitable colormap.
- Uses appropriate figure size for the heatmap.

Areas for minor improvement (not critical for functionality but good practice):
- The ground truth includes error handling (e.g., `FileNotFoundError`, `KeyError`), which is robust for real-world applications but not explicitly requested in the prompt. The generated code lacks explicit error handling, which is acceptable given the prompt's focus on the core task.
- The ground truth encapsulates the logic within a function and uses constants for paths and column names, which improves reusability and maintainability. The generated code is more direct, which is fine for a single script but less modular.
- The ground truth adds more detailed `print` statements which can help in debugging or understanding the flow of execution.

Overall, the generated code fully satisfies the core requirements of the prompt.",llama-3.3-8b-instruct,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
def load_dataset():
    try:
        data = pd.read_csv('clash_royale_cards.csv')
        return data
    except FileNotFoundError:
        print(""The file does not exist"")
        return None

# Calculate average values for each card rarity
def calculate_average_values(data):
    averaged_data = data.groupby('rarity').agg({
        'elixirCost': 'mean',
        'Win Rate': 'mean',
        'Usage': 'mean'
    })
    return averaged_data

# Main f...",9,"The generated code correctly solves the task by loading the CSV, grouping by rarity, calculating the required averages, and displaying them. It also includes proper imports and error handling for the file not found scenario. The structure is good, utilizing functions to modularize the code, which improves readability and maintainability.

One minor area for improvement is that the task explicitly asked to ""Provide only the Python code without explanations,"" and the generated code includes comments (though fewer than the ground truth in terms of length). This is a very minor point, and the code itself is functionally sound and well-structured. The use of `.agg()` is an alternative to direct column selection after `groupby().mean()`, both achieving the same result. The ground truth's more concise `df.groupby('rarity')[['elixirCost', 'Win Rate', 'Usage']].mean()` is slightly more common for simple mean calculations, but the `.agg()` method is perfectly valid and arguably more explicit for multiple aggregations or different aggregation types.",llama-3.3-8b-instruct,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# Create a new binary target column
data['is_high_win_rate'] = np.where(data['Win Rate'] > 55, 1, 0)

# Select the features
features = data[['elixirCost', 'Usage', 'rarity']]

# Apply one-hot encoding
encoder = OneHotEncoder()
encoded_rarity = encoder.fit_transform(features[['rarity']])
encoded_rarity ...",6,"The generated code correctly loads the dataset, creates the `is_high_win_rate` target column, and splits the data into training and testing sets with the specified parameters. However, there are issues with the feature selection and the one-hot encoding process, which significantly impact the correctness and usability of the resulting `X` dataframe.

Specifically:
1.  **Feature Selection Error**: `features = data[['elixirCost', 'Usage', 'rarity']]` creates a new DataFrame `features` that is a *copy* of the relevant columns from `data`. Subsequent operations (like `encoder.fit_transform(features[['rarity']])`) operate on this `features` DataFrame. While this is not inherently wrong, the `pd.concat` step later incorrectly tries to combine `features.drop('rarity', axis=1)` (which is `elixirCost` and `Usage` from the *original* `features` copy) with the `encoded_rarity`. This implies that `features` should have been kept as part of the main `data` DataFrame, or handled more carefully. Also, `features.drop('rarity', axis=1)` could lead to `SettingWithCopyWarning` if `features` was a view.

2.  **OneHotEncoder Issues**:
    *   `OneHotEncoder` expects 2D array-like input. While `features[['rarity']]` is correct, `fit_transform` returns a sparse matrix.
    *   `encoder.get_feature_names(['rarity'])` is deprecated. The correct way to get feature names is `encoder.get_feature_names_out()`.
    *   The most significant issue is how the encoded `rarity` columns are integrated back. `pd.concat([features.drop('rarity', axis=1), encoded_rarity, data[['is_high_win_rate']]], axis=1)` concatenates based on index. If the original DataFrame `data` had a non-default index (or if rows were dropped), this concatenation could misalign the data, leading to incorrect feature-target pairings. `OneHotEncoder` does not preserve the DataFrame index. When `encoded_rarity` is created from `toarray()`, its index defaults to 0, 1, 2... and if `features.drop('rarity', axis=1)` has a different or misaligned index, the `concat` will fail to correctly merge by default. It's much safer to use `pd.get_dummies` for this task when working with DataFrames, as shown in the ground truth, as it handles index alignment correctly.

3.  **Missing `np` import**: The line `data['is_high_win_rate'] = np.where(data['Win Rate'] > 55, 1, 0)` requires `import numpy as np`, which is missing.

4.  **Inefficient Feature Engineering**: The process of separating features, encoding one, and then manually concatenating them back with the target column is unnecessarily complex and prone to index misalignment errors compared to using `pd.get_dummies` directly on the feature DataFrame `X` (as done in the ground truth) and then performing the split.

To improve, the code should:
*   Import `numpy`.
*   Use `encoder.get_feature_names_out()` for correct column naming.
*   Ensure proper index alignment when concatenating. A simpler approach would be to create `X` with all features (numerical + categorical), apply `get_dummies` to `X`, and then split `X` and `y`.
*   Consider using `pd.get_dummies` for simplicity and robustness in this specific case, as it's often preferred for one-hot encoding directly within DataFrames.",llama-3.3-8b-instruct,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv('iris.csv')

# Define features X
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Define the target y
y = df['Species']

# Encode the categorical Species target variable into numerical labels
le = LabelEncoder(...",9,"The generated code correctly implements all specified requirements: loading data, defining features and target, encoding the target, splitting data (70/30, random_state=42), training a KNN classifier with `n_neighbors=3`, and reporting accuracy.

**Strengths:**
- Correct imports and usage of scikit-learn modules.
- Accurate selection of features and target variables.
- Correct application of `LabelEncoder`.
- Proper data splitting with `random_state`.
- Correct KNN model instantiation and training.
- Accurate calculation and printing of the accuracy score.

**Minor Area for Improvement (Not critical for this task but good practice):**
- The ground truth includes `stratify=y_encoded` in `train_test_split`. While not explicitly asked for, it's a good practice, especially with imbalanced datasets, to ensure that the proportions of target classes are preserved in both training and testing sets. For the Iris dataset, which is well-balanced, omitting `stratify` might not lead to a significant difference in results for a single run, but its inclusion makes the split more robust. Given the prompt didn't *require* stratification, its absence doesn't penalize the core correctness too much here.
- The ground truth includes error handling for `FileNotFoundError`. While not explicitly requested, it's good practice for robust code. For this specific task, it's a minor detail.
- The output format for accuracy differs slightly (two decimal places vs. four suggested by ground truth). This is a formatting choice and doesn't affect correctness.",llama-3.3-8b-instruct,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
def load_dataset():
    try:
        data = pd.read_csv('iris.csv')
        return data
    except FileNotFoundError:
        print(""The file iris.csv was not found."")
        return None

# Perform independent two-sample t-test
def perform_t_test(data):
    # Filter the data to get two groups
    versicolor = data[data['Species'] == 'Iris-versicolor']
    virginica = data[data['Species'] == 'Iris-virginica']

  ...",9,"The generated code is highly correct and complete, addressing almost all requirements.

**Strong Points:**
*   **Correctness:** Loads data, correctly filters, performs the t-test, and interprets the results based on alpha.
*   **Modularity:** Uses functions (`load_dataset`, `perform_t_test`, `main`) which is good practice for larger scripts.
*   **Hypotheses Statement:** Clearly states the null and alternative hypotheses.
*   **Output:** Prints t-statistic, p-value, and the interpretation based on alpha.
*   **Imports:** Uses correct imports (`pandas`, `scipy.stats.ttest_ind`).
*   **Error Handling:** Includes basic `FileNotFoundError` handling for the CSV.

**Areas for Minor Improvement (preventing a perfect 10):**
*   **Hypotheses Detail:** While stated, the hypotheses could be slightly more precise, aligning with the ground truth's ""statistically significant difference in the mean PetalLengthCm"" which explicitly links to ""mean"" and the specific metric. The generated code says ""is the same"" and ""is not the same"", which is functionally correct but less formal/specific to the statistical phrasing sometimes preferred. It also doesn't include the symbolic representation (e.g., μ_versicolor = μ_virginica) which was in the ground truth.
*   **Output Formatting:** The f-string for printing `t_stat` and `p_value` doesn't include formatting to a specific number of decimal places (e.g., `:.4f`), which can make the output less consistent or readable compared to the ground truth.
*   **Filtering Efficiency:** The generated code creates new dataframes for `versicolor` and `virginica`. While not a major performance issue for this dataset, the ground truth's approach of directly selecting the 'PetalLengthCm' series is slightly more memory-efficient as it avoids creating intermediate DataFrames, especially if the original DataFrames had many columns.

Overall, a very strong submission that fulfills the task requirements effectively.",llama-3.3-8b-instruct,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
def load_dataset():
    try:
        data = pd.read_csv('iris.csv')
        return data
    except FileNotFoundError:
        print(""The file iris.csv was not found."")
        return None

# Generate box plots
def generate_box_plots(data):
    features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
    for feature in features:
        plt.figure(figsize=(10, 6))
        plt.boxplot([data[data[...",7,"The generated code correctly loads the dataset and generates box plots for each numerical feature grouped by species, fulfilling the main task requirements. It also includes appropriate titles for the plots.

However, there are a few areas for improvement:
1.  **Plotting Library choice**: The task description explicitly mentions ""Use pandas and matplotlib or seaborn for plotting."" The generated code uses `matplotlib.pyplot.boxplot` directly. While it works, `seaborn.boxplot` (as used in the ground truth) is generally preferred for its simplicity and aesthetic appeal when dealing with grouped box plots from a DataFrame, as it handles the grouping (`x` argument) and data (`data` argument) more declaratively. The prompt implies a preference for Seaborn by mentioning it alongside matplotlib.
2.  **Code for plotting**: The way `plt.boxplot` is used requires manually filtering data for each species and passing them as separate lists. This is a bit more verbose and less idiomatic for data analysis with pandas/seaborn than using `seaborn.boxplot(x='Species', y=feature, data=data)`.
3.  **No `seaborn` import**: Although the prompt mentions ""Uses pandas and matplotlib or seaborn for plotting"", there's no import for seaborn, which is a minor detail but worth noting given the prompt.
4.  **Error Handling**: The `FileNotFoundError` handling is present but simply prints a message and returns `None`, which is then checked in `main`. The ground truth includes `exit()` which is a more definitive way to stop execution if essential data is missing.
5.  **Code Structure (Minor)**: While functional, wrapping all code in functions and a `main` block is good practice, but for a simple script generating plots, a more direct sequential structure with defined constants (as in the ground truth) can also be highly readable and equally robust.

Overall, it's a solid attempt that accomplishes the core task but could be improved by leveraging `seaborn` for more concise and elegant plotting code, and minor enhancements in error handling.",llama-3.3-8b-instruct,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv('heart.csv')

# Select features and target variable
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']
X = df[features]
y = ...",7,"The generated code correctly loads the data, selects the specified features and target, and performs one-hot encoding for the categorical features. It also correctly splits the data, trains a Logistic Regression model, and reports the accuracy.

However, there is a significant issue in how preprocessing is handled. The code preprocesses the *entire* `X` dataset *before* splitting it into training and testing sets: `X_preprocessed = preprocessor.fit_transform(X)`. This leads to data leakage because the `OneHotEncoder` learns its categories and applies the transformation based on the full dataset, which includes information from the test set. The correct approach, as demonstrated in the ground truth, is to use a `Pipeline` or `ColumnTransformer` within the `train_test_split` context so that `fit_transform` is applied only to the training data, and `transform` is applied to the test data.

Specifically:
- **Correctness:** The data leakage stemming from preprocessing the full dataset before splitting introduces an incorrect evaluation of the model's performance on unseen data.
- **Completeness:** While it performs most steps, the lack of `remainder='passthrough'` in `ColumnTransformer` is a minor omission for the numerical features, though it doesn't cause an error here because all selected features are either numerical or explicitly handled as categorical. The Ground Truth uses `max_iter=1000` for `LogisticRegression` to ensure convergence, which the generated code omits, although for this dataset, it might not be critical.

To improve, the preprocessing step should be integrated into a `Pipeline` that first applies the `ColumnTransformer` and then the `LogisticRegression` model. This ensures that the preprocessing parameters are learned *only* from the training data.",llama-3.3-8b-instruct,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
data = pd.read_csv('heart.csv')

# Create two groups based on the `Sex` column
male_group = data[data['Sex'] == 'M']['Cholesterol']
female_group = data[data['Sex'] == 'F']['Cholesterol']

# Perform an independent two-sample t-test
t_stat, p_value = stats.ttest_ind(male_group, female_group)

# State the null and alternative hypotheses
print(""Null Hypothesis: The mean Cholesterol levels are equal between male and female pati...",8,"The generated code successfully addresses most of the requirements. It correctly loads the data, separates it into male and female cholesterol groups, performs an independent two-sample t-test using `scipy.stats.ttest_ind`, states the hypotheses, prints the t-statistic and p-value, and interprets the result based on a significance level of 0.05.

**Strengths:**
*   Correct use of pandas for data loading and filtering.
*   Correct application of `scipy.stats.ttest_ind` for the t-test.
*   Clear and accurate statement of null and alternative hypotheses.
*   Correct interpretation of the p-value against the alpha level.

**Areas for Improvement (to reach a 9 or 10):**
*   **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and missing columns, which is a good practice for production-ready code. The generated code lacks this.
*   **Data Cleaning (dropna):** The ground truth explicitly uses `.dropna()` when creating the male and female cholesterol groups. While `ttest_ind` will likely handle NaNs by discarding them if present, explicitly handling them ensures clarity and prevents potential issues if the behavior of the t-test function changes or if subsequent operations were added.
*   **Encapsulation/Function:** The ground truth encapsulates the logic within a function (`perform_cholesterol_t_test`), making the code reusable and testable. The generated code is a script, which is acceptable for a simple task but less modular.
*   **`equal_var` parameter in `ttest_ind`:** The ground truth explicitly sets `equal_var=False` (Welch's t-test), which is generally more robust as it doesn't assume equal variances, a common violation in real-world data. The generated code uses the default, which is `equal_var=True` (Student's t-test). While not strictly ""wrong"" without knowing population variances, using Welch's is often preferred unless equality of variance is confirmed. Given the problem statement did not specify which type of t-test to perform, `equal_var=True` is not an error, but mentioning or choosing Welch's would show deeper understanding.
*   **Output Formatting:** The ground truth formats the t-statistic and p-value to a specific number of decimal places, improving readability.

Overall, it's a solid solution for the core task. The missing error handling and lack of function encapsulation are the main reasons it's not a perfect score.",llama-3.3-8b-instruct,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Function to identify and cap outliers
def handle_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    print(f'Number of outliers in {column}: {len(outliers)}')
    df[column] = np.where(...",3,"The generated code fails significantly in meeting the core requirements of the task.

1.  **Correctness**: The primary issue is that the code attempts to compare ""before"" and ""after"" statistics on the *same* DataFrame (`df`) after it has already been modified. The `handle_outliers` function modifies `df` in-place. Therefore, when `print('Before outlier treatment:')` is called, the `df` has already had its outliers treated, making the ""before"" statistics inaccurate. This fundamental flaw means the code does not correctly demonstrate the comparison required by the task.

2.  **Completeness**:
    *   **Task 1 (Identify outliers using IQR)**: Partially done for reporting. The number of outliers is reported *before* winsorization, which is good.
    *   **Task 2 (Report number of outliers)**: Done.
    *   **Task 3 (Create new dataset version where outliers are capped)**: The code modifies the existing `df` in-place within the function. It doesn't create a *new dataset version* while preserving the original for comparison. This directly leads to the correctness issue described above.
    *   **Task 4 (Compare mean and standard deviation before and after treatment)**: This is the main point of failure due to the in-place modification. The ""before"" statistics are not taken from the truly original data.
    *   **Error Handling**: No `try-except` block for `FileNotFoundError` when loading the CSV.

3.  **Code Quality**:
    *   The use of a function `handle_outliers` is a good practice for modularity.
    *   The `np.where` usage for winsorization is correct.
    *   Lack of comments and clear structure (e.g., separating distinct steps as in the ground truth) makes it less readable given the task's step-by-step requirements.

To improve, the generated code *must* make a copy of the DataFrame *before* applying the outlier treatment, and then calculate ""before"" statistics from the original DataFrame and ""after"" statistics from the treated copy.",llama-3.3-8b-instruct,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
data = pd.read_csv('heart.csv')

# Create a grouped bar chart
chest_pain_types = data['ChestPainType'].unique()
heart_diseases = [0, 1]
bar_width = 0.2
x = np.arange(len(chest_pain_types))

fig, ax = plt.subplots()
for i, heart_disease in enumerate(heart_diseases):
    ax.bar(x + i * bar_width, data.groupby('ChestPainType')['HeartDisease'].apply(lambda x: (x == heart_disease).sum()).values, bar_w...",6,"The generated code correctly loads the dataset and attempts to create a grouped bar chart. It uses `matplotlib` for plotting and includes a title, labeled axes, and a legend. However, there are several areas for improvement. The grouped bar chart is implemented manually using loops and `ax.bar` calls, which is less idiomatic and more verbose than using `seaborn.countplot` for this specific type of visualization. The x-axis labels are not correctly centered for the grouped bars, as `ax.set_xticks(x + bar_width)` only shifts the tick locations but doesn't average for the group. The custom bar plotting also doesn't inherently handle missing categories in `ChestPainType` if they don't appear in the `unique()` values, whereas `seaborn.countplot` would implicitly count zeroes. Error handling (e.g., `FileNotFoundError`) is missing. The hardcoded `heart_diseases` list (0 and 1) is fine for this problem, but in a more general scenario, it might not be robust. The approach is functional but could be made more robust, readable, and concise using more appropriate libraries/functions like `seaborn`.",llama-3.3-8b-instruct,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Clean the `in_shazam_charts` column
df['in_shazam_charts'] = df['in_shazam_charts'].str.replace(',', '').astype(float, errors='coerce')
print(""Count of NaNs in 'in_shazam_charts':"", df['in_shazam_charts'].isna().sum())

# Group the data by `key` and `mode`
grouped_df = df.groupby(['key', 'mode'])

# Calculate and display the average `danceability_%` and average `valence_%` for each group
for name, group in ...",8,"The generated code correctly loads the CSV, cleans the `in_shazam_charts` column by removing commas and converting to numeric (coercing errors to NaN and reporting the count), and groups the data by `key` and `mode`. It then calculates and displays the average `danceability_%` and `valence_%` for each group.

**Strong points:**
*   Correctly handles loading the CSV.
*   The cleaning of `in_shazam_charts` is concise and correct (`.str.replace(',', '').astype(float, errors='coerce')`).
*   Reports the NaN count accurately.
*   Correctly groups by `key` and `mode`.
*   Calculates the means for the desired columns.

**Areas for improvement:**
*   The display of the grouped results is done via a loop, which is less idiomatic and efficient in pandas compared to directly aggregating and printing the result (`grouped_df[['danceability_%', 'valence_%']].mean()`). While it produces the correct output, it deviates from the more conventional pandas approach for displaying aggregated results, especially for larger datasets. The ground truth code directly prints the aggregated DataFrame, which is generally preferred.
*   The ground truth also includes `try-except` for `FileNotFoundError`, which is good practice. While the prompt assumes the file is in the current directory, robust code often includes this check. However, since the prompt didn't *demand* error handling, it's not a major deduction.",llama-3.3-8b-instruct,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Ensure the `streams` column is numeric
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams'] + df.select_dtypes(include=['int64', 'float64']).columns)

# Select features and target
X = df[['in_spotify_playlis...",7,"The generated code correctly performs most of the required steps: loading the data, handling the `streams` column coercing to numeric and dropping NaNs, selecting features and target, splitting data, training a Linear Regression model, and evaluating it by reporting R2 and MSE.

However, there's a significant deviation from the prompt's requirement regarding dropping NaNs:
The prompt explicitly states: ""drop rows with NaN in `streams` or predictor columns for this task."" The predictor columns are `in_spotify_playlists` and `energy_%`.

The generated code uses `df.dropna(subset=['streams'] + df.select_dtypes(include=['int64', 'float64']).columns)`. This line attempts to drop NaNs from `streams` and ALL numeric columns, which is not what was asked. It should specifically drop NaNs from `streams`, `in_spotify_playlists`, and `energy_%`. While `in_spotify_playlists` and `energy_%` are likely numeric, this approach is less precise and potentially problematic if there are other numeric columns that *should not* be dropped for NaN in this specific task.

Additionally, the generated code lacks the `try-except` block for `FileNotFoundError` as seen in the ground truth, which is good practice for robust code.

Overall, it's a good attempt, but the specific NaN handling for predictor columns is a notable miss that affects correctness based on the precise prompt instructions.",llama-3.3-8b-instruct,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Combine released_year, released_month, and released_day into a single datetime column
df['release_date'] = pd.to_datetime(df['released_year'].astype(str) + '-' + 
                                   df['released_month'].astype(str).str.zfill(2) + '-' + 
                                   df['released_day'].astype(str).str.zfill(2), 
                                   errors='coerce')

# Cr...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, converts the date components into a single datetime column `release_date` while handling errors using `errors='coerce'`, and calculates `days_since_release` from '2023-12-31'. Finally, it displays the requested columns for the first 5 tracks.

The code is concise and directly implements the logic. The use of `str.zfill(2)` ensures robust date string formatting for months and days, which is a good practice. `errors='coerce'` perfectly handles invalid dates as required. The calculation of `days_since_release` using `.dt.days` is also correct.

The only minor point, which doesn't detract significantly from correctness but is worth noting for robustness, is that the `days_since_release` column can contain `NaN` values where `release_date` was `NaT`. By default, `.dt.days` returns a float column if NaNs are present after calculation. If the requirement was strictly for an integer *type* even with NaNs, then converting to nullable integer type (`'Int64'`) as shown in the ground truth would be a slight improvement. However, given the prompt only states ""an integer"", and pandas handles `NaN` values in float columns naturally, this is a very minor distinction in the context of the problem.

Strong points:
- Correct implementation of all steps.
- Effective use of `errors='coerce'` for robust date conversion.
- Proper handling of string formatting for date components (`str.zfill(2)`).
- Clear and readable code.
- Successfully meets the display requirement.",llama-3.3-8b-instruct,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the 'Video views' and 'Likes' columns
df['Video views'] = df['Video views'].str.replace(',', '').astype(float, errors='coerce').fillna(0)
df['Likes'] = df['Likes'].str.replace(',', '').astype(float, errors='coerce').fillna(0)

# Fill missing values in the 'Category' column
df['Category'] = df['Category'].fillna('Unknown')

# Group the data by 'Category'
grouped_df = df.groupby(...",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans the `Video views` and `Likes` columns by removing commas, converting to numeric types (with error handling for non-numeric values), and filling NaNs with 0. It also correctly fills missing `Category` values with 'Unknown'. Finally, it groups the data by `Category` and calculates the sum of cleaned `Video views` and `Likes` per category.

The code is concise and uses efficient pandas operations for cleaning and aggregation. The use of `astype(float, errors='coerce')` is a good way to handle non-numeric values by converting them to NaN.

One minor point that makes the ground truth slightly more robust is the explicit checks for column existence. While not strictly necessary for a perfect dataset, it adds robustness against malformed input files. However, for the given task and implicit assumption of valid column names, the generated code is excellent. Another minor stylistic point is that printing results in a loop is less ""pandastic"" than directly printing the aggregated DataFrame, as done in the ground truth (`print(category_summary)`). Both achieve the desired result, but `print(category_summary)` provides a cleaner, more tabular output. However, the requirement was ""calculate and display"", and the generated code correctly displays the sums.",llama-3.3-8b-instruct,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean the 'Likes' column
df['Likes'] = df['Likes'].fillna(0)
df['Likes'] = df['Likes'].str.replace(',', '').astype(float, errors='coerce')

# Fill missing values in the 'Category' column
df['Category'] = df['Category'].fillna('Unknown')

# Create two groups of cleaned 'Likes' values
music_likes = df[df['Category'] == 'Music']['Likes']
sports_likes = df[df['Category'] == 'Sports'...",7,"The generated code addresses most of the requirements but has a critical flaw in the `Likes` column cleaning, which could lead to incorrect results.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly fills missing `Category` values with 'Unknown'.
*   Correctly identifies and separates the 'Music' and 'Sports' groups.
*   Correctly performs the independent two-sample t-test using `scipy.stats.ttest_ind`.
*   Correctly states the null and alternative hypotheses, albeit a bit less formally than the ground truth.
*   Correctly prints the t-statistic and p-value.
*   Correctly interprets the p-value based on alpha = 0.05.
*   Uses proper imports (`pandas`, `scipy.stats`).

**Areas for Improvement / Critical Flaw:**
*   **Critical Flaw in `Likes` Cleaning (Score Impact):** The order of operations for cleaning `Likes` is incorrect and problematic:
    ```python
    df['Likes'] = df['Likes'].fillna(0) # This fills NaNs with 0 BEFORE string replacement and conversion
    df['Likes'] = df['Likes'].str.replace(',', '').astype(float, errors='coerce')
    ```
    If an original 'Likes' value is a non-numeric string (e.g., 'Not Available') which should become NaN and then 0, `fillna(0)` at the beginning will turn such strings into `0`. Then `str.replace(',', '')` on `0` (an integer/float) will raise an error if not handled by `astype(str)` first, or `astype(float, errors='coerce')` on `0` will work, but the original intent of treating 'conversion failures as NaN, then filling with 0' is violated. The ground truth correctly converts to string, removes commas, then converts to numeric with `errors='coerce'`, and *then* fills NaNs with 0. The generated code effectively treats invalid string entries as `0` instead of `NaN` then `0`. This is a subtle but important difference in data cleaning logic.
*   **No NaN Handling After Numeric Conversion in Likes:** Although `astype(float, errors='coerce')` handles conversion failures by placing `NaN`, the code then doesn't explicitly `fillna(0)` *after* this conversion. The first `fillna(0)` only handles original NaNs, not the ones created by `errors='coerce'`. This means if `df['Likes']` originally had values like `['1,000', 'abc', '2,000']`, 'abc' would turn into `0` by the first `fillna(0)` and then be correctly converted to float, but `errors='coerce'` would not have a chance to convert it to NaN first.
*   **Lack of Robustness (File Not Found):** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which makes the code more robust. The generated code assumes the file will always be present.
*   **No check for empty groups:** The ground truth includes checks (`if len(music_likes) < 2 or len(sports_likes) < 2`) to ensure there's enough data for a meaningful t-test. This is important for preventing potential errors or misleading results if one or both groups are too small.
*   **Default `equal_var` in `ttest_ind`:** The generated code uses the default `equal_var=True` for `ttest_ind`. While often reasonable, for real-world data science, `equal_var=False` (Welch's t-test) is generally safer as it does not assume equal population variances, which is often violated. The ground truth explicitly uses `equal_var=False`.

The core issue is the `Likes` column cleaning, which deviates from the specified logic and could silently produce incorrect numerical values, thus making the t-test results invalid. The other points are about robustness and best practices.",llama-3.3-8b-instruct,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean Likes and Dislikes columns
df['Likes'] = df['Likes'].apply(lambda x: float(str(x).replace(',', ''))).replace([np.inf, -np.inf], np.nan)
df['Dislikes'] = df['Dislikes'].apply(lambda x: float(str(x).replace(',', ''))).replace([np.inf, -np.inf], np.nan)
df['Likes'] = df['Likes'].fillna(0)
df['Dislikes'] = df['Dislikes'].fillna(0)

# Create new columns
df['TotalEngagement'] = df['L...",7,"The generated code addresses most of the requirements but has a few issues that prevent it from being perfect.

**Positives:**
*   **Correct Logic for Feature Engineering:** It correctly calculates `TotalEngagement` and `LikeDislikeRatio` according to the specifications.
*   **Correct Denominator for Ratio:** The `+1` in the `LikeDislikeRatio` calculation is correctly implemented to avoid division by zero.
*   **Handling of NaNs:** It correctly fills NaNs with 0 in `Likes` and `Dislikes`.
*   **Good Imports:** It includes necessary imports (`pandas` and `numpy`).

**Areas for Improvement / Issues:**
*   **Error Handling for CSV Loading:** The code lacks proper error handling for `pd.read_csv`, which means it would crash if the file is not found, unlike the ground truth.
*   **Robustness of `Likes`/`Dislikes` Cleaning:**
    *   The `apply(lambda x: float(str(x).replace(',', '')))` approach is less robust than `pd.to_numeric(..., errors='coerce')`. If `str(x).replace(',', '')` results in a non-numeric string (e.g., an empty string or a string with other characters), `float()` would raise a `ValueError` instead of converting to `NaN`. `pd.to_numeric(errors='coerce')` is designed precisely for this kind of conversion, automatically handling non-numeric values by turning them into `NaN`.
    *   While `replace([np.inf, -np.inf], np.nan)` is present, it's mostly redundant for this specific problem if `pd.to_numeric(errors='coerce')` was used, as `float()` on string conversion generally wouldn't produce `inf` unless the original data itself had 'inf' strings (which is unlikely in typical `Likes`/`Dislikes` data).
*   **Column Name Mismatch:** The task specifically asks to display the `Video` title, but the generated code uses `Title`. While `Title` might be the actual column name in the dataset, the prompt explicitly states `Video`. This is a minor detail but important for strict adherence to the prompt.
*   **Efficiency:** The `apply` method can be slower for large datasets compared to vectorized operations like `str.replace()` followed by `pd.to_numeric()`.

Overall, it's a solid attempt that gets most of the core logic right but could improve on robustness of cleaning and adhere more strictly to the specified column name.",llama-3.3-8b-instruct,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the 'Launched Price (USA)' column
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''), errors='coerce')

# Clean the 'Screen Size' column
df['Screen Size'] = pd.to_numeric(df['Screen Size'].str.replace(' inches'), errors='coerce')

# Group the data by 'Company Name' and 'Launched Year'
grouped_df = df.groupby(['Company N...",8,"The generated code correctly addresses the prompt's requirements. It properly loads the CSV, cleans both specified columns by removing unwanted strings and converting to numeric with error handling, groups the data by the correct columns, and calculates the average of the cleaned numeric columns. The final aggregated data is displayed as requested.

Areas for minor improvement:
1.  **Readability/Conciseness of String Cleaning:** While `df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', '')` works, it can sometimes be more readable or slightly more efficient to use a single `str.replace` with regex for multiple replacements, especially if there were many patterns or if whitespace handling within the replacement was needed (though not strictly necessary here).
2.  **Explicit Column Naming in Aggregation:** The ground truth code created new columns like `'Cleaned Price (USD)'` and `'Cleaned Screen Size (inches)'` for the cleaned data. The generated code reuses the original column names for the cleaned numeric data. While this works and is implicit from the `pd.to_numeric` conversion, explicitly creating new columns for cleaned data is a good practice for clarity and to avoid confusion if the original string columns were still needed later.
3.  **Error Handling for File Loading:** The ground truth included `try-except` blocks for `pd.read_csv`, which is good practice for robustness in real-world scenarios, especially in a benchmark context where the file might be missing. The generated code assumes the file exists.
4.  **Displaying Aggregated Data:** The generated code directly prints `aggregated_df`. The ground truth adds a descriptive header, which improves presentation.

Overall, it's a solid solution achieving the core task requirements accurately.",llama-3.3-8b-instruct,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Define a function to clean camera columns
def clean_camera(x):
    match = re.search(r'\d+', x)
    return int(match.group()) if match else 0

# Apply the function to create cleaned columns
df['Cleaned Front Camera'] = df['Front Camera'].apply(clean_camera)
df['Cleaned Back Camera'] = df['Back Camera'].apply(clean_camera)

# Create a new feature 'Total Camera MP'
df['Total Camera MP'] = df['Cleane...",8,"The generated code correctly addresses the core requirements of the task: loading the CSV, cleaning camera specifications to extract MP values, creating the 'Total Camera MP' feature, and displaying the specified columns for the first 10 rows. The `clean_camera` function is concise and effective for its purpose.

Areas for improvement:
1.  **Robustness of `clean_camera`:** The generated code's `clean_camera` function does not explicitly handle `NaN` or non-string inputs. While `re.search` on a non-string might raise a `TypeError` if not handled, `apply` typically converts non-strings to strings before passing them to the function, but it's still safer to explicitly check for `pd.isna` or convert to string within the function, as done in the ground truth. This is a minor robustness point but could be an issue with edge cases in the data.
2.  **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for production-ready code. The generated code assumes the file will always be present.
3.  **Readability/Comments:** While the code is relatively straightforward, adding comments to explain each major step (load, clean, sum, display) would improve readability, especially for more complex tasks.
4.  **Output Formatting:** The ground truth uses `.to_string()` for better console formatting of the DataFrame output, which is a nice touch for presentation.

Despite these minor points, the solution is functionally correct and achieves the task's goals.",llama-3.3-8b-instruct,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean specific columns
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
df['Screen Size'] = pd.to_numeric(df['Screen Si...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, cleans the specified columns by removing non-numeric characters and converting to numeric with error handling, defines features and target, splits the data, trains a Linear Regression model, and evaluates it using R2 and MAE.

Strong points:
- All required imports are present.
- Column cleaning is correctly implemented, including handling multiple patterns for 'Launched Price (USA)' and converting errors to NaN.
- Dropping NaNs in the specified subset of columns is done correctly after cleaning.
- Data splitting, model training, and evaluation are all performed as requested.
- The output format for R2 and MAE is clear.

Minor areas for improvement/differences from ground truth:
- The generated code doesn't include `try-except` for `FileNotFoundError` when loading the CSV, which makes it slightly less robust than the ground truth.
- The ground truth includes `regex=False` in `str.replace`, which is good practice for literal string replacement. The generated code implicitly handles this for the given patterns, but explicitly setting it can prevent unexpected behavior with special regex characters.
- The output formatting of R2 and MAE in the generated code (`print(f'R-squared (R2) score: {r2}')`) doesn't format the numbers to a specific decimal precision (`.4f`, `.2f`) as done in the ground truth, which is a minor aesthetic difference but doesn't affect correctness.
- The generated code assumes all columns exist; the ground truth added a check for column existence and prints a warning if a column is missing, improving robustness.

Overall, the generated code is highly competent and meets all task requirements effectively.",llama-3.3-8b-instruct,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
from scipy.stats import chi2_contingency, ttest_ind, f_oneway
import numpy as np

# Load the dataset
df = pd.read_csv('titanic.csv')

# Conduct a chi-square test for association between Pclass and survival
chi2, p, dof, expected = chi2_contingency(pd.crosstab(df['Pclass'], df['Survived']))
print(""Chi-square test for Pclass and Survival:"")
print(f""p-value: {p:.4f}"")
if p < 0.05:
    print(""Reject the null hypothesis: There is a significant association between Pclass ...",6,"The generated code attempts to address all parts of the prompt but has several critical issues and omissions, particularly regarding data handling, statistical correctness, and effect size calculations.

**Issues:**

1.  **Missing Data Handling:** The ground truth correctly handles missing 'Age' and 'Fare' values using `dropna()`. The generated code does not explicitly handle these, which can lead to errors or incorrect statistical results, especially for `ttest_ind` and `f_oneway` if NaNs are present in the 'Age' or 'Fare' columns. `ttest_ind` and `f_oneway` in `scipy.stats` typically raise an error if NaNs are present in the input arrays, or they might implicitly drop them, leading to potentially different sample sizes than expected without explicit handling.
2.  **Chi-square Effect Size (Cramer's V) Calculation:** The generated code's formula for Cramer's V `np.sqrt(chi2 / (chi2 + dof))` is incorrect. The correct formula used in the ground truth is `sqrt(chi2 / (n * min_dim))`, where `n` is the total number of observations and `min_dim` is `min(rows-1, cols-1)`. The generated code's formula is not standard for Cramer's V and would likely produce an incorrect effect size.
3.  **T-test Effect Size (Cohen's d) Calculation:** The generated code attempts to calculate Cohen's d as `np.abs(t_stat) / np.sqrt(2)`. This formula is incorrect and does not represent Cohen's d, which is typically calculated as the difference between means divided by the pooled standard deviation (`(mean1 - mean2) / pooled_std`). The ground truth provides a correct calculation of Cohen's d, including handling of pooled standard deviation.
4.  **ANOVA Effect Size (Eta-squared) Calculation:** The generated code's formula for eta-squared `1 - (1 - f_stat) / (fare_prices[0].size + fare_prices[1].size + fare_prices[2].size)` is fundamentally incorrect. Eta-squared is calculated as `SS_between / SS_total`. The ground truth correctly implements the sum of squares calculation to derive eta-squared.
5.  **Lack of Robustness:** The generated code assumes the 'titanic.csv' file exists without error handling (e.g., `try-except FileNotFoundError`), unlike the ground truth.
6.  **Readability/Completeness:** While functional, the output is less informative than the ground truth. It lacks detailed print statements, explicit hypothesis statements, and a consistent alpha level definition at the beginning, making the interpretation less clear. The ground truth also prints the test statistics alongside p-values, which is standard practice.
7.  **ANOVA Input:** While `f_oneway(*fare_prices)` works, the arrays for `fare_prices` created by `df.loc[df['Pclass'] == i, 'Fare'].values` might still contain NaNs if the 'Fare' column itself had them, leading to issues with `f_oneway` if not handled. The ground truth explicitly uses `dropna()` on 'Fare' before splitting.

**Strengths:**

*   Correctly identifies and uses the appropriate `scipy.stats` functions (`chi2_contingency`, `ttest_ind`, `f_oneway`).
*   Uses a correct alpha level (0.05) for interpretation.
*   Basic structure follows the task requirements.

To improve, the code needs to:
*   Implement robust missing data handling.
*   Correctly implement the formulas for all effect sizes (Cramer's V, Cohen's d, Eta-squared).
*   Add more informative print statements for test statistics, means, and clearer conclusions.
*   Consider adding data loading error handling.",llama-3.3-8b-instruct,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Error loading dataset: {e}"")

# Group data by year and calculate averages
def calculate_averages(data):
    averages = data.groupby('Year')[['Rating', 'Runtime']].mean()
    return averages

# Create line plot of average Rating over Year
def plot_average_rating(averages):
    pl...",6,"The generated code correctly addresses the main requirements of the task. It loads the dataset, groups by year, calculates the average rating and runtime, and plots the average rating over the years with a title and labels.

However, there are significant issues that prevent a higher score:
1.  **Robustness and Data Cleaning:** The generated code *completely skips* robust data cleaning. The `imdb_top_250.csv` file often has `Year` as `(YYYY)` and `Runtime` with ' min' (e.g., '142 min'). The ground truth code correctly handles these data cleaning steps, which are crucial for the code to run successfully on the raw dataset. Without this cleaning, the `groupby` on `Year` or `mean` on `Runtime` will fail.
2.  **Error Handling (Partial):** While `load_dataset` has a `try-except` for file loading, it doesn't handle errors during the grouping or plotting steps, which could occur if data types are incorrect due to the lack of cleaning.
3.  **No `Runtime` Plot:** The prompt asks to calculate the average `Rating` *and* `Runtime`, but only `Rating` is plotted. While only `Rating` was explicitly asked to be plotted, the calculation of both implies they should be handled, which the generated code does but doesn't necessarily show off. The primary omission is the data cleaning to make `Runtime` usable.

To improve, the code needs to incorporate robust data cleaning for the 'Year' and 'Runtime' columns, similar to the ground truth, to ensure it can successfully process the typical `imdb_top_250.csv` dataset. This involves extracting numeric values from 'Year' (e.g., removing parentheses) and converting 'Runtime' (e.g., removing ' min' and converting to int).",llama-3.3-8b-instruct,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
data = pd.read_csv('imdb_top_250.csv')

# Create two groups of movies based on the `Year` column
group1 = data[data['Year'] < 1990]
group2 = data[data['Year'] >= 1990]

# Perform an independent two-sample t-test
t_stat, p_value = stats.ttest_ind(group1['Rating'], group2['Rating'])

# State the null and alternative hypotheses
print(""Null Hypothesis: The mean ratings of movies released before 1990 and in or after 1990 are eq...",7,"The generated code correctly performs the core task of loading data, grouping it, performing a t-test, stating hypotheses, and interpreting the results. However, it lacks robustness and some best practices present in the ground truth.

**Areas for Improvement:**
1.  **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. The generated code assumes the file will always be present, which can lead to a script crash if the file is missing.
2.  **Data Type Conversion and Cleaning:** The ground truth explicitly converts 'Year' and 'Rating' columns to numeric types and handles potential `NaN` values resulting from conversion errors (`errors='coerce'`, `dropna`). This is crucial for real-world datasets where data types might not be perfectly clean. The generated code implicitly relies on pandas reading these as numbers, which might not always be the case, especially for 'Year' if it contains non-numeric characters.
3.  **Group Size Check:** The ground truth includes a check to ensure that each group has at least 2 samples for the t-test. While unlikely for this specific dataset, it's a good practice to prevent errors in edge cases or with smaller, more volatile datasets.
4.  **Hypotheses Clarity:** The hypotheses stated in the generated code are slightly less formal/detailed than the ground truth's, which explicitly mentions ""statistically significant difference"" and sets a clear scientific tone.
5.  **`equal_var` Parameter:** The `ttest_ind` function has an `equal_var` parameter (defaulting to `True`). While often acceptable for introductory cases, the ground truth explicitly uses `equal_var=True` and briefly comments on its implication, which shows a deeper understanding of the statistical test.

Overall, it's a functional solution that addresses the main requirements but could be more robust and complete for a production-level or more thorough analysis script.",llama-3.3-8b-instruct,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

def load_and_engineer_data():
    # Load the dataset
    data = pd.read_csv('imdb_top_250.csv')

    # Create a new column named `Decade` by categorizing the `Year` column
    data['Decade'] = (data['Year'] // 10) * 10

    # Format the decade as a string (e.g., '1990s')
    data['Decade'] = data['Decade'].apply(lambda x: f""{x}s"")

    # Display the `Title`, `Year`, and the new `Decade` column for the first 10 movies
    print(data[['Title', 'Year', 'Decade']].head...",9,"The generated code correctly performs all specified tasks: loading the dataset, creating the `Decade` column accurately, and displaying the required columns for the first 10 rows. The approach to calculating and formatting the `Decade` column is concise and effective.

Strengths:
- **Correctness:** The `Decade` calculation `(data['Year'] // 10) * 10` is accurate for determining the start of the decade, and the `f""{x}s""` formatting correctly appends 's'.
- **Completeness:** All requirements from the prompt (load data, create `Decade`, display specific columns for first 10 rows) are met.
- **Readability:** The code is straightforward and easy to understand.
- **Proper Imports:** `pandas` is correctly imported.

Areas for minor improvement (not critical enough to significantly reduce score for this specific task):
- The generated code directly modifies the dataframe. While acceptable for a small script, in a larger application, wrapping the engineering logic in a function that returns a new dataframe (as in the ground truth) could be better practice for modularity and preventing unintended side effects.
- Error handling (e.g., for `FileNotFoundError` or missing columns) is not included, which is generally good practice for robust code, though not explicitly requested in the prompt's requirements beyond ""Provide only the Python code without explanations."" However, since the ground truth includes it, it's worth noting as a best practice.
- The `apply(lambda x: f""{x}s"")` call for string formatting is functionally correct but could be slightly less performant than vectorized string operations (like `astype(str) + 's'`) for very large datasets, though for this dataset size, the difference is negligible.",llama-3.3-8b-instruct,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Load the dataset
def load_dataset():
    try:
        data = pd.read_csv('penguins.csv')
        return data
    except FileNotFoundError:
        print(""The file 'penguins.csv' was not found."")
        return None

# Perform data cleaning and aggregation
def clean_and_aggregate(data):
    # Impute missing values in numerical columns
    numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
    for col in numerical_cols:
   ...",9,"The generated code correctly performs all the required steps: loading the data, imputing numerical columns with the mean, imputing the 'sex' column with the mode, grouping by 'species' and 'sex', and calculating the mean 'body_mass_g'. The code is well-structured using functions, which improves readability and modularity. Implicitly handles `body_mass_g` being numeric as `.mean()` will coerce. The use of `.iloc[0]` for mode is robust.

A marginal improvement could be to print the final result with a descriptive header like the ground truth does, rather than just the DataFrame output. Also, the ground truth includes more robust error handling (e.g., `KeyError` for missing columns), which is good practice for production-ready code, although for a benchmark task, it might be considered an extra.",llama-3.3-8b-instruct,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the data to include only records where `species` is 'Adelie' or 'Gentoo'
df_filtered = df[df['species'].isin(['Adelie', 'Gentoo'])]

# Remove any rows where `flipper_length_mm` or `species` is missing (NA)
df_filtered = df_filtered.dropna(subset=['flipper_length_mm', 'species'])

# Perform an independent two-sample t-test
adelie_flux = df_filtered[df_filtered['species'] ...",9,"The generated code correctly performs all the required steps: loading the data, filtering, handling NAs (though implicitly for `species` through prior filtering, it still correctly applies `dropna`), performing the t-test, stating hypotheses, and interpreting the results.

**Strong points:**
*   Correctly uses `pandas` for data manipulation and `scipy.stats.ttest_ind` for the t-test.
*   Correctly filters data for 'Adelie' and 'Gentoo'.
*   Correctly drops NA values in the specified columns within the filtered dataset.
*   Clearly states the null and alternative hypotheses.
*   Prints the t-statistic and p-value.
*   Correctly interprets the p-value against the alpha level of 0.05.

**Areas for minor improvement:**
*   The ground truth included robust error handling for `FileNotFoundError` and other file loading issues, which is good practice for real-world scenarios. The generated code assumes the file exists.
*   The ground truth also explicitly checks if groups are empty or have insufficient data points before performing the t-test (e.g., `len(adelie_flipper_lengths) < 2`), which is a good robustness check. The generated code would throw an error if one of the flipper length series became empty after filtering/NA removal.
*   The `ttest_ind` function has an `equal_var` parameter which defaults to `True`. While often a reasonable default, specifying `equal_var=False` (Welch's t-test) is often preferred for robustness when variances are unequal, which is common in real data. The ground truth explicitly chose `equal_var=False` and commented on it; mentioning and deciding on this parameter would be a slight improvement. However, since the problem did not explicitly state which t-test variant to use, the default is acceptable.
*   The hypothesis statements in the ground truth were slightly more formal and precise (e.g., ""There is no statistically significant difference in the *mean* flipper length between Adelie and Gentoo penguins (μ_Adelie = μ_Gentoo)""). The generated code's statements are correct in meaning but a bit less formal.

Overall, it's a very good solution that directly addresses all core requirements of the task. The minor areas for improvement relate to defensive programming and explicit parameter choices often seen in more robust statistical code.",llama-3.3-8b-instruct,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression

# Load the dataset
df = pd.read_csv('penguins.csv')

# Select features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]
y = df['species']

# Drop rows with missing values
df_clean = df.dropna(subset=['species', 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', ...",9,"The generated code correctly implements all specified requirements. It loads the dataset, selects the correct features and target, handles missing values in the specified columns, encodes the target variable, splits the data with the correct parameters, trains a Logistic Regression model with `max_iter=1000`, and reports the accuracy.

A minor improvement could be to explicitly import `accuracy_score` from `sklearn.metrics` for calculating accuracy, as `model.score` is primarily for evaluating the model on the full test set used during training/validation and `accuracy_score` is the standard metric. However, `model.score` returns the mean accuracy on the given test data and labels for classification models, which is what was asked for.

The `X` and `y` are re-assigned after `dropna` which leads to a slight redundancy but is correct. It creates a new `df_clean` and then re-selects `X` and `y` from it, rather than just using the variables `X`, `y` that were previously assigned using the original `df`. This is not incorrect but could be slightly more efficient by selecting `X` and `y` directly from `df_clean` without the intermediate assignment from `df` before dropping NAs.

Overall, it's a very good solution that addresses all aspects of the task.",llama-3.3-8b-instruct,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load dataset: {str(e)}"")
        return None

# Preprocess the dataset
def preprocess_dataset(data):
    data = data.dropna(subset=['body_mass_g', 'species'])
    return data

# Calculate average body mass for each species
def calculate_average_bo...",9,"The generated code correctly addresses all requirements of the task. It loads the data, correctly handles NAs by dropping rows in `body_mass_g` and `species`, calculates the average `body_mass_g` per `species`, and generates a bar plot with a title and labeled axes.

The code is well-structured using functions, which enhances readability and reusability. Imports are correct. The plot customization is minimal but sufficient for the task's requirements (title and labels are present).

Areas for minor improvement (not critical enough to significantly reduce the score but notable compared to the ground truth):
- The error handling in `load_dataset` is generic (`Exception as e`) rather than specific (`FileNotFoundError`), and it prints an error but doesn't exit the script, which could lead to `None` being passed down and subsequent errors. The ground truth's `exit()` is more robust for a critical loading step.
- The plot's axis labels and title are hardcoded strings, whereas the ground truth dynamically generates them from variable names, making it more robust to column name changes.
- The generated plot lacks some aesthetic enhancements present in the ground truth, such as `figsize`, `palette`, `xticks` rotation, and `grid`, which improve readability but weren't explicitly requested as requirements. However, the core requirements of title and labels are met.",llama-3.3-8b-instruct,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
def load_dataset():
    try:
        data = pd.read_csv('penguins.csv')
        return data
    except FileNotFoundError:
        print(""The file penguins.csv was not found."")
        return None

# Perform feature engineering and correlation analysis
def main():
    # Load the dataset
    data = load_dataset()
    if data is None:
        return

    # Drop rows where culmen_length_mm, culmen...",9,"The generated code correctly performs all the required steps: loading the dataset, handling NAs in the specified columns, creating the `culmen_area` feature, calculating the Pearson correlation matrix for the specified features, and visualizing it as an annotated heatmap with the correct title. The code structure using functions `load_dataset` and `main` is good for organization.

Strengths:
- **Correctness:** All steps are implemented accurately.
- **Completeness:** All requirements from the task description are met.
- **Readability:** The code is clear and easy to understand.
- **Error Handling:** Basic `FileNotFoundError` handling is present.

Areas for minor improvement (to achieve a perfect 10):
- The `fmt="".2f""` argument in `sns.heatmap` was not included, which would format the annotations to two decimal places, making the output cleaner as seen in the ground truth.
- The plot could benefit from `plt.xticks(rotation=45, ha='right')` and `plt.yticks(rotation=0)` for better label readability, especially if there were more variables.
- The `square=True` argument in `sns.heatmap` is okay, but `figsize` is often sufficient for controlling aspect ratio. The `linewidths=.5` from the ground truth can add a subtle visual separation between cells.
- The prompt mentioned ""Pearson correlation matrix"". While `corr()` defaults to Pearson, explicitly stating `method='pearson'` as in the ground truth adds clarity and robustness against future changes to defaults or requirements.
- The `exit()` or `sys.exit()` calls in the ground truth are generally better than `return None` which means the program continues to execute, potentially leading to subsequent errors before stopping. Though for this specific task, the `if data is None: return` effectively stops the process.",llama-3.3-8b-instruct,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
def load_dataset(file_name):
    try:
        dataset = pd.read_csv(file_name)
        return dataset
    except Exception as e:
        print(f""Failed to load dataset: {str(e)}"")

# Preprocess the dataset
def preprocess_dataset(dataset):
    # Select features and target
    features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
    X = dataset[features]
    y = dataset['species'...",8,"The generated code correctly addresses most of the requirements.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly selects the specified features and target.
*   Correctly imputes numerical NA values with the mean.
*   Correctly imputes 'sex' NA values with 'Unknown' and maps 'MALE', 'FEMALE', 'Unknown' to 0, 1, -1 respectively.
*   Applies one-hot encoding to the 'island' column.
*   Prints the shape and the head of the processed features as required.
*   Good use of functions (`load_dataset`, `preprocess_dataset`, `main`) for better code organization, which is a plus for code quality.

**Areas for improvement:**
*   **No `copy()` after feature selection:** The line `X = dataset[features]` creates a view, not a copy. Subsequent modifications like `X[feature] = X[feature].fillna(...)` or `X['sex'] = X['sex'].fillna(...)` can lead to `SettingWithCopyWarning` and potentially not modify the original `dataset` DataFrame as intended, or conversely, unexpectedly modify it. Using `X = dataset[features].copy()` would prevent this.
*   **`drop_first=True` in `pd.get_dummies`:** While `drop_first=True` is often used to prevent multicollinearity in linear models, the task merely asked for one-hot encoding, not necessarily dropping the first dummy variable. The ground truth didn't drop it. It's not strictly incorrect but deviates slightly from a direct interpretation of ""one-hot encoding"" without qualification.
*   **Error Handling in `fillna` for `sex` mapping:** If there were any values in the 'sex' column other than 'MALE', 'FEMALE', or 'Unknown' (after imputation), the `.map()` function would introduce `NaN` values. The current code doesn't explicitly handle these potential `NaN`s after mapping (e.g., by mapping them to -1 as a fallback). The ground truth specifically added a check for this and mapped any resulting NaNs to -1. This is a minor robustness issue.
*   The `try-except` block in `load_dataset` is quite generic (`except Exception as e`). While it works, a more specific `FileNotFoundError` would be slightly better practice for clearer error messages, though `Exception` catches it too.

Overall, the code is highly functional and meets the core requirements. The issues are mainly related to robustness, best practices (like `copy()`), and minor deviations from the specific interpretation of ""one-hot encoding"" that the ground truth used.",llama-3.3-8b-instruct,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

# Create a pandas DataFrame with the given data
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107], 
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 
        'Quantity': [1, 2, 1, 1, 3, 1, 1], 
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150], 
        'CustomerID': ['CUST001', 'CUST002...",9,"The generated code correctly performs all requested operations: creating the DataFrame, calculating total spending per customer, identifying top 3 customers by spending, and calculating total quantity sold per category.

**Strong points:**
- Correctly imports pandas.
- Correctly creates the DataFrame.
- Accurately calculates `LineTotal` (named `TotalSpending` in the generated code) by multiplying `Quantity` and `Price_USD`.
- Correctly groups by `CustomerID` and sums the `LineTotal` to get total spending.
- Correctly identifies the top 3 customers using `sort_values` and `head`.
- Correctly calculates total quantity sold per category using `groupby` and `sum`.

**Areas for minor improvement:**
- The `reset_index()` on `customer_spending` is not strictly necessary for the subsequent `sort_values` and `head` if the intent is just to display the top customers. A Series directly sorted would also work, similar to the ground truth. However, it's not incorrect to convert it back to a DataFrame.
- The output format for the ""Top 3 Customers"" is a DataFrame printout, whereas the ground truth specifically formatted each customer with a label. While the information is present, the ground truth's specific formatted print might be slightly preferred for human readability if strict output formatting was a hidden requirement. This is a minor stylistic difference rather than a functional error.
- Lack of explicit print statements like ""--- Creating DataFrame ---"" before each step, which were present in the ground truth for better execution flow visibility. This doesn't affect correctness but can impact user experience.",llama-3.3-8b-instruct,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
df = pd.read_csv('titanic.csv')

# Handle missing values
df.fillna(df.mean()...",4,"The generated code attempts to address the requirements but falls short in several critical areas, making it functionally incorrect or incomplete for a robust solution to the task.

**Critical Issues:**

1.  **Incorrect Data Loading and Handling of `titanic.csv`:** The prompt implies the use of a standard `titanic.csv` dataset, which typically contains columns like `PassengerId`, `Name`, `Ticket`, `Cabin`, `Age`, `Embarked`, `Pclass`, `Sex`, `SibSp`, `Parch`, `Fare`, and `Survived`.
    *   **Missing Columns Error:** The generated code performs `df['sex'] = df['sex'].map({'male': 0, 'female': 1})` and `df['class'] = pd.get_dummies(df['class'], drop_first=True)` and `df['deck'] = pd.get_dummies(df['deck'], drop_first=True)`. The standard `titanic.csv` does not have `sex`, `class`, or `deck` columns (it has `Sex`, `Pclass`, and `Cabin` or similar). This will lead to `KeyError` or incorrect processing.
    *   **Dropping Irrelevant Columns:** The ground truth correctly identifies and drops `PassengerId`, `Name`, `Ticket`, `Cabin` which are irrelevant or problematic. The generated code does not do this, leading to potential issues with `df.mean()` on non-numeric types and including non-predictive features in the model.
    *   **Target Column Name:** The ground truth correctly uses `Survived`, while the generated code uses `survived` (lowercase), which might also cause a `KeyError` depending on the actual column name in the CSV.

2.  **Poor Missing Value Handling:**
    *   `df.fillna(df.mean(), inplace=True)` is generally problematic. It will fail on non-numeric columns (`Name`, `Sex`, `Embarked`, `Ticket`, `Cabin`) unless they are dropped or encoded first. Even for numeric columns, `mean` for `Age` (often better median) or `Fare` is okay, but it doesn't handle `Embarked` (categorical, needs mode).
    *   The ground truth specifically handles `Age` with median and `Embarked` with mode, which is more appropriate.

3.  **Incomplete Categorical Encoding:**
    *   It tries to encode `sex`, `class`, `deck` (wrong column names). The `Embarked` column, which is a key categorical feature in the Titanic dataset, is completely missed.
    *   The `Pclass` column should also be treated as categorical (as done in ground truth via one-hot encoding within a pipeline) but is not explicitly handled as such.

4.  **No Preprocessing Pipeline:** The generated code manually scales and then trains, rather than using `sklearn.pipeline.Pipeline` and `ColumnTransformer`. This makes the code less robust, harder to maintain, and prone to data leakage if scaling is done before `train_test_split` (though here it's done correctly *after* splitting). A proper pipeline like in the ground truth is standard practice for complex preprocessing.

5.  **Insufficient Feature Interpretation:**
    *   `print(""Feature Coefficients:"", model.coef_)` alone is almost useless. If one-hot encoding is performed manually, the user needs to know *which* coefficient corresponds to *which* encoded feature.
    *   The ground truth correctly extracts feature names after one-hot encoding within the pipeline and provides a clear interpretation for the coefficients.

6.  **Suboptimal Visualization:**
    *   The confusion matrix plotting is rudimentary (`plt.imshow`) and doesn't display labels or values directly on the matrix, making it less readable than `ConfusionMatrixDisplay.from_estimator`.
    *   The ROC curve `fpr, tpr, _ = plt.subplots()` is an incorrect way to get ROC curve values for plotting; it should use `RocCurveDisplay.from_estimator` or `roc_curve` from `sklearn.metrics` explicitly. The current code will likely fail or plot an empty graph.

7.  **Incomplete Cross-Validation:**
    *   `cross_val_score` is used, which is a good start. However, it only returns scores for the primary metric (accuracy by default for classification).
    *   The ground truth uses `cross_validate` with multiple scoring metrics (`accuracy`, `precision`, `recall`, `f1`, `roc_auc`) and explicitly reports the mean and standard deviation for each, providing a much more comprehensive assessment of model stability across various metrics.

Essentially, the code attempts to fulfill the requirements but lacks the depth, correctness, and best practices expected for a robust data science solution, particularly in the critical data preprocessing and evaluation stages, due to misunderstanding the dataset's structure and the expected advanced features of `sklearn` pipelines.",llama-3.3-8b-instruct,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']),
        'Ticker': ['TECHCO']*8,
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
        'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0, 157.0, 154...",10,"The generated code is correct, complete, and adheres to good code quality practices.

**Correctness:**
- It correctly creates the pandas DataFrame using the provided data.
- It correctly sets 'Date' as the index.
- It accurately calculates the 3-day Simple Moving Average (SMA) of the 'Close' price and assigns it to the 'SMA_3Day' column. It correctly handles the initial `NaN` values as per the requirement by using `rolling(window=3).mean()`.
- It accurately calculates the percentage change in 'Volume' from the previous day and assigns it to the 'Volume_Pct_Change' column. It correctly handles the first day with a `NaN` value using `pct_change()`.
- It prints the resulting DataFrame as requested.

**Code Quality:**
- Necessary import (`pandas` at the top) is included.
- The code is concise and uses pandas functionalities effectively (e.g., chaining `.set_index()` during DataFrame creation).
- Variable names are clear and descriptive.

**Completeness:**
- All requirements of the task are met:
    - DataFrame creation with specified data.
    - 'Date' set as index.
    - 3-day SMA calculation.
    - Volume percentage change calculation.
    - Correct handling of `NaN` values for initial days in both new columns.
    - Printing the final DataFrame.

Overall, the solution is excellent and mirrors the ground truth very closely in its approach and correctness.",llama-3.3-8b-instruct,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

# Create DataFrame with student performance data
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0...",9,"The generated code correctly performs all the requested tasks. It successfully creates the DataFrame, calculates the 'Score_Improvement' column, computes the average 'Score_Improvement' for each subject, and identifies students meeting the specific criteria.

The code is well-structured and uses appropriate pandas functionalities. The use of `.unique()` for iterating through subjects is a valid approach, although `groupby()` (as in the ground truth) is often more idiomatic and potentially more efficient for such aggregations. The final printout for students meeting criteria is clear, and the handling of the empty result is also correct.

The only minor area for improvement is in the calculation of average score improvement per subject. While correct, the loop approach (`for subject in df['Subject'].unique():`) is less ""pandastic"" than a `groupby().mean()` operation, which is typically more efficient and concise for aggregations across groups. However, since the task asked to *calculate and print*, both approaches satisfy the requirement. The ground truth's use of `groupby()` is slightly more efficient and idiomatic for pandas.",llama-3.3-8b-instruct,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Filter the DataFrame to include only employees from the...",9,"The generated code correctly defines the DataFrame, filters it according to the specified conditions, and accurately calculates the average number of ""ProjectsCompleted"" for the filtered subset. It also includes an appropriate check for an empty filtered DataFrame. The code is clean, well-structured, and directly addresses all parts of the task.

One minor point for improvement, which is not critical for correctness but good practice for benchmarks, would be to format the output of the average to two decimal places (e.g., `f""{average_projects_completed:.2f}""`) for consistency and readability, as done in the ground truth code. However, the current output is still correct.",llama-3.3-8b-instruct,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}

df = pd.DataFrame(data)

# Calculate conversion rate for users on ""Desktop""
desktop_conversion_rate = (df[df['DeviceType'] == 'Desktop']['Converted'].s...",4,"The generated code correctly defines the DataFrame. However, it fails to correctly calculate the conversion rates due to an AttributeError. The `.percent` attribute does not exist for a float result of the division. The `Conversión` column contains boolean values, and `sum()` on a boolean Series will count `True` values as 1 and `False` values as 0, which is correct for counting conversions. The `len()` correctly gets the total number of users for each device type. The problem is in the final step where `.percent` is used. Furthermore, the print statements are slightly off in their multiplication by 100, which would already be handled if `.percent` was removed and the calculation was directly multiplied by 100. The ground truth utilizes the more idiomatic `mean()` for boolean series, which directly gives the proportion of `True` values. The generated code also lacks handling for potential empty dataframes for a given `DeviceType`, which could lead to a `ZeroDivisionError` if `len()` returns 0.",llama-3.3-8b-instruct,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

for sensor in df[""SensorID""].unique():
    avg_air_quality = df[df[""SensorID""] ...",9,"The generated code correctly defines the DataFrame and accurately calculates and prints the average ""AirQualityIndex"" for each unique ""SensorID"". It meets both core requirements of the task.

**Strong points:**
- **Correctness:** The code correctly initializes the DataFrame as specified and calculates the average AirQualityIndex for each sensor.
- **Completeness:** It addresses both parts of the task: creating the DataFrame and performing the calculation/printing the results.
- **Readability:** The code is straightforward and easy to understand. The f-string used for printing provides clear output.
- **Efficiency (acceptable):** While not the most idiomatic pandas way (using `groupby().mean()` is generally preferred for performance on larger datasets), the loop with boolean indexing works perfectly fine for this small dataset and provides an equally correct result.

**Areas for minor improvement:**
- **Idiomatic Pandas:** For calculating statistics per group, `df.groupby('SensorID')['AirQualityIndex'].mean()` is generally more performant and often considered more ""pandastic"" than iterating through unique values and filtering. For this small dataset, the performance difference is negligible, but it's a good practice to be aware of for larger-scale data processing.
- **Output formatting:** The ground truth code formats the average to two decimal places (`.2f`), which is good for presentation. The generated code does not, but this is a minor aesthetic point and not explicitly required by ""clearly indicate"".

Overall, it's a very good solution that fully solves the problem. The minor suggestion is more about stylistic best practices in pandas than a major correctness flaw.",llama-3.3-8b-instruct,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

# Create a new feature ""EngagementScore""
df['EngagementScore'] = df['Likes'] + (df['Sh...",9,"The generated code correctly defines the DataFrame, calculates the ""EngagementScore"" as specified, filters for Instagram posts, and accurately computes their average engagement score. The code is clean, concise, and uses proper pandas operations.

The only minor point of improvement is that the prompt specified formatting the output to two decimal places for the average engagement score (e.g., `average_engagement_score_instagram:.2f`). The generated code outputs the raw float which, while correct, doesn't perfectly match the output formatting detail in the ground truth's `print` statement. Also, the ground truth includes a check for an empty filtered DataFrame, which is good practice, though not strictly necessary for this specific dataset. Given the task's simplicity, the generated code is highly effective and correct.",llama-3.3-8b-instruct,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male""...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the ""LengthOfStay"" column accurately, filters the DataFrame for ""Flu"" diagnoses, and correctly calculates and prints the average ""LengthOfStay"" for these patients. The code is clean, efficient, and directly addresses all parts of the task without unnecessary complexity. It uses appropriate pandas functionalities for datetime operations, filtering, and aggregation. The output format is also clear and concise.",llama-3.3-8b-instruct,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}
df = pd.DataFrame(data)

# Filter the DataFrame to in...",9,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates their average rating, fulfilling all the requirements of the task. The code is clean, well-structured, and easy to understand. The only minor improvement would be to use an f-string for printing for better formatting and to handle the edge case where no Sci-Fi movies are found (though for this specific dataset, it's not strictly necessary as Sci-Fi movies are present).",llama-3.3-8b-instruct,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

df = pd.DataFrame(data)

# Filter the DataFrame to include only cars of the ""Toyota"" brand
to...",10,"The generated code is correct, complete, and follows good practices. It successfully defines the DataFrame, filters for Toyota cars, and accurately calculates their average mileage. All requirements of the task are met. The code is clean and easy to understand.",llama-3.3-8b-instruct,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 80...",10,"The generated code is entirely correct, complete, and adheres to good code quality practices. It successfully creates the DataFrame, filters it for Fire-type Pokemon, and accurately calculates and prints their average ""Attack"" stat, precisely fulfilling all the task requirements. The variable names are clear, and the logic is straightforward. No improvements are needed for this specific task.",llama-3.3-8b-instruct,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Load the dataset
df = pd.read_csv('titanic.csv')

# Create a multi-panel figure showing survival rates by different demographic groups
fig, axs = plt.subplots(1, 3, figsize=(20, 6))

sns.countplot(x='class', hue='survived', data=df, ax=axs[0])
axs[0].set_title('Survival Rates by Class')

sns.countplot(x='sex', hue='survived', data=df, ax=axs[1])
axs[1].set_title('Survival Rates by G...",6,"The generated code addresses most of the task requirements but lacks in sophistication and data storytelling compared to the ground truth.

**Positives:**
- All required libraries are imported.
- All 5 visualization types are attempted.
- The interactive Plotly scatter plot is correctly implemented and serves its purpose well.
- Basic preprocessing (age grouping) is included.

**Areas for Improvement / Missing Aspects:**

**1. Data Preprocessing:**
    - **Missing Value Handling:** The generated code does not handle missing `Age` values (or any other missing values like `Fare`). This could lead to errors or inaccurate visualizations if not handled. The ground truth fills `Age` with the median.
    - **Data Type Conversion:** While `age_group` is created, `class` and `sex` are used directly. The ground truth converts `Pclass` and `Survived` to more descriptive categorical types (`Pclass_Cat`, `Survived_Cat`) which improves readability on plots.

**2. Multi-panel figure (Survival Rates):**
    - **Incorrect Graph Type (Strength):** The prompt asks for ""survival rates by different demographic groups"". Using `sns.countplot` with `hue='survived'` shows the *counts* of survivors and non-survivors, not the *rates*. To show rates, one should calculate the mean survival rate per group and then use a bar chart. This is a significant conceptual flaw for presenting proportions.
    - **Aesthetic and Readability:** The ground truth calculates and plots the actual survival rates, which is more directly informative and aligns better with the term ""rates"".

**3. Heatmap:**
    - **Correctness:** `pd.crosstab` with `aggfunc='mean'` is an effective way to generate the pivot table for the heatmap, and the heatmap itself is correctly implemented.

**4. Violin Plots:**
    - **Extra Plot:** The generated code creates *two* violin plots (one for age distribution, one for fare distribution). The prompt specifically asked for ""violin plots comparing age distributions of survivors vs non-survivors by gender"". While visualizing fare distribution is not bad, it goes beyond the explicit requirement and lessens focus on the age comparison.
    - **Lack of Detail:** The ground truth uses `split=True` for the violin plot, which is a more compact and elegant way to compare two distributions (survivors vs non-survivors) within the same violin, making direct comparison easier. The generated code places them beside each other, which works but isn't as visually efficient for direct comparison.

**5. Dashboard-style figure:**
    - **Simplistic Layout:** The dashboard layout (`2,2,1` etc.) is very basic. The ground truth uses `gridspec` for a more flexible and robust layout, allowing for variable subplot sizes and better arrangement for storytelling.
    - **Redundancy/Repetition:** The dashboard reuses `countplot` for class and sex, which again shows counts instead of rates. The ground truth introduces new, complementary plots (KDE for age, histogram for fare) and *crucially* adds a textual insights panel, which is a key component of ""data storytelling"" often found in dashboards. The generated dashboard is essentially just a grid of previously shown plots.
    - **Lack of Storytelling:** The most significant missing piece is the narrative and insights. The ground truth explicitly includes a text subplot for ""Key Insights,"" which is vital for a ""comprehensive dashboard-style figure combining multiple visualizations with proper annotations and insights."" The generated code merely combines existing plots without any explicit commentary.

**Overall:**
The generated code provides functional plots that meet most *basic* interpretations of the requirements. However, it falls short on the ""advanced data visualization and storytelling"" aspect. The choice of `countplot` over direct rate visualization, the lack of robust data preprocessing (missing values, proper categorical types), and the limited ""storytelling"" in the dashboard are key areas that prevent a higher score. The ground truth demonstrates best practices in all these areas, leading to more insightful and professional visualizations.",llama-3.3-8b-instruct,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, ...",9,"The generated code correctly defines the DataFrame, filters for Dystopian genre, and accurately calculates the average stock for that genre. It fulfills all the requirements of the task. The code is concise and demonstrates a good understanding of pandas operations.

The only minor area for improvement, as seen in the ground truth, would be to add a check for an empty filtered DataFrame (e.g., `if not dystopian_books.empty:`) before calculating the mean, to handle edge cases gracefully, although for the given data, this is not strictly necessary as there are indeed Dystopian books. Also, formatting the output string to two decimal places (e.g., `{average_stock:.2f}`) would be a good practice for monetary or numerical values, mirroring the ground truth's attention to detail. However, these are minor refinements and do not detract from the core correctness and functionality.",llama-3.3-8b-instruct,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency, shapiro, Cohend
import numpy as np

# Load dataset
df = pd.read_csv('water_potability.csv')

# Conduct t-tests
pH_potable = df[df['potability'] == 1]['pH']
pH_non_potable = df[df['potability'] == 0]['pH']

Hardness_potable = df[df['potability'] == 1]['Hardness']
Hardness_non_potable = df[df['potability'] == 0]['Hardness']

Solids_potable = df[df['potability'] == 1]['Solids']
Solids_non_potable = df[df['potability'...",4,"The generated code attempts to address the task but falls short on several critical aspects, primarily correctness, completeness, and robustness.

**Key Issues:**

1.  **Missing Value Handling:** The most significant omission is the complete lack of any handling for missing values. The `water_potability.csv` dataset is known to have NaNs, and without imputation or dropping, subsequent statistical tests will either fail or produce incorrect results due to `NaN` propagation (e.g., `ttest_ind` will return `NaN` if either input contains `NaN`). The ground truth specifically addresses this with mean imputation.

2.  **Incomplete T-tests:** The generated code only performs t-tests for three specific parameters ('pH', 'Hardness', 'Solids') instead of ""each water quality parameter"" as requested. This misses several other important features in the dataset (e.g., Chloramines, Sulfate, Conductivity, Organic_carbon, Trihalomethanes, Turbidity). The ground truth iterates through all numerical features, which is the correct approach.

3.  **Incorrect Cohen's d Calculation:** The generated code uses `Cohend(t_test_pH.statistic, t_test_pH.pvalue, 0.05)`, which is fundamentally incorrect.
    *   `Cohend` is not a standard function/class in `scipy.stats` or `numpy`. It appears to be a hallucinated or misunderstood function.
    *   Cohen's d is calculated from the means and standard deviations of the two groups, not directly from the t-statistic, p-value, and alpha. The current implementation would raise an `AttributeError` or `NameError`. The ground truth correctly calculates Cohen's d based on the pooled standard deviation and means.

4.  **Incorrect Chi-square Test:**
    *   The task explicitly asks for a ""chi-square goodness-of-fit test to check if potability follows expected distribution."" This typically involves comparing observed frequencies of a single categorical variable against expected frequencies.
    *   The generated code uses `chi2_contingency` with `pd.crosstab(df['potability'], df['Will_it_be_pretty'])`.
        *   `chi2_contingency` is for testing independence between two categorical variables (a chi-square test of independence), not a goodness-of-fit test for a single variable against an expected distribution.
        *   More critically, `Will_it_be_pretty` is not a column in the `water_potability.csv` dataset. This will cause a `KeyError` and the code will crash.
    *   The ground truth correctly implements a `chisquare` (goodness-of-fit) test against a 50/50 expected distribution for the `Potability` column.

5.  **Hardcoded Column Names for Normality/T-tests:** Similar to t-tests, the normality tests are only performed for pH, Hardness, and Solids, rather than all key numerical features. This leads to incompleteness.

6.  **Missing `Cohend` import/definition:** `Cohend` used in the code is not imported or defined, leading to a `NameError`. Even if it were defined, its usage for Cohen's d calculation is incorrect.

7.  **Lack of Robustness:**
    *   No error handling for `FileNotFoundError` when loading the CSV.
    *   No checks for `NaN` or infinite values before performing statistical tests (related to missing value handling).
    *   No consideration for empty groups or very small sample sizes in t-tests that might cause issues.

**Summary of Score Justification:**
The code demonstrates some understanding of statistical tests (importing `ttest_ind`, `shapiro`), but fails significantly in applying them correctly, addressing critical prerequisites (missing values), and fulfilling all aspects of the multi-part task. The `Cohend` error and the incorrect/non-existent column in the chi-square test are major flaws that prevent the code from running successfully or producing meaningful results as intended.",llama-3.3-8b-instruct,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt
import numpy as np

# Load dataset
df = pd...",6,"The generated code addresses most of the requirements but has significant shortcomings in data preprocessing, evaluation completeness, and the professionalism of the output.

**Strengths:**
*   **Multiple Models:** Correctly implements Logistic Regression, Random Forest, and SVM.
*   **Evaluation Metrics:** Calculates accuracy, precision, recall, F1-score, and ROC-AUC.
*   **Confusion Matrices and ROC Curves:** Attempts to generate these, though the visualization is problematic.
*   **K-Fold Cross-Validation:** Implements a basic k-fold CV.
*   **Feature Importance:** Attempts to show feature importance for Random Forest.

**Areas for Improvement / Missing Aspects:**

1.  **Data Preprocessing (Critical Issue):**
    *   **Missing Values:** Uses `df.dropna()` which **removes rows with any missing values**. The ground truth correctly uses `SimpleImputer(strategy='median')` and applies it *after* train-test split (or within a pipeline) to prevent data leakage and retain more data. `dropna()` can lead to significant data loss, especially if missing values are scattered.
    *   **Train-Test Split Stratification:** The generated code uses `train_test_split` without `stratify=y`. For binary classification (especially with potentially imbalanced datasets), stratification is crucial to ensure both train and test sets have representative class distributions. This is done properly in the ground truth.
    *   **Preprocessing in CV (Critical Issue):** In the k-fold cross-validation section, the generated code performs scaling *before* the CV loop on the entire `X` and `y` without using a `Pipeline`. This means the `X` that is split in the k-fold loop is already scaled data. More importantly, it *doesn't* apply imputation or scaling separately within each fold. This is a severe data leakage issue. Preprocessing (imputation and scaling) should either be part of a `Pipeline` passed to `cross_val_score` or manually applied (`fit_transform` on train fold, `transform` on validation fold) within each fold. The ground truth correctly demonstrates the pipeline approach for CV.

2.  **Visualization:**
    *   **Confusion Matrix:** The generated code calculates the confusion matrix and prints it as an array. It does not *display* it as a readable heatmap, which was requested.
    *   **ROC Curve:** It generates the `fpr`, `tpr`, and plots it, but calls `plt.show()` *inside* the loop for each model, resulting in separate plots for each model rather than a combined plot on one axes for comparison (as done in the ground truth). The labeling and presentation are also basic.

3.  **Completeness/Robustness:**
    *   **SVM `predict_proba`:** For SVM, `probability=True` must be set during model initialization to get `predict_proba`. The generated code does not include this, so `model.predict_proba(X_test)` would fail for SVM.
    *   **Feature Importance for non-tree models:** Only checks for 'Random Forest'. The ground truth also provides coefficients for Logistic Regression and a disclaimer for SVM, which is more comprehensive.
    *   **Model Selection Criteria:** Selects the best model based solely on accuracy from the CV loop, which might not be the best single metric for potentially imbalanced datasets. The ground truth uses F1-score from CV, which is often more robust.
    *   **Missing Imports:** `ConfusionMatrixDisplay`, `RocCurveDisplay` (for better visualization), `SimpleImputer`, `StratifiedKFold` (for stratified CV).
    *   **Code Structure/Comments:** The ground truth has clear section headers and comments, making the code much easier to follow. The generated code is more monolithic.
    *   **Error Handling:** Lacks basic error handling for file loading.
    *   **Reproducibility:** While `random_state` is used for `train_test_split` and `KFold`, it's missing for the `RandomForestClassifier` and `SVC` initializations, which means results might vary between runs.

Overall, the generated code attempts to fulfill the requirements but falls short on crucial best practices for data science workflows, particularly in preprocessing and cross-validation, leading to potentially misleading results.",llama-3.3-8b-instruct,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('water_potability.csv')

# Create new features
df['pH_Chloramines_ratio'] = df['pH'] / df['Chloramines']
df['Hardness_Solids_ratio'] = df['Hardness'] / df['Solids']
df['pH_squared'] = df['pH'] ** 2
df['Chloram...",6,"The generated code addresses most of the requirements but often with limited depth or correctness.

**Strengths:**
*   **Basic Structure:** It correctly loads data, attempts feature engineering, correlation analysis, and visualization.
*   **Import Statements:** All necessary libraries are imported.
*   **Visualization Attempts:** It does try to generate several types of plots.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling:** The code completely ignores missing values, which are present in the `water_potability.csv` dataset. This will lead to errors or incorrect results when performing calculations or visualizations on columns like 'pH', 'Sulfate', or 'Trihalomethanes'. The ground truth correctly imputes these.
2.  **Limited Feature Engineering:**
    *   The prompt specifically asked for ""meaningful features such as ratios between chemical parameters (e.g., pH/Chloramines, Hardness/Solids), interaction terms, and polynomial features.""
    *   The generated code only creates two ratio features and one polynomial feature (pH\_squared) and one interaction term. This is far less extensive than what the prompt implies and what the ground truth provides (multiple ratios, multiple interactions, and a more comprehensive set of polynomial features).
    *   The division by zero issue for ratio features is not handled (e.g., `df['Chloramines']` could be zero). The ground truth includes a small epsilon.
3.  **Correlation Analysis:**
    *   `top_5_features` calculation is slightly off. `corr_with_potability.index[1:6]` will give you the top 5 most correlated features *including* positive and negative correlations, but the prompt asks for ""top 5 features *most* correlated"" which implies absolute correlation. The ground truth correctly uses `.abs()`.
    *   The `corr_matrix` is calculated *before* the engineered features are added for some columns, so the initial `corr_matrix` will not reflect all engineered features, potentially causing issues down the line. However, the engineered features are added to `df` itself, so for the final correlation calculation, it *will* include them.
4.  **Feature Selection:**
    *   **SelectKBest Output:** `selector.get_support(indices=True)` returns numerical indices, not the names of the features, making the output less interpretable. The ground truth uses `selector.get_support()` with column names.
    *   **RFE Estimator:** `RFE(RandomForestClassifier(), ...)` is missing `estimator=` keyword argument, although it might still run positionally. More importantly, `n_features_to_select` is fixed at 5, which may not be optimal. The ground truth also explicitly sets `random_state` for reproducibility.
    *   **Scaling:** Feature selection methods, especially those based on statistical tests (like F-classif) or distance-based algorithms (though less critical for tree-based like RF, it's good practice for consistency), often benefit from feature scaling. The ground truth correctly applies `StandardScaler`.
5.  **Visualizations:**
    *   **Correlation Heatmap:** `annot=True` makes the heatmap unreadable with many features. The ground truth correctly uses `annot=False` for the full matrix and provides `fmt="".2f""` if annotation is desired.
    *   **Feature Importance:** The bar plot of ""Features Correlated with Water Potability"" is **not** a feature importance plot. It's just a bar plot of correlation coefficients. The prompt specifically asked for ""feature importance plots"" (from a model like RandomForest, as shown in the ground truth) and ""pairwise scatter plots"".
    *   **Pairwise Scatter Plots:** The generated code produces separate scatter plots for each of the top 5 features against `Potability`. The prompt asked for ""pairwise scatter plots for top correlated features,"" which typically implies a `seaborn.pairplot` that shows all combinations among those top features (including their distributions and their relationships with each other, not just against the target). The ground truth correctly uses `pairplot`.
    *   **Plot Titles/Labels:** While titles are present, more descriptive labels, axes adjustments, and overall plot aesthetic (e.g., Seaborn style) would improve quality.
6.  **Comparison of Feature Sets:** The comparison is very minimal, only printing the raw support arrays/indices. The prompt asks for an analysis of ""overlap"" and comparison, which goes beyond just printing whether a feature was selected. The ground truth provides detailed overlap and uniqueness analysis.
7.  **Readability/Robustness:** The ground truth includes a `try-except` block for file loading and `epsilon` for ratio calculations, making it more robust.",llama-3.3-8b-instruct,LLM_judge_config
