task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd

# Assume 'winequality-red.csv' is in the current directory
file_path = 'winequality-red.csv'

# Load the dataset
try:
    df = pd.read_csv(file_path)
except FileNotFoundError:
    print(f""Error: The file '{file_path}' was not found. Please ensure it is in the current directory."")
    exit()

print(""--- Wine Quality Dataset Analysis ---"")

# 1. List all column names and identify the data type of each column
print(""\n1. Column Names and Data Types:"")
print(""---------...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, identifies data types, calculates descriptive statistics for numerical columns, and reports dataset dimensions. The identification of categorical/ordinal columns is particularly sophisticated, offering detailed reasoning for each column based on its data type and specific domain knowledge for the 'quality' column.

**Strong Points:**
- **Robust CSV Loading:** Includes error handling for `FileNotFoundError`.
- **Detailed Data Type Identification:** The code goes beyond just Pandas dtypes; it attempts to interpret the type (e.g., Numerical (Continuous), Numerical (Ordinal)) which is a valuable addition.
- **Sophisticated Categorical/Ordinal Identification:** It correctly identifies 'quality' as ordinal based on domain knowledge and provides excellent heuristics for other potential categorical/ordinal columns (e.g., low cardinality integer columns). It also correctly handles Pandas `object`, `string`, and `category` dtypes.
- **Clear Output Formatting:** The print statements are well-organized and easy to read, especially the column name and data type table.
- **Correct Descriptive Statistics:** Uses `df.describe()` appropriately for numerical columns.
- **Correct Dimension Reporting:** Accurately reports rows and columns.

**Areas for Minor Improvement (to reach a 10):**
- **Redundant `interpreted_width` and `dtype_width` reassignment:** The lines `dtype_width = 12` and `interpreted_width = 25` inside the ""List all column names and identify the data type of each column"" section are redundant since they are already initialized correctly with `len(header_dtype)` and `len(header_interpreted)` and then updated based on content. While not an error, it's slightly less efficient. A general approach would be to dynamically calculate the max length for `str(dtype)` as well, rather than fixing it to 12.
- **Slightly Over-complex Column Type Identification:** While the detailed interpretation is good, the prompt for ""categorical/ordinal"" initially implied a simpler classification that just identifies non-numerical columns or those that *appear* categorical. The generated code also includes `is_integer_dtype` and `is_float_dtype`, which could be simplified by using `pd.api.types.is_numeric_dtype` as a first check, then drilling down for `integer` vs `float` if needed. However, the current approach is not incorrect, just a bit more verbose than strictly necessary for basic classification. The ground truth used `pd.api.types.is_numeric_dtype` and then a single `elif` for `is_object_dtype` or `is_categorical_dtype`, which is cleaner for the fundamental classification, although less analytical than the generated code's more in-depth type interpretation.

Overall, an excellent solution that not only meets but often exceeds the prompt's requirements in terms of analytical depth.",gemini-2.5-pro-preview,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
try:
    df = pd.read_csv('video_games_sales.csv')
    print(""1) First 10 rows of the dataset:"")
    print(df.head(10))
except FileNotFoundError:
    print(""Error: 'video_games_sales.csv' not found in the current directory."")
    exit()

print(""\n"" + ""-""*50 + ""\n"")

# 2) Show the shape of the dataset (rows and columns)
print(""2) Shape of the dataset (rows, columns):"")
print(df.shape)

print(""\n"" + ""-""*50 + ""\n"")

...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It properly loads the dataset, displays information, identifies top games, shows unique values, calculates summary statistics, and finds missing values.

Strengths:
- Correctly implements all requested EDA steps.
- Handles `FileNotFoundError` for robust file loading.
- Uses `df.head(10)`, `df.shape`, `df.info()`, `df.sort_values().head(10)`, `df['column'].unique()`, `df[columns].describe()`, and `df.isnull().sum()` effectively for each task.
- Clean and readable code structure with clear print statements.

Areas for minor improvement (to reach a perfect 10):
- For ""Missing values in the dataset"", `df.isnull().sum()` shows all columns, even those with zero missing values. The ground truth code filters this to only show columns with actual missing values, which is often more useful in EDA.
- The ground truth code adds checks (`if 'ColumnName' in df.columns:`), especially for columns like `Global_Sales`, `Platform`, `Genre`, and the sales columns to prevent errors if a critical column is missing from the dataset. While less common for well-known datasets, it makes the code more robust to unexpected data variations.
- The `df.info()` output already includes column names and data types, so the `print(df.info())` line in section 3 will also print `None` because `info()` returns `None`. It's generally just called without printing its return value, as its purpose is to print to the console directly. (Though this is a very minor point and doesn't affect functionality).",gemini-2.5-pro-preview,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd

# 1. Load the dataset
try:
    df = pd.read_csv('video_games_sales.csv')
except FileNotFoundError:
    print(""Error: 'video_games_sales.csv' not found in the current directory."")
    exit()

# 2. Handle missing values in the Year column
# Convert 'Year' to numeric, coercing errors (e.g., 'N/A' becomes NaN)
df['Year'] = pd.to_numeric(df['Year'], errors='coerce')
median_year = df['Year'].median()
df['Year'].fillna(median_year, inplace=True)

# Ensure 'Year' is intege...",8,"The generated code correctly addresses most of the requirements and produces a valid data cleaning pipeline.

**Strong Points:**
- Correctly loads the dataset and handles `FileNotFoundError`.
- Accurately converts 'Year' to numeric and fills missing values with the median.
- Correctly handles the conversion of 'Year' to integer type after filling NaNs.
- Successfully removes missing publisher information and duplicate games.
- Correctly creates the 'Years_Since_Release' column.
- Filters out games with `Global_Sales` less than 0.1 million.
- Displays the final shape and head as required.

**Areas for Improvement (Why not a 10):**

1.  **Robustness of `Year` handling (Minor):** The ground truth handles a potential edge case where `Years_Since_Release` might become negative if the calculated year is greater than `CURRENT_YEAR` (e.g., if median year is 2025 and `CURRENT_YEAR` is 2023). While unlikely with real release years, it's a good practice for robustness. The generated code calculates `current_year - df['Year']`, which could result in negative values if `df['Year']` is greater than `current_year`. The ground truth includes `df['Years_Since_Release'] = df['Years_Since_Release'].apply(lambda x: max(0, x))` to address this.
2.  **Handling of `Global_Sales` (Minor):** The ground truth explicitly converts `Global_Sales` to numeric with `errors='coerce'` and then drops NaNs before filtering. This makes the filtering step more robust against non-numeric values in the `Global_Sales` column. The generated code assumes `Global_Sales` is already numeric or will be implicitly handled by the comparison, which might lead to errors or unexpected behavior if there are non-numeric strings in that column.
3.  **No `numpy` import/usage:** While not strictly needed for the `df.median()` method, `numpy` is often imported alongside `pandas` in data science tasks and might have been used for other potential operations (e.g., `np.nan`). Its absence is not an error but a minor deviation from common practice, given the common use of `pandas` and `numpy` together.
4.  **Implicit Prints (Ground Truth Advantage):** The ground truth provides more informative print statements throughout the process, showing the shape after each major transformation. This is excellent for debugging and understanding the pipeline's effects. The generated code only shows the final shape. While not explicitly asked for in the prompt, it significantly improves code clarity and debuggability in a real-world scenario.",gemini-2.5-pro-preview,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. Load the dataset
# Assuming 'video_games_sales.csv' is in the current directory.
df = pd.read_csv('video_games_sales.csv')

# Preliminary Data Cleaning and Type Conversion
# Convert 'Year' to numeric. Errors during conversion will become NaN.
df['Year'] = pd.to_numeric(df['Year'], errors='coerce')

# Convert sales columns to numeric. Errors will become NaN.
sales_cols = ['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global...",9,"The generated code largely fulfills all requirements of the task. It correctly loads the dataset, performs necessary data cleaning (though some improvements could be made for robustness), and generates all specified visualizations with proper titles and labels.

**Strong Points:**
- All 6 requested visualizations (bar, pie, histogram, scatter, line) are generated.
- Plots include appropriate titles and x/y labels.
- Uses `matplotlib.pyplot` as requested.
- Includes basic data cleaning for numeric conversions.
- Effective use of `plt.tight_layout()` for better plot presentation.
- Good use of `figsize`.
- The histogram logic for capping sales at the 98th percentile is a good practical improvement for skewed data.
- The `try-except` block for file loading in the ground truth is more robust, but the generated code's initial approach is common for simpler scripts.

**Areas for Improvement (Why not a full 10):**
- **Robustness in Data Loading/Cleaning:** The ground truth code includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice. The generated code assumes the file exists.
- **Consistency in Column Name:** The ground truth code assumes `Year_of_Release` for the year column, which is more common in `vgsales.csv`. The generated code uses `Year`. While this might work for some datasets, it's a potential point of failure if the column name differs. **Self-correction: The problem statement says `video_games_sales.csv`, which commonly has `Year`, not `Year_of_Release`. So, the generated code's choice is valid for the file name.**
- **Redundant `dropna` for each plot:** While creating specific dataframes for each plot (`df_platform_sales`, `df_genre_dist`, etc.) by dropping NaNs ensures that each plot operates on clean data for its specific columns, it also creates many temporary dataframes. A more efficient approach, as seen in the ground truth, is to perform a more comprehensive cleanup of critical columns globally once, then reuse the cleaned `df`. However, the generated code's approach is not incorrect, just less efficient for very large datasets.
- **Pie Chart ""Other"" Category:** The ground truth more thoughtfully handles the pie chart by combining small categories into an ""Other"" slice, which is a common and good practice to avoid overcrowding the pie chart with too many tiny slices. The generated code plots all genres, which can be less readable if there are many.
- **Specific Year Filtering for Line Plot:** The ground truth applies a general filter `(df['Year_of_Release'] >= 1980) & (df['Year_of_Release'] <= 2020)`. The generated code uses `1980-2016`, which is very specific to the typical range of the `vgsales.csv` dataset, showing good domain awareness. However, stating in the comments why this range is chosen (`sparse/incomplete data outside this`) would make it even clearer.

Overall, the generated code is very good and produces correct and well-formatted plots as requested. The minor improvements are mostly about robustness and best practices that a more seasoned data scientist might apply, rather than core functionality.",gemini-2.5-pro-preview,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd

def perform_eda():
    # --- 1. Load the dataset and display the first 10 rows ---
    print(""--- 1. Loading dataset and displaying first 10 rows ---"")
    try:
        df = pd.read_csv('diabetes.csv')
    except FileNotFoundError:
        print(""Error: 'diabetes.csv' not found in the current directory."")
        print(""Please make sure the file is in the same directory as the script."")
        return
    
    print(""First 10 rows of the dataset:"")
    print(df.hea...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong Points:**
- **Correctness:** All requested actions (loading, shape, dtypes, outcome percentage, missing values, summary stats, zero value checks) are performed accurately.
- **Completeness:** Covers all 7 points outlined in the task description.
- **Error Handling:** Gracefully handles `FileNotFoundError` for the CSV, which is a good practice.
- **Clarity and Readability:** The print statements clearly delineate each section, making the output easy to follow. The inclusion of additional context for ""Outcome"" 0 and ""Pregnancies"" 0 is helpful.
- **Modularity:** Encapsulating the EDA logic within a `perform_eda` function is good practice.
- **Specific Zero Value Check:** Correctly identifies the commonly problematic columns for zero values in the diabetes dataset.

**Areas for Minor Improvement (preventing a perfect 10):**
- **Slightly verbose output for zero values:** While helpful, presenting the ""For context, zero counts in other relevant columns:"" part for Pregnancies and Outcome could be integrated more cleanly or conditionally, as those are already explicitly excluded from the 'suspicious zero' check. The ground truth only prints warnings for the designated 'suspicious' columns.
- **Consistency in output spacing/separators:** The separator `""=""*70` is used, whereas the ground truth uses `""=""*50`. This is a minor aesthetic point.
- **`df.info()` output:** While `df.info()` is correct for task 3, it prints directly to `stdout` and includes boilerplate like `RangeIndex` and `Memory usage`, which might be considered slightly less ""clean"" if the goal was *only* column names and dtypes. The ground truth leverages this while maintaining a clean look around it.

Overall, an excellent submission that demonstrates strong understanding and implementation skills for exploratory data analysis.",gemini-2.5-pro-preview,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# 1. Load the dataset
try:
    df = pd.read_csv('diabetes.csv')
except FileNotFoundError:
    print(""Error: 'diabetes.csv' not found in the current directory."")
    exit()

# 2. Identify columns with zero values that represent missing data
cols_to_replace_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

# 3. Replace zero values in these columns with NaN
df[cols_to_replace_ze...",8,"The generated code largely fulfills the requirements of the task.

**Strong points:**
*   Correctly loads the dataset and handles `FileNotFoundError`.
*   Accurately identifies and replaces zeros with NaN in the specified columns.
*   Correctly fills NaN values with the median of each column.
*   Successfully removes duplicate rows.
*   Correctly creates the `BMI_Category` column using `pd.cut` with appropriate bins and labels.
*   Applies `StandardScaler` to numerical features.
*   Displays the shape and head of the cleaned dataset as required.

**Areas for improvement:**
1.  **Robustness in column handling (Step 3 & 4):** The ground truth code includes checks (`if col in df.columns:`) when replacing zeros and filling NaNs. The generated code directly applies `replace` and `fillna` to `cols_to_replace_zeros` without checking if all columns exist. While `diabetes.csv` likely contains these columns, more robust code would handle cases where a column might be missing.
2.  **`BMI_Category` bin definition (Step 6):** The `bmi_bins` definition uses `-np.inf` and `np.inf` correctly. However, the `right=False` argument means intervals are `[left, right)`. For 'Normal' (18.5-24.9), the bin `[18.5, 25.0)` is correct with `right=True` (which is default for `pd.cut`) or with `right=False` and bins like `[18.5, 24.999]`. But with `right=False`, `[18.5, 25.0]` would capture 25.0 which is not desired for 'Normal'. The ground truth with `bins = [0, 18.5, 24.9, 29.9, np.inf]` and `right=True` correctly defines the ranges including the upper bound in the bin (e.g., `<18.5`, `[18.5, 24.9]`, etc.). The generated code's definition `[18.5-24.9]: Normal ('Normal') -> [18.5, 25.0)` with `right=False` means `[18.5, 25.0)` will include 18.5 but exclude 25.0, which is functionally equivalent to `[18.5, 24.9]` if 24.9 is included in `Normal`. However, the definition for Overweight `[25.0, 30.0)` for 25-29.9 and Obese `[30.0, +inf)` for `>=30` is slightly off. The ground truth's `[0, 18.5, 24.9, 29.9, np.inf]` with `right=True` is more standard for these ranges (e.g., 24.9 falls into Normal, 25.0 into Overweight).
    *   Underweight (<18.5): Ground Truth `(0, 18.5]` - `right=True`
    *   Normal (18.5-24.9): Ground Truth `(18.5, 24.9]` - `right=True`
    *   Overweight (25-29.9): Ground Truth `(24.9, 29.9]` - `right=True`
    *   Obese (>=30): Ground Truth `(29.9, np.inf]` - `right=True`

    The generated code's `right=False` means `[left, right)`.
    *   Underweight (<18.5): `[-np.inf, 18.5)` - is correct as per instructions.
    *   Normal (18.5-24.9): `[18.5, 25.0)` - includes 18.5, excludes 25.0. This range definition technically excludes 24.9, if integers are used for categories. It should be `[18.5, 24.9]` or `[18.5, 25.0)` with `right=False`. The prompt specifies 18.5-24.9. A common way `pd.cut` works is to define the upper bound of the category. If using `right=True`, `(18.5, 24.9]` which includes 24.9 would be more precise.
    *   Overweight (25-29.9): `[25.0, 30.0)` - includes 25.0, excludes 30.0. This is fine.
    *   Obese (>=30): `[30.0, np.inf)` - includes 30.0. This is fine.

    The main discrepancy is the `Normal` category, where (18.5-24.9) means 24.9 is included. GT's `[18.5, 24.9]` with `right=True` would include 24.9. Gen code's `[18.5, 25.0)` with `right=False` would define Normal as `18.5 <= BMI < 25.0`. This is a minor semantic difference but worth noting for precision.

3.  **Numerical feature scaling (Step 7):** The generated code explicitly lists numerical features. While this works, it hardcodes the feature names. The ground truth code dynamically identifies numerical columns using `df.select_dtypes(include=np.number).columns.tolist()` and then removes `Outcome`, which is more robust to changes in the dataset's numerical columns (e.g., if a new numerical column is added, or an existing one removed, without affecting the target). The provided list in the generated code is correct for this dataset, but the dynamic approach is better practice. Also, it adds a warning for when `existing_numerical_features` is empty, which is good.
4.  **Clarity for output (Step 8):** The ground truth prints the `original_shape` for context, which is helpful for evaluation. The generated code does not include this.",gemini-2.5-pro-preview,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
df = pd.read_csv('diabetes.csv')

# Identify numerical features
numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
features_for_boxplots = [col for col in numerical_features if col != 'Outcome']

# 2) Create a correlation heatmap
plt.figure(figsize=(12, 10))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt="".2f...",9,"The generated code is highly correct and complete, successfully addressing almost all requirements of the task.

**Strong Points:**
- **Correctness and Completeness:** All 7 types of visualizations are generated correctly.
- **Proper Imports:** Necessary libraries (pandas, matplotlib, seaborn) are imported.
- **Appropriate Plot Types:** The code correctly uses `sns.heatmap`, `sns.histplot`, `sns.boxplot`, `plt.pie`, `sns.scatterplot`, and `sns.pairplot` for the respective tasks.
- **Good Formatting:** Titles, labels, and legends are generally well-implemented for clarity.
- **Handling of Numerical Features:** The identification of numerical features for histograms and box plots is robust.
- **Specific Scatter Plots:** The dual scatter plots for BMI vs Glucose and Age vs Pregnancies are correctly implemented.
- **Customized Legends and Sizing:** The scatter plots and pair plot have customized legends and appropriate figure sizes.
- **Detailed Pie Chart:** The pie chart correctly sorts and labels the outcomes.

**Areas for Improvement (Minor):**
- **Error Handling for File Loading:** The ground truth code includes robust error handling for `FileNotFoundError` when loading the CSV, which is a good practice for generalizable scripts. The generated code assumes the file exists.
- **Single Figure for Histograms:** The ground truth code generates histograms for numerical features on a single figure using subplots, which makes for a more compact and comparable display. The generated code creates a new figure for each histogram, leading to many individual plot windows. This isn't strictly incorrect but less efficient for viewing.
- **Single Figure for Box Plots:** Similar to histograms, creating a single figure with subplots for all box plots (as in the ground truth) would be more efficient for comparison than individual figures.
- **Consistency in Color Palettes:** While custom palettes are used, for scatter plots and pair plots, using a consistent classification palette (e.g., `coolwarm` or `viridis` as in the ground truth) across all relevant plots might offer better visual cohesion, though the chosen colors are not incorrect.
- **Handling of Missing 'Outcome' in Numerical Features:** For the correlation heatmap, the ground truth explicitly uses `df[numerical_cols].corr()`, where `numerical_cols` excludes 'Outcome' to show correlations *between features*. The generated code uses `df.corr()`, which includes 'Outcome' in the correlation matrix, showing its correlation with other features, which is also valuable but slightly different from ""relationships between all numerical variables"" if 'Outcome' is considered a target rather than a feature for that specific visualization. Both approaches are defensible, but the ground truth's approach specifically targets feature-feature correlations for the heatmap.

Overall, the generated code is very good and largely fulfills the requirements, with only minor stylistic and efficiency improvements possible.",gemini-2.5-pro-preview,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
try:
    df = pd.read_csv('world-happiness-report-2021.csv')
    print(""1) First 10 rows of the dataset:"")
    print(df.head(10))
    print(""\n"" + ""=""*50 + ""\n"")

    # 2) Show the shape of the dataset (rows and columns)
    print(""2) Shape of the dataset (rows, columns):"")
    print(df.shape)
    print(""\n"" + ""=""*50 + ""\n"")

    # 3) List all column names and their data types
    print(""3) Column names and their ...",8,"The generated code largely fulfills all requirements of the task.

**Strong Points:**
- **Correctness:** All tasks are correctly implemented.
- **Completeness:** All 7 required tasks are addressed.
- **Error Handling:** Includes `try-except` blocks for `FileNotFoundError` and general exceptions, which is good practice.
- **Clarity:** Uses print statements to clearly label each section of the analysis.
- **Robustness (GDP column):** The generated code attempts to handle variations in the GDP column name (`Logged GDP per capita`, `GDP per capita`, `Explained by: Log GDP per capita`), which indicates thought about data variability.
- **Output of `df.info()`:** The note about `df.info()` printing `None` is insightful and indicates understanding of how `df.info()` works.

**Areas for Improvement:**
- **Redundant `df.info()` print:** `print(df.info())` prints `None` after displaying the info summary. While the comment explains it, it's better to just call `df.info()` without wrapping it in `print()`.
- **Clarity of `df.info()` output:** The ground truth directly calls `df.info()` which is usually preferred as `df.info()` has its own formatting.
- **Consistency in Separators:** The ground truth uses `print(""\n"" + ""=""*70 + ""\n"")` for separators, while the generated code uses `print(""\n"" + ""=""*50 + ""\n"")`. While not a critical issue, consistency is generally preferable.
- **Redundant `unique()` call for Regional Indicator:** For task 4, the requested output is ""unique regional indicators and count of countries per region"". The generated code first prints `df['Regional indicator'].unique()` and then `value_counts()`. `value_counts()` inherently shows the unique values as its index, so printing `unique()` separately is redundant for the task's requirement.
- **Top 10 Happiest Countries output:** The generated code includes 'Regional indicator' in the output for the top 10 happiest countries, which was not explicitly requested. While potentially useful, adherence to the prompt is generally better.
- **Structured Error Handling:** The ground truth exits after a `FileNotFoundError`, which is a more appropriate action as subsequent operations would fail anyway. The generated code falls through to general error handling with `print` which might lead to further errors or confusing output.

Overall, the generated code is very good and provides a comprehensive solution to the task. The suggested improvements are minor refinements.",gemini-2.5-pro-preview,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder

# 1) Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# 2) Handle any missing values
# Identify numerical and categorical columns from the original dataframe
numerical_cols = df.select_dtypes(include=np.number).columns
categorical_cols = df.select_dtypes(include='object').columns

# Fill missing values for numerical columns with mean
for col in numerical_cols:
   ...",9,"The generated code is very good and largely fulfills all the requirements of the task.

**Strong Points:**
- **Correctness:** All core functionalities (loading, missing values, derived features, outlier removal, scaling, encoding) are implemented correctly.
- **Completeness:** All specified requirements are addressed.
- **Code Quality:**
    - Proper imports are included.
    - The code is well-structured and follows a logical flow, aligning with the task's numbered steps.
    - Handling of missing values is robust, correctly differentiating between numerical and categorical columns.
    - The creation of derived features (`Happiness_Category`, `GDP_Category`, `Well_being_Score`) is accurate.
    - Outlier removal using IQR is correctly implemented.
    - Standard scaling and label encoding are applied to the correct features.
    - Defensive programming (e.g., checking for column existence before processing, checking for numerical features to scale) is a good practice.
    - `df.reset_index(drop=True, inplace=True)` after outlier removal is a good touch to maintain a clean index.
    - Using `.to_string()` for `df.head()` output is a minor but nice improvement for cleaner printing.

**Areas for Minor Improvement (to reach a perfect 10):**
- **File Not Found Error Handling:** The ground truth code includes a `try-except` block for `FileNotFoundError` when loading the dataset, which is a good practice for robust code that might be deployed or run in various environments. The generated code does not include this. While not explicitly asked for, it demonstrates a more complete solution.
- **`GDP_Category` labels:** The generated code uses `'Q1 (Lowest)', 'Q2', 'Q3', 'Q4 (Highest)'` as labels for `pd.qcut`. While functionally correct, the task merely asked for `'Q1', 'Q2', 'Q3', 'Q4'`. The added ""Lowest/Highest"" is an unnecessary embellishment that deviates slightly from the exact specification, albeit non-critically. The `duplicates='drop'` argument helps ensure robustness in `pd.qcut` in cases where bin edges might not be unique (e.g., if many values are identical), which is a good addition.
- **Order of `Happiness_Category` conditions:** The generated code defines conditions for 'Low' then 'Medium' then 'High'. The ground truth defines 'High' then 'Medium' then 'Low'. Both are valid as `np.select` applies conditions in order and stops at the first true one. However, starting with the outer bounds (`<4` and `>6.5`) first might be slightly more intuitive for clarity, but this is a very minor stylistic point.

Overall, an excellent solution that clearly demonstrates a strong understanding of data preprocessing in Python.",gemini-2.5-pro-preview,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import numpy as np
from math import pi
from sklearn.preprocessing import MinMaxScaler

# 1. Load the dataset
try:
    df = pd.read_csv('world-happiness-report-2021.csv')
except FileNotFoundError:
    print(""Error: 'world-happiness-report-2021.csv' not found. Make sure the file is in the current directory."")
    exit()

# --- Visualizations ---

# 2. World map visualization showing happ...",9,"The generated code is largely correct and addresses almost all requirements outlined in the prompt. It successfully creates all the requested visualization types with appropriate libraries (Plotly, Matplotlib, Seaborn).

**Strong Points:**
*   **Correctness and Completeness:** All 7 visualization types are generated, and most are well-formatted with titles and labels.
*   **Data Loading:** Handles `FileNotFoundError` gracefully.
*   **Map Visualization:** Uses `plotly.express` correctly for the choropleth map.
*   **Bar Chart:** Correctly calculates and plots average happiness by region.
*   **Correlation Heatmap:** Correctly selects relevant columns and plots the heatmap with annotations.
*   **Scatter Plots:** Generates both requested scatter plots.
*   **Box Plots:** Effectively displays distributions by region, and even orders them by median score for better readability.
*   **Radar Chart:** This is often a complex visualization, and the code correctly implements it by normalizing values, selecting top countries, and plotting the profiles.

**Areas for Improvement (minor points):**
*   **Column Renaming:** The ground truth renames 'Ladder score' to 'Happiness Score' for consistency and better readability across all plots. The generated code does not do this, leading to 'Ladder score' appearing in plot titles and labels, which is less user-friendly than 'Happiness Score'. While not strictly a correctness issue, it impacts consistency and readability.
*   **Separate Scatter Plots:** The prompt implicitly asks for ""scatter plots"" (plural), and the ground truth combines them into a single figure with subplots, which is a more efficient use of space and aligns with common visualization practices when showing related plots. The generated code creates them as two separate `plt.figure()` calls, which is acceptable but less integrated.
*   **Radar Chart Legend Position:** The `bbox_to_anchor` for the radar chart legend `(0.1, 0.1)` places it inside the plot, obscuring some chart data. The ground truth places it outside to avoid this. This is a minor aesthetic point.
*   **Radar Chart Tick Labels:** The yticklabels for the radar chart are dynamically generated, but `ax.set_yticklabels_position(1.0)` is deprecated in newer Matplotlib versions. Also, the `str(round(x,1))` approach can lead to slightly less clean labels than simply letting Matplotlib handle them if no custom formatting is strictly needed.

Overall, the generated code is very good and demonstrates a strong understanding of Python visualization libraries for data science tasks. The points for improvement are minor enhancements rather than critical errors.",gemini-2.5-pro-preview,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd
import sys

def perform_eda():
    # 1) Load the dataset and parse the 'date' column as datetime objects.
    try:
        df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
    except FileNotFoundError:
        print(""Error: 'synthetic_covid19_data.csv' not found. Please ensure it's in the current directory."")
        sys.exit()
    except Exception as e:
        print(f""Error loading or parsing the CSV file: {e}"")
        sys.exit()

    print(""E...",8,"The generated code correctly addresses most of the requirements and demonstrates good code quality and structure.

**Strong Points:**
*   **Correctness:** It correctly loads data, parses dates, displays head/shape/info, calculates continental sums, identifies top locations, calculates summary statistics, and counts missing values.
*   **Robustness:** Includes `try-except` blocks for file loading, which is good practice.
*   **Clarity:** Uses descriptive print statements to clearly delineate each section of the EDA.
*   **Modularity:** Encapsulates the EDA logic within a function (`perform_eda`), which is good for larger scripts.

**Areas for Improvement (to reach a perfect 10):**
*   **Robustness for Grouped Aggregation (Continental Summary):** The generated code `df.groupby('continent')[required_cols_agg].sum()` does not explicitly handle `NaN` values in the 'continent' column. If 'continent' has NaNs, `groupby` will create a group for `NaN`, which might not always be desired for aggregation. The ground truth correctly uses `df.dropna(subset=['continent'])` to ensure only valid continents are considered for summation.
*   **Robustness for Top Locations 'total_cases':** The generated code for finding top locations assumes 'total_cases' is numeric and doesn't explicitly convert it using `pd.to_numeric` with `errors='coerce'`, which is safer if the column might contain non-numeric data. The ground truth includes this. While `sort_values` might implicitly handle this by raising an error or casting, explicitly coercing to numeric is better practice.
*   **Top Locations Logic:** The generated code uses `idxmax()` on `date` after grouping by `location`. While this correctly finds the index of the latest date, the ground truth's approach of `sort_values` then `drop_duplicates(subset=['location'], keep='last')` is a very common and robust pattern for this exact task, often preferred for its explicit handling of tie-breaking (though not strictly necessary here given dates are unique per day). Both achieve the same result for this specific case, but the ground truth's approach might be slightly more idiomatic for ""get the latest record per group"".
*   **Summary Statistics (`.describe()` vs. individual `agg`):** The prompt requested ""mean, median, min, max, std"". While `.describe()` provides these along with count and quartiles, the ground truth explicitly uses `.agg(['mean', 'median', 'min', 'max', 'std'])`, which aligns more precisely with the list of requested statistics. Both are acceptable, but `.agg` is more direct to the specific request.
*   **Error Handling/Messaging Consistency:** The `sys.exit()` calls are good, but the Ground Truth's approach might be slightly more user-friendly by printing an error and then `exit()` without requiring `sys` import, which is a minor point.

Overall, the generated code is very good and largely fulfills the task requirements effectively. The suggested improvements are incremental for robustness and precise adherence to phrasing.",gemini-2.5-pro-preview,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
# Standard winequality-red.csv uses semicolon as a separator
try:
    df = pd.read_csv('winequality-red.csv', sep=';')
except FileNotFoundError:
    print(""Error: 'winequality-red.csv' not found. Make sure the file is in the current directory."")
    exit()
except Exception as e:
    print(f""Error loading CSV file: {e}"")
    print(""Please ensure the file is a semicolon-separated CSV."")
    # At...",8,"The generated code correctly addresses all three main requirements of the task: calculating mean quality, grouping by quality for average alcohol, and creating a bar chart visualization.

**Strengths:**
*   **Correctness:** All calculations (mean quality, grouped average alcohol) are performed correctly.
*   **Imports:** Proper imports for `pandas`, `matplotlib.pyplot`, and `seaborn` are included.
*   **Visualization:** A bar chart is correctly generated using seaborn, visualizing the relationship between quality and alcohol.
*   **Robustness (partial):** The `try-except` block for file loading is a good addition, attempting to handle `FileNotFoundError` and even hinting at potential `sep=';'` issues, which is common for this dataset. This makes it more robust than the ground truth in this specific aspect.
*   **Readability:** The code is well-structured and easy to read.

**Areas for Improvement (to achieve a higher score):**
*   **CSV Separator:** While the generated code attempts to address the `sep=';'` issue, the ground truth explicitly assumes the common `winequality-red.csv` (which is often comma-separated in benchmarks, despite some raw sources being semicolon). If the task implicitly assumes the `winequality-red.csv` linked from common sources like UCI (which *is* semicolon-separated), then the generated code's initial `sep=';'` is actually more accurate. However, if the `pd.read_csv` without `sep` works, it implies a comma. The generated code's fallback logic is good but could be streamlined if a specific separator was guaranteed by the problem.
*   **Chart Refinements:**
    *   The `y` label in the generated code's plot is `Average Alcohol Content (%)`. While the ground truth also uses `(%)`, the ground truth adds more specific labels (`Quality Score` vs `Wine Quality Score`). This is a minor stylistic difference.
    *   The generated code uses `average_alcohol_by_quality.index` and `average_alcohol_by_quality.values` for plotting, which works. However, transforming the grouped Series back into a DataFrame with `.reset_index()` (as done in the ground truth) often leads to more standard and potentially more readable plotting calls with `x` and `y` directly referencing column names, especially when passing the `data` argument to seaborn.
    *   Adding `plt.tight_layout()` (as in the ground truth) can help with plot display in some environments.
    *   Adding data labels on the bars (like in the ground truth) would further enhance the visualization's informativeness.
*   **Output Consistency:** The print statements for average quality and grouped alcohol are good, but the ground truth adds specific numbering (`1. Average 'quality' ...`, `2. Average 'alcohol' ...`) which adheres slightly more to a ""report"" style, though this is a minor detail.",gemini-2.5-pro-preview,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# 1) Load the dataset and parse the 'date' column as datetime objects.
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2) Handle missing values
# Fill 'hosp_patients' and 'icu_patients' with 0
df['hosp_patients'] = df['hosp_patients'].fillna(0)
df['icu_patients'] = df['icu_patients'].fillna(0)

# For other numerical columns like 'positive_rate', 'reproduction_rate',
# fill missing values with the mean of their respective c...",9,"The generated code is largely correct and addresses almost all requirements of the task.

**Strong points:**
- Correctly loads the data and parses the 'date' column using `parse_dates`.
- Correctly handles missing values for `hosp_patients` and `icu_patients` by filling with 0.
- Correctly fills missing values for `positive_rate` and `reproduction_rate` with their respective means.
- Correctly calculates `mortality_rate` and `vaccination_rate_per_capita`.
- Correctly handles potential `NaN`/`inf` values arising from division by zero for both new columns, replacing them with 0. The approach of `replace([np.inf, -np.inf], np.nan).fillna(0)` is concise and effective.
- Correctly filters the dataset for the year 2021 and uses `.copy()` to avoid `SettingWithCopyWarning`.
- Displays the shape and head of the cleaned dataset as requested.
- Includes all necessary imports.

**Areas for minor improvement:**
- The task specified handling `NaN` or `inf` values for `mortality_rate` by replacing them with 0. While the generated code's `replace([np.inf, -np.inf], np.nan).fillna(0)` effectively handles both, the ground truth explicitly uses `replace([np.inf, -np.inf], 0, inplace=True)` for infs and then `fillna(0, inplace=True)` for NaNs, which more directly maps to the wording ""replacing resulting NaNs or infs with 0"". The generated code's method is functionally equivalent for the target output, but the ground truth's approach might be seen as slightly more direct interpretation of the requirements.
- The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is robust programming practice. While not explicitly asked for, itâ€™s a good addition for a real-world scenario. The generated code omits this.
- The ground truth explicitly converts columns to numeric using `pd.to_numeric` before performing calculations on them or before calculating means, to ensure they are of the correct type and to handle potential non-numeric data gracefully. The generated code assumes these columns are already numeric, which might be true for synthetic data but not always for real-world datasets. This is a subtle point but contributes to robustness.",gemini-2.5-pro-preview,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def create_covid_visualizations():
    # Load the dataset and parse the 'date' column
    try:
        df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
    except FileNotFoundError:
        print(""Error: 'synthetic_covid19_data.csv' not found in the current directory."")
        return

    # Set a general theme for seaborn plots
    sns.set_theme(style=""whitegrid"")

    # 1. Time series line ...",9,"The generated code is well-structured and addresses most of the requirements. It correctly loads the data, parses the date, and generates all requested visualizations.

Strengths:
- **Robustness:** Includes `try-except` for file loading, which is good practice.
- **Modularity:** Encapsulates the entire logic in a function `create_covid_visualizations()`, which is a good design choice.
- **Data Handling:** The code correctly parses the 'date' column.
- **Visualization Types:** All five requested visualization types (line plot, bar chart, scatter plot, histogram, box plot) are generated.
- **Labels and Titles:** All plots include appropriate titles and labels.
- **Specific requirements:** Correctly handles `new_cases` for India, and `reproduction_rate` histogram.
- **Bar Chart Logic:** The handling of `total_deaths` by continent is significantly more robust than previous attempts by models, correctly identifying and filtering out aggregate locations to sum country-specific latest deaths. This is a very good improvement.
- **Scatter Plot Logic:** Correctly uses the latest entry per location for the scatter plot, and appropriately uses a log scale for both axes, which is a good analytical choice for these types of variables.
- **Missing Column Handling:** The `if column in df.columns:` checks for each plot's required columns make the code more robust against variations in input data, which is excellent.

Areas for minor improvement:
- **Scatter Plot Hue/Size:** The scatter plot in the prompt only specified `hue='continent'` and `size='population'`. The generated code uses `hue='continent'` but doesn't explicitly use `size='population'` or `sizes=(20, 400)` as in the ground truth, opting for a fixed `s=100`. While not a critical error, aligning better with the ground truth's analytical choices would be better.
- **Redundant `parse_dates`:** While `pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])` is a valid way to parse dates, the explicit `df['date'] = pd.to_datetime(df['date'])` after loading (as in the ground truth) is more explicit and can sometimes be more flexible if date formats are inconsistent, though `parse_dates` generally works well.

Overall, this is a very strong and capable solution.",gemini-2.5-pro-preview,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# 1. Load the dataset
# Assuming 'clash_royale_cards.csv' is in the current directory.
df = pd.read_csv('clash_royale_cards.csv')

# 1. List all column names and identify the data type of each column.
print(""All column names:"")
all_column_names = df.columns.tolist()
print(all_column_names)

print(""\nData type of each column:"")
data_types = df.dtypes
print(data_types)

# 2. For the numerical columns `elixirCost`, `Win Rate`, and `Usage`,
#    calculate and report ba...",8,"The generated code correctly addresses all requirements of the task: it loads the CSV, lists column names and data types, calculates descriptive statistics for the specified numerical columns, and reports the total number of rows.

**Strong points:**
- **Correctness:** The code successfully performs all requested operations.
- **Error Handling (Implicit):** While not explicit `try-except` for `FileNotFoundError` during CSV loading, it correctly filters for existing columns before calculating statistics, which is good practice.
- **Specific Statistics:** It correctly extracts 'mean', 'median' (from '50%'), 'std', 'min', 'max' from the `describe()` output, demonstrating a good understanding of `pandas.DataFrame.describe()` functionality.
- **Readability:** The comments clearly delineate the different sections of the task.

**Areas for Improvement:**
- **Robustness in File Loading:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions during `pd.read_csv`. The generated code assumes the file is always present, which could lead to a `FileNotFoundError` crashing the script if the file is missing. Adding robust error handling for file loading would make the code more production-ready.
- **Output Clarity:** The ground truth code uses `print` statements to clearly label each section of the output (e.g., ""--- Column Names and Data Types ---"", ""--- Descriptive Statistics for Numerical Columns ---""). The generated code's print statements are less prominent, making the output slightly less organized and harder to parse at a glance. For example, printing `df.columns` as a list is less readable than iterating and printing each column on a new line as done in the ground truth.
- **Redundancy in `numerical_columns_df` check:** The line `numerical_columns_df = df_subset_for_stats.select_dtypes(include='number')` after `actual_cols_for_stats` is filtered might be redundant if the specified columns (`elixirCost`, `Win Rate`, `Usage`) are guaranteed to be numerical. However, it adds a layer of safety if non-numerical data were present in those columns, so it's not strictly a flaw but worth noting depending on dataset assumptions.
- **`describe()` output handling:** The `selected_stats.rename(index={'50%': 'median'})` is a good way to get the median, but the ground truth's `agg` method is often more direct when you know exactly which statistics you want and may avoid potential issues if `describe()` output structure changes or unexpected index labels appear. However, both approaches are valid.",gemini-2.5-pro-preview,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

try:
    # Load the dataset
    df = pd.read_csv('clash_royale_cards.csv')

    # 1) Filter the dataset for elixirCost <= 3
    filtered_df = df[df['elixirCost'] <= 3]

    # 2) Select 'Card' and 'Win Rate' columns
    selected_data = filtered_df[['Card', 'Win Rate']]

    # 3) Sort by 'Win Rate' in descending order
    sorted_data = selected_data.sort_values(by='Win Rate', ascending=False)

    # Display the resulting data
    print(sorted_data)

except FileNotFou...",9,"The generated code correctly performs all the requested actions: loading the CSV, filtering by `elixirCost` <= 3, selecting the `Card` and `Win Rate` columns, and sorting by `Win Rate` in descending order. It also includes proper imports and error handling for common issues like `FileNotFoundError` and `KeyError`.

The code is concise and directly addresses the requirements. The use of chained operations is effective for this specific task.

Areas for minor improvement (which prevent a perfect 10):
- The ground truth includes constants for column names and file paths, which improves readability and maintainability for larger projects or if these strings need to be changed often. The generated code hardcodes these strings.
- The ground truth also explicitly checks if required columns exist before attempting to select them, which is a robustness improvement (though the `KeyError` catch in the generated code also handles this, albeit reactively).
- The task implicitly asks for proper imports, which are present, and assumes the CSV is in the current directory, which is handled.",gemini-2.5-pro-preview,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the clash_royale_cards.csv dataset using pandas
df = pd.read_csv('clash_royale_cards.csv')

# 2) Generate a histogram to visualize the distribution of the `elixirCost` column
# Using seaborn for plotting. `discrete=True` is suitable for integer data like elixirCost.
# `shrink=0.8` adds some space between bars.
sns.histplot(data=df, x='elixirCost', discrete=True, shrink=0.8)

# Ensure the plot has a tit...",8,"The generated code correctly loads the CSV, generates a histogram for `elixirCost`, and includes a title and axis labels. This fulfills all primary requirements of the task. The use of `sns.histplot` with `discrete=True` is a good choice for this type of data, and `shrink=0.8` is a nice touch for readability.

Areas for minor improvement (preventing a perfect score):
1.  **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and missing columns. The generated code lacks this, which makes it less resilient in real-world scenarios where data might be missing or malformed.
2.  **Customization:** While the plot has a title and labels, the ground truth provides more detailed customization (e.g., `fontsize`, `edgecolor`, `bins` for clearer integer representation, `xticks` for precise labeling of discrete values, and a grid) which enhances the plot's professionalism and readability.
3.  **Clarity of Y-axis Label:** ""Frequency"" is good, but ""Number of Cards"" (used in ground truth) might be slightly more precise for this specific dataset and visualization.",gemini-2.5-pro-preview,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: 'iris.csv' not found in the current directory."")
    exit()

# 1) List all column names and identify the data type of each column
print(""1. Column Names and Data Types:"")
print(""-------------------------------"")
for col in df.columns:
    print(f""Column: {col}, Data Type: {df[col].dtype}"")
print(""\nAlternatively, using df.info():"")
df.info(verbose=True)
print(""\n"")
...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, identifies column names and data types, calculates descriptive statistics for the specified numerical columns, and reports unique species counts.

Strong points:
- Proper use of `pandas` for data loading and manipulation.
- Correctly identifies column names and data types. Uses both a loop and `df.info()` for reporting data types, which is comprehensive.
- Accurately calculates descriptive statistics (mean, median, std, min, max) for the specified numerical columns. It correctly renames '50%' to 'median' which is a nice touch for clarity.
- Correctly handles the counting of unique species and their respective counts.
- Includes basic error handling for `FileNotFoundError` and missing columns, which improves robustness.

Areas for minor improvement (to reach a 10):
- While the error handling for `FileNotFoundError` and missing columns is present, it could be slightly more robust by explicitly exiting or handling the flow if critical columns are missing earlier in the script.
- The output formatting for column names and data types (using a loop) could be slightly more concise by directly printing `df.dtypes` similar to the ground truth, although `df.info()` covers it well.
- The `print(""Alternatively, using df.info():"")` and `print(""\n"")` statements, while informative, add a bit of unnecessary verbosity to the output compared to a more direct presentation. The ground truth's use of `df.dtypes` is cleaner for just listing types.

Overall, it's a very good solution that fulfills all the requirements accurately.",gemini-2.5-pro-preview,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: 'iris.csv' not found in the current directory."")
    # Create a dummy DataFrame for demonstration if iris.csv is not present
    # This part is for testing/demonstration if the file is missing,
    # but the core logic assumes the file exists as per the prompt.
    # In a real scenario, you would handle the FileNotFoundError appropriately.
    # For this problem, we...",9,"The generated code correctly loads the 'iris.csv' dataset, filters it to include only 'Iris-setosa' species, and then selects and displays the 'SepalLengthCm' and 'SepalWidthCm' columns. Imports are proper, and the structure is clean.

However, the error handling for `FileNotFoundError` is less robust than the ground truth. While it catches the error and prints a message, it then calls `exit()`, which abruptly terminates the script. The ground truth provides a more user-friendly message and suggests what the user can do, and also includes `KeyError` handling for missing columns, which is a common issue with datasets. The comment about creating a dummy DataFrame is distracting and unnecessary; simply printing the error and exiting is acceptable for a basic solution, but the `exit()` call makes it more abrupt than the ground truth's graceful exit.

Overall, it's a very good solution that fulfills the core task requirements completely and correctly. The minor points relate to robustness in error handling beyond the immediate prompt.",gemini-2.5-pro-preview,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the iris.csv dataset
# Assumes 'iris.csv' is in the current directory
try:
    iris_df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: 'iris.csv' not found. Please ensure the file is in the current directory."")
    exit()

# Create a figure and an axes object for the plot
fig, ax = plt.subplots()

# Define a color mapping for the species
# Typical species in Iris dataset are 'Iris-setosa', 'Iris-versicolo...",10,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, creates a scatter plot with `SepalLengthCm` vs `SepalWidthCm`, colors the points based on `Species`, and includes a title, x-axis label, and y-axis label. The imports are correct, and the use of `groupby()` for plotting by species is a clean and efficient approach. Error handling for `FileNotFoundError` is also included. The use of `fig, ax = plt.subplots()` is a good practice for more explicit control over the plot. The `species_color_map` is a thoughtful addition, although the fallback to `matplotlib`'s default color cycle if a species isn't in the map is a minor edge case but acceptable.",gemini-2.5-pro-preview,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

def analyze_heart_dataset():
    """"""
    Loads the heart.csv dataset, performs analysis, and prints results.
    """"""
    try:
        # 1. Load the dataset
        df = pd.read_csv('heart.csv')
    except FileNotFoundError:
        print(""Error: 'heart.csv' not found in the current directory."")
        return
    except Exception as e:
        print(f""An error occurred while loading the CSV: {e}"")
        return

    print(""--- à¤¹à¤¾à¤°à¥à¤Ÿ.csv à¤¡à¥‡à¤Ÿà¤¾à¤¸à¥‡à¤Ÿ à¤µà¤¿à¤¶à¥à¤²à¥‡à¤·à¤£ ---"")

   ...",8,"The generated code correctly loads the dataset, lists column names and data types, calculates descriptive statistics for the specified numerical columns, and reports value counts for 'HeartDisease'. It also includes proper imports and basic error handling.

Strengths:
- Correctly loads the CSV and handles `FileNotFoundError`.
- Accurately lists column names and data types.
- Correctly calculates mean, median, std, min, and max for the specified numerical columns using `agg()`.
- Correctly reports value counts for `HeartDisease`.
- Includes checks for the existence of columns, which is good practice.
- Encapsulates the logic within a function, which is a good structural choice.

Areas for Improvement:
- The output strings are in Hindi, which was not specified in the prompt. While technically functional, it deviates from the implicit expectation of English output for a technical task.
- The descriptive statistics output format from `agg()` is slightly different from `describe().loc[...]` used in the ground truth. While both provide the correct statistics, the ground truth's output is arguably better formatted to show stats row-wise for each column.
- The line `df_numerical_subset[col] = pd.to_numeric(df_numerical_subset[col], errors='coerce')` is redundant here because `pd.read_csv` usually infers numeric types correctly, and if not, `agg` would fail on non-numeric data, which is handled implicitly. It adds an unnecessary step unless there's an expectation that the columns might not be numeric after initial load.",gemini-2.5-pro-preview,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('heart.csv')
except FileNotFoundError:
    print(""Error: 'heart.csv' not found in the current directory."")
    exit()

# 1) Filter the dataset
filtered_df = df[(df['Age'] > 50) & (df['HeartDisease'] == 1)]

# 2) Select and display the specified columns from the filtered dataset
selected_columns_df = filtered_df[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]

print(selected_columns_df)
```",9,"The generated code correctly addresses all parts of the task: it loads the CSV, filters the DataFrame based on the specified conditions (`Age` > 50 AND `HeartDisease` == 1), and then selects and displays the required columns (`Age`, `Sex`, `ChestPainType`, `Cholesterol`). The imports are correct, and the code structure is clear and concise.

One minor area for improvement, which leads to a slight reduction from a perfect score, is the lack of a function encapsulation. While not strictly required by the prompt, wrapping the logic in a function would improve reusability and modularity, similar to the ground truth. Additionally, the error handling for `FileNotFoundError` is present, which is good, but it simply exits the program instead of potentially offering a more graceful recovery or returning an error status (though for a simple script, `exit()` is acceptable). However, given the prompt's focus on the ""Python code,"" and without specific instructions for function encapsulation, the provided solution is highly effective and accurate.",gemini-2.5-pro-preview,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

try:
    # Load the dataset
    df = pd.read_csv('winequality-red.csv')

    # 1. Filter wines
    filtered_wines = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]

    if not filtered_wines.empty:
        # 2. Identify the wine with the highest 'quality' score from the filtered subset
        highest_quality_wine = filtered_wines.loc[filtered_wines['quality'].idxmax()]

        # 3. Display all features of this specific wine
        print(""Features of the wine with al...",9,"The generated code is highly correct and directly addresses all parts of the task.

**Strong Points:**
- **Correct Filtering:** Accurately applies the compound filtering condition `(df['alcohol'] > 12) & (df['pH'] < 3.3)`.
- **Correct Highest Quality Identification:** Uses `idxmax()` correctly to find the wine with the highest quality within the filtered subset.
- **Correct Feature Display:** Displays all features of the identified wine.
- **Error Handling:** Includes basic `try-except` blocks for `FileNotFoundError` and general exceptions, which is good practice.
- **Handles Empty Filtered Subset:** Correctly checks if `filtered_wines` is empty before attempting to find the highest quality wine.

**Areas for Minor Improvement (to reach a perfect 10):**
- **Function Encapsulation:** The ground truth encapsulates the logic within a function (`analyze_wine_quality`), which makes the code reusable and more organized, especially for larger scripts. The generated code places the logic directly in the global scope.
- **Clarity of Output:** While the output is correct, the ground truth provides more verbose and informative print statements (e.g., dataset shape, number of filtered wines), which can be helpful for debugging or understanding the processing steps.
- **Magic String/Path:** The dataset path `'winequality-red.csv'` is hardcoded. While acceptable for this small task, defining it as a constant like `DATASET_PATH` as in the ground truth is a good practice for maintainability.
- **`to_string()` for Display:** The ground truth uses `print(best_wine_in_subset.to_string())` to ensure all features are displayed without potential truncation, which `print(Series)` might do for very wide DataFrames or many columns. This is a minor point, as for typical datasets `print(Series)` often works fine.",gemini-2.5-pro-preview,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns # Seaborn can be imported for its styling improvements, even if not directly used for plotting functions

def load_and_plot_heart_data():
    """"""
    Loads the heart.csv dataset, generates and displays a histogram for 'Age'
    and a bar chart for 'Sex'.
    """"""
    # 1) Load the heart.csv dataset using pandas
    try:
        df = pd.read_csv('heart.csv')
    except FileNotFoundError:
        print(""Error: 'hear...",7,"The generated code correctly addresses most aspects of the task, loading the data, creating a histogram for 'Age', and a bar chart for 'Sex', and adding titles and labels.

**Strengths:**
*   Correctly loads the CSV using pandas.
*   Generates both a histogram for 'Age' and a bar chart for 'Sex'.
*   Plots have titles and labeled axes.
*   Includes proper imports (pandas, matplotlib, seaborn).
*   Includes robust error handling for `FileNotFoundError`.
*   Uses `sns.set_style(""whitegrid"")` which improves the visual aesthetic.

**Areas for Improvement:**
*   **Histogram vs. Seaborn `histplot`:** While `plt.hist` fulfills the requirement, the ground truth code used `sns.histplot` which is generally preferred in data science for its enhanced features (like KDE, better default aesthetics, and easier bin management). The generated code imports `seaborn` but doesn't fully leverage it for the histogram.
*   **Mapping 'Sex' values:** The most common `heart.csv` datasets encode 'Sex' as 0 and 1. The ground truth correctly assumes this and maps them to 'Female' and 'Male' for better readability in the bar chart. The generated code directly plots the ""M"" and ""F"" from the dummy data, which might not be consistent with a typical `heart.csv` where 0 and 1 are used. If the actual `heart.csv` uses 'M' and 'F', then this is fine, but it's a common discrepancy.
*   **Redundant dummy data handling:** The code loads `heart.csv` and if not found, creates a dummy DataFrame _and_ prints a message about it. While robust, the task description implies the `heart.csv` will be present. The dummy data, specifically for `Sex` ('M', 'F' strings), could lead to a mismatch if the actual `heart.csv` uses integers (0, 1) and the plotting code doesn't handle that conversion. This can complicate debugging if the submission is run against an integer-based `heart.csv`.
*   **`plt.tight_layout()` is missing:** The ground truth code includes `plt.tight_layout()` which is good practice to prevent labels and titles from overlapping, especially with multiple plots. This is missing in the generated code.

Overall, it's a good attempt, but a few refinements, especially leveraging Seaborn more fully and handling the 'Sex' column encoding universally, would make it excellent.",gemini-2.5-pro-preview,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

try:
    # Attempt to load the dataset, assuming UTF-8 encoding by default.
    # Some spotify CSVs might require encoding='latin1' or encoding='ISO-8859-1'
    # For this problem, we'll assume default or that any encoding issues are outside scope.
    df = pd.read_csv('spotify_2023.csv')
except FileNotFoundError:
    print(""Error: 'spotify_2023.csv' not found in the current directory."")
    raise SystemExit()
except Exception as e:
    print(f""Error reading CSV fi...",8,"The generated code largely fulfills the requirements of the task.

**Strong Points:**
- **Correctness:**
    - Loads the CSV correctly.
    - Correctly lists column names and data types.
    - Accurately converts 'streams' to numeric, handling errors by coercing to NaN, and correctly reports NaN count.
    - Calculates descriptive statistics for the specified columns using `agg`, which is flexible.
    - Handles missing columns gracefully by checking for their existence (`if col in df.columns`).
    - The unique artist count is handled very robustly, considering cases like multiple artists per entry (comma-separated), stripping whitespace, and dropping NaNs. This is a very good addition beyond the literal requirement.
- **Code Quality:**
    - Good use of `try-except` for file loading.
    - Clear comments explaining sections and choices.
    - Variable names are descriptive.
    - Readability is good.
- **Completeness:**
    - Addresses all major requirements of the prompt.

**Areas for Improvement (Why not a 9 or 10):**

1.  **Reporting NaNs for `streams` Conversion:** The prompt specifically asked to ""report any NaNs"" for the `streams` column after conversion. While the code reports `nan_count_streams`, the ground truth provided a sample of rows with NaNs, which is a more comprehensive way to ""report"" NaNs, helping to understand *where* they occurred. The generated code only provides indices, which is less user-friendly than showing the actual data.
2.  **Descriptive Statistics Output Format:** The prompt asked to ""calculate and report basic descriptive statistics (mean, median, std, min, max)"". The `desc_stats = df[existing_stat_cols].agg(['mean', 'median', 'std', 'min', 'max'])` works, but the ground truth's use of `describe().loc[['mean', '50%', 'std', 'min', 'max']]` is a more idiomatic pandas way to get these specific stats from `describe()` and arguably presents the output in a slightly more standard statistical summary format (columns as stats, rows as features). While `agg` is not wrong, it's a difference in approach.
3.  **Column Type Conversion for Numerical Stats:** The generated code includes a loop to explicitly convert columns to numeric (`if df[col].dtype == 'object': df[col] = pd.to_numeric(df[col], errors='coerce')`) before calculating stats. While conceptually sound for robustness, the prompt implied these (apart from `streams`) would likely already be numeric or directly convertible without intermediate steps. If `bpm` was an object type in the CSV due to bad data, this handles it, but it wasn't an explicit requirement for these *other* columns to handle non-numeric data during their statistical calculation.

These are minor points, but addressing them would make the solution even more robust and aligned with typical ""reporting"" expectations in data analysis. The handling of multiple artists is a particularly strong element that goes above and beyond, which is commendable.",gemini-2.5-pro-preview,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

# 1. Load the dataset
# Assume 'spotify_2023.csv' is in the current directory.
df = pd.read_csv('spotify_2023.csv')

# 2. Filter the dataset for songs where released_year is 2023
df_2023 = df[df['released_year'] == 2023].copy()

# 3. Ensure the 'streams' column is treated as numeric (coerce errors to NaN)
df_2023['streams'] = pd.to_numeric(df_2023['streams'], errors='coerce')

# 4. Select the 'track_name', 'artist(s)_name', and 'streams' columns
selected_data = df_...",9,"The generated code correctly implements all the requirements of the task: it loads the CSV, filters by 'released_year' == 2023, coercizes the 'streams' column to numeric, selects the specified columns, and sorts the data by 'streams' in descending order. The use of `.copy()` after filtering is good practice to prevent `SettingWithCopyWarning`. The code is clean and well-structured, making it easy to understand.

A minor improvement could be to include error handling for `FileNotFoundError` when loading the CSV, similar to the ground truth, which is good practice in production code. Also, while not strictly required by the prompt, handling NaNs in the 'streams' column (e.g., dropping them before sorting if they shouldn't appear in the sorted list) could be considered depending on the expected output, but the current implementation correctly places them at the end when sorting ascending=False. However, given the prompt's simplicity, the current solution is excellent and fully addresses the explicit requirements.",gemini-2.5-pro-preview,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 1. Generate a histogram to visualize the distribution of the `bpm` column.
plt.figure(figsize=(10, 6))
plt.hist(df['bpm'], bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of bpm')
plt.xlabel('bpm')
plt.ylabel('Frequency')
plt.show()

# 2. Create a scatter plot visualizing the relationship between `danceability_%` (x-axis) and `energy_%` (y-axis).
plt.figu...",8,"The generated code correctly addresses all requirements of the task. It loads the CSV, creates a histogram for 'bpm', and a scatter plot for 'danceability_%' vs 'energy_%'. Both plots have appropriate titles and axis labels. Imports are correct.

Areas for improvement (not critical for this task but good practice):
- The `plt.show()` calls are placed immediately after each plot definition. While this works, typically in a script generating multiple plots, `plt.show()` is called only once at the end to display all generated figures simultaneously. Having them after each plot would display the first plot, require the user to close it, then display the second, etc.
- No error handling for `FileNotFoundError` or missing columns is included, which the ground truth code does. For a robust solution, this would be beneficial.
- The `bins` parameter for the histogram is set to a fixed number (30). Using `bins='auto'` (as in the ground truth) often provides a better default visual representation based on the data's distribution.",gemini-2.5-pro-preview,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd

# Assume 'youtube_trendings.csv' is in the current directory.
df = pd.read_csv('youtube_trendings.csv')

# 1) List all column names and their inferred data types.
print(""1. Column names and their inferred data types:"")
print(df.dtypes)
print(""--------------------------------------------------"")

# 2) For the columns `Video views`, `Likes`, and `Dislikes`:
#    - remove any commas
#    - convert them to numeric types (e.g., float)
#    - if conversion fails for any ...",9,"The generated code correctly addresses all requirements in the task. It properly loads the data, cleans the specified numeric columns by removing commas, converting to numeric, and filling NaNs with 0. It calculates and reports descriptive statistics accurately. It also correctly handles missing values in the 'Category' column and reports the unique count.

Strengths:
- Correctly implements all required cleaning steps for `Video views`, `Likes`, and `Dislikes`.
- Uses `pd.to_numeric` with `errors='coerce'` effectively for robust conversion.
- Correctly fills NaNs with 0.
- Uses `agg` for concise descriptive statistics calculation.
- Correctly fills missing 'Category' values with 'Unknown'.
- Correctly counts unique 'Category' values.
- Includes checks for column existence before processing 'Category', which is good practice.

Areas for minor improvement (not critical for score but good practice):
- The `df.dtypes` output for step 1 is less comprehensive than `df.info()`. `df.info()` provides non-null counts which are often more useful for understanding data quality at a glance. The task asked to ""List all column names and their inferred data types"", and while `df.dtypes` does this, `df.info()` is the standard tool for this purpose and provides additional context.
- Error handling for `pd.read_csv`: The ground truth included a `try-except FileNotFoundError`, which is robust. The generated code assumes the file will always be present, which is typical for benchmark tasks but less robust for production code. However, the task specifically states ""Assume the CSV file is in the current directory,"" so this is a minor point.
- The `astype(float)` call after filling NaNs is redundant because `pd.to_numeric` would likely result in float if `NaN`s are present, and `fillna(0)` maintains the numeric type. While not incorrect, it's an unnecessary step.",gemini-2.5-pro-preview,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

# 1. Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# 2. Process 'Video views' column
# Original column name is 'Video views'
# Remove commas, convert to numeric, handle errors, fill NaNs, ensure float type
df['Video views'] = pd.to_numeric(
    df['Video views'].astype(str).str.replace(',', '', regex=False),
    errors='coerce'
).fillna(0).astype(float)

# 3. Filter dataset
# The 'Video views' column is now cleaned and numeric (float)
filtered_df = df...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It properly loads the CSV, handles the 'Video views' column cleaning effectively (removing commas, converting to numeric, handling errors, and filling NaNs with 0), filters the data, selects the specified columns, and sorts them correctly in descending order.

**Strong points:**
*   Correctly uses `pd.to_numeric` with `errors='coerce'` and `fillna(0)`.
*   Includes `astype(str)` before `str.replace` which is robust in case the column is not already string type.
*   The chaining of operations for cleaning the 'Video views' column is concise and efficient (`pd.to_numeric(...).fillna(0).astype(float)`).
*   Correctly filters the data and selects the required columns.
*   Sorts the data as requested.

**Areas for minor improvement (not critical for correctness, but good practice):**
*   The generated code does not include a `try-except FileNotFoundError` block for `pd.read_csv`, which was present in the ground truth. While not explicitly asked for, it's a good practice for robust code to handle missing files. However, the prompt did say ""assume the CSV file is in the current directory"", so this isn't a strict error.
*   The `astype(float)` on the `Video views` column after `fillna(0)` is slightly redundant if the intention was just to have a numeric type, as `pd.to_numeric` would likely result in float anyway if there are NaNs to coerce. However, it explicitly ensures the type, which isn't wrong. The problem asked for ""numeric type (e.g., float)"", so `float` is a good choice.

Overall, the solution is very solid and directly solves the problem.",gemini-2.5-pro-preview,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1) Load the dataset and clean specified columns
try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    # Fallback for when the script is run without the CSV,
    # to demonstrate the cleaning and plotting logic.
    # This part can be removed if the CSV is guaranteed to be present.
    print(""Warning: 'youtube_trendings.csv' not found. Using a dummy DataFrame for demonstration."")
    data = {
        'Vide...",9,"The generated code correctly performs all the required operations.

**Strong Points:**
*   **Correctness:**
    *   Successfully loads the `youtube_trendings.csv` dataset.
    *   Accurately handles the cleaning of 'Likes' and 'Video views' columns:
        *   Removes commas using `str.replace(',', '', regex=False)`.
        *   Converts to numeric types using `pd.to_numeric` with `errors='coerce'` to handle conversion failures by turning them into NaNs.
        *   Fills NaNs with 0 using `fillna(0)`.
    *   Generates a scatter plot with 'Video views' on the x-axis and 'Likes' on the y-axis.
    *   Sets the correct title ('Video Views vs. Likes') and axis labels ('Video Views (cleaned)', 'Likes (cleaned)').
    *   Includes proper imports (`pandas`, `matplotlib.pyplot`).
*   **Completeness:** Addresses all specified requirements of the task.
*   **Code Quality:** The code is well-structured, readable, and includes comments where necessary. The use of `try-except` for file loading and the loop for column cleaning are good practices.

**Areas for Improvement (minor):**
*   **Dummy DataFrame Handling:** The `FileNotFoundError` fallback prints a warning and uses a dummy DataFrame. While useful for local testing when the file isn't present, the prompt implies the file *will* be present and asks for the code *without explanations*. In a benchmark setting, if the file is assumed to be present, this fallback might be considered extra and adds complexity. If the task is strictly about providing the final production code for a given dataset, exiting on `FileNotFoundError` (as in the ground truth) would be more direct. However, for robustness during development, it's a good pattern.
*   **Column Not Found Handling:** In the `for` loop, if a column is not found, it prints a warning and then creates the column with `df[col_name] = 0`. While this prevents the plot from failing, it silently adds a column of zeros, which might not always be the desired behavior if the column is truly missing. The ground truth's approach of skipping cleaning and plotting if columns are missing is arguably safer as it signals a potential data issue. However, for the specific task of ensuring the plot doesn't crash, the generated code's approach works.
*   **Redundant `x_values`/`y_values` checks for plot:** After the cleaning loop, the code checks `if 'Video views' in df.columns` again before assigning `x_values`. Since the previous `for` loop *guarantees* that 'Video views' and 'Likes' will exist (either from the CSV or as a newly created column of zeros), this check is technically redundant. `plt.scatter(df['Video views'], df['Likes'], alpha=0.5)` would suffice directly.

Overall, the generated code is very good and adheres to all primary requirements. The minor points are about robustness strategies for edge cases that could be debated based on strict benchmark interpretation vs. practical development.",gemini-2.5-pro-preview,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

def clean_and_analyze_mobiles_data(csv_path='mobiles_2025.csv'):
    """"""
    Loads, cleans, and analyzes the mobiles dataset.
    """"""
    try:
        df = pd.read_csv(csv_path)
    except FileNotFoundError:
        print(f""Error: The file {csv_path} was not found in the current directory."")
        return

    # 1) Clean Mobile Weight: remove 'g' and convert to numeric
    if 'Mobile Weight' in df.columns:
        df['Mobile Weight'] = df['Mobil...",8,"The generated code generally performs well and addresses most of the requirements. It correctly loads the dataset, cleans the specified columns by removing specific substrings ('g', 'GB', 'mAh', 'USD ', ',') and converting them to numeric, handling errors by coercing to NaN. It also lists the data types and calculates the count of unique company names.

**Strong Points:**
- Correctly uses `pd.to_numeric` with `errors='coerce'` for robust conversion.
- Handles `FileNotFoundError` gracefully.
- Includes `if col in df.columns` checks, which adds robustness.
- The dummy CSV creation is helpful for local testing, though the prompt asked for only the Python code for the task itself.

**Areas for Improvement:**
- **Exact Output Format for Statistics:** The ground truth code calculates statistics using `describe().loc[['mean', '50%', 'std', 'min', 'max']]`, providing a compact table. The generated code iterates through each column printing individual statistics, which is correct in content but not exactly the same format. The prompt asked to ""calculate and report basic descriptive statistics (mean, median, std, min, max)"", which ideally implies reporting them together for cleaner output (like `describe()` does).
- **Redundant `astype(str)`:** While generally safe, some of the `df['Column'].astype(str)` calls are sometimes redundant if the column is already of a type that handles `str.replace` without issues (e.g., object dtype from CSV read initially). However, it doesn't cause harm.
- **`strip()` after replace:** The ground truth code uses `.str.strip()` after `replace` operations to clean up any leading/trailing whitespace that might remain, which is good practice. The generated code omits this. For example, '180g ' (with a trailing space) would become '180 ' and then potentially cause issues or remain as a float with a trailing space if not stripped before `to_numeric`. In this case, `pd.to_numeric` is robust enough to handle trailing spaces, but it's a good practice for general string cleaning. While it works here, it's a slight difference in robustness.
- **Output Formatting:** The descriptive statistics are printed one-by-one per column, which is less compact than `describe()`. The requirement asked to ""report basic descriptive statistics"", and while its method meets the ""basic"" criteria, the `describe()` approach is more standard for pandas.

Overall, the generated code fulfills the core requirements effectively with only minor stylistic and robustness differences compared to the provided ground truth.",gemini-2.5-pro-preview,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: 'mobiles_2025.csv' not found in the current directory."")
    exit()

# 1) Clean the `Launched Price (India)` column
# Ensure the column is treated as string before attempting string operations
df['Launched Price (India)'] = df['Launched Price (India)'].astype(str)
# Remove 'INR '
df['Launched Price (India)'] = df['Launched Price (India)']....",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans both the 'Launched Price (India)' and 'RAM' columns by removing specified strings and converting to numeric with error handling, filters the data for Apple phones under 100,000 INR, selects the requested columns, and sorts the final output correctly. The use of `astype(str)` before string operations is a good practice to prevent errors on mixed types. The `try-except` block for file loading is also well-implemented.

Minor potential improvements (not critical for score):
- The `numpy` import is not used.
- The `errors='coerce'` in `pd.to_numeric` for `RAM` handles cases where there might be non-numeric characters after 'GB' removal, but an explicit attempt to remove other non-digit characters (e.g., using `df['RAM'].str.extract('(\d+)').astype(float)`) could make it even more robust, although the current approach is sufficient for the prompt.
- The code doesn't explicitly handle the case where `filtered_df` might be empty before attempting to select columns or sort, although `pandas` typically handles this gracefully without raising an error. The ground truth includes a check for an empty filtered DataFrame, which is good for user experience.",gemini-2.5-pro-preview,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: 'mobiles_2025.csv' not found in the current directory."")
    exit()

# 1) Clean the RAM column
if 'RAM' in df.columns:
    df['RAM_cleaned'] = df['RAM'].astype(str).str.replace('GB', '', case=False).str.strip()
    df['RAM_cleaned'] = pd.to_numeric(df['RAM_cleaned'], errors='coerce')
else:
    print(""Warn...",9,"The generated code successfully addresses almost all requirements of the task.

**Strong Points:**
- **Correct Data Loading:** Handles `FileNotFoundError` gracefully.
- **Robust Cleaning:** Both `RAM` and `Battery Capacity` columns are cleaned correctly, including removal of units (`GB`, `mAh`) and conversion to numeric types with `errors='coerce'` for handling non-numeric values (setting them to NaN). It also correctly uses `astype(str)` before string operations, which is good practice.
- **Error Handling for Missing Columns:** The code proactively checks for the existence of 'RAM', 'Battery Capacity', and 'Company Name' columns, providing warnings and setting up fallback mechanisms. This makes the code more robust to variations in the input data.
- **Correct Plotting:** Uses `seaborn.scatterplot` for visualization, which is appropriate for this task. It correctly maps `RAM_cleaned` to x-axis, `Battery_Capacity_cleaned` to y-axis, and `Company Name` to `hue` for coloring.
- **Proper Labels and Title:** The plot includes a title and correct x/y axis labels.
- **Code Structure and Imports:** Follows good practices for imports and overall structure.
- **Additional Improvements:** Includes `palette='viridis'` for better visual appeal, `alpha` for transparency, and `s` for point size, which enhances the plot readability. It also adds a legend with `bbox_to_anchor` which is a nice touch for preventing legend overlap.

**Areas for Minor Improvement:**
- **Typo in Placeholder:** In the `RAM` column cleaning section, there's a typo: `float_value('nan')` should be `float('nan')`. This would cause an error if the `RAM` column were genuinely missing.
- **Redundant `hue` condition:** The `hue='Company Name' if 'Company Name' in df.columns else None` inside the `scatterplot` call is technically redundant because the preceding `if 'Company Name' not in df.columns:` block already ensures `df['Company Name']` exists by adding an 'Unknown' column. While harmless, it's not strictly necessary.
- **Consistency of cleaned column names:** `RAM_cleaned` vs `Battery_Capacity_cleaned` (using underscore for space) is a minor cosmetic inconsistency. The ground truth used `_cleaned` for both. This is very minor but good to note.

Overall, the code is very well-written, robust, and correctly addresses the problem. The minor typo is the only significant actionable feedback.",gemini-2.5-pro-preview,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 5 rows
try:
    df = pd.read_csv('titanic.csv')
    print(""First 5 rows of the dataset:"")
    print(df.head())
    print(""\n"" + ""=""*50 + ""\n"")

    # 2) Show the shape of the dataset (rows and columns)
    print(""Shape of the dataset (rows, columns):"")
    print(df.shape)
    print(""\n"" + ""=""*50 + ""\n"")

    # 3) List all column names and their data types
    print(""Column names and their data types:"")
    # df.info() pri...",9,"The generated code is highly correct, complete, and well-structured, successfully addressing all requirements of the task.

**Strong points:**
*   **Correctness:** All tasks (loading, shape, dtypes, survival rate, missing values) are implemented accurately.
*   **Completeness:** All 5 requirements are explicitly met.
*   **Robustness:** Includes `try-except` blocks for `FileNotFoundError` and a general `Exception`, which is excellent for real-world scenarios.
*   **Clarity:** The output is well-labeled, making it easy to understand the results of each step.
*   **Survival Rate Calculation:** Correctly calculates the survival rate, and includes checks for the presence of the 'Survived' column and an empty dataset, which adds robustness.
*   **`df.info()` usage:** Correctly identifies that `df.info()` is comprehensive for EDA, even though `df.dtypes` could also show column types.

**Areas for minor improvement (to reach a perfect 10):**
*   The original markdown task asked for ""Provide only the Python code without explanations."" The generated code includes comments (e.g., `# 1) Load the dataset and display the first 5 rows`) and extra print statements like `print(""\n"" + ""=""*50 + ""\n"")` that, while useful for user understanding, were not strictly part of the requested ""basic exploratory data analysis output"". If interpreting ""without explanations"" as minimal output and comments only when necessary for understanding and not pedagogical, then these additions would be slightly outside the strict interpretation. However, for a user-facing script, they are beneficial.
*   The Ground Truth explicitly printed a header for each step (e.g., `""1. First 5 rows of the dataset:""`). The generated code also does this but prefixes it with the step number only for the first print (`print(""First 5 rows of the dataset:"")`) and subsequently just uses string headings (e.g., `print(""Shape of the dataset (rows, columns):"")`). This is a minor consistency point.

Overall, an excellent piece of code that would perform very well in a data science context.",gemini-2.5-pro-preview,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

# 1. Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Attempt to clean 'Runtime' column if it exists and is of object type
# This handles cases where Runtime might be like '120 min'
if 'Runtime' in df.columns and df['Runtime'].dtype == 'object':
    df['Runtime'] = df['Runtime'].str.extract(r'(\d+)', expand=False)
    df['Runtime'] = pd.to_numeric(df['Runtime'], errors='coerce')

# 1. List all column names and their data types
print(""Column names and their ...",7,"The generated code correctly addresses most aspects of the task. It loads the dataset, lists column names and data types, calculates descriptive statistics for the specified numerical columns, and reports the total number of movies. The `Runtime` column preprocessing is decent, using `str.extract` which is robust in many cases.

However, there are a few areas for improvement:

1.  **Error Handling for File Not Found:** The generated code lacks a `try-except` block for `pd.read_csv`, meaning it will crash if `imdb_top_250.csv` is not found. The ground truth includes this, which is good practice for common data science scripts.
2.  **Descriptive Statistics Output:** While the individual statistics are correct, the output format is not as clean or consolidated as using `df.describe()`. The ground truth uses `df.describe().loc[['mean', '50%', 'std', 'min', 'max']]` which provides a much more concise and readable table. The generated code prints each statistic individually, which can become verbose if there are many columns.
3.  **Redundant Runtime Preprocessing Check:** The `str.extract(r'(\d+)', expand=False)` and `pd.to_numeric` for `Runtime` is done unconditionally *if* it passes the `dtype == 'object'` check. This is generally okay, but the ground truth explicitly uses `str.replace(' min', '')` which is a more direct way to clean ""120 min"" strings. The `str.extract` might be overly generic.
4.  **Clarity of `df.dtypes` Output:** The prompt asks to ""List all column names and their data types."" While `print(df.dtypes)` does this, `df.info()` as used in the ground truth provides a more comprehensive summary, including non-null counts and memory usage, which is often preferred for initial data exploration.

Overall, it's a functional solution, but could be more robust and produce a more idiomatic pandas output for descriptive statistics.",gemini-2.5-pro-preview,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

try:
    # Load the dataset
    df = pd.read_csv('imdb_top_250.csv')

    # 1) Filter the dataset to include only movies with a Rating greater than or equal to 9.0
    filtered_df = df[df['Rating'] >= 9.0]

    # 2) From this filtered dataset, select the Title and Rating columns
    selected_columns_df = filtered_df[['Title', 'Rating']]

    # 3) Sort the resulting data by Rating in descending order
    sorted_df = selected_columns_df.sort_values(by='Rating', ascen...",9,"The generated code correctly addresses all requirements of the task: it loads the CSV, filters by rating, selects the specified columns, and sorts in descending order. It also includes appropriate error handling for `FileNotFoundError` and `KeyError`, which is good practice.

A minor improvement could be to define the `DATASET_PATH` as a variable at the top, similar to the ground truth, for better maintainability (though for a single use, it's not strictly necessary). However, this is a very minor point and doesn't detract from the correctness or overall quality of the solution. The code is clean, readable, and directly implements the requested logic.",gemini-2.5-pro-preview,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
# Assume 'imdb_top_250.csv' is in the current directory
df = pd.read_csv('imdb_top_250.csv')

# 1. Create a histogram to visualize the distribution of the 'Rating' column
plt.figure(figsize=(10, 6))
plt.hist(df['Rating'], bins=15, color='skyblue', edgecolor='black')
plt.title('Distribution of Movie Ratings')
plt.xlabel('Rating')
plt.ylabel('Number of Movies')
plt.grid(axis='y', linestyle='--', alpha=0.7)

# 2. Crea...",7,"The generated code correctly loads the dataset, creates a histogram for 'Rating', and a scatter plot for 'Rating' vs. 'Runtime'. It also includes titles and axis labels as requested.

However, there are a couple of significant issues that prevent a higher score:
1.  **Missing Runtime Preprocessing:** The 'Runtime' column often comes in a string format (e.g., ""120 min""). The generated code attempts to directly use `df['Runtime']` for the scatter plot, which will likely cause a `TypeError` if the column contains non-numeric strings, or produce incorrect plots if pandas infers a non-numeric dtype. The ground truth code handles this robustly by extracting numeric values and converting them. This is a common data cleaning step for such datasets and is crucial for the scatter plot to function correctly.
2.  **No `plt.show()` after each plot:** While `plt.tight_layout()` and a single `plt.show()` at the end *can* work to display all plots in one window, it's generally better practice to call `plt.show()` after each `plt.figure()` block, especially when generating distinct plots that might be intended for separate viewing or saving. The ground truth handles this by calling `plt.show()` after each plot. Without `plt.show()` after the histogram, the code generates the second figure in the same `plt` instance, making it behave like subplots, which might not be the intent of ""two plots."" If a single `plt.show()` is used at the end, it will display both plots but only after the entire script has run, which might not be ideal for interactive development or if subsequent code depends on a plot being shown and closed immediately.

Overall, the code provides a good attempt, but the lack of error handling for file loading and, more critically, the missing preprocessing for the 'Runtime' column are significant oversights for a ""data science benchmark"" context. Another minor point is the missing `seaborn` import; while `matplotlib` can do histograms, `seaborn` often provides more aesthetically pleasing and informative plots (e.g., `sns.histplot` with `kde`).",gemini-2.5-pro-preview,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Assume 'penguins.csv' is in the current directory
df = pd.read_csv('penguins.csv')

# 1) Report the number of missing values (NA) for each column.
print(""1. Number of missing values (NA) for each column:"")
missing_values_per_column = df.isnull().sum()
print(missing_values_per_column)
print(""\n--------------------------------------------------\n"")

# 2) For the numerical columns `culmen_length_mm`, `culmen_depth_mm`, 
#    `flipper_length_mm`, and `body_mass_g`, f...",9,"The generated code successfully completes all tasks outlined in the problem description, demonstrating a good understanding of DataFrame operations in pandas.

Strong points:
- **Correctness:**
    - Properly loads the CSV.
    - Correctly calculates and reports missing values for all columns.
    - Accurately identifies the four numerical columns and drops rows where *any* of them have missing values, creating a new DataFrame (`df_cleaned_numerical`) for this specific analysis.
    - Correctly calculates the requested descriptive statistics (mean, median, std, min, max) for the four numerical columns on the *filtered* data.
    - Accurately counts and reports unique values for `species` and `island` columns.
- **Completeness:** All parts of the task are addressed.
- **Code Quality:**
    - Good use of `df.isnull().sum()` for missing values.
    - Correct use of `dropna(subset=...)` for conditional row dropping.
    - Efficient use of `.agg()` for multiple descriptive statistics.
    - Includes checks for column existence (`'species' in df.columns`) before calling `.nunique()`, which is a good defensive programming practice, although it could print a clearer message if the column is missing in the case of `df_cleaned_numerical`.

Areas for minor improvement (prevented a perfect 10):
- **Error Handling (File Loading):** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions during file loading. The generated code assumes the file will always be present, which is less robust in a real-world scenario.
- **Reporting Clarity:** The ground truth code provides more descriptive print statements (e.g., explicitly stating the original and filtered row counts, which can be useful for understanding the impact of the `dropna` operation). While the generated code's output is correct, adding this context would enhance readability for the user.
- **Consistency in Missing Value Report:** The ground truth filters the missing values report to only show columns with missing values (i.e., `missing_values_count[missing_values_count > 0]`). The generated code prints all columns, including those with zero missing values, which is fine but slightly less concise for identifying problematic columns.",gemini-2.5-pro-preview,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'penguins.csv' not found in the current directory."")
    exit()

# 1) Filter the dataset
filtered_df = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')]

# 2) Select specific columns
selected_columns_df = filtered_df[['culmen_length_mm', 'culmen_depth_mm']]

# 3) Drop rows with missing values in the selected columns
cleaned_df = selected_columns_df...",10,"The generated code correctly implements all specified requirements: it loads the `penguins.csv` dataset, filters by 'Adelie' species and 'Torgersen' island, selects the `culmen_length_mm` and `culmen_depth_mm` columns, drops rows with missing values in these specific columns, and displays the first 5 rows. The code is well-structured, uses proper imports, and includes basic error handling for file loading. It is functionally identical to the ground truth in achieving the task's objectives.",gemini-2.5-pro-preview,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'penguins.csv' not found in the current directory."")
    exit()

# 3) Filter out rows with missing values in the relevant columns
columns_to_check = ['culmen_length_mm', 'culmen_depth_mm', 'species']
df_cleaned = df.dropna(subset=columns_to_check)

# 1) Create a scatter plot of culmen_length_mm vs culmen_dept...",9,"The generated code successfully completes almost all aspects of the task. It correctly loads the data, filters missing values as specified, and creates a scatter plot with the correct axes and colors by species. It also includes appropriate titles and labels.

Strong points:
- Correctly uses `pd.read_csv`, `df.dropna(subset=...)`, `sns.scatterplot`, `plt.title`, `plt.xlabel`, `plt.ylabel`.
- The filtering logic `df.dropna(subset=columns_to_check)` accurately addresses requirement 3.
- The use of `hue='species'` in `sns.scatterplot` correctly fulfills requirement 2.
- Includes error handling for `FileNotFoundError`.

Areas for minor improvement (why it's not a 10):
- The `exit()` call in the `FileNotFoundError` block is generally discouraged in library code or functions, as it abruptly terminates the script instead of allowing the caller to handle the error or gracefully exit. A `return` statement would be more appropriate if this code were part of a larger function.
- It omits a function wrapper, which is good practice for modularity and reusability, especially in data science scripts. Although the prompt asked for ""only the Python code without explanations,"" and didn't explicitly demand a function, the ground truth demonstrates a robust approach.
- The `plt.legend(title='Species')` is somewhat redundant as Seaborn's `hue` argument typically adds a legend automatically. While not harmful, it's an unnecessary line.",gemini-2.5-pro-preview,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

# 1. Read the dataset
# Assume 'penguins.csv' is in the current directory.
# If the file is not found, pandas will raise a FileNotFoundError.
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'penguins.csv' not found. Please ensure the file is in the current directory."")
    exit() # Exit the script if the file is not found

# 2. For the numerical columns `culmen_length_mm` and `culmen_depth_mm`,
#    fill any NA values with the ...",8,"The generated code correctly performs all the required tasks: loading the dataset, filling NAs in the specified numerical columns with their respective means, calculating and printing the mean, median, and standard deviation for these cleaned columns, and printing the total number of rows and columns. The error handling for `FileNotFoundError` is also present.

**Strong Points:**
*   Correctly loads the CSV.
*   Correctly implements NA imputation with the mean for each specified column separately.
*   Correctly calculates and prints the required statistics (mean, median, std dev).
*   Correctly prints dataset dimensions.
*   Includes basic error handling for `FileNotFoundError`.
*   Adds checks for column existence which is good defensive programming, although the `exit()` upon not finding a column is a bit harsh if only one of two columns is missing (though less likely in a well-defined dataset).

**Areas for Improvement to reach a 9 or 10:**
*   **Redundancy in NA handling and statistics calculation:** The code repeats the `mean()` and `fillna()` logic, and then the statistics calculation (`mean()`, `median()`, `std()`) for each of the two columns (`culmen_length_mm` and `culmen_depth_mm`). This could be made more concise and robust by iterating over a list of columns (e.g., `required_columns`) for both tasks, similar to how the ground truth code does it. This improves maintainability and scalability if more columns were to be added.
*   **Consistency in output formatting:** While the statistics are printed, the ground truth uses an f-string for the column name header (e.g., `Statistics for 'culmen_length_mm':`) which is slightly clearer than the generated code's more generic `Statistics for cleaned culmen_length_mm:` and then adds extra newlines (`print()`) for readability. The ground truth's use of a print for each `df.shape` component also felt slightly more organized, though `num_rows` and `num_cols` are perfectly correct.
*   The ground truth also provided useful print statements during the NA imputation process, showing initial NA counts and the mean used, which is helpful for debugging and understanding the imputation process. This was not required by the prompt, but adds to the completeness and robustness.",gemini-2.5-pro-preview,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'penguins.csv' not found. Make sure the file is in the current directory."")
    # In a real script, you might exit or raise the error
    # For this exercise, we'll let it proceed if the file is missing,
    # though subsequent pandas operations would fail.
    # If the problem implies the file must exist,...",8,"The generated code correctly addresses all stated requirements of the task. It loads the dataset, handles NA values in `flipper_length_mm`, and generates both the specified histogram and bar chart with appropriate titles and labels.

**Strong points:**
*   **Correct Task Fulfillment:** All points in the task description are covered: data loading, NA handling, histogram, and bar chart, all with correct labeling.
*   **Appropriate Library Usage:** Correctly uses `pandas` for data manipulation and `matplotlib.pyplot` and `seaborn` for visualization.
*   **NA Handling:** Correctly uses `df.dropna(subset=['flipper_length_mm'])` to handle NAs in the specified column.
*   **Visualization Choice:** Uses `sns.histplot` for the histogram and `sns.countplot` for the bar chart of counts, which are appropriate and efficient choices from Seaborn.
*   **Labeling:** Titles and axis labels are correctly applied as requested.
*   **`plt.tight_layout()`:** The inclusion of `plt.tight_layout()` is good practice for preventing labels from overlapping.

**Areas for Improvement (to reach a 9 or 10):**
*   **Redundant `pd.read_csv` and Error Handling:** The code has a `try-except` block for `FileNotFoundError` which is then immediately followed by an unhandled `pd.read_csv('penguins.csv')` call. This makes the `try-except` block ineffective for file not found scenarios, as the script would still crash at the second `pd.read_csv` if the file were missing. The problem statement says ""Assume the CSV is in the current directory,"" implying robust error handling might not be strictly necessary, but if included, it should be effective. The ground truth code handles this robustly by exiting.
*   **Clarity on `ylabel` for Histogram:** While `histplot`'s default y-axis label is 'Count' (or 'Frequency' depending on normalization), explicitly defining it as 'Count' would have matched the ground truth more closely if 'Frequency' isn't explicitly favored. This is a minor stylistic point.
*   **Consistency in `plt.show()`:** The generated code calls `plt.show()` only once at the very end. While this displays both plots, calling `plt.show()` after each figure's creation (as done in the ground truth) ensures that plots appear sequentially if running in an interactive environment or if a user saves plots individually. For a script that just runs and displays everything at once, the single `plt.show()` is fine, but separate calls can sometimes be clearer for complex scripts.
*   **Seaborn `kde` on Histogram:** The ground truth includes `kde=True` for `sns.histplot` which usually provides a more complete picture of the distribution. The generated code explicitly sets `kde=False`. While not incorrect (the task only asked for a histogram), including KDE would have been a slightly more informative visualization, aligning often with best practices for distributional plots using Seaborn. This is a subtle difference in interpretation of ""histogram"".",gemini-2.5-pro-preview,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

def analyze_penguins():
    """"""
    Loads the penguins.csv dataset, filters for penguins from 'Torgersen' island,
    and counts the number of penguins for each species in this subset.
    """"""
    try:
        # 1) Load the dataset
        df = pd.read_csv('penguins.csv')

        # 2) Filter the data to include only penguins from the 'Torgersen' island
        torgersen_penguins = df[df['island'] == 'Torgersen']

        # 3) For this filtered subset, count the nu...",8,"The generated code correctly implements all the requirements of the task: it loads the dataset, filters by 'Torgersen' island, and counts the species. The use of a `try-except` block for file handling is good practice. Encapsulating the logic in a function `analyze_penguins` is also a positive for code organization.

Areas for improvement (minor):
1.  **Missing edge case handling for empty filter result:** The ground truth explicitly checks if `torgersen_island_df` is empty after filtering. While `value_counts()` on an empty Series might not crash, a more informative message would be helpful if no 'Torgersen' penguins are found.
2.  **Missing column existence checks:** The ground truth includes checks like `if 'island' not in df.columns:` to prevent `KeyError` if the expected columns are missing. This makes the code more robust.
3.  **Less explicit printing:** The ground truth provides more verbose and structured print statements, indicating success or failure at various steps, which can be useful for debugging and user understanding.

Overall, the generated code is functional, clean, and addresses the core requirements effectively. The suggested improvements are primarily about robustness and user experience, not core functionality.",gemini-2.5-pro-preview,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

# Create the DataFrame
df = pd.DataFrame(data...",10,"The generated code perfectly matches the ground truth code and correctly addresses all aspects of the task. It properly defines the DataFrame, prints its shape, and lists the data types of each column, all with correct syntax and output formatting. The code quality is excellent, with clear variable names and logical flow. Specifically, it demonstrates:
- Correct import of `pandas`.
- Accurate definition of the DataFrame using the provided data dictionary.
- Correct usage of `df.shape` to print the DataFrame's dimensions.
- Correct usage of `df.dtypes` to list column data types.
- Inclusion of the requested separators for readability.

There are no missing requirements or inaccuracies.",gemini-2.5-pro-preview,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

def create_titanic_visualizations():
    # Load the dataset
    try:
        df = pd.read_csv('titanic.csv')
    except FileNotFoundError:
        print(""Error: 'titanic.csv' not found in the current directory."")
        return

    # 1. Bar chart showing the count of survivors vs non-survivors
    plt.figure(figsize=(6, 4))
    survivor_counts = df['Survived'].value_counts().sort_index()
    # Map numerical Survived values to meanin...",8,"The generated code successfully creates four distinct plots with appropriate titles and axis labels, addressing most of the task requirements.

**Strong Points:**
*   **Correct Plots:** All four requested plot types (bar charts for survival, class, gender, and histogram for age) are correctly generated.
*   **Proper Titles and Labels:** Each plot includes a suitable title and correct x and y axis labels.
*   **Handling Missing Age Values (Partial):** It handles missing age values by dropping them for the histogram, which is a valid approach, although not necessarily the ""most appropriate"" in all data science contexts (median/mean imputation is often preferred).
*   **Proper Imports:** `pandas` and `matplotlib.pyplot` are correctly imported.
*   **Code Structure:** Encapsulating the logic within a function (`create_titanic_visualizations`) and using `if __name__ == '__main__':` is good practice.
*   **Error Handling:** Includes a basic `FileNotFoundError` check for the CSV.

**Areas for Improvement:**
*   **Age Missing Value Handling:** While `dropna()` is a valid approach, the ground truth's choice of median imputation is generally considered more robust for histograms as it preserves more data and might be more representative of the overall distribution if the missing data mechanism is not entirely random. The prompt stated ""handle missing values appropriately,"" and while `dropna()` is appropriate, imputation is often ""more appropriate"" to retain data.
*   **Consistency in Plotting Methods:** For bar charts, the generated code mixes `plt.bar` (for survival) and `.plot(kind='bar')` for others. While both achieve similar results, using `.plot(kind='bar')` directly on the `value_counts()` Series is often more concise and idiomatic for pandas Series.
*   **Plot Customization/Refinement:** The ground truth includes minor refinements like `grid` lines and `tight_layout()` for every plot, which improve readability. `tight_layout()` is used consistently, but grid lines are missing.
*   **X-tick Rotation:** While `plt.xticks(rotation=0)` is fine, the ground truth's use of `plt.xticks(rotation=0)` across all bar charts ensures consistency and prevents potential label overlap if the category names were longer. The generated code also does this for the first plot but implicitly relies on default behavior for others, which might or might not be 0 depending on Matplotlib version/defaults. (Upon review, it seems `rotation=0` is set for all of them, which is good).

The primary difference from the ground truth is the choice of handling missing age values and some minor aesthetic differences (like grid lines), but the core task is well-addressed.",gemini-2.5-pro-preview,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]
}
df = pd.DataFrame(data)

# 1) Calculate and print the average 'Midterm_Score...",9,"The generated code correctly performs all requested tasks: it creates the DataFrame, calculates and prints the average midterm score, calculates and prints the average final score, and correctly identifies the subject with the highest average final score. The code structure is clear, and necessary imports are included.

The only minor point of improvement compared to the ground truth is the formatting of the printed average scores. The ground truth uses f-strings with `:.2f` to format the scores to two decimal places, which is good practice for numerical averages. The generated code does not include this formatting, though it doesn't detract from correctness. The generated code also condensed the output for the subject with the highest average final score, which is perfectly acceptable. Overall, a very good solution.",gemini-2.5-pro-preview,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

# Data for the DataFrame
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
    'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
    'Volume': [100000...",10,"The generated code is excellent. It correctly imports pandas, creates the DataFrame, sets 'Date' as the index, calculates the 'Daily_Range' column, finds and prints the date with the highest 'Close' price, and finally prints the full DataFrame with the new column. The code is clean, follows proper conventions, and addresses all requirements of the task. The use of `idxmax()` to find the date and `strftime('%Y-%m-%d')` for formatting is both correct and efficient.",gemini-2.5-pro-preview,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004'...",10,"The generated code is correct, complete, and of high quality. It accurately creates the DataFrame, calculates the 'Total_Revenue' column, correctly identifies the product sold the most by quantity, and prints the first 5 rows with the new column, fulfilling all requirements of the task. The structure is clear, and necessary imports are present. There are no identifiable issues or areas for significant improvement.",gemini-2.5-pro-preview,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 1) Calculate and print the average ""SessionDurationMinutes""
average_session_duration = df[""SessionDura...",10,"The generated code correctly defines the DataFrame, calculates the average session duration, and counts users by device type as per the task requirements. The code is clean, efficient, and directly addresses all parts of the problem. It properly imports pandas, creates the DataFrame, and uses appropriate pandas methods (`.mean()` and `.value_counts()`) for the calculations. The output is clear and easy to understand. Both the correctness and completeness are excellent. The code quality is also very good, adhering to standard Python practices for data manipulation tasks.",gemini-2.5-pro-preview,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

# Create the DataFrame
df = pd.DataFrame(data)

# 1) Find and print...",10,"The generated code is excellent. It correctly defines the DataFrame exactly as specified in the task, and then accurately finds and prints both the maximum ""TemperatureCelsius"" and the minimum ""HumidityPercent"". The use of `df[""ColumnName""].max()` and `df[""ColumnName""].min()` is the standard and most efficient way to perform these operations in pandas. The code is clean, well-structured, and includes all necessary imports. It fully addresses all requirements of the prompt. The printed output is clear and descriptive.",gemini-2.5-pro-preview,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# 1) Print the total number of ""Likes"" across all posts.
total_likes = df[""Likes""].sum()
print(f""Total num...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the total number of likes, and accurately identifies the `PostType` with the highest average number of shares. The code is well-structured, uses appropriate pandas functions, and directly addresses all parts of the task requirements. The addition of printing the actual average shares for the highest PostType, while optional, enhances the clarity of the output. The comments are also clear and helpful.",gemini-2.5-pro-preview,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male""...",9,"The generated code is almost identical to the ground truth code and correctly addresses all aspects of the task. It successfully defines the DataFrame, calculates the average age, and counts patients by gender.

The only minor difference, which is negligible in terms of correctness and output, is the formatting of the `average_age` print statement. The ground truth code formats it to two decimal places (`:.2f`), whereas the generated code does not specify a format, leading to a default float representation. This is a very minor stylistic difference and does not impact the correctness of the computation or the interpretability of the result.

Strong points:
- **Correctness:** All calculations and DataFrame creation are accurate.
- **Completeness:** Addresses all requirements of the task.
- **Code Quality:** Proper imports and clear, readable code structure.
- **Efficiency:** Uses appropriate pandas methods for the required operations.

Improvement:
- While not strictly necessary, adding `.2f` to the f-string for `average_age` would make the output consistent with the ground truth and often more presentable for numerical outputs.",gemini-2.5-pro-preview,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}
df = pd.DataFrame(data)

# 1) Print the ""Title"" of the movie with the highest ""Rati...",10,"The generated code is correct, complete, and demonstrates good code quality. It accurately defines the DataFrame, correctly identifies the title of the movie with the highest rating using `idxmax()`, and precisely calculates the average year of all movies. The use of f-strings for printing enhances readability. The solution correctly addresses all parts of the task efficiently.",gemini-2.5-pro-preview,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the car sales data
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

# Create the pandas DataFrame
df = pd.DataFrame(data)

# 1) Print...",10,"The generated code is excellent. It correctly defines the DataFrame exactly as specified, calculates the total number of cars, and computes the average ""PriceUSD"". The output formatting for the average price is also correct, using `:.2f` for two decimal places. The code is clean, well-structured, and directly addresses all parts of the task. Using `len(df)` for the total number of cars is a perfectly valid and common alternative to `df.shape[0]`.",gemini-2.5-pro-preview,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame data
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}

# Create the DataFrame
df = pd.DataFrame(data)

# 1)...",10,"The generated code is excellent. It correctly defines the DataFrame as specified, and accurately identifies and prints the names of the Pokemon with the highest HP and lowest Defense. The variable names are clear, and the use of `idxmax()` and `idxmin()` followed by `loc` is the correct and efficient way to achieve the task. The comments are concise and helpful. It perfectly meets all requirements of the task.",gemini-2.5-pro-preview,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd
import re

# 1) Load the dataset
try:
    df = pd.read_csv('titanic.csv')
except FileNotFoundError:
    print(""Error: 'titanic.csv' not found in the current directory."")
    exit()

# Identify columns with missing values
print(""Missing values before cleaning:"")
print(df.isnull().sum())
print(""-"" * 30)

# 2) Fill missing Age values with the median age
median_age = df['Age'].median()
df['Age'].fillna(median_age, inplace=True)

# 3) Fill missing Embarked values with th...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, handles missing values for 'Age' and 'Embarked' as specified, drops the 'Cabin' column, and extracts titles from the 'Name' column to create a new 'Title' column. The code also correctly displays the shape and the first 5 rows of the cleaned dataset.

The code quality is good, with proper imports and a logical flow matching the task's steps. The error handling for `FileNotFoundError` is also a good practice.

One minor difference from the ground truth is that the generated code doesn't normalize the less common titles (e.g., 'Mlle' to 'Miss', 'Capt' to 'Rare'). While the prompt didn't explicitly ask for this, the ground truth included it as good practice for feature engineering, which makes the 'Title' column more useful for subsequent analysis. The generated code explicitly mentions this as an optional step it decided to omit for strict adherence, which is understandable given the prompt. However, from an ""expert code evaluator"" perspective for data cleaning, consolidating titles is often an expected next step. This is a very minor point and doesn't detract significantly from the solution's correctness or completeness in relation to the explicit instructions.",gemini-2.5-pro-preview,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}
df = pd.DataFrame(d...",9,"The generated code correctly defines the DataFrame, accurately identifies the title of the book with the highest price, and correctly calculates the total number of books in stock. The imports are proper, and the code structure is clear and easy to understand.

Improvements: The print statements in the generated code just output the title and the total stock value without any descriptive text. While functionally correct, adding a more informative print statement (like in the ground truth code) would improve user comprehension and make the output more professional. This is a minor point, but it slightly affects the ""Completeness"" aspect from a user-experience perspective. For example, instead of `print(title_highest_price)`, it could be `print(f""The title of the book with the highest price is: {title_highest_price}"")`.",gemini-2.5-pro-preview,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
try:
    df = pd.read_csv('water_potability.csv')
    print(""1) First 10 rows of the dataset:"")
    print(df.head(10))
    print(""\n"" + ""=""*50 + ""\n"")

    # 2) Show the shape of the dataset (rows and columns)
    print(""2) Shape of the dataset (rows, columns):"")
    print(df.shape)
    print(""\n"" + ""=""*50 + ""\n"")

    # 3) List all column names and their data types
    print(""3) Column names and their data types:...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
*   **Correctness:** All requested analyses (loading, shape, dtypes, potability percentage, missing values, summary statistics) are performed correctly.
*   **Clarity:** The print statements clearly label each section of the output, making it easy to follow.
*   **Error Handling:** Includes `try-except` blocks for `FileNotFoundError` and general exceptions when loading the dataset, which is good practice.
*   **Potability Calculation:** Uses `value_counts(normalize=True) * 100` which is a concise and correct way to get percentages.

**Areas for Improvement (to achieve a higher score):**
*   **`df.dtypes` vs `df.info()`:** While `df.dtypes` fulfills the request of listing column names and their data types, `df.info()` (used in the ground truth) is generally preferred in EDA as it also provides non-null counts and memory usage, offering a more complete picture of the dataset's structure at a glance.
*   **Missing Values Display:** The generated code prints `df.isnull().sum()`. The ground truth additionally filters this to `missing_values[missing_values > 0]` and checks if `missing_values.sum() == 0`. This offers a cleaner output by only showing columns with missing values and explicitly stating if none are found. The current implementation will print all columns, even those with 0 missing values, which can be verbose for wide datasets.
*   **Interpretive Prints for Potability:** The ground truth includes a print statement explaining ""1 = Potable, 0 = Non-Potable"", which enhances the readability of the potability breakdown.
*   **`__name__ == '__main__'` Block:** While technically not incorrect, wrapping the entire script execution within `if __name__ == '__main__':` is a more standard Python practice when designing scripts that might also be imported as modules. The current code executes everything at the top level and then has an empty `if __name__ == '__main__':` block, which isn't necessary.

Overall, it's a very good attempt that successfully completes the core task. The suggested improvements are mainly for enhancing output clarity and adhering to slightly better EDA practices.",gemini-2.5-pro-preview,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd

# 1. Load the dataset
df = pd.read_csv('water_potability.csv')

# --- Capture Before Statistics (Part of Step 1 & 5) ---
# Identify columns with missing values (initial state)
initial_shape = df.shape
initial_missing_values = df.isnull().sum()

# 2. Fill missing values using appropriate strategies (mean for numerical columns)
for column in df.columns:
    if df[column].isnull().any(): # Check if column has any NaN values
        if pd.api.types.is_numeric_dtype(df[...",7,"The generated code addresses most of the requirements but has significant deviations from the ground truth's approach to data range validation, which could be less effective for robust data cleaning.

**Strengths:**
*   **Missing Values:** Correctly identifies and fills missing numerical values with the mean.
*   **Duplicates:** Correctly identifies and removes duplicate rows.
*   **Summary:** Provides a before/after summary of missing values and dataset shape, fulfilling part of the requirement.
*   **Readability:** The code is reasonably well-structured and easy to read.

**Areas for Improvement/Deviations from Ground Truth:**
*   **Error Handling for File Loading:** The generated code lacks `try-except` for `FileNotFoundError` when loading the CSV, which is present in the ground truth and is a good practice for robust code.
*   **Range Validation Strategy:**
    *   **Clipping vs. Removal:** The generated code uses `clip()` for range validation, which keeps all rows but forces values within the specified bounds. The ground truth (which is often preferred in strict data cleaning for potential data quality issues) **removes** rows that are outside reasonable physical ranges. Clipping might be acceptable in some scenarios, but removing outright physically impossible values is generally a stronger cleaning step. The prompt also specifies ""Verify that all values are within reasonable ranges"" which implies checking and potentially handling invalid values, not just constraining them to a specific range (which clipping does). Removing rows is a more aggressive and often more appropriate way to handle data points that are clearly errors.
    *   **Positive Value Check:** The ground truth explicitly checks for non-negative values for several parameters, ensuring physical impossibility is handled. The generated code's `value_ranges` dictionary implicitly handles this if the `min_val` is set to 0, but it relies on pre-defined hardcoded ranges for all columns, which might not be exhaustive or universally applicable for all ""reasonable ranges"" across the diverse parameters. The ground truth's approach of verifying `pH` and then separately checking for non-negativity for other relevant parameters is more explicit and robust.
*   **Summary Detail:** The summary output is somewhat less detailed than the ground truth. For example, it doesn't explicitly state how many duplicate rows were removed or how many rows were affected by range checks, which the ground truth clearly shows. The prompt also implicitly requires showing the impact of *each* cleaning step.
*   **Numerical Column Identification for Missing Values:** While functional, the ground truth's approach of `df.select_dtypes(include=np.number).columns[df.isnull().any()].tolist()` is more concise and arguably more robust for identifying *only* numerical columns that *actually have* missing values, rather than iterating through all columns and then checking `is_numeric_dtype`.

For a ""expert code evaluator,"" the key difference lies in the range validation. Clipping is a softer approach than removing, and for physically impossible values, removal is generally preferred. This choice impacts the quality of the ""cleaned"" data significantly.",gemini-2.5-pro-preview,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def load_and_clean_data(filepath='water_potability.csv'):
    """"""Loads the dataset and handles missing values by dropping rows.""""""
    try:
        df = pd.read_csv(filepath)
    except FileNotFoundError:
        print(f""Error: '{filepath}' not found in the current directory."")
        return None
    
    # Drop rows with any NaN values for consistent plotting
    df_cleaned = df.dropna()
    return df_cleaned
...",8,"The generated code creates all the required visualizations with proper titles and labels. It uses functions to modularize the code, which is good practice. The handling of missing values by dropping rows is a consistent approach, although the ground truth uses `dropna()` specifically for the histogram features and relies on `corr()` for the heatmap.

However, there are a few areas for improvement:

1.  **Missing Value Handling:** The generated code performs `df.dropna()` at the very beginning. While this ensures consistency, it leads to a loss of a significant portion of the dataset, especially if `ph` has many missing values. The ground truth code handles `NaN`s more judiciously: `histplot` and `boxplot` automatically ignore NaNs for their respective plots, and `corr()` also handles NaNs pairwise. For real-world data science, dropping all rows with NaNs upfront can be problematic. This is a key difference.
2.  **Histogram Plotting:** The generated code creates a separate `plt.figure()` and `plt.show()` for *each* histogram. This results in three separate histogram windows, which is less ideal than displaying them as subplots within a single figure, as done in the ground truth. The task implies ""histograms for pH, Hardness, and Chloramines showing their distributions,"" which usually means a single, composite figure for comparison.
3.  **Redundancy in `xticklabels`:** For the bar chart and boxplot, the ground truth explicitly sets `xticks` and `labels` to map 0 and 1 to 'Not Potable' and 'Potable' respectively. The generated code uses `'Not Potable (0)'` and `'Potable (1)'` directly, which is less clean and somewhat redundant if the labels are already descriptive.
4.  **Error Message and Handling:** While the file not found error handling is good, the message `TheDataFrame is empty after cleaning (e.g., all rows had NaNs)` could be slightly refined (e.g., ""The DataFrame is empty after dropping rows with NaNs..."").

Overall, the code is functional and meets most requirements. The handling of missing values is a crucial difference that makes the ground truth more robust and less data-loss-prone. The histogram plotting approach is also less efficient than using subplots.",gemini-2.5-pro-preview,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor

def perform_feature_engineering_and_selection():
    # 1. Load Data
    try:
        # Try loading with comma delimiter first
        df = pd.read_csv('winequality-red.csv')
        # If it seems to have loaded as a single column (likely wrong delimiter), try semicolon
        if df.shape[1] == 1 and ';' in df.iloc[0,0]: # Check...",8,"The generated code successfully addresses most aspects of the task, demonstrating a good understanding of feature engineering and selection.

**Strong points:**
- **Robust CSV loading:** The code attempts different delimiters and includes error handling for `FileNotFoundError`.
- **Meaningful Engineered Features:** Provides three meaningful new features (`total_acidity`, `free_sulfur_ratio`, `alcohol_squared`) and a bonus one, with conditional creation based on column existence. These are good examples of common feature engineering types (sum, ratio, polynomial).
- **Robust Feature Engineering:** Handles potential division by zero and `NaN`/`inf` values introduced during feature engineering, which is crucial for real-world data.
- **Correct Feature Selection:** Both RFE and SelectKBest are implemented correctly, and they target the specified number of features.
- **Comparison of Methods:** The code correctly identifies common, RFE-only, and KBest-only features, comparing the outputs as requested.
- **Clear Output Display:** The final chosen features and their scores/rankings are displayed clearly.
- **Good Code Structure:** Encapsulates the logic in a function and uses a `if __name__ == '__main__':` block.

**Areas for improvement (to reach a higher score):**
- **Feature Scaling:** The ground truth code includes `StandardScaler` which is generally good practice before RFE (though `RandomForestRegressor` is less sensitive to it than some other models). While not strictly necessary for `RandomForestRegressor` and `f_regression`, it's a critical tool in ML pipelines for ensuring features are on a comparable scale, especially when comparing different models or using distance-based algorithms. The generated code omits this.
- **RFE Feature Importance Display:** The ground truth code correctly trains the final RFE estimator on *only* the selected features to get their importances, which is the most appropriate way to display importances for selected RFE features. The generated code tries `selector_rfe.estimator_.feature_importances_` which might not always work as expected or give importances for the *full* feature set used in RFE's initial fit, not just the selected ones. The current approach will try to display importances from the last iteration of the estimator *before* it gets wrapped by RFE, which can be misleading regarding the final selected features.
- **Clarity in Comparison:** While the comparison is done, the ground truth code also provides a brief written explanation of *why* the methods might differ (wrapper vs. filter, multivariate vs. univariate), which adds significant value to the ""compare and contrast"" requirement.
- **Minor warnings/redundancy:** The `print(""Warning: ... not created"")` messages are good, but the subsequent `fillna(0, inplace=True)` for ratios might not be strictly necessary if `np.where` properly handles division by zero as intended, and `fillna(X_engineered[col].mean())` is a robust fallback for general NaNs. However, the use of `np.where` for division by zero makes the ratio engineering more robust than simply adding epsilon.

Overall, the generated code performs very well against the given task. The missing scaling and slight nuance in RFE importance display are minor points that could be refined.",gemini-2.5-pro-preview,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, cross_validate
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor, StackingRegressor
import xgboost as xgb
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from s...",7,"The generated code provides a good attempt at addressing the complex task. It correctly implements multiple ensemble methods (Random Forest, Gradient Boosting, XGBoost, AdaBoost), performs 5-fold cross-validation, and includes both voting and stacking ensembles. It also attempts hyperparameter tuning with GridSearchCV and performs residual analysis.

However, there are several areas for improvement:

1.  **Data Preprocessing:** The generated code moves the preprocessing logic into a function that returns `X` and `y` already processed. While this can be cleaner, it deviates significantly from the standard `sklearn.pipeline.Pipeline` and `ColumnTransformer` approach demonstrated in the ground truth for handling numerical and categorical features more robustly within the ML workflow (especially with `SimpleImputer` and `OneHotEncoder` within a pipeline). This makes it harder to properly integrate preprocessing into cross-validation and hyperparameter tuning for models that reside within pipelines, and incorrectly separates the preprocessing from the modeling steps for each fold. The `drop_first=True` argument in `pd.get_dummies` for one-hot encoding means it won't correctly align with unseen categories if the training and test sets have different unique values for categorical features. It also doesn't handle `User_Score` 'tbd' conversion.
2.  **Target Transformation Consistency:** The code applies `np.log1p` to `Global_Sales` but doesn't consistently apply `np.expm1` to predictions *before* evaluating them. While `evaluate_model_on_original_scale` does this, the direct cross-validation calculation (`cross_validate` output and `cv_model_scores`) is performed on the log-transformed `y`, which is not directly comparable to the metrics required for the original scale, and the problem specifically asked for evaluation on the *original scale*. This makes the initial cross-validation metrics misleading.
3.  **Cross-Validation within Pipelines:** The ground truth correctly integrates the preprocessor into the `Pipeline` for each model, ensuring that the preprocessing steps are applied independently within each cross-validation fold. The generated code separates preprocessing, doing it once globally. This leads to data leakage from the test set into the training set during one-hot encoding and imputation if not handled carefully, and means the feature engineering (Publisher_Encoded) is also done uniformly rather than per fold.
4.  **Feature Importance:** The `plot_feature_importances` function takes `model` trained on `X_train` and `feature_names` from `X_train.columns`. This is problematic because `X_train.columns` will reflect the names after `pd.get_dummies` global preprocessing, but `feature_importances_` might shift indices if some features are not present in a given split or if the scaling affects coefficient interpretation (though for tree models, not directly). The ground truth correctly uses `ColumnTransformer` to get the actual feature names after transformation, including one-hot encoded ones.
5.  **Completeness & Best Practices:**
    *   The `StackingRegressor` in the generated code comments out `AdaBoost`, which is fine, but it should be explicitly stated as a choice rather than left as a comment.
    *   The `tuned_best_model` is evaluated only on the test set, not with cross-validation after tuning, which would give a more robust estimate of its performance.
    *   The generated code includes a `main()` function, which is good practice, but the requested output was only the Python code without explanations, implying a script-like structure similar to the ground truth.
    *   Missing `SimpleImputer` for handling missing values within numerical features or `User_Score`.
    *   The `StandardScaler` is commented out, indicating it was considered but not integrated. While not strictly necessary for tree models, it's good to show awareness.

To improve, the generated code should: incorporate `ColumnTransformer` and `Pipeline` for end-to-end preprocessing within cross-validation, ensure all evaluations (including cross-validation) are correctly performed on the original scale by transforming predictions, and refine feature importance plotting by getting correct feature names from the preprocessor.",gemini-2.5-pro-preview,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Statsmodels
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.stats.outliers_influence import OLSInfluence # For prediction intervals manually if needed

# Scikit-learn
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.prepro...",7,"The generated code attempts to address all requirements of the prompt and demonstrates a good understanding of various data science techniques.

**Strong Points:**
- **Comprehensive Coverage:** Addresses most of the requested tasks, including ARIMA, seasonal decomposition, multivariate regression, polynomial regression with regularization, Bayesian regression, change point detection, survival analysis, and time series cross-validation.
- **Correct Imports:** Uses appropriate libraries for each task.
- **Good Structure:** The code is well-sectioned with clear headings for each task.
- **Error Handling:** Includes `try-except` blocks for file loading and some model fitting, which is good practice.
- **Plotting:** Generates plots for visualizations where appropriate (ARIMA, seasonal decomposition, change point, Kaplan-Meier).
- **Time Series Cross-Validation:** Correctly uses `TimeSeriesSplit` for validation.
- **PyMC3 (renamed PyMC in recent versions):** Successfully implements the Bayesian regression model, which is a complex task.

**Areas for Improvement / Missing Aspects:**
- **ARIMA Implementation:** The prompt asked for `pmdarima` for auto_arima to find the best model automatically, while the generated code uses `statsmodels.tsa.arima.model.ARIMA` with a fixed order (5,1,0). The ground truth specifically used `pm.auto_arima` as requested. This is a significant deviation for the ARIMA task.
- **Seasonal Decomposition Period:** For yearly data, `seasonal_decompose` might struggle with a fixed small period like 7 if the time series is not long enough or doesn't exhibit such a clear cyclical pattern. The ground truth handled this more robustly by checking data length and providing a plausible explanation.
- **Multivariate Regression Interaction Terms:** The generated code's formula for interaction terms `Year * C(Genre) + Year * C(Platform_Reduced) + C(Genre):C(Platform_Reduced)` is correct in syntax, but the chosen `C(Genre):C(Platform_Reduced)` term is a very large number of interaction terms when combined with `Year * C(Genre)` and `Year * C(Platform_Reduced)`, especially if `Genre` and `Platform` have many categories. The ground truth performed a more robust cardinality reduction strategy to prevent model instability and excessive features. The error handling for the model fit also suggests potential issues here.
- **Polynomial Regression Interaction Terms:** The requirement was to build a multivariate regression with `Genre, Platform, Publisher` and *interaction terms*. The polynomial regression section uses `Year` for polynomial features and `Genre` as a categorical feature, but doesn't explicitly include `Platform` and `Publisher` or multi-feature interactions as part of the *polynomial regression* model itself which is a slightly different interpretation. The task implied polynomial regression could include these features.
- **Survival Analysis Artificial Longevity:** The `Years_On_Market` and `Event_Observed` are artificially generated with random choices in the generated code. While the prompt asks for *modeling* game longevity, it implicitly suggests deriving it from the existing sales data (e.g., how long a game remained profitable or appeared in sales data), not creating artificial data. The ground truth provides a more realistic (though simplified) derivation of longevity from the dataset itself.
- **Confidence/Prediction Intervals for Ridge/Lasso:** The generated code acknowledges that these are not directly provided by scikit-learn but doesn't offer any approximation or alternative method (like bootstrapping or calculating from residuals, which the ground truth does as an example).
- **Survival Analysis Plotting (CoxPH):** While it plots coefficients, it doesn't plot predicted survival functions for example cases directly as the ground truth does, which provides a more direct prediction interval interpretation.
- **Data Cleaning for 'Year' column:** The ground truth handled the `Year_of_Release` column (which is named 'Year' in the generated solution's data) more robustly by coercing errors to NaN first, then dropping NaNs, instead of directly dropping 'N/A' strings which might miss other non-numeric values.
- **PyMC version compatibility:** The generated code uses `pymc` which typically refers to PyMC v4+, while the ground truth used `pymc3`. Although `pymc` is the current maintainer, `pymc3` was widely used for specific tasks which `pymc` might have a slightly different API. This is a minor point, but worth noting.

The code is certainly runnable and implements most of the concepts but some details, especially in ARIMA and the data derivation for survival analysis, deviate from the expected expert-level implementation or specific prompt requests.",gemini-2.5-pro-preview,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE
import umap
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.st...",8,"The generated code provides a comprehensive solution for the given task, implementing most of the required functionalities.

**Strong Points:**
*   **Comprehensive Feature Engineering:** The code correctly implements the specified feature engineering (""Regional\_Sales\_Distribution"", ""Market\_Position"", ""Publisher\_Portfolio\_Size"").
*   **Multiple Clustering Algorithms:** K-Means, DBSCAN, Hierarchical Clustering, and GMM are all implemented.
*   **Optimal Cluster Determination:** Elbow method (WCSS), Silhouette Score, AIC/BIC for GMM, and even Gap Statistic (with a try-except block) are included, demonstrating a robust approach.
*   **Dimensionality Reduction:** t-SNE and UMAP are correctly used for visualization of clusters.
*   **Consensus Clustering:** A good attempt at consensus clustering using co-occurrence matrix and hierarchical clustering is made, which is an advanced technique.
*   **Detailed Cluster Profiling:** Statistical significance testing (ANOVA, Chi-squared) and visualization of profiles are included, which is excellent.
*   **Anomaly Detection:** Both Isolation Forest and LOF are implemented.
*   **Association Rule Mining:** Data preparation (discretization, one-hot encoding) and using `mlxtend` for Apriori and association rules looks good.
*   **Recommendation System:** A basic content-based recommendation system based on clusters is built.
*   **Clustering Validation:** Internal validation metrics (Silhouette, Davies-Bouldin, Calinski-Harabasz) are implemented.
*   **Code Structure and Readability:** The code is well-organized into functions, making it modular and easier to understand.
*   **Error Handling:** Includes `try-except` blocks for file loading and Gap Statistic, and checks for empty dataframes or insufficient samples before performing certain operations.

**Areas for Improvement (Why not a 9 or 10):**
*   **Data Preprocessing and Consistency:**
    *   The `load_and_preprocess_data` function filters `df[df['Global_Sales'] > 0].copy()`, but `df_original_global` accessible outside this function (which should represent the *original* dataframe) isn't updated. This creates a slight mismatch in subsequent steps where `df_original_global` is used for profiling or ARM, but `df_scaled_global` comes from a subset. The ground truth explicitly creates `df_original` *before* the filter to correctly use it for profiling later.
    *   The `df_original_global_subset` variable is used in anomaly detection and recommendation system, trying to align with the scaled data, but the consistent way the ground truth handles this (copying `df_original` *before* processing `X_processed` and then merging labels) is cleaner and less prone to indexing issues.
    *   The `df_original_global` in `__main__` is assigned the returned value from `load_and_preprocess_data`, which *already* has filtered rows. This will lead to discrepancies when trying to align original data with scaled data using `iloc[df_scaled_global_indices]` if `df_original_global` itself has already been filtered. A better approach would be to return the *unfiltered* df from `load_and_preprocess_data` (or the initial load of it) and then create `X` (for scaled and clustered data) from a *processed subset* of that `df`. The ground truth handles this more robustly by creating `df_original = df.copy()` *before* any feature engineering or filtering that might remove rows used for clustering.
*   **DBSCAN Parameter Selection:** While attempting to be adaptive using `np.percentile(distances, 90)` for `eps`, this is a heuristic that might not be optimal for all datasets. Visual inspection of the k-distance graph is good, but automating the ""knee"" detection (like the ground truth uses `KneeLocator`) is more robust.
*   **Consensus Clustering Implementation:** The consensus clustering uses `AgglomerativeClustering` with an `affinity='precomputed'` and `distance_matrix`. This is a valid approach, but sometimes using a full consensus ensemble clustering method (e.g., from a specialized library or specific algorithms like M3C) may achieve more stable results, or running multiple different clustering algorithms for the bootstraps (not just K-Means). The ground truth's approach of summing co-occurrences across K-Means, GMM, etc., and then clustering *that* co-occurrence matrix is conceptually more robust in the context of ""consensus across different algorithms"". The generated code only uses K-Means for consensus bootstrapping.
*   **Dendrogram for Hierarchical Clustering:** The comment to limit dendrogram to smaller datasets due to complexity and runtime is good, but the code for it is commented out. Showing it dynamically based on dataset size (as suggested in the comment) would be a nice addition.
*   **Gap Statistic:** While included, the range `min(max_k + 1, 8)` for `gap_k_range` is quite small and potentially restrictive for finding optimal clusters. It's a pragmatic choice for speed but limits its utility for larger optimal `k`.
*   **Minor details:**
    *   `umap.umap_` vs. `umap` for import (minor but inconsistent).
    *   The `KneeLocator` from `kneed` is an excellent addition for elbow detection and is present in the ground truth but not explicitly used in the generated code's elbow detection.

Overall, the generated code is very good, demonstrating a strong understanding of the task requirements and relevant data science techniques. The identified areas for improvement primarily relate to greater consistency in data handling across different stages and some minor algorithmic refinements.",gemini-2.5-pro-preview,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import (
    train_test_split,
    cross_val_score,
    StratifiedKFold,
    cross_val_predict
)
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import (
    accuracy_score,
    roc_auc_score,
    classification_report,
    confusion_matrix,
    f...",7,"The generated code provides a good attempt at addressing the complex requirements of the task. It correctly identifies and implements many of the requested components, including various ensemble methods, Optuna for hyperparameter tuning, SHAP for interpretation, and basic calibration. However, there are several areas for improvement, particularly regarding the rigor of nested cross-validation implementation, feature engineering, and some minor issues with plotting and CatBoost integration.

**Strong Points:**
- **Comprehensive Ensemble Methods:** Implementation of Voting (soft/hard), Bagging (DT, KNN), RandomForest, ExtraTrees, AdaBoost, Gradient Boosting, XGBoost, LightGBM, and Stacking is well-covered.
- **Bayesian Optimization (Optuna):** Correctly integrates Optuna for hyperparameter tuning of LightGBM, demonstrating the core functionality.
- **SHAP Interpretation:** Successfully uses SHAP for global interpretation (summary plots) and attempts local interpretation, and correctly handles `shap_values[1]` for binary classification.
- **Model Calibration:** Implements `CalibratedClassifierCV` (Isotonic) and plots calibration curves along with Brier scores, which is good.
- **Diversity Analysis:** Attempts to analyze ensemble diversity through correlation matrices and Q-statistic, which is a valuable addition.
- **Data Preprocessing:** Handles missing values and scales numerical features.

**Areas for Improvement / Missing Aspects:**
1.  **Feature Engineering Robustness:**
    *   The `Pregnancy_Category` and `Age_Group` features are created, which is good.
    *   However, the imputation logic `df.groupby('Outcome')[col].transform(lambda x: x.fillna(x.median()))` introduces data leakage, as it uses the `Outcome` (target) to impute missing values. This should be avoided, and imputation should typically occur on the training data only, without knowledge of the test/target.
    *   The feature engineering is less ""comprehensive"" than the ground truth, which includes `PolynomialFeatures` and explicit interaction terms.

2.  **Nested Cross-Validation Implementation:**
    *   The attempt at nested CV (Section 5) is conceptually correct in setting up outer and inner folds.
    *   However, the data preprocessing (`current_preprocessor.fit_transform(X_train_outer)`) needs to be carefully managed within the loop. The `fit_transform` on `X_train_outer` is correct inside the loop, but the way `numerical_features` and `categorical_features` are defined globally might lead to issues if the feature list changes dynamically or if numerical features are not distinct in each fold.
    *   Crucially, the nested CV evaluates only a single model (LGBM tuned by Optuna), not ""all"" models and ensembles as requested. The task asked for ""unbiased model evaluation"" for all models, implying all models discussed in the code should be evaluated via nested CV, or at least a representative set. The ground truth has a more rigorous nested CV loop where each model's performance was tracked across outer folds.
    *   The `n_trials` for Optuna inside the nested loop is very small (10 trials), making the tuning quite superficial for proper evaluation.

3.  **CatBoost Integration:**
    *   The CatBoost handling of categorical features is attempted, but complex. The manual scaling of numerical features and then re-concatenating with categorical features is prone to errors (e.g., `X_train_catboost_numerical_scaled = scaler_cat.fit_transform(X_train[numerical_features])` uses a part of `X_train_val` as `X_train`, which is confusing and assumes `X_train` is `X_train_val`'s `train` split, not the global `X_train`). The indices for `cat_features` for CatBoost are correctly identified, but the data preparation was a bit convoluted.
    *   The ground truth demonstrates a cleaner approach of just fitting the CatBoost model directly with the full `X`, assuming no explicit `cat_features` are needed after initial preprocessing, which simplifies the pipeline for this specific dataset.

4.  **Custom Ensemble:** The custom ensemble uses static `X_train`, `X_val`, `X_test` splits which is fine for demonstration, but a more robust approach (especially if integrated into nested CV) would involve dynamically splitting within the CV loop. The dynamic weighting is based on AUC on the validation set, which is a valid strategy.

5.  **Plots and Visualizations:**
    *   SHAP plots are attempted correctly.
    *   Calibration plots are good.
    *   The `force_plot` from SHAP is not included (though the ground truth notes it might not display directly, it's provided in the code).

6.  **Minor Code Quality:**
    *   `use_label_encoder=False` for XGBoost is now standard practice.
    *   `sparse_output=False` for `OneHotEncoder` is good.
    *   Logging warnings were handled, good.
    *   The `evaluate_model` function is helpful, but the `classification_report` output is commented out.

To receive a higher score, the nested cross-validation should be more thoroughly implemented, covering all specified models or at least a representative set, and the data preprocessing and feature engineering within the nested CV loop should be more robust and avoid data leakage. CatBoost integration could be simpler and more directly leverage its capabilities with the full `X` and `y` if features are handled appropriately.",gemini-2.5-pro-preview,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import pymc as pm
import arviz as az
import statsmodels.api as sm
from statsmodels.sandbox.regression.gmm import IV2SLS
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors
from lifelines import WeibullAFTFitter
import matplotlib.pyplot as plt
import seaborn as sns

# Set random seed for ...",6,"The generated code attempts to address all requirements but suffers from several critical issues, primarily with the PyMC model definitions and the implementation of advanced statistical techniques like Bayesian survival analysis and instrumental variables.

**Correctness (4/10):**
*   **PyMC Version Mismatch:** The code imports `pymc as pm`, suggesting PyMC v4+, but the syntax and functions used are primarily from PyMC3 (e.g., `pm.Data` for observed variables, `pm.sample_posterior_predictive` directly from `pm`). This leads to potential runtime errors or incorrect behavior if run with PyMC v4+. For instance, `pm.Data` is specifically for *mutable* shared variables in PyMC3, not for static observed data in PyMC4 (`pm.Constant` or just direct NumPy arrays/Tensors for observed). `az.from_pymc3` suggests a mix of PyMC3 and ArviZ compatible usage.
*   **Hierarchical Model:** While the intention is there, the setup for `alpha_group_offset` and its combination with `mu_alpha_group` and `sigma_alpha_group` is a common PyMC3 pattern, but might not be the most robust or idiomatic way in PyMC4+ if the user intended that. The coefficient `betas_h` is given `dims=""coeffs""` implying fixed effects across columns, but the way `x_data` is used (`pm.math.dot(x_data_h, betas_h)`) correctly applies them.
*   **Mixed-Effects Model:** The attempt at a random slope for Glucose is logically sound, but the implementation of `betas_fixed_me` as `dims=len(X_cols)-1` and the subsequent manual slicing `x_data_me[:, idx_no_glucose]` is cumbersome and error-prone. A more robust PyMC framework often uses a design matrix or more explicit indexing. The `try-except` block correctly anticipates potential issues with this complex model.
*   **Bayesian Survival Model:** The code attempts a Bayesian Weibull model but explicitly states difficulty and skips the full implementation due to complexity. This is a critical omission for a core task requirement. The `lifelines` model is frequentist.
*   **IV 2SLS:** The use of `statsmodels.sandbox.regression.gmm.IV2SLS` is a valid approach for frequentist 2SLS, but the task explicitly asks for ""causal inference methods"" and the example uses a binary outcome (`Outcome`), making OLS-based 2SLS (which assumes continuous outcomes or a linear probability model) less appropriate than a Bayesian or proper binary IV model (e.g., bivariate probit), which is not attempted. The scaling of inputs for `IV2SLS` is good practice.
*   **Gaussian Process:** The GP model setup (`pm.gp.cov.ExpQuad`, `pm.gp.Marginal`) seems correct for PyMC3/v4 for a simple regression. The scaling of `X_gp` and `y_gp` is appropriate.
*   **Propensity Score Matching:** The PSM implementation is a basic nearest-neighbor matching. While it demonstrates the concept, it lacks robustness features like caliper matching, common support checks, and balance diagnostics, which are crucial for a truly ""complete"" PSM analysis. The `causalinference` library comment hints at more advanced usage but doesn't implement it.
*   **Model Comparison:** The `compare_df` uses `_in_globals()` which is not standard Python and will cause a `NameError`. It should directly check if `trace_logistic` and `trace_hierarchical` are defined/not None.

**Code Quality (7/10):**
*   **Imports:** Mostly correct for the libraries intended, although the `pymc` vs `pymc3` ambiguity is an issue. `IV2SLS` is from `statsmodels.sandbox` which suggests it's not a primary, fully stable part of `statsmodels`. `LabelEncoder` is used appropriately.
*   **Structure:** The code is well-structured with clear headings for each task, making it readable and followable.
*   **Comments:** Comments are helpful, explaining the intent of different sections.
*   **Reproducibility:** Use of `RANDOM_SEED` and `rng` is good.
*   **Clarity:** Variable naming is generally clear.
*   **Error Handling:** The `try-except FileNotFoundError` is good. The `try-except` for mixed-effects and IV models is also good, acknowledging the potential for issues.
*   **Data Preprocessing:** Missing value handling and feature scaling are well-implemented. Age group creation for hierarchical modeling is good.

**Completeness (5/10):**
*   **Hierarchical Bayesian Models (2 & 3):** Implemented, but the mixed-effects random slope model (`#4`) is attempted but noted as difficult.
*   **Bayesian Logistic Regression (4):** Implemented with informative priors.
*   **MCMC Sampling (5):** Implemented for all PyMC models.
*   **Survival Analysis (6):** The `lifelines` part is frequentist. The Bayesian part is acknowledged as complex and skipped, which is a major gap. The simulation of `time_to_event` and `event_observed` is a good workaround for missing data.
*   **Gaussian Processes (7):** Implemented correctly for a simple case.
*   **Causal Inference (8):** Both PSM and IV are attempted. PSM is very basic. IV uses a frequentist 2SLS approach and notes its limitations for binary outcomes. A true Bayesian Causal Inference or more robust frequentist approach (e.g., `DoWhy`, `EconML` or full `causalinference` usage) and acknowledging the binary outcome challenge for IV would have been better.
*   **Bayesian Model Comparison (9):** Implemented with `az.compare`, but the code uses a non-standard `_in_globals()` check.
*   **Credible Intervals & Posterior Predictive Checks (10):** Implemented and plotted for the main PyMC models. Plotting for credible intervals is done via `az.summary` and `az.plot_ppc`, which is good.

**Overall Feedback:**
The generated code demonstrates a good understanding of the overall requirements and attempts to use the specified libraries. However, the critical issues around PyMC version/syntax and the skipping of the Bayesian survival model significantly detract from the ""expert"" level required. The causal inference implementations are conceptual but technically rudimentary compared to what full causal inference pipelines typically involve. Debugging the `_in_globals()` issue and ensuring PyMC v4+ compatibility would be high priorities. A more robust implementation of Bayesian survival, perhaps with `pymc_experimental.distributions.survival` if using PyMC v4+, would be necessary for a higher score.",gemini-2.5-pro-preview,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, QuantileTransformer
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering, OPTICS
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE
import umap
from sklearn.metri...",8,"The generated code provides a comprehensive solution that addresses most of the advanced requirements of the task.

**Strong Points:**
- **Comprehensive Imports:** All necessary libraries are imported.
- **Robust Preprocessing:** Handles missing values logically (replacing 0s with NaN and then imputing median).
- **Advanced Feature Engineering:** Successfully creates metabolic syndrome indicators and discretizes continuous variables into categories, which is crucial for ARM and risk stratification.
- **Multiple Clustering Algorithms:** Implements K-Means, DBSCAN, Hierarchical, GMM, Spectral, and OPTICS.
- **Advanced Dimensionality Reduction:** Correctly applies t-SNE, UMAP, and an Autoencoder for visualization.
- **Optimal Parameter Determination:** Includes methods like Elbow, Silhouette, and even references Gap Statistic for `k` selection, and k-distance graph for DBSCAN `eps` estimation.
- **Consensus Clustering:** Implements a logical approach using a co-association matrix and hierarchical clustering.
- **Detailed Patient Phenotyping:** Provides descriptive statistics and includes statistical significance testing (ANOVA/Kruskal-Wallis, Chi-squared) for cluster differences.
- **Anomaly Detection:** Utilizes Isolation Forest, LOF, and One-Class SVM effectively.
- **Association Rule Mining:** Discretizes data and implements Apriori and association rules, including filtering for ""Diabetes"" relation.
- **Patient Risk Stratification:** Builds a system based on clustering results and provides an insightful clinical interpretation.
- **Code Structure and Readability:** The code is well-structured with clear section headers and comments, making it easy to follow.
- **Reproducibility:** Uses `random_state` and `np.random.seed`, `tf.random.set_seed`.

**Areas for Improvement (to achieve a higher score):**
- **Optimal Parameter Determination Execution:** While the code includes the logic for Elbow, Silhouette, and K-distance plots, `plt.show()` calls are commented out. This means the user would not see these crucial plots to visually determine optimal parameters like `k` for K-Means or `eps` for DBSCAN. While the code attempts to pick an `OPTIMAL_K` based on silhouette, visual inspection is often vital for these methods. For DBSCAN and OPTICS, the `eps` and `min_samples` are still placeholder values instead of being determined programmatically from the k-distance graph's elbow point (which requires a more advanced elbow detection algorithm or manual input).
- **Time-Series Clustering (Conceptual vs. Implementation):** The task explicitly asked for ""time-series clustering if temporal patterns exist in the data"". The generated code correctly identifies that the `diabetes.csv` dataset is cross-sectional and provides a good conceptual explanation. However, a more robust solution might include a placeholder function or a brief example of how `tslearn` (as mentioned) would be invoked *if* the dataset *were* temporal, to fully demonstrate knowledge of temporal clustering even if not directly applicable to this specific dataset.
- **Consensus Clustering `AgglomerativeClustering` Input:** The `AgglomerativeClustering` line for consensus clustering uses `metric='precomputed'` but still passes the `distance_matrix_consensus` directly, which while logical, is often expected to be computed using `linkage` from `scipy.cluster.hierarchy` when dealing with a full pairwise distance matrix, especially if dendrogram plotting is desired. The Ground Truth uses `scipy.cluster.hierarchy.linkage` with a condensed distance matrix, which is more standard.
- **Error Handling/Robustness for Metric Calculations:** For silhouette and other metrics, ensuring `len(np.unique(labels)) > 1` is good, but sometimes algorithms can produce very few or only one cluster which might be `0` or `-1` (noise). More robust checks for valid labels before calculating metrics (e.g., `if len(np.unique(labels[labels != -1])) > 1`) would be beneficial.
- **Plotting Integration:** The code suppresses `plt.show()` calls, which is fine for a non-interactive script, but it means the user running the script won't see the visualizations by default. A toggle or explicit instruction to uncomment them would be helpful.
- **Autoencoder Activation Function:** The autoencoder's output layer uses `sigmoid` activation. While this might be appropriate if the scaled data was normalized to 0-1 range (e.g., with MinMaxScaler), `StandardScaler` outputs data that can be negative or greater than 1. A `linear` activation function is generally more appropriate for reconstructing scaled numerical data that's not restricted to a [0,1] range. The Ground Truth uses `linear`.",gemini-2.5-pro-preview,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocessing & Feature Engineering
from sklearn.model_selection import train_test_split, KFold, cross_val_score, RepeatedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error, r2_score
...",8,"The generated code provides a comprehensive solution to the problem, covering most of the requirements.

**Strong points:**
*   **Comprehensive Preprocessing & Feature Engineering:** Correctly handles numerical and categorical features, imputation, scaling, and includes `PolynomialFeatures` for advanced engineering. The `ColumnTransformer` setup is good.
*   **Multiple Ensemble Methods:** Implements a wide range of ensemble methods including Voting Regressor (hard/soft), Bagging (with DT and SVR), and all specified Boosting algorithms (AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost).
*   **Stacking Ensemble:** Correctly sets up a `StackingRegressor` with multiple base learners and a meta-learner.
*   **Bayesian Optimization:** Uses Optuna for hyperparameter tuning for XGBoost, demonstrating a good approach.
*   **Model Interpretation (SHAP and LIME):** Successfully integrates SHAP for global and individual interpretations (summary plots, dependence plot) and LIME for local interpretations, which is excellent.
*   **Model Calibration and Uncertainty Quantification:** Includes a calibration plot and uses quantile regression with Gradient Boosting for prediction intervals, calculating coverage. This is a strong implementation.
*   **Ensemble Diversity Analysis:** Correctly assesses diversity using the correlation matrix of base learner predictions.
*   **Code Quality:** Good structure using functions, clear comments, and appropriate imports. Reproducibility is ensured with random seeds.

**Areas for Improvement (leading to a score of 8 instead of 9/10):**

*   **Nested Cross-Validation:** While the code includes the logic for nested CV, it explicitly states that the Optuna tuning is pre-done and not performed *inside* each outer fold due to computational intensity. For a truly robust nested CV, the hyperparameter tuning (Optuna) should ideally be performed within each inner loop of each outer fold. The current implementation evaluates an *already tuned* model with nested CV, which is not the standard interpretation of the ""nested CV for robust model evaluation"" requirement aiming to get an unbiased estimate of the *tuned model's* performance from scratch. The ground truth code also simplifies this, but explicitly mentions where the true nested CV would involve Optuna.
*   **Custom Ensemble with Dynamic Weight Assignment:** The implementation creates this ensemble, calculates weights based on validation set MSE, and then applies them to test set predictions. This is a good approach. However, for future predictions with this custom ensemble, one would typically train each base model on the *full* training dataset and then combine their predictions using the optimized weights. The current structure only uses the optimized weights on a held-out test set for evaluation, but doesn't explicitly provide a `predict` function for the final custom ensemble for new data, or explicitly state that the base models for this ensemble must be retrained on the full training set. The ground truth code explicitly creates `final_custom_ensemble_models` and a `predict_custom_ensemble` function.
*   **Multi-level Stacking:** The implementation of multi-level stacking is described as ""illustrative concept"" and ""simplified"". It does not fully demonstrate a robust multi-level stacking pipeline that handles independent training and OOF predictions correctly between levels to avoid data leakage. The ground truth code, although also simplified, still uses `cross_val_predict` to generate OOF predictions for Level 1, which is key for proper multi-level stacking.
*   **LIME Visualization:** While the LIME explanation `.show_in_notebook` or `.as_pyplot_figure()` lines are commented out, enabling them would complete the ""Create... visualization"" aspect of the interpretation requirement. The ground truth code includes `explanation.show_in_notebook` and `plt.show()` for LIME.
*   **Ensemble Diversity (Model Combination Optimization):** The code attempts to optimize weights for a Voting Regressor using Optuna, which is good. However, the ground truth code also demonstrated a direct optimization of weights for a linear combination of predictions (using `scipy.optimize.minimize`), which is a more explicit and common approach for ""model combination optimization"" when you have OOF predictions from various models. The current approach optimizes weights for a specific `VotingRegressor` setup, which is valid but less general than optimizing linear combinations of *any* model's predictions.

Overall, the code is very strong and demonstrates a deep understanding of the requirements. The minor shortcomings are mostly in the areas of full robustness/completeness in specific advanced ensemble techniques and cross-validation, and minor missing explicit function for a custom ensemble.",gemini-2.5-pro-preview,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc as pm
import arviz as az
import semopy
import statsmodels.api as sm
from statsmodels.sandbox.regression.gmm import IV2SLS
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neighbors import NearestNeighbors
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import matplotlib.pyplot as plt
# import seaborn as sns # Arviz uses it, no...",7,"The generated code attempts to address all requirements of the task, demonstrating an understanding of the diverse statistical and causal inference methods requested. It correctly sets up PyMC models, performs SEM (using `semopy`), implements IV and PSM, and includes Bayesian model comparison and robust inference checks.

However, there are several issues that prevent a higher score:
1.  **PyMC3 vs PyMC4 (PyMC):** The ground truth explicitly requested PyMC3 or PyMC4. The generated code uses `pymc` (which is PyMC4), but assumes PyMC3 features for some critical aspects like `pm.set_data_buffer_size` (not needed in PyMC4's `pm.sample` directly for data handling) and the way `pm.Data` is used, which is generally correct for PyMC4 but the way `observed_RVs` are accessed in `robust_inference_analysis` can be tricky with PyMC4's graph structure (it's often better to explicitly name the observed variable in the model or get it from `model.observed_data`).
2.  **Missing Value Handling:** The preprocessing is basic (`median` for numerical, `mode` for categorical) compared to the ""comprehensive missing value handling"" detailed in the ground truth using `ColumnTransformer` and `Pipeline` for different columns with different strategies (median/mode) and subsequent standardization. The generated code's `standardize_data` is only applied to predictors, not the full dataset, which might leave unscaled target/outcome variables or categorical features without proper encoding for some models. The `df_sem` copy is made from `df_raw` which means it's not preprocessed *at all* for numerical stability in SEM.
3.  **SEM Implementation:** The ground truth used an approximation of SEM within PyMC3 by chaining Bayesian regressions, illustrating a direct Bayesian approach. The generated code correctly uses `semopy`, which is a good Python package for pure SEM. However, `semopy` is for frequentist SEM, not Bayesian SEM as might be implied by ""Bayesian models... SEM"" in the task. The instructions did say ""Build structural equation models (SEM)"", so `semopy` is technically fine, but it deviates from the overall Bayesian theme. More critically, the `df_raw` (unscaled) is used for SEM, but SEM often benefits from or requires scaled data, or at least careful specification of variance/covariance structure if unscaled.
4.  **Causal Inference - IV:** The IV implementation using `IV2SLS` from `statsmodels.sandbox.regression.gmm` is very old and should not be used in modern `statsmodels`. `sm.Ols` can be used to manually implement 2SLS, or `sm.IV.iv_w2sls` for a better implementation, but `IV2SLS` is deprecated. Also, the choice of 'Freedom to make life choices' as an IV for GDP is not theoretically justified and could be a weak instrument. The ground truth at least explicitly states its instrument is simulated for demonstration.
5.  **Gaussian Processes:** The generated code uses `sklearn.gaussian_process` for GPR, which is a frequentist approach. The task explicitly asked for ""Gaussian processes for non-parametric modeling *using PyMC3 or PyMC4*"". The ground truth correctly implements it using `pm.gp`. This is a significant deviation.
6.  **PPC and Log-Likelihood Calculation:** The `robust_inference_analysis` function attempts to detect the observed variable for PPC and `az.compare` tries to compute log-likelihood if not present. While this is a good attempt at robustness, it relies on heuristics that might fail for more complex models or non-standard naming. Some models (like mediation or hierarchical) have multiple observed variables or a specific naming convention that needs to be handled carefully for `az.compute_log_likelihood` and `az.plot_ppc`. The log-likelihood computation for `az.compare` is crucial and was marked with a warning/incomplete handling.
7.  **Mediation Analysis:** The mediation model builds two separate likelihoods for social support and happiness. While this is valid, the choice to use `observed=social_support_obs` within the `mu_y` for happiness means that the mediation calculation is effectively using the *observed* mediator, not the *predicted* (or latent) mediator as would be the case in a fully generative SEM. This is a common simplification but technically deviates from a full Bayesian SEM mediation.
8.  **Difference-in-Differences:** The simulation of the DiD problem is good, but the task asked to ""simulate time component"" and ""policy impact assessment"". The implementation focuses directly on the regression result rather than visualizing the pre-post trends that are crucial for validating the parallel trends assumption in DiD.
9.  **Imports:** `seaborn` is commented out, but `az.plot_ppc` and `az.plot_compare` often leverage `seaborn` internally for better aesthetics. Removing it is a minor point but `matplotlib.pyplot as plt` is used consistently.

Overall, the code provides a reasonable scaffold. It attempts to meet almost all requirements but falls short on the depth and correctness of implementation for certain advanced aspects (GP, SEM, IV specific libraries). Debugging the `az.compare` and `az.plot_ppc` would likely be necessary due to issues with how observed likelihoods are identified and handled. The reliance on `pm.Data` for mutable observed variables is good for PyMC4, but the overall use of `pm.sample` often needs more careful tuning for convergence, especially for smaller chain counts (500 tune, 500 draw, 2 chains is very low for some models).",gemini-2.5-pro-preview,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering, MeanShift
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding
from sklearn.decomposition import PCA # Autoencoder substitute for simplicity if TF is too heavy...",7,"The generated code attempts to address almost all requirements listed in the prompt, which is commendable. It includes multiple clustering algorithms, dimensionality reduction, anomaly detection, network analysis, and even a rudimentary recommendation system.

However, there are several areas for improvement affecting its correctness and completeness, preventing a higher score:

1.  **Correctness in Preprocessing/Feature Engineering:**
    *   The prompt asked for ""advanced feature engineering including happiness profiles and regional characteristics."" The generated code creates `Happiness_Profile` (which is a categorical binning of `Ladder score`, useful for ARM but not for direct clustering features) and simply dummies `Regional indicator` but comments out its concatenation to `df_processed`, leaving it unused in `X_scaled`. The ground truth, however, integrates regional characteristics via one-hot encoding *directly into the features for clustering, after scaling*, which is more aligned with ""regional characteristics"" as features affecting clustering alongside other numerical features.
    *   The generated code `dropna`s the input data based on numerical columns. While this might be a safe general practice, the specified dataset (world-happiness-report-2021.csv) usually doesn't have many NaNs in these core features, and the ground truth uses `fillna` with medians/modes for robustness without dropping rows, which is better for maximizing data utilization.
    *   The ground truth shows more advanced feature engineering like creating `GDP_x_SocialSupport`, `Health_Freedom_Ratio`, etc., which the generated code completely omits. This directly impacts the ""advanced feature engineering"" requirement.

2.  **Correctness in Optimal Parameter Determination (Point 4):**
    *   The prompt explicitly asks for ""silhouette analysis, gap statistic, elbow method, and stability analysis.""
    *   The generated code implements `plot_silhouette` and `plot_elbow` and dynamically chooses `optimal_k_kmeans` based on silhouette, or gap statistic if available. This is good.
    *   However, for non-KMeans algorithms, it primarily uses silhouette for optimal K. While acceptable, the prompt implies a broader application of these methods for *all* relevant algorithms, or at least a clearer discussion of why it's applied selectively.
    *   The stability analysis is implemented only for K-Means using ARI, which is a good start, but it's only a partial fulfillment of this broad requirement. The ground truth explicitly states that full implementation for these is beyond the scope, acknowledging the complexity, whereas the generated code attempts a partial demonstration.

3.  **Completeness & Robustness in Clustering:**
    *   **DBSCAN:** The manual selection of `eps_dbscan` after plotting the k-distance graph, while a common practice, is still an arbitrary choice. The ground truth iterates through ranges of `eps` and `min_samples` and finds the best `min_samples` for HDBSCAN or uses silhouette for DBSCAN, making it more programmatic. The generated code also includes `MeanShift` which was *not* requested.
    *   **HDBSCAN:** The HDBSCAN parameter tuning in the generated code is very minimal (fixed three `min_cluster_size` values) compared to more robust approaches. It also uses a threshold for noise (90% noise points) which is arbitrary and might filter out valid HDBSCAN results which often produce a high percentage of noise. The ground truth iterates through a range of `min_cluster_size` values and relies on silhouette.
    *   The error handling for clustering (e.g., `silhouette_score` requiring >1 unique labels) is present but could be more robustly integrated into a comprehensive scoring loop.

4.  **Completeness in Dimensionality Reduction:**
    *   The `LocallyLinearEmbedding` (LLE) from `sklearn.manifold` is included in the imports but not used in the `advanced_dimensionality_reduction` section, missing a potential manifold learning technique.
    *   The `n_neighbors` parameter for UMAP is set to `min(15, X_scaled.shape[0]-1)`, which is okay for this dataset, but the ground truth uses a similar, slightly more conservative choice, `min(X.shape[0]-1, 30)` for t-SNE for example.

5.  **Small structural/minor issues:**
    *   The generated code uses `main()` function which is good, but the ground truth uses a class structure, which is arguably better for organizing complex machine learning pipelines with state management.
    *   The plotting functions might have some issues (e.g., `color_map_func` for `viridis` with `len(unique_clusters_in_graph)-1` in denominator might cause division by zero if only one unique cluster. Though unlikely for good clustering, robust code handles `len > 1` directly).
    *   In network analysis, `astype(int)` for adj_matrix will convert `True` to 1 and `False` to 0, which is correct, but directly using boolean array for graph creation `nx.from_numpy_array(similarity_matrix > threshold)` or `nx.from_numpy_array(affinity_matrix)` is more conventional.

6.  **Insights and Recommendation System:**
    *   The policy insights are somewhat generic and the recommendation system is basic (""countries in the same cluster""). While it touches on the idea, it lacks the depth of ""policy insights based on clustering results with geopolitical validation"" implied by the prompt. The ground truth provides more detailed qualitative analysis and specifically mentions ""geopolitical validation"" as a crucial step for domain experts. The ground truth's recommendation system also attempts to find similar countries *within* the cluster.

Overall, the generated code provides a strong foundation and implements many of the requirements, but some parts are less robust, less advanced, or incomplete compared to a truly expert-level solution.",gemini-2.5-pro-preview,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
import pmdarima as pm
from sklearn.metrics import mean_squared_error
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings(""ignore"")

def perform_time_series_forecasting():
    # 1) Load data, parse 'date', filter for 'Germany', and set 'date' as index.
    try:
        df = pd.read_csv('synthetic_covid19_data.csv')
    except FileNotFo...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong Points:**
- **Correct Implementation:** It correctly implements SARIMA with exogenous variables, handles data loading, filtering, and indexing as required.
- **Missing Value Handling:** The missing value handling is comprehensive, including `ffill`, `bfill`, and specific handling for `stringency_index` and `people_fully_vaccinated`. It also includes type conversion with `errors='coerce'`, which is good practice.
- **Hyperparameter Tuning:** It correctly uses `pm.auto_arima` to find the optimal SARIMA orders, including specifying `m=7` for weekly seasonality.
- **Training and Forecasting:** The model training and forecasting steps are implemented correctly, passing `exog` to both `auto_arima` and `SARIMAX`.
- **Evaluation:** RMSE calculation is correct.
- **Plotting:** The plotting is well-done, showing actual vs. forecasted values and including historical training data for context.
- **Robustness:** Includes good error handling for file not found and insufficient data for splitting. Fallback mechanisms for `auto_arima` and `SARIMAX` fitting are a nice addition for more robust code, especially for potentially problematic datasets.
- **Code Quality:** The code is well-structured within a `perform_time_series_forecasting` function, has clear variable names, and includes appropriate comments. Warnings are suppressed, resulting in cleaner output.

**Areas for Minor Improvement (to reach a perfect 10):**
- **Negative Forecasts:** The ground truth includes a line `forecast_values[forecast_values < 0] = 0` to handle illogical negative forecasts for 'new_cases'. The generated code misses this specific quality check. While not explicitly asked for, it's a good practice for count data like 'new_cases'.
- **Consistency in Exogenous Variable Naming:** The generated code uses `exog` in the `df_germany` context and `exog_cols` for column names. While functional, maintaining consistency (e.g., sticking to `exog_variables` as in the prompt and ground truth) often improves readability.
- **Minor Redundancy in NaN Handling:** The `df_germany['people_fully_vaccinated'] = df_germany['people_fully_vaccinated'].fillna(method='ffill').fillna(0)` approach is a bit redundant if `fillna(0)` is intended as a final catch-all. `ffill` then `bfill` (as done for stringency_index) followed by a final `fillna(0)` (if any NaNs still exist) is typically more robust for completely empty beginning or end segments. However, the current approach is functional.

Overall, this is an excellent solution that demonstrates a strong understanding of time series forecasting principles and robust Python programming.",gemini-2.5-pro-preview,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import scikit_posthocs as sp

def perform_comparative_analysis(csv_filepath='winequality-red.csv'):
    # 1. Load Data and Define Quality Groups
    try:
        df = pd.read_csv(csv_filepath)
    except FileNotFoundError:
        print(f""Error: The file {csv_filepath} was not found in the current directory."")
        return

    # ...",9,"The generated code is very good, correctly implementing almost all requirements.

**Strong Points:**
- **Correct Quality Grouping:** Accurately defines low, medium, and high quality based on the given score ranges.
- **Appropriate Statistical Test Selection:** Correctly applies Shapiro-Wilk for normality and Levene's for variance homogeneity to then decide between ANOVA and Kruskal-Wallis.
- **Correct Post-hoc Tests:** Uses Tukey's HSD after ANOVA and correctly attempts to use a suitable non-parametric post-hoc (Dunn's test via `scikit-posthocs`) after Kruskal-Wallis. This is a common and appropriate choice.
- **Meaningful Visualizations:** Generates box plots for significantly differing features using Seaborn, which is good practice.
- **Clear Summarization:** Provides a clear summary of features that showed significant differences.
- **Robustness:** Includes `try-except` for file loading and handles cases of insufficient data for statistical tests. Uses `dropna()` where appropriate.
- **Code Structure:** Encapsulates the logic in a function `perform_comparative_analysis` and uses a `if __name__ == '__main__':` block, which is good practice.

**Areas for Improvement (Minor):**
1.  **Dunn's Test Dependency (`scikit_posthocs`):** While `scikit_posthocs` is excellent for Dunn's test, it's an external library not explicitly listed in the allowed imports (`pandas, scipy.stats, matplotlib, seaborn, statsmodels`). The ground truth opted for pairwise Mann-Whitney U with Bonferroni correction as a standard way to approximate Dunn's without this extra dependency, which adheres more strictly to the listed imports. If `scikit_posthocs` is an allowed implicit dependency, then this is not an issue. If not, this deviates slightly.
2.  **`pairwise_tukeyhsd` Data Handling:** The `pairwise_tukeyhsd` call in the generated code uses `df[feature].dropna()` and `df['quality_group'].dropna()`. This is generally okay if there are no NaNs in `quality_group`, but for combined data, it's safer to ensure that the `endog` and `groups` arguments align perfectly in length and order. The ground truth explicitly concatenates data and labels based on the filtered groups, which guarantees this alignment. While it might not cause an error here, it's a subtle point of robustness.
3.  **Levene's Test on Sparse Data:** The condition `if len(valid_groups_data_for_levene) < 2:` for Levene's test is good, but `g.nunique() > 1` (to exclude groups with zero variance) might be slightly overzealous. `scipy.stats.levene` can handle groups with zero variance, though it might issue a warning or return `nan` p-values. The main concern for Levene's is typically `len(group_data) < 2`. The current logic, however, leads to `variances_homogeneous` being `False` if not enough groups have variance, which correctly pushes towards Kruskal-Wallis, so it's a safe choice.
4.  **Minor Output Formatting:** The summary for Dunn's test in the generated code explicitly prints ""Significant pairwise differences (p < 0.05 from Dunn's test):"" then iterates. The ground truth integrates this more directly into the output of the `pairwise_tukeyhsd` object itself or as part of a single summary string, which can be slightly cleaner for automated processing, but this is a very minor stylistic point.

Overall, the solution demonstrates a strong understanding of statistical analysis workflows for comparative studies. The use of `scikit_posthocs` is a valid, arguably superior, approach for Dunn's test, which might be an implicit understanding of ""appropriately perform statistical tests"" even if the library wasn't explicitly listed.",gemini-2.5-pro-preview,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, f1_score
import xgboost as xgb
import shap # Ensure SHAP is installed: pip install shap

def load_and_prepare_data(file_path):
    """"""Loads data, parses 'date', and sorts.""""""
 ...",7,"The generated code provides a good attempt at the problem but deviates significantly from the ground truth and has some critical issues, particularly around feature engineering and data handling which impact correctness and completeness.

**Correctness Issues:**

1.  **Target Variable Definition:**
    *   The prompt specifies: ""A 'high_daily_death_increase' is defined as a day where 'new_deaths' for a 'location' is in the top 10% of its own historical 'new_deaths' (excluding days with zero new deaths).""
    *   The generated code uses `df_copy['new_deaths_positive_for_percentile'] = df_copy['new_deaths'].where(df_copy['new_deaths'] > 0)` and then `df_copy.groupby('location')['new_deaths_positive_for_percentile'].quantile(0.9)`. While the calculation for the percentile is mostly correct, the `where` operation directly turns `0` values into `NaN`, which is effectively excluding them.
    *   The target definition `(df_copy['new_deaths'] > df_copy['death_90th_percentile_loc']) & (df_copy['new_deaths'] > 0)` correctly captures the `new_deaths > 0` condition, but the Ground Truth's approach of setting `new_deaths_for_percentile` to NaN for `new_deaths <= 0` and then mapping directly onto the target condition is slightly cleaner and less prone to misinterpretation in the logic chains. The generated code also tries to handle `NaN` targets by `dropna`, which is less robust than the ground truth.

2.  **Feature Engineering (Rolling/Lag Features):**
    *   **Rolling Mean/Std:** The generated code applies `shift(1)` *before* the rolling window. This is incorrect. The rolling mean/std should be calculated on the *original* sequence up to the current day (or the previous day for a truly predictive feature), and then the result can be shifted. Applying `shift(1)` first means the rolling window only sees data from `t-2` onwards (`t-1` becomes `NaN`), which might hide the actual mean/std at `t-1`. The `min_periods=1` also indicates that a simpler `groupby(...).rolling(...).mean()` followed by `shift(1)` would be the correct approach for causal features. The Ground Truth correctly uses `transform(lambda x: x.rolling(window=7, min_periods=1).mean())` at the specific index, and for prediction, it applies the shift implicitly by taking *all* past data into effect during the rolling. A true causal model would `shift` the *results* of the rolling window to `t-1`. The generated code does `df_eng.groupby('location')[col].shift(1)` then `shifted_col.groupby(df_eng['location']).rolling(...)`. This is conceptually flawed for what a ""rolling 7-day mean"" typically means in time series for *prediction* at `t`. The Ground Truth's approach `x.rolling(window=7, min_periods=1).mean()` gives the *current* 7-day mean, which should then be `shift(1)` if strictly only prior knowledge is allowed for prediction at time `t`. The Ground Truth's features are current rolling means. So, `new_cases_rolling_7d_mean` for `date` `D` uses `new_cases` up to `D`. If the intent was for predicting `D` using data up to `D-1`, this would need a `.shift(1)` *after* the rolling calculation. The generated code's implementation of `shifted_col` and then `rolling` is not standard for this type of feature and unnecessarily complex/potentially incorrect.

3.  **Feature Engineering (`days_since_first_case`):**
    *   The generated code attempts to map `first_case_date_map = df_eng[df_eng['new_cases'] > 0].groupby('location')['date'].min()`, which correctly finds the *first date with a positive case*. However, it then does `(df_eng['date'] - df_eng['first_case_date_loc']).dt.days`. This calculation would result in `NaN` for locations that never had a `new_cases > 0`, which then needs `fillna(0)`.
    *   The Ground Truth's `(x - x.min()).dt.days` is simpler and more robust, calculating days since the *first recorded date* for that location (not necessarily the first case date), which is often a more stable and relevant feature for overall time progression. If ""first case date"" was strictly required, the Ground Truth would apply the `shift()` or `transform()` on the first `new_cases > 0` date. Given the prompt just says ""days since first case"", the ground truth's interpretation is reasonable and simpler. The generated code's logic is more directly aligned with ""first *case*"" but complicates handling if no cases ever happen.

4.  **Missing Value Handling:**
    *   The generated code identifies some `cols_to_fill_zero` but does a very piecemeal fillna(0) for engineered features and original features.
    *   The Ground Truth has a more comprehensive and robust strategy: `ffill()` within each group for engineered features, followed by `fillna(0)`. This is a better practice for time series as `ffill` propagates known past values, which is causally sound, whereas `fillna(0)` can lose information if original `NaN`s in the original dataset (e.g., `hosp_patients`) were meant to be filled differently. The generated code's `fillna(0)` is simple but simplistic and potentially incorrect.
    *   The handling of vaccination ratio for `Inf` values is better in the generated code (`replace([np.inf, -np.inf], np.nan)`) before `fillna(0)` than in the Ground Truth which directly replaces `inf` with `0`. Replacing with `NaN` first and then `fillna(0)` is technically more correct from a `NaN` handling perspective.

5.  **Data Split:**
    *   The generated code implements time-based splitting manually using `iloc` on a sorted DataFrame. This is correct for time-based splitting.
    *   However, `TimeSeriesSplit` was suggested for *hyperparameter tuning* (cross-validation within the training set), not for the initial train-test split. The generated code considers using `TimeSeriesSplit` for hyperparameter tuning but defaults to standard `GridSearchCV` (`cv=3`), which is acceptable but misses the nuance of the request for time-series consideration throughout. The Ground Truth correctly just uses standard `cv=3` for GridSearchCV which is fine for the prompt, but it does `sort_values('date')` before the split to ensure temporal order, which is crucial.

6.  **SHAP Feature Names:**
    *   The generated code's way of getting `feature_names_out` from the `ColumnTransformer` is problematic. `preprocessor.get_feature_names_out()` is the correct modern `sklearn` way. The `try-except` block for older scikit-learn is a bit clunky, and the manual construction (`num_feature_names + cat_feature_names`) might not correctly handle feature order if the ColumnTransformer reorders things internally. The Ground Truth explicitly gets names from `numerical_features` and `ohe_feature_names` from the fitted OHE, which is more robust.

**Code Quality and Completeness:**

1.  **Structure:** The generated code uses functions (`load_and_prepare_data`, `create_target_variable`, `engineer_features`) which is good for modularity. However, the `main()` function encapsulates the entire logic pipeline, which is a neat approach. The Ground Truth has a linear script structure which is simpler for a single-task script but less modular.
2.  **Imports:** All necessary imports are present.
3.  **Completeness:**
    *   **Hyperparameter Tuning:** It performs GridSearchCV with `n_estimators`, `max_depth`, `learning_rate` as required.
    *   **Evaluation:** ROC-AUC and F1-score are calculated.
    *   **SHAP:** SHAP is used, and it tries to extract feature names, and plots both bar and beeswarm plots (the Ground Truth only asks for one, but two is not bad).
    *   **Missing from Generated:**
        *   The explicit handling of `eval_metric='logloss'` for XGBoost is present in both, which is good.
        *   `scale_pos_weight` for class imbalance is a good addition in the generated code, not explicitly requested but good practice.
        *   The generated code handles `use_label_encoder=False` for XGBoost which is for deprecation warning, important.
        *   The Ground Truth code includes `print` statements that explain step-by-step progress, which is very helpful for debugging and understanding flow. The generated code lacks this level of detailed output.
        *   The generated code's `dropna` on the target variable at the end of `create_target_variable` might discard data points that could have been potentially useful for feature engineering if the target could not be determined. The Ground Truth implicitly handles this by setting the target to 0 if the percentile condition isn't met, even if the percentile itself was NaN for that location (e.g., all 0 deaths), which is more robust as it doesn't drop rows only because of the target definition.
        *   The generated code's `X.dropna()` just before the split is dangerous as it drops rows that might have been fine or had NaNs that `fillna(0)` should have caught. This suggests remaining NaNs after the prior `fillna(0)` calls, indicating the fillna strategy might not be fully effective, or that other columns (not explicitly in selected features) still have NaNs. This indicates a potential flaw in the overall robustness of NaN handling.

In summary, while the generated code covers most high-level requirements (target, features, model, tuning, evaluation, SHAP), its implementation details for feature engineering and missing data handling are less robust and correct compared to the ground truth, leading to an overall lower score. The conceptual errors in rolling features and the aggressive `dropna` reduce its reliability.",gemini-2.5-pro-preview,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.formula.api as smf

def run_covid_modeling_analysis():
    """"""
    Performs advanced statistical modeling to assess the impact of 
    'stringency_index' and 'people_fully_vaccinated' percentage on 
    'reproduction_rate', considering 'location' and 'continent' effects.
    """"""

    # 1) Load data, parse 'date'. Calculate 'vaccination_percentage'.
    try:
        df = pd.read_csv('synthetic_covid19_data.csv')
    except FileNo...",6,"The generated code attempts to solve the problem and includes significant scaffolding for error handling and data imputation, which is commendable. However, it deviates significantly from the task's specified mixed-effects model structure, leading to an incorrect implementation of the random effects.

Here's a detailed breakdown:

**Correctness (3/10):**
*   **Major Flaw in Mixed-Effects Model Specification:** The task explicitly asks for ""random intercepts for 'location' nested within 'continent'"". The generated code attempts `vc_formula={""continent_re"": ""0 + C(continent)""}` with `groups=df_clean[""location""]`. This is *not* a correct way to specify `location` nested within `continent`. `vc_formula` is for variance components for *group-specific slopes* or *additional random effects structures on the main grouping variable*, not for nesting a grouping variable *within* another. The correct way to implement ""location nested within continent"" random intercepts in `statsmodels` `mixedlm` is to create a combined `(continent, location)` ID and use that as the `groups` variable, as seen in the ground truth.
*   **Imputation Strategy Issues:** While the generated code includes a good variety of imputation steps (grouped interpolation, ffill/bfill, grouped median, global median), the order and some specifics might lead to less optimal results or unnecessary steps compared to the ground truth's more streamlined and common approach (grouped linear interpolation followed by global median for remaining edge cases). For example, applying `ffill().bfill()` after `interpolate` within groups for the *same* column is redundant if `limit_direction='both'` is used, and then applying grouped median after that might overwrite valid interpolated values if median is different.
*   **Handling of `vaccination_percentage` Calculation:** The `vaccination_percentage` calculation includes `df.replace([np.inf, -np.inf], np.nan, inplace=True)`, which is a good safety measure, but the overall chaining of operations to calculate it could be more robust. The ground truth handles this more clearly by first ensuring population is not zero or NaN.
*   **Dummy Data Generation:** The dummy data generation for `FileNotFoundError` is a nice touch for self-contained testing, but it's not part of the core task requirements and adds complexity.

**Code Quality (7/10):**
*   **Structure:** Encapsulating the logic in a function `run_covid_modeling_analysis()` is good practice.
*   **Readability:** The code is well-commented and attempts to explain its steps.
*   **Error Handling:** Excellent `try-except` blocks for file loading and model fitting, with very informative error messages and suggestions for debugging. This is a significant strength.
*   **Imports:** Proper imports are present.
*   **Warnings:** It does not suppress warnings, which can be verbose for `statsmodels` convergence issues. The ground truth explicitly handles this.

**Completeness (6/10):**
*   **Missing Value Handling:** It largely covers the requirement, though the method used differs and has some minor issues as noted above.
*   **Model Implementation:** Fails to correctly implement the specified mixed-effects model's random effects structure. This is a critical omission. It tries a variance components formula with `C(continent)` which signifies a different model (random slopes for continent within location, or random effects related to continent at some level) than nested random intercepts.
*   **Reporting:** It reports fixed effects coefficients, standard errors, and p-values, and provides interpretations.
*   **Random Effects Variance:** It discusses the variance components based on its incorrect model specification (`Group Var` for location and `continent_re Var` as a specific variance component formula) rather than the variance for the requested nested random intercepts. This part is tied to the incorrect model.
*   **Assumptions Discussion:** The discussion of model assumptions is comprehensive and accurate.

**Overall Feedback:**
The generated code demonstrates a strong understanding of general Python programming, data handling (including robust error checks and imputation attempts), and model interpretation. However, the core requirement of correctly implementing the specified mixed-effects random effects structure ('location' nested within 'continent') is fundamentally misunderstood. The chosen `vc_formula` approach is incorrect for this specific nesting. This flaw renders the statistical model incorrect for the task, significantly impacting the correctness score. If the random effects were correctly implemented, this solution would be much stronger. The imputation logic, while extensive, could also be simplified.",gemini-2.5-pro-preview,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# 1) Load the dataset
try:
    df = pd.read_csv('clash_royale_cards.csv')
except FileNotFoundError:
    print(""Error: 'clash_royale_cards.csv' not found. Make sure the file is in the current directory."")
    exit()

# 2) Select the features for clustering
features = ['e...",9,"The generated code is very good and largely fulfills all requirements of the task.

**Strong Points:**
- **Correctness:** Successfully loads data, selects and standardizes features, implements methods for choosing K (both Elbow and Silhouette, which is excellent), applies K-Means, performs PCA, and visualizes the results.
- **Completeness:** Addresses all specified requirements: K-Means, specific features, standardization, K determination (with two methods), PCA, and visualization with cluster coloring.
- **Robustness:** Includes good error handling for `FileNotFoundError` and missing columns. It also includes basic checks for non-numeric data and NaN values, which is a good practice, although the task prompt didn't explicitly require it.
- **Clarity for K Selection:** The code explicitly plots both WCSS (Elbow Method) and Silhouette scores, giving the user more robust information to choose K. It also clearly marks the `chosen_k` as an example and advises the user to inspect the plots.
- **Plotting:** Generates two plots for determining K, and the final PCA plot is well-labeled with a title and a legend for clusters.

**Areas for Improvement (minor points):**
- **Consistency in `n_init`:** The ground truth sets `n_init=10` explicitly for reproducibility and to handle the future default change. The generated code uses `n_init='auto'`, which is generally fine but might be less explicit for demonstration purposes if specific reproducibility is paramount in all KMeans runs. However, 'auto' is the recommended parameter for newer scikit-learn versions to achieve the behavior of `n_init=10` in older versions or optimal number of initializations, so this is very minor.
- **PCA Explained Variance:** The ground truth code prints the explained variance ratios in the axis labels which is very helpful for interpreting the PCA plot. The generated code prints them separately at the end. While both are correct, embedding them in the plot labels directly enhances the visualization.
- **`random_state` for PCA:** The ground truth did not set a `random_state` for PCA, which is fine as PCA is deterministic. The generated code sets `random_state=42` for PCA, which is unnecessary and doesn't affect the output for PCA itself (only for randomized SVD, which is not the default for `n_components` less than `n_features`). It's not incorrect, just slightly redundant.

Overall, an excellent solution that not only meets but slightly exceeds the requirements by providing both K-determination methods.",gemini-2.5-pro-preview,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# 1. Load the dataset
# Assume the CSV file is in the current directory.
try:
    df = pd.read_csv('clash_royale_cards.csv')
except FileNotFoundError:
    print(""Error: 'clash_...",9,"The generated code is highly correct and demonstrates a sophisticated understanding of scikit-learn's preprocessing and modeling pipelines. It correctly loads the data, identifies features and target, and implements a robust preprocessing step for the `rarity` column using `ColumnTransformer` and `OneHotEncoder` within a `Pipeline`. This is a very good practice for data science workflows. The data splitting, model training (RandomForestRegressor), and evaluation (R-squared and MSE) are all correctly implemented.

The use of `ColumnTransformer` and `Pipeline` is a strong point, as it ensures that preprocessing steps learned on the training data are consistently applied to new data (test set) without data leakage.

One minor point of improvement could be explicitly handling missing values if they were present in `elixirCost` or `Usage`, similar to what the ground truth did (though simply dropping rows for demonstration). While the prompt didn't explicitly ask for general missing value handling, the ground truth included it as a robust measure. However, given `rarity` is a categorical feature for which `OneHotEncoder` can handle unseen categories (with `handle_unknown='ignore'`), and the prompt focused on `rarity` preprocessing, this is a very minor detail.

Overall, excellent code that goes beyond the basic requirements in terms of best practices.",gemini-2.5-pro-preview,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
try:
    df = pd.read_csv('clash_royale_cards.csv')
except FileNotFoundError:
    print(""Error: 'clash_royale_cards.csv' not found. Please ensure the file is in the current directory."")
    exit()

# Verify that the required columns exist
if 'rarity' not in df.columns or 'Win Rate' not in df.columns:
    missing_cols = []
    if 'rarity' not in df.columns:
        missing_cols.append('rarity')
    if 'Win Rate' not in d...",8,"The generated code successfully performs the required ANOVA test and addresses most of the task requirements.

**Strong Points:**
*   **Correctness:** Loads data, identifies unique rarity categories, performs ANOVA using `scipy.stats.f_oneway`, states hypotheses, prints F-statistic and p-value, and correctly interprets the result based on `alpha = 0.05`.
*   **Error Handling:** Includes good error handling for `FileNotFoundError` and missing columns.
*   **Data Cleaning (Win Rate):** Attempts a more robust conversion for the 'Win Rate' column, handling percentage strings (`'50.5%'`) before converting to numeric. This is an improvement over the ground truth.
*   **Completeness:** All explicit requirements from the prompt are met. Hypotheses are stated clearly.
*   **Code Quality:** Imports are correct, variable names are clear, and comments explain the steps.

**Areas for Improvement (Why not a 9 or 10):**

1.  **Handling of `Win Rate` NaNs/Non-numeric after conversion:** The code converts `Win Rate` to numeric, but if values *after* the conversion (or even the percentage conversion) still result in `NaN`s, these `NaN`s are only handled *within* the `groups_for_anova` loop by `.dropna()`. The ground truth explicitly uses `df.dropna(subset=['Win Rate', 'rarity'], inplace=True)` early on, which is a more robust way to ensure that only complete and valid cases are considered for the test and potential subsequent analysis. The generated code relies on `dropna()` per group *after* the initial conversion, which might miss issues if an entire `rarity` category becomes effectively empty or only has one value due to a high number of NaNs for that category. It also doesn't handle NaNs in `rarity` if any were present.
2.  **Edge Case: Groups with insufficient data:** While the generated code correctly checks `if not win_rates_for_category.empty:`, ANOVA requires at least two data points per group for its assumptions to hold, even if `f_oneway` might technically run with one. The ground truth explicitly warns and filters out groups with less than 2 observations, leading to a more statistically sound ANOVA. The generated code would include groups with a single observation, potentially leading to warnings later or unexpected behavior, or a less reliable result in extreme cases (though `f_oneway` will handle single observations by assigning 0 variance to them).
3.  **No check for minimum number of groups:** The ground truth checks `if len(rarity_categories) < 2:` at the beginning and `if len(win_rates_by_rarity) < 2:` before performing ANOVA. The generated code only checks `if len(groups_for_anova) < 2:` right before performing ANOVA. While this covers the final condition, adding the earlier check for `unique_rarities` could provide earlier feedback if the dataset inherently lacks sufficient categories for the test.

Overall, it's a very good solution that mostly matches the ground truth's robustness, and even adds a notable improvement in `Win Rate` conversion. The minor points listed are about small improvements in robustness for edge cases and statistical rigor.",gemini-2.5-pro-preview,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# 1) Load the dataset
# Assume iris.csv is in the current directory
try:
    df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: 'ir...",9,"The generated code is very good and correctly addresses almost all requirements of the task.

**Strong points:**
- **Correct Imports:** All necessary libraries are imported.
- **Data Loading and Preprocessing:** Data is loaded correctly, features and target are identified, and the target variable is appropriately encoded using `LabelEncoder`.
- **Data Splitting:** Data is split into training and testing sets with the correct proportion (80/20) and `random_state`.
- **Model Training:** All three specified models (Logistic Regression, SVC with linear kernel, Random Forest) are correctly initialized and trained. The `n_estimators` and `random_state` for RandomForest, and `kernel` and `random_state` for SVC are correctly set.
- **Performance Evaluation:** Accuracy, precision (macro), recall (macro), and F1-score (macro) are calculated and printed for each model as required. The use of `zero_division=0` in metric calculations is a good practice for robustness.
- **Best Model Identification:** The code correctly identifies and prints the best model based on the highest macro-averaged F1-score.
- **Code Structure and Readability:** The code is well-structured with comments indicating different sections of the task, making it easy to understand.

**Areas for minor improvement:**
- **`max_iter` for Logistic Regression:** While not explicitly required in the prompt, Logistic Regression often benefits from increasing `max_iter` to ensure convergence, especially with default solvers. The ground truth code included `max_iter=200`, which is a common practice to avoid convergence warnings. The generated code uses default, which might (rarely) lead to a `ConvergenceWarning`.
- **`stratify` in `train_test_split`:** The ground truth code used `stratify=y_encoded` which is generally recommended for classification tasks, especially with imbalanced classes, to ensure class distribution uniformity across splits. While not explicitly asked for, it's a best practice that would make the split more robust. The generated code did not include this, which is a minor oversight, but doesn't necessarily lead to incorrect results for a balanced dataset like Iris.
- **Handling 'Id' column:** The generated code includes an `if 'Id' in df.columns: df = df.drop('Id', axis=1)` block. While this makes the code more robust to various `iris.csv` versions, the prompt specifically mentioned the columns to use for `X` and `y`, so an 'Id' column would automatically be ignored by the feature selection anyway. It's not incorrect, but slightly redundant given the problem statement.",gemini-2.5-pro-preview,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load the dataset
try:
    df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: 'iris.csv' not found in the current directory."")
    exit()

# 2) Select the features
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
X = df[features]
true_species = d...",8,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, selects and standardizes features, applies K-Means and PCA as specified, and generates the two required scatter plots color-coded correctly. The use of `n_init='auto'` and `random_state=42` for K-Means is correct.

**Strong points:**
*   Correct implementation of all core data science steps (loading, feature selection, scaling, K-Means, PCA).
*   Correct plotting of PCA components colored by K-Means labels and true species labels.
*   Proper use of `LabelEncoder` for true species labels.
*   Good error handling for file not found.
*   Effective use of subplots to display both plots side-by-side, which is a good presentation choice, even if not explicitly requested.

**Areas for improvement to reach a perfect score:**
*   **Missing Colorbars:** The ground truth code includes colorbars for both plots, which are standard for continuous color mappings and help in interpreting the colors used. The generated code uses `cmap='viridis'` but lacks colorbars, making the mapping less explicit, especially since the K-Means labels are integers and the species labels are mapped to integers. The `legend_elements` approach can work for discrete categories, but a colorbar provides a direct visual scale. For discrete categories as seen here, explicitly using `cbar` for the K-Means plot (or converting it to a legend on the plot) and `cbar` with custom tick labels for the species plot (as in GT) would be better.
*   **Commented-out `legend_elements` for K-Means:** While the intention was to add a legend, it was commented out for the K-Means plot. It would have been better to either implement it fully or omit the commented code.
*   **Slightly less informative titles for plots:** While correct, the ground truth's titles are slightly more descriptive (""Points Colored by K-Means Cluster Labels"" vs. ""with K-Means Clusters""). This is minor but contributes to clarity.

Overall, it's a very solid attempt with only minor omissions for a perfect rating.",gemini-2.5-pro-preview,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# 1) Load the dataset
# Assume iris.csv is in the current directory
df = pd.read_csv('iris.csv')

# It's good practice to remove an 'Id' column if it exists from typical Iris datasets,
# though the problem description implies features are only the four measurements and Species.
# We'll check and ...",9,"The generated code correctly addresses all requirements of the task. It loads the data, creates the specified features, defines X and y correctly, encodes the target, trains a Gradient Boosting Classifier, performs 5-fold cross-validation, and reports the mean accuracy. The use of `StratifiedKFold` for `cv` in `cross_val_score` is a good practice for classification tasks to maintain class distribution, although the problem statement only specified ""5-fold cross-validation"" without explicitly requiring stratification.

One minor point that could be considered for a perfect score is the `try-except` block for `FileNotFoundError` in the ground truth. While not strictly required by the prompt, it makes the code more robust. Also, the `Id` column dropping logic is a bit extra, as the prompt implies the standard Iris features. However, these are very minor deviations from a perfect solution and do not detract from its core correctness or completeness per the specified requirements. The code is well-structured and uses proper imports.",gemini-2.5-pro-preview,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# 1) Load the dataset
# Assuming 'heart.csv' is in the current directory
df = pd.read_csv('heart.csv')

# 2) Define features X and target y
# The columns t...",8,"The generated code correctly addresses most of the task requirements. It loads the dataset, correctly identifies features and target, performs one-hot encoding on categorical features and scaling on numerical features, splits the data, trains a RandomForestClassifier, and evaluates it using the specified metrics.

The main point of improvement for the generated code is the handling of preprocessing. While it correctly identifies the features and applies the `ColumnTransformer`, it applies `fit_transform` to the entire `X` dataset *before* splitting it into training and testing sets. This is a common mistake that leads to data leakage because the scaler and one-hot encoder learn parameters (like mean/std and categories) from the entire dataset, including the test set. The ground truth code correctly avoids this by implementing a `Pipeline` which ensures that `fit_transform` is only called on the training data (`X_train`) and `transform` on the test data (`X_test`).

Specific points:
*   **Correctness (8/10):** The model training and evaluation logic is mostly correct. The only significant issue is the data leakage mentioned above.
*   **Code Quality (9/10):** Imports are proper, code structure is clear, and variable names are descriptive. The use of `ColumnTransformer` is appropriate. The added `zero_division=0` in metrics is a good touch for robustness, though not strictly required by the prompt, it doesn't hurt.
*   **Completeness (9/10):** All specified steps are attempted. The missing part is the proper implementation of the preprocessing workflow to prevent data leakage, which would ideally involve a `Pipeline`. The prompt's wording ""Split the preprocessed data"" is ambiguous and can lead to this interpretation, but best practice dictates preprocessing only on training data.",gemini-2.5-pro-preview,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# 1. Load the dataset
df = pd.read_csv('heart.csv')

# 2. Select numerical features
features_to_cluster = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']
X = df[features_to_cluster]

# 3. Standardize selected features
scaler = StandardScaler()
X_scaled = scaler....",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong Points:**
- **Correctness:** The code correctly loads the dataset, selects specified features, standardizes them, implements the elbow and silhouette methods to choose K, applies K-Means with the specified parameters, performs PCA, and visualizes the results.
- **Completeness:** All steps outlined in the task description are covered. The plots for WCSS and Silhouette scores are generated, and a PCA visualization of clusters is provided.
- **Code Quality:** Imports are proper, variable names are clear, and the code is structured logically following the task's steps. The use of `n_init='auto'` and `random_state=42` is correct.
- **Visualization:** The plots are well-labeled and provide the necessary information. The PCA plot includes a legend for the clusters.

**Areas for Minor Improvement (Why not a 10):**
- **File not found handling:** The ground truth code includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is robust practice. The generated code assumes the file exists in the current directory and would crash if it didn't, which is a minor robustness issue compared to the ground truth.
- **`numpy` import:** While not strictly necessary for this specific code, the ground truth includes `import numpy as np`. Sometimes PCA or other sklearn-related operations may implicitly benefit from `numpy` being imported, or it might be used for small array operations. It's a minimal omission.
- **PCA `random_state`:** While not incorrect, `PCA` itself is a deterministic algorithm for `fit_transform` given the same input. `random_state` is typically only relevant for algorithms with a stochastic component (like KMeans initialization). Omitting it for PCA is fine, but including it doesn't harm. The Ground Truth didn't include `random_state` for PCA.

Overall, this is an excellent solution that successfully fulfills the task requirements. The minor points are mostly about defensive programming and slight best practices, not core functionality failures.",gemini-2.5-pro-preview,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report

# Load the dataset
try:
    df = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: 'winequality-red.csv' not found in the current directory."")
    exit()

# 1. Create binary target variable
#...",9,"The generated code correctly addresses almost all aspects of the task.

**Strong points:**
- **Correct binary target creation:** The lambda function `apply(lambda x: 1 if x >= 7 else 0)` achieves the same result as `.astype(int)` for binary conversion from boolean.
- **Proper data splitting:** `train_test_split` with `stratify=y` and `random_state` is used correctly.
- **Feature scaling:** `StandardScaler` is applied properly to training and test sets.
- **Logistic Regression model training:** Model initialization and fitting are correct.
- **Evaluation metrics:** Accuracy, F1-score, and classification report are all calculated and printed.
- **Feature Importance:** Correctly extracts and sorts features by absolute coefficients.

**Areas for minor improvement:**
- **Handling of `quality` column drop:** The generated code drops `quality` immediately after creating `quality_binary`, which is fine, but the order of operations in the ground truth (creating `is_good_wine`, then dropping `quality` and `is_good_wine` from `X` while keeping `is_good_wine` for `y`) is slightly more explicit about what's being removed from features. This is a minor stylistic difference.
- **Missing value handling:** The generated code includes a commented-out line for `X.fillna(X.mean())` and states ""For this dataset, we assume it's clean or imputation is handled."" While the dataset *is* clean, the ground truth explicitly checks for and handles missing values, showing more robust programming for general cases, which was part of the prompt (""handle missing values if any""). The prompt asked to assume the CSV file is in the current directory, but not to assume it's clean.
- **Clarity of `X_train_scaled` as DataFrame:** The ground truth converts `X_train_scaled` and `X_test_scaled` back into DataFrames (`X_train_scaled_df`, `X_test_scaled_df`) to maintain column names for easier interpretation of feature importance. The generated code directly uses the numpy arrays for model training, which is numerically correct, but when printing feature importance, it relies on `feature_names = X.columns.tolist()` which might implicitly assume the order of features remains the same in the numpy array, which is correct for `StandardScaler` output but less explicit about the linkage than using DataFrames with column names. This is a minor point related to code robustness and readability for interpretation.",gemini-2.5-pro-preview,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

# 1) Load the dataset
# Assume heart.csv is in the current directory
try:
    df = pd.read_csv('heart.csv')
except FileNotFoundError:
    print(""Error: 'heart.csv' not found. Please ensure the file is in the current directory."")
    exit()

# 2) Create a box plot showing the distribution of Oldpeak for patients with HeartDisease=0 and HeartDi...",8,"The generated code correctly addresses most of the requirements of the task.

**Strong Points:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Data Loading:** Handles `FileNotFoundError` robustly.
*   **Box Plot:** Correctly generates the box plot, labels axes, and sets a title.
*   **Logistic Regression:**
    *   Correctly sets up X and y for the model.
    *   Prints the coefficient correctly.
    *   Calculates and interprets the odds ratio with a good explanatory sentence.
*   **Discretization (`pd.qcut`):**
    *   Correctly uses `pd.qcut` for 3 quantile bins.
    *   Includes an attempt to handle `ValueError` by retrying with `duplicates='drop'`, which is a good practice for `qcut`.
*   **Proportion Calculation:** Correctly calculates the proportion of `HeartDisease=1` within each bin using `groupby().mean()`.
*   **Output:** Presents the results clearly.

**Areas for Improvement (Why not a 9 or 10):**

*   **Box Plot Customization:** The ground truth added `plt.xticks` for clearer labels (e.g., 'No Heart Disease' vs 'Heart Disease') and `plt.grid(axis='y')` instead of just `plt.grid(True)`. These are minor visual enhancements but contribute to better presentation.
*   **Interpretation of Odds Ratio:** While the interpretation is good, the ground truth provides a more detailed explanation of the percentage increase/decrease, which adds more analytical depth. This wasn't explicitly asked for but is a common next step in interpreting odds ratios.
*   **Robustness in `qcut` Fallback:** The generated code attempts `duplicates='drop'` which is good. However, if `pd.qcut` completely fails due to extreme data distribution (e.g., all values are identical), the ground truth illustrates a fallback to `pd.cut` (fixed width binning) which is a more complete solution for rare edge cases, ensuring the task proceeds even if quantiles are problematic. While `duplicates='drop'` often solves the issue, a full fallback is even more robust.
*   **Visualization of Proportions:** The prompt asked to ""display"" the proportion, which the generated code does by printing the DataFrame. The ground truth also includes an optional bar plot visualization for these proportions, which enhances clarity and understanding, and aligns well with the ""investigate the relationship"" aspect of the task. While not strictly required by ""display"", it's a good practice in data analysis.
*   **Print statements:** The ground truth used more informative print statements for overall flow (e.g., ""Dataset loaded successfully"", ""Box plot displayed""). This helps users follow the execution flow.",gemini-2.5-pro-preview,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

# 1) Load data
# Assuming the CSV file 'spotify_2023.csv' is in the current working directory.
# If not, pandas will raise a FileNotFoundError.
df = pd.read_csv('spotify_2023.csv')
...",9,"The generated code provides a very good solution to the task. It correctly addresses all requirements, including data loading, `streams` conversion, target variable creation, feature selection, one-hot encoding, data splitting, model training, and evaluation.

**Strong points:**

*   **Correctness:** All steps are implemented accurately as per the prompt.
*   **Robust One-Hot Encoding:** The use of `ColumnTransformer` for one-hot encoding is a superior and more robust method than `pd.get_dummies` in a production-like setting, as it cleanly separates preprocessing steps and handles unseen categories (`handle_unknown='ignore'`). It also correctly applies `fit_transform` to training and `transform` to test data to prevent data leakage.
*   **Clear Feature Separation:** Explicitly defining `numerical_features` and `categorical_features` makes the code very readable and maintainable.
*   **Data Splitting Before Preprocessing:** The code correctly splits the data before applying the `ColumnTransformer`, which is crucial to avoid data leakage.
*   **Comprehensive Evaluation:** All required evaluation metrics (F1-score, ROC AUC, confusion matrix) are calculated and displayed.
*   **Code Structure and Readability:** The code is well-structured with comments explicitly linking to the task requirements, making it easy to follow.

**Areas for Improvement (minor):**

*   **Error Handling for CSV Load:** While the comment states `FileNotFoundError` will be raised, adding an explicit `try-except` block for file loading, similar to the ground truth, would make it more robust and user-friendly by providing a specific error message.
*   **NaN Handling after Feature Selection:** The line `model_df.dropna(inplace=True)` is generally good practice to ensure no NaNs in the model's input. However, in this specific case, `streams` NaNs are already handled. If any of the *selected features* themselves had NaNs, this step would be useful. For the given prompt, where *only* `streams` requires initial attention, it's a bit redundant but harmless. The prompt only strictly required dropping NaNs due to `streams` conversion.
*   **Stratification in Train-Test Split:** The ground truth includes `stratify=y` in `train_test_split`. This is a good practice for imbalanced datasets (which is often the case when creating a target based on a percentile), ensuring that the proportion of popular/not popular songs is maintained in both training and testing sets. The generated code omits this. While not explicitly asked in the prompt, it significantly improves the reliability of the model evaluation for classification tasks with potential class imbalance.

Overall, the generated code is highly competent and demonstrates strong understanding of the required data science pipeline. The use of `ColumnTransformer` is a notable highlight. Addressing the `stratify` parameter would make it even more robust.",gemini-2.5-pro-preview,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

def perform_spotify_clustering():
    # 1) Load data
    try:
        df = pd.read_csv('spotify_2023.csv', encoding='latin1') # Common encoding for such files
    except FileNotFoundError:
        print(""Error: 'spotify_2023.csv' not found in the current directory."")
        return
    except Exception as e:
     ...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong Points:**
*   **Correctness:** All specified steps (loading data, feature selection, NaN handling, standardization, K-Means, PCA, plotting, mean calculation) are implemented correctly.
*   **Clarity and Structure:** The code is well-organized within a function `perform_spotify_clustering()`, making it modular and readable.
*   **Robustness:** Includes error handling for `FileNotFoundError` and missing columns, which is a good practice not explicitly requested but beneficial.
*   **Completeness:** It correctly standardizes features, applies K-Means with the specified parameters, performs PCA, generates the scatter plot, and calculates & displays the mean original features per cluster.
*   **`n_init='auto'`:** Correctly uses `n_init='auto'` for KMeans.
*   **`random_state`:** Consistently uses `random_state=42` for reproducibility in K-Means and PCA.
*   **Original Features for Means:** Correctly uses the *original* (unstandardized) features to calculate the cluster means, as requested.

**Areas for Minor Improvement (for a perfect 10):**
*   **PCA Plot Title:** The requested title was ""K-Means Clusters (K=4) on PCA-Reduced Spotify Audio Features"". The generated code uses ""K-Means Clustering of Spotify Audio Features (PCA-reduced)"", which is very similar but not an exact match. This is a very minor detail.
*   **NaN Handling Output:** The ground truth code included a print statement showing how many rows were dropped due to NaNs. While not strictly required by the prompt, it's good for user feedback and debugging. The generated code only checks if `df_features_cleaned` is empty. Adding a count of dropped rows would be slightly more informative.

These are very minor points. Overall, the generated code performs excellently.",gemini-2.5-pro-preview,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. Load data
# Assuming 'spotify_2023.csv' is in the current directory.
# The problem statement does not specify encoding, so default 'utf-8' is assumed.
# If you encounter encoding errors, you might try:
# df = pd.read_csv('spotify_2023.csv', encoding='latin1')
# or
# df = pd.read_csv('spotify_2023.csv', encoding='ISO-8859-1')
df = pd.read_csv('spotify_2023.csv')

# 2. Convert `streams` to numeric (coerce errors to NaN)
# This ste...",8,"The generated code correctly addresses all requirements of the task.

**Strong points:**
*   **Correctness:** It accurately loads data, converts `streams` to numeric with error coercion, filters data based on both `streams` and `released_year` criteria, groups by year, calculates average audio features, and generates individual line plots for each feature.
*   **Clarity and Structure:** The code is well-structured with clear comments explaining each step, making it easy to understand the logic flow.
*   **Imports:** Proper imports for `pandas` and `matplotlib.pyplot` are included.
*   **Filtering Logic:** The filtering for `streams` > 10M and `released_year` between 2010 and 2023 is correctly implemented.
*   **Aggregation and Plotting:** The grouping and calculation of yearly averages are correct, and the plots are generated with appropriate titles, labels, and gridlines.
*   **Error Handling (Implicit):** The check for `yearly_averages.empty` is a good addition to prevent plotting errors if filters result in no data.
*   **X-axis Ticks:** The `plt.xticks(ticks=yearly_averages.index.map(int), rotation=45)` is a good detail for ensuring integer years and readability.

**Areas for Improvement (to reach a 10):**
*   **Plotting Efficiency:** While functionally correct, generating three separate `plt.figure()` and `plt.show()` calls means three separate plot windows pop up. The ground truth code uses `plt.subplot` to display all three plots in a single figure, which is generally preferred for presenting related trends together. This is a minor aesthetic/presentation point rather than a correctness issue.

Overall, the generated code is a very good solution to the problem. The only significant improvement would be to combine the plots into a single figure for better visualization, as demonstrated in the ground truth.",gemini-2.5-pro-preview,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder # Imported as per prompt request

# 1) Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# 2) For Likes, Dislikes, and Video views: remove commas, convert to numeric, and fill resulting NaNs with 0.
cols_to_clean = ['Likes', 'Dislikes', 'Video views']...",8,"The generated code correctly implements all the requested steps, demonstrating a good understanding of data cleaning, feature engineering, and model training/evaluation.

**Strong Points:**
*   **Correct Cleaning:** The code correctly handles commas, converts to numeric, fills NaNs with 0, and converts to `int` for 'Likes', 'Dislikes', and 'Video views'.
*   **Published Year:** Correctly extracts the year and handles potential NaNs.
*   **Category Handling:** Fills missing 'Category' values with 'Unknown' and applies one-hot encoding using `pd.get_dummies` effectively.
*   **Feature and Target Definition:** Correctly identifies and uses the specified features (`Dislikes`, `Video views`, `published_year`, one-hot encoded `Category`) and target (`Likes`).
*   **Data Splitting:** Uses the correct `test_size` and `random_state` for `train_test_split`.
*   **Model Training and Evaluation:** Correctly trains a `LinearRegression` model and computes R2 and MAE.
*   **Imports:** All necessary imports are present.

**Areas for Improvement (Minor):**
*   **Redundant `OneHotEncoder` import:** While the prompt requested `from sklearn.preprocessing import OneHotEncoder`, the code uses `pd.get_dummies` which is perfectly fine and often preferred for direct DataFrame integration. Importing `OneHotEncoder` is technically redundant if not used, but it's a very minor point given the prompt's instruction.
*   **More Robust `published_year` NaN handling:** The ground truth fills with 0 if parsing failed, which is a simpler approach for a benchmark. The generated code attempts a more robust median/mode imputation, which is good practice for real-world scenarios but beyond the minimal requirement of ""fill resulting NaNs with 0"" for that specific column as per the ground truth (though technically the prompt only specified ""fill resulting NaNs with 0"" for Likes/Dislikes/Video Views, and for 'published' just ""treat as numeric feature""). The ground truth's `fillna(0)` aligns more directly with the *spirit* of the fill NaN zeroes instruction for other numeric columns. However, the generated code's approach is not incorrect, just more complex than strictly necessary for this specific prompt.
*   **Output Formatting:** The output for R2 and MAE in the ground truth uses f-strings with specific decimal formatting (`.4f`, `.2f`) for clearer presentation, which the generated code could adopt.",gemini-2.5-pro-preview,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

def analyze_youtube_trends(csv_file_path='youtube_trendings.csv'):
    """"""
    Analyzes trends in YouTube trending videos from a CSV dataset.

    The function performs the following steps:
    1. Loads the dataset.
    2. Cleans the 'Video views' column (removes commas, converts to numeric, fills NaNs with 0).
    3. Ensures the 'published' column (year) is treated as a numeric or integer type.
    4. Groups the data by the 'publish...",8,"The generated code correctly addresses most of the requirements.

**Strong points:**
- **Data Loading:** Handles `FileNotFoundError` and general exceptions effectively.
- **`Video views` Cleaning:** Correctly removes commas, converts to numeric, and fills NaNs with 0 using robust methods (`astype(str)`, `str.replace`, `pd.to_numeric(errors='coerce')`, `fillna(0)`).
- **`published` year handling:** Attempts to convert the `published` column to datetime and extract the year, then converts to integer. It also includes checks for column existence.
- **Grouping and Calculations:** Correctly groups by `published_year` and calculates both the mean of `Video views` and the count of unique `Video` titles.
- **Plotting:** Creates two subplots with appropriate titles and axis labels as requested. `plt.tight_layout()` is used for better plot presentation.
- **Modularity:** Encapsulates the logic within a function `analyze_youtube_trends`.
- **Dummy Data Creation:** Includes a helpful `if __name__ == '__main__':` block to create a dummy CSV file, which is excellent for testing and demonstration.

**Areas for improvement:**
- **`published` column handling detail:** The task stated ""Ensure the `published` column (year) is treated as a numeric or integer type."" The ground truth used `publishedAt` as the source and extracted the year. The generated code assumes a column named `published`, and its handling of non-datetime values is a bit broad (`try-except` for `pd.to_datetime` and then `pd.to_numeric`). If the 'published' column was truly just a year number, the `to_datetime` step would be unnecessary. If it contains full dates (like `publishedAt` in the ground truth), the ground truth's approach is more explicit and robust for date strings. The `dropna(subset=['published_year'], inplace=True)` for published year might drop valuable rows, whereas `fillna(0)` in the ground truth maintains all rows, which might be preferred depending on analytical goals. The prompt said ""published column (year)"", which implies it might be a year already or a date from which a year needs to be extracted. The ground truth correctly assumes the common case of a `publishedAt` date column and extracts the year from it.
- **Clarity in `published` column name:** The prompt was slightly ambiguous about the exact column name for ""published year"" (it mentioned `published` column (year)). The ground truth correctly identified `publishedAt` as the typical date column to extract the year from. The generated code assumed a column named `published`. If the actual dataset uses `publishedAt`, this code would fail. It's a minor point, but good to be explicit or handle both `published` and `publishedAt` if uncertainty exists.
- **Missing `numpy` import:** While not directly used in the final version of the generated code for calculations, `numpy` is often imported alongside pandas and matplotlib for data science tasks. Including it for consistency is a minor point.
- **Axis labels for `Video views`:** While the y-axis is labeled 'Average Views', for very large numbers of views, it might benefit from `ticklabel_format(style='plain')` to prevent scientific notation, similar to the ground truth. This is a minor aesthetic improvement.",gemini-2.5-pro-preview,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
import string

# Set up stop words
try:
    import nltk
    from nltk.corpus import stopwords
    try:
        # Attempt to load stopwords directly
        ENGLISH_STOP_WORDS = set(stopwords.words('english'))
    except LookupError:
        # If not found, download them
        nltk.download('stopwords', quiet=True)
        ENGLISH_STOP_WORDS = set(stopwords.words('english'))
except ImportError:
    # Fallback list if NLTK i...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It successfully loads the data, creates the `title_word_count`, cleans the `Video views` column, calculates and prints the Pearson correlation, filters data by year, and extracts/processes words to find the 5 most common ones.

**Strong points:**
- **Robust Stop Word Handling:** The code includes excellent error handling for NLTK stop words, attempting to download them if not found, and even providing a comprehensive fallback list if NLTK isn't available. This is a very good practice.
- **Explicit Type Conversions:** Uses `astype(str)` for title and views before processing, which is robust.
- **Comprehensive Punctuation Removal:** Uses `re.sub(punctuation_to_replace_pattern, ' ', lower_text)` which correctly replaces punctuation with spaces, preventing words from merging (e.g., ""word.other"").
- **Clear Filtering Logic:** The `filtered_df.empty` check and the `if all_processed_words:` check contribute to robust handling of edge cases.
- **Clarity and Readability:** The code is well-structured and easy to follow.

**Minor areas for improvement (to achieve a perfect 10):**
- **Consistency in Output Formatting:** The correlation printout uses `print(f""Pearson correlation between title_word_count and Video views: {correlation:.4f}"")` which is good. For the most common words, it uses `print(f""{word}: {count}"")`. The ground truth used `print(f""- '{word}': {count} occurrences"")` which is slightly more verbose and self-explanatory. This is a very minor aesthetic point.
- **Handling of `.astype(int)` for `Video views`:** While `df['Video views'] = df['Video views'].fillna(0).astype(int)` is generally fine, it implicitly truncates any non-integer values when converting to int. For 'views', this is unlikely to be an issue, but it's a detail to note for more general numeric column cleaning. The `fillna(0)` is crucial and well-placed.
- **Redundant `.copy()`:** While `filtered_df = df[df['published_year'].isin([2023, 2024])].copy()` is harmless and often good practice to avoid SettingWithCopyWarning, it's not strictly necessary for this specific task because `filtered_df` is only read from, not modified in a way that would trigger the warning on the original `df`.

Overall, this is a very high-quality solution that demonstrates a strong understanding of Python, Pandas, regular expressions, and text processing.",gemini-2.5-pro-preview,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score

def run_classification_pipeline():
    # 1) Load data
    try:
        df = pd.read_csv('mobiles_2025.csv')
    except FileNotFoundError:
        print(""Error: 'mobiles_2025.csv' not found. Make sure the file is in the current directory."")
        return

    # 2) Clean `Launched Pr...",8,"The generated code correctly implements most of the requirements.

**Strong Points:**
- **Correct Data Loading and Error Handling:** Handles `FileNotFoundError` for the CSV.
- **Robust Cleaning of `Launched Price (USA)`:** Correctly removes 'USD ' and commas, converts to numeric with `errors='coerce'`, creates `is_expensive_usa` target, handles NaNs, and converts to `int`. It also uses `pd.notnull` correctly for the mapping.
- **Good Feature Cleaning:** Correctly handles `Mobile Weight`, `RAM`, `Battery Capacity`, and `Screen Size` by stripping units and converting to numeric.
- **`Front Camera` and `Back Camera` Cleaning:** The `extract_first_mp` function is a reasonable approach to extract the first numerical MP value, although it deviates slightly from the regex-based `MP` extraction in the ground truth. It defaults to 0.0 correctly.
- **Correct One-Hot Encoding:** Uses `pd.get_dummies` for `Company Name` and `Processor`. Includes warnings if columns are missing.
- **Proper Feature and Target Definition:** Identifies cleaned numerical and one-hot encoded features correctly for `X` and `y`.
- **Comprehensive NaN Handling:** Drops NaNs from the target as required, and also explicitly drops NaNs from the final `X` and `y` before splitting, which is crucial. It also includes checks for empty DataFrames after NaN drops.
- **Correct Data Splitting:** Uses `train_test_split` with `test_size=0.2` and `random_state=42`.
- **Stratified Split Handling:** Attempts stratification and includes robust error handling for cases where stratification might fail due to too few samples in a class, falling back to non-stratified split. This is a very good addition.
- **Correct Model Training:** Uses `RandomForestClassifier` with the specified `n_estimators` and `random_state`.
- **Correct Evaluation Metrics:** Calculates and prints F1-score (macro average) and ROC AUC score.
- **ROC AUC Robustness:** Includes checks if ROC AUC can be calculated (i.e., if target has at least two classes in `y_test` and `y_train`).
- **Good Code Structure:** Encapsulates the logic in a function `run_classification_pipeline` and uses an `if __name__ == '__main__':` block.
- **Warning Messages:** Provides helpful warning messages when columns are not found, which aids debugging.

**Areas for Improvement (minor deviations from ground truth/best practices):**
- **Camera Cleaning Regex:** The `extract_first_mp` function extracts *any* first number rather than specifically a number followed by ""MP"". While it might work for this dataset, the ground truth's `r'(\d+(\.\d+)?)\s*MP'` regex is more precise for the task's requirement of ""extract the first numerical MP value"".
- **Redundant `fillna('Unknown')` for Categorical Columns:** The `pd.get_dummies` function's `dummy_na=False` (default) handles NaNs by not creating dummy columns for them, which is acceptable unless explicit 'Unknown' categories are desired. The `fillna('Unknown')` before `get_dummies` might create an 'Unknown' category, which might not be strictly necessary based on the prompt if NaNs are simply to be ignored by `get_dummies`. The ground truth does not explicitly handle NaNs in categorical columns before one-hot encoding, letting `pd.get_dummies` `dummy_na=False` implicitly ignore them.
- **Minor Missing Check/Warning in Feature Cleaning:** For `Launched Year`, the generated code adds it to `numerical_feature_base_names` *after* potentially adding `np.nan` if the column is missing. The ground truth explicitly includes ""Launched Year is already numeric"" in the comments implying it expects the column to be present or for downstream NaNs to be handled, which the generated code does well. It's a small difference.
- **F1-score `zero_division`:** The addition of `zero_division=0` to `f1_score` is a good defensive practice, but the prompt didn't strictly require it. It doesn't harm correctness.

Overall, the generated code is highly functional and well-structured, successfully addressing almost all aspects of the task with good error handling and robustness. The minor points are mostly about slightly different interpretations or defensive coding beyond the core requirements.",gemini-2.5-pro-preview,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# 1) Load the dataset
# Assuming 'mobiles_2025.csv' is in the current directory
df = pd.read_csv('mobiles_2025.csv')

# 2) Define a function to clean price columns
def clean_price(value):
    # Handle NaN values first
    if pd.isna(value):
        return np.nan
    
    # Handle if the value is already numeric (int or float), but not boolean
    # Booleans are instances of int, so `isinstance(True, bool)` check is important
    if isinstance(val...",9,"The generated code is highly correct and complete, successfully addressing almost all requirements of the task.

**Strong Points:**
*   **Correct Cleaning Logic:** The `clean_price` function correctly handles NaN input, removes all specified currency symbols, commas, and strips whitespace. It also correctly converts to float and returns `np.nan` on `ValueError`. The addition of `if not price_str: return np.nan` for strings that become empty after cleaning is a good robustness check.
*   **Handling of numeric values:** The `clean_price` function includes good logic to handle cases where the input might already be numeric (int/float), ensuring it's cast to a float consistently, and correctly handles boolean types to avoid accidental conversion.
*   **Correct `Average Global Price` Calculation:** Uses `mean(axis=1, skipna=True)` which is precisely what's required to calculate the average using only available (non-NaN) prices.
*   **Correct Top 5 Identification and Display:** The code correctly sorts, selects the top 5, and displays the `Model Name` and `Average Global Price` in the requested format.
*   **Code Structure and Readability:** The code is well-structured, uses meaningful variable names, and includes comments explaining each step which is helpful.

**Areas for Improvement (minor):**
*   **Error Handling for File Not Found:** The generated code assumes the CSV file exists and will raise a `FileNotFoundError` if it doesn't. The ground truth code includes a `try-except` block for `FileNotFoundError`, which is a good practice for robustness in real-world scenarios.
*   **Handling Missing Columns Gracefully:** The generated code applies `df[col_name].apply(clean_price)`. If a column listed in `price_column_names` does not exist in the DataFrame, this will raise a `KeyError`. The ground truth code checks `if col in df.columns:` before applying the cleaning, which makes it more robust to slight variations in the CSV header. While the prompt implies the columns *will* exist, robust code often accounts for this.

Overall, the solution is excellent and demonstrates a strong understanding of pandas and data cleaning. The minor points are about making the code even more robust to potential file or data inconsistencies, which is a nuanced aspect beyond direct task fulfillment.",gemini-2.5-pro-preview,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import sys

# 1. Load the dataset
try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: 'mobiles_2025.csv' not found in the current directory."", file=sys.stderr)
    sys.exit(1)

# Check for essential columns
required_columns = ['RAM', 'Battery Capacity', 'Launched Price (USA)', 'Company Name', 'Launched Year']
for col in required_columns:
    if col not in df.columns:
        print(f""Error: Require...",7,"The generated code correctly performs the data loading, cleaning, filtering for Apple products, and grouping by `Launched Year`. It also attempts to create box plots as required.

However, there are several areas for improvement:

1.  **Redundant Cleaning (Minor):** The `RAM`, `Battery Capacity`, and `Launched Price (USA)` columns are cleaned into *new* columns (`RAM_cleaned`, `Battery_Capacity_cleaned`, `Launched_Price_USA_cleaned`). The task implies that the cleaning should happen *in place* on the original column names. While not strictly a functional error, it deviates from typical practice and might lead to confusion if the original columns were still needed later for other purposes. The ground truth cleans in place.

2.  **`Launched Year` Handling:**
    *   The `Launched Year` cleaning is less robust than the ground truth. It performs `dropna` *before* attempting `pd.to_numeric(..., errors='raise').astype(int)`. If the `Launched Year` column contains non-numeric strings, `errors='raise'` will cause a `ValueError` and block execution for that year, even if `dropna` handles `NaN`. The ground truth uses `errors='coerce'` first, then drops NaNs, which is more forgiving and robust for mixed types.
    *   The warning message for `Launched Year` mixed types suggests ""Attempting to sort and use as is,"" but if `astype(int)` fails, `Launched Year` remains in its original problematic state, and `groupby` might still sort lexicographically if it's an object type. The actual plotting loop then just uses `year` directly, which could still be a string or mixed type affecting `fig.suptitle`. The ground truth explicitly converts it to `int` and relies on an initial `dropna` on it after `to_numeric(errors='coerce')`.

3.  **Plotting Library (Minor):** The prompt requested `matplotlib.pyplot or seaborn`. The generated code only uses `matplotlib.pyplot` for box plots, whereas `seaborn` often produces more aesthetically pleasing and concise plots (as seen in the ground truth). While not a failure, using `seaborn.boxplot` would have been a better choice given the prompt's suggestions.

4.  **Plotting Refinements:**
    *   The `tick_params` to hide x-axis labels is slightly redundant; `boxplot` in `matplotlib` usually defaults to not having x-axis labels for a single box.
    *   The `if not plot_data_ram.empty:` checks within the plotting loop are good for robustness, but `sns.boxplot` handles empty data gracefully (it plots nothing) without needing explicit `text` placeholders, relying on the `dropna` of the main dataframe to ensure most data is present. It makes the code a bit more verbose than necessary compared to a `seaborn` approach.

Overall, the code fulfills most requirements, but the cleaning steps for `Launched Year` and the minor inefficiencies/less robust choices in cleaning and plotting prevent it from achieving a higher score.",gemini-2.5-pro-preview,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# Sklearn Preprocessing
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_val_score, learning_curve, validation_curve
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, LabelEncoder
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.pipeline import Pipeline
from sklearn...",7,"The generated code provides a good attempt at meeting the complex requirements, demonstrating an understanding of many advanced ML concepts. However, it falls short in a few critical areas, particularly regarding the robustness and correctness of the preprocessing pipeline, and full implementation of all specified techniques.

**Strong Points:**
*   **Comprehensive Imports:** Includes most necessary libraries.
*   **Structured Approach:** Follows a logical flow from data loading, preprocessing, model tuning, ensemble building, evaluation, feature selection, and interpretability.
*   **Multiple Imputation:** Attempts multiple imputation for `Age` using `KNNImputer`, which is a decent step beyond simple mean/median, though not the full IterativeImputer (MICE) requested.
*   **Feature Engineering:** Includes name title extraction, family size, `IsAlone`, `HasCabin`, `Deck`, Age_Group, and interaction terms (`Age_Class`, `Fare_Class`).
*   **Polynomial Features:** Implements polynomial features, although the integration into the preprocessing pipeline is somewhat manual and less robust than expected.
*   **Multiple Base Models:** Uses RandomForest, XGBoost, SVM, and Neural Network.
*   **Hyperparameter Tuning:** Employs `RandomizedSearchCV` for base models.
*   **Ensemble Methods:** Implements both Stacking and Voting classifiers.
*   **Feature Selection:** Uses SelectKBest, RFE, and SelectFromModel.
*   **Extensive Evaluation:** Includes stratified K-fold cross-validation, learning curves, validation curves, and statistical significance tests.
*   **Interpretability:** Uses SHAP for both XGBoost and RandomForest.

**Areas for Improvement (Why it's not higher):**

1.  **Preprocessing Robustness and Pipeline Integration:**
    *   **Custom Transformer Missing:** The prompt explicitly asked for a `custom transformer` for name/cabin and feature engineering. The generated code implements this logic inside a `preprocess_data` function, which is less ideal for a `ColumnTransformer`/`Pipeline` setup where custom steps are transformers.
    *   **Multiple Imputation (MICE):** The prompt specified ""Multiple Imputation by Chained Equations (`IterativeImputer`)"". The generated code uses `KNNImputer`, which is a different (though valid) multiple imputation strategy. The ground truth explicitly shows `IterativeImputer`.
    *   **ColumnTransformer/Pipeline:** The preprocessing is done as a standalone function (`preprocess_data`) followed by manual scaling and then manual polynomial feature creation. Replicating `ColumnTransformer` with `Pipeline` steps (as in the ground truth) would make the preprocessing robust for new data and align better with standard sklearn practices for complex pipelines. The current approach makes it harder to correctly apply `SHAP` or `LIME` (which expect a consistent pipeline or a way to translate raw data to preprocessed data) and evaluate feature selection within an actual `Pipeline`.
    *   **Feature Name Consistency:** Handling feature names after custom transformations and polynomial features, especially for `ColumnTransformer`, is crucial for interpretability tools like SHAP. The manual `final_feature_names` approach works but is prone to errors if the order changes.

2.  **Model Interpretability (LIME):**
    *   The prompt specifically requested SHAP *or* LIME. The generated code only implements SHAP. LIME would provide local explanations, which is a different and valuable perspective.

3.  **Feature Selection Integration:**
    *   The current feature selection approach applies selection *before* tuning models for each subset. While this is one valid strategy, the prompt asks to ""compare their impact"" and implies that models should be re-tuned or at least re-evaluated *after* feature selection, ideally showing how performance changes. The generated code does this partially by re-evaluating the Stacking Ensemble but doesn't show full re-tuning for each feature set. The ground truth re-trains a simple RF on the reduced sets to demonstrate impact.

4.  **Minor Issues:**
    *   `LabelEncoder` on `Survived` is unnecessary for binary classification tasks, and `pd.get_dummies` (or `OneHotEncoder` via `ColumnTransformer`) is generally preferred for other categorical features. The generated code internally maps categorical features to integers, which might not be ideal for all models (e.g., Tree-based models might handle it, but linear models or SVMs often prefer one-hot encoding).
    *   The `preprocess_data` function modifies 'Sex', 'Embarked', 'Deck', 'Title' into integer encodings. This means `StandardScaler` will be applied to them later, which is generally not ideal for ordinal encodings, and certainly not for nominal ones. One-hot encoding (as used in the ground truth via `ColumnTransformer`) would be much better.
    *   The `early_stopping` and `n_iter_no_change` in `MLPClassifier` params dictionary are for the model itself, not related to the `RandomizedSearchCV` parameters. They are hardcoded model parameters.
    *   The `use_label_encoder=False` for XGBoost is deprecated, but valid in this specific scenario.
    *   `seaborn-v0_8-whitegrid` styling might not be available in all environments.

To get a higher score, the code needs to consolidate its preprocessing steps into an `sklearn.pipeline.Pipeline` with `ColumnTransformer` and implement the requested `IterativeImputer` and LIME.",gemini-2.5-pro-preview,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# 1. Load the dataset
try:
    df = pd.read_csv('imdb_top_250.csv')
except FileNotFoundError:
    print(""Error: 'imdb_top_250.csv' not found in the current directory."")
    exit()

# 2. Define features X
X = df[['Year', 'Runtime']]

# 3. Define the target y
y = df['Rating']

# 4. Split the data into trainin...",7,"The generated code correctly addresses most of the task requirements: it loads the data, defines features and target, splits the data, trains a Linear Regression model, and evaluates it using R-squared and MAE. It includes all necessary imports and follows a logical flow.

However, the major flaw is the lack of robust data preprocessing, especially for the 'Runtime' column. The `imdb_top_250.csv` dataset typically has 'Runtime' in a format like 'X min'. The generated code directly tries to use this column as a feature, which will lead to a `ValueError` during the `train_test_split` or `model.fit()` if the column is not numeric. The ground truth code correctly handles this by stripping ' min' and converting to numeric with `errors='coerce'`. This omission makes the generated code fragile and likely to fail with the given dataset.

Additionally, the generated code assumes `Year` and `Rating` are directly numeric, which might not always be true in a real-world scenario (though often handled implicitly by pandas if values are clean). It also lacks checks for missing values or empty dataframes after potential issues. More robust error handling for column existence would also improve it.

While the core logic is correct, the lack of data cleaning makes it less robust for the described dataset.",gemini-2.5-pro-preview,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 1) Load the dataset
try:
    df = pd.read_csv('imdb_top_250.csv')
except FileNotFoundError:
    print(""Error: 'imdb_top_250.csv' not found. Make sure the file is in the current directory."")
    exit()

# 2) Select the features `Rating` and `Runtime` for clustering
# First, clean 'Runtime' column by removing ' min' and converting to numeric
if 'Runti...",9,"The generated code is very good and largely fulfills all requirements.

**Strong Points:**
- **Correctness:** Successfully loads data, selects features, standardizes, applies K-Means with specified parameters (K=3, n_init='auto', random_state=42).
- **Visualization:** Creates a scatter plot of original `Runtime` vs. `Rating` correctly colored by cluster labels, with appropriate title and axis labels.
- **Cluster Means:** Calculates and prints the mean `Rating` and `Runtime` for each cluster as requested.
- **Error Handling:** Includes `try-except` for `FileNotFoundError`.
- **Imports:** All necessary imports are present.
- **Code Structure:** The code is well-structured and follows the task's numbered steps.

**Areas for Minor Improvement (to reach a perfect 10):**
1.  **Robust `Runtime` Cleaning:** The cleaning for the `Runtime` column `df['Runtime'].str.replace(' min', '').astype(float)` is less robust than the ground truth. If `Runtime` had mixed types, or non-numeric strings that don't just include "" min"", `astype(float)` would raise an error. The ground truth's approach using `astype(str)` first, then `pd.to_numeric(..., errors='coerce')` is more resilient to varied input formats, handling cases where ' min' might not be present or other non-numeric values exist. While `imdb_top_250.csv` typically has consistent 'X min' format, robust code should anticipate variations.
2.  **`Runtime_Numeric` Column:** The generated code creates a new column `Runtime_Numeric` for the cleaned runtime. While not explicitly wrong, selecting features directly from `df[['Rating', 'Runtime_Numeric']]` immediately after creating this column is fine. The ground truth cleans the 'Runtime' column *in place*, simplifying subsequent feature selection.
3.  **No `dropna`:** The generated code does not include a `dropna` step after cleaning/converting `Rating` and `Runtime`. If, after cleaning, some `Runtime` or `Rating` values become `NaN` (due to `errors='coerce'` if it were used, or other data issues), `StandardScaler` would fail. The ground truth explicitly handles this by dropping rows with `NaN` in the selected features. This is a crucial robustness step for real-world data.

Overall, the generated code is highly functional and accurate for the given task. The suggested improvements primarily focus on robust data cleaning practices that are essential for real-world data science tasks.",gemini-2.5-pro-preview,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string # Included as per requirements
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import nltk

# Ensure NLTK resources 'stopwords' and 'punkt' are available
try:
    _ = stopwords.words('english')
except LookupError:
    nltk.download('stopwords', quiet=True)
try:
    _ = word_tokenize(""sample sentence"")
except LookupError:
    nltk.download('punkt', quiet=True)

# 1) Load the dataset
df = pd.read_...",7,"The generated code correctly addresses most of the task requirements.

**Strengths:**
*   Correctly loads the dataset and creates the `title_length` feature.
*   Calculates and prints the Pearson correlation coefficient.
*   Handles NLTK downloads correctly.
*   Performs tokenization and stop word removal.
*   Correctly identifies and prints the 10 most common words.
*   Includes all required imports.
*   Handles potential NaN values in 'Title' before processing.

**Areas for Improvement:**
*   **Punctuation Removal Robustness:** The method used for punctuation removal (`''.join(char for char in processed_title if char.isalnum() or char.isspace())`) is less robust than using regex (like `re.sub(r'[^a-zA-Z0-9\s]', '', text)` as in the ground truth). The current method might miss certain non-alphanumeric characters or perform less efficiently for very large datasets. The ground truth uses `re` which was not imported in the generated code explicitly, though the prompt asked for `string` (which is less ideal for this specific punctuation removal task itself).
*   **Correlation Calculation Robustness:** While the correlation calculation is functionally correct, it assumes 'Rating' is numeric without explicitly converting it or handling potential non-numeric values or NaNs in 'Rating'. The ground truth explicitly uses `pd.to_numeric(df['Rating'], errors='coerce')` and `dropna` to ensure a robust correlation calculation, which is a good practice for real-world data where column types aren't always guaranteed or clean.
*   **Processing Efficiency:** The iteration through `df['Title']` and building `all_processed_words` using a loop is less idiomatic and potentially less efficient than using Pandas' `apply` methods for string operations and tokenization (`df['Title'].apply(...)`) as shown in the ground truth. This is a minor point for smaller datasets but can become relevant for larger ones.

Overall, it's a good attempt that covers the core requirements. The main limitations are in the robustness of the preprocessing and correlation calculation, which are often critical in data science tasks.",gemini-2.5-pro-preview,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'penguins.csv' not found in the current directory."")
    exit()

# Pre-empti...",7,"The generated code attempts to address all requirements and demonstrates good understanding of data preprocessing pipelines using `ColumnTransformer` and `Pipeline`, which is a more robust and cleaner approach for complex preprocessing steps.

**Strengths:**
*   **Pipeline usage:** Correctly uses `ColumnTransformer` and `Pipeline` for preprocessing and model training, which is good practice.
*   **Error handling (`FileNotFoundError`):** Includes a check for the CSV file.
*   **Target NA handling:** Explicitly drops rows with NA in the target variable, which is good practice for supervised learning.
*   **Numerical imputation:** Correctly uses median for numerical feature imputation.
*   **Categorical imputation (`sex`):** Correctly uses mode for 'sex' imputation and includes a robust check for empty mode series and NaN values in the mode.
*   **Scaling and One-Hot Encoding:** Correctly applies `StandardScaler` to numerical features and `OneHotEncoder` to categorical features within the pipeline.
*   **Label Encoding:** Correctly applies `LabelEncoder` to the target variable.
*   **Cross-validation:** Correctly performs 5-fold cross-validation and prints the mean accuracy.
*   **Robustness checks:** Includes checks for empty dataframes before cross-validation.

**Areas for Improvement (Why it's not higher):**
*   **Imputation of `island` (`mode` and drop remaining NAs):** The logic for `island` imputation is slightly off. The requirement states: ""if a row still has NA in `island` after mode imputation... drop those rows"". The generated code correctly identifies the mode, but then if `island_mode_series` is empty (meaning all 'island' values were NA), it doesn't explicitly fill, letting the NAs persist, which are then correctly dropped. However, the ground truth explicitly handles the ""all NA"" case for 'island' by immediately dropping them (which implies the mode is inaccessible or doesn't make sense). The generated code's logic implies that if `island_mode_series` is empty, no `fillna` happens, and the `dropna` later handles it. This is technically correct for the outcome, but the wording ""if mode imputation results in NA for island"" suggests the imputation step itself could result in NA, which then need dropping. The generated code's `fillna(mode)` works well when a mode exists, but it doesn't explicitly *ensure* that if all `island` values were NA, those rows are dropped *because* they couldn't be mode imputed. The ground truth handles this scenario more directly where `mode()` yields an empty Series (`X['island'].isnull().all()`). The current code implicitly relies on the `dropna` step to fulfill the ""if mode imputation results in NA ... drop"" clause, rather than explicitly dealing with the ""mode itself is NA/empty"" scenario as part of the imputation logic. This is a subtle difference, but one that makes the ground truth's handling of `island` more explicit and robust for edge cases.

*   **Handling of `sex` NAs after placeholder:** The generated code uses `'Unknown'` as a fallback for `sex` if its mode is empty or invalid. While this is a valid strategy for handling NAs, the task implied that all NAs in categorical features would be imputed with their *respective modes*. If all 'sex' values were NA, then the mode would be undefined, and filling with `'Unknown'` deviates slightly from the strict ""impute using mode"" instruction, even if it's a practical solution to avoid errors. The ground truth explicitly states ""impute using their respective modes"" and doesn't consider a fallback.

*   **Readability of `sex` and `island` NA handling:** The `if not sex_mode_series.empty and pd.notna(sex_mode_series[0]):` logic is correct but a bit verbose for simple mode imputation. The ground truth's `X['sex'].mode()[0]` followed by `fillna` assumes `mode()` will always return at least one element for non-all-NA columns, which is typically true for `mode()` in pandas when not all values are NA.

The generated code is very close to perfect, but the minor deviations in the precise interpretation of the `island` and `sex` imputation rules, particularly regarding handling situations where the mode itself might be undefinable for an entirely NA column (which the problem statement explicitly highlighted for `island`), prevent a higher score.",gemini-2.5-pro-preview,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Select the features
feature_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
# Include 'species' for later use, ensuring it's aligned with the processed data
df_processed = df[feature_cols + ['speci...",9,"The generated code correctly implements all specified tasks: loading data, selecting features, dropping NA, standardizing, performing K-Means and PCA, and generating both required scatter plots with appropriate coloring and titles. The code quality is generally good, with proper imports and a logical flow.

**Strengths:**
- All steps are clearly addressed and implemented correctly.
- Uses `n_init='auto'` and `random_state=42` for K-Means.
- Correctly handles the `species` column alignment by first including it in `df_processed` and then extracting `original_species_labels` *after* NA removal.
- Plots are generated correctly with appropriate labels and titles.

**Areas for Minor Improvement:**
- Error handling for `FileNotFoundError` (as seen in the ground truth) is missing. While not explicitly asked for, it's good practice.
- The `palette='viridis'` was used for the second plot (original species labels). While it works, `tab10` or a similar categorical palette might be more typical and distinct for categorical `species` labels, as seen in the ground truth.
- `pca_df['Species'] = original_species_labels.values` ensures correct assignment, but `reset_index(drop=True)` on `original_species_labels` before assignment (as in ground truth) is a slightly more robust way to guarantee alignment if the `original_species_labels` Series had a non-default or misaligned index after filtering. In this specific case, because `df_processed` was created from `df` and then NAs dropped, the indexing likely remains aligned, but it's a subtle point.

Overall, it's a very strong solution. The minor points are mostly about robustness and minor stylistic choices rather than core correctness.",gemini-2.5-pro-preview,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'penguins.csv' not found in the current directory."")
    exit()

# 2) Impute missing values
# Drop rows if `island` is NA
df.dropna(subset=['island'], inplace=True)

# Impute numerical ...",8,"The generated code successfully addresses most of the requirements. It correctly loads the data, performs the specified imputations (mean for numerical, mode for sex, dropping NA islands), and one-hot encodes `island` and `sex`. The GradientBoostingClassifier is trained, and feature importances are extracted and printed with their correct names.

However, there's a minor deviation in the plotting section. The prompt asks for ""For each `species`, create box plots to visualize the distribution of `flipper_length_mm` (using imputed data) grouped by the imputed `sex` column."" The ground truth interprets this as a single plot showing `flipper_length_mm` vs `species` with `sex` as hue. The generated code creates *separate* plots for each species, showing `flipper_length_mm` vs `sex`. While this is a valid visualization, it breaks the requirement of ""For each species, create box plots..."", which implies a single plot with species on the x-axis and sex as hue, as typically interpreted in such benchmark tasks. The phrasing ""For each species, create box plots... grouped by the imputed sex"" usually means `x=species, hue=sex` or `x=sex, y=flipper, col=species` using `seaborn.catplot`. The generated code does `x=sex, y=flipper` for each species, which is different from a specific ""grouping"" per species in a single plot.

Additionally, the code includes a line `if '.' in df['sex'].unique(): df['sex'].replace('.', np.nan, inplace=True)` which is not specified in the prompt and might not be necessary for the provided `penguins.csv` dataset, although it's a good defensive coding practice.

Overall, the code is well-structured, uses proper imports, and is highly functional. The plotting difference is a slight misinterpretation rather than a functional error.",gemini-2.5-pro-preview,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer

# 1. Load the dataset
# Assuming ""penguins.csv"" is in the current directory as per problem statement.
df = pd.read_csv(""penguins.csv"")

# 2. Impute numerical columns with KNNImputer
numerical_cols_spec = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
valid_numerical_cols_for_imputation = []

for col_name in numerical_cols_spec:
    if col_name in df.columns:
        # Ensure column is numeric; entri...",8,"The generated code correctly addresses all requirements of the task. It loads the dataset, performs KNN imputation on the specified numerical columns, imputes the 'sex' column with its mode, and verifies the remaining NA values.

**Strong points:**
*   **Correct Imputation Logic:** Both KNN and mode imputation are correctly applied.
*   **Robustness:** The code includes checks for column existence (`if col_name in df.columns:`), handles cases where numerical columns might not be purely numeric (`pd.to_numeric(errors='coerce')`), and manages scenarios where a numerical column might be entirely NaN or a categorical column might have no mode. This makes the code more robust to unexpected data conditions.
*   **Clear Structure:** The code is well-commented and follows the sequential steps outlined in the task.
*   **Verification:** It correctly prints the total NA values at the end, fulfilling the verification requirement.

**Areas for improvement (minor):**
*   **File Not Found Handling:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is good practice. The generated code assumes the file will always be found. Adding this would make it more robust.
*   **Redundant Type Conversion Check:** The internal check `if df[col_name].isnull().all():` inside the `for` loop, after `pd.to_numeric(df[col_name], errors='coerce')`, is slightly redundant. An `isnull().all()` check before calling `fit_transform` on the entire `valid_numerical_cols_for_imputation` dataframe can be more efficient, especially if `fit_transform` handles this gracefully with a warning/error (which `KNNImputer` typically does). However, the current approach works.
*   **Verbosity:** While the commented-out print statements are good for development, the final solution should generally only print what is explicitly requested. The current output meets the requirement.
*   **Consistency in Warnings:** Some warnings are more verbose than others (e.g., ""Column will retain all NaNs"" vs. just ""Skipping it for KNN imputation""). Minor consistency refinement could be considered.

Overall, it's a solid solution and demonstrates a good understanding of both the data cleaning task and robust Python coding practices.",gemini-2.5-pro-preview,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

def perform_penguin_hypothesis_test(csv_file_path='penguins.csv', alpha=0.05):
    """"""
    Performs a hypothesis test on the penguins dataset to compare culmen lengths
    between Adelie and Gentoo penguins.

    Args:
        csv_file_path (str): Path to the penguins.csv file.
        alpha (float): Significance level for the hypothesis test.
    """"""
    try:
        # 1) Load the dataset using pandas
        penguins_df = pd.read_csv(csv_f...",9,"The generated code is highly correct, complete, and well-structured. It successfully addresses all the requirements of the task.

**Strong points:**
*   **Correctness:** Accurately loads data, filters species, handles NAs, performs the t-test, and interprets the p-value against the significance level.
*   **Completeness:** All 6 requirements from the prompt are met explicitly.
*   **Code Quality:**
    *   Uses a function (`perform_penguin_hypothesis_test`) which promotes reusability and organization.
    *   Includes a `try-except` block for `FileNotFoundError`, making it robust.
    *   Adds checks for empty dataframes or insufficient samples after filtering, preventing errors.
    *   Provides clear print statements for diagnostic information and the conclusion.
    *   The `equal_var=False` (Welch's t-test) in `ttest_ind` is a good, more robust default choice than Student's t-test, although the prompt did not specify which one. The ground truth used `equal_var=True`. For a benchmark, sticking to the standard t-test (`equal_var=True`) or explicitly making a choice and documenting it might be slightly preferred if not specified, but Welch's is often a better real-world choice. The generated code explicitly states its choice (`We assume unequal variances by default...`) which is good.
    *   The addition of `nan_policy='raise'` strengthens the NA handling.
    *   Includes a `if __name__ == '__main__':` block with a dummy CSV creation for easy testing, which is excellent for demonstration and debugging.

**Minor Improvements (for a perfect 10):**
*   **Consistency in `equal_var`:** While using `equal_var=False` is a robust choice and well-documented by the generated code, the prompt implies a standard t-test (which often assumes equal variances) by not specifying. The ground truth uses `equal_var=True`. If the benchmark expects a precise match or a specific default, this could be a minor divergence. For typical data science tasks, the generated code's choice is arguably better.
*   **Docstring Detail:** The docstring is good, but could perhaps mention the specific t-test (Welch's) being used because `equal_var=False` is set.
*   **Clarity on Hypothesis:** While the null and alternative hypotheses are correctly stated in comments, explicitly mentioning what the null hypothesis is in the print statements *before* the conclusion could make the output slightly clearer for a novice user.

Overall, this is an excellent piece of code that directly addresses the problem with robust error handling and good practices.",gemini-2.5-pro-preview,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report

def build_and_evaluate_rf_penguin_model():
    # 1) Load the dataset
    try:
        df = ...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Correct Preprocessing:** The code correctly implements median imputation for numerical features, mode imputation for 'sex', and one-hot encoding for 'island' and 'sex' using `ColumnTransformer` and `Pipeline`. This is a robust and flexible approach.
- **Label Encoding:** It correctly label encodes the target variable `species`.
- **Stratified Split:** Data splitting is done correctly, including stratification by `species` and `random_state`.
- **Model Training and Evaluation:** The `RandomForestClassifier` is trained with the specified parameters, and evaluation metrics (accuracy and classification report) are printed as required.
- **Error Handling:** Includes `try-except` for file not found and basic checks for missing columns, which is good practice.
- **Clarity and Comments:** The code is well-structured with comments explaining each step, making it readable.
- **Handling NaNs in Target:** The explicit handling of NaNs in the target variable `y` by dropping corresponding rows is a good addition for robustness and often necessary in real-world scenarios, although not explicitly mentioned in the prompt.

**Areas for Improvement (minor):**
- **Pipeline Integration:** While the preprocessing is well-done using `ColumnTransformer` and `Pipeline`, applying the preprocessing separately then training the model (`X_train_processed = preprocessor.fit_transform(X_train)`, `rf_model.fit(X_train_processed, y_train)`) means the entire process (preprocessing + classification) is not encapsulated in one single `sklearn.pipeline.Pipeline` object. The ground truth code demonstrates a more elegant way to combine `preprocessor` and `classifier` into a single `Pipeline`. This simplifies deployment and ensures the exact same preprocessing steps are applied consistently (fit on train, transform on test) without manual intervention. While the generated code logically separates this correctly, it could be slightly more idiomatic `sklearn` by integrating it into a single pipeline.
- **Redundant `numpy` import:** `import numpy as np` is present but `np` is not used anywhere in the code. It's a minor redundancy.

Overall, this is a very high-quality solution that demonstrates a strong understanding of `sklearn` and data preprocessing best practices. The one minor point is the less-than-fully integrated pipeline for the entire model, which is a common best practice in `sklearn` workflows.",gemini-2.5-pro-preview,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# 1) Calculate the Pearson correla...",9,"The generated code correctly defines the DataFrame, calculates the specified Pearson correlation coefficients, prints them, and correctly identifies the feature with the stronger absolute correlation with ""PerformanceScore"". The code is well-structured and follows the task requirements closely.

The only minor point for improvement is in the printing of the stronger correlation. While it correctly identifies the feature, the ground truth provides more detailed output showing the absolute correlation values for comparison, which is helpful for the user. Another minor point is that the ground truth explicitly formatted the output correlations to 4 decimal places, which makes the output cleaner and aligns with typical statistical reporting. The generated code also formats to 4 decimal places but the output string is slightly less descriptive. The commented-out alternative correlation calculation is also good, but not part of the core requirement. Overall, a very good solution.",gemini-2.5-pro-preview,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test
from lifelines.datasets import load_rossi # For example of interaction formula

from scipy import stats
import statsmodels.api as sm
import statsmodels.formula.api as smf

# For Propensity Score Matching
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeig...",7,"The generated code attempts to address all requirements of the prompt and demonstrates a good understanding of the requested statistical methods. The structure is modular with functions for each section, which is good for readability and maintainability. It includes correct imports and visualisations.

However, there are several ""weaknesses"" that prevent a higher score:
1.  **Fundamental Misinterpretation of ""Time-to-Event"" for Titanic Data**: Both the generated code and the ground truth *correctly* interpret the ""voyage as time-to-event"" as `T=1` and `E=1-Survived` (death as event). This simplification, while necessary given the data, fundamentally changes the nature of the survival analysis. When duration `T` is constant (always 1 for everyone), a Cox Proportional Hazards model essentially collapses into a logistic regression. The concept of ""hazard rate"" over time becomes trivial, and assumptions like ""proportional hazards"" are virtually meaningless or untestable in the traditional sense. The generated code acknowledges this briefly for PH test, but it still proceeds as if it's typical survival analysis. This is a subtle point, but crucial for true ""survival analysis"" rigor. The prompt *asked* for this interpretation, so the code faithfully implements it. However, a truly expert code might flag this as a data limitation rather than just implementing it.

2.  **Implementation of Kaplan-Meier (KM)**: While the KM plots are generated, given `Duration=1` for all, the KM curves become degenerate steps. The ""median survival time"" will always be 1 (or `NaN` if less than 50% died). This is a direct consequence of the `T=1` assumption. The generated code correctly implements KM and log-rank tests, but the output `kmf.median_survival_time_` will be misleading given the `T=1` context.

3.  **Cox Proportional Hazards Model & Diagnostics**:
    *   **Column Naming**: The `.str.replace('[^A-Za-z0-9_]+', '', regex=True)` and subsequent `.rename` calls for `Pclass20` and `Pclass30` suggest some issues with the initial `pd.get_dummies` naming and compatibility with patsy formulas. While fixes are attempted, it shows a slight lack of robustness in data preparation or a mismatch in expectations between `get_dummies` output and formula parsing. This can be streamlined.
    *   **Penalization**: `CoxPHFitter(penalizer=0.01)` is used without specific justification. While not incorrect, it wasn't requested, and it complicates the Likelihood Ratio Test comparison. The code then correctly shows how to do LRT for unpenalized models, which is good, but indicates the use of `penalizer` might not be fully thought through in this context.
    *   **Proportional Hazards Assumption**: The code correctly notes the difficulty of `check_assumptions` with `Duration=1`. This is a good self-awareness, but it highlights the limitation of the approach forced by the prompt.

4.  **Propensity Score Matching (PSM)**:
    *   **Covariates**: The choice of covariates for PSM is reasonable, but the `Embarked_Q_True` / `_STrue` naming issue arises again.
    *   **Matching Algorithm**: `NearestNeighbors` is appropriate. The manual concatenation after `kneighbors` could be more robustly handled (e.g., using `matched_indices` directly to reconstruct the matched data frame).
    *   **SMD Calculation**: The SMD calculation `(mean_treated - mean_control) / std_pooled` needs `std_pooled` to be a pooled *estimate* of standard deviation, often computed as `np.sqrt(((mean_treated**2 + mean_control**2)/2))`, rather than just `(std_treated + std_control) / 2`. The current formula `np.sqrt((treated_units[col].var() + control_units[col].var()) / 2)` or `np.sqrt((matched_pairs_treated[col].var() + matched_pairs_control[col].var()) / 2)` is the correct formula for the denominator of the pooled standard deviation when sample sizes are equal, but the actual implementation has some minor inconsistencies in variable names.
    *   **Treatment Effect Estimation**: The `ATT` calculation is basic (mean comparison). While adequate, more advanced PSM analysis (e.g., inverse probability weighting) could extend this.

5.  **Bootstrap Resampling**:
    *   The `resample` function is used correctly.
    *   The `try-except` block for `cph_boot.fit` is good for robustness in bootstrapping.
    *   The interpretation of bootstrapped KM results at `t=1` should again highlight that `Duration=1` makes the KM curve degenerate.

6.  **Bayesian Analysis**:
    *   Uses `pymc` (PyMC 4+) which is the modern version, whereas the prompt explicitly mentioned `pymc3`. This is a minor discrepancy of version, but `pymc` is the current standard.
    *   The model (Bayesian Logistic Regression) is well-defined and appropriate for predicting `Survived` (which is `1 - EventObserved`). Priors are chosen reasonably.
    *   `az.summary` and plotting are correctly used.

7.  **Model Validation and Sensitivity Analysis**:
    *   Implementations for age imputation and Fare outlier handling are good.
    *   The conceptual discussions are valuable, but specific code examples for Bayesian prior sensitivity (perhaps re-running with different sigmas) would strengthen this section.
    *   The handling of column names during reprocessing steps for sensitivity analysis is a bit repetitive and could be encapsulated in a helper function.

Overall, the code is very strong in attempting to fulfill all requirements and demonstrates good coding practices (modularity, comments, visualizations). The main detraction is the implicit limitations stemming from the `T=1` assumption for survival analysis on the Titanic data, which impacts the meaningfulness of some results, even though the prompt implicitly endorses this simplification by asking for it. There are also several minor issues with variable consistency, column name handling in one-hot encoding across different sections leading to manual renames, and minor inconsistencies in SMD calculations as noted.",gemini-2.5-pro-preview,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
import numpy as np

# Create the DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
        ...",10,"The generated code is excellent and correctly addresses all aspects of the task.

**Correctness:**
*   Successfully creates the DataFrame as specified.
*   Correctly calculates and prints the Pearson correlation coefficient.
*   Generates a scatter plot with 'Attendance_Rate' on the x-axis, 'Final_Score' on the y-axis, and colors the points based on 'Subject'. The plot includes the required title, axis labels, and a legend.
*   Correctly implements the logic to assign 'Overall_Grade' based on 'Final_Score' using `np.select`. The implicit conditions in the generated code for grades 'B' and 'C' are correct due to the order of operations in `np.select`.
*   Accurately counts and prints the distribution of students in each grade category, using `reindex` and `sort_index` to ensure a consistent and complete output even if a grade category has zero students.

**Code Quality:**
*   Properly imports all necessary libraries (`pandas`, `matplotlib.pyplot`, `seaborn`, `scipy.stats`, `numpy`).
*   The code is well-structured and easy to read, with comments indicating different sections of the task.
*   Uses `f-strings` for clear output formatting.
*   The plot uses `seaborn.scatterplot` which is appropriate and provides good defaults for aesthetics.
*   Good use of `plt.figure(figsize=...)` for plot sizing and `plt.show()` to display the plot.
*   The `reindex(['A', 'B', 'C', 'D'], fill_value=0).sort_index()` for grade counts is a robust and elegant solution.

**Completeness:**
*   All requirements from the task description are met: DataFrame creation, correlation calculation and printing, scatter plot generation with specific styling and labeling, grade assignment, and grade distribution printing.
*   The plot is generated and displayed without issues.

The code is robust, clean, and directly answers all parts of the prompt.",gemini-2.5-pro-preview,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05',
                          '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']),
    'Ticker': ['TECHCO']*10,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0],
    'Low': [...",10,"The generated code is excellent. It correctly performs all requested operations: DataFrame creation, daily return calculation, SMA calculations for both 3 and 5 days, plotting with all required details (title, labels, legend), and accurately identifies and prints the bullish crossover dates. The code is well-structured, uses appropriate pandas and matplotlib functions, and includes comments for clarity. The logic for identifying crossovers is precise. The plot is correctly generated and displayed.",gemini-2.5-pro-preview,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001',...",9,"The generated code is highly correct and complete, addressing all parts of the task effectively.

**Strong points:**
*   **Correctness:** All calculations (total sales per product, average order value, repeat customers) are accurate. The DataFrame creation and plotting function as expected.
*   **Completeness:** All four sub-tasks (DataFrame creation, product revenue calculation and visualization, average order value, identifying repeat customers) are fully addressed.
*   **Code Quality:** Imports are correct. The code is well-structured and uses meaningful variable names. Comments are helpful, guiding the reader through each step.
*   **Plotting:** The horizontal bar chart is correctly generated with appropriate labels and title. The use of `plt.gca().invert_yaxis()` to ensure the largest bar is at the top is a nice detail, though the ground truth's use of `sort_values(ascending=False)` followed by standard plotting also achieves this implicitly for `barh` if the data is already sorted.
*   **Edge Case Handling:** The average order value calculation includes a check for `num_unique_orders > 0`, which is good practice to prevent division by zero errors.

**Minor Improvements (to reach a perfect 10):**
*   **Input Data Printing (Minor):** The ground truth prints the initial DataFrame, which can be useful for debugging and checking the starting point. This is a very minor omission.
*   **Clarity in Repeat Customer Output:** While the generated code prints the `repeat_customers` Series directly, the ground truth formats it more explicitly with a header and iterated print statements, which can be slightly clearer for human readability. This is a stylistic preference.
*   **Consistency in `Revenue` Column Name:** The generated code introduces `Total_Revenue_Per_Item` for the individual transaction revenue, which is accurate, but then uses `product_revenue` for the grouped sum. The ground truth consistently uses `Revenue` for the column representing the individual transaction revenue, then `product_revenue` for the grouped sum, which might be marginally clearer in terms of keeping the base calculation column name simple. This is a very minor point of style.",gemini-2.5-pro-preview,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd
import numpy as np

# 1. Define the initial DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 2. Create a new numerical feature ""DeviceType_Numeric""
device_mapping = ...",9,"The generated code is highly correct and complete. It successfully defines the DataFrame, creates the `DeviceType_Numeric` feature, and calculates `PagesPerMinute` while correctly handling division by zero. The feature selection and printing of the head are also correctly implemented.

A minor difference from the ground truth is in the handling of division by zero:
- Ground Truth: `replace([np.inf, -np.inf], np.nan, inplace=True)` then `fillna(0, inplace=True)`. This ensures that `np.inf`/`-np.inf` first become `NaN` and then `0` along with any `NaN` that might have resulted from `0/0`.
- Generated Code: `replace([np.inf, -np.inf], 0, inplace=True)` then `fillna(0, inplace=True)`. This directly replaces `np.inf`/`-np.inf` with `0`. The subsequent `fillna(0)` handles `NaN` (which would primarily come from `0/0` in this case).

Both approaches achieve the desired outcome of replacing `NaN` or `Inf` with `0`. The generated code's approach is slightly more direct for infinities. This is a very minor stylistic difference and doesn't impact correctness for the given task. The code is well-structured and uses appropriate pandas operations.",gemini-2.5-pro-preview,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26, 45, 28]
}
d...",10,"The generated code correctly defines the DataFrame, accurately calculates the mean and standard deviation for ""TemperatureCelsius"", precisely identifies outliers based on the 1.5 standard deviation threshold, and correctly prints the specified details for each outlier. The code is well-structured, easy to understand, and addresses all requirements of the task. It handles the case where no outliers are found gracefully. The use of `iterrows()` to loop through identified outliers for printing is appropriate.",gemini-2.5-pro-preview,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [1...",10,"The generated code correctly performs all requested tasks. It accurately defines the DataFrame, calculates the `EngagementScore`, and then correctly identifies the `PostType` with the highest average `EngagementScore` for each `Platform`. The printing format is also correct. The approach of using `groupby().idxmax()` is a clear and efficient way to achieve the desired result, providing a robust solution. The code is well-structured and uses appropriate pandas operations. The small difference in printing format (`.1f` vs `.2f`) is a minor stylistic choice and doesn't affect correctness.",gemini-2.5-pro-preview,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""C...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the LengthOfStay, computes the average LengthOfStay and patient count per diagnosis, prints the required information in a clear format, and accurately identifies the diagnosis with the longest average stay. The code is well-structured and uses appropriate pandas functions. It fully addresses all requirements of the task. The addition of the `if not diagnosis_summary.empty:` check is a good practice for robustness, although not strictly necessary for this specific dataset.",gemini-2.5-pro-preview,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

# Define the movie ratings data
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
  ...",10,"The generated code correctly defines the DataFrame, calculates the ""WeightedRating"" precisely as specified, adds it as a new column, and accurately identifies and prints the title and weighted rating of the movie with the highest ""WeightedRating"". The code is well-structured, easy to read, and addresses all requirements of the task. The comments clearly delineate each step and its purpose. It also correctly handles the vectorized calculation for ""WeightedRating"" and uses `idxmax()` for efficient retrieval of the highest value. The use of f-strings for printing the output is also good practice. The only minor difference is the precision for the output, but `.4f` is perfectly acceptable and even slightly better than `.6f` for readability for this data.",gemini-2.5-pro-preview,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

# Define the initial DataFrame
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000...",10,"The generated code correctly defines the initial DataFrame as per the task requirements. It accurately calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"" using `groupby().agg()` and creates a new DataFrame with these averages. The use of named aggregations (`AveragePriceUSD=('PriceUSD', 'mean')`) is a good practice for clear output column names. The inclusion of `.reset_index()` explicitly converts the ""Brand"" from an index back into a column, which is a sensible choice for presentation and aligns well with the task's implicit request for a ""DataFrame showing these averages grouped by 'Brand'"". Finally, it correctly prints the resulting grouped DataFrame. The code is clean, well-structured, and fully addresses all requirements of the prompt.",gemini-2.5-pro-preview,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 8...",10,"The generated code is excellent. It correctly defines the DataFrame, adds the ""TotalStats"" column, accurately identifies the Pokemon with the highest total stats, and correctly calculates the average speed for each ""Type1"". The code is well-structured, uses appropriate pandas functions, and includes helpful comments. It fully addresses all requirements of the task. The inclusion of an alternative method for calculating TotalStats is a nice touch, demonstrating understanding of different pandas approaches.",gemini-2.5-pro-preview,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.decomposition impor...",9,"The generated code is highly comprehensive and addresses almost all aspects of the complex task.

**Strong Points:**
- **Advanced Feature Engineering:** Implements social network analysis (FamilySize, TicketGroupSize, FarePerPerson), socioeconomic indicators (Title, Deck, HasCabin), and text-based features (NameLength, TicketPrefix, TicketNumberLength). This goes beyond basic feature engineering.
- **Multiple Clustering Algorithms:** Successfully applies K-means, Hierarchical, DBSCAN, and Gaussian Mixture Models.
- **Proper Preprocessing and Scaling:** Uses `StandardScaler` and `OneHotEncoder` within `ColumnTransformer` and `Pipeline` for robust preprocessing. Handles missing values.
- **Optimal Cluster Determination:** Employs multiple validation metrics (silhouette, Calinski-Harabasz, Davies-Bouldin, inertia/BIC/AIC) and visualizes them for analysis. Uses `NearestNeighbors` for DBSCAN `eps` estimation.
- **Dimensionality Reduction:** Incorporates PCA, t-SNE, and UMAP for visualization and uses PCA-reduced data for clustering, which is a good practice.
- **Comprehensive Analysis and Visualizations:** Creates detailed cluster profiles, survival rate plots, feature distribution plots (violin plots, countplots), and heatmaps of scaled feature means. Visualizes clusters in reduced dimensions (PCA, t-SNE, UMAP).
- **Cluster Stability Analysis:** Implements a simplified but effective cluster stability check using Adjusted Rand Index for multiple K-Means runs.
- **Consensus Clustering:** Provides a good basic implementation of consensus clustering using a co-association matrix, which is a rare and advanced requirement.
- **Code Quality:** Good structure, proper imports, use of `RANDOM_STATE` for reproducibility, and suppression of warnings.

**Areas for Minor Improvement (to reach a perfect 10):**
- **Optimal K Determination Clarity/Automation:** While it plots the metrics, the actual selection of `optimal_k` for K-Means and Hierarchical Clustering is hardcoded (e.g., `optimal_k_kmeans = 4`) immediately after showing the plots, without explicitly stating the visual justification or a more programmatic interpretation of the metrics (e.g., finding the maximum silhouette score or the ""elbow"" point more robustly in code). The ground truth code also hardcodes `optimal_k = 4` for demonstration, so this is a minor point. However, the generated code *does* try to find `np.argmax(silhouette_scores_kmeans)` which is better.
- **DBSCAN `eps` Selection:** The `eps_dbscan = 1.5` remains a placeholder that requires manual tuning based on the k-distance plot. While the code correctly attempts to visualize it, a perfect solution might offer alternative programmatic ways to suggest `eps` or explicitly note the need for manual inspection more prominently.
- **Feature Name Derivation for `X_processed_df`:** The `try-except` block for `get_feature_names_out` is good, but the manual construction of `feature_names_out` in the `except` block for older sklearn versions needs to correctly handle the order and types of features once OneHotEncoder is applied, especially if some numerical features become boolean if they had sparse data. The current implementation in `except` block might mix up feature ordering slightly. The ground truth avoided this by just calling `get_feature_names_out` directly. However, the use of `try-except` is a good attempt at robustness.
- **Small inconsistency in `analyze_clusters`:** `df_analysis_orig_scale` is created by copying `df_original[features_for_clustering]` which means it might miss other automatically generated features like `FamilyGroup`, `FarePerPerson` which are engineered earlier but not necessarily in `features_for_clustering` if selected for clustering. However, the code passes `df_featured` to `analyze_clusters`, which includes engineered features, so this is handled correctly. `features_for_clustering` only defines what goes into `X_processed`. The analysis uses `df_featured` which is good.

Overall, this is an excellent solution, demonstrating a deep understanding of unsupervised learning techniques and data science best practices.",gemini-2.5-pro-preview,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dyst...",10,"The generated code is correct, complete, and of good quality. It precisely follows all instructions: correctly defines the DataFrame, accurately calculates and prints the count of books and average price per author, and correctly identifies and prints the author with the most books in stock. The use of `groupby().agg()` for the statistics and `idxmax()` for finding the author with the most stock demonstrates efficient and idiomatic pandas usage. The output formatting is clear and readable.",gemini-2.5-pro-preview,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocessing
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, learning_curve, validation_curve, cross_val_score
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.impute import IterativeImputer
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin

# Models
from sklearn.linear_model i...",8,"The generated code demonstrates a strong understanding of the task requirements and implements most of the advanced features requested. It successfully covers comprehensive data preprocessing (multiple imputation, outlier handling, polynomial features), builds and optimizes multiple base models, and implements ensemble methods (voting, stacking). The evaluation also includes learning curves, validation curves, SHAP for interpretability, statistical significance testing, and model calibration with plots.

Here's a breakdown:

**Strong Points:**
- **Comprehensive Preprocessing:** Correctly implements `IterativeImputer`, a custom `OutlierCapper`, and `PolynomialFeatures` within a `Pipeline`. The `OutlierCapper` is well-implemented as a custom transformer.
- **Model Optimization:** Utilizes `GridSearchCV` for base model optimization, which is a key requirement. Defines relevant parameter grids.
- **Ensemble Methods:** Implements both `VotingClassifier` (hard and soft) and `StackingClassifier` effectively.
- **Advanced Evaluation:**
    - Includes `learning_curve` and `validation_curve` plots with custom helper functions.
    - Uses `shap` for model interpretability with correct handling for tree-based models and a fallback to KernelExplainer (though commented out for performance reasons, is a thoughtful addition).
    - Performs statistical significance testing using paired t-tests on CV scores.
    - Implements model calibration using `CalibratedClassifierCV` with both isotonic and sigmoid methods, and visualizes `CalibrationDisplay`.
    - Includes bootstrap sampling for confidence intervals, a good advanced evaluation technique.
- **Code Quality:** Generally well-structured with clear sections, comments, and proper imports. Uses `RANDOM_STATE` for reproducibility and `N_JOBS` for efficiency. Helper functions for plotting learning/validation curves are a nice touch.
- **Robustness:** Handles `FileNotFoundError` for data loading and includes `try-except` blocks for model training and SHAP analysis.

**Areas for Improvement / Minor Issues:**

1.  **Blending:** The ground truth includes a custom blending implementation, which is often distinct from stacking. The generated code did not explicitly implement blending as a separate technique (it implicitly covers some of its aspects through stacking or could be mistaken for it). The task specifically requested ""stacking, voting, and blending techniques.""
2.  **Dashboard:** While it prints a performance table and generates ROC curves, it doesn't create a ""comprehensive model comparison dashboard with performance metrics, feature importance, and prediction confidence intervals"" in a single, integrated visual format as suggested by the ground truth's final section. The ground truth creates a `dashboard_df` and plots multiple metrics with CIs. The generated code has the pieces (bootstrap CIs, performance metrics, SHAP for feature importance), but the integration into one ""dashboard"" is less explicit/visual.
3.  **Bootstrap CIs:** The ground truth calculates bootstrap CIs for all models and for multiple metrics (ROC AUC, F1-Score). The generated code calculates for a *single* `bootstrap_model_name` for ROC AUC. This is a partial implementation of the requirement.
4.  **`OutlierCapper` `get_feature_names_out`:** The implementation of `get_feature_names_out` in the custom `OutlierCapper` is technically correct but the `input_features` argument handling could be slightly more robust for general `ColumnTransformer` usage (it works here because it sits inside a numeric pipeline). Ground truth simply relies on `np.clip` on the entire DataFrame, which implies column names are kept if the input is a DataFrame, but the generated code's numerical array processing then requires explicit re-creation of `pd.DataFrame` for SHAP to maintain column names.
5.  **`shap.sample` for KernelExplainer:** The `print(explainer.expected_value)` and SHAP summary plot `if` condition using `explainer.expected_value is not None` suggests a potential issue for KernelExplainer in some SHAP versions where `expected_value` might be `None` or not defined consistently. It's a minor detail but can lead to `if` conditions not taking the intended branch for KernelExplainer. The prompt implies `shal.summary_plot` with dataframes and the ground truth handles this via converting arrays back to dataframes.
6.  **`use_label_encoder=False` for XGBoost:** The generated code includes `use_label_encoder=False` which is good for newer XGBoost versions, but the `eval_metric='logloss'` could lead to a `UserWarning` if not explicitly set for `multi:softprob` or `binary:logistic` which is already done. The ground truth sets `objective` explicitly.
7.  **`initial_strategy='mean'` in IterativeImputer:** While not critical, `initial_strategy='mean'` can be replaced with `initial_strategy='random'` or more robust options if the data has specific properties. The choice is fine, but it's worth noting.

Overall, the generated code is highly competent and addresses almost all advanced requirements of the prompt. The missing explicit blending and the less comprehensive dashboard/bootstrap CIs are the main distinctions from a perfect score.",gemini-2.5-pro-preview,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Scikit-learn imports
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.experimental import enable_iterative_imputer # Required for IterativeImputer
from sklearn.impute import KNNImputer, IterativeImputer
from sklearn.linear_model import LogisticRegression, BayesianRidge
from s...",8,"The generated code demonstrates a strong understanding of the requested advanced statistical modeling and anomaly detection techniques. It successfully implements multiple imputation methods, builds various statistical models (logistic regression, GAM, Bayesian logistic regression), performs advanced hypothesis testing, and applies anomaly detection algorithms. Dimensionality reduction (PCA and Factor Analysis) and model diagnostics are also covered.

**Strengths:**
- **Comprehensive Imputation:** Explores MICE (IterativeImputer with BayesianRidge and DecisionTreeRegressor) and KNN imputation, and includes a visual comparison, which is excellent.
- **Diverse Modeling Techniques:** Successfully implements regularized logistic regression, PyGAM, and makes a good attempt at PyMC3 Bayesian logistic regression.
- **Advanced Hypothesis Testing:** Covers MANOVA, permutation tests, and bootstrap confidence intervals, including a more complex example of bootstrapping a regression coefficient.
- **Anomaly Detection:** Implements Isolation Forest, LOF, and One-Class SVM with clear explanations and visualization.
- **Dimensionality Reduction:** Correctly applies and visualizes PCA and Factor Analysis, including factor loadings.
- **Robust Methods:** Explicitly mentions and uses `RobustScaler` and discusses `sm.RLM`.
- **Model Diagnostics:** Implements residual analysis, influence measures (Cook's Distance, Leverage), and a custom Hosmer-Lemeshow test, along with a diagnostic dashboard.
- **Proper Imports and Structure:** Code is well-structured with clear headings and comments.
- **Reproducibility:** Uses `random_state` where appropriate.

**Areas for Improvement/Minor Issues:**
- **PyMC3 Implementation:** While the attempt at PyMC3 is commendable, the posterior predictive part for the test set is a bit manual and less idiomatic PyMC3 than it could be (using `pm.sample_posterior_predictive` with `pm.Data` for `X_test_scaled_df`). The error handling `try-except` block for PyMC3 is good, but it makes the code run even if PyMC3 isn't working as expected. The usage of `theano.tensor as tt` is a giveaway that it's using an older version of PyMC3 (version 3.x), which often relied on Theano for its backend. PyMC (since v4) has moved to Aesara/PyTensor, so this part of the code might not be forward-compatible with newer PyMC versions without modification. The instruction specified `pymc3` but it's important to note the version dependency.
- **GAM Plots:** The `subplot` indexing for GAM partial dependence plots is a bit off (`i-1` and `i`) and might cause issues for all features if not carefully handled, as `i-1` for the first feature might lead to an invalid index.
- **Bootstrap CI for Coefficient:** The manual bootstrap loop for the logistic regression coefficient is correct for demonstration but could be computationally intensive for a large number of bootstraps. It also has a `try-except` for model convergence, which is good practice.
- **Manova error handling:** The `try-except` for MANOVA is quite generic. A more specific error message or suggestion for common MANOVA issues (e.g., singular matrices, group sizes) would be helpful.
- **Robust Statistical Methods Section:** While RobustScaler is used and RLM is mentioned, it's not explicitly applied to the classification problem, as noted in the code. A small example of how robust methods might be applied to handle outliers in the dataset for a continuous variable (like the ground truth did for `ph`) would have made this section even stronger.
- **Q-Q plot for residuals:** While a Q-Q plot of deviance residuals is shown, it's important to note that deviance residuals from a GLM are not expected to be normally distributed, so interpreting its normality in the same way as OLS residuals can be misleading. The ground truth mentioned this, which was helpful.
- **Package Versions:** The task implied `pymc3`, so the use of Theano components is understandable if that was the installed version during testing. However, the rapidly evolving landscape of Python packages (especially PyMC) means specific versions can dictate exact syntax.",gemini-2.5-pro-preview,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.manifold import TSNE
import umap # umap-learn package
from sklearn.metrics import silhouette_s...",9,"The generated code is very good and largely addresses all requirements of the task.

**Strong Points:**
- **Comprehensive Feature Engineering:** The code successfully implements various water quality indices, chemical balance ratios, and contamination risk scores. It even includes a thoughtful `pH_ideal_score` and a simplified WQI.
- **Multiple Clustering Algorithms:** It correctly applies K-means, Hierarchical, DBSCAN, Gaussian Mixture Models, and Spectral Clustering.
- **Thorough Optimal Cluster Determination:** It uses multiple metrics (Elbow, Silhouette, Davies-Bouldin, Calinski-Harabasz) with plotting and even includes a simplified Gap Statistic.
- **Robust Dimensionality Reduction:** PCA, t-SNE, and UMAP are all implemented correctly for visualization. Factor Analysis is attempted, and error handling is present.
- **Cluster Stability Analysis:** Bootstrap resampling with ARI for K-means is a good inclusion for stability.
- **Detailed Cluster Profiling:** It correctly calculates and presents mean profiles for each cluster, including potability rates.
- **Excellent Visualization Dashboard:** Generates 2D plots for all DR techniques, radar charts for cluster profiles, box plots for feature distribution per cluster, and even the contingency table for external validation. It wisely saves these plots as HTML/PNG files.
- **Cluster-based Anomaly Detection:** Implements multiple anomaly detection methods (distance to centroid for K-means, noise points for DBSCAN, log-likelihood for GMM) and visualizes K-means anomalies.
- **Good Code Quality:** Proper imports, clear variable names, and logical code flow. Error handling is present for `FileNotFoundError`, and `n_init='auto'` for KMeans is a good modern practice.
- **Domain Knowledge Integration:** The code explicitly prints interpretations related to WHO guidelines, which is excellent for external validation.

**Areas for Minor Improvement:**
- **Gap Statistic Implementation:** While present, the simplified Gap Statistic calculation may not be perfectly robust (e.g., how the reference data is generated and the optimal K determined). The plotting of the `valid_k_range_metrics` for `silhouette_scores_kmeans` etc. should be adjusted to be `k_range[1:]` (or equivalent) because silhouette score is undefined for k=1.
- **Spectral Clustering `n_neighbors`:** `n_neighbors=max(10, optimal_k + 1)` is a reasonable heuristic.
- **Factor Analysis Error:** The `FactorAnalysis` block has a potential issue if `X_scaled.shape[0] < fa_n_components`, not `X_scaled.shape[0] > fa_n_components`. It's `n_samples > n_components`, so the `if` condition for `X_scaled.shape[0] > fa_n_components` should be `X_scaled.shape[0] >= fa_n_components` (if `n_components` can be up to `X_scaled.shape[0]`). However, if `fa_n_components` is limited to `min(10, X_scaled.shape[1])`, the check `X_scaled.shape[0] > fa_n_components` is only relevant if `X_scaled.shape[0]` (number of samples) is less than 10. The main issue is `n_components` generally should be less than or equal to `n_features`. The error message is `ValueError: n_components must be <= n_features` or occasionally `n_samples must be >= max(n_components, n_features)` for FA in some edge cases. The current error handling for FA is adequate for general use, but the condition could be more precise/robust.
- **UMAP `n_neighbors` vs. `len(X_scaled)-2`**: `min(15, len(X_scaled)-2)` is a useful heuristic for small datasets. `n_neighbors` must be less than `n_samples`. The error handling for UMAP `ValueError` is good.
- **Hover data for Plotly:** For `hover_data=[hover_df_subset]`, `plotly` expects a list of column names (strings) or a dictionary mapping column names to corresponding series. Passing a DataFrame directly within the list can lead to unexpected behavior or an empty display of hover data. It should likely be `hover_data=hover_df_subset.columns.tolist()`.

Overall, this is a very strong submission that fulfills nearly all aspects of a complex task. The minor issues are primarily refinements and common pitfalls, not critical failures.",gemini-2.5-pro-preview,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import linregress
import numpy as np

def analyze_wine_correlations():
    # 1) Load the dataset
    try:
        df = pd.read_csv('winequality-red.csv')
    except FileNotFoundError:
        print(""Error: 'winequality-red.csv' not found in the current directory."")
        return

    # Calculate the correlation matrix
    correlation_matrix = df.corr()

    # Get correlations with 'quality'
    ...",9,"The generated code correctly addresses all aspects of the task, including calculating correlations, identifying top positive and negative features, generating a heatmap, and creating scatter plots with linear regression lines.

**Strong points:**
- All required libraries are imported correctly.
- The code calculates the correlation matrix and handles the target 'quality' column appropriately.
- It correctly identifies and prints the top 3 positive and top 3 negative correlations.
- The heatmap is generated as specified, showing correlations and using a `coolwarm` colormap.
- Scatter plots with linear regression lines using `scipy.stats.linregress` are correctly implemented for the most positive and most negative features. R-squared values are correctly displayed on the plots.
- Includes error handling for file not found and missing 'quality' column.
- Uses `pd.api.types.is_numeric_dtype` for robustness in plotting, which is good practice (though might be redundant for this specific dataset where all features are numeric).
- The code is well-structured within a function and includes a `if __name__ == '__main__':` block.
- Good use of `tight_layout()` for plots.

**Areas for minor improvement (not critical enough to deduct points significantly):**
- The `top_3_negative = quality_correlations.tail(3).sort_values(ascending=True)` line is a bit convoluted. Since `quality_correlations` is already sorted descending, `quality_correlations.tail(3)` will give the three smallest (most negative) correlations. Sorting them again `ascending=True` on these three values will put the *least* negative at the top of that specific printed list, not necessarily the most negative. However, `most_negative_feature = quality_correlations.index[-1]` correctly identifies the single most negative one from the original series. The print formatting might be slightly confusing but the values are correct. The ground truth correctly just uses `.tail(3)` from the *descending* sorted list.
- Printing the most positive/negative features uses `iloc` on `quality_correlations.index[0]` and `quality_correlations.index[-1]` which is slightly less direct than using `quality_correlations.head(1)` and `quality_correlations.tail(1)` or similar. However, it achieves the correct result.
- The ground truth code added some basic dataset info prints (`.head()`, `.info()`, `.describe()`) which are helpful for initial exploration, but not explicitly requested by the prompt. The generated code focuses purely on the requested analysis steps.",gemini-2.5-pro-preview,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
from scipy import stats

def perform_hypothesis_tests():
    ALPHA = 0.05

    # 1) Load the dataset and clean any missing values
    try:
        df = pd.read_csv('video_games_sales.csv')
    except FileNotFoundError:
        print(""Error: 'video_games_sales.csv' not found in the current directory."")
        return

    # Drop rows with missing values in columns critical for the analyses
    critical_cols = ['Year_of_Release', 'Publisher', 'Genre', 'Global_Sales', ...",8,"The generated code is well-structured and addresses most of the requirements. It correctly implements all the requested statistical tests (t-test, chi-square, Mann-Whitney U) and includes proper data cleaning for the relevant columns.

Strong points:
- Proper error handling for `FileNotFoundError`.
- Comprehensive cleaning of critical columns.
- Uses `alternative='greater'` for the Nintendo t-test as specified in the prompt (""significantly higher"").
- Uses `equal_var=False` (Welch's t-test) which is good practice.
- Correctly identifies and filters for top 5 genres and platforms for the chi-square test.
- Correctly applies `alternative='two-sided'` for tests checking for ""significantly different"" sales.
- Provides clear interpretations of the results based on p-values and the defined `ALPHA`.
- Uses a `try-except` block for the Mann-Whitney U test, which is robust.

Areas for improvement to reach a higher score:
1.  **Mann-Whitney U Test Grouping for ""Before 2000""**: The prompt stated ""before 2000"" and ""after 2000"". The ground truth used `<= 2000` for ""before 2000"" and `> 2000` for ""after 2000"". The generated code uses `> 2000` for ""after 2000"" and `< 2000` for ""before 2000"", which excludes games released *exactly* in 2000 from both groups. While a minor difference, explicitly defining the year 2000 inclusion/exclusion is important. The ground truth's definition (`<=2000` vs `>2000`) implicitly covers all games.
2.  **Explicit Null/Alternative Hypotheses**: The ground truth explicitly states the null and alternative hypotheses for each test, which makes the output more formal and easier to understand for someone reviewing the statistical analysis. The generated code only offers the interpretation.
3.  **Detailed print statements (intermediate results/statistics)**: While the p-value and interpretation are present, including the t-statistic/chi^2 statistic / U-statistic values, and degrees of freedom for chi-square (as in the ground truth) would make the output more complete from a statistical reporting perspective.
4.  **Cleaning `Year_of_Release`**: The ground truth first drops `NaN`s from `Global_Sales` and then from `Year_of_Release`, before converting `Year_of_Release` to int. The generated code drops `NaN`s from all critical columns including `Year_of_Release` in one go, then converts. Both are acceptable, but the ground truth's sequencing is slightly more explicit in terms of the order of operations for `Year_of_Release` cleaning and conversion. The current implementation is fine, but the order of `dropna` and `astype(int)` should be considered.
5.  **Chi-square warning for Expected Frequencies**: The generated code correctly includes a warning if expected frequencies are less than 5, which is an excellent addition for the validity of the chi-square test.

Overall, it's a very good attempt that successfully completes the core tasks. The suggested improvements are mostly about enhancing the statistical reporting and robustness rather than fixing critical errors.",gemini-2.5-pro-preview,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import numpy as np

def build_classification_model():
    # 1) Load the dataset
    try:
        df = pd.read_csv('video_games_sales.csv')
    except FileNotFoundError:
        print(""Error: 'video_games_sales.csv' n...",7,"The generated code correctly addresses most of the requirements and demonstrates a good understanding of the task. However, there are several areas where it deviates from the ground truth or could be improved for better robustness and adherence to the prompt's nuances.

**Strengths:**
- Correctly loads the dataset and handles `FileNotFoundError`.
- Correctly creates the 'High_Sales' target variable.
- Correctly uses Label Encoding for categorical features.
- Correctly creates the 'Total_Regional_Sales' and 'Years_Since_Release' features.
- Correctly splits the data into training and testing sets with a proper 80-20 split and stratification.
- Trains a Random Forest Classifier with 100 estimators.
- Evaluates the model using accuracy, precision, recall, F1-score, and confusion matrix.
- Displays feature importance correctly.
- Includes necessary imports.

**Areas for Improvement/Deviations from Ground Truth:**

1.  **Missing Value Handling:**
    *   **Ground Truth:** Explicitly drops NaNs for `Global_Sales`, `Year`, `Publisher`, and all regional sales columns (`NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`) because they are crucial for target creation and features.
    *   **Generated:** Drops NaNs for `Year`, `Genre`, `Publisher`. Then specifically tries to `pd.to_numeric` sales columns and drops NaNs from them. This is less explicit and potentially less robust than the ground truth's direct `dropna` on all relevant sales columns at once. The generated code's initial `df.dropna(subset=['Year', 'Genre', 'Publisher'], inplace=True)` is also slightly different â€“ `Genre` was not explicitly mentioned in the ground truth's critical `dropna` list, though it's still a good practice to handle it if it were missing. The comment about ""Fill any other potential NaNs"" and the subsequent `df.dropna(subset=numeric_sales_cols, inplace=True)` is good, but the overall sequence is a bit less streamlined than the ground truthâ€™s comprehensive `dropna` for all relevant columns at once.

2.  **Feature Selection:**
    *   **Ground Truth:** Uses `Platform`, `Genre`, `Publisher` (encoded), `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`, `Total_Regional_Sales`, `Years_Since_Release` as features. This includes both the individual regional sales and their sum.
    *   **Generated:** Uses `Platform_Encoded`, `Genre_Encoded`, `Publisher_Encoded`, `Total_Regional_Sales`, `Years_Since_Release`. It explicitly notes: ""We avoid features that are part of the target (Global_Sales) or highly correlated intermediate steps (individual regional sales if Total_Regional_Sales is used)."" While this decision is reasonable in a general machine learning context to avoid multicollinearity, the prompt did not specify this. The ground truth, by including individual regional sales *and* their sum, aims for completeness as per the prompt's implicit requirement to use all relevant features, which includes the raw sales data. The prompt asked to ""prepare features by encoding categorical variables"" and ""create additional features like 'Total_Regional_Sales'"". It didn't imply *replacing* the individual sales. The ground truth interprets this better.

3.  **Label Encoding Column Naming:**
    *   **Ground Truth:** Overwrites the original columns with encoded values (e.g., `df['Platform'] = le.fit_transform(df[col])`). This is a common and often preferred approach when the original text column is no longer needed.
    *   **Generated:** Creates new columns ending with `_Encoded` (e.g., `df[col + '_Encoded'] = le.fit_transform(df[col])`). While not strictly incorrect, it adds more columns to the DataFrame and requires explicit management of which columns to use later. The ground truth's approach is more concise for this specific task.

4.  **Feature Imputation after `pd.to_numeric`:**
    *   **Generated:** `X = X.apply(pd.to_numeric, errors='coerce')` followed by `X.fillna(0, inplace=True)`. This step seems unnecessary if missing value handling was done properly at the start. If there are NaNs after initial drops, it implies an issue with the initial cleaning or data type conversion. `fillna(0)` can introduce bias, especially if NaNs represent missing information rather than actual zero values. The ground truth handles NaNs comprehensively at the beginning, ensuring no NaNs reach the model training phase from the relevant columns.

5.  **Random Forest `class_weight` parameter:**
    *   **Generated:** `class_weight='balanced_subsample'` is added. While this is a good practice for imbalanced datasets, it was not requested in the prompt. The ground truth code did not include it, adhering strictly to the prompt's requirements (""Train a Random Forest classifier with 100 estimators"").

6.  **Function Encapsulation:**
    *   **Generated:** Encapsulates the entire logic in `build_classification_model()`. While a good programming practice for larger projects, the prompt asked for ""Provide only the Python code without explanations"" and the ground truth directly executed the script, which is acceptable for a benchmarking task described this way. The specific structure of the ground truth code is also a reference.

7.  **Output Messages:** The print statements in the generated code are more concise and less informative regarding data processing steps compared to the ground truth (e.g., no print of shapes, missing values information, etc.). While not a functional requirement, the ground truth's verbose output helps trace data flow and quality.

Overall, the generated code fulfills the core requirements but has some stylistic and implementation differences, primarily in missing value handling and feature selection choices, that make it slightly less aligned with the ground truth's intent for this specific benchmark.",gemini-2.5-pro-preview,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.decomposition import PCA

# 1. Load dataset and handle initial missing values
df = pd.read_csv('video_games_sales.csv')

# Initial Missing Value Handling
df.dropna(subset=['Year', 'Publisher'], inplace=True)
df['Year'] = df['Year'].astype(int)

# Fill NaNs in sales columns with 0, as it likely means no sales. This...",7,"The generated code addresses most of the requirements of the task.

Strengths:
- **Comprehensive Feature Engineering**: All requested engineered features (`Sales_Ratio_NA_EU`, `Publisher_Avg_Sales`, `Genre_Market_Share`, `Platform_Popularity`, `Sales_Momentum`) are correctly implemented.
- **Correlation Analysis**: The correlation heatmap is generated correctly.
- **PCA Implementation**: PCA is applied, and the explained variance is mentioned.
- **Polynomial Features**: Polynomial features of degree 2 for sales columns are created.
- **Interaction Features**: Interaction features between 'Year' and 'Genre' are correctly generated using dummy variables.
- **Outlier Handling**: The IQR method is applied for outlier handling on relevant sales and engineered features.
- **Missing Value Handling**: The code attempts to handle missing values at various stages, including `Year`, `Publisher`, and sales columns.

Areas for Improvement:
- **Missing Value Strategy**: The initial missing value handling for `Year` and `Publisher` involves `dropna(subset=[...])`. The task specifies handling missing values. While dropping rows is a method, median/mode imputation (as in the ground truth) is generally preferred to preserve data, especially for `Year` and `Publisher` which are crucial for subsequent feature engineering. For `Genre`, `dropna` is not explicitly used, which can lead to issues if there are NaNs in `Genre` later.
- **Handling Division by Zero in `Sales_Ratio_NA_EU`**: The ground truth adds a small epsilon to the denominator (`EU_Sales + 1e-6`) to prevent `ZeroDivisionError`. The generated code uses `replace([np.inf, -np.inf], np.nan, inplace=True)` and then `fillna(median())`. While this works, directly preventing the `ZeroDivisionError` is more robust.
- **PCA Explained Variance Visualization**: The task requests explaining variance. The generated code prints a summary (""X components explain Y% variance""), but the ground truth also includes a clear plot of the cumulative explained variance, which is a better way to visualize this aspect.
- **Verbosity/Output**: The ground truth provides more informative print statements throughout the process, showing the state of the DataFrame after each major step and the head of the DataFrame with new features. The generated code is less verbose, making it harder to follow the step-by-step impact.
- **Robustness in Imputation**: Some imputation steps `(fillna(median()))` are done within the feature engineering steps, rather than a centralized, more robust initial imputation phase for all columns that critically need it upfront.
- **Clarity of `df_poly_new_terms` handling**: While the polynomial features are created, the method of `drop`ping original sales columns and `concat`enating only `df_poly_new_terms` is a bit over-engineered and can be simplified by just adding selected polynomial outputs directly. Also, the `correlations_with_global` analysis for polynomial features is good but not explicitly requested.
- **Warning Suppression**: The ground truth includes `warnings.filterwarnings('ignore')` which is good practice for benchmark scripts to keep terminal output clean, especially with older versions of libraries. The generated code omits this.",gemini-2.5-pro-preview,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

def perform_diabetes_analysis(csv_path='diabetes.csv', alpha=0.05):
    """"""
    Performs hypothesis testing and statistical analysis on the diabetes dataset.
    """"""
    print(f""Significance level alpha = {alpha}\n"")

    # 1) Load the dataset and clean zero values
    try:
        df = pd.read_csv(csv_path)
    except FileNotFoundError:
        print(f""Error: The file {csv_path} was not found in the current directory."")
 ...",9,"The generated code is very good, well-structured, and addresses almost all requirements accurately. The use of a function `perform_diabetes_analysis` with parameters for `csv_path` and `alpha` is excellent for modularity and reusability. The included dummy `diabetes.csv` creation is a thoughtful addition for reproducibility.

**Strengths:**
- **Code Structure:** Encapsulated in a function, good docstrings, clear print statements for each test.
- **Data Cleaning:** Correctly identifies columns to clean, replaces 0s with NaN, and then uses median imputation.
- **Hypothesis Tests:**
    - **T-test:** Correctly identifies diabetic/non-diabetic groups and performs the t-test. The use of `alternative='greater'` for the t-test (Glucose) is a good specific hypothesis, though the prompt asked for ""significantly higher"" which implies a one-sided test, the ground truth used a two-sided test with `equal_var=False`. The generated code makes a conscious choice for one-sided which is appropriate given the phrasing.
    - **Mann-Whitney U test:** Correctly applied for BMI.
    - **Chi-square test:** Correctly defines age groups and applies `pd.crosstab` and `stats.chi2_contingency`.
    - **Pearson Correlation:** Correctly applied for Pregnancies and Age.
    - **ANOVA:** Correctly defines BMI categories and uses `stats.f_oneway`. The handling of potentially empty categories is robust.
- **Interpretation:** Provides clear interpretation of p-values against alpha for each test.
- **Robustness:** Includes `dropna()` calls before statistical tests, which is good practice. The `try-except` block for file loading is also good.

**Areas for minor improvement (to reach a perfect 10):**
- **T-test `equal_var` parameter:** The `ttest_ind` function by default assumes equal variances (`equal_var=True`). It's often safer, especially without checking assumptions (e.g., Levene's test), to use `equal_var=False` (Welch's t-test) which does not assume equal variances. The ground truth explicitly uses this. While 'greater' is appropriate, the choice of `equal_var` is also important.
- **BMI Categorization (ANOVA):** The `pd.cut` `right` parameter for BMI categories:
    - Current code: `right=False` means intervals are `[a, b)`. So, `[0, 18.5), [18.5, 25), [25, 30), [30, inf)`.
    - Ground truth: `right=False` for BMI also. This is consistent.
    - However, the labels for Normal and Overweight (e.g., ""(18.5-24.9)"") imply that 25 is excluded from ""normal"" and included in ""overweight"". The chosen `bins=[0, 18.5, 25, 30, float('inf')]` and `right=False` handles this correctly as `[18.5, 25)` will exclude values >=25. So this is actually correct for the specified categories.
- **Categorization consistency:** For age groups, `right=True` was used for `pd.cut` in the ground truth, meaning `(0, 29]`, `(29, 50]`, etc. Your code uses `right=True` for age as well, which is consistent with typical usage and handles the boundaries (`<30`, `30-50`, `>50`) correctly.
- **Print statements:** While good, adding the means/medians for all tests where applicable (e.g., for ANOVA groups) would enhance the summary information without cluttering too much, similar to what the ground truth did for the t-test and Mann-Whitney U test. The generated code does add mean glucose per BMI category, which is great.

Overall, a very high-quality solution that demonstrates a strong understanding of the task and statistical analysis principles.",gemini-2.5-pro-preview,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score...",8,"The generated code provides a largely correct and comprehensive solution to the task. It successfully implements data loading, preprocessing (median imputation of zeros), feature engineering (Glucose_BMI_ratio, Age_group, Insulin_level), data splitting, and a robust machine learning pipeline including multiple models, cross-validation, hyperparameter tuning (for the best performing model), and comprehensive evaluation metrics. Visualizations for confusion matrices and ROC curves are also included, along with feature importance for tree-based models.

Here's a breakdown of the strong points and areas for improvement:

**Strong Points:**
*   **Correctness:** The core logic for data preprocessing, feature engineering, model training, and evaluation is sound.
*   **Feature Engineering:** Successfully implemented all three required new features. The `Insulin_level` tertile categorization is accurate.
*   **Preprocessing Pipeline:** Effective use of `ColumnTransformer` and `Pipeline` for preprocessing numerical (scaling) and categorical (one-hot encoding) features, which is good practice. `drop='first'` for `OneHotEncoder` is a sensible choice to avoid multicollinearity.
*   **Model Training and Evaluation:** Correctly trains and evaluates multiple classification models, performs cross-validation, and calculates required metrics (accuracy, precision, recall, F1, ROC-AUC).
*   **Hyperparameter Tuning:** Correctly identifies the ""best"" model from cross-validation and performs `GridSearchCV` for tuning. The parameter grids are reasonable.
*   **Visualization:** Successfully generates confusion matrices and ROC curves.
*   **Feature Importance:** Correctly extracts and displays feature importance for tree-based models, including handling the transformed feature names.
*   **Robustness:** Includes `try-except` blocks for file loading and `zero_division=0` in metrics, which adds robustness.
*   **Code Structure:** Well-organized with clear comments indicating task sections.

**Areas for Improvement (Why it's not a 9 or 10):**

1.  **`Glucose_BMI_ratio` Handling:** While the ground truth also handles `inf` and `nan` for this ratio, the generated code doesn't explicitly mention that division by zero (if BMI was 0 before imputation) could cause `inf`. The `df.replace([np.inf, -np.inf], np.nan, inplace=True); df['Glucose_BMI_ratio'].fillna(df['Glucose_BMI_ratio'].median(), inplace=True)` is a good robust way to handle it, but it's an implicit fix for a potential issue that wasn't entirely described in the prompt. The median imputation for the ratio is reasonable.
2.  **`Age_group` Binning:** The `age_bins = [0, 30, 50, df['Age'].max() + 1]` for `Age_group` is an acceptable interpretation, but the ground truth `[0, 20, 30, 40, 50, 60, np.inf]` is a more standard and granular way to create age groups, as usually done in such analytical tasks. The generated code's simplified bins might lose some granularity if the task specifically implied more detailed age groups.
3.  **SVC `probability=True`:** The generated code correctly sets `probability=True` for SVC, which is crucial for ROC-AUC calculation (and `predict_proba`). However, the `max_iter` for Logistic Regression in the generated code is unnecessary and could be removed as `liblinear` doesn't typically require it for convergence on such datasets and it's not a parameter for `liblinear` solver. The ground truth also uses `probability=True`.
4.  **Error Handling in CV and Tuning Loop:** When a model fails during cross-validation or fitting, the generated code simply prints an error and moves on, setting `fitted_pipelines[name] = None`. This is good for robustness, but the subsequent conditional checks (`if not cv_results` and `if best_model_name_cv and best_model_name_cv in param_grids`) could be slightly cleaner for clarity, though they functionally work.
5.  **Feature Importance Naming:** While the `cleaned_feature_names = [fn.split('__')[-1] for fn in feature_names_transformed]` is a good attempt to clean up `ColumnTransformer`'s output for readability, it relies on the specific `__` separator. The ground truth's approach of reconstructing `all_feature_names` after `fit_transform` (by getting `get_feature_names_out` from the OHE transformer specifically) is a slightly more robust way to handle all feature names, especially if `remainder='passthrough'` was used. However, for the given `remainder='drop'`, the generated code's approach is acceptable.
6.  **Minor Redundancy in `base_models_defs` and `fitted_pipelines`:** The ground truth directly fits the models within the loop, storing the *fitted* model. The generated code defines `base_models_defs` and then creates a new `pipeline` each time, implicitly refitting on the full training data for evaluation. This is a common pattern, but it means that the models used for final evaluation are not necessarily the exact models *just* used for their CV scores, if `cross_val_score` creates internal copies. The ground truth's approach of having one loop for CV, then training the model *once* on the full training data for test set evaluation is efficient and clear. However, the generated code then stores `fitted_pipelines` which is also valid.

Overall, it's a very good attempt, fulfilling almost all requirements with solid implementation. The mentioned improvements are minor polish or alternative approaches rather than critical flaws.",gemini-2.5-pro-preview,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from imblearn.over_sampling import SMOTE

def advanced_feature_en...",8,"The generated code successfully implements almost all specified requirements, demonstrating a good understanding of advanced feature engineering and dimensionality reduction techniques.

**Strengths:**
- **Comprehensive Feature Engineering:** It correctly creates all requested engineered features ('Metabolic_Score', 'Risk_Factor_Count', 'Glucose_per_Age', 'BMI_BloodPressure_interaction', 'Pedigree_Age_product') and also handles the intermediate flags, which is good practice.
- **Proper Missing Value Handling:** Correctly identifies and imputes 0 values in specified columns using `SimpleImputer` with a median strategy.
- **Polynomial Features:** Correctly applies polynomial features, creating a `PolynomialFeatures` object and integrating the new features into the DataFrame while dropping the originals to avoid redundancy.
- **Feature Scaling:** Implements both `StandardScaler` and `MinMaxScaler` as requested, demonstrating knowledge of different scaling techniques.
- **Correlation Filtering:** The correlation filtering logic is well-implemented and removes highly correlated features effectively.
- **SMOTE Implementation:** Correctly applies SMOTE to handle class imbalance, converting the result back to a DataFrame, which is crucial for subsequent steps.
- **PCA with Variance Explanation:** PCA is applied, and the code explains the variance ratios and cumulative explained variance, including a plot, which is a nice addition. It also demonstrates selecting components for a certain variance explained.
- **Feature Selection (SelectKBest, RFE):** Both SelectKBest and RFE are correctly applied with appropriate estimators and parameters.
- **Feature Importance (Random Forest & Visualization):** Random Forest is used for feature importance evaluation, and the results are visualized, fulfilling the requirement.
- **Code Structure and Readability:** The code is well-organized into a function, includes comments, and has informative print statements, making it easy to follow the pipeline. It also includes a robust `if __name__ == '__main__':` block to create a dummy CSV if not found, aiding reproducibility.

**Areas for Improvement (to reach a higher score):**
- **Order of Operations (Minor):** The ground truth performs correlation filtering *before* PCA but *after* initial scaling. The generated code performs correlation filtering *before* any scaling but then uses the scaled data for PCA and feature selection. While correlation filtering can theoretically be done at different stages, doing it after initial scaling (as in the ground Truth) is often more robust, especially when features have vastly different scales. It reduces the chance of misleading correlations if scaling dramatically changes feature relationships (though less likely for linear correlations).
- **SMOTE Placement:** The generated code applies SMOTE to the *entire* X and y datasets directly after scaling and filtering. While SMOTE was applied before feature selection methods, for robust model training, SMOTE should ideally be applied *only* to the training data **after** splitting the dataset into train/test sets, as in the ground truth. Applying SMOTE to the entire dataset (including the future test set) leads to data leakage and an overly optimistic evaluation of model performance. The ground truth correctly splits the data first then applies SMOTE to the training set. This is a significant point for real-world machine learning pipelines.
- **Clarity on Selection for Downstream Tasks:** While the code demonstrates multiple scaling methods and multiple feature selection methods, it's not explicitly stated which transformed dataset (e.g., PCA transformed, SelectKBest, RFE output) would be used for a hypothetical model training step following the pipeline. The ground truth explicitly states `X_processed = X_scaled_standard_df` and then performs feature selection on `X_filtered_corr`, and SMOTE then again on `X_final = X_filtered_corr`. The generated code uses `X_smote_df` (which is `X_processed` after SMOTE) for PCA and feature selection, which is a plausible choice, but consistency in picking the final feature set can be highlighted.
- **RFE `max_iter`:** The `max_iter` was manually reduced to `200` in RFE in the generated code comments saying ""Reduced max_iter for speed, ensure convergence"". This isn't strictly necessary and could potentially lead to non-convergence issues for specific datasets if the default `max_iter` was higher for a reason.

Despite the minor order of operations choice and the more significant SMOTE placement issue, the generated code is highly functional and demonstrates a strong grasp of the required techniques.",gemini-2.5-pro-preview,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Significance level
ALPHA = 0.05

def interpret_p_value(p_value, alpha, test_name=""Test""):
    if pd.isna(p_value):
        return f""{test_name} p-value: N/A (insufficient data or variance for test).""
    if p_value < alpha:
        return f""{test_name} p-value: {p_value:.4f}. Reject H0 (significant result at alpha={alpha}).""
    else:
        return f""{test_name} p-value: {p_value:.4f}. Fail to reject H0 (not a significa...",7,"The generated code addresses most of the requirements and uses appropriate statistical tests. However, there are several areas for improvement, particularly regarding the interpretation of results and consistency in test parameters with the ground truth.

**Strengths:**
*   **Correctness and Completeness (mostly):** The code attempts to perform all the requested statistical tests (t-test, Mann-Whitney U, ANOVA, Pearson correlation, Chi-square).
*   **Data Handling:** It loads the data and handles missing values for critical columns, which is a good practice.
*   **Structured Output:** The use of `print` statements with section headers makes the output readable.
*   **Robustness in missing value checks:** The generated code adds multiple checks for `NaN` values, empty dataframes, and insufficient data points before performing a test, which is a good defensive programming practice.
*   **Levene's Test:** The code dynamically checks for equal variance using Levene's test before performing t-tests, which is a more robust approach than simply assuming equal variance.

**Areas for Improvement:**

1.  **Interpretation of Results (Crucial for this Task):**
    *   The `interpret_p_value` function correctly states whether to reject or fail to reject the null hypothesis. However, the task explicitly asked for **interpretation of results** (e.g., ""Western European countries have significantly HIGHER happiness scores""). The generated code mostly stops at the p-value interpretation and does not provide these deeper, context-specific interpretations when the null hypothesis is rejected. This is a significant gap compared to the ground truth.
    *   For instance, in Test 1, if H0 is rejected (`alternative='greater'` means H1 is ""mean WE > mean others""), the interpretation should explicitly state that Western Europe has significantly *higher* happiness. The generated code falls short here. Similarly for Generosity by Freedom scores.
    *   For Tests 2 (Mann-Whitney U) and 4 (Correlation), the interpretation of *direction* (higher/lower, positive/negative correlation) if significant is missing.

2.  **Hypothesis Direction (Task 2 & 7):**
    *   **Test 1 (Western Europe vs. Other Regions):** The task specified ""significantly higher happiness scores"" for Western Europe, implying a one-tailed test. The generated code correctly uses `alternative='greater'` for this.
    *   **Test 7 (Freedom vs. Generosity):** The task asked for ""significantly different generosity levels"" (undirected), implying a two-tailed test. The generated code correctly uses `alternative='two-sided'`. This is a subtle but important detail handled correctly.

3.  **Chi-square Test (Task 6) - Categorization and Robustness:**
    *   The `pd.qcut` in the generated code might fail if there are too many duplicate values in `Ladder score`, leading to fewer than 3 unique quantiles (e.g., all scores are the same, or very few unique scores). The `duplicates='drop'` argument helps but still might lead to less than 3 categories. The ground truth uses fixed `quantile(1/3)` and `quantile(2/3)` thresholds which might result in imbalanced bins but guarantees 3 *potential* categories. The generated code's warning and handling of `nunique() < 2` is a good attempt at robustness, but the ground truth's approach for categorization might be slightly more stable for specifically creating 3 categories.
    *   The `pd.crosstab` in the generated code may produce an empty (or single row/column) table if certain categories or regions are missing (which `dropna` should mostly prevent, but still possible). However, the `stats.chi2_contingency` function requires at least a 2x2 table for a meaningful test. The generated code tries to handle this by checking dimensions, which is good.

4.  **Redundant `nan_policy='omit'` in `ttest_ind` and `mannwhitneyu`:** The `dropna` at the beginning based on `cols_to_use` should ensure no NaNs in the relevant series. While `nan_policy='omit'` doesn't hurt, it's technically redundant if the initial data cleaning is thorough.

5.  **Small Data Handling (e.g., Mann-Whitney U, Pearson):** The code attempts to add checks for small datasets (e.g., `n_total_countries >= 4` for Mann-Whitney, `len(gdp_data) >= 3` for Pearson). This is good for preventing errors but introduces complexity where the initial `dropna` on critical columns should be assumed to provide sufficient data for the subsequent tests as per the problem description (though it doesn't hurt to be defensive).

Overall, the generated code demonstrates a strong understanding of statistical testing and good programming practices but misses some key interpretation requirements and could simplify some robustness checks given the initial `dropna` handling.",gemini-2.5-pro-preview,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.pipeline import Pipeline

# Glo...",6,"The generated code attempts to address all requirements of the prompt and demonstrates a good understanding of the steps involved in a regression modeling task. However, it has several issues that prevent it from achieving a higher score, primarily related to correctness in implementation, particularly with scaling and hyperparameter tuning consistency, and some minor deviations from the prompt's explicit requirements.

**Issues and Improvements:**

1.  **Preprocessing - Missing Values:** The prompt simply asks to ""handle missing values"". The ground truth chose to `dropna(subset=...)` which is a valid strategy. The generated code attempts to impute with the median. While imputation is generally a good practice, for this specific dataset and the chosen features/target, there are no missing values in `Ladder score`, `Logged GDP per capita`, `Social support`, `Healthy life expectancy`, `Freedom to make life choices`, `Generosity`, and `Perceptions of corruption` in the given `world-happiness-report-2021.csv` as per an `isnull().sum()` check. Therefore, the imputation logic is redundant and adds unnecessary complexity for this specific dataset, though it's good general practice. The `dropna` method is simpler and equally effective given the data.

2.  **Categorical Variables:** The prompt explicitly asks to ""encoding categorical variables"". Both the ground truth and the generated code correctly identify that for the *specified features*, there are no categorical columns needing encoding. The ground truth explicitly states this, which is good. The generated code mentions ""This case should not happen with the specified features"" implying it considered it, which is fine.

3.  **Consistency in Scaling (`train_and_evaluate_models`, `tune_hyperparameters`, `perform_cross_validation`):**
    *   The `train_and_evaluate_models` function has a conditional logic for scaling (using `X_train_scaled` for linear models and `X_train` for tree models). This is good.
    *   However, `tune_hyperparameters` takes `X_train_data`. It needs to dynamically pass either `X_train_scaled` or `X_train` based on the `model_name`. The `main` function *does* correctly pass `X_train_for_tuning` which implements this.
    *   **Major Flaw in `perform_cross_validation`:** When performing cross-validation for linear models, the generated code creates a `Pipeline` with `StandardScaler()` and `model_instance`. This is the correct way to handle scaling within cross-validation to prevent data leakage. However, for tree-based models, it directly passes the `model_instance` to `cross_val_score` without a pipeline. If the `model_instance` was trained on `X_train` (unscaled), then passing `X` (full dataset) (which is unscaled) is correct. The issue is that `best_tuned_model` or `model_instance` passed to `cross_val_score` `X` and `y` for cross-validation on the *entire dataset*. The prompt implies this (`X`, `y` from `df_cleaned`). If the tree model was trained on `X_train` but then applied to the full `X` in cross-validation, it's consistent.
    *   **Pipeline for Tuned Linear Models:** When `tune_hyperparameters` receives a linear model (e.g., Ridge) it fine-tunes its parameters. The problem is that the resulting `best_tuned_model` for linear models (like Ridge) doesn't inherently include the `StandardScaler`. When `evaluate_tuned_model` or `predict_new_data` tries to use this `best_tuned_model`, it might be expecting scaled data when it receives unscaled, or vice versa, causing errors or inconsistent results. The generated code's `predict_new_data` *does* pass a `scaler` object but uses `model_name in LINEAR_MODEL_NAMES` to decide scaling. This means the `best_tuned_model` itself needs to be a pipeline in these cases, or the scaling must be explicitly handled before prediction. The ground truth implements explicit `StandardScaler` on `X` and `y` first, then uses `X_train`, `X_test` from that. The generated code uses pipeline in `perform_cross_validation` but not for the `best_tuned_model` itself, which could lead to issues.

4.  **Hyperparameter Tuning Target:** The prompt says ""Perform hyperparameter tuning for the best model using GridSearchCV"". The generated code selects the best *tunable* model (excluding Linear Regression) by iterating through `results_df`. This is a reasonable interpretation. The ground truth explicitly chose Gradient Boosting for tuning regardless of being the best initial model, which is also a valid strategic choice (tuning the model that often performs well). The generated approach is more robust to change in initial model performance.

5.  **`plot_feature_importance` for Linear Models:** The generated code attempts to plot feature importance for linear models using `np.abs(model.coef_)`. While this conceptually represents importance, the actual `feature_importances_` attribute only applies to tree-based models. A scatter plot or bar chart of coefficients might be more appropriate for linear models, or simply restricting this visualization to tree-based models as the ground truth does. The ground truth correctly restricts this visualization to models that have `feature_importances_`.

6.  **`predict_new_data` function:**
    *   The `predict_new_data` function is well-structured.
    *   It correctly handles the scaling for linear models using the `scaler` object.
    *   It also handles column order correctly.
    *   The analysis of results is a good addition.

7.  **Code Structure and Readability:**
    *   The use of functions (`load_data`, `preprocess_data`, etc.) is excellent for modularity and readability.
    *   Global constants are used appropriately.
    *   Docstrings are present.

8.  **Minor Deviation from Prompt (Random State):** The prompt specified `Linear Regression, Ridge Regression, Lasso Regression, Random Forest Regressor, and Gradient Boosting Regressor`. The generated code correctly includes all of them. The ground truth explicitly sets `random_state=42` for Ridge and Lasso, which aids reproducibility; the generated code sets it for all applicable models in the `models` dictionary, which is better.

9.  **Completeness:** All major requirements are attempted: data loading/preprocessing, feature/target selection, train/test split, multiple models, hyperparameter tuning, evaluation metrics, residual plots, feature importances, cross-validation, and new predictions. The issues are mainly in the correctness/consistency of execution rather than outright missing features.

Overall, the generated code is a good effort but needs refinement in handling data pipelines, especially for scaled versus unscaled data with the tuned models across different functions like evaluation, cross-validation, and prediction. The explicit handling of the `StandardScaler` within a `Pipeline` for linear models, even after tuning, would make the solution more robust.",gemini-2.5-pro-preview,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_regression, RFE, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load and preprocess the data...",9,"The generated code is highly correct and complete, addressing almost all aspects of the complex prompt. It demonstrates a strong understanding of data preprocessing, feature engineering, dimensionality reduction, and feature selection techniques.

**Strengths:**

*   **Comprehensive Feature Engineering:** Successfully created all requested engineered features: 'Happiness_Efficiency', 'Social_Wellness_Index', 'Governance_Score', 'Regional_Happiness_Rank', and 'GDP_vs_Regional_Average'.
*   **Effective Preprocessing:** Includes robust column name cleaning, handling of 'explained by' columns, and `dropna` for missing values.
*   **Correct Polynomial Features:** Applied `PolynomialFeatures` to key happiness factors and correctly handled `include_bias=False`. It also thoughtfully handles adding `higher_degree_poly_names` to avoid duplicating original columns.
*   **Multiple Scaling Methods:** Implemented `StandardScaler`, `MinMaxScaler`, and `RobustScaler` as requested, providing appropriate DataFrame outputs for each.
*   **PCA and t-SNE with Visualization:** Correctly applied both PCA and t-SNE for dimensionality reduction and generated visualizations with `matplotlib` and `seaborn`, coloring by `Ladder score`. Adjusts `perplexity` for t-SNE based on sample size which is good practice.
*   **Multiple Feature Selection Methods:** Implemented `SelectKBest` (with `f_regression` and `mutual_info_regression` for scores, though only `f_regression` was used to select the features), `RFE` (with `LinearRegression`), and correlation-based filtering.
*   **Mutual Information Scoring:** Explicitly calculated and printed mutual information scores, as requested.
*   **Interaction Features:** Created explicit interaction features between economic and social factors, in addition to the polynomial features.
*   **Random Forest Feature Importance:** Used `RandomForestRegressor` to validate feature importance and provided a visualization of the top features.
*   **Code Quality:** Good use of imports, clear variable names, and logical code structure.

**Areas for Minor Improvement:**

*   **`mutual_info_regression` for `SelectKBest`:** While `mutual_info_regression` was used to show scores, the `SelectKBest` step explicitly calls `f_regression` twice. The prompt implies using `mutual_info_regression` as a `score_func` for `SelectKBest`, which the ground truth code also does. This is a very minor point, as `f_regression` is also a valid score function.
*   **Explicit `plt.show()` calls:** The generated code comments out `plt.show()` for all plots, which means the plots won't display by default in an interactive environment. For a benchmark tasked with showing capability, having these uncommented would be ideal, or explicitly stating that plots are saved. The ground truth correctly uses `plt.show()`.
*   **`X = X.loc[:,~X.columns.duplicated()]`:** While a good defensive measure, ensure this doesn't accidentally drop a useful explicitly engineered interaction if a polynomial feature happened to generate the *exact same* column name. In this specific case, the polynomial features' naming convention (`feature1 feature2`) is distinct enough from the explicit interaction names (`eco_x_social_explicit`) that it's unlikely to cause an issue here, but it's something to be aware of in general. The ground truth avoided this by adding new polynomial terms to a copy and carefully concatenating.

Overall, this is an excellent solution that demonstrates a comprehensive understanding of the task and a high level of programming proficiency in Python for data science. The minor points are easily addressable and do not detract significantly from the robust implementation.",gemini-2.5-pro-preview,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# 1) Load the dataset, parse the 'date' column, and filter for 'location' == 'USA'.
try:
    df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
except FileNotFoundError:
    print(""Error: 'synthetic_covid19_data.csv' not found. Make sure the file is in t...",8,"The generated code correctly addresses most of the task requirements, including loading and filtering data, resampling, decomposition, ADF test, conditional differencing, and plotting ACF/PACF.

Strong points:
- Correctly loads, parses dates, and filters data.
- Handles `FileNotFoundError` robustly.
- Correctly resamples to weekly frequency and sums cases.
- Implements `seasonal_decompose` with an additive model and appropriate period.
- Plots decomposition, ACF, and PACF correctly.
- Implements the ADF test and prints results clearly.
- Correctly applies first-order differencing if the initial ADF test indicates non-stationarity.
- Uses `dropna()` appropriately after `diff()`.
- Provides good default `lags` for ACF/PACF plots relative to series length.

Areas for improvement to achieve a higher score:
- **Error Handling/Edge Cases for Decomposition:** While it has a warning for short series, the `seasonal_decompose` function can still fail if the series is too short for the specified period (e.g., `ValueError: You must have at least 2 complete periods for seasonal_decompose`). The ground truth code has more comprehensive checks and adjustments for this, including a fallback for `decomposition_period`. The generated code sometimes attempts decomposition even when `series_len < decomposition_period`, which can lead to errors unless `extrapolate_trend='freq'` handles it. For example, if `series_len` is 50 and `decomposition_period` is 52, it will likely still throw an error ""period must be less than or equal to the number of data points"". The condition `if series_len < 2 * decomposition_period:` is a correct check but doesn't prevent all errors for `seasonal_decompose`. A more robust check `if series_len < decomposition_period:` before attempting decomposition would be better.
- **Handling of zero/no data:** The ground truth includes `weekly_cases = weekly_cases[weekly_cases > 0].dropna()` before decomposition, which cleans up weeks with zero cases that might affect decomposition or be irrelevant. The generated code `weekly_cases_usa.fillna(0)` is generally good practice but might retain long stretches of zeros that could affect decomposition.
- **ADF Test on empty/short series:** The `adf_test` function could improve its handling of very short series (e.g., `len(timeseries_cleaned) < 2`) as `adfuller` requires at least two data points.
- **ACF/PACF plot condition:** The condition for plotting ACF/PACF is a bit complex: `if cleaned_stationary_series.empty: ... elif p_value is None and series_name_for_acf_pacf == ""Weekly New Cases"": ... else:`. It could be simplified to just checking if `cleaned_stationary_series` is valid and has enough points. If ADF test failed or didn't run, it still makes sense to plot the (potentially differenced) series, but the current logic for `p_value is None` might prevent plotting in some scenarios.
- **Suppression of matplotlib/statsmodels warnings:** The ground truth proactively suppresses common warnings from these libraries, which improves user experience, especially during automated testing. This is not a strict requirement but a good practice for cleaner output.",gemini-2.5-pro-preview,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

def analyze_ph_groups():
    # 1. Load the dataset
    try:
        df = pd.read_csv('winequality-red.csv')
    except FileNotFoundError:
        print(""Error: 'winequality-red.csv' not found in the current directory."")
        return

    # 2. Categorize wines based on 'pH' values
    bins = [0, 3.2, 3.4, df['pH'].max() + 0.1]  # Add 0.1 to max to ensure inclusion
    labels = ['Low pH'...",7,"The generated code addresses most parts of the task but has a few inaccuracies and areas for improvement.

**Strengths:**
*   Correctly loads the data and handles `FileNotFoundError`.
*   Successfully calculates average quality per group and prints it.
*   Performs the ANOVA test and correctly interprets the p-value.
*   Generates a visualization with error bars (though the error bar calculation could be more robust).
*   Correctly identifies and handles cases where groups might be empty or have too few samples for ANOVA.
*   Includes a `main` guard for the function call.

**Areas for Improvement:**
*   **pH Group Categorization (Minor Issue):** The initial `pd.cut` with `bins = [0, 3.2, 3.4, df['pH'].max() + 0.1]` is then overridden by `np.select`. While `np.select` gets the grouping correct, the original `pd.cut` approach was perfectly capable if the bins and `right` parameter were set correctly (e.g., `bins=[0, 3.2, 3.4, np.inf]`, `right=False`). The task explicitly asked for `pd.cut` and the current implementation uses `np.select` instead, effectively sidestepping the direct `pd.cut` use for the actual categorization. The labels for `np.select` are also slightly off compared to original request: ""Medium pH (3.2 to 3.4)"" vs ""Medium pH (3.2-3.4)"".
*   **Confidence Interval Calculation (Major Issue):** The confidence interval calculation needs significant improvement.
    *   The `ci_95_half_width` calculation handles `count <= 1` by setting it to `np.nan`, which is good.
    *   However, for the `stats.t.ppf` calculation, `(1 + confidence_level) / 2.` is redundant; `(1 - confidence_level) / 2.` and `(1 + confidence_level) / 2.` are the correct alpha values for the two tails, or simply `(1 - alpha/2)` for the upper tail percent point function. A more direct way is `stats.t.interval(confidence_level, df=degrees_freedom, loc=mean, scale=sem)`. The current approach of calculating `ci_half_width` separately from the `mean` and then applying it symmetricall assumes normality centered around the mean, which is what the t-dist CI is, but the implementation for the confidence interval `yerr` in the plot is calculated as `mean - lower_bound` from the interval, not just `stats.t.ppf(...) * sem`. The ground truth example directly calculates the lower CI and uses `mean - lower_CI` for `yerr` to handle potential slight asymmetries or just to ensure it's displayed correctly from the mean.
    *   The plot uses `yerr=summary_stats['ci_95_half_width']` which makes the error bars symmetric. This is acceptable for t-distribution based CIs.
*   **Plotting Order:** The `reindex(ph_group_labels)` helps maintain the order for summary statistics, which is good for the plot.
*   **Dataset Path Handling:** The `DATASET_PATH` as a constant is a good practice missing in the generated code, but not explicitly requested. The generated code directly hardcodes the filename.

To improve, focus on simplifying the pH categorization using `pd.cut` as intended, and then re-evaluate the confidence interval calculation using `stats.t.interval` directly for robustness.",gemini-2.5-pro-preview,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# 1) Load the dataset and parse 'date'
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

# 2) Select features for imputation and modeling, and target ...",6,"The generated code addresses most of the requirements but has a significant logical flaw related to data scaling and a minor issue with feature selection for imputation.

**Correctness:**
The primary issue affecting correctness is the scaling. The requirement was to ""Scale **all numerical features** using StandardScaler."" The generated code only scales the `numerical_features_list`, but after one-hot encoding, the dummy variables for 'continent' are also numerical features that should be scaled, or at least handled consistently. By scaling only a subset of the features *after* combining them into `X`, and more critically, *prior* to splitting and fitting the `StandardScaler` only on the training set, it introduces a problem of data leakage if not handled carefully, and an inconsistency in scaling. The ground truth scales *after* splitting, which is the correct approach.

The ground truth correctly scales `X_train` and `X_test` *after* the split, ensuring that all numerical features (including the one-hot encoded ones which are binary numerical features) are treated consistently, and that the scaler is fit only on the training data.

The generated code's imputation strategy for 'continent' (imputing with mode) was not explicitly asked for by the prompt, which specified ""Handle missing values in selected features and target by imputing with the median."" The prompt did not specify how to handle missing categorical features, but the ground truth handles this implicitly by dropping NaNs (which is typically done in `get_dummies` if not told otherwise, or by imputing before `get_dummies`), and focuses *median* imputation only on numerical features and target. The generated code also explicitly calls `fillna` for `continent` which is redundant if `drop_first=True` was used with `get_dummies` and the missing values were in the original dataframe prior to encoding. However, the ground truth implicitly handles this by only creating `df_selected` with the chosen columns and then imputing, meaning if `continent` had missing values that weren't numerical, they would be dropped upon `get_dummies` if `NaN`s appeared as a category or handled by the encoding process.

**Code Quality:**
- Imports are correct.
- Structure is generally good, following the steps outlined in the prompt.
- Use of `.loc` for `fillna` is good practice to prevent `SettingWithCopyWarning`.

**Completeness:**
- Loads data and parses date: Yes.
- Selects features and target: Yes.
- Handles missing values by median imputation: Mostly yes, but the additional mode imputation for 'continent' wasn't part of the prompt's explicit requirement for median imputation, and the primary scaling issue.
- Encodes 'continent': Yes.
- Scales numerical features: Partially, as explained above, it misses the scaled one-hot encoded features and applies scaling before splitting potentially incorrectly.
- Splits data: Yes, but the scaling step is misplaced relative to the split.
- Trains models: Yes.
- Evaluates models: Yes.
- Prints metrics: Yes.

The main issue is the incorrect application of `StandardScaler`. The scaler should be fit on `X_train` and then both `X_train` and `X_test` should be transformed using that *same* scaler. The generated code attempts to do this, but it also tries to scale only a subset of `X_train` and `X_test` (only `numerical_features_list` within `X_train` and `X_test` after `pd.concat`), which is problematic because the one-hot encoded features are also numerical and should be part of the scaling process if all numerical features are to be scaled. If they are not to be scaled, then the column selection within the `StandardScaler` application needs to be more precise, and `StandardScaler` is typically for continuous features, not binary. The most common and correct approach for mixed data types (continuous and binary/one-hot encoded) is to scale only the continuous features, and leave the binary features as is. The ground truth scales everything, which is a common practice with `StandardScaler` when applied to the entire feature set after encoding, as it will handle binary 0s and 1s. The key is applying it *after* the split and fitting on `X_train`.

The generated code applies `StandardScaler` to `X_train[numerical_features_list]` and `X_test[numerical_features_list]` *after* `X` is formed by `pd.concat`. This means the one-hot encoded features remain unscaled. While not always wrong to leave binary features unscaled, the prompt asks to ""Scale all numerical features"", and after one-hot encoding, these are numerical. A more robust approach would be to scale the entire X_train/X_test as the ground truth does, or to explicitly define which parts are scaled vs. not. The ground truth handles this by scaling `X_train_scaled` and `X_test_scaled` (which include the one-hot encoded variables), which is a common practice.",gemini-2.5-pro-preview,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
from scipy import stats
# statsmodels is not strictly required for the functions used here (scipy.stats covers them),
# but it's a common library for more advanced statistical modeling.
# import statsmodels

def perform_hypothesis_tests(csv_path='synthetic_covid19_data.csv'):
    # 1) Load the dataset and handle missing values
    try:
        df = pd.read_csv(csv_path)
    except FileNotFoundError:
        print(f""Error: The file {csv_path} was not found."")
       ...",8,"The generated code largely addresses all requirements of the task with good robustness.

**Strengths:**
*   **Comprehensive handling of missing values:** The imputation logic is well-implemented, including checks for columns not present in the dataframe and columns that are entirely NaN.
*   **Robustness in edge cases:** The code includes checks for empty DataFrames, insufficient data points for statistical tests (e.g., `<2` for Pearson, `<2` groups for ANOVA, `<2` per group for t-test), and zero standard deviation for Pearson correlation.
*   **Clear Null Hypotheses and Conclusions:** For each test, the null hypothesis is clearly stated, and the conclusion correctly interprets the p-value against the alpha level.
*   **Correct Test Selection:** ANOVA for multiple groups, Pearson for correlation, and t-test for two groups are correctly chosen.
*   **Handling of 'date' column:** The t-test part correctly tries to parse the 'date' column and handles cases where it might be missing or unparseable.
*   **Levene's Test:** The inclusion of Levene's test to determine `equal_var` for the t-test is a good practice for robustness, although the problem statement didn't explicitly ask for it.
*   **Self-contained and Runnable:** The `if __name__ == '__main__':` block with dummy data generation makes the code runnable out-of-the-box, which is excellent for testing and demonstration.

**Areas for Improvement (to achieve a 9 or 10):**

1.  **Imputation Logic (Minor Issue):** While it imputes NaNs, the task specifically asked for median imputation on *relevant* columns. The generated code effectively does this, but the early `if df[col].isnull().all():` block for `median_val = df[col].median()` will assign `NaN` as the median, which then `fillna(NaN)` does not correctly impute. For a column that is *entirely* NaN, it's impossible to impute with its own median. The ground truth correctly assumes that `median()` will return `NaN` for an all-NaN series and `fillna(NaN)` will not change it, which is correct behavior for this scenario. However, the print statement `Warning: Column '{col}' is entirely NaN. Median imputation will result in NaN (no change).` is good. The overall flow is correct.

2.  **Explicit `ALPHA` Constant:** The ground truth defines `ALPHA = 0.05` once at the top. The generated code redefines `alpha = 0.05` locally within each test function. While this doesn't cause functional issues, defining it globally or passing it as a parameter to a helper function would be slightly cleaner for consistency.

3.  **T-test `equal_var` (Minor Deviation/Enhancement):** The ground truth uses `equal_var=False` (Welch's t-test) which is generally more robust as it doesn't assume equal variances. The generated code uses Levene's test to decide `equal_var`, which is theoretically sound but adds complexity and dependency on Levene's test result. For a benchmark where direct comparison is key, adhering to a simpler interpretation or specified assumption (like Welch's) would be slightly more aligned if not otherwise specified. The problem statement asked for ""an appropriate t-test,"" and both are appropriate; however, the ground truth picked Welch.

4.  **T-test 'date' Column Handling (Minor):** The logic for getting the latest entry per country is a bit more verbose than the ground truth's. The ground truth sorts by date and then uses `groupby('country').last()`, which implicitly handles `NaN` dates by sorting them to the beginning or end depending on `na_position` and `.last()` picking the valid last one. The generated code's `.reset_index()` after `groupby().last()` is also good.

5.  **Print Statements:** The print statements are helpful for debugging and understanding flow, but some are very verbose, especially the `Conclusion: ... likely due to insufficient data or groups meeting criteria.` This could be slightly streamlined for a clean output.

Overall, the code is very well-written, handles many edge cases, and provides robust statistical analysis. The minor points listed above are mostly about style, strict adherence to one interpretation (Welch's t-test), or slight verbosity rather than fundamental correctness issues.",gemini-2.5-pro-preview,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load the dataset
try:
    df = pd.read_csv('clash_royale_cards.csv')
except FileNotFoundError:
    print(""Error: 'clash_royale_cards.csv' not found in the current directory."")
    exit()

# 2. Select the numerical columns
numerical_cols = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']

# Ensure the columns exist and handle potential non-numeric data issues if any
# For simplicity, we...",9,"The generated code successfully completes all aspects of the task: it loads the data, selects the correct numerical columns, calculates the correlation matrix, and visualizes it as a heatmap with appropriate labels and formatting. The imports are correct, and the code structure is logical.

A strong point is the inclusion of `try-except` for file loading and the explicit `.copy()` when selecting columns, which are good practices. The code also attempts to handle potential string percentage columns (`'Win Rate'`, `'Usage'`, etc.) by converting them to numeric, although the original problem statement did not explicitly require data cleaning for `clash_royale_cards.csv` (which typically has these columns as numeric or easily coercible). This demonstrates a robustness beyond the minimal requirements, which is good, but also adds a bit of unnecessary complexity if the data doesn't require it based on the prompt.

The only minor area for improvement is that the `DATASET_PATH` and `NUMERICAL_COLUMNS` are hardcoded directly into the script rather than defined as constants at the top or passed as arguments to a function, as seen in the ground truth. While not a strict requirement for basic functionality, wrapping the logic in a function (like `analyze_clash_royale_data` in the ground truth) and defining constants improves reusability, testability, and clarity for more complex projects. However, for the given task, the provided code is very good and directly addresses all requirements.",gemini-2.5-pro-preview,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

try:
    # Load the dataset
    df = pd.read_csv('clash_royale_cards.csv')

    # Group the data by the 'rarity' column
    grouped_by_rarity = df.groupby('rarity')

    # Calculate the average elixirCost, Win Rate, and Usage for each rarity
    average_stats = grouped_by_rarity[['elixirCost', 'Win Rate', 'Usage']].mean()

    # Display the calculated average values
    print(average_stats)

except FileNotFoundError:
    print(""Error: 'clash_royale_cards.csv' not f...",10,"The generated code correctly loads the CSV, groups the data by 'rarity', calculates the mean for 'elixirCost', 'Win Rate', and 'Usage', and then prints the result. It also includes robust error handling for `FileNotFoundError` and `KeyError`, which is a good practice not explicitly required but demonstrates higher quality. The structure, imports, and output are all correct and complete as per the task requirements.",gemini-2.5-pro-preview,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# 1. Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 2. Create a new binary target column named is_high_win_rate
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)

# 3. Select the features (initial selection before encoding) and target
selected_feature_columns = ['elixirCost', 'Usage', 'rarity']
target_column_name = 'is_high_win_rate'

# Prep...",9,"The generated code correctly addresses all requirements of the task. It loads the data, creates the `is_high_win_rate` target column, selects the specified features, correctly applies one-hot encoding using `sklearn.preprocessing.OneHotEncoder` (which is a more robust approach than `pd.get_dummies` for pipelines, especially with `handle_unknown`), and splits the data into training and testing sets with the correct parameters. The code is well-structured and includes appropriate imports, although the `try-except` block for `FileNotFoundError` in the ground truth is a good practice not included here (but not strictly required by the prompt). The only minor point is the increased verbosity compared to `pd.get_dummies` for this specific task, but using `OneHotEncoder` directly is often a good choice for integrating into more complex ML pipelines. The `handle_unknown='ignore'` is a particularly good detail.",gemini-2.5-pro-preview,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 1) Load the dataset
try:
    df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: 'iris.csv' not found in the current directory."")
    exit()

# 2) Define features X
feature_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
X ...",10,"The generated code is excellent. It correctly implements all the requirements of the task:
- Correctly loads the dataset and handles potential `FileNotFoundError`.
- Defines features `X` and target `y` as specified.
- Encodes the target variable using `LabelEncoder`.
- Splits the data into training and testing sets with the correct `test_size`, `random_state`, and crucially, `stratify=y_encoded` which is important for classification tasks with potentially imbalanced classes.
- Initializes and trains the `KNeighborsClassifier` with `n_neighbors=3`.
- Makes predictions and accurately reports the `accuracy_score`.
- All necessary imports are included.
The code is clean, well-structured, and directly addresses all parts of the prompt. It's nearly identical to the ground truth, demonstrating a complete and accurate solution.",gemini-2.5-pro-preview,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
try:
    df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: 'iris.csv' not found in the current directory."")
    exit()

# 2) Filter the data to get two groups
versicolor = df[df['Species'] == 'Iris-versicolor']
virginica = df[df['Species'] == 'Iris-virginica']

# Extract PetalLengthCm for each group
petal_length_versicolor = versicolor['PetalLengthCm']
petal_length_virginica = virginica['PetalLeng...",9,"The generated code correctly addresses all requirements of the task: it loads the data, filters the groups, performs the t-test, states hypotheses, prints results, and interprets them correctly. The structure is logical and imports are proper.

Areas for minor improvement:
1.  **Hypothesis Printing:** The ground truth separates the print statements for hypotheses from the `print(""--- Independent Two-Sample T-Test for PetalLengthCm ---"")` banner, which makes the output clearer and more aligned with the flow of information. The generated code prints hypotheses upfront, then a separator, then the t-test results. While not incorrect, integrating the hypothesis statement more directly with the ""test results"" section (as seen in the ground truth) might feel more cohesive.
2.  **Redundant Variable Creation in Filtering:** The generated code first creates `versicolor` and `virginica` DataFrames, then extracts `PetalLengthCm` from them. This is functionally correct but slightly less direct than the ground truth which immediately extracts the series: `df[df['Species'] == 'Iris-versicolor']['PetalLengthCm']`. This is a minor efficiency/style point.
3.  **NaN/Length Checks:** The generated code includes checks for NaN values and minimum data points, which is good robust programming practice. However, `ttest_ind` typically handles NaNs by raising an error or by default skipping them. If strict NaN handling (like removal warnings) is desired, it's good to include. The ground truth implicitly relies on `ttest_ind`'s behavior (which typically raises an error on NaNs by default, ensuring non-NaN data). For this specific problem, there are no NaNs in `PetalLengthCm` for Iris, so it's not strictly necessary, but demonstrates good defensive coding.
4.  **Error Handling for Empty Groups:** The ground truth explicitly checks if the filtered groups are empty before proceeding, which is a good edge case handling practice. The generated code's `if len(petal_length_versicolor) < 2` partially covers this, but an explicit check for completely empty groups after initial filtering (`if versicolor.empty or virginica.empty:`) would be slightly more robust, akin to the ground truth.

Overall, it's a very strong submission that meets all core requirements.",gemini-2.5-pro-preview,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def generate_iris_boxplots():
    """"""
    Loads the iris.csv dataset and generates box plots for each numerical feature,
    grouped by species.
    """"""
    # 1) Load the dataset
    try:
        df = pd.read_csv('iris.csv')
    except FileNotFoundError:
        print(""Error: 'iris.csv' not found in the current directory."")
        return

    # Define the numerical features
    numerical_features = ['SepalLengt...",9,"The generated code correctly loads the iris dataset, iterates through the numerical features, and generates box plots grouped by `Species`. Each plot has an appropriate title. The code includes necessary imports and uses `matplotlib.pyplot` and `seaborn` as requested. The use of a function `generate_iris_boxplots` and the `if __name__ == '__main__':` block is good practice for modularity.

One minor area for improvement, which keeps it from a perfect 10, is the error handling for `FileNotFoundError`. While it gracefully handles the error by printing a message and returning, the ground truth's approach of using `exit()` ensures that the script terminates if the crucial data file is missing, which is a more robust way to handle unrecoverable errors in a script environment. Additionally, the ground truth code also handles a broader `Exception` which is more comprehensive. However, for a data science task focusing on plotting, the current error handling is generally acceptable. The ground truth also adds more refined plot aesthetics like `tight_layout` and more verbose `xlabel`/`ylabel` for explicit parameter passing (though the defaults are often good enough).",gemini-2.5-pro-preview,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# 1) Load the dataset
try:
    df = pd.read_csv('heart.csv')
except FileNotFoundError:
    print(""Error: 'heart.csv' not found in the current directory."")
    exit()

# 2) Select features a...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It correctly loads the dataset, selects specified features and target, performs one-hot encoding on `Sex` and `ChestPainType` using `ColumnTransformer` within a `Pipeline`, splits the data with the specified `random_state`, trains a Logistic Regression model, and reports the accuracy.

Strong points:
- Correct use of `ColumnTransformer` and `Pipeline` for preprocessing and model training.
- Correct handling of `remainder='passthrough'` to include numerical features.
- Correct `train_test_split` parameters (`test_size=0.25`, `random_state=42`).
- Accurate calculation and printing of the accuracy score.
- Good error handling for `FileNotFoundError`.

Areas for minor improvement (not critical, but could make it slightly better):
- The `LogisticRegression` instantiation includes `solver='liblinear'` and `random_state=42'`. While `liblinear` is a valid solver, the prompt asked for ""default parameters"". The ground truth code used `max_iter=1000` which is a good practice for convergence if default `max_iter` (100) is insufficient, but it's not strictly a default parameter itself. So, this is a very minor deviation. Removing these extra parameters would align perfectly with ""default parameters"" but doesn't negatively impact correctness here.
- The `print` statement for accuracy uses ""Accuracy:"" instead of ""Model Accuracy on Test Set:"" (as in the ground truth). This is a trivial cosmetic difference.",gemini-2.5-pro-preview,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

def perform_cholesterol_ttest(csv_filepath='heart.csv'):
    """"""
    Loads the heart.csv dataset, performs an independent two-sample t-test
    on Cholesterol levels between males and females, and prints the results.
    """"""
    # 1) Load the dataset
    try:
        df = pd.read_csv(csv_filepath)
    except FileNotFoundError:
        print(f""Error: The file {csv_filepath} was not found in the current directory."")
        return

    # 2) Cr...",9,"The generated code is very good and addresses almost all requirements of the task.

**Strong Points:**
- **Correctness:** It correctly loads the data, separates males and females, performs the independent two-sample t-test, states hypotheses, prints t-statistic and p-value, and interprets the result based on the specified alpha.
- **Hypotheses:** The hypotheses are clearly stated and accurately reflect the null and alternative for a two-tailed test.
- **Interpretation:** The interpretation of the p-value against alpha is correct and leads to the right conclusion (reject/fail to reject H0).
- **Robustness:** It includes `try-except` for `FileNotFoundError` and checks if groups are empty after filtering and `dropna()`, which is good practice.
- **`equal_var=False`:** Correctly uses `equal_var=False` for Welch's t-test, which is generally more robust when variance equality is not assumed or tested.

**Areas for Minor Improvement (Why not a 10):**
- **Slightly less explicit error messages for empty groups:** While it checks for empty groups, the ground truth code gives more specific counts (e.g., ""Male Cholesterol samples: 0""). The generated code's message regarding NaNs is useful but less direct for a simple ""not enough data"" scenario. The current message implies NaNs caused the emptiness, but sometimes a group might just not exist or have only one sample.
- **Redundant `nan_policy='propagate'` comment/parameter:** Although `propagate` is the default for `ttest_ind` and mentioning it isn't wrong, the comment about `dropna()` preceding it makes it slightly redundant as `nan_policy` won't be actively used if there are no NaNs. This is very minor stylistic.
- **Dummy data creation:** While useful for local testing, including dummy data creation in a final solution for a benchmark task is generally discouraged. The prompt asked for ""only the Python code without explanations,"" and this part falls outside the core task. The comments acknowledge this, but ideally, it wouldn't be present.

Overall, it's a highly functional and robust solution conforming well to the task requirements.",gemini-2.5-pro-preview,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

def handle_outliers_wine_data():
    # Load the dataset
    try:
        df = pd.read_csv('winequality-red.csv')
    except FileNotFoundError:
        print(""Error: 'winequality-red.csv' not found in the current directory."")
        return

    # Columns to process
    columns_to_process = ['volatile acidity', 'residual sugar']
    
    # Store original statistics
    stats_before = {}
    # Store outlier counts
    outlier_counts = {}
    # Crea...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, identifies outliers using the IQR method for the specified columns, reports the number of outliers, performs winsorization (capping) by creating a new DataFrame, and compares the mean and standard deviation before and after treatment.

**Strong Points:**
- Correct implementation of IQR method for outlier detection.
- Proper use of `np.where` for winsorization.
- Clear and concise reporting of outlier counts.
- Correct calculation and comparison of statistics (mean, std dev) before and after treatment.
- Good code structure with a function `handle_outliers_wine_data` and a `if __name__ == '__main__':` block, demonstrating good practice.
- Includes error handling for `FileNotFoundError`.

**Minor Improvements:**
- The output for ""Number of outliers found"" is correct, but the Ground Truth explicitly mentioned ""Report the number of outliers found for each column"", which the generated code does, but a slight rephrasing of the print statement to match the prompt's exact wording could be considered (though it's functionally identical).
- The prompt asked ""Provide only the Python code without explanations."" The generated code includes print statements that serve as explanations of the process steps, which is fine, but the *comment* `# Alternative using clip:` is an internal comment, not an explanation to the user as per the prompt. It's a very minor point and not critical for the task's performance.

Overall, the code is highly effective and well-written for the given task.",gemini-2.5-pro-preview,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load the dataset
# Assume 'heart.csv' is in the current directory
df = pd.read_csv('heart.csv')

# Define the order for ChestPainType categories
chest_pain_order = ['TA', 'ATA', 'NAP', 'ASY']

# Ensure 'ChestPainType' is treated as a categorical variable with the specified order
# This ensures all specified categories appear on the x-axis in the correct order,
# even if some categories might have no data in...",9,"The generated code correctly loads the dataset and creates a grouped bar chart as requested. It successfully displays the counts of `HeartDisease` (0 and 1) for each `ChestPainType` category. The plot includes a title, labeled axes, and a legend, fulfilling all requirements.

**Strong points:**
- Correctly uses `sns.countplot` for grouped bar chart.
- Sets appropriate plot title, x-axis label, and y-axis label.
- Correctly orders `ChestPainType` categories using `pd.Categorical`.
- Creates a descriptive legend by mapping 0 and 1 to ""No Heart Disease (0)"" and ""Heart Disease (1)"" respectively, which is a good practice for readability.
- Uses `plt.figure(figsize=...)` and `plt.tight_layout()` for better presentation.

**Areas for minor improvement:**
- The task implicitly assumes a callable function for the code as the ground truth provides a function. The generated code is a script, which is acceptable but a function would make it more reusable and testable.
- There's no error handling for `FileNotFoundError` or other exceptions when loading the CSV, which is present in the ground truth code and is a good practice for robust code.
- The `plt.show()` call is commented out, which means the plot wouldn't actually display if the code were run as is. While it mentions the alternative `plt.savefig`, for a direct display task, `plt.show()` should be active.",gemini-2.5-pro-preview,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

def process_spotify_data(file_path='spotify_2023.csv'):
    """"""
    Loads, cleans, groups, and analyzes Spotify data from a CSV file.

    Args:
        file_path (str): The path to the Spotify CSV file.
    """"""
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        print(f""Error: '{file_path}' not found in the current directory."")
        return
    except Exception as e:
        # Fallback for potential encoding issues often seen with ...",9,"The generated code is highly functional and addresses almost all aspects of the task with robust error handling and good practices.

**Strong Points:**
- **Robust Error Handling:** The `try-except` block for `FileNotFoundError` is excellent. The added `latin1` encoding fallback is a thoughtful touch for common CSV issues, although not strictly required by the prompt, it adds to the robustness.
- **Clear Cleaning Steps:** The `in_shazam_charts` cleaning, including converting to `str` first, removing commas, and `pd.to_numeric` with `errors='coerce'`, is correctly implemented. It also correctly reports the count of NaNs.
- **Column Existence Checks:** The code checks for the existence of columns (`in_shazam_charts`, `danceability_%`, `valence_%`, `key`, `mode`) before attempting operations, which prevents `KeyError` and makes the code more resilient to variations in input data.
- **Correct Grouping and Aggregation:** The `groupby(['key', 'mode'])` and subsequent `mean()` (implicitly through `agg` with `mean`) are correctly applied for `danceability_%` and `valence_%`.
- **Modularity:** Encapsulating the logic in a function `process_spotify_data` and using `if __name__ == '__main__'` is good practice for reusability.
- **Descriptive Variable Names:** Variables like `nan_count_shazam`, `grouped_data`, `average_metrics` are well-named.

**Areas for Minor Improvement (Why not a 10):**
- **Specific Output Format for NaNs:** The ground truth code specifically asks to ""report count of NaNs"". While the generated code correctly reports it, the prompt implies ""displaying"" it as part of the direct output rather than embedded in a function that needs to be called. In an expert code evaluation context, following the exact output specification is key. The ground truth `print(f""Count of NaNs in 'in_shazam_charts' after cleaning and coercion: {nan_count_shazam}"")` is direct. The generated code places this within a function, and the print statements are internal to the function's execution rather than being explicitly returned or highly visible outside. This is a very minor stylistic point related to the ""display"" aspect of the prompt.
- **Redundant `observed=True` and `dropna=True` for common types:** For grouping by string or numeric columns like `key` and `mode`, `observed=True` and `dropna=True` are often the default or have no practical effect if there are no NaN values. While not incorrect, they can be omitted if the default behavior is sufficient, making the code slightly cleaner.
- The prompt asks to ""calculate and display"" the averages. The generated code prints them, which is acceptable, but the ground truth directly prints the result of the aggregation without an extra level of dictionary creation, which is a bit more concise for this specific aggregation task. The `agg_dict` approach is more flexible for multiple aggregation functions, but for simple means, directly passing the list of columns to `mean()` is also valid and perhaps simpler for this specific task.

Overall, the generated code is excellent, demonstrating a solid understanding of Pandas operations and good software engineering principles. The minor points are mostly about adherence to the exact ""display"" aspect and conciseness, rather than fundamental correctness or quality issues.",gemini-2.5-pro-preview,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# 1) Load the dataset
try:
    df = pd.read_csv('spotify_2023.csv', encoding='latin1') # Added encoding for potential issues
except FileNotFoundError:
    print(""Error: 'spotify_2023.csv' not found in the current directory."")
    exit()

# 2) Ensure the `streams` column is numeric
# Coerce errors to NaN
df['...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong Points:**
- **Correct Data Loading and NaN Handling:** The code correctly loads the CSV, converts the `streams` column to numeric, and handles NaNs by coercing errors and then dropping rows with NaNs in the relevant columns (streams and predictors). This aligns perfectly with the requirements.
- **Correct Feature and Target Selection:** It correctly identifies `in_spotify_playlists` and `energy_%` as features (X) and `streams` as the target (y).
- **Correct Data Splitting:** The data is split into training and testing sets with the specified 80/20 ratio and `random_state=42`.
- **Correct Model Training and Evaluation:** A Linear Regression model is correctly trained, and R2 and MSE metrics are calculated and reported.
- **Proper Imports:** All necessary libraries are imported.
- **Robustness:** The `try-except` block for file loading is good practice. The addition of `encoding='latin1'` is a cautious but sometimes necessary step for CSV files, though not explicitly required by the prompt, it doesn't detract from correctness.

**Areas for Minor Improvement (preventing a perfect 10):**
- **Unnecessary Coercion of Predictor Columns:** The loop `for col in predictor_columns: X[col] = pd.to_numeric(X[col], errors='coerce')` and subsequent `X = X.dropna(); y = y[X.index]` is redundant and potentially problematic after `df.dropna(subset=columns_for_nan_check)`. The `dropna` step already ensures that `in_spotify_playlists` and `energy_%` are clean (assuming they are numeric in the CSV or can be coerced to numeric). If these columns were truly non-numeric *after* the initial `dropna`, this secondary step would be necessary, but usually, `dropna` on the original `df` is sufficient for numerical columns. The prompt explicitly says to drop NaNs in `streams` *or predictor columns* after coercion, which is done by the first `dropna` on `df_cleaned`. Re-doing it on `X` and `y` separately can lead to unexpected behavior if `X` or `y` are modified independently. In this specific case, the `df.dropna(subset=columns_for_nan_check)` should already guarantee numeric-like features unless `in_spotify_playlists` or `energy_%` had non-numeric *strings* that were not `NaN` but would become `NaN` upon `pd.to_numeric`. The ground truth handles this by assuming the features are numeric or effectively numerical after the single `dropna`. It's a slight over-complication for the given task description, as it implies a double-cleaning.
- **Lack of Informative Print Statements:** While not strictly required by the prompt (""Provide only the Python code without explanations""), the ground truth includes helpful print statements about data loading, dropped rows, and shapes, which improve user experience and debugging. The generated code lacks these, which is a minor point from a benchmark perspective but a significant one in real-world applications.",gemini-2.5-pro-preview,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

# 1) Load the dataset
try:
    df = pd.read_csv('spotify_2023.csv')
except FileNotFoundError:
    print(""Error: 'spotify_2023.csv' not found in the current directory."")
    exit()

# 2) Combine released_year, released_month, and released_day into release_date
# Ensure the date components are suitable for pd.to_datetime
# If columns are not numeric, they might need conversion, e.g., pd.to_numeric(df['column'], errors='coerce')
# For simplicity, assuming they are alr...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, combines the date components into a single `release_date` column, correctly handles invalid dates using `errors='coerce'`, calculates `days_since_release` from the specified fixed date, and displays the required information for the first 5 tracks. The use of `pd.to_datetime` with a dictionary of date parts is an effective and robust way to create the `release_date` column. The conversion of `days_since_release` to `'Int64'` for nullable integers is also correct and aligns with best practices for handling potential NaNs.

Minor areas for improvement, preventing a perfect 10:
1.  **Redundant Column Checks (Minor):** The checks for `date_cols` (released_year, etc.) and `track_name` are good defensive programming, but for a benchmark scenario where the schema is typically fixed, they add a bit of verbosity without being strictly necessary for the core task execution. However, they don't break functionality.
2.  **Error Handling Granularity (Minor):** The `except Exception as e:` block for date conversion is a bit broad. While it catches errors, a more specific error type if an issue is expected would be slightly better practice, though `pd.to_datetime` is quite robust. The commented-out alternative approach for date string concatenation is unnecessary given the robustness of the dictionary approach used.

Overall, it's a very strong solution that effectively solves the problem.",gemini-2.5-pro-preview,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
# Assume 'youtube_trendings.csv' is in the current directory
df = pd.read_csv('youtube_trendings.csv')

# 1. Clean 'Video views' and 'Likes' columns
# Remove commas, convert to numeric, and fill NaNs with 0
# Ensure columns are treated as strings before trying to replace commas
df['Video views'] = df['Video views'].astype(str).str.replace(',', '', regex=False)
df['Video views'] = pd.to_numeric(df['Video views'], errors='coerce').fillna(0)

df['Li...",9,"The generated code is highly correct and directly addresses all requirements of the task. It successfully loads the data, cleans the 'Video views' and 'Likes' columns by removing commas, converting to numeric, and filling NaNs with 0. It also correctly fills missing 'Category' values with 'Unknown'. Finally, it groups by 'Category' and calculates the sum of cleaned 'Video views' and 'Likes'. The code is clean, concise, and uses appropriate pandas functions.

The minor point keeping it from a perfect 10 is the lack of robustness in error handling, such as checking if columns exist or handling `FileNotFoundError` for the CSV, which the ground truth includes. While the prompt implied a straightforward scenario (CSV in current directory, columns exist), in a production environment, such checks would be crucial for a more resilient solution. However, for a benchmark focused on functionality, it performs excellently.",gemini-2.5-pro-preview,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
# Assume 'youtube_trendings.csv' is in the current directory.
df = pd.read_csv('youtube_trendings.csv')

# 2) For the `Likes` column:
#    - remove any commas
#    - convert it to a numeric type (e.g., float)
#    - if conversion fails for any value, treat that value as NaN
#    - then fill all resulting NaNs with 0.
df['Likes'] = df['Likes'].astype(str)  # Ensure string type for .str accessor
df['Likes'] = df['Likes']....",9,"The generated code correctly addresses all requirements of the task. It properly loads the dataset, cleans the 'Likes' column by handling commas, converting to numeric, and filling NaNs. It correctly fills missing 'Category' values. The code accurately performs the independent two-sample t-test between 'Music' and 'Sports' categories for the cleaned 'Likes'. It clearly states the null and alternative hypotheses, prints the t-statistic and p-value, and provides a correct interpretation based on the significance level.

The only minor point for improvement, which is a nuance, is that the `nan_policy='propagate'` argument in `ttest_ind` is not strictly necessary here because the `Likes` column has already been cleaned and `NaN`s filled with 0. While it doesn't cause an error, it's a redundant argument given the prior cleaning steps. Additionally, the ground truth code includes robust error handling for `FileNotFoundError` and cases where `music_likes` or `sports_likes` might be too small, which is generally good practice for production-ready code. The generated code also includes a check for `pd.isna(p_value)` related to group size, which is good. Overall, it's a very strong and correct solution.",gemini-2.5-pro-preview,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

def feature_engineer_youtube_data(csv_path='youtube_trendings.csv'):
    """"""
    Loads the youtube_trendings.csv dataset, performs feature engineering,
    and displays specified columns.
    """"""
    try:
        # 1) Load the dataset
        df = pd.read_csv(csv_path)
    except FileNotFoundError:
        print(f""Error: The file {csv_path} was not found in the current directory."")
        return

    # 2) Clean 'Likes' and 'Dislikes' columns
   ...",10,"The generated code is excellent and comprehensively addresses all the requirements of the task.

**Strong points:**
*   **Correctness:** It correctly loads the dataset, cleans the `Likes` and `Dislikes` columns (handling commas, non-numeric values as NaNs, and filling NaNs with 0), calculates `TotalEngagement` and `LikeDislikeRatio` precisely as specified.
*   **Robustness:** The code includes proper error handling for `FileNotFoundError` when loading the CSV.
*   **Column Handling:** It checks if 'Likes', 'Dislikes', and 'Video' columns exist, providing warnings if they are missing and handling missing `Likes`/`Dislikes` by creating them filled with zeros to prevent subsequent errors, which is a very robust design choice.
*   **Code Quality:** The code is well-structured within a function, uses appropriate imports (`pandas`, `numpy`), and includes a helpful docstring. The use of `regex=False` in `str.replace` for simpler comma removal is a good detail.
*   **Completeness:** All five steps outlined in the task are perfectly implemented and the output format is exactly as requested (displaying `Video`, `TotalEngagement`, and `LikeDislikeRatio`).
*   **Testability:** The inclusion of a `if __name__ == '__main__':` block with dummy CSV creation for local testing is a significant advantage and demonstrates a thorough approach to code development.

**Minor or no improvements needed:**
*   The use of `numpy` import is not strictly necessary as `pandas` handles all the operations. However, it's a common data science import and doesn't negatively impact the code.
*   The `if df[col].dtype == 'object':` check before `str.replace` is technically redundant if `pd.to_numeric` is going to be used with `errors='coerce'` anyway, as `pd.to_numeric` can handle various input types. However, it doesn't harm the code and might be seen as defensive programming.

Overall, this is a highly professional and correct solution that meets all requirements and demonstrates good coding practices.",gemini-2.5-pro-preview,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np # Often useful, though not strictly necessary for this exact solution

def clean_and_aggregate_mobile_data(file_path='mobiles_2025.csv'):
    """"""
    Loads, cleans, groups, and aggregates mobile data from a CSV file.
    """"""
    try:
        # 1. Load the dataset
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        print(f""Error: The file '{file_path}' was not found in the current directory."")
        # Create a dummy DataFram...",7,"The generated code has some good points, but also significant issues that prevent it from being a strong solution.

**Strong Points:**
*   Correctly uses `pd.read_csv`.
*   Correctly handles cleaning of 'Launched Price (USA)' by removing 'USD ' and ',' and converting to numeric with `errors='coerce'`.
*   Correctly handles cleaning of 'Screen Size' by removing ' inches' and converting to numeric with `errors='coerce'`.
*   Correctly uses `groupby()` on 'Company Name' and 'Launched Year'.
*   Correctly calculates the mean for the cleaned columns within each group.
*   Includes `astype(str)` before string operations, which is good for robustness.

**Areas for Improvement (Why it's not higher):**

1.  **Redundant and Confusing Structure:** The most glaring issue is the convoluted structure. It defines a function `clean_and_aggregate_mobile_data` that includes error handling (including creating dummy data if the file is not found) and then, *below* the `if __name__ == '__main__':` block, it duplicates the *entire* logic for loading, cleaning, grouping, and displaying outside of the function. This is highly redundant and confusing. The prompt asks for ""only the Python code,"" implying a single, coherent script. The `if __name__ == '__main__':` block should ideally just call the function, or the logic should simply exist at the script level without a function if unnecessary complexity is to be avoided (as implied by the brevity of the ground truth).
2.  **Unnecessary Imports/Comments:** It imports `numpy as np` but never uses it. The extensive commenting about ""assuming the CSV file is in the current directory"" and then proceeding to implement (and then comment out the use of) dummy data or file not found handling directly contradicts the strict prompt ""assume the CSV file is in the current directory"" and ""Provide only the Python code without explanations."" The prompt wants the functional code, not a discussion about error handling strategies.
3.  **Variable Naming for Cleaned Columns:** The ground truth code created *new* columns for the cleaned data (`Cleaned Price (USD)`, `Cleaned Screen Size (inches)`), which is generally a safer practice as it preserves the original data and makes the cleaning process explicit. The generated code overwrites the original columns (`df[price_col] = ...`, `df[screen_col] = ...`). While technically correct for the final aggregation, creating new columns for cleaned data is often preferred for clarity and debugging, and it was a subtle requirement reflected in the ground truth's naming of the aggregations. This is a minor point but contributes to the overall clarity.
4.  **No `reset_index()`:** The generated code's aggregation results in a MultiIndex DataFrame. While valid, the ground truth uses `reset_index()` to convert the group keys (`Company Name`, `Launched Year`) back into regular columns, making the output flatter and often easier to work with or view. This wasn't explicitly asked for, but it's a common and usually desired last step for grouped aggregations.
5.  **Output Clarity:** The ground truth renames the aggregated columns to `Average_Launched_Price_USD` and `Average_Screen_Size_inches` for better clarity in the final output. The generated code uses the original column names as the aggregation names, which works, but is less descriptive. The generated code does have a commented-out section for renaming, indicating the thought was there, but it wasn't implemented in the final active code block.

To achieve a higher score, the generated code needs to be streamlined into a single, clean, and direct solution as requested, avoiding the verbose and contradictory structure.",gemini-2.5-pro-preview,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

# 1. Load the dataset
try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: 'mobiles_2025.csv' not found in the current directory."")
    # Create a dummy dataframe for the rest of the script to run without error
    data = {
        'Model Name': [f'Phone {i}' for i in range(15)],
        'Front Camera': ['12MP / 4K', '8MP', '16 MP (f/2.0)', 'No Camera', '32MP + 8MP', '50MP', '10.8 MP', '13 MP', '48MP / 1080p', '12MP', ...",9,"The generated code successfully completes almost all aspects of the task.

**Strong Points:**
*   **Correctness:** The `clean_camera_spec` function correctly extracts the first numerical MP value using regex and handles cases where no digits are found by returning 0. It also correctly handles non-string inputs by returning 0.
*   **Completeness:** All four main requirements (load data, define cleaning function, apply it, create total, display first 10 rows) are addressed.
*   **Code Quality:** Imports are correct. The function is well-defined and applied appropriately.
*   **Robustness:** The addition of a dummy DataFrame when `mobiles_2025.csv` is not found is a good practice for making the script runnable even without the actual file, which is helpful for development and testing.

**Areas for Minor Improvement:**
*   **Output Formatting:** The ground truth uses `.to_string()` for displaying the head of the DataFrame, which often provides cleaner, more readable output in a console compared to the default `print(df.head(10))` which can sometimes truncate columns or have less consistent alignment. While not a critical error, it's a small polish.
*   **Docstrings/Comments:** The `clean_camera_spec` function has a good docstring, but the ground truth adds more comprehensive docstrings and comments for clarity, particularly explaining the `try-except` block and the purpose of each main step. The generated code is clear enough, but more detailed comments align better with ""expert data scientist"" level code.

Overall, it's a very good solution that correctly fulfills the task requirements.",gemini-2.5-pro-preview,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# 1) Load the dataset
try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: 'mobiles_2025.csv' not found. Make sure the file is in the current directory."")
    exit()

# 2) Clean data
# Helper function for robust cleaning and conversion
def clean_and_convert(series, remov...",9,"The generated code is highly correct and complete, addressing almost all aspects of the task. It successfully loads the data, cleans the specified columns by removing non-numeric characters and converting to numeric, handles errors by coercing to NaN and then dropping rows, splits the data, trains a Linear Regression model, and evaluates it with R-squared and MAE.

**Strong points:**
- Correctly implements all steps of the task.
- Uses `try-except` for file loading.
- Effectively cleans the specified columns, including handling multiple patterns for 'Launched Price (USA)'.
- Correctly uses `errors='coerce'` with `pd.to_numeric` and `dropna(subset=...)`.
- Proper use of `train_test_split` with `random_state`.
- Correctly trains and evaluates the Linear Regression model.
- Includes all necessary imports.

**Minor areas for improvement (to reach a perfect 10):**
- The `clean_and_convert` function is created but then the 'Launched Price (USA)' cleaning is done separately and less generically. The pricing logic for 'USD ' and ',' could have been integrated into the `clean_and_convert` function by allowing `remove_strings` to be a list of lists or by processing it similarly to the ground truth's `details['pattern']`. This makes the code slightly less uniform for cleaning different columns.
- The `print` statements for evaluation metrics could use f-strings for better formatting (e.g., `:.4f` for R2 and `:.2f` for MAE as in the ground truth) to match a standard output format for these metrics, although this is a minor stylistic point.

Overall, it's a very strong submission that meets the requirements effectively.",gemini-2.5-pro-preview,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

ALPHA = 0.05

def interpret_cramers_v(v, df_min):
    """"""Interprets Cramer's V based on df_min = min(k-1, r-1).""""""
    if df_min <= 0: 
        return ""interpretation not applicable (df_min <= 0)""
    
    # Thresholds vary by source; these are common guidelines.
    if df_min == 1:
        thresholds = [0.10, 0.30, 0.50]
    elif df_min == 2:
        thresholds = [0.07, 0.21, 0.35]
    elif df_min == 3:
        thresholds...",9,"The generated code is excellent, demonstrating a strong understanding of statistical analysis and good programming practices.

**Strong Points:**
- **Correctness:** All three statistical tests (Chi-square, T-test, ANOVA) are correctly implemented.
- **Effect Size Calculation:** Cramer's V, Cohen's d, and Eta-squared are all calculated and reported correctly.
- **Interpretation:** The inclusion of `interpret_cramers_v`, `interpret_cohens_d`, and `interpret_eta_squared` functions is a major plus, providing qualitative interpretations of effect sizes, beyond just numerical values. This goes above and beyond the typical requirements.
- **Robustness:** Handles missing 'Age' and 'Fare' values gracefully using `dropna`.
- **Clear Output:** The output is well-formatted and includes all required information (p-values, test statistics, conclusions, effect sizes).
- **Modularity:** Encapsulating the logic within a `main` function improves organization.
- **Assumptions (T-test):** Uses `equal_var=False` for the t-test, which is generally a safer default (Welch's t-test) when equality of variance cannot be assumed or tested. The ground truth code performs a Levene's test to decide, which is also a valid approach, but opting for Welch's by default is a good choice if no prior check is made.
- **Edge Cases for Effect Sizes:** The code correctly handles cases where Cramer's V cannot be calculated (e.g., `df_min_chi2` is 0) and provides better handling for Cohen's d when standard deviations are zero (though the `s_pooled_welch_approx` calculation is slightly non-standard for pooled SD, it's a reasonable approximation for Cohen's d with Welch's).

**Areas for Minor Improvement (to reach a perfect 10):**
- **Cramer's V `df_min` calculation:** The ground truth code calculates `min_dim = min(contingency_table.shape) - 1`. The generated code uses `df_min_chi2 = min(r_chi2 - 1, k_chi2 - 1)`. Both are effectively calculating the same thing (min(rows-1, cols-1)). The `phi2` calculation for Cramer's V in the generated code (`phi2 = chi2_stat / n_chi2`) is standard, then Cramer's V is `sqrt(phi2 / df_min_chi2)`. The ground truth calculates `np.sqrt(chi2_stat / (n * min_dim))`. Both formulas are correct and equivalent. The generated code's implementation of `interpret_cramers_v`, while conceptually good, relies on hardcoded thresholds that also depend on `df_min`, which might be too specific (general guidelines for Cramer's V are often less dependent on `df_min` for interpretation or use different interpretation schemes). However, this is more a nuance of statistical interpretation than a code error.
- **T-test equal_var assumption:** The ground truth explicitly performs Levene's test to determine `equal_var`. While the generated code defaults to `equal_var=False` (Welch's t-test), which is robust, discussing this explicit choice or optionally including Levene's test would align more closely with common statistical practice for hypothesis testing assumptions.
- **ANOVA Post-hoc:** While mentioned in the conclusion, explicitly stating that post-hoc tests are needed if ANOVA is significant is good, but the code doesn't include them. The prompt didn't ask for them, but it's a natural next step in ANOVA.
- **Minor Variable Naming:** `ALPHA` is good as a constant, but `interpret_cramers_v` uses `df_min` as a parameter name, which could be more descriptive for its role in Cramer's V (e.g., `min_of_dimensions_minus_one`). This is a very minor point.

Overall, the generated code is highly functional, well-structured, and provides excellent value with the interpretive functions for effect sizes. It successfully meets all requirements of the prompt and adds valuable extensions.",gemini-2.5-pro-preview,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
try:
    df = pd.read_csv('imdb_top_250.csv')
except FileNotFoundError:
    print(""Error: 'imdb_top_250.csv' not found in the current directory."")
    exit()

# Clean 'Runtime' column: remove ' min' and convert to numeric
if 'Runtime' in df.columns and df['Runtime'].dtype == 'object':
    df['Runtime'] = df['Runtime'].str.replace(' min', '', regex=False).astype(int)
else:
    # Assuming Run...",8,"The generated code correctly loads the dataset, performs the required grouping and aggregation, and generates the specified plot with appropriate labels and title. It also includes good error handling for file not found and robust handling for the 'Year' and 'Runtime' column cleaning.

Strong points:
- Correctly loads the CSV and handles `FileNotFoundError`.
- Appropriately cleans the 'Runtime' column by removing ' min' and converting to integer, and uses `pd.to_numeric` with `errors='coerce'` as a fallback.
- Implements robust cleaning for the 'Year' column using regex to extract four-digit years and handles `NaN` values after extraction, which is crucial for typical IMDb datasets.
- Correctly groups the data by `Year` and calculates the mean for `Rating` and `Runtime` using `groupby().agg()`.
- Successfully creates a line plot using seaborn (which is acceptable as a plotting library) showing the trend of average `Rating` over `Year`.
- Includes a title and labeled axes as required.
- Uses `plt.tight_layout()` for better plot presentation.

Areas for improvement to achieve a higher score (e.g., 9 or 10):
- The `Runtime` cleaning could benefit from a more explicit regex similar to the ground truth to handle cases where there might be other characters or non-integer values more gracefully (e.g., `re.search(r'\d+', x)`). The current `str.replace(' min', '')` assumes a fixed format and might not be as robust.
- The `Year` cleaning is good, but the subsequent `elif not pd.api.types.is_numeric_dtype(df['Year']):` block is a bit redundant after the initial `str.extract` and `astype(int)` part, as the column should already be numeric. It indicates a slight over-defensiveness in cleaning that could be streamlined.
- The task implicitly asks for Python code ""without explanations"" in the final output, but the generated code includes comments that are somewhat explanatory. For a final submission, these would ideally be removed or condensed into docstrings for functions.
- The `exit()` calls are generally discouraged within functions or main script blocks when a more graceful error propagation (e.g., raising an exception, returning an error status) might be preferable in larger applications, though for a simple script, it's acceptable.
- The ground truth includes an `analyze_imdb_data` function, encapsulating the logic, which is good practice for reusability. The generated code places the logic directly in the global scope. While not strictly required by the prompt, it would be a micro-optimization for code structure.",gemini-2.5-pro-preview,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

def perform_imdb_ttest():
    # 1. Load the dataset
    try:
        df = pd.read_csv('imdb_top_250.csv')
    except FileNotFoundError:
        print(""Error: 'imdb_top_250.csv' not found. Make sure the file is in the current directory."")
        return

    # Ensure 'Year' and 'Rating' are numeric, handling potential errors
    if not pd.api.types.is_numeric_dtype(df['Year']):
        df['Year'] = pd.to_numeric(df['Year'], errors='coerce')
 ...",9,"The generated code is very good and correctly addresses almost all requirements of the task.

**Strong points:**
*   **Correctness:** Loads data, correctly creates groups, performs t-test, states hypotheses, prints results, and interprets the p-value against alpha.
*   **Completeness:** Addresses all specified steps, including data loading, group creation, t-test execution, hypothesis stating, result printing, and interpretation.
*   **Robustness:** Includes `try-except` for `FileNotFoundError`, handles non-numeric 'Year' and 'Rating' columns using `pd.to_numeric` with `errors='coerce'`, and `dropna`. It also checks if groups have sufficient data before performing the t-test, which is good practice.
*   **Code Quality:** Well-structured with a main function `perform_imdb_ttest` and an `if __name__ == '__main__':` block, which is excellent for reusability and execution.
*   **Hypothesis Statement:** Clearly states the null and alternative hypotheses, including the statistical notation for mean equality/inequality.
*   **Interpretation:** Correctly interprets the p-value relative to alpha and provides clear conclusions.

**Areas for minor improvement (why it's not a 10):**
*   **`equal_var` parameter:** The prompt asked for an ""independent two-sample t-test"" without specifying assumptions about variance. The ground truth used `equal_var=True` (Student's t-test), which is the standard assumption if not otherwise stated. The generated code uses `equal_var=False` (Welch's t-test), which is generally robust but might produce different results and wasn't explicitly asked for. While Welch's is often a safer default, it subtly deviates from what might be implied by a basic ""t-test"" request without further context. Adding a comment about the choice of `equal_var=False` would be beneficial, or defaulting to `True` if no variance assumption is specified.
*   **`nan_policy='omit'` in `ttest_ind`:** While good for robustness, the `df.dropna(subset=['Year', 'Rating'], inplace=True)` should ideally ensure no NaNs are passed to the t-test, making `nan_policy='omit'` redundant in this specific scenario. It doesn't hurt, but suggests a lack of full confidence in the preceding `dropna`.
*   **Redundant `is_numeric_dtype` checks:** The `pd.to_numeric(errors='coerce')` implicitly handles non-numeric data types by converting them to numeric or `NaN`. The explicit `if not pd.api.types.is_numeric_dtype(...)` checks are not strictly necessary before `pd.to_numeric` as `pd.to_numeric` will attempt the conversion anyway.

These are minor points. The code is highly functional and well-written.",gemini-2.5-pro-preview,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

def create_decade_column(df):
    """"""
    Creates a 'Decade' column from the 'Year' column.
    """"""
    # Ensure 'Year' column is numeric. Sometimes it might be loaded as object if there are non-numeric chars.
    # For this specific problem, assuming 'Year' is clean or can be coerced.
    # If 'Year' might have non-numeric parts like ""(1994)"" we'd need more cleaning.
    # For now, assuming it's a simple year string or integer.
    df['Year'] = pd.to_numeric(df['Y...",9,"The generated code correctly addresses all requirements of the task: it loads the CSV, creates the `Decade` column accurately, and displays the requested columns for the first 10 movies. The `create_decade_column` function properly handles potential non-numeric 'Year' entries by coercing to numeric and dropping NaNs, which is good practice. The use of `df.copy()` before processing is also a good defensive programming technique to avoid `SettingWithCopyWarning`.

A minor point for improvement, compared to the ground truth, is the error handling. While it correctly catches `FileNotFoundError` and provides a fallback with a dummy DataFrame, the ground truth's error handling is more comprehensive, including checks for `KeyError` and a general `Exception` catch, which makes it more robust for unexpected issues with the dataset structure itself. Additionally, the `print` statements during display are slightly less descriptive than the ground truth's, which explicitly labels the output. However, these are minor details and do not detract from the core functionality.",gemini-2.5-pro-preview,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd
import numpy as np

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'penguins.csv' not found in the current directory."")
    exit()

# 2) For the numerical columns, impute any missing values (NA) using the mean
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']

# Check if all specified numerical columns exist
missing_num_cols = [col for col in numerical_cols if col...",7,"The generated code correctly performs the imputation and aggregation steps for the most part. The strong points include:
- Correctly loading the CSV and handling `FileNotFoundError`.
- Correctly identifying numerical columns and performing mean imputation.
- Correctly identifying the categorical `sex` column and performing mode imputation.
- Correctly grouping by `species` and `sex`.
- Correctly calculating the average `body_mass_g`.
- Including explicit checks for column existence before operations, which adds robustness.
- Handling of potential alternative names like `bill_length_mm` (though this was an over-engineering for the given task and not strictly necessary).

However, there are several areas for improvement that prevent a higher score:
1.  **Over-engineering/Unnecessary Complexity**: The most significant issue is the extensive and complex code added to handle potential missing numerical columns and their alternative names (`bill_length_mm` vs `culmen_length_mm`). The problem statement implies working with the exact column names provided (`culmen_length_mm`, etc.). This adds a lot of unnecessary code, makes it harder to read, and also introduces bugs in how `numerical_cols` are updated and re-filtered after potential renames (e.g., `numerical_cols = [c if c != 'culmen_length_mm' else 'bill_length_mm' for c in numerical_cols]` and then further re-filtering). This complexity was not requested and actually detracts from the clarity and correctness of the solution for the specified task.
2.  **Redundant and Potentially Buggy Column Handling**: The `numerical_cols` list is redefined multiple times, and the logic regarding `final_missing_num_cols` and `numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']` after the attempted renames is confusing and could lead to issues if the original names were truly missing and alternatives were used. The problem expected a direct use of the specified columns.
3.  **Handling of `sex` column non-standard missing values (`.`)**: While a thoughtful addition, the problem specifically stated ""impute any missing values (NA)"". Standard `pandas` operations using `fillna` and `isnull()` implicitly handle `NaN` and `None`. The explicit `replace('.', np.nan, inplace=True)` is not typically needed unless the dataset explicitly uses `.` as a missing value marker, which `penguins.csv` does not for the `sex` column. This adds unnecessary code.
4.  **Minor Logic Flow**: The `grouped_data = df.groupby(['species', 'sex'])` is done before the final `body_mass_g` check, but the actual calculation is inside an if block, which is fine, but the flow could be slightly more streamlined.

The core logic is correct, but the added complexity for edge cases not directly requested by the prompt makes the code harder to read, maintain, and potentially introduces subtle bugs related to column name handling. The ground truth code handles `KeyError` for missing columns more gracefully without over-engineering detection and renaming logic.",gemini-2.5-pro-preview,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

def perform_penguin_ttest():
    # 1) Load the dataset
    try:
        df = pd.read_csv('penguins.csv')
    except FileNotFoundError:
        print(""Error: 'penguins.csv' not found in the current directory."")
        return

    # 2) Filter the data to include only records where `species` is 'Adelie' or 'Gentoo'.
    df_filtered = df[df['species'].isin(['Adelie', 'Gentoo'])].copy()

    # 3) Within this filtered dataset, remove any rows whe...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong Points:**
- **Correctness:** The code correctly loads the data, filters by species, handles NAs appropriately using `dropna` on the specified subset, and performs the two-sample t-test.
- **Hypotheses:** The null and alternative hypotheses are clearly stated and accurately convey the intent of the test.
- **T-test Parameters:** It correctly uses `equal_var=False` for the t-test, which is a robust choice (Welch's t-test) when variances are not assumed to be equal, and a good default if not explicitly stated.
- **Interpretation:** The interpretation of the p-value against the alpha level is correct and leads to the right conclusion about rejecting or failing to reject the null hypothesis.
- **Error Handling:** Includes basic error handling for `FileNotFoundError`.
- **Code Structure:** Encapsulating the logic within a function (`perform_penguin_ttest`) and using `if __name__ == '__main__':` is good practice.

**Areas for Minor Improvement (for a perfect 10):**
- **Consistency in P-value Printing:** The prompt asked to ""Print the calculated t-statistic and the p-value."" While the t-statistic is formatted to 4 decimal places, the p-value is initially printed without specific formatting. Then, it introduces checks for very small p-values and uses scientific notation for interpretation. A consistent `:.4f` or similar formatting for the initial p-value print would match the t-statistic and be more aligned with a standard report of these values, even if the subsequent interpretations use different formatting.
- **Redundant `species` in `dropna` subset:** While specified in the prompt, including `'species'` in `subset` for `dropna` is somewhat redundant after the initial `isin` filter, as `species` will not have NA values in the filtered dataframe. The ground truth code also mentioned this point. This is a very minor point and doesn't affect correctness.
- **Clarity in `alpha` Definition:** The `alpha` variable is defined within the `interpret` section. Defining it at the top level (similar to `ALPHA` in the ground truth) would make it clearer as a constant parameter for the entire test.

Overall, this is an excellent solution that meets the requirements very well.",gemini-2.5-pro-preview,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1) Load the dataset
# Assume the CSV file is in the current directory.
df = pd.read_csv('penguins.csv')

# 2) Select the features X using columns: `culmen_length_mm`, `culmen_depth_mm`, 
#    `flipper_length_mm`, `body_mass_g`. The target y is the `species` column.
featur...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task. It successfully loads the data, selects features and target, correctly handles missing values using `dropna` on the specified subset of columns, encodes the target variable, splits the data appropriately, trains a Logistic Regression model with `max_iter=1000`, and reports the accuracy.

The only minor area for improvement is the lack of robust error handling for file loading (e.g., `FileNotFoundError`) and missing columns, which is present in the ground truth code. While not strictly requested in the prompt's explicit requirements list, robust error handling is good practice for production-ready code. However, for a data science benchmark focusing on the core ML pipeline, this is a very minor omission. The code is well-structured, clear, and follows best practices for a typical data science workflow.",gemini-2.5-pro-preview,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
# Assuming 'penguins.csv' is in the current directory.
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'penguins.csv' not found. Please ensure the file is in the current directory."")
    exit()

# 2) Drop rows where `body_mass_g` or `species` is NA
# This ensures that calculations and plots are based on complete data for these critical columns.
df_clean...",10,"The generated code is excellent and fulfills all requirements of the task.

**Strong points:**
*   **Correctness:** All steps (loading, NA handling, aggregation, plotting) are implemented correctly as per problem statement.
*   **Clarity and Readability:** The code is well-commented, explaining each step effectively. Variable names are descriptive.
*   **Robustness:** Includes a `try-except` block for `FileNotFoundError` during data loading, which is good practice.
*   **Proper Libraries:** Uses `pandas` for data manipulation and `matplotlib.pyplot` and `seaborn` for plotting, as specified.
*   **Specific Requirements Met:**
    *   Loads `penguins.csv`.
    *   Correctly drops rows where `body_mass_g` or `species` is NA using `dropna(subset=...)`.
    *   Calculates the average `body_mass_g` for each `species` using `groupby().mean().reset_index()`.
    *   Generates a bar plot using `seaborn.barplot`.
    *   Includes a clear title (`Average Body Mass by Penguin Species`) and labeled axes (`Species`, `Average Body Mass (g)`).
    *   Uses `plt.figure(figsize=...)` and `plt.tight_layout()` for better plot presentation.

**Minor observations (not deductions):**
*   The ground truth code includes additional print statements for debugging/verification (e.g., initial/final shape, NA counts), which are helpful in a debugging context but not explicitly required for task completion. The generated code focuses purely on the functional requirements, which is also perfectly acceptable for a direct task solution.
*   The ground truth code parameterizes column names and paths, which can be beneficial for larger, more complex projects, but for a single-task script like this, hardcoding the strings is fine and perhaps simpler.

Overall, the generated code is a high-quality solution that perfectly addresses the prompt.",gemini-2.5-pro-preview,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1) Load the dataset
# Assume 'penguins.csv' is in the current working directory.
df = pd.read_csv('penguins.csv')

# 2) Drop rows where `culmen_length_mm`, `culmen_depth_mm`, `flipper_length_mm`, or `body_mass_g` are NA.
columns_to_clean_na = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df = df.dropna(subset=columns_to_clean_na)

# 3) Create a new feature `culmen_area`
# culmen_a...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, handles NA values as specified, creates the new `culmen_area` feature, calculates the Pearson correlation matrix for the correct columns, and visualizes it as an annotated heatmap with the specified title. The code is well-structured, uses appropriate libraries, and includes comments that explain each step.

**Strong points:**
- All steps are correctly implemented as per the task description.
- Correct use of `dropna(subset=...)` for handling NAs.
- Correct creation of the `culmen_area` feature.
- Correct calculation of the correlation matrix for the specified columns.
- The heatmap visualization is accurate and well-formatted with `annot=True`, `cmap='coolwarm'`, `fmt="".2f""`, and the specified title.
- Good use of `plt.figure(figsize=...)`, `plt.xticks` rotation, `plt.yticks` rotation, and `plt.tight_layout()` for better visualization.
- The `vmin` and `vmax` for the heatmap are explicitly set to -1 and 1, which is good practice for correlation heatmaps.

**Potential minor improvements (not critical for the task but good practice):**
- **Error Handling:** The ground truth code includes robust error handling for `FileNotFoundError` when loading the CSV and checks if the DataFrame becomes empty after dropping NAs. The generated code assumes the file exists and that data will remain, which is common in benchmark scenarios but less robust in production.
- **`df.copy()`:** The ground truth uses `.copy()` after `dropna` (`df.dropna(...).copy()`) to explicitly create a new DataFrame and avoid potential `SettingWithCopyWarning` if subsequent operations were to modify the DataFrame in place. While not strictly necessary for this specific sequence of operations where `df` is reassigned, it's a good practice to be aware of.

Overall, the generated code is highly effective and correct for the primary task requirements.",gemini-2.5-pro-preview,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd

# 1) Load the dataset using pandas.
# Assume the CSV is in the current directory.
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'penguins.csv' not found in the current directory."")
    print(""Please make sure the dataset file is available."")
    # Create a dummy DataFrame to allow the rest of the script to run for demonstration if needed,
    # or exit. For this problem, let's allow it to fail if file not found as per prompt....",9,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
- **Data Loading:** Correctly loads the CSV and includes basic error handling for `FileNotFoundError`.
- **Feature Selection:** Correctly identifies and selects the specified features.
- **Numerical Imputation:** Accurately imputes missing numerical values with the mean of their respective columns.
- **`sex` column handling:** Correctly imputes 'Unknown' for NA values and then maps 'MALE', 'FEMALE', and 'Unknown' to the specified integer values.
- **One-Hot Encoding:** Correctly applies one-hot encoding to the `island` column.
- **Output:** Prints the correct shape and the first 5 rows of the processed feature matrix.
- **Code Clarity:** The code is well-structured and easy to understand with comments explaining each step.

**Areas for improvement (minor points):**
- **Target Variable:** The task explicitly mentions the target variable `species` and that the goal is to predict it, implying it should be handled/separated. While the instructions for *processing features* don't strictly require `y` to be separated, the prompt implies ""prepare the penguins.csv dataset for a classification task"" and using the target `species`. The generated code only processes `X` and does not explicitly separate `y` (the `species` column) from `X` or mention it in the final output, which is a minor omission given the classification context. The ground truth code correctly separates `X` and `y`.
- **Robustness in `sex` mapping:** The ground truth code includes a fallback `fillna(-1)` for unmapped values in 'sex' after `map`, which adds a layer of robustness. While not strictly required by the prompt's wording (""map 'MALE' to 0, 'FEMALE' to 1, and 'Unknown' to -1""), it's a good practice and addresses potential edge cases if `sex` had other unexpected string values. The generated code correctly points this out as a ""Note,"" demonstrating awareness, but doesn't implement it.
- **Redundant comment:** `target_col = 'species' # Defined for clarity, not processed as per requirements for X` while technically true, it highlights that the target isn't being kept, which is a very minor point.

Overall, the generated code is highly functional and accurate given the specific processing steps requested for the features. The only significant difference from a perfect score is the lack of explicit target variable separation and handling, which is crucial for preparing a dataset *for a classification task*.",gemini-2.5-pro-preview,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004'...",9,"The generated code correctly performs all the requested tasks: creating the DataFrame, calculating total spending per customer, identifying top 3 customers, and calculating total quantity sold per category. The import is correct, and the code structure is logical and easy to follow.

Minor improvement: The ground truth code added intermediate print statements that show the process and intermediate results, which makes the output more comprehensive and user-friendly, especially in a benchmark evaluation context. While not strictly required by the prompt, it significantly enhances clarity for evaluation. The ground truth also formatted the currency output, which is a nice touch for presentation. The generated code uses `sort_values().head()` which is functionally equivalent to `nlargest()` used in the ground truth and equally valid.",gemini-2.5-pro-preview,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.metrics import (
    accuracy_score, 
    precision_sco...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Comprehensive Preprocessing:** The code intelligently handles missing values using `SimpleImputer` within pipelines, which is a robust approach. It also performs excellent feature engineering (FamilySize, IsAlone, Title extraction and consolidation), which is a significant improvement over the ground truth and likely leads to a better model.
- **Robust Pipeline Usage:** Excellent use of `Pipeline` and `ColumnTransformer` for bundling preprocessing and model training. This ensures proper data leakage prevention during cross-validation.
- **Detailed Evaluation:** All required metrics (accuracy, precision, recall, f1, ROC-AUC) are calculated and displayed.
- **Clear Visualizations:** Both a confusion matrix and ROC curve are generated correctly. The addition of plotting feature coefficients is a nice touch.
- **Proper Cross-Validation:** Uses `StratifiedKFold` and `cross_val_score` for a robust assessment of model stability, which is a good practice for imbalanced datasets.
- **Code Structure and Readability:** The code is well-organized with clear comments and logical steps.

**Areas for Improvement (minor):**
- **Coefficients for One-Hot Encoded Features (drop='first'):** The generated code uses `drop='first'` for `OneHotEncoder`. While this is a valid technique to prevent multicollinearity, it makes the interpretation of coefficients slightly more complex as one category is implicitly the baseline. The ground truth's approach (`handle_unknown='ignore'` without `drop='first'` and then using `get_feature_names_out()` directly) might be easier for a direct interpretation of all categories. The generated code's `all_feature_names` reconstruction is correct but slightly more manual.
- **Implicit Dropping of Features:** The generated code manually drops `PassengerId` and `Ticket` at the beginning, which is fine, but the `remainder='drop'` in `ColumnTransformer` handles this more generically. The `Cabin` column is also handled via feature engineering into `HasCabin`, but the original `Name` column could have been directly dropped by the ColumnTransformer if not used for `Title` extraction. The current approach is more sophisticated but could be simplified if `Title` feature engineering was not a requirement.
- **Interpretation of Coefficients:** While the code plots and states that positive/negative coefficients increase/decrease log-odds, a slightly more detailed explanation (like the ground truth's mention of comparing to an omitted baseline for one-hot encoded features) would be beneficial, especially given `drop='first'`.

Overall, this is a very strong submission, demonstrating advanced data science practices like thoughtful feature engineering and robust use of Scikit-learn pipelines. The minor suggestions are more about refinement than correction.",gemini-2.5-pro-preview,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

# Data for the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']),
        'Ticker': ['TECHCO']*8,
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
        'Close': [152.0, 151.0, 153.5, 155.0,...",10,"The generated code is entirely correct and complete. It successfully creates the DataFrame, sets the 'Date' as the index, correctly calculates the 3-day Simple Moving Average for 'Close' prices (handling NaNs for initial days), calculates the percentage change in 'Volume' (handling NaN for the first day), and prints the resulting DataFrame. The code quality is excellent, with proper imports and clear steps. It perfectly matches the requirements outlined in the task.",gemini-2.5-pro-preview,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]}
df ...",10,"The generated code is excellent. It correctly performs all the requested tasks:

1.  **DataFrame Creation**: The DataFrame is created accurately with the provided data.
2.  **'Score_Improvement' Calculation**: The new column is correctly calculated as `Final_Score - Midterm_Score`.
3.  **Average 'Score_Improvement' per Subject**: The `groupby('Subject')` and `mean()` operations are used appropriately to calculate and print the average improvement for each subject.
4.  **Student Identification**: The code accurately filters the DataFrame based on `Final_Score > 90` and `Attendance_Rate >= 0.95` and then extracts the `StudentID` of the students who meet these criteria. It also handles the case where no students meet the criteria gracefully.

The code quality is good, using standard pandas operations, and it includes necessary imports. The output is clear and well-formatted. All requirements of the task have been met completely and correctly.",gemini-2.5-pro-preview,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# 1) Filter the DataFrame
filtered...",9,"The generated code correctly defines the DataFrame, filters it accurately based on the specified conditions (""Department"" is ""Tech"" and ""PerformanceScore"" is greater than 3), and correctly calculates the average ""ProjectsCompleted"" for the filtered subset. It also includes a good check for an empty filtered DataFrame to prevent errors, which was an important part of the ground truth code's robustness.

The only minor area for improvement to achieve a perfect score would be the formatting of the printed output for the average. The ground truth code formats the output to two decimal places (`.2f`), which is good practice for averages. The prompt asked to ""calculate and print"", and while the generated code does print, the formatting isn't explicitly mentioned as a requirement, so it's a minor point. The feedback for the empty case is also slightly less specific than the ground truth. Overall, it's a very solid and functionally correct solution.",gemini-2.5-pro-preview,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 1) Calculate the conversion rate for users on ""Desktop""
# Filter for Desktop users
desktop_users_df = ...",9,"The generated code correctly defines the DataFrame, calculates the conversion rates for Desktop and Mobile users separately, and prints them clearly labeled. The calculation method is correct, and it includes appropriate handling for potential division by zero scenarios for both device types, which is good practice.

The code's approach to determining conversion rates (counting converted users and dividing by total users) is perfectly valid. The ground truth's use of `.mean()` on the boolean 'Converted' column is a slightly more idiomatic and concise pandas way to achieve the same result because `True` is treated as 1 and `False` as 0 in numerical operations, so the mean directly gives the proportion of `True` values. However, both methods are correct and produce the same output.

The addition of the commented-out `groupby` alternative is a nice touch, showing awareness of more advanced pandas techniques, although it wasn't explicitly asked for in the prompt. The commented-out code is also correct in its implementation for handling potential missing device types in the grouped result.

One minor point for improvement, which is not critical, is that pandas' `.mean()` on a boolean series is generally preferred for calculating proportions/rates as it's more concise. However, the current implementation is clear and correct.",gemini-2.5-pro-preview,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

# Create the DataFrame
df = pd.DataFrame(data)

# 1) For each uniqu...",10,"The generated code is excellent. It correctly defines the DataFrame exactly as specified in the task prompt. It then accurately calculates the average ""AirQualityIndex"" for each unique ""SensorID"" using `groupby(""SensorID"")[""AirQualityIndex""].mean()`. The output is clear, correctly formatted with two decimal places, and clearly indicates which average belongs to which sensor, fulfilling all requirements of the task. The code is well-structured, uses proper imports, and is highly readable.",gemini-2.5-pro-preview,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

# 1) Create a new feature ""EngagementScore""
df[""EngagementScore...",9,"The generated code correctly defines the DataFrame, calculates the EngagementScore, filters for Instagram posts, and accurately calculates their average EngagementScore. The code is well-structured and follows the task requirements.

However, there's a minor point where the ground truth is slightly more robust: the ground truth includes a check (`if not instagram_posts_df.empty:`) before calculating the mean. While for this specific dataset it's not an issue (as Instagram posts exist), in a more general scenario, attempting to calculate `mean()` on an empty DataFrame would result in `NaN` or an error depending on the pandas version and context. The ground truth also uses f-string formatting with `.2f` for the output, which is a nice touch for presentation.

These are minor improvements and don't detract significantly from the correctness or quality for this specific task.",gemini-2.5-pro-preview,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

# Define the initial DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"":...",10,"The generated code is correct, complete, and well-structured. It successfully defines the DataFrame, calculates the ""LengthOfStay"" column, filters for ""Flu"" diagnoses, and accurately computes and prints the average length of stay for flu patients. The code adheres to all requirements of the task.",gemini-2.5-pro-preview,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

# Define the movie ratings data
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}

# Create the pandas ...",9,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates and prints their average rating, fulfilling all task requirements. The code is well-structured and uses proper pandas syntax.

One minor improvement could be to format the output of the average rating to two decimal places for better readability, similar to the ground truth. Also, while not strictly necessary for this specific dataset given that Sci-Fi movies are guaranteed to exist, a more robust solution might include handling the edge case where no Sci-Fi movies are found (though this is more of a defensive programming consideration than a strict requirement for this task, which provides the data).",gemini-2.5-pro-preview,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

# Define the initial DataFrame
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

# 1) Filter the DataFrame to include o...",10,"The generated code correctly defines the DataFrame, filters for Toyota cars, and accurately calculates their average mileage. It adheres to all requirements of the task. The code is well-structured, uses appropriate pandas operations, and produces the correct output. The variable naming is clear and descriptive.",gemini-2.5-pro-preview,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

# Define the data
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, ...",9,"The generated code correctly defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat, addressing all the requirements. The code exhibits good readability and structure.

The only minor area for improvement is the print statement for the average attack. While functional, the ground truth code includes f-string formatting (`:.2f`) to limit the decimal places, which can be beneficial for presenting numerical results cleanly. Additionally, the ground truth includes an `if not fire_pokemon_df.empty:` check before calculating the mean, which is a good practice to prevent potential errors if the `fire_pokemon_df` were ever empty, although for this specific dataset it's not strictly necessary.

Overall, the generated code is highly correct and complete for the given task. The extra print statements showing the original and filtered DataFrames are not explicitly asked for but don't detract from the solution.",gemini-2.5-pro-preview,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import seaborn as sns
import plotly.express as px
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('titanic.csv')
except FileNotFoundError:
    print(""Error: titanic.csv not found. Make sure the file is in the current directory."")
    exit()

# --- Data Preprocessing ---
df_viz = df.copy()

# Fill missing Age values with the median
df_viz['Age'].fillna(df_viz['Age'].median(), inpl...",9,"The generated code is highly comprehensive and mostly aligns with the ground truth, demonstrating a strong understanding of the task requirements. It successfully creates all requested visualization types and incorporates advanced features like interactive plots and a dashboard layout with textual insights.

**Strong Points:**
- **Correctness and Completeness:** All 5 visualization requirements are met with appropriate chart types.
- **Data Preprocessing:** Handles missing values and creates necessary categorical features (AgeGroup, Pclass_str, Survived_str, FamilySize) for effective visualization.
- **Multi-panel Figure (1):** Uses `subplots` effectively, similar to ground truth. Adds annotations (percentage labels) to the bars, which is a nice touch for storytelling.
- **Heatmap (2):** Correctly pivots data and uses `reindex` to ensure proper ordering, similar to ground truth. Uses percentage formatting for annotations which is good.
- **Violin Plot (3):** Correctly uses `hue` and `split` for comparing distributions by gender and survival status. Uses specified colors, enhancing clarity.
- **Interactive Scatter Plot (4):** Excellent use of `plotly.express` with `color`, `symbol`, `hover_name`, `hover_data`, `log_y` (for Fare), and custom `color_discrete_map`/`symbol_map`. This is arguably more detailed and robust than the ground truth's Plotly implementation in terms of visual customization and hover information.
- **Dashboard (5):**
    - Uses `gridspec` for flexible layout, accurately mimicking a dashboard style.
    - Includes diverse plot types within the dashboard (bar plots, KDE plot, heatmap).
    - **Excellent addition:** The inclusion of ""Key Storylines"" as a dedicated text block is a significant improvement over the ground truth's simpler text insights, aligning better with the ""storytelling"" aspect of the prompt. It provides well-summarized observations.
    - Uses annotations on bar charts within the dashboard.

**Areas for Minor Improvement (Prevented a perfect 10):**
- **Consistency in Missing Value Handling:** The ground truth only fills `Age` and `Fare` NaNs, while the generated code also fills `Embarked` and `Cabin`. While filling these might be good practice for general data cleaning, the prompt's visualization tasks primarily focus on `Age`, `Fare`, `Pclass`, and `Sex/Gender`, so filling `Embarked` and `Cabin` wasn't strictly necessary for *these specific visualizations*. It's a very minor point, as it doesn't negatively impact the visualizations requested.
- **Redundant `ci=None` in Bar Plots:** For calculated mean survival rates, `ci=None` is often used (as in the generated code), but for `df.groupby(...).mean()`, the confidence interval isn't directly applicable in the same way `sns.barplot` typically calculates it for raw data. The ground truth avoided this by pre-calculating the means and then plotting. The generated code also pre-calculates for `AgeGroup` but not for `Pclass_str` or `Sex`. Both approaches work, but pre-calculation can sometimes be cleaner if you just want to display the mean without confidence intervals.
- **Slightly Less Polished Matplotlib Aesthetics (Minor):** While good, the Matplotlib plots in the ground truth often had slightly more refined title/label sizing and placement, and general `rcParams` configuration that wasn't explicitly set in the generated code's Matplotlib sections (though `sns.set_theme` helps a lot). For instance, the ground truth explicitly set `plt.rcParams['figure.dpi']` and `plt.rcParams['figure.figsize']` globally, which can be useful.

Overall, the generated code is very high quality and demonstrates a deep understanding of data visualization principles and libraries. The storytelling aspect with the detailed ""Key Storylines"" is a standout feature.",gemini-2.5-pro-preview,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

# Define the book inventory data
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
 ...",9,"The generated code correctly defines the DataFrame, filters it for the 'Dystopian' genre, and accurately calculates the average 'Stock' for the filtered books. It adheres to all specified requirements of the task. The code quality is good, with clear variable names and logical flow.

The only minor area for improvement, which is not strictly required by the prompt but present in the ground truth, would be adding a check `if not dystopian_books_df.empty:` before calculating the mean to handle cases where no 'Dystopian' books exist. This makes the code more robust against edge cases, although for the given dataset, it's not an issue. The formatting of the output for the average stock could also optionally be set to two decimal places for consistency if desired, as done in the ground truth (`:.2f`). However, these are minor points and do not detract from the core correctness and functionality.",gemini-2.5-pro-preview,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

def main():
    alpha = 0.05

    # Load data
    try:
        df = pd.read_csv('water_potability.csv')
    except FileNotFoundError:
        print(""Error: 'water_potability.csv' not found in the current directory."")
        print(""Please ensure the file is present or provide the correct path."")
        return

    # Handle missing values by imputing with the mean for numerical columns
    # This is a simple strategy; more...",9,"The generated code provides a very good solution to the problem, demonstrating a strong understanding of statistical testing and good programming practices.

Here's a breakdown based on the evaluation criteria:

**Correctness (5/5):**
*   **T-tests:** Correctly performs independent t-tests. It even includes an extra check for Levene's test to decide on `equal_var`, which is a statistically sound improvement over simply assuming unequal variance.
*   **Chi-square:** Correctly implements the chi-square goodness-of-fit test. Assumes a uniform distribution, which is a reasonable default when no specific ""expected distribution"" is provided.
*   **Normality Tests:** Correctly uses Shapiro-Wilk for normality. Includes a helpful note about the sample size limitation/reliability for Shapiro-Wilk.
*   **Cohen's d:** Implements Cohen's d calculation correctly, including robust handling for cases where variance might be zero or sample sizes too small. Provides interpretation ranges based on Cohen's guidelines.
*   **P-value/Interpretation:** Correctly displays p-values, test statistics, and provides clear interpretations based on the significance level.
*   **Missing Values:** Handles missing values by mean imputation, as specified.

**Code Quality (5/5):**
*   **Proper Imports:** All necessary imports (`pandas`, `numpy`, `scipy.stats`) are present.
*   **Structure:** The code is well-structured within a `main` function, with clear print statements and section headers, making the output readable and understandable.
*   **Comments:** Includes helpful comments explaining the logic, especially for the missing value imputation and Cohen's d calculation.
*   **Error Handling:** Includes `try-except` for file loading and checks for column existence.
*   **Readability:** Variable names are descriptive.

**Completeness (4/5):**
The solution addresses almost all requirements:
*   Conducts t-tests: Yes.
*   Performs chi-square: Yes.
*   Applies normality tests: Yes.
*   Calculates effect sizes (Cohen's d): Yes.
*   Displays p-values, test statistics, and interpretation: Yes.
*   Handles missing values: Yes.
*   Proper imports: Yes.

**Minor Improvements/Considerations (leading to a score of 9 instead of 10):**

1.  **Feature List Initialization:** The `numerical_features` list is hardcoded in the `main` function. While practically correct for `water_potability.csv`, a more robust approach (like the ground truth) would be to dynamically determine numerical columns excluding 'Potability' from the loaded DataFrame (`df.select_dtypes(include=np.number).columns.drop('Potability', errors='ignore')`). This makes the code more generalizable to similar datasets.
2.  **`nan_policy='raise'` in `ttest_ind`:** This is good practice. However, the `dropna()` calls for `group_potable` and `group_non_potable` right before the t-test are technically redundant if the `numerical_cols_with_na` imputation at the beginning handles all numerical missing values correctly. It doesn't harm, but it's an unnecessary step. The comment `(should be, but good practice)` acknowledges this.
3.  **Cohen's D Interpretation Ranges:** The generated code uses slightly different ranges for Cohen's d interpretation (e.g., `< 0.01` negligible, `< 0.2` very small, etc.) compared to the ground truth's more standard ""trivial, small, medium, large."" While there's no single universally agreed-upon set of ranges for Cohen's d, the ground truth's adherence to the more widely accepted common guidelines makes it slightly preferable for benchmark purposes. The generated code's ranges are more granular but might confuse someone expecting the typical ""small, medium, large"" categories.
4.  **Redundant `if feature not in df.columns` checks:** Since `numerical_features` is manually defined, these checks within the loops are not strictly necessary if one assumes the manual list is correct. If the list were dynamically generated, it would be redundant.

Overall, the generated code is highly functional, robust, and well-organized, demonstrating a very strong capability for statistical analysis. The points above are minor refinements rather than critical errors.",gemini-2.5-pro-preview,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
from sklearn.pipeline im...",9,"The generated code is robust, well-structured, and capable of performing the requested tasks. It demonstrates a solid understanding of data science workflows.

**Strong points:**
*   **Comprehensive Data Preprocessing:** Correctly handles missing values using median imputation and performs feature scaling. The train-test split is done with stratification, which is crucial for imbalanced datasets. It converts back to DataFrames after imputation/scaling, preserving column names, which is very helpful for feature importance later.
*   **Multiple Model Training & Evaluation:** Trains the specified models (Logistic Regression, Random Forest, SVM) and evaluates them using all requested metrics (accuracy, precision, recall, F1-score, ROC-AUC).
*   **Effective Visualizations:** Generates confusion matrices and ROC curves correctly for each model using `matplotlib` and `seaborn`. The ROC curve aggregation is also well-handled.
*   **Correct K-Fold Cross-Validation:** Implements k-fold cross-validation (k=5) and, critically, uses a `Pipeline` for the cross-validation, which correctly applies imputation and scaling *within* each fold, preventing data leakage. This is a very important detail that many implementations miss.
*   **Best Model Selection:** Compares models effectively based on a comprehensive set of test set metrics and cross-validation results.
*   **Feature Importance:** Correctly extracts and displays feature importances for Random Forest and Logistic Regression, and gracefully handles SVC which doesn't have a direct `feature_importances_` or `coef_` attribute. The visualization of feature importances is also well done.
*   **Code Structure and Readability:** The code is well-organized into logical sections with clear comments and appropriate function calls. The use of a `run_water_potability_classification()` function and `if __name__ == '__main__':` block is good practice.
*   **Robustness:** Includes error handling for file not found and addresses potential issues like `zero_division` in metrics.
*   **Reproducibility:** Uses a `RANDOM_SEED` for all random operations.

**Areas for minor improvement:**
*   **ROC Curve Display (Minor):** While the code generates ROC curves, it uses `roc_curve` and then plots them manually. The `RocCurveDisplay.from_estimator` (as used in the ground truth) is a slightly more modern and convenient way to handle ROC curve plotting directly from the model, and automatically adds the AUC score to the legend.
*   **SVM `predict_proba` Handling:** The fallback logic for `predict_proba` when it's not directly available for SVM (or other models) is a bit convoluted. While `SVC(probability=True)` is used, it's good to be aware that it can be computationally intensive, and sometimes `decision_function` is preferred. The scaling of `decision_function` to probabilities in the generated code is a very simplistic approach and not truly representative probabilities. For a production-ready model, one would typically train the SVC with `probability=True` or use a different model if calibrated probabilities are absolutely critical and SVC's `probability=True` overhead is too high. However, for a benchmark, this is a minor point as `probability=True` is correctly set.
*   **Consistency in `X_train_scaled` naming:** Initially, `X_train_scaled = scaler.fit_transform(X_train)` results in a numpy array. Then it's converted to `X_train_scaled_df = pd.DataFrame(...)`. Consistency from the start (e.g., using `X_train_scaled_df` throughout for scaled dataframes) would slightly improve clarity. This is largely stylistic.

Overall, this is an excellent solution that meets almost all requirements with high quality.",gemini-2.5-pro-preview,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer

def perform_feature_analysis():
    # 1. Load and Prepare Data
    try:
        df = pd.read_csv('water_potability.csv')
    except FileNotFoundError:
        print(""Er...",9,"The generated code is highly correct and comprehensive, addressing nearly all aspects of the detailed task.

Strengths:
- **Comprehensive Feature Engineering**: The code successfully creates ratio features, polynomial features, and interaction terms, adhering to the task's requirement for ""meaningful features.""
- **Robust Missing Value Handling**: Uses `SimpleImputer` with 'median' strategy, which is a good choice for numerical data and handles missing values effectively.
- **Thorough Correlation Analysis**: Correctly calculates the correlation matrix and identifies the top 5 features most correlated with potability, excluding 'Potability' itself.
- **Multiple Feature Selection Methods**: Successfully implements both `SelectKBest` (with `f_classif`) and `RFE` (with `RandomForestClassifier`), showing a good understanding of feature selection techniques.
- **Advanced Visualizations**: Generates all requested visualizations (correlation heatmap, feature importance plot, and pairwise scatter plots). The plot titles, labels, and sizing are well-chosen for clarity. It even saves the plots to files, which is a nice touch.
- **Comparative Analysis of Feature Sets**: Provides a clear comparison of the feature sets selected by `SelectKBest` and `RFE`, including overlap and unique features.
- **Code Quality**: Proper imports, clear variable names, and logical flow are evident. The use of a function `perform_feature_analysis()` encapsulates the logic well.
- **Error Handling**: Includes `try-except` for file not found, which is good practice.

Areas for Minor Improvement:
- **Division by Zero in Ratios**: While `epsilon` is used, the ground truth code specifically lists which original columns have missing values and imputes them before feature engineering, ensuring `ph` and `Sulfate` are handled correctly before appearing in ratio denominators, which the generated code does too using an imputer first. The selected ratio features are good.
- **Polynomial Feature Naming and Duplication**: The generated code's approach to polynomial features using `poly.get_feature_names_out` and then concatenating `df_poly_features` which already contains original features with `df_ratios` is generally sound. However, the ground truth explicitly iterates to only add new features to `X_fe` if they aren't already present after `get_feature_names_out` to avoid potential redundant columns (e.g., 'ph' appearing twice if `df_poly_features` and `df_ratios` both add it inadvertently, though `PolynomialFeatures` typically returns the original column name as well). The generated approach with `df_poly_features` containing the original columns naturally and then `df_ratios` adding new ones via `pd.concat` is efficient and conceptually fine, as `pd.concat` handles duplicate column names by default.

Overall, the generated code is excellent. It demonstrates strong capabilities in data science methodologies, including feature engineering, statistical analysis, and robust machine learning techniques, coupled with good coding practices. The minor points are more about stylistic differences in feature engineering concatenation / naming rather than outright errors.",gemini-2.5-pro-preview,LLM_judge_config
